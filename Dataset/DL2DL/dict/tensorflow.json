{"tf": {"tf.AggregationMethod": {"description": "A class listing aggregation methods used to combine gradients.", "Class Variables": {"ADD_N": "0", "DEFAULT": "0", "EXPERIMENTAL_ACCUMULATE_N": "2", "EXPERIMENTAL_TREE": "1"}}, "tf.CriticalSection": {"description": "Critical section.", "Attributes": {"name": ""}}, "tf.DeviceSpec": {"description": "Represents a (possibly partial) specification for a TensorFlow device.", "Args": {"job": "string.  Optional job name.", "replica": "int.  Optional replica index.", "task": "int.  Optional task index.", "device_type": "Optional device type string (e.g. \"CPU\" or \"GPU\")", "device_index": "int.  Optional device index.  If left unspecified, device\nrepresents 'any' device_index."}, "Attributes": {"device_index": "", "device_type": "", "job": "", "replica": "", "task": ""}}, "tf.GradientTape": {"description": "Record operations for automatic differentiation.", "Args": {"persistent": "Boolean controlling whether a persistent gradient tape\nis created. False by default, which means at most one call can\nbe made to the gradient() method on this object.", "watch_accessed_variables": "Boolean controlling whether the tape will\nautomatically watch any (trainable) variables accessed while the tape\nis active. Defaults to True meaning gradients can be requested from any\nresult computed in the tape derived from reading a trainable Variable.\nIf False users must explicitly watch any Variables they want to\nrequest gradients from."}}, "tf.Graph": {"description": "A TensorFlow computation, represented as a dataflow graph.", "Attributes": {"building_function": "Returns True iff this graph represents a function.", "collections": "Returns the names of the collections known to this graph.", "finalized": "True if this graph has been finalized.", "graph_def_versions": "The GraphDef version information of this graph.\nFor details on the meaning of each version, see\nGraphDef.", "seed": "The graph-level random seed of this graph.", "version": "Returns a version number that increases as ops are added to the graph.\nNote that this is unrelated to the\ntf.Graph.graph_def_versions."}}, "tf.IndexedSlices": {"description": "A sparse representation of a set of tensor slices at given indices.", "Attributes": {"dense_shape": "A 1-D Tensor containing the shape of the corresponding dense tensor.", "device": "The name of the device on which values will be produced, or None.", "dtype": "The DType of elements in this tensor.", "graph": "The Graph that contains the values, indices, and shape tensors.", "indices": "A 1-D Tensor containing the indices of the slices.", "name": "The name of this IndexedSlices.", "op": "The Operation that produces values as an output.", "shape": "Gets the tf.TensorShape representing the shape of the dense tensor.", "values": "A Tensor containing the values of the slices."}}, "tf.IndexedSlicesSpec": {"description": "Type specification for a tf.IndexedSlices.", "Args": {"shape": "The dense shape of the IndexedSlices, or None to allow any\ndense shape.", "dtype": "tf.DType of values in the IndexedSlices.", "indices_dtype": "tf.DType of the indices in the IndexedSlices.  One\nof tf.int32 or tf.int64.", "dense_shape_dtype": "tf.DType of the dense_shape in the IndexedSlices.\nOne of tf.int32, tf.int64, or None (if the IndexedSlices has\nno dense_shape tensor).", "indices_shape": "The shape of the indices component, which indicates\nhow many slices are in the IndexedSlices."}, "Attributes": {"value_type": ""}}, "tf.Module": {"description": "Base neural network module class.", "Attributes": {"name": "Returns the name of this module as passed or determined in the ctor.\nNote: This is not the same as the self.name_scope.name which includes\nparent module names.", "name_scope": "Returns a tf.name_scope instance for this class.", "non_trainable_variables": "Sequence of non-trainable variables owned by this module and its submodules.Note: this method uses reflection to find variables on the current instance\nand submodules. For performance reasons you may wish to cache the result\nof calling this method if you don't expect the return value to change.", "submodules": "Sequence of all sub-modules.\nSubmodules are modules which are properties of this module, or found as\nproperties of modules which are properties of this module (and so on).\na = tf.Module()b = tf.Module()c = tf.Module()a.b = bb.c = clist(a.submodules) == [b, c]Truelist(b.submodules) == [c]Truelist(c.submodules) == []True", "trainable_variables": "Sequence of trainable variables owned by this module and its submodules.\nNote: this method uses reflection to find variables on the current instance\nand submodules. For performance reasons you may wish to cache the result\nof calling this method if you don't expect the return value to change.", "variables": "Sequence of variables owned by this module and its submodules.Note: this method uses reflection to find variables on the current instance\nand submodules. For performance reasons you may wish to cache the result\nof calling this method if you don't expect the return value to change."}}, "tf.Operation": {"description": "Represents a graph node that performs computation on tensors.", "Args": {"node_def": "node_def_pb2.NodeDef.  NodeDef for the Operation. Used for\nattributes of node_def_pb2.NodeDef, typically name, op, and\ndevice.  The input attribute is irrelevant here as it will be\ncomputed when generating the model.", "g": "Graph. The parent graph.", "inputs": "list of Tensor objects. The inputs to this Operation.", "output_types": "list of DType objects.  List of the types of the Tensors\ncomputed by this operation.  The length of this list indicates the\nnumber of output endpoints of the Operation.", "control_inputs": "list of operations or tensors from which to have a control\ndependency.", "input_types": "List of DType objects representing the types of the tensors\naccepted by the Operation.  By default uses [x.dtype.base_dtype for x\nin inputs].  Operations that expect reference-typed inputs must specify\nthese explicitly.", "original_op": "Optional. Used to associate the new Operation with an\nexisting Operation (for example, a replica with the op that was\nreplicated).", "op_def": "Optional. The op_def_pb2.OpDef proto that describes the op type\nthat this Operation represents."}, "Raises": {"TypeError": "if control inputs are not Operations or Tensors,\nor if node_def is not a NodeDef,\nor if g is not a Graph,\nor if inputs are not tensors,\nor if inputs and input_types are incompatible.", "ValueError": "if the node_def name is not valid."}, "Attributes": {"control_inputs": "The Operation objects on which this op has a control dependency.\nBefore this op is executed, TensorFlow will ensure that the\noperations in self.control_inputs have finished executing. This\nmechanism can be used to run ops sequentially for performance\nreasons, or to ensure that the side effects of an op are observed\nin the correct order.", "device": "The name of the device to which this op has been assigned, if any.", "graph": "The Graph that contains this operation.", "inputs": "The sequence of Tensor objects representing the data inputs of this op.", "name": "The full name of this operation.", "node_def": "Returns the NodeDef representation of this operation.", "op_def": "Returns the OpDef proto that represents the type of this op.", "outputs": "The list of Tensor objects representing the outputs of this op.", "traceback": "Returns the call stack from when this operation was constructed.", "type": "The type of the op (e.g. \"MatMul\")."}}, "tf.OptionalSpec": {"description": "Type specification for tf.experimental.Optional.", "Attributes": {"element_spec": "A (nested) structure of TypeSpec objects that represents the\ntype specification of the optional element.", "value_type": "The Python type for values that are compatible with this TypeSpec.\nIn particular, all values that are compatible with this TypeSpec must be an\ninstance of this type."}}, "tf.RaggedTensor": {"description": "Represents a ragged tensor.", "Attributes": {"dtype": "The DType of values in this tensor.", "flat_values": "The innermost values tensor for this ragged tensor.\nConcretely, if rt.values is a Tensor, then rt.flat_values is\nrt.values; otherwise, rt.flat_values is rt.values.flat_values.\nConceptually, flat_values is the tensor formed by flattening the\noutermost dimension and all of the ragged dimensions into a single\ndimension.\nrt.flat_values.shape = [nvals] + rt.shape[rt.ragged_rank + 1:]\n(where nvals is the number of items in the flattened dimensions).\nExample:\nrt = tf.ragged.constant([[[3, 1, 4, 1], [], [5, 9, 2]], [], [[6], []]])print(rt.flat_values)tf.Tensor([3 1 4 1 5 9 2 6], shape=(8,), dtype=int32)", "nested_row_splits": "A tuple containing the row_splits for all ragged dimensions.\nrt.nested_row_splits is a tuple containing the row_splits tensors for\nall ragged dimensions in rt, ordered from outermost to innermost.  In\nparticular, rt.nested_row_splits = (rt.row_splits,) + value_splits where:\n* `value_splits = ()` if `rt.values` is a `Tensor`.* `value_splits = rt.values.nested_row_splits` otherwise.\nExample:\nrt = tf.ragged.constant(\u00a0 \u00a0 [[[[3, 1, 4, 1], [], [5, 9, 2]], [], [[6], []]]])for i, splits in enumerate(rt.nested_row_splits):\u00a0 print('Splits for dimension %d: %s' % (i+1, splits.numpy()))Splits for dimension 1: [0 3]Splits for dimension 2: [0 3 3 5]Splits for dimension 3: [0 4 4 7 8 8]", "ragged_rank": "The number of times the RaggedTensor's flat_values is partitioned.\nvalues = tf.ragged.constant([[1, 2, 3], [4], [5, 6], [7, 8, 9, 10]])values.ragged_rank1\nrt = tf.RaggedTensor.from_uniform_row_length(values, 2)rt.ragged_rank2", "row_splits": "The row-split indices for this ragged tensor's values.\nrt.row_splits specifies where the values for each row begin and end in\nrt.values.  In particular, the values for row rt[i] are stored in\nthe slice rt.values[rt.row_splits[i]:rt.row_splits[i+1]].\nExample:\nrt = tf.ragged.constant([[3, 1, 4, 1], [], [5, 9, 2], [6], []])print(rt.row_splits) \u00a0# indices of row splits in rt.valuestf.Tensor([0 4 4 7 8 8], shape=(6,), dtype=int64)", "shape": "The statically known shape of this ragged tensor.\ntf.ragged.constant([[0], [1, 2]]).shapeTensorShape([2, None])\ntf.ragged.constant([[[0, 1]], [[1, 2], [3, 4]]], ragged_rank=1).shapeTensorShape([2, None, 2])", "uniform_row_length": "The length of each row in this ragged tensor, or None if rows are ragged.\nrt1 = tf.ragged.constant([[1, 2, 3], [4], [5, 6], [7, 8, 9, 10]])print(rt1.uniform_row_length) \u00a0# rows are ragged.None\nrt2 = tf.RaggedTensor.from_uniform_row_length(\u00a0 \u00a0 values=rt1, uniform_row_length=2)print(rt2)<tf.RaggedTensor [[[1, 2, 3], [4]], [[5, 6], [7, 8, 9, 10]]]>print(rt2.uniform_row_length) \u00a0# rows are not ragged (all have size 2).tf.Tensor(2, shape=(), dtype=int64)\nA RaggedTensor's rows are only considered to be uniform (i.e. non-ragged)\nif it can be determined statically (at graph construction time) that the\nrows all have the same length.", "values": "The concatenated rows for this ragged tensor.\nrt.values is a potentially ragged tensor formed by flattening the two\noutermost dimensions of rt into a single dimension.\nrt.values.shape = [nvals] + rt.shape[2:] (where nvals is the\nnumber of items in the outer two dimensions of rt).\nrt.ragged_rank = self.ragged_rank - 1\nExample:\nrt = tf.ragged.constant([[3, 1, 4, 1], [], [5, 9, 2], [6], []])print(rt.values)tf.Tensor([3 1 4 1 5 9 2 6], shape=(8,), dtype=int32)"}}, "tf.RaggedTensorSpec": {"description": "Type specification for a tf.RaggedTensor.", "Args": {"shape": "The shape of the RaggedTensor, or None to allow any shape.  If a\nshape is specified, then all ragged dimensions must have size None.", "dtype": "tf.DType of values in the RaggedTensor.", "ragged_rank": "Python integer, the number of times the RaggedTensor's\nflat_values is partitioned.  Defaults to shape.ndims - 1.", "row_splits_dtype": "dtype for the RaggedTensor's row_splits tensor. One\nof tf.int32 or tf.int64.", "flat_values_spec": "TypeSpec for flat_value of the RaggedTensor. It shall be\nprovided when the flat_values is a CompositeTensor rather then Tensor.\nIf both dtype and flat_values_spec and  are provided, dtype must\nbe the same as flat_values_spec.dtype. (experimental)"}, "Attributes": {"dtype": "The tf.dtypes.DType specified by this type for the RaggedTensor.\nrt = tf.ragged.constant([[\"a\"], [\"b\", \"c\"]], dtype=tf.string)tf.type_spec_from_value(rt).dtypetf.string", "flat_values_spec": "The TypeSpec of the flat_values of RaggedTensor.", "ragged_rank": "The number of times the RaggedTensor's flat_values is partitioned.\nDefaults to shape.ndims - 1.\nvalues = tf.ragged.constant([[1, 2, 3], [4], [5, 6], [7, 8, 9, 10]])tf.type_spec_from_value(values).ragged_rank1\nrt1 = tf.RaggedTensor.from_uniform_row_length(values, 2)tf.type_spec_from_value(rt1).ragged_rank2", "row_splits_dtype": "The tf.dtypes.DType of the RaggedTensor's row_splits.\nrt = tf.ragged.constant([[1, 2, 3], [4]], row_splits_dtype=tf.int64)tf.type_spec_from_value(rt).row_splits_dtypetf.int64", "shape": "The statically known shape of the RaggedTensor.\nrt = tf.ragged.constant([[0], [1, 2]])tf.type_spec_from_value(rt).shapeTensorShape([2, None])\nrt = tf.ragged.constant([[[0, 1]], [[1, 2], [3, 4]]], ragged_rank=1)tf.type_spec_from_value(rt).shapeTensorShape([2, None, 2])", "value_type": "The Python type for values that are compatible with this TypeSpec.\nIn particular, all values that are compatible with this TypeSpec must be an\ninstance of this type."}}, "tf.RegisterGradient": {"description": "A decorator for registering the gradient function for an op type.", "Args": {"op_type": "The string type of an operation. This corresponds to the\nOpDef.name field for the proto that defines the operation."}, "Raises": {"TypeError": "If op_type is not string."}}, "tf.SparseTensorSpec": {"description": "Type specification for a tf.sparse.SparseTensor.", "Args": {"shape": "The dense shape of the SparseTensor, or None to allow any dense\nshape.", "dtype": "tf.DType of values in the SparseTensor."}, "Attributes": {"dtype": "The tf.dtypes.DType specified by this type for the SparseTensor.", "shape": "The tf.TensorShape specified by this type for the SparseTensor.", "value_type": ""}}, "tf.Tensor": {"description": "A tf.Tensor represents a multidimensional array of elements.", "Args": {"op": "An Operation. Operation that computes this tensor.", "value_index": "An int. Index of the operation's endpoint that produces\nthis tensor.", "dtype": "A DType. Type of elements stored in this tensor."}, "Raises": {"TypeError": "If the op is not an Operation."}, "Attributes": {"device": "The name of the device on which this tensor will be produced, or None.", "dtype": "The DType of elements in this tensor.", "graph": "The Graph that contains this tensor.", "name": "The string name of this tensor.", "op": "The Operation that produces this tensor as an output.", "shape": "Returns a tf.TensorShape that represents the shape of this tensor.\nt = tf.constant([1,2,3,4,5])t.shapeTensorShape([5])\ntf.Tensor.shape is equivalent to tf.Tensor.get_shape().\nIn a tf.function or when building a model using\ntf.keras.Input, they return the build-time shape of the\ntensor, which may be partially unknown.\nA tf.TensorShape is not a tensor. Use tf.shape(t) to get a tensor\ncontaining the shape, calculated at runtime.\nSee tf.Tensor.get_shape(), and tf.TensorShape for details and examples.", "value_index": "The index of this tensor in the outputs of its Operation."}, "Class Variables": {"OVERLOADABLE_OPERATORS": "{\u00a0'__abs__',\u00a0'__add__',\u00a0'__and__',\u00a0'__div__',\u00a0'__eq__',\u00a0'__floordiv__',\u00a0'__ge__',\u00a0'__getitem__',\u00a0'__gt__',\u00a0'__invert__',\u00a0'__le__',\u00a0'__lt__',\u00a0'__matmul__',\u00a0'__mod__',\u00a0'__mul__',\u00a0'__ne__',\u00a0'__neg__',\u00a0'__or__',\u00a0'__pow__',\u00a0'__radd__',\u00a0'__rand__',\u00a0'__rdiv__',\u00a0'__rfloordiv__',\u00a0'__rmatmul__',\u00a0'__rmod__',\u00a0'__rmul__',\u00a0'__ror__',\u00a0'__rpow__',\u00a0'__rsub__',\u00a0'__rtruediv__',\u00a0'__rxor__',\u00a0'__sub__',\u00a0'__truediv__',\u00a0'__xor__'}"}}, "tf.TensorArray": {"description": "Class wrapping dynamic-sized, per-time-step, write-once Tensor arrays.", "Args": {"dtype": "(required) data type of the TensorArray.", "size": "(optional) int32 scalar Tensor: the size of the TensorArray.\nRequired if handle is not provided.", "dynamic_size": "(optional) Python bool: If true, writes to the TensorArray\ncan grow the TensorArray past its initial size.  Default: False.", "clear_after_read": "Boolean (optional, default: True).  If True, clear\nTensorArray values after reading them.  This disables read-many\nsemantics, but allows early release of memory.", "tensor_array_name": "(optional) Python string: the name of the TensorArray.\nThis is used when creating the TensorArray handle.  If this value is\nset, handle should be None.", "handle": "(optional) A Tensor handle to an existing TensorArray.  If this\nis set, tensor_array_name should be None. Only supported in graph mode.", "flow": "(optional) A float Tensor scalar coming from an existing\nTensorArray.flow. Only supported in graph mode.", "infer_shape": "(optional, default: True) If True, shape inference is\nenabled.  In this case, all elements must have the same shape.", "element_shape": "(optional, default: None) A TensorShape object specifying\nthe shape constraints of each of the elements of the TensorArray. Need\nnot be fully defined.", "colocate_with_first_write_call": "If True, the TensorArray will be\ncolocated on the same device as the Tensor used on its first write\n(write operations include write, unstack, and split).  If False,\nthe TensorArray will be placed on the device determined by the device\ncontext available during its initialization.", "name": "A name for the operation (optional)."}, "Raises": {"ValueError": "if both handle and tensor_array_name are provided.", "TypeError": "if handle is provided but is not a Tensor."}, "Attributes": {"dtype": "The data type of this TensorArray.", "dynamic_size": "Python bool; if True the TensorArray can grow dynamically.", "element_shape": "The tf.TensorShape of elements in this TensorArray.", "flow": "The flow Tensor forcing ops leading to this TensorArray state.", "handle": "The reference to the TensorArray."}}, "tf.TensorArraySpec": {"description": "Type specification for a tf.TensorArray.", "Args": {"element_shape": "The shape of each element in the TensorArray.", "dtype": "Data type of the TensorArray.", "dynamic_size": "Whether the TensorArray can grow past its initial size.", "infer_shape": "Whether shape inference is enabled."}, "Attributes": {"value_type": ""}}, "tf.TensorShape": {"description": "Represents the shape of a Tensor.", "Args": {"dims": "A list of Dimensions, or None if the shape is unspecified."}, "Raises": {"TypeError": "If dims cannot be converted to a list of dimensions."}, "Attributes": {"dims": "Deprecated.  Returns list of dimensions for this shape.\nSuggest TensorShape.as_list instead.", "ndims": "Deprecated accessor for rank.", "rank": "Returns the rank of this shape, or None if it is unspecified."}}, "tf.TensorSpec": {"description": "Describes a tf.Tensor.", "Args": {"shape": "Value convertible to tf.TensorShape. The shape of the tensor.", "dtype": "Value convertible to tf.DType. The type of the tensor values.", "name": "Optional name for the Tensor."}, "Raises": {"TypeError": "If shape is not convertible to a tf.TensorShape, or dtype is\nnot convertible to a tf.DType."}, "Attributes": {"dtype": "Returns the dtype of elements in the tensor.", "name": "Returns the (optionally provided) name of the described tensor.", "shape": "Returns the TensorShape that represents the shape of the tensor.", "value_type": "The Python type for values that are compatible with this TypeSpec."}}, "tf.TypeSpec": {"description": "Specifies a TensorFlow value type.", "Attributes": {"value_type": "The Python type for values that are compatible with this TypeSpec.\nIn particular, all values that are compatible with this TypeSpec must be an\ninstance of this type."}}, "tf.UnconnectedGradients": {"description": "Controls how gradient computation behaves when y does not depend on x.", "Class Variables": {"NONE": "<UnconnectedGradients.NONE: 'none'>", "ZERO": "<UnconnectedGradients.ZERO: 'zero'>"}}, "tf.Variable": {"description": "See the [variable guide](https://tensorflow.org/guide/variable).", "Args": {"initial_value": "A Tensor, or Python object convertible to a Tensor,\nwhich is the initial value for the Variable. The initial value must have\na shape specified unless validate_shape is set to False. Can also be a\ncallable with no argument that returns the initial value when called. In\nthat case, dtype must be specified. (Note that initializer functions\nfrom init_ops.py must first be bound to a shape before being used here.)", "trainable": "If True, GradientTapes automatically watch uses of this\nvariable. Defaults to True, unless synchronization is set to\nON_READ, in which case it defaults to False.", "validate_shape": "If False, allows the variable to be initialized with a\nvalue of unknown shape. If True, the default, the shape of\ninitial_value must be known.", "caching_device": "Note: This argument is only valid when using a v1-style\nSession. Optional device string describing where the Variable should\nbe cached for reading. Defaults to the Variable's device. If not None,\ncaches on another device. Typical use is to cache on the device where\nthe Ops using the Variable reside, to deduplicate copying through\nSwitch and other conditional statements.", "name": "Optional name for the variable. Defaults to 'Variable' and gets\nuniquified automatically.", "variable_def": "VariableDef protocol buffer. If not None, recreates the\nVariable object with its contents, referencing the variable's nodes in\nthe graph, which must already exist. The graph is not changed.\nvariable_def and the other arguments are mutually exclusive.", "dtype": "If set, initial_value will be converted to the given type. If\nNone, either the datatype will be kept (if initial_value is a\nTensor), or convert_to_tensor will decide.", "import_scope": "Optional string. Name scope to add to the Variable. Only\nused when initializing from protocol buffer.", "constraint": "An optional projection function to be applied to the variable\nafter being updated by an Optimizer (e.g. used to implement norm\nconstraints or value constraints for layer weights). The function must\ntake as input the unprojected Tensor representing the value of the\nvariable and return the Tensor for the projected value (which must have\nthe same shape). Constraints are not safe to use when doing asynchronous\ndistributed training.", "synchronization": "Indicates when a distributed a variable will be\naggregated. Accepted values are constants defined in the class\ntf.VariableSynchronization. By default the synchronization is set to\nAUTO and the current DistributionStrategy chooses when to\nsynchronize.", "aggregation": "Indicates how a distributed variable will be aggregated.\nAccepted values are constants defined in the class\ntf.VariableAggregation.", "shape": "(optional) The shape of this variable. If None, the shape of\ninitial_value will be used. When setting this argument to\ntf.TensorShape(None) (representing an unspecified shape), the variable\ncan be assigned with values of different shapes."}, "Raises": {"ValueError": "If the initial value is not specified, or does not have a\nshape and validate_shape is True."}, "Attributes": {"aggregation": "", "constraint": "Returns the constraint function associated with this variable.", "device": "The device of this variable.", "dtype": "The DType of this variable.", "graph": "The Graph of this variable.", "initial_value": "Returns the Tensor used as the initial value for the variable.\nNote that this is different from initialized_value() which runs\nthe op that initializes the variable before returning its value.\nThis method returns the tensor that is used by the op that initializes\nthe variable.", "initializer": "The initializer operation for this variable.", "name": "The name of this variable.", "op": "The Operation of this variable.", "shape": "The TensorShape of this variable.", "synchronization": "", "trainable": ""}}, "tf.Variable.SaveSliceInfo": {"description": "Information on how to save this Variable as a slice.", "Args": {"full_name": "Name of the full variable of which this Variable is a\nslice.", "full_shape": "Shape of the full variable, as a list of int.", "var_offset": "Offset of this Variable into the full variable, as a list\nof int.", "var_shape": "Shape of this Variable, as a list of int.", "save_slice_info_def": "SaveSliceInfoDef protocol buffer. If not None,\nrecreates the SaveSliceInfo object its contents. save_slice_info_def\nand other arguments are mutually exclusive.", "import_scope": "Optional string. Name scope to add. Only used when\ninitializing from protocol buffer."}, "Attributes": {"spec": "Computes the spec string used for saving."}}, "tf.VariableAggregation": {"description": "Indicates how a distributed variable will be aggregated.", "Class Variables": {"MEAN": "<VariableAggregationV2.MEAN: 2>", "NONE": "<VariableAggregationV2.NONE: 0>", "ONLY_FIRST_REPLICA": "<VariableAggregationV2.ONLY_FIRST_REPLICA: 3>", "SUM": "<VariableAggregationV2.SUM: 1>"}}, "tf.VariableSynchronization": {"description": "Indicates when a distributed variable will be synced.", "Class Variables": {"AUTO": "<VariableSynchronization.AUTO: 0>", "NONE": "<VariableSynchronization.NONE: 1>", "ON_READ": "<VariableSynchronization.ON_READ: 3>", "ON_WRITE": "<VariableSynchronization.ON_WRITE: 2>"}}, "tf.approx_top_k": {"description": "Returns min/max k values and their indices of the input operand in an approximate manner.", "Args": {"input": "A Tensor. Must be one of the following types: half, bfloat16, float32.\nArray to search. Must be at least 1-D of the floating type", "k": "An int that is >= 0. Specifies the number of min/max-k.", "reduction_dimension": "An optional int. Defaults to -1.\nInteger dimension along which to search. Default: -1.", "recall_target": "An optional float. Defaults to 0.95.\nRecall target for the approximation. Range in (0,1]", "is_max_k": "An optional bool. Defaults to True.\nWhen true, computes max-k; otherwise computes min-k.", "reduction_input_size_override": "An optional int. Defaults to -1.\nWhen set to a positive value, it overrides the size determined by\ninput[reduction_dim] for evaluating the recall. This option is useful when\nthe given input is only a subset of the overall computation in SPMD or\ndistributed pipelines, where the true input size cannot be deferred by the\ninput shape.", "aggregate_to_topk": "An optional bool. Defaults to True.\nWhen true, aggregates approximate results to top-k. When false, returns the\napproximate results. The number of the approximate results is implementation\ndefined and is greater equals to the specified k.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (values, indices)."}, "tf.argsort": {"description": "Returns the indices of a tensor that give its sorted order along an axis.", "Args": {"values": "1-D or higher numeric Tensor.", "axis": "The axis along which to sort. The default is -1, which sorts the last\naxis.", "direction": "The direction in which to sort the values ('ASCENDING' or\n'DESCENDING').", "stable": "If True, equal elements in the original tensor will not be\nre-ordered in the returned order. Unstable sort is not yet implemented,\nbut will eventually be the default for performance reasons. If you require\na stable order, pass stable=True for forwards compatibility.", "name": "Optional name for the operation."}, "Returns": "An int32 Tensor with the same shape as values. The indices that would\nsort each slice of the given values along the given axis.", "Raises": {"ValueError": "If axis is not a constant scalar, or the direction is invalid.", "tf.errors.InvalidArgumentError": "If the values.dtype is not a float or\nint type."}}, "tf.batch_to_space": {"description": "BatchToSpace for N-D tensors of type T.", "Args": {"input": "A N-D Tensor with shape input_shape = [batch] + spatial_shape +\nremaining_shape, where spatial_shape has M dimensions.", "block_shape": "A 1-D Tensor with shape [M]. Must be one of the following\ntypes: int32, int64. All values must be >= 1. For backwards\ncompatibility with TF 1.0, this parameter may be an int, in which case it\nis converted to\nnumpy.array([block_shape, block_shape],\ndtype=numpy.int64).", "crops": "A  2-D Tensor with shape [M, 2]. Must be one of the\nfollowing types: int32, int64. All values must be >= 0.\ncrops[i] = [crop_start, crop_end] specifies the amount to crop from\ninput dimension i + 1, which corresponds to spatial dimension i.\nIt is required that\ncrop_start[i] + crop_end[i] <= block_shape[i] * input_shape[i + 1].\nThis operation is equivalent to the following steps:\n\nReshape input to reshaped of shape: [block_shape[0], ...,\nblock_shape[M-1], batch / prod(block_shape), input_shape[1], ...,\ninput_shape[N-1]]\nPermute dimensions of reshaped to produce permuted of shape\n[batch / prod(block_shape),  input_shape[1], block_shape[0], ...,\ninput_shape[M], block_shape[M-1], input_shape[M+1],\n..., input_shape[N-1]]\nReshape permuted to produce reshaped_permuted of shape\n[batch / prod(block_shape), input_shape[1] * block_shape[0], ...,\ninput_shape[M] * block_shape[M-1], input_shape[M+1], ...,\ninput_shape[N-1]]\nCrop the start and end of dimensions [1, ..., M] of\nreshaped_permuted according to crops to produce the output\nof shape:\n[batch / prod(block_shape),  input_shape[1] *\n block_shape[0] - crops[0,0] - crops[0,1], ..., input_shape[M] *\n block_shape[M-1] - crops[M-1,0] - crops[M-1,1],  input_shape[M+1],\n ..., input_shape[N-1]]", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.bitcast": {"description": "Bitcasts a tensor from one type to another without copying data.", "Args": {"input": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, int64, int32, uint8, uint16, uint32, uint64, int8, int16, complex64, complex128, qint8, quint8, qint16, quint16, qint32.", "type": "A tf.DType from: tf.bfloat16, tf.half, tf.float32, tf.float64, tf.int64, tf.int32, tf.uint8, tf.uint16, tf.uint32, tf.uint64, tf.int8, tf.int16, tf.complex64, tf.complex128, tf.qint8, tf.quint8, tf.qint16, tf.quint16, tf.qint32.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type type."}, "tf.boolean_mask": {"description": "Apply boolean mask to tensor.", "Args": {"tensor": "N-D Tensor.", "mask": "K-D boolean Tensor, K <= N and K must be known statically.", "axis": "A 0-D int Tensor representing the axis in tensor to mask from. By\ndefault, axis is 0 which will mask from the first dimension. Otherwise K +\naxis <= N.", "name": "A name for this operation (optional)."}, "Returns": "(N-K+1)-dimensional tensor populated by entries in tensor corresponding\nto True values in mask.", "Raises": {"ValueError": "If shapes do not conform."}}, "tf.broadcast_dynamic_shape": {"description": "Computes the shape of a broadcast given symbolic shapes.", "Args": {"shape_x": "A rank 1 integer Tensor, representing the shape of x.", "shape_y": "A rank 1 integer Tensor, representing the shape of y."}, "Returns": "A rank 1 integer Tensor representing the broadcasted shape.", "Raises": {"InvalidArgumentError": "If the two shapes are incompatible for\nbroadcasting."}}, "tf.broadcast_static_shape": {"description": "Computes the shape of a broadcast given known shapes.", "Args": {"shape_x": "A TensorShape", "shape_y": "A TensorShape"}, "Returns": "A TensorShape representing the broadcasted shape.", "Raises": {"ValueError": "If the two shapes can not be broadcasted."}}, "tf.broadcast_to": {"description": "Broadcast an array for a compatible shape.", "Args": {"input": "A Tensor. A Tensor to broadcast.", "shape": "A Tensor. Must be one of the following types: int32, int64.\nAn 1-D int Tensor. The shape of the desired output.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.case": {"description": "Create a case operation.", "Args": {"pred_fn_pairs": "List of pairs of a boolean scalar tensor and a callable which\nreturns a list of tensors.", "default": "Optional callable that returns a list of tensors.", "exclusive": "True iff at most one predicate is allowed to evaluate to True.", "strict": "A boolean that enables/disables 'strict' mode; see above.", "name": "A name for this operation (optional)."}, "Returns": "The tensors returned by the first pair whose predicate evaluated to True, or\nthose returned by default if none does.", "Raises": {"TypeError": "If fns[i] is not callable for any i, or default is not\ncallable."}}, "tf.cast": {"description": "Casts a tensor to a new type.", "Args": {"x": "A Tensor or SparseTensor or IndexedSlices of numeric type. It could\nbe uint8, uint16, uint32, uint64, int8, int16, int32,\nint64, float16, float32, float64, complex64, complex128,\nbfloat16.", "dtype": "The destination type. The list of supported dtypes is the same as\nx.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor or SparseTensor or IndexedSlices with same shape as x and\nsame type as dtype.", "Raises": {"TypeError": "If x cannot be cast to the dtype."}}, "tf.clip_by_global_norm": {"description": "Clips values of multiple tensors by the ratio of the sum of their norms.", "Args": {"t_list": "A tuple or list of mixed Tensors, IndexedSlices, or None.", "clip_norm": "A 0-D (scalar) Tensor > 0. The clipping ratio.", "use_norm": "A 0-D (scalar) Tensor of type float (optional). The global\nnorm to use. If not provided, global_norm() is used to compute the norm.", "name": "A name for the operation (optional)."}, "Returns": "list_clipped\n\n\nA list of Tensors of the same type as list_t.", "Raises": {"TypeError": "If t_list is not a sequence."}}, "tf.clip_by_norm": {"description": "Clips tensor values to a maximum L2-norm.", "Args": {"t": "A Tensor or IndexedSlices.  This must be a floating point type.", "clip_norm": "A 0-D (scalar) Tensor > 0. A maximum clipping value, also\nfloating point", "axes": "A 1-D (vector) Tensor of type int32 containing the dimensions\nto use for computing the L2-norm. If None (the default), uses all\ndimensions.", "name": "A name for the operation (optional)."}, "Returns": "A clipped Tensor or IndexedSlices.", "Raises": {"ValueError": "If the clip_norm tensor is not a 0-D scalar tensor.", "TypeError": "If dtype of the input is not a floating point or\ncomplex type."}}, "tf.clip_by_value": {"description": "Clips tensor values to a specified min and max.", "Args": {"t": "A Tensor or IndexedSlices.", "clip_value_min": "The minimum value to clip to. A scalar Tensor or one that\nis broadcastable to the shape of t.", "clip_value_max": "The maximum value to clip to. A scalar Tensor or one that\nis broadcastable to the shape of t.", "name": "A name for the operation (optional)."}, "Returns": "A clipped Tensor or IndexedSlices.", "Raises": {"TypeError": "If dtype of the input is int32 and dtype of\nthe clip_value_min or clip_value_max is float32"}}, "tf.concat": {"description": "Concatenates tensors along one dimension.", "Args": {"values": "A list of Tensor objects or a single Tensor.", "axis": "0-D int32 Tensor.  Dimension along which to concatenate. Must be\nin the range [-rank(values), rank(values)). As in Python, indexing for\naxis is 0-based. Positive axis in the rage of [0, rank(values)) refers\nto axis-th dimension. And negative axis refers to axis +\nrank(values)-th dimension.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor resulting from concatenation of the input tensors."}, "tf.cond": {"description": "Return true_fn() if the predicate pred is true else false_fn().", "Args": {"pred": "A scalar determining whether to return the result of true_fn or\nfalse_fn.", "true_fn": "The callable to be performed if pred is true.", "false_fn": "The callable to be performed if pred is false.", "name": "Optional name prefix for the returned tensors."}, "Returns": "Tensors returned by the call to either true_fn or false_fn. If the\ncallables return a singleton list, the element is extracted from the list.", "Raises": {"TypeError": "if true_fn or false_fn is not callable.", "ValueError": "if true_fn and false_fn do not return the same number of\ntensors, or return tensors of different types."}}, "tf.constant": {"description": "Creates a constant tensor from a tensor-like object.", "Args": {"value": "A constant value (or list) of output type dtype.", "dtype": "The type of the elements of the resulting tensor.", "shape": "Optional dimensions of resulting tensor.", "name": "Optional name for the tensor."}, "Returns": "A Constant Tensor.", "Raises": {"TypeError": "if shape is incorrectly specified or unsupported.", "ValueError": "if called on a symbolic tensor."}}, "tf.constant_initializer": {"description": "Initializer that generates tensors with constant values.", "Args": {"value": "A Python scalar, list or tuple of values, or a N-dimensional numpy\narray. All elements of the initialized variable will be set to the\ncorresponding value in the value argument."}, "Raises": {"TypeError": "If the input value is not one of the expected types."}}, "tf.control_dependencies": {"description": "Wrapper for Graph.control_dependencies() using the default graph.", "Args": {"control_inputs": "A list of Operation or Tensor objects which must be\nexecuted or computed before running the operations defined in the context.\nCan also be None to clear the control dependencies. If eager execution\nis enabled, any callable object in the control_inputs list will be\ncalled."}, "Returns": "A context manager that specifies control dependencies for all\noperations constructed within the context."}, "tf.convert_to_tensor": {"description": "Converts the given value to a Tensor.", "Args": {"value": "An object whose type has a registered Tensor conversion function.", "dtype": "Optional element type for the returned tensor. If missing, the type\nis inferred from the type of value.", "dtype_hint": "Optional element type for the returned tensor, used when dtype\nis None. In some cases, a caller may not have a dtype in mind when\nconverting to a tensor, so dtype_hint can be used as a soft preference.\nIf the conversion to dtype_hint is not possible, this argument has no\neffect.", "name": "Optional name to use if a new Tensor is created."}, "Returns": "A Tensor based on value.", "Raises": {"TypeError": "If no conversion function is registered for value to dtype.", "RuntimeError": "If a registered conversion function returns an invalid value.", "ValueError": "If the value is a tensor not of given dtype in graph mode."}}, "tf.custom_gradient": {"description": "Decorator to define a function with a custom gradient.", "Args": {"f": "function f(*x) that returns a tuple (y, grad_fn) where:\n\nx is a sequence of (nested structures of) Tensor inputs to the\nfunction.\ny is a (nested structure of) Tensor outputs of applying TensorFlow\noperations in f to x.\ngrad_fn is a function with the signature g(*grad_ys) which returns\na list of Tensors the same size as (flattened) x - the derivatives\nof Tensors in y with respect to the Tensors in x.  grad_ys is\na sequence of Tensors the same size as (flattened) y holding the\ninitial value gradients for each Tensor in y.\nIn a pure mathematical sense, a vector-argument vector-valued function\nf's derivatives should be its Jacobian matrix J. Here we are\nexpressing the Jacobian J as a function grad_fn which defines how\nJ will transform a vector grad_ys when left-multiplied with it\n(grad_ys * J, the vector-Jacobian product, or VJP). This functional\nrepresentation of a matrix is convenient to use for chain-rule\ncalculation (in e.g. the back-propagation algorithm).\nIf f uses Variables (that are not part of the\ninputs), i.e. through get_variable, then grad_fn should have\nsignature g(*grad_ys, variables=None), where variables is a list of\nthe Variables, and return a 2-tuple (grad_xs, grad_vars), where\ngrad_xs is the same as above, and grad_vars is a list<Tensor>\nwith the derivatives of Tensors in y with respect to the variables\n(that is, grad_vars has one Tensor per variable in variables)."}, "Returns": "A function h(x) which returns the same value as f(x)[0] and whose\ngradient (as calculated by tf.gradients) is determined by f(x)[1]."}, "tf.device": {"description": "Specifies the device for ops created/executed in this context.", "Args": {"device_name": "The device name to use in the context."}, "Returns": "A context manager that specifies the default device to use for newly\ncreated ops.", "Raises": {"RuntimeError": "If a function is passed in."}}, "tf.dynamic_partition": {"description": "Partitions data into num_partitions tensors using indices from partitions.", "Args": {"data": "A Tensor.", "partitions": "A Tensor of type int32.\nAny shape.  Indices in the range [0, num_partitions).", "num_partitions": "An int that is >= 1.\nThe number of partitions to output.", "name": "A name for the operation (optional)."}, "Returns": "A list of num_partitions Tensor objects with the same type as data."}, "tf.dynamic_stitch": {"description": "Interleave the values from the data tensors into a single tensor.", "Args": {"indices": "A list of at least 1 Tensor objects with type int32.", "data": "A list with the same length as indices of Tensor objects with the same type.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as data."}, "tf.edit_distance": {"description": "Computes the Levenshtein distance between sequences.", "Args": {"hypothesis": "A SparseTensor containing hypothesis sequences.", "truth": "A SparseTensor containing truth sequences.", "normalize": "A bool. If True, normalizes the Levenshtein distance by\nlength of truth.", "name": "A name for the operation (optional)."}, "Returns": "A dense Tensor with rank R - 1, where R is the rank of the\nSparseTensor inputs hypothesis and truth.", "Raises": {"TypeError": "If either hypothesis or truth are not a SparseTensor."}}, "tf.einsum": {"description": "Tensor contraction over specified indices and outer product.", "Args": {"equation": "a str describing the contraction, in the same format as\nnumpy.einsum.", "*inputs": "the inputs to contract (each one a Tensor), whose shapes should\nbe consistent with equation.", "**kwargs": "optimize: Optimization strategy to use to find contraction path using\nopt_einsum. Must be 'greedy', 'optimal', 'branch-2', 'branch-all' or\n'auto'. (optional, default: 'greedy').\nname: A name for the operation (optional)."}, "Returns": "The contracted Tensor, with shape determined by equation.", "Raises": {"ValueError": "If\n\nthe format of equation is incorrect,\nnumber of inputs or their shapes are inconsistent with equation."}}, "tf.ensure_shape": {"description": "Updates the shape of a tensor and checks at runtime that the shape holds.", "Args": {"x": "A Tensor.", "shape": "A TensorShape representing the shape of this tensor, a\nTensorShapeProto, a list, a tuple, or None.", "name": "A name for this operation (optional). Defaults to \"EnsureShape\"."}, "Returns": "A Tensor. Has the same type and contents as x.", "Raises": {"tf.errors.InvalidArgumentError": "If shape is incompatible with the shape\nof x."}}, "tf.executing_eagerly": {"description": "Checks whether the current thread has eager execution enabled.", "Returns": "True if the current thread has eager execution enabled."}, "tf.expand_dims": {"description": "Returns a tensor with a length 1 axis inserted at index axis.", "Args": {"input": "A Tensor.", "axis": "Integer specifying the dimension index at which to expand the\nshape of input. Given an input of D dimensions, axis must be in range\n[-(D+1), D] (inclusive).", "name": "Optional string. The name of the output Tensor."}, "Returns": "A tensor with the same data as input, with an additional dimension\ninserted at the index specified by axis.", "Raises": {"TypeError": "If axis is not specified.", "InvalidArgumentError": "If axis is out of range [-(D+1), D]."}}, "tf.extract_volume_patches": {"description": "Extract patches from input and put them in the &#34;depth&#34; output dimension. 3D extension of extract_image_patches.", "Args": {"input": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.\n5-D Tensor with shape [batch, in_planes, in_rows, in_cols, depth].", "ksizes": "A list of ints that has length >= 5.\nThe size of the sliding window for each dimension of input.", "strides": "A list of ints that has length >= 5.\n1-D of length 5. How far the centers of two consecutive patches are in\ninput. Must be: [1, stride_planes, stride_rows, stride_cols, 1].", "padding": "A string from: \"SAME\", \"VALID\".\nThe type of padding algorithm to use.\nThe size-related attributes are specified as follows:\nksizes = [1, ksize_planes, ksize_rows, ksize_cols, 1]strides = [1, stride_planes, strides_rows, strides_cols, 1]", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.eye": {"description": "Construct an identity matrix, or a batch of matrices.", "Args": {"num_rows": "Non-negative int32 scalar Tensor giving the number of rows\nin each batch matrix.", "num_columns": "Optional non-negative int32 scalar Tensor giving the number\nof columns in each batch matrix.  Defaults to num_rows.", "batch_shape": "A list or tuple of Python integers or a 1-D int32 Tensor.\nIf provided, the returned Tensor will have leading batch dimensions of\nthis shape.", "dtype": "The type of an element in the resulting Tensor", "name": "A name for this Op.  Defaults to \"eye\"."}, "Returns": "A Tensor of shape batch_shape + [num_rows, num_columns]"}, "tf.fill": {"description": "Creates a tensor filled with a scalar value.", "Args": {"dims": "A 1-D sequence of non-negative numbers. Represents the shape of the\noutput tf.Tensor. Entries should be of type: int32, int64.", "value": "A value to fill the returned tf.Tensor.", "name": "Optional string. The name of the output tf.Tensor."}, "Returns": "A tf.Tensor with shape dims and the same dtype as value.", "Raises": {"InvalidArgumentError": "dims contains negative entries.", "NotFoundError": "dims contains non-integer entries."}}, "tf.fingerprint": {"description": "Generates fingerprint values.", "Args": {"data": "A Tensor. Must have rank 1 or higher.", "method": "A Tensor of type tf.string. Fingerprint method used by this op.\nCurrently available method is farmhash64.", "name": "A name for the operation (optional)."}, "Returns": "A two-dimensional Tensor of type tf.uint8. The first dimension equals to\ndata's first dimension, and the second dimension size depends on the\nfingerprint algorithm."}, "tf.foldl": {"description": "foldl on the list of tensors unpacked from elems on dimension 0. (deprecated argument values)", "Args": {"fn": "The callable to be performed.", "elems": "A tensor or (possibly nested) sequence of tensors, each of which will\nbe unpacked along their first dimension.  The nested sequence of the\nresulting slices will be the first argument to fn.", "initializer": "(optional) A tensor or (possibly nested) sequence of tensors,\nas the initial value for the accumulator.", "parallel_iterations": "(optional) The number of iterations allowed to run in\nparallel.", "back_prop": "(optional) Deprecated. False disables support for back\npropagation. Prefer using tf.stop_gradient instead.", "swap_memory": "(optional) True enables GPU-CPU memory swapping.", "name": "(optional) Name prefix for the returned tensors."}, "Returns": "A tensor or (possibly nested) sequence of tensors, resulting from applying\nfn consecutively to the list of tensors unpacked from elems, from first\nto last.", "Raises": {"TypeError": "if fn is not callable."}}, "tf.foldr": {"description": "foldr on the list of tensors unpacked from elems on dimension 0. (deprecated argument values)", "Args": {"fn": "The callable to be performed.", "elems": "A tensor or (possibly nested) sequence of tensors, each of which will\nbe unpacked along their first dimension.  The nested sequence of the\nresulting slices will be the first argument to fn.", "initializer": "(optional) A tensor or (possibly nested) sequence of tensors,\nas the initial value for the accumulator.", "parallel_iterations": "(optional) The number of iterations allowed to run in\nparallel.", "back_prop": "(optional) Deprecated. False disables support for back\npropagation. Prefer using tf.stop_gradient instead.", "swap_memory": "(optional) True enables GPU-CPU memory swapping.", "name": "(optional) Name prefix for the returned tensors."}, "Returns": "A tensor or (possibly nested) sequence of tensors, resulting from applying\nfn consecutively to the list of tensors unpacked from elems, from last\nto first.", "Raises": {"TypeError": "if fn is not callable."}}, "tf.function": {"description": "Compiles a function into a callable TensorFlow graph. (deprecated arguments) (deprecated arguments)", "Args": {"func": "the function to be compiled. If func is None, tf.function returns\na decorator that can be invoked with a single argument - func. In other\nwords, tf.function(input_signature=...)(func) is equivalent to\ntf.function(func, input_signature=...). The former can be used as\ndecorator.", "input_signature": "A possibly nested sequence of tf.TensorSpec objects\nspecifying the shapes and dtypes of the Tensors that will be supplied to\nthis function. If None, a separate function is instantiated for each\ninferred input signature.  If input_signature is specified, every input to\nfunc must be a Tensor, and func cannot accept **kwargs.", "autograph": "Whether autograph should be applied on func before tracing a\ngraph. Data-dependent Python control flow statements require\nautograph=True. For more information, see the\ntf.function and AutoGraph guide.", "jit_compile": "If True, compiles the function using\nXLA. XLA performs compiler optimizations,\nsuch as fusion, and attempts to emit more efficient code. This may\ndrastically improve the performance. If set to True,\nthe whole function needs to be compilable by XLA, or an\nerrors.InvalidArgumentError is thrown.\nIf None (default), compiles the function with XLA when running on TPU\nand goes through the regular function execution path when running on\nother devices.\nIf False, executes the function without XLA compilation.  Set this value\nto False when directly running a multi-device function on TPUs (e.g. two\nTPU cores, one TPU core and its host CPU).\nNot all functions are compilable, see a list of\nsharp corners.", "reduce_retracing": "When True, tf.function attempts to reduce the\namount of retracing, for example by using more generic shapes. This\ncan be controlled for user objects by customizing their associated\ntf.types.experimental.TraceType.", "experimental_implements": "If provided, contains a name of a \"known\" function\nthis implements. For example \"mycompany.my_recurrent_cell\".\nThis is stored as an attribute in inference function,\nwhich can then be detected when processing serialized function.\nSee standardizing composite ops\nfor details.  For an example of utilizing this attribute see this\nexample\nThe code above automatically detects and substitutes function that\nimplements \"embedded_matmul\" and allows TFLite to substitute its own\nimplementations. For instance, a tensorflow user can use this\n attribute to mark that their function also implements\nembedded_matmul (perhaps more efficiently!)\nby specifying it using this parameter:\n@tf.function(experimental_implements=\"embedded_matmul\")\nThis can either be specified as just the string name of the function or\na NameAttrList corresponding to a list of key-value attributes associated\nwith the function name. The name of the function will be in the 'name'\nfield of the NameAttrList. To define a formal TF op for this function\nimplements, try the experimental composite TF\nproject.", "experimental_autograph_options": "Optional tuple of\ntf.autograph.experimental.Feature values.", "experimental_relax_shapes": "Deprecated. Use reduce_retracing\ninstead.", "experimental_compile": "Deprecated alias to 'jit_compile'.", "experimental_follow_type_hints": "When True, the function may use type\nannotations from func to optimize the tracing performance. For example,\narguments annotated with tf.Tensor will automatically be converted\nto a Tensor."}, "Returns": "If func is not None, returns a tf.types.experimental.GenericFunction.\nIf func is None, returns a decorator that, when invoked with a single\nfunc argument, returns a tf.types.experimental.GenericFunction.", "Raises": {}}, "tf.gather": {"description": "Gather slices from params axis axis according to indices. (deprecated arguments)", "Args": {"params": "The Tensor from which to gather values. Must be at least rank\naxis + 1.", "indices": "The index Tensor.  Must be one of the following types: int32,\nint64. The values must be in range [0, params.shape[axis]).", "validate_indices": "Deprecated, does nothing. Indices are always validated on\nCPU, never validated on GPU.\nCaution: On CPU, if an out of bound index is found, an error is raised.\nOn GPU, if an out of bound index is found, a 0 is stored in the\ncorresponding output value.", "axis": "A Tensor. Must be one of the following types: int32, int64. The\naxis in params to gather indices from. Must be greater than or equal\nto batch_dims.  Defaults to the first non-batch dimension. Supports\nnegative indexes.", "batch_dims": "An integer.  The number of batch dimensions.  Must be less\nthan or equal to rank(indices).", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as params."}, "tf.gather_nd": {"description": "Gather slices from params into a Tensor with shape specified by indices.", "Args": {"params": "A Tensor. The tensor from which to gather values.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nIndex tensor.", "name": "A name for the operation (optional).", "batch_dims": "An integer or a scalar 'Tensor'. The number of batch dimensions."}, "Returns": "A Tensor. Has the same type as params."}, "tf.get_current_name_scope": {"description": "Returns current full name scope specified by tf.name_scope(...)s."}, "tf.get_logger": {"description": "Return TF logger instance."}, "tf.get_static_value": {"description": "Returns the constant value of the given tensor, if efficiently calculable.", "Args": {"tensor": "The Tensor to be evaluated.", "partial": "If True, the returned numpy array is allowed to have partially\nevaluated values. Values that can't be evaluated will be None."}, "Returns": "A numpy ndarray containing the constant value of the given tensor,\nor None if it cannot be calculated.", "Raises": {"TypeError": "if tensor is not an ops.Tensor."}}, "tf.grad_pass_through": {"description": "Creates a grad-pass-through op with the forward behavior provided in f.", "Args": {"f": "function f(*x) that returns a Tensor or nested structure of Tensor\noutputs."}, "Returns": "A function h(x) which returns the same values as f(x) and whose\ngradients are the same as those of an identity function."}, "tf.gradients": {"description": "Constructs symbolic derivatives of sum of ys w.r.t. x in xs.", "Args": {"ys": "A Tensor or list of tensors to be differentiated.", "xs": "A Tensor or list of tensors to be used for differentiation.", "grad_ys": "Optional. A Tensor or list of tensors the same size as\nys and holding the gradients computed for each y in ys.", "name": "Optional name to use for grouping all the gradient ops together.\ndefaults to 'gradients'.", "gate_gradients": "If True, add a tuple around the gradients returned\nfor an operations.  This avoids some race conditions.", "aggregation_method": "Specifies the method used to combine gradient terms.\nAccepted values are constants defined in the class AggregationMethod.", "stop_gradients": "Optional. A Tensor or list of tensors not to differentiate\nthrough.", "unconnected_gradients": "Optional. Specifies the gradient value returned when\nthe given input tensors are unconnected. Accepted values are constants\ndefined in the class tf.UnconnectedGradients and the default value is\nnone."}, "Returns": "A list of Tensor of length len(xs) where each tensor is the sum(dy/dx)\nfor y in ys and for x in xs.", "Raises": {"LookupError": "if one of the operations between x and y does not\nhave a registered gradient function.", "ValueError": "if the arguments are invalid.", "RuntimeError": "if called in Eager mode."}}, "tf.group": {"description": "Create an op that groups multiple operations.", "Args": {"*inputs": "Zero or more tensors to group.", "name": "A name for this operation (optional)."}, "Returns": "An Operation that executes all its inputs.", "Raises": {"ValueError": "If an unknown keyword argument is provided."}}, "tf.guarantee_const": {"description": "Promise to the TF runtime that the input tensor is a constant. (deprecated)", "Args": {"input": "A Tensor.", "name": "A name for this operation."}, "Returns": "A Tensor. Has the same dtype as input."}, "tf.hessians": {"description": "Constructs the Hessian of sum of ys with respect to x in xs.", "Args": {"ys": "A Tensor or list of tensors to be differentiated.", "xs": "A Tensor or list of tensors to be used for differentiation.", "gate_gradients": "See gradients() documentation for details.", "aggregation_method": "See gradients() documentation for details.", "name": "Optional name to use for grouping all the gradient ops together.\ndefaults to 'hessians'."}, "Returns": "A list of Hessian matrices of sum(ys) for each x in xs.", "Raises": {"LookupError": "if one of the operations between xs and ys does not\nhave a registered gradient function."}}, "tf.histogram_fixed_width": {"description": "Return histogram of values.", "Args": {"values": "Numeric Tensor.", "value_range": "Shape [2] Tensor of same dtype as values.\nvalues <= value_range[0] will be mapped to hist[0],\nvalues >= value_range[1] will be mapped to hist[-1].", "nbins": "Scalar int32 Tensor.  Number of histogram bins.", "dtype": "dtype for returned histogram.", "name": "A name for this operation (defaults to 'histogram_fixed_width')."}, "Returns": "A 1-D Tensor holding histogram of values.", "Raises": {"TypeError": "If any unsupported dtype is provided.", "tf.errors.InvalidArgumentError": "If value_range does not\nsatisfy value_range[0] < value_range[1]."}}, "tf.histogram_fixed_width_bins": {"description": "Bins the given values for use in a histogram.", "Args": {"values": "Numeric Tensor.", "value_range": "Shape [2] Tensor of same dtype as values.\nvalues <= value_range[0] will be mapped to hist[0],\nvalues >= value_range[1] will be mapped to hist[-1].", "nbins": "Scalar int32 Tensor.  Number of histogram bins.", "dtype": "dtype for returned histogram.", "name": "A name for this operation (defaults to 'histogram_fixed_width')."}, "Returns": "A Tensor holding the indices of the binned values whose shape matches\nvalues.", "Raises": {"TypeError": "If any unsupported dtype is provided.", "tf.errors.InvalidArgumentError": "If value_range does not\nsatisfy value_range[0] < value_range[1]."}}, "tf.identity": {"description": "Return a Tensor with the same shape and contents as input.", "Args": {"input": "A Tensor, a Variable, a CompositeTensor or anything that can be\nconverted to a tensor using tf.convert_to_tensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor or CompositeTensor. Has the same type and contents as input."}, "tf.identity_n": {"description": "Returns a list of tensors with the same shapes and contents as the input", "Args": {"input": "A list of Tensor objects.", "name": "A name for the operation (optional)."}, "Returns": "A list of Tensor objects. Has the same type as input."}, "tf.init_scope": {"description": "A context manager that lifts ops out of control-flow scopes and function-building graphs.", "Raises": {"RuntimeError": "if graph state is incompatible with this initialization."}}, "tf.inside_function": {"description": "Indicates whether the caller code is executing inside a tf.function.", "Returns": "Boolean, True if the caller code is executing inside a tf.function\nrather than eagerly."}, "tf.is_tensor": {"description": "Checks whether x is a TF-native type that can be passed to many TF ops.", "Args": {"x": "A python object to check."}, "Returns": "True if x is a TensorFlow-native type."}, "tf.linspace": {"description": "Generates evenly-spaced values in an interval along a given axis.", "Args": {"start": "A Tensor. Must be one of the following types: bfloat16,\nfloat32, float64. N-D tensor. First entry in the range.", "stop": "A Tensor. Must have the same type and shape as start. N-D tensor.\nLast entry in the range.", "num": "A Tensor. Must be one of the following types: int32, int64. 0-D\ntensor. Number of values to generate.", "name": "A name for the operation (optional).", "axis": "Axis along which the operation is performed (used only when N-D\ntensors are provided)."}, "Returns": "A Tensor. Has the same type as start."}, "tf.load_library": {"description": "Loads a TensorFlow plugin.", "Args": {"library_location": "Path to the plugin or the folder of plugins.\nRelative or absolute filesystem path to a dynamic library file or folder."}, "Returns": "None", "Raises": {"OSError": "When the file to be loaded is not found.", "RuntimeError": "when unable to load the library."}}, "tf.load_op_library": {"description": "Loads a TensorFlow plugin, containing custom ops and kernels.", "Args": {"library_filename": "Path to the plugin.\nRelative or absolute filesystem path to a dynamic library file."}, "Returns": "A python module containing the Python wrappers for Ops defined in\nthe plugin.", "Raises": {"RuntimeError": "when unable to load the library or get the python wrappers."}}, "tf.make_ndarray": {"description": "Create a numpy ndarray from a tensor.", "Args": {"tensor": "A TensorProto."}, "Returns": "A numpy array with the tensor contents.", "Raises": {"TypeError": "if tensor has unsupported type."}}, "tf.make_tensor_proto": {"description": "Create a TensorProto.", "Args": {"values": "Values to put in the TensorProto.", "dtype": "Optional tensor_pb2 DataType value.", "shape": "List of integers representing the dimensions of tensor.", "verify_shape": "Boolean that enables verification of a shape of values.", "allow_broadcast": "Boolean that enables allowing scalars and 1 length vector\nbroadcasting. Cannot be true when verify_shape is true."}, "Returns": "A TensorProto. Depending on the type, it may contain data in the\n\"tensor_content\" attribute, which is not directly useful to Python programs.\nTo access the values you should convert the proto back to a numpy ndarray\nwith tf.make_ndarray(proto).\nIf values is a TensorProto, it is immediately returned; dtype and\nshape are ignored.", "Raises": {"TypeError": "if unsupported types are provided.", "ValueError": "if arguments have inappropriate values or if verify_shape is\nTrue and shape of values is not equals to a shape from the argument."}}, "tf.map_fn": {"description": "Transforms elems by applying fn to each element unstacked on axis 0. (deprecated arguments)", "Args": {"fn": "The callable to be performed.  It accepts one argument, which will have\nthe same (possibly nested) structure as elems.  Its output must have the\nsame structure as fn_output_signature if one is provided; otherwise it\nmust have the same structure as elems.", "elems": "A tensor or (possibly nested) sequence of tensors, each of which will\nbe unstacked along their first dimension.  fn will be applied to the\nnested sequence of the resulting slices.  elems may include ragged and\nsparse tensors. elems must consist of at least one tensor.", "dtype": "Deprecated: Equivalent to fn_output_signature.", "parallel_iterations": "(optional) The number of iterations allowed to run in\nparallel. When graph building, the default value is 10. While executing\neagerly, the default value is set to 1.", "back_prop": "(optional) Deprecated: prefer using tf.stop_gradient instead.  False disables support for back propagation.", "swap_memory": "(optional) True enables GPU-CPU memory swapping.", "infer_shape": "(optional) False disables tests for consistent output shapes.", "name": "(optional) Name prefix for the returned tensors.", "fn_output_signature": "The output signature of fn. Must be specified if\nfn's input and output signatures are different (i.e., if their\nstructures, dtypes, or tensor types do not match).\nfn_output_signature can be specified using any of the following:\n\nA tf.DType or tf.TensorSpec (to describe a tf.Tensor)\nA tf.RaggedTensorSpec (to describe a tf.RaggedTensor)\nA tf.SparseTensorSpec (to describe a tf.sparse.SparseTensor)\nA (possibly nested) tuple, list, or dict containing the above types."}, "Returns": "A tensor or (possibly nested) sequence of tensors.  Each tensor stacks the\nresults of applying fn to tensors unstacked from elems along the first\ndimension, from first to last.  The result may include ragged and sparse\ntensors.", "Raises": {"TypeError": "if fn is not callable or the structure of the output of\nfn and fn_output_signature do not match.", "ValueError": "if the lengths of the output of fn and fn_output_signature\ndo not match, or if the elems does not contain any tensor."}}, "tf.meshgrid": {"description": "Broadcasts parameters for evaluation on an N-D grid.", "Args": {"*args": "Tensors with rank 1.", "**kwargs": "indexing: Either 'xy' or 'ij' (optional, default: 'xy').\nname: A name for the operation (optional)."}, "Returns": "outputs\n\n\nA list of N Tensors with rank N.", "Raises": {"TypeError": "When no keyword arguments (kwargs) are passed.", "ValueError": "When indexing keyword argument is not one of xy or ij."}}, "tf.name_scope": {"description": "A context manager for use when defining a Python op.", "Args": {"name": "The prefix to use on all names created within the name scope."}, "Raises": {"ValueError": "If name is not a string."}, "Attributes": {"name": ""}}, "tf.no_gradient": {"description": "Specifies that ops of type op_type is not differentiable.", "Args": {"op_type": "The string type of an operation. This corresponds to the\nOpDef.name field for the proto that defines the operation."}, "Raises": {"TypeError": "If op_type is not a string."}}, "tf.no_op": {"description": "Does nothing. Only useful as a placeholder for control edges.", "Args": {"name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.nondifferentiable_batch_function": {"description": "Batches the computation done by the decorated function.", "Args": {"num_batch_threads": "Number of scheduling threads for processing batches\nof work. Determines the number of batches processed in parallel.", "max_batch_size": "Batch sizes will never be bigger than this.", "batch_timeout_micros": "Maximum number of microseconds to wait before\noutputting an incomplete batch.", "allowed_batch_sizes": "Optional list of allowed batch sizes. If left empty,\ndoes nothing. Otherwise, supplies a list of batch sizes, causing the op\nto pad batches up to one of those sizes. The entries must increase\nmonotonically, and the final entry must equal max_batch_size.", "max_enqueued_batches": "The maximum depth of the batch queue. Defaults to 10.", "autograph": "Whether to use autograph to compile python and eager style code\nfor efficient graph-mode execution.", "enable_large_batch_splitting": "The value of this option doesn't affect\nprocessing output given the same input; it affects implementation details\nas stated below: 1. Improve batching efficiency by eliminating unnecessary\nadding. 2.max_batch_size specifies the limit of input and\nallowed_batch_sizes specifies the limit of a task to be processed. API\nuser can give an input of size 128 when 'max_execution_batch_size'\nis 32 -> implementation can split input of 128 into 4 x 32, schedule\nconcurrent processing, and then return concatenated results corresponding\nto 128."}, "Returns": "The decorated function will return the unbatched computation output Tensors."}, "tf.norm": {"description": "Computes the norm of vectors, matrices, and tensors.", "Args": {"tensor": "Tensor of types float32, float64, complex64, complex128", "ord": "Order of the norm. Supported values are 'fro', 'euclidean',\n1, 2, np.inf and any positive real number yielding the corresponding\np-norm. Default is 'euclidean' which is equivalent to Frobenius norm if\ntensor is a matrix and equivalent to 2-norm for vectors.\nSome restrictions apply:\n  a) The Frobenius norm 'fro' is not defined for vectors,\n  b) If axis is a 2-tuple (matrix norm), only 'euclidean', 'fro', 1,\n     2, np.inf are supported.\nSee the description of axis on how to compute norms for a batch of\nvectors or matrices stored in a tensor.", "axis": "If axis is None (the default), the input is considered a vector\nand a single vector norm is computed over the entire set of values in the\ntensor, i.e. norm(tensor, ord=ord) is equivalent to\nnorm(reshape(tensor, [-1]), ord=ord).\nIf axis is a Python integer, the input is considered a batch of vectors,\nand axis determines the axis in tensor over which to compute vector\nnorms.\nIf axis is a 2-tuple of Python integers it is considered a batch of\nmatrices and axis determines the axes in tensor over which to compute\na matrix norm.\nNegative indices are supported. Example: If you are passing a tensor that\ncan be either a matrix or a batch of matrices at runtime, pass\naxis=[-2,-1] instead of axis=None to make sure that matrix norms are\ncomputed.", "keepdims": "If True, the axis indicated in axis are kept with size 1.\nOtherwise, the dimensions in axis are removed from the output shape.", "name": "The name of the op."}, "Returns": "output\n\n\nA Tensor of the same type as tensor, containing the vector or\nmatrix norms. If keepdims is True then the rank of output is equal to\nthe rank of tensor. Otherwise, if axis is none the output is a scalar,\nif axis is an integer, the rank of output is one less than the rank\nof tensor, if axis is a 2-tuple the rank of output is two less\nthan the rank of tensor.", "Raises": {"ValueError": "If ord or axis is invalid."}}, "tf.numpy_function": {"description": "Wraps a python function and uses it as a TensorFlow op.", "Args": {"func": "A Python function, which accepts numpy.ndarray objects as arguments\nand returns a list of numpy.ndarray objects (or a single\nnumpy.ndarray). This function must accept as many arguments as there are\ntensors in inp, and these argument types will match the corresponding\ntf.Tensor objects in inp. The returns numpy.ndarrays must match the\nnumber and types defined Tout.\nImportant Note: Input and output numpy.ndarrays of func are not\n  guaranteed to be copies. In some cases their underlying memory will be\n  shared with the corresponding TensorFlow tensors. In-place modification\n  or storing func input or return values in python datastructures\n  without explicit (np.)copy can have non-deterministic consequences.", "inp": "A list of tf.Tensor objects.", "Tout": "A list or tuple of tensorflow data types or a single tensorflow data\ntype if there is only one, indicating what func returns.", "stateful": "(Boolean.) Setting this argument to False tells the runtime to\ntreat the function as stateless, which enables certain optimizations.\nA function is stateless when given the same input it will return the\nsame output and have no side effects; its only purpose is to have a\nreturn value.\nThe behavior for a stateful function with the stateful argument False\nis undefined. In particular, caution should be taken when\nmutating the input arguments as this is a stateful operation.", "name": "(Optional) A name for the operation."}, "Returns": "Single or list of tf.Tensor which func computes."}, "tf.one_hot": {"description": "Returns a one-hot tensor.", "Args": {"indices": "A Tensor of indices.", "depth": "A scalar defining the depth of the one hot dimension.", "on_value": "A scalar defining the value to fill in output when indices[j]\n= i. (default: 1)", "off_value": "A scalar defining the value to fill in output when indices[j]\n!= i. (default: 0)", "axis": "The axis to fill (default: -1, a new inner-most axis).", "dtype": "The data type of the output tensor.", "name": "A name for the operation (optional)."}, "Returns": "output\n\n\nThe one-hot tensor.", "Raises": {"TypeError": "If dtype of on_value and off_value don't match one another"}}, "tf.ones": {"description": "Creates a tensor with all elements set to one (1).", "Args": {"shape": "A list of integers, a tuple of integers, or\na 1-D Tensor of type int32.", "dtype": "Optional DType of an element in the resulting Tensor. Default is\ntf.float32.", "name": "Optional string. A name for the operation."}, "Returns": "A Tensor with all elements set to one (1)."}, "tf.ones_initializer": {"description": "Initializer that generates tensors initialized to 1."}, "tf.ones_like": {"description": "Creates a tensor of all ones that has the same shape as the input.", "Args": {"input": "A Tensor.", "dtype": "A type for the returned Tensor. Must be float16, float32,\nfloat64, int8, uint8, int16, uint16, int32, int64,\ncomplex64, complex128, bool or string.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor with all elements set to one."}, "tf.pad": {"description": "Pads a tensor.", "Args": {"tensor": "A Tensor.", "paddings": "A Tensor of type int32.", "mode": "One of \"CONSTANT\", \"REFLECT\", or \"SYMMETRIC\" (case-insensitive)", "constant_values": "In \"CONSTANT\" mode, the scalar pad value to use. Must be\nsame type as tensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as tensor.", "Raises": {"ValueError": "When mode is not one of \"CONSTANT\", \"REFLECT\", or \"SYMMETRIC\"."}}, "tf.parallel_stack": {"description": "Stacks a list of rank-R tensors into one rank-(R&#43;1) tensor in parallel.", "Args": {"values": "A list of Tensor objects with the same shape and type.", "name": "A name for this operation (optional)."}, "Returns": "output\n\n\nA stacked Tensor with the same type as values.", "Raises": {"RuntimeError": "if executed in eager mode."}}, "tf.print": {"description": "Print the specified inputs.", "Args": {"*inputs": "Positional arguments that are the inputs to print. Inputs in the\nprinted output will be separated by spaces. Inputs may be python\nprimitives, tensors, data structures such as dicts and lists that may\ncontain tensors (with the data structures possibly nested in arbitrary\nways), and printable python objects.", "output_stream": "The output stream, logging level, or file to print to.\nDefaults to sys.stderr, but sys.stdout, tf.compat.v1.logging.info,\ntf.compat.v1.logging.warning, tf.compat.v1.logging.error,\nabsl.logging.info, absl.logging.warning and absl.logging.error are also\nsupported. To print to a file, pass a string started with \"file://\"\nfollowed by the file path, e.g., \"file:///tmp/foo.out\".", "summarize": "The first and last summarize elements within each dimension are\nrecursively printed per Tensor. If None, then the first 3 and last 3\nelements of each dimension are printed for each tensor. If set to -1, it\nwill print all elements of every tensor.", "sep": "The string to use to separate the inputs. Defaults to \" \".", "end": "End character that is appended at the end the printed string. Defaults\nto the newline character.", "name": "A name for the operation (optional)."}, "Returns": "None when executing eagerly. During graph tracing this returns\na TF operator that prints the specified inputs in the specified output\nstream or logging level. This operator will be automatically executed\nexcept inside of tf.compat.v1 graphs and sessions.", "Raises": {"ValueError": "If an unsupported output stream is specified."}}, "tf.py_function": {"description": "Wraps a python function into a TensorFlow op that executes it eagerly.", "Args": {"func": "A Python function that accepts inp as arguments, and returns a\nvalue (or list of values) whose type is described by Tout.", "inp": "Input arguments for func.  A list whose elements are Tensors or\nCompositeTensors (such as tf.RaggedTensor); or a single Tensor or\nCompositeTensor.", "Tout": "The type(s) of the value(s) returned by func.  One of the\nfollowing.\n\nIf func returns a Tensor (or a value that can be converted to a\nTensor): the tf.DType for that value.\nIf func returns a CompositeTensor: The tf.TypeSpec for that value.\nIf func returns None: the empty list ([]).\nIf func returns a list of Tensor and CompositeTensor values:\na corresponding list of tf.DTypes and tf.TypeSpecs for each value.", "name": "A name for the operation (optional)."}, "Returns": "The value(s) computed by func: a Tensor, CompositeTensor, or list of\nTensor and CompositeTensor; or an empty list if func returns None."}, "tf.random_index_shuffle": {"description": "Outputs the position of value in a permutation of [0, ..., max_index].", "Args": {"index": "A Tensor. Must be one of the following types: int32, uint32, int64, uint64.\nA scalar tensor or a vector of dtype dtype. The index (or indices) to be shuffled. Must be within [0, max_index].", "seed": "A Tensor. Must be one of the following types: int32, uint32, int64, uint64.\nA tensor of dtype Tseed and shape [3] or [n, 3]. The random seed.", "max_index": "A Tensor. Must have the same type as index.\nA scalar tensor or vector of dtype dtype. The upper bound(s) of the interval (inclusive).", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as index."}, "tf.random_normal_initializer": {"description": "Initializer that generates tensors with a normal distribution.", "Args": {"mean": "a python scalar or a scalar tensor. Mean of the random values to\ngenerate.", "stddev": "a python scalar or a scalar tensor. Standard deviation of the random\nvalues to generate.", "seed": "A Python integer. Used to create random seeds. See\ntf.random.set_seed for behavior."}}, "tf.random_uniform_initializer": {"description": "Initializer that generates tensors with a uniform distribution.", "Args": {"minval": "A python scalar or a scalar tensor. Lower bound of the range of\nrandom values to generate (inclusive).", "maxval": "A python scalar or a scalar tensor. Upper bound of the range of\nrandom values to generate (exclusive).", "seed": "A Python integer. Used to create random seeds. See\ntf.random.set_seed for behavior."}}, "tf.range": {"description": "Creates a sequence of numbers.", "Args": {"start": "A 0-D Tensor (scalar). Acts as first entry in the range if limit\nis not None; otherwise, acts as range limit and first entry defaults to 0.", "limit": "A 0-D Tensor (scalar). Upper limit of sequence, exclusive. If None,\ndefaults to the value of start while the first entry of the range\ndefaults to 0.", "delta": "A 0-D Tensor (scalar). Number that increments start. Defaults to\n1.", "dtype": "The type of the elements of the resulting tensor.", "name": "A name for the operation. Defaults to \"range\"."}, "Returns": "An 1-D Tensor of type dtype."}, "tf.rank": {"description": "Returns the rank of a tensor.", "Args": {"input": "A Tensor or SparseTensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int32."}, "tf.realdiv": {"description": "Returns x / y element-wise for real types.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, uint8, int8, uint16, int16, int32, uint32, uint64, int64, complex64, complex128.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.recompute_grad": {"description": "Defines a function as a recompute-checkpoint for the tape auto-diff.", "Args": {"f": "function f(*x) that returns a Tensor or sequence of Tensor outputs."}, "Returns": "A function g wrapping f that defines a custom gradient, which recomputes\nf on the backwards pass of a gradient call."}, "tf.register_tensor_conversion_function": {"description": "Registers a function for converting objects of base_type to Tensor.", "Args": {"base_type": "The base type or tuple of base types for all objects that\nconversion_func accepts.", "conversion_func": "A function that converts instances of base_type to\nTensor.", "priority": "Optional integer that indicates the priority for applying this\nconversion function. Conversion functions with smaller priority values run\nearlier than conversion functions with larger priority values. Defaults to\n100."}, "Raises": {"TypeError": "If the arguments do not have the appropriate type."}}, "tf.repeat": {"description": "Repeat elements of input.", "Args": {"input": "An N-dimensional Tensor.", "repeats": "An 1-D int Tensor. The number of repetitions for each element.\nrepeats is broadcasted to fit the shape of the given axis. len(repeats)\nmust equal input.shape[axis] if axis is not None.", "axis": "An int. The axis along which to repeat values. By default (axis=None),\nuse the flattened input array, and return a flat output array.", "name": "A name for the operation."}, "Returns": "A Tensor which has the same shape as input, except along the given axis.\nIf axis is None then the output array is flattened to match the flattened\ninput array."}, "tf.required_space_to_batch_paddings": {"description": "Calculate padding required to make block_shape divide input_shape.", "Args": {"input_shape": "int32 Tensor of shape [N].", "block_shape": "int32 Tensor of shape [N].", "base_paddings": "Optional int32 Tensor of shape [N, 2].  Specifies the minimum\namount of padding to use.  All elements must be >= 0.  If not specified,\ndefaults to 0.", "name": "string.  Optional name prefix."}, "Returns": "(paddings, crops), where:\npaddings and crops are int32 Tensors of rank 2 and shape [N, 2]"}, "tf.reshape": {"description": "Reshapes a tensor.", "Args": {"tensor": "A Tensor.", "shape": "A Tensor. Must be one of the following types: int32, int64.\nDefines the shape of the output tensor.", "name": "Optional string. A name for the operation."}, "Returns": "A Tensor. Has the same type as tensor."}, "tf.reverse": {"description": "Reverses specific dimensions of a tensor.", "Args": {"tensor": "A Tensor. Must be one of the following types: uint8, int8, uint16, int16, int32, uint32, int64, uint64, bool, bfloat16, half, float32, float64, complex64, complex128, string.\nUp to 8-D.", "axis": "A Tensor. Must be one of the following types: int32, int64.\n1-D. The indices of the dimensions to reverse. Must be in the range\n[-rank(tensor), rank(tensor)).", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as tensor."}, "tf.reverse_sequence": {"description": "Reverses variable length slices.", "Args": {"input": "A Tensor. The input to reverse.", "seq_lengths": "A Tensor. Must be one of the following types: int32,\nint64. 1-D with length input.dims(batch_axis) and max(seq_lengths) <=\ninput.dims(seq_axis)", "seq_axis": "An int. The dimension which is partially reversed.", "batch_axis": "An optional int. Defaults to 0. The dimension along which\nreversal is performed.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.roll": {"description": "Rolls the elements of a tensor along an axis.", "Args": {"input": "A Tensor.", "shift": "A Tensor. Must be one of the following types: int32, int64.\nDimension must be 0-D or 1-D. shift[i] specifies the number of places by which\nelements are shifted positively (towards larger indices) along the dimension\nspecified by axis[i]. Negative shifts will roll the elements in the opposite\ndirection.", "axis": "A Tensor. Must be one of the following types: int32, int64.\nDimension must be 0-D or 1-D. axis[i] specifies the dimension that the shift\nshift[i] should occur. If the same axis is referenced more than once, the\ntotal shift for that axis will be the sum of all the shifts that belong to that\naxis.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.scan": {"description": "scan on the list of tensors unpacked from elems on dimension 0. (deprecated argument values)", "Args": {"fn": "The callable to be performed.  It accepts two arguments.  The first will\nhave the same structure as initializer if one is provided, otherwise it\nwill have the same structure as elems.  The second will have the same\n(possibly nested) structure as elems.  Its output must have the same\nstructure as initializer if one is provided, otherwise it must have the\nsame structure as elems.", "elems": "A tensor or (possibly nested) sequence of tensors, each of which will\nbe unpacked along their first dimension.  The nested sequence of the\nresulting slices will be the first argument to fn.", "initializer": "(optional) A tensor or (possibly nested) sequence of tensors,\ninitial value for the accumulator, and the expected output type of fn.", "parallel_iterations": "(optional) The number of iterations allowed to run in\nparallel.", "back_prop": "(optional) Deprecated. False disables support for back\npropagation. Prefer using tf.stop_gradient instead.", "swap_memory": "(optional) True enables GPU-CPU memory swapping.", "infer_shape": "(optional) False disables tests for consistent output shapes.", "reverse": "(optional) True scans the tensor last to first (instead of first to\nlast).", "name": "(optional) Name prefix for the returned tensors."}, "Returns": "A tensor or (possibly nested) sequence of tensors.  Each tensor packs the\nresults of applying fn to tensors unpacked from elems along the first\ndimension, and the previous accumulator value(s), from first to last (or\nlast to first, if reverse=True).", "Raises": {"TypeError": "if fn is not callable or the structure of the output of\nfn and initializer do not match.", "ValueError": "if the lengths of the output of fn and initializer\ndo not match."}}, "tf.scatter_nd": {"description": "Scatters updates into a tensor of shape shape according to indices.", "Args": {"indices": "A Tensor. Must be one of the following types: int16, int32, int64.\nTensor of indices.", "updates": "A Tensor. Values to scatter into the output tensor.", "shape": "A Tensor. Must have the same type as indices.\n1-D. The shape of the output tensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as updates."}, "tf.searchsorted": {"description": "Searches for where a value would go in a sorted sequence.", "Args": {"sorted_sequence": "N-D Tensor containing a sorted sequence.", "values": "N-D Tensor containing the search values.", "side": "'left' or 'right'; 'left' corresponds to lower_bound and 'right' to\nupper_bound.", "out_type": "The output type (int32 or int64).  Default is tf.int32.", "name": "Optional name for the operation."}, "Returns": "An N-D Tensor the size of values containing the result of applying\neither lower_bound or upper_bound (depending on side) to each value.  The\nresult is not a global index to the entire Tensor, but the index in the\nlast dimension.", "Raises": {"ValueError": "If the last dimension of sorted_sequence >= 2^31-1 elements.\nIf the total size of values exceeds 2^31 - 1 elements.\nIf the first N-1 dimensions of the two tensors don't match."}}, "tf.sequence_mask": {"description": "Returns a mask tensor representing the first N positions of each cell.", "Args": {"lengths": "integer tensor, all its values <= maxlen.", "maxlen": "scalar integer tensor, size of last dimension of returned tensor.\nDefault is the maximum value in lengths.", "dtype": "output type of the resulting tensor.", "name": "name of the op."}, "Returns": "A mask tensor of shape lengths.shape + (maxlen,), cast to specified dtype.", "Raises": {"ValueError": "if maxlen is not a scalar."}}, "tf.shape": {"description": "Returns a tensor containing the shape of the input tensor.", "Args": {"input": "A Tensor or SparseTensor.", "out_type": "(Optional) The specified output type of the operation (int32 or\nint64). Defaults to tf.int32.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type out_type."}, "tf.shape_n": {"description": "Returns shape of tensors.", "Args": {"input": "A list of at least 1 Tensor object with the same type.", "out_type": "The specified output type of the operation (int32 or int64).\nDefaults to tf.int32(optional).", "name": "A name for the operation (optional)."}, "Returns": "A list with the same length as input of Tensor objects with\ntype out_type."}, "tf.size": {"description": "Returns the size of a tensor.", "Args": {"input": "A Tensor or SparseTensor.", "name": "A name for the operation (optional).", "out_type": "(Optional) The specified non-quantized numeric output type of the\noperation. Defaults to tf.int32."}, "Returns": "A Tensor of type out_type. Defaults to tf.int32."}, "tf.slice": {"description": "Extracts a slice from a tensor.", "Args": {"input_": "A Tensor.", "begin": "An int32 or int64 Tensor.", "size": "An int32 or int64 Tensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor the same type as input_."}, "tf.sort": {"description": "Sorts a tensor.", "Args": {"values": "1-D or higher numeric Tensor.", "axis": "The axis along which to sort. The default is -1, which sorts the last\naxis.", "direction": "The direction in which to sort the values ('ASCENDING' or\n'DESCENDING').", "name": "Optional name for the operation."}, "Returns": "A Tensor with the same dtype and shape as values, with the elements\nsorted along the given axis.", "Raises": {"tf.errors.InvalidArgumentError": "If the values.dtype is not a float or\nint type.", "ValueError": "If axis is not a constant scalar, or the direction is invalid."}}, "tf.space_to_batch": {"description": "SpaceToBatch for N-D tensors of type T.", "Args": {"input": "A Tensor.\nN-D with shape input_shape = [batch] + spatial_shape + remaining_shape,\nwhere spatial_shape has M dimensions.", "block_shape": "A Tensor. Must be one of the following types: int32, int64.\n1-D with shape [M], all values must be >= 1.", "paddings": "A Tensor. Must be one of the following types: int32, int64.\n2-D with shape [M, 2], all values must be >= 0.\n  paddings[i] = [pad_start, pad_end] specifies the padding for input dimension\n  i + 1, which corresponds to spatial dimension i.  It is required that\n  block_shape[i] divides input_shape[i + 1] + pad_start + pad_end.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.space_to_batch_nd": {"description": "SpaceToBatch for N-D tensors of type T.", "Args": {"input": "A Tensor.\nN-D with shape input_shape = [batch] + spatial_shape + remaining_shape,\nwhere spatial_shape has M dimensions.", "block_shape": "A Tensor. Must be one of the following types: int32, int64.\n1-D with shape [M], all values must be >= 1.", "paddings": "A Tensor. Must be one of the following types: int32, int64.\n2-D with shape [M, 2], all values must be >= 0.\n  paddings[i] = [pad_start, pad_end] specifies the padding for input dimension\n  i + 1, which corresponds to spatial dimension i.  It is required that\n  block_shape[i] divides input_shape[i + 1] + pad_start + pad_end.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.split": {"description": "Splits a tensor value into a list of sub tensors.", "Args": {"value": "The Tensor to split.", "num_or_size_splits": "Either an int indicating the number of splits\nalong axis or a 1-D integer Tensor or Python list containing the sizes\nof each output tensor along axis. If an int, then it must evenly\ndivide value.shape[axis]; otherwise the sum of sizes along the split\naxis must match that of the value.", "axis": "An int or scalar int32 Tensor. The dimension along which\nto split. Must be in the range [-rank(value), rank(value)). Defaults to\n0.", "num": "Optional, an int, used to specify the number of outputs when it\ncannot be inferred from the shape of size_splits.", "name": "A name for the operation (optional)."}, "Returns": "if num_or_size_splits is an int returns a list of\nnum_or_size_splits Tensor objects; if num_or_size_splits is a 1-D\nlist or 1-D Tensor returns num_or_size_splits.get_shape[0]\nTensor objects resulting from splitting value.", "Raises": {"ValueError": "If num_or_size_splits is a scalar Tensor."}}, "tf.squeeze": {"description": "Removes dimensions of size 1 from the shape of a tensor.", "Args": {"input": "A Tensor. The input to squeeze.", "axis": "An optional list of ints. Defaults to []. If specified, only\nsqueezes the dimensions listed. The dimension index starts at 0. It is an\nerror to squeeze a dimension that is not 1. Must be in the range\n[-rank(input), rank(input)). Must be specified if input is a\nRaggedTensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input.\nContains the same data as input, but has one or more dimensions of\nsize 1 removed.", "Raises": {"ValueError": "The input cannot be converted to a tensor, or the specified\naxis cannot be squeezed."}}, "tf.stack": {"description": "Stacks a list of rank-R tensors into one rank-(R&#43;1) tensor.", "Args": {"values": "A list of Tensor objects with the same shape and type.", "axis": "An int. The axis to stack along. Defaults to the first dimension.\nNegative values wrap around, so the valid range is [-(R+1), R+1).", "name": "A name for this operation (optional)."}, "Returns": "output\n\n\nA stacked Tensor with the same type as values.", "Raises": {"ValueError": "If axis is out of the range [-(R+1), R+1)."}}, "tf.stop_gradient": {"description": "Stops gradient computation.", "Args": {"input": "A Tensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.strided_slice": {"description": "Extracts a strided slice of a tensor (generalized Python array indexing).", "Args": {"input_": "A Tensor.", "begin": "An int32 or int64 Tensor.", "end": "An int32 or int64 Tensor.", "strides": "An int32 or int64 Tensor.", "begin_mask": "An int32 mask.", "end_mask": "An int32 mask.", "ellipsis_mask": "An int32 mask.", "new_axis_mask": "An int32 mask.", "shrink_axis_mask": "An int32 mask.", "var": "The variable corresponding to input_ or None", "name": "A name for the operation (optional)."}, "Returns": "A Tensor the same type as input."}, "tf.switch_case": {"description": "Create a switch/case operation, i.e. an integer-indexed conditional.", "Args": {"branch_index": "An int Tensor specifying which of branch_fns should be\nexecuted.", "branch_fns": "A dict mapping ints to callables, or a list of\n(int, callable) pairs, or simply a list of callables (in which case the\nindex serves as the key). Each callable must return a matching structure\nof tensors.", "default": "Optional callable that returns a structure of tensors.", "name": "A name for this operation (optional)."}, "Returns": "The tensors returned by the callable identified by branch_index, or those\nreturned by default if no key matches and default was provided, or those\nreturned by the max-keyed branch_fn if no default is provided.", "Raises": {"TypeError": "If fns[i] is not callable for any i, or default is not\ncallable."}}, "tf.tensor_scatter_nd_add": {"description": "Adds sparse updates to an existing tensor according to indices.", "Args": {"tensor": "A Tensor. Tensor to copy/update.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nIndex tensor.", "updates": "A Tensor. Must have the same type as tensor.\nUpdates to scatter into output.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as tensor."}, "tf.tensor_scatter_nd_max": {"description": "Apply a sparse update to a tensor taking the element-wise maximum.", "Args": {"tensor": "A Tensor. Tensor to update.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nIndex tensor.", "updates": "A Tensor. Must have the same type as tensor.\nUpdates to scatter into output.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as tensor."}, "tf.tensor_scatter_nd_min": {"Args": {"tensor": "A Tensor. Tensor to update.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nIndex tensor.", "updates": "A Tensor. Must have the same type as tensor.\nUpdates to scatter into output.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as tensor."}, "tf.tensor_scatter_nd_sub": {"description": "Subtracts sparse updates from an existing tensor according to indices.", "Args": {"tensor": "A Tensor. Tensor to copy/update.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nIndex tensor.", "updates": "A Tensor. Must have the same type as tensor.\nUpdates to scatter into output.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as tensor."}, "tf.tensor_scatter_nd_update": {"description": "Scatter updates into an existing tensor according to indices.", "Args": {"tensor": "Tensor to copy/update.", "indices": "Indices to update.", "updates": "Updates to apply at the indices.", "name": "Optional name for the operation."}, "Returns": "A new tensor with the given shape and updates applied according to the\nindices."}, "tf.tensordot": {"description": "Tensor contraction of a and b along specified axes and outer product.", "Args": {"a": "Tensor of type float32 or float64.", "b": "Tensor with the same type as a.", "axes": "Either a scalar N, or a list or an int32 Tensor of shape [2, k].\nIf axes is a scalar, sum over the last N axes of a and the first N axes of\nb in order. If axes is a list or Tensor the first and second row contain\nthe set of unique integers specifying axes along which the contraction is\ncomputed, for a and b, respectively. The number of axes for a and\nb must be equal. If axes=0, computes the outer product between a and\nb.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor with the same type as a.", "Raises": {"ValueError": "If the shapes of a, b, and axes are incompatible.", "IndexError": "If the values in axes exceed the rank of the corresponding\ntensor."}}, "tf.tile": {"description": "Constructs a tensor by tiling a given tensor.", "Args": {"input": "A Tensor. 1-D or higher.", "multiples": "A Tensor. Must be one of the following types: int32, int64.\n1-D. Length must be the same as the number of dimensions in input", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.timestamp": {"description": "Provides the time since epoch in seconds.", "Args": {"name": "A name for the operation (optional)."}, "Returns": "A Tensor of type float64."}, "tf.transpose": {"description": "Transposes a, where a is a Tensor.", "Args": {"a": "A Tensor.", "perm": "A permutation of the dimensions of a.  This should be a vector.", "conjugate": "Optional bool. Setting it to True is mathematically equivalent\nto tf.math.conj(tf.transpose(input)).", "name": "A name for the operation (optional)."}, "Returns": "A transposed Tensor."}, "tf.truncatediv": {"description": "Returns x / y element-wise for integer types.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, uint8, int8, uint16, int16, int32, uint32, uint64, int64, complex64, complex128.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.truncatemod": {"description": "Returns element-wise remainder of division. This emulates C semantics in that", "Args": {"x": "A Tensor. Must be one of the following types: int32, int64, bfloat16, half, float32, float64.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.tuple": {"description": "Groups tensors together.", "Args": {"tensors": "A list of Tensors or IndexedSlices, some entries can be None.", "control_inputs": "List of additional ops to finish before returning.", "name": "(optional) A name to use as a name_scope for the operation."}, "Returns": "Same as tensors.", "Raises": {"ValueError": "If tensors does not contain any Tensor or IndexedSlices.", "TypeError": "If control_inputs is not a list of Operation or Tensor\nobjects."}}, "tf.type_spec_from_value": {"description": "Returns a tf.TypeSpec that represents the given value.", "Args": {"value": "A value that can be accepted or returned by TensorFlow APIs. Accepted\ntypes for value include tf.Tensor, any value that can be converted to\ntf.Tensor using tf.convert_to_tensor, and any subclass of\nCompositeTensor (such as tf.RaggedTensor)."}, "Returns": "A TypeSpec that is compatible with value.", "Raises": {"TypeError": "If a TypeSpec cannot be built for value, because its type\nis not supported."}}, "tf.unique": {"description": "Finds unique elements in a 1-D tensor.", "Args": {"x": "A Tensor. 1-D.", "out_idx": "An optional tf.DType from: tf.int32, tf.int64. Defaults to tf.int32.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (y, idx)."}, "tf.unique_with_counts": {"description": "Finds unique elements in a 1-D tensor.", "Args": {"x": "A Tensor. 1-D.", "out_idx": "An optional tf.DType from: tf.int32, tf.int64. Defaults to tf.int32.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (y, idx, count)."}, "tf.unravel_index": {"description": "Converts an array of flat indices into a tuple of coordinate arrays.", "Args": {"indices": "A Tensor. Must be one of the following types: int32, int64.\nAn 0-D or 1-D int Tensor whose elements are indices into the\nflattened version of an array of dimensions dims.", "dims": "A Tensor. Must have the same type as indices.\nAn 1-D int Tensor. The shape of the array to use for unraveling\nindices.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as indices."}, "tf.unstack": {"description": "Unpacks the given dimension of a rank-R tensor into rank-(R-1) tensors.", "Args": {"value": "A rank R > 0 Tensor to be unstacked.", "num": "An int. The length of the dimension axis. Automatically inferred if\nNone (the default).", "axis": "An int. The axis to unstack along. Defaults to the first dimension.\nNegative values wrap around, so the valid range is [-R, R).", "name": "A name for the operation (optional)."}, "Returns": "The list of Tensor objects unstacked from value.", "Raises": {"ValueError": "If num is unspecified and cannot be inferred.", "InvalidArgumentError": "If num does not match the shape of value."}}, "tf.variable_creator_scope": {"description": "Scope which defines a variable creation function to be used by variable().", "Args": {"variable_creator": "the passed creator"}}, "tf.vectorized_map": {"description": "Parallel map on the list of tensors unpacked from elems on dimension 0.", "Args": {"fn": "The callable to be performed. It accepts one argument, which will have\nthe same (possibly nested) structure as elems, and returns a possibly\nnested structure of Tensors and Operations, which may be different than\nthe structure of elems.", "elems": "A tensor or (possibly nested) sequence of tensors, each of which will\nbe unpacked along their first dimension. The nested sequence of the\nresulting slices will be mapped over by fn. The first dimensions of all\nelements must broadcast to a consistent value; equivalently, each\nelement tensor must have first dimension of either B or 1, for some\ncommon batch size B >= 1.", "fallback_to_while_loop": "If true, on failing to vectorize an operation,\nthe unsupported op is wrapped in a tf.while_loop to execute the map\niterations. Note that this fallback only happens for unsupported ops and\nother parts of fn are still vectorized. If false, on encountering an\nunsupported op, a ValueError is thrown. Note that the fallbacks can result\nin slowdowns since vectorization often yields speedup of one to two orders\nof magnitude."}, "Returns": "A tensor or (possibly nested) sequence of tensors. Each tensor packs the\nresults of applying fn to tensors unpacked from elems along the first\ndimension, from first to last.\nAlthough they are less common as user-visible inputs and outputs, note that\ntensors of type tf.variant which represent tensor lists (for example from\ntf.raw_ops.TensorListFromTensor) are vectorized by stacking the list\ncontents rather than the variant itself, and so the container tensor will\nhave a scalar shape when returned rather than the usual stacked shape. This\nimproves the performance of control flow gradient vectorization.", "Raises": {"ValueError": "If vectorization fails and fallback_to_while_loop is False."}}, "tf.where": {"description": "Returns the indices of non-zero elements, or multiplexes x and y.", "Args": {"condition": "A tf.Tensor of dtype bool, or any numeric dtype. condition\nmust have dtype bool when x and y are provided.", "x": "If provided, a Tensor which is of the same type as y, and has a shape\nbroadcastable with condition and y.", "y": "If provided, a Tensor which is of the same type as x, and has a shape\nbroadcastable with condition and x.", "name": "A name of the operation (optional)."}, "Returns": "If x and y are provided:\n  A Tensor with the same type as x and y, and shape that\n  is broadcast from condition, x, and y.\nOtherwise, a Tensor with shape [tf.math.count_nonzero(condition),\ntf.rank(condition)].", "Raises": {"ValueError": "When exactly one of x or y is non-None, or the shapes\nare not all broadcastable."}}, "tf.while_loop": {"description": "Repeat body while the condition cond is true. (deprecated argument values)", "Args": {"cond": "A callable that represents the termination condition of the loop.", "body": "A callable that represents the loop body.", "loop_vars": "A (possibly nested) tuple, namedtuple or list of numpy array,\nTensor, and TensorArray objects.", "shape_invariants": "The shape invariants for the loop variables.", "parallel_iterations": "The number of iterations allowed to run in parallel. It\nmust be a positive integer.", "back_prop": "(optional) Deprecated. False disables support for back\npropagation. Prefer using tf.stop_gradient instead.", "swap_memory": "Whether GPU-CPU memory swap is enabled for this loop.", "maximum_iterations": "Optional maximum number of iterations of the while loop\nto run.  If provided, the cond output is AND-ed with an additional\ncondition ensuring the number of iterations executed is no greater than\nmaximum_iterations.", "name": "Optional name prefix for the returned tensors."}, "Returns": "The output tensors for the loop variables after the loop. The return value\nhas the same structure as loop_vars.", "Raises": {"TypeError": "if cond or body is not callable.", "ValueError": "if loop_vars is empty."}}, "tf.zeros": {"description": "Creates a tensor with all elements set to zero.", "Args": {"shape": "A list of integers, a tuple of integers, or\na 1-D Tensor of type int32.", "dtype": "The DType of an element in the resulting Tensor.", "name": "Optional string. A name for the operation."}, "Returns": "A Tensor with all elements set to zero."}, "tf.zeros_initializer": {"description": "Initializer that generates tensors initialized to 0."}, "tf.zeros_like": {"description": "Creates a tensor with all elements set to zero.", "Args": {"input": "A Tensor or array-like object.", "dtype": "A type for the returned Tensor. Must be float16, float32,\nfloat64, int8, uint8, int16, uint16, int32, int64,\ncomplex64, complex128, bool or string (optional).", "name": "A name for the operation (optional)."}, "Returns": "A Tensor with all elements set to zero."}}, "tf.audio": {"tf.audio.decode_wav": {"description": "Decode a 16-bit PCM WAV file to a float tensor.", "Args": {"contents": "A Tensor of type string.\nThe WAV-encoded audio, usually from a file.", "desired_channels": "An optional int. Defaults to -1.\nNumber of sample channels wanted.", "desired_samples": "An optional int. Defaults to -1.\nLength of audio requested.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (audio, sample_rate)."}, "tf.audio.encode_wav": {"description": "Encode audio data using the WAV file format.", "Args": {"audio": "A Tensor of type float32. 2-D with shape [length, channels].", "sample_rate": "A Tensor of type int32.\nScalar containing the sample frequency.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}}, "tf.autodiff": {"tf.autodiff.ForwardAccumulator": {"description": "Computes Jacobian-vector products (&#34;JVP&#34;s) using forward-mode autodiff.", "Args": {"primals": "A tensor or nested structure of tensors to watch.", "tangents": "A tensor or nested structure of tensors, with the same nesting\nstructure as primals, with each element being a vector with the same\nsize as the corresponding primal element."}, "Raises": {"ValueError": "If the same tensor or variable is specified multiple times in\nprimals."}}}, "tf.autograph": {"tf.autograph.set_verbosity": {"description": "Sets the AutoGraph verbosity level.", "Args": {"level": "int, the verbosity level; larger values specify increased verbosity;\n0 means no logging. When reporting bugs, it is recommended to set this\nvalue to a larger number, like 10.", "alsologtostdout": "bool, whether to also output log messages to sys.stdout."}}, "tf.autograph.to_code": {"description": "Returns the source code generated by AutoGraph, as a string.", "Args": {"entity": "Python callable or class to convert.", "recursive": "Whether to recursively convert any functions that the converted\nfunction may call.", "experimental_optional_features": "None, a tuple of, or a single\ntf.autograph.experimental.Feature value."}, "Returns": "The converted code as string."}, "tf.autograph.to_graph": {"description": "Converts a Python entity into a TensorFlow graph.", "Args": {"entity": "Python callable or class to convert.", "recursive": "Whether to recursively convert any functions that the converted\nfunction may call.", "experimental_optional_features": "None, a tuple of, or a single\ntf.autograph.experimental.Feature value."}, "Returns": "Same as entity, the converted Python function or class.", "Raises": {"ValueError": "If the entity could not be converted."}}, "tf.autograph.trace": {"description": "Traces argument information at compilation time.", "Args": {"*args": "Arguments to print to sys.stdout."}}}, "tf.bitwise": {"tf.bitwise.bitwise_and": {"description": "Elementwise computes the bitwise AND of x and y.", "Args": {"x": "A Tensor. Must be one of the following types: int8, int16, int32, int64, uint8, uint16, uint32, uint64.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.bitwise.bitwise_or": {"description": "Elementwise computes the bitwise OR of x and y.", "Args": {"x": "A Tensor. Must be one of the following types: int8, int16, int32, int64, uint8, uint16, uint32, uint64.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.bitwise.bitwise_xor": {"description": "Elementwise computes the bitwise XOR of x and y.", "Args": {"x": "A Tensor. Must be one of the following types: int8, int16, int32, int64, uint8, uint16, uint32, uint64.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.bitwise.invert": {"description": "Invert (flip) each bit of supported types; for example, type uint8 value 01010101 becomes 10101010.", "Args": {"x": "A Tensor. Must be one of the following types: int8, int16, int32, int64, uint8, uint16, uint32, uint64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.bitwise.left_shift": {"description": "Elementwise computes the bitwise left-shift of x and y.", "Args": {"x": "A Tensor. Must be one of the following types: int8, int16, int32, int64, uint8, uint16, uint32, uint64.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.bitwise.right_shift": {"description": "Elementwise computes the bitwise right-shift of x and y.", "Args": {"x": "A Tensor. Must be one of the following types: int8, int16, int32, int64, uint8, uint16, uint32, uint64.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}}, "tf.compat": {"tf.compat.as_bytes": {"description": "Converts bytearray, bytes, or unicode python input types to bytes.", "Args": {"bytes_or_text": "A bytearray, bytes, str, or unicode object.", "encoding": "A string indicating the charset for encoding unicode."}, "Returns": "A bytes object.", "Raises": {"TypeError": "If bytes_or_text is not a binary or unicode string."}}, "tf.compat.as_str": {}, "tf.compat.as_str_any": {"description": "Converts input to str type.", "Args": {"value": "A object that can be converted to str."}, "Returns": "A str object."}, "tf.compat.as_text": {"description": "Converts any string-like python input types to unicode.", "Args": {"bytes_or_text": "A bytes, str, or unicode object.", "encoding": "A string indicating the charset for decoding unicode."}, "Returns": "A unicode (Python 2) or str (Python 3) object.", "Raises": {"TypeError": "If bytes_or_text is not a binary or unicode string."}}, "tf.compat.dimension_at_index": {"description": "Compatibility utility required to allow for both V1 and V2 behavior in TF.", "Args": {"shape": "A TensorShape instance.", "index": "An integer index."}, "Returns": "A dimension object."}, "tf.compat.dimension_value": {"description": "Compatibility utility required to allow for both V1 and V2 behavior in TF.", "Args": {"dimension": "Either a Dimension instance, an integer, or None."}, "Returns": "A plain value, i.e. an integer or None."}, "tf.compat.forward_compatibility_horizon": {"description": "Context manager for testing forward compatibility of generated graphs.", "Args": {"year": "A year (e.g., 2018). Must be an int.", "month": "A month (1 <= month <= 12) in year. Must be an int.", "day": "A day (1 <= day <= 31, or 30, or 29, or 28) in month. Must be an\nint."}}, "tf.compat.forward_compatible": {"description": "Return true if the forward compatibility window has expired.", "Args": {"year": "A year (e.g., 2018). Must be an int.", "month": "A month (1 <= month <= 12) in year. Must be an int.", "day": "A day (1 <= day <= 31, or 30, or 29, or 28) in month. Must be an\nint."}, "Returns": "True if the caller can expect that serialized TensorFlow graphs produced\ncan be consumed by programs that are compiled with the TensorFlow library\nsource code after (year, month, day)."}, "tf.compat.path_to_str": {"description": "Converts input which is a PathLike object to str type.", "Args": {"path": "An object that can be converted to path representation."}, "Returns": "A str object."}}, "tf.compat.v1": {"tf.compat.v1.AttrValue": {"description": "A ProtocolMessage"}, "tf.compat.v1.AttrValue.ListValue": {"description": "A ProtocolMessage"}, "tf.compat.v1.ConditionalAccumulator": {"description": "A conditional accumulator for aggregating gradients.", "Args": {"dtype": "Datatype of the accumulated gradients.", "shape": "Shape of the accumulated gradients.", "shared_name": "Optional. If non-empty, this accumulator will be shared under\nthe given name across multiple sessions.", "name": "Optional name for the accumulator.", "reduction_type": "Reduction type to use when taking the gradient."}, "Attributes": {"accumulator_ref": "The underlying accumulator reference.", "dtype": "The datatype of the gradients accumulated by this accumulator.", "name": "The name of the underlying accumulator."}}, "tf.compat.v1.ConditionalAccumulatorBase": {"description": "A conditional accumulator for aggregating gradients.", "Args": {"dtype": "Datatype of the accumulated gradients.", "shape": "Shape of the accumulated gradients.", "accumulator_ref": "A handle to the conditional accumulator, created by sub-\nclasses"}, "Attributes": {"accumulator_ref": "The underlying accumulator reference.", "dtype": "The datatype of the gradients accumulated by this accumulator.", "name": "The name of the underlying accumulator."}}, "tf.compat.v1.ConfigProto": {"description": "A ProtocolMessage", "Attributes": {"allow_soft_placement": "bool allow_soft_placement", "cluster_def": "ClusterDef cluster_def", "device_count": "repeated DeviceCountEntry device_count", "device_filters": "repeated string device_filters", "experimental": "Experimental experimental", "gpu_options": "GPUOptions gpu_options", "graph_options": "GraphOptions graph_options", "inter_op_parallelism_threads": "int32 inter_op_parallelism_threads", "intra_op_parallelism_threads": "int32 intra_op_parallelism_threads", "isolate_session_state": "bool isolate_session_state", "log_device_placement": "bool log_device_placement", "operation_timeout_in_ms": "int64 operation_timeout_in_ms", "placement_period": "int32 placement_period", "rpc_options": "RPCOptions rpc_options", "session_inter_op_thread_pool": "repeated ThreadPoolOptionProto session_inter_op_thread_pool", "share_cluster_devices_in_session": "bool share_cluster_devices_in_session", "use_per_session_threads": "bool use_per_session_threads"}}, "tf.compat.v1.ConfigProto.DeviceCountEntry": {"description": "A ProtocolMessage"}, "tf.compat.v1.ConfigProto.Experimental": {"description": "A ProtocolMessage", "Class Variables": {"MLIR_BRIDGE_ROLLOUT_DISABLED": "2", "MLIR_BRIDGE_ROLLOUT_ENABLED": "1", "MLIR_BRIDGE_ROLLOUT_SAFE_MODE_ENABLED": "3", "MLIR_BRIDGE_ROLLOUT_SAFE_MODE_FALLBACK_ENABLED": "4", "MLIR_BRIDGE_ROLLOUT_UNSPECIFIED": "0", "MlirBridgeRollout": "Instance of google.protobuf.internal.enum_type_wrapper.EnumTypeWrapper"}}, "tf.compat.v1.DeviceSpec": {"description": "Represents a (possibly partial) specification for a TensorFlow device.", "Args": {"job": "string.  Optional job name.", "replica": "int.  Optional replica index.", "task": "int.  Optional task index.", "device_type": "Optional device type string (e.g. \"CPU\" or \"GPU\")", "device_index": "int.  Optional device index.  If left unspecified, device\nrepresents 'any' device_index."}, "Attributes": {"device_index": "", "device_type": "", "job": "", "replica": "", "task": ""}}, "tf.compat.v1.Dimension": {"description": "Represents the value of one dimension in a TensorShape.", "Attributes": {"value": "The value of this dimension, or None if it is unknown."}}, "tf.compat.v1.Event": {"description": "A ProtocolMessage"}, "tf.compat.v1.FixedLengthRecordReader": {"description": "A Reader that outputs fixed-length records from a file.", "Args": {"record_bytes": "An int.", "header_bytes": "An optional int. Defaults to 0.", "footer_bytes": "An optional int. Defaults to 0.", "hop_bytes": "An optional int. Defaults to 0.", "name": "A name for the operation (optional).", "encoding": "The type of encoding for the file. Defaults to none."}, "Attributes": {"reader_ref": "Op that implements the reader.", "supports_serialize": "Whether the Reader implementation can serialize its state."}}, "tf.compat.v1.GPUOptions": {"description": "A ProtocolMessage"}, "tf.compat.v1.GPUOptions.Experimental": {"description": "A ProtocolMessage"}, "tf.compat.v1.GPUOptions.Experimental.VirtualDevices": {"description": "A ProtocolMessage"}, "tf.compat.v1.GraphDef": {"description": "A protobuf containing the graph of operations.", "Attributes": {"library": "FunctionDefLibrary library", "node": "repeated NodeDef node", "version": "int32 version", "versions": "VersionDef versions"}}, "tf.compat.v1.GraphKeys": {"description": "Standard names to use for graph collections.", "Class Variables": {"ACTIVATIONS": "'activations'", "ASSET_FILEPATHS": "'asset_filepaths'", "BIASES": "'biases'", "CONCATENATED_VARIABLES": "'concatenated_variables'", "COND_CONTEXT": "'cond_context'", "EVAL_STEP": "'eval_step'", "GLOBAL_STEP": "'global_step'", "GLOBAL_VARIABLES": "'variables'", "INIT_OP": "'init_op'", "LOCAL_INIT_OP": "'local_init_op'", "LOCAL_RESOURCES": "'local_resources'", "LOCAL_VARIABLES": "'local_variables'", "LOSSES": "'losses'", "METRIC_VARIABLES": "'metric_variables'", "MODEL_VARIABLES": "'model_variables'", "MOVING_AVERAGE_VARIABLES": "'moving_average_variables'", "QUEUE_RUNNERS": "'queue_runners'", "READY_FOR_LOCAL_INIT_OP": "'ready_for_local_init_op'", "READY_OP": "'ready_op'", "REGULARIZATION_LOSSES": "'regularization_losses'", "RESOURCES": "'resources'", "SAVEABLE_OBJECTS": "'saveable_objects'", "SAVERS": "'savers'", "SUMMARIES": "'summaries'", "SUMMARY_OP": "'summary_op'", "TABLE_INITIALIZERS": "'table_initializer'", "TRAINABLE_RESOURCE_VARIABLES": "'trainable_resource_variables'", "TRAINABLE_VARIABLES": "'trainable_variables'", "TRAIN_OP": "'train_op'", "UPDATE_OPS": "'update_ops'", "VARIABLES": "'variables'", "WEIGHTS": "'weights'", "WHILE_CONTEXT": "'while_context'"}}, "tf.compat.v1.GraphOptions": {"description": "A ProtocolMessage"}, "tf.compat.v1.HistogramProto": {"description": "A ProtocolMessage"}, "tf.compat.v1.IdentityReader": {"description": "A Reader that outputs the queued work as both the key and value.", "Args": {"name": "A name for the operation (optional)."}, "Attributes": {"reader_ref": "Op that implements the reader.", "supports_serialize": "Whether the Reader implementation can serialize its state."}}, "tf.compat.v1.InteractiveSession": {"description": "A TensorFlow Session for use in interactive contexts, such as a shell.", "Args": {"target": "(Optional.) The execution engine to connect to. Defaults to using\nan in-process engine.", "graph": "(Optional.) The Graph to be launched (described above).", "config": "(Optional) ConfigProto proto used to configure the session."}, "Attributes": {"graph": "The graph that was launched in this session.", "graph_def": "A serializable version of the underlying TensorFlow graph.", "sess_str": "The TensorFlow process to which this session will connect."}}, "tf.compat.v1.LMDBReader": {"description": "A Reader that outputs the records from a LMDB file.", "Args": {"name": "A name for the operation (optional).", "options": "A LMDBRecordOptions object (optional)."}, "Attributes": {"reader_ref": "Op that implements the reader.", "supports_serialize": "Whether the Reader implementation can serialize its state."}}, "tf.compat.v1.LogMessage": {"description": "A ProtocolMessage", "Class Variables": {"DEBUGGING": "10", "ERROR": "40", "FATAL": "50", "INFO": "20", "Level": "Instance of google.protobuf.internal.enum_type_wrapper.EnumTypeWrapper", "UNKNOWN": "0", "WARN": "30"}}, "tf.compat.v1.MetaGraphDef": {"description": "A ProtocolMessage"}, "tf.compat.v1.MetaGraphDef.CollectionDefEntry": {"description": "A ProtocolMessage"}, "tf.compat.v1.MetaGraphDef.MetaInfoDef": {"description": "A ProtocolMessage"}, "tf.compat.v1.MetaGraphDef.MetaInfoDef.FunctionAliasesEntry": {"description": "A ProtocolMessage"}, "tf.compat.v1.MetaGraphDef.SignatureDefEntry": {"description": "A ProtocolMessage"}, "tf.compat.v1.NameAttrList": {"description": "A ProtocolMessage"}, "tf.compat.v1.NameAttrList.AttrEntry": {"description": "A ProtocolMessage"}, "tf.compat.v1.NodeDef": {"description": "A ProtocolMessage"}, "tf.compat.v1.NodeDef.AttrEntry": {"description": "A ProtocolMessage"}, "tf.compat.v1.NodeDef.ExperimentalDebugInfo": {"description": "A ProtocolMessage"}, "tf.compat.v1.OptimizerOptions": {"description": "A ProtocolMessage", "Class Variables": {"DEFAULT": "0", "GlobalJitLevel": "Instance of google.protobuf.internal.enum_type_wrapper.EnumTypeWrapper", "L0": "-1", "L1": "0", "Level": "Instance of google.protobuf.internal.enum_type_wrapper.EnumTypeWrapper", "OFF": "-1", "ON_1": "1", "ON_2": "2"}}, "tf.compat.v1.Print": {"description": "Prints a list of tensors. (deprecated)", "Args": {"input_": "A tensor passed through this op.", "data": "A list of tensors to print out when op is evaluated.", "message": "A string, prefix of the error message.", "first_n": "Only log first_n number of times. Negative numbers log always;\nthis is the default.", "summarize": "Only print this many entries of each tensor. If None, then a\nmaximum of 3 elements are printed per input tensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type and contents as input_.\nsess = tf.compat.v1.Session()with sess.as_default():\u00a0 \u00a0 tensor = tf.range(10)\u00a0 \u00a0 print_op = tf.print(tensor)\u00a0 \u00a0 with tf.control_dependencies([print_op]):\u00a0 \u00a0 \u00a0 out = tf.add(tensor, tensor)\u00a0 \u00a0 sess.run(out)"}, "tf.compat.v1.ReaderBase": {"description": "Base class for different Reader types, that produce a record every step.", "Args": {"reader_ref": "The operation that implements the reader.", "supports_serialize": "True if the reader implementation can\nserialize its state."}, "Raises": {"RuntimeError": "If eager execution is enabled."}, "Attributes": {"reader_ref": "Op that implements the reader.", "supports_serialize": "Whether the Reader implementation can serialize its state."}}, "tf.compat.v1.RunMetadata": {"description": "A ProtocolMessage"}, "tf.compat.v1.RunMetadata.FunctionGraphs": {"description": "A ProtocolMessage"}, "tf.compat.v1.RunOptions": {"description": "A ProtocolMessage", "Class Variables": {"FULL_TRACE": "3", "HARDWARE_TRACE": "2", "NO_TRACE": "0", "SOFTWARE_TRACE": "1", "TraceLevel": "Instance of google.protobuf.internal.enum_type_wrapper.EnumTypeWrapper"}}, "tf.compat.v1.RunOptions.Experimental": {"description": "A ProtocolMessage"}, "tf.compat.v1.RunOptions.Experimental.RunHandlerPoolOptions": {"description": "A ProtocolMessage"}, "tf.compat.v1.Session": {"description": "A class for running TensorFlow operations.", "Args": {"target": "(Optional.) The execution engine to connect to. Defaults to using\nan in-process engine. See\nDistributed TensorFlow for\n  more examples.", "graph": "(Optional.) The Graph to be launched (described above).", "config": "(Optional.) A\nConfigProto\n  protocol buffer with configuration options for the session."}, "Attributes": {"graph": "The graph that was launched in this session.", "graph_def": "A serializable version of the underlying TensorFlow graph.", "sess_str": "The TensorFlow process to which this session will connect."}}, "tf.compat.v1.SessionLog": {"description": "A ProtocolMessage", "Attributes": {"checkpoint_path": "string checkpoint_path", "msg": "string msg", "status": "SessionStatus status"}, "Class Variables": {"CHECKPOINT": "3", "START": "1", "STATUS_UNSPECIFIED": "0", "STOP": "2", "SessionStatus": "Instance of google.protobuf.internal.enum_type_wrapper.EnumTypeWrapper"}}, "tf.compat.v1.SparseConditionalAccumulator": {"description": "A conditional accumulator for aggregating sparse gradients.", "Args": {"dtype": "Datatype of the accumulated gradients.", "shape": "Shape of the accumulated gradients.", "shared_name": "Optional. If non-empty, this accumulator will be shared under\nthe given name across multiple sessions.", "name": "Optional name for the accumulator.", "reduction_type": "Reduction type to use when taking the gradient."}, "Attributes": {"accumulator_ref": "The underlying accumulator reference.", "dtype": "The datatype of the gradients accumulated by this accumulator.", "name": "The name of the underlying accumulator."}}, "tf.compat.v1.SparseTensorValue": {"description": "SparseTensorValue(indices, values, dense_shape)", "Attributes": {"indices": "A namedtuple alias for field number 0", "values": "A namedtuple alias for field number 1", "dense_shape": "A namedtuple alias for field number 2"}}, "tf.compat.v1.Summary": {"description": "A ProtocolMessage", "Attributes": {"value": "repeated Value value"}}, "tf.compat.v1.Summary.Audio": {"description": "A ProtocolMessage"}, "tf.compat.v1.Summary.Image": {"description": "A ProtocolMessage"}, "tf.compat.v1.Summary.Value": {"description": "A ProtocolMessage"}, "tf.compat.v1.SummaryMetadata": {"description": "A ProtocolMessage"}, "tf.compat.v1.SummaryMetadata.PluginData": {"description": "A ProtocolMessage"}, "tf.compat.v1.TFRecordReader": {"description": "A Reader that outputs the records from a TFRecords file.", "Args": {"name": "A name for the operation (optional).", "options": "A TFRecordOptions object (optional)."}, "Attributes": {"reader_ref": "Op that implements the reader.", "supports_serialize": "Whether the Reader implementation can serialize its state."}}, "tf.compat.v1.TensorInfo": {"description": "A ProtocolMessage"}, "tf.compat.v1.TensorInfo.CompositeTensor": {"description": "A ProtocolMessage"}, "tf.compat.v1.TensorInfo.CooSparse": {"description": "A ProtocolMessage"}, "tf.compat.v1.TextLineReader": {"description": "A Reader that outputs the lines of a file delimited by newlines.", "Args": {"skip_header_lines": "An optional int. Defaults to 0.  Number of lines\nto skip from the beginning of every file.", "name": "A name for the operation (optional)."}, "Attributes": {"reader_ref": "Op that implements the reader.", "supports_serialize": "Whether the Reader implementation can serialize its state."}}, "tf.compat.v1.Variable": {"description": "See the [Variables Guide](https://tensorflow.org/guide/variables).", "Args": {"initial_value": "A Tensor, or Python object convertible to a Tensor,\nwhich is the initial value for the Variable. The initial value must have\na shape specified unless validate_shape is set to False. Can also be a\ncallable with no argument that returns the initial value when called. In\nthat case, dtype must be specified. (Note that initializer functions\nfrom init_ops.py must first be bound to a shape before being used here.)", "trainable": "If True, also adds the variable to the graph collection\nGraphKeys.TRAINABLE_VARIABLES. This collection is used as the default\nlist of variables to use by the Optimizer classes. Defaults to True,\nunless synchronization is set to ON_READ, in which case it defaults\nto False.", "collections": "List of graph collections keys. The new variable is added to\nthese collections. Defaults to [GraphKeys.GLOBAL_VARIABLES].", "validate_shape": "If False, allows the variable to be initialized with a\nvalue of unknown shape. If True, the default, the shape of\ninitial_value must be known.", "caching_device": "Optional device string describing where the Variable\nshould be cached for reading.  Defaults to the Variable's device. If not\nNone, caches on another device.  Typical use is to cache on the device\nwhere the Ops using the Variable reside, to deduplicate copying through\nSwitch and other conditional statements.", "name": "Optional name for the variable. Defaults to 'Variable' and gets\nuniquified automatically.", "variable_def": "VariableDef protocol buffer. If not None, recreates the\nVariable object with its contents, referencing the variable's nodes in\nthe graph, which must already exist. The graph is not changed.\nvariable_def and the other arguments are mutually exclusive.", "dtype": "If set, initial_value will be converted to the given type. If\nNone, either the datatype will be kept (if initial_value is a\nTensor), or convert_to_tensor will decide.", "expected_shape": "A TensorShape. If set, initial_value is expected to have\nthis shape.", "import_scope": "Optional string. Name scope to add to the Variable. Only\nused when initializing from protocol buffer.", "constraint": "An optional projection function to be applied to the variable\nafter being updated by an Optimizer (e.g. used to implement norm\nconstraints or value constraints for layer weights). The function must\ntake as input the unprojected Tensor representing the value of the\nvariable and return the Tensor for the projected value (which must have\nthe same shape). Constraints are not safe to use when doing asynchronous\ndistributed training.", "use_resource": "whether to use resource variables.", "synchronization": "Indicates when a distributed a variable will be\naggregated. Accepted values are constants defined in the class\ntf.VariableSynchronization. By default the synchronization is set to\nAUTO and the current DistributionStrategy chooses when to\nsynchronize.", "aggregation": "Indicates how a distributed variable will be aggregated.\nAccepted values are constants defined in the class\ntf.VariableAggregation.", "shape": "(optional) The shape of this variable. If None, the shape of\ninitial_value will be used. When setting this argument to\ntf.TensorShape(None) (representing an unspecified shape), the variable\ncan be assigned with values of different shapes."}, "Raises": {"ValueError": "If the initial value is not specified, or does not have a\nshape and validate_shape is True.", "RuntimeError": "If eager execution is enabled."}, "Attributes": {"aggregation": "", "constraint": "Returns the constraint function associated with this variable.", "device": "The device of this variable.", "dtype": "The DType of this variable.", "graph": "The Graph of this variable.", "initial_value": "Returns the Tensor used as the initial value for the variable.\nNote that this is different from initialized_value() which runs\nthe op that initializes the variable before returning its value.\nThis method returns the tensor that is used by the op that initializes\nthe variable.", "initializer": "The initializer operation for this variable.", "name": "The name of this variable.", "op": "The Operation of this variable.", "shape": "The TensorShape of this variable.", "synchronization": "", "trainable": ""}}, "tf.compat.v1.VariableAggregation": {"description": "Indicates how a distributed variable will be aggregated.", "Class Variables": {"MEAN": "<VariableAggregation.MEAN: 2>", "NONE": "<VariableAggregation.NONE: 0>", "ONLY_FIRST_REPLICA": "<VariableAggregation.ONLY_FIRST_REPLICA: 3>", "SUM": "<VariableAggregation.SUM: 1>"}}, "tf.compat.v1.VariableScope": {"description": "Variable scope object to carry defaults to provide to get_variable.", "Attributes": {"name": "name of the current scope, used as prefix in get_variable.", "initializer": "default initializer passed to get_variable.", "regularizer": "default regularizer passed to get_variable.", "reuse": "Boolean, None, or tf.compat.v1.AUTO_REUSE, setting the reuse in\nget_variable. When eager execution is enabled this argument is always\nforced to be False.", "caching_device": "string, callable, or None: the caching device passed to\nget_variable.", "partitioner": "callable or None: the partitioner passed to get_variable.", "custom_getter": "default custom getter passed to get_variable.", "name_scope": "The name passed to tf.name_scope.", "dtype": "default type passed to get_variable (defaults to DT_FLOAT).", "use_resource": "if False, create a normal Variable; if True create an\nexperimental ResourceVariable with well-defined semantics. Defaults to\nFalse (will later change to True). When eager execution is enabled this\nargument is always forced to be True.", "constraint": "An optional projection function to be applied to the variable\nafter being updated by an Optimizer (e.g. used to implement norm\nconstraints or value constraints for layer weights). The function must\ntake as input the unprojected Tensor representing the value of the\nvariable and return the Tensor for the projected value (which must have\nthe same shape). Constraints are not safe to use when doing asynchronous\ndistributed training.", "original_name_scope": ""}}, "tf.compat.v1.WholeFileReader": {"description": "A Reader that outputs the entire contents of a file as a value.", "Args": {"name": "A name for the operation (optional)."}, "Attributes": {"reader_ref": "Op that implements the reader.", "supports_serialize": "Whether the Reader implementation can serialize its state."}}, "tf.compat.v1.add_check_numerics_ops": {"description": "Connect a tf.debugging.check_numerics to every floating point tensor.", "Returns": "A group op depending on all check_numerics ops added.", "Raises": {"ValueError": "If the graph contains any numeric operations in a control flow\nstructure.", "RuntimeError": "If called with eager execution enabled."}}, "tf.compat.v1.add_to_collection": {"description": "Wrapper for Graph.add_to_collection() using the default graph.", "Args": {"name": "The key for the collection. For example, the GraphKeys class\ncontains many standard names for collections.", "value": "The value to add to the collection."}}, "tf.compat.v1.add_to_collections": {"description": "Wrapper for Graph.add_to_collections() using the default graph.", "Args": {"names": "The key for the collections. The GraphKeys class contains many\nstandard names for collections.", "value": "The value to add to the collections."}}, "tf.compat.v1.all_variables": {"description": "Use tf.compat.v1.global_variables instead. (deprecated)"}, "tf.compat.v1.arg_max": {"description": "Returns the index with the largest value across dimensions of a tensor.", "Args": {"input": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64, bool.", "dimension": "A Tensor. Must be one of the following types: int16, int32, int64.\nint16, int32 or int64, must be in the range [-rank(input), rank(input)).\nDescribes which dimension of the input Tensor to reduce across. For vectors,\nuse dimension = 0.", "output_type": "An optional tf.DType from: tf.int16, tf.uint16, tf.int32, tf.int64. Defaults to tf.int64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type output_type."}, "tf.compat.v1.arg_min": {"description": "Returns the index with the smallest value across dimensions of a tensor.", "Args": {"input": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64, bool.", "dimension": "A Tensor. Must be one of the following types: int32, int64.\nint32 or int64, must be in the range [-rank(input), rank(input)).\nDescribes which dimension of the input Tensor to reduce across. For vectors,\nuse dimension = 0.", "output_type": "An optional tf.DType from: tf.int32, tf.int64. Defaults to tf.int64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type output_type."}, "tf.compat.v1.argmax": {"description": "Returns the index with the largest value across axes of a tensor. (deprecated arguments)", "Args": {"input": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64, bool.", "axis": "A Tensor. Must be one of the following types: int16, int32, int64.\nint16, int32 or int64, must be in the range [-rank(input), rank(input)).\nDescribes which axis of the input Tensor to reduce across. For vectors,\nuse axis = 0.", "output_type": "An optional tf.DType from: tf.int16, tf.uint16, tf.int32, tf.int64. Defaults to tf.int64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type output_type."}, "tf.compat.v1.argmin": {"description": "Returns the index with the smallest value across axes of a tensor. (deprecated arguments)", "Args": {"input": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64, bool.", "axis": "A Tensor. Must be one of the following types: int32, int64.\nint32 or int64, must be in the range [-rank(input), rank(input)).\nDescribes which axis of the input Tensor to reduce across. For vectors,\nuse axis = 0.", "output_type": "An optional tf.DType from: tf.int32, tf.int64. Defaults to tf.int64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type output_type."}, "tf.compat.v1.assert_equal": {"description": "Assert the condition x == y holds element-wise.", "Args": {"x": "Numeric Tensor.", "y": "Numeric Tensor, same dtype as and broadcastable to x.", "data": "The tensors to print out if the condition is False.  Defaults to\nerror message and first few entries of x, y.", "summarize": "Print this many entries of each tensor.", "message": "A string to prefix to the default message.", "name": "A name for this operation (optional).  Defaults to \"assert_equal\"."}, "Returns": "Op that raises InvalidArgumentError if x == y is False.", "Raises": {"InvalidArgumentError": "if the check can be performed immediately and\nx == y is False. The check can be performed immediately during\neager execution or if x and y are statically known."}}, "tf.compat.v1.assert_greater": {"description": "Assert the condition x &gt; y holds element-wise.", "Args": {"x": "Numeric Tensor.", "y": "Numeric Tensor, same dtype as and broadcastable to x.", "data": "The tensors to print out if the condition is False.  Defaults to\nerror message and first few entries of x, y.", "summarize": "Print this many entries of each tensor.", "message": "A string to prefix to the default message.", "name": "A name for this operation (optional).  Defaults to \"assert_greater\"."}, "Returns": "Op that raises InvalidArgumentError if x > y is False.", "Raises": {"InvalidArgumentError": "if the check can be performed immediately and\nx > y is False. The check can be performed immediately during\neager execution or if x and y are statically known."}}, "tf.compat.v1.assert_greater_equal": {"description": "Assert the condition x &gt;= y holds element-wise.", "Args": {"x": "Numeric Tensor.", "y": "Numeric Tensor, same dtype as and broadcastable to x.", "data": "The tensors to print out if the condition is False.  Defaults to\nerror message and first few entries of x, y.", "summarize": "Print this many entries of each tensor.", "message": "A string to prefix to the default message.", "name": "A name for this operation (optional).  Defaults to \"assert_greater_equal\"."}, "Returns": "Op that raises InvalidArgumentError if x >= y is False.", "Raises": {"InvalidArgumentError": "if the check can be performed immediately and\nx >= y is False. The check can be performed immediately during\neager execution or if x and y are statically known."}}, "tf.compat.v1.assert_integer": {"description": "Assert that x is of integer dtype.", "Args": {"x": "Tensor whose basetype is integer and is not quantized.", "message": "A string to prefix to the default message.", "name": "A name for this operation (optional).  Defaults to \"assert_integer\"."}, "Raises": {"TypeError": "If x.dtype is anything other than non-quantized integer."}, "Returns": "A no_op that does nothing.  Type can be determined statically."}, "tf.compat.v1.assert_less": {"description": "Assert the condition x &lt; y holds element-wise.", "Args": {"x": "Numeric Tensor.", "y": "Numeric Tensor, same dtype as and broadcastable to x.", "data": "The tensors to print out if the condition is False.  Defaults to\nerror message and first few entries of x, y.", "summarize": "Print this many entries of each tensor.", "message": "A string to prefix to the default message.", "name": "A name for this operation (optional).  Defaults to \"assert_less\"."}, "Returns": "Op that raises InvalidArgumentError if x < y is False.", "Raises": {"InvalidArgumentError": "if the check can be performed immediately and\nx < y is False. The check can be performed immediately during\neager execution or if x and y are statically known."}}, "tf.compat.v1.assert_less_equal": {"description": "Assert the condition x &lt;= y holds element-wise.", "Args": {"x": "Numeric Tensor.", "y": "Numeric Tensor, same dtype as and broadcastable to x.", "data": "The tensors to print out if the condition is False.  Defaults to\nerror message and first few entries of x, y.", "summarize": "Print this many entries of each tensor.", "message": "A string to prefix to the default message.", "name": "A name for this operation (optional).  Defaults to \"assert_less_equal\"."}, "Returns": "Op that raises InvalidArgumentError if x <= y is False.", "Raises": {"InvalidArgumentError": "if the check can be performed immediately and\nx <= y is False. The check can be performed immediately during\neager execution or if x and y are statically known."}}, "tf.compat.v1.assert_near": {"description": "Assert the condition x and y are close element-wise.", "Args": {"x": "Float or complex Tensor.", "y": "Float or complex Tensor, same dtype as, and broadcastable to, x.", "rtol": "Tensor.  Same dtype as, and broadcastable to, x.\nThe relative tolerance.  Default is 10 * eps.", "atol": "Tensor.  Same dtype as, and broadcastable to, x.\nThe absolute tolerance.  Default is 10 * eps.", "data": "The tensors to print out if the condition is False.  Defaults to\nerror message and first few entries of x, y.", "summarize": "Print this many entries of each tensor.", "message": "A string to prefix to the default message.", "name": "A name for this operation (optional).  Defaults to \"assert_near\"."}, "Returns": "Op that raises InvalidArgumentError if x and y are not close enough."}, "tf.compat.v1.assert_negative": {"description": "Assert the condition x &lt; 0 holds element-wise.", "Args": {"x": "Numeric Tensor.", "data": "The tensors to print out if the condition is False.  Defaults to\nerror message and first few entries of x.", "summarize": "Print this many entries of each tensor.", "message": "A string to prefix to the default message.", "name": "A name for this operation (optional).  Defaults to \"assert_negative\"."}, "Returns": "Op that raises InvalidArgumentError if x < 0 is False.", "Raises": {"InvalidArgumentError": "if the check can be performed immediately and\nx < 0 is False. The check can be performed immediately during\neager execution or if x is statically known."}}, "tf.compat.v1.assert_non_negative": {"description": "Assert the condition x &gt;= 0 holds element-wise.", "Args": {"x": "Numeric Tensor.", "data": "The tensors to print out if the condition is False.  Defaults to\nerror message and first few entries of x.", "summarize": "Print this many entries of each tensor.", "message": "A string to prefix to the default message.", "name": "A name for this operation (optional).  Defaults to \"assert_non_negative\"."}, "Returns": "Op that raises InvalidArgumentError if x >= 0 is False.", "Raises": {"InvalidArgumentError": "if the check can be performed immediately and\nx >= 0 is False. The check can be performed immediately during\neager execution or if x is statically known."}}, "tf.compat.v1.assert_non_positive": {"description": "Assert the condition x &lt;= 0 holds element-wise.", "Args": {"x": "Numeric Tensor.", "data": "The tensors to print out if the condition is False.  Defaults to\nerror message and first few entries of x.", "summarize": "Print this many entries of each tensor.", "message": "A string to prefix to the default message.", "name": "A name for this operation (optional).  Defaults to \"assert_non_positive\"."}, "Returns": "Op that raises InvalidArgumentError if x <= 0 is False.", "Raises": {"InvalidArgumentError": "if the check can be performed immediately and\nx <= 0 is False. The check can be performed immediately during\neager execution or if x is statically known."}}, "tf.compat.v1.assert_none_equal": {"description": "Assert the condition x != y holds element-wise.", "Args": {"x": "Numeric Tensor.", "y": "Numeric Tensor, same dtype as and broadcastable to x.", "data": "The tensors to print out if the condition is False.  Defaults to\nerror message and first few entries of x, y.", "summarize": "Print this many entries of each tensor.", "message": "A string to prefix to the default message.", "name": "A name for this operation (optional).  Defaults to \"assert_none_equal\"."}, "Returns": "Op that raises InvalidArgumentError if x != y is False.", "Raises": {"InvalidArgumentError": "if the check can be performed immediately and\nx != y is False. The check can be performed immediately during\neager execution or if x and y are statically known."}}, "tf.compat.v1.assert_positive": {"description": "Assert the condition x &gt; 0 holds element-wise.", "Args": {"x": "Numeric Tensor.", "data": "The tensors to print out if the condition is False.  Defaults to\nerror message and first few entries of x.", "summarize": "Print this many entries of each tensor.", "message": "A string to prefix to the default message.", "name": "A name for this operation (optional).  Defaults to \"assert_positive\"."}, "Returns": "Op that raises InvalidArgumentError if x > 0 is False.", "Raises": {"InvalidArgumentError": "if the check can be performed immediately and\nx > 0 is False. The check can be performed immediately during\neager execution or if x is statically known."}}, "tf.compat.v1.assert_rank": {"description": "Assert x has rank equal to rank.", "Args": {"x": "Numeric Tensor.", "rank": "Scalar integer Tensor.", "data": "The tensors to print out if the condition is False.  Defaults to\nerror message and the shape of x.", "summarize": "Print this many entries of each tensor.", "message": "A string to prefix to the default message.", "name": "A name for this operation (optional).  Defaults to \"assert_rank\"."}, "Returns": "Op raising InvalidArgumentError unless x has specified rank.\nIf static checks determine x has correct rank, a no_op is returned.", "Raises": {"ValueError": "If static checks determine x has wrong rank."}}, "tf.compat.v1.assert_rank_at_least": {"description": "Assert x has rank equal to rank or higher.", "Args": {"x": "Numeric Tensor.", "rank": "Scalar Tensor.", "data": "The tensors to print out if the condition is False.  Defaults to\nerror message and first few entries of x.", "summarize": "Print this many entries of each tensor.", "message": "A string to prefix to the default message.", "name": "A name for this operation (optional).\nDefaults to \"assert_rank_at_least\"."}, "Returns": "Op raising InvalidArgumentError unless x has specified rank or higher.\nIf static checks determine x has correct rank, a no_op is returned.", "Raises": {"ValueError": "If static checks determine x has wrong rank."}}, "tf.compat.v1.assert_rank_in": {"description": "Assert x has rank in ranks.", "Args": {"x": "Numeric Tensor.", "ranks": "Iterable of scalar Tensor objects.", "data": "The tensors to print out if the condition is False.  Defaults to\nerror message and first few entries of x.", "summarize": "Print this many entries of each tensor.", "message": "A string to prefix to the default message.", "name": "A name for this operation (optional).\nDefaults to \"assert_rank_in\"."}, "Returns": "Op raising InvalidArgumentError unless rank of x is in ranks.\nIf static checks determine x has matching rank, a no_op is returned.", "Raises": {"ValueError": "If static checks determine x has mismatched rank."}}, "tf.compat.v1.assert_scalar": {"description": "Asserts that the given tensor is a scalar (i.e. zero-dimensional).", "Args": {"tensor": "A Tensor.", "name": "A name for this operation. Defaults to \"assert_scalar\"", "message": "A string to prefix to the default message."}, "Returns": "The input tensor (potentially converted to a Tensor).", "Raises": {"ValueError": "If the tensor is not scalar (rank 0), or if its shape is\nunknown."}}, "tf.compat.v1.assert_type": {"description": "Statically asserts that the given Tensor is of the specified type.", "Args": {"tensor": "A Tensor or SparseTensor.", "tf_type": "A tensorflow type (dtypes.float32, tf.int64, dtypes.bool,\netc).", "message": "A string to prefix to the default message.", "name": "A name to give this Op.  Defaults to \"assert_type\""}, "Raises": {"TypeError": "If the tensors data type doesn't match tf_type."}, "Returns": "A no_op that does nothing.  Type can be determined statically."}, "tf.compat.v1.assert_variables_initialized": {"description": "Returns an Op to check if variables are initialized.", "Args": {"var_list": "List of Variable objects to check. Defaults to the value of\nglobal_variables()."}, "Returns": "An Op, or None if there are no variables."}, "tf.compat.v1.assign": {"description": "Update ref by assigning value to it.", "Args": {"ref": "A mutable Tensor. Should be from a Variable node. May be\nuninitialized.", "value": "A Tensor. Must have the same shape and dtype as ref. The value to\nbe assigned to the variable.", "validate_shape": "An optional bool. Defaults to True. If true, the\noperation will validate that the shape of 'value' matches the shape of the\nTensor being assigned to.  If false, 'ref' will take on the shape of\n'value'.", "use_locking": "An optional bool. Defaults to True. If True, the assignment\nwill be protected by a lock; otherwise the behavior is undefined, but may\nexhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor that will hold the new value of ref after\nthe assignment has completed."}, "tf.compat.v1.assign_add": {"description": "Update ref by adding value to it.", "Args": {"ref": "A mutable Tensor. Must be one of the following types: float32,\nfloat64, int64, int32, uint8, uint16, int16, int8,\ncomplex64, complex128, qint8, quint8, qint32, half. Should be\nfrom a Variable node.", "value": "A Tensor. Must have the same shape and dtype as ref. The value to\nbe added to the variable.", "use_locking": "An optional bool. Defaults to False. If True, the addition\nwill be protected by a lock; otherwise the behavior is undefined, but may\nexhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "Same as ref.  Returned as a convenience for operations that want\nto use the new value after the variable has been updated."}, "tf.compat.v1.assign_sub": {"description": "Update ref by subtracting value from it.", "Args": {"ref": "A mutable Tensor. Must be one of the following types: float32,\nfloat64, int64, int32, uint8, uint16, int16, int8,\ncomplex64, complex128, qint8, quint8, qint32, half. Should be\nfrom a Variable node.", "value": "A Tensor. Must have the same shape and dtype as ref. The value to\nbe subtracted to the variable.", "use_locking": "An optional bool. Defaults to False. If True, the\nsubtraction will be protected by a lock; otherwise the behavior is\nundefined, but may exhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "Same as ref.  Returned as a convenience for operations that want\nto use the new value after the variable has been updated."}, "tf.compat.v1.batch_gather": {"description": "Gather slices from params according to indices with leading batch dims. (deprecated)"}, "tf.compat.v1.batch_scatter_update": {"description": "Generalization of tf.compat.v1.scatter_update to axis different than 0. (deprecated)", "Args": {"ref": "Variable to scatter onto.", "indices": "Tensor containing indices as described above.", "updates": "Tensor of updates to apply to ref.", "use_locking": "Boolean indicating whether to lock the writing operation.", "name": "Optional scope name string."}, "Returns": "Ref to variable after it has been modified.", "Raises": {"ValueError": "If the initial ndims of ref, indices, and updates are\nnot the same."}}, "tf.compat.v1.batch_to_space": {"description": "BatchToSpace for 4-D tensors of type T.", "Args": {"input": "A Tensor. 4-D tensor with shape\n[batch*block_size*block_size, height_pad/block_size, width_pad/block_size,\n  depth]. Note that the batch size of the input tensor must be divisible by\nblock_size * block_size.", "crops": "A Tensor. Must be one of the following types: int32, int64.\n2-D tensor of non-negative integers with shape [2, 2]. It specifies\nhow many elements to crop from the intermediate result across the spatial\ndimensions as follows:\ncrops = [[crop_top, crop_bottom], [crop_left, crop_right]]", "block_size": "An int that is >= 2.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.compat.v1.batch_to_space_nd": {"description": "BatchToSpace for N-D tensors of type T.", "Args": {"input": "A Tensor.\nN-D with shape input_shape = [batch] + spatial_shape + remaining_shape,\nwhere spatial_shape has M dimensions.", "block_shape": "A Tensor. Must be one of the following types: int32, int64.\n1-D with shape [M], all values must be >= 1.", "crops": "A Tensor. Must be one of the following types: int32, int64.\n2-D with shape [M, 2], all values must be >= 0.\n  crops[i] = [crop_start, crop_end] specifies the amount to crop from input\n  dimension i + 1, which corresponds to spatial dimension i.  It is\n  required that\n  crop_start[i] + crop_end[i] <= block_shape[i] * input_shape[i + 1].\nThis operation is equivalent to the following steps:\n\nReshape input to reshaped of shape:\n [block_shape[0], ..., block_shape[M-1],\n  batch / prod(block_shape),\n  input_shape[1], ..., input_shape[N-1]]\nPermute dimensions of reshaped to produce permuted of shape\n [batch / prod(block_shape),\ninput_shape[1], block_shape[0],\n  ...,\n  input_shape[M], block_shape[M-1],\ninput_shape[M+1], ..., input_shape[N-1]]\nReshape permuted to produce reshaped_permuted of shape\n [batch / prod(block_shape),\ninput_shape[1] * block_shape[0],\n  ...,\n  input_shape[M] * block_shape[M-1],\ninput_shape[M+1],\n  ...,\n  input_shape[N-1]]\nCrop the start and end of dimensions [1, ..., M] of\nreshaped_permuted according to crops to produce the output of shape:\n [batch / prod(block_shape),\ninput_shape[1] * block_shape[0] - crops[0,0] - crops[0,1],\n  ...,\n  input_shape[M] * block_shape[M-1] - crops[M-1,0] - crops[M-1,1],\ninput_shape[M+1], ..., input_shape[N-1]]\n\nSome examples:\n(1) For the following input of shape [4, 1, 1, 1], block_shape = [2, 2], and\n    crops = [[0, 0], [0, 0]]:\n[[[[1]]], [[[2]]], [[[3]]], [[[4]]]]\nThe output tensor has shape [1, 2, 2, 1] and value:\nx = [[[[1], [2]], [[3], [4]]]]\n(2) For the following input of shape [4, 1, 1, 3], block_shape = [2, 2], and\n    crops = [[0, 0], [0, 0]]:\n[[[[1, 2, 3]]], [[[4, 5, 6]]], [[[7, 8, 9]]], [[[10, 11, 12]]]]\nThe output tensor has shape [1, 2, 2, 3] and value:\nx = [[[[1, 2, 3], [4, 5, 6]],\u00a0 \u00a0 \u00a0 [[7, 8, 9], [10, 11, 12]]]]\n(3) For the following input of shape [4, 2, 2, 1], block_shape = [2, 2], and\n    crops = [[0, 0], [0, 0]]:\nx = [[[[1], [3]], [[9], [11]]],\u00a0 \u00a0 \u00a0[[[2], [4]], [[10], [12]]],\u00a0 \u00a0 \u00a0[[[5], [7]], [[13], [15]]],\u00a0 \u00a0 \u00a0[[[6], [8]], [[14], [16]]]]\nThe output tensor has shape [1, 4, 4, 1] and value:\nx = [[[[1], \u00a0 [2], \u00a0[3], \u00a0[4]],\u00a0 \u00a0 \u00a0[[5], \u00a0 [6], \u00a0[7], \u00a0[8]],\u00a0 \u00a0 \u00a0[[9], \u00a0[10], [11], \u00a0[12]],\u00a0 \u00a0 \u00a0[[13], [14], [15], \u00a0[16]]]]\n(4) For the following input of shape [8, 1, 3, 1], block_shape = [2, 2], and\n    crops = [[0, 0], [2, 0]]:\nx = [[[[0], [1], [3]]], [[[0], [9], [11]]],\u00a0 \u00a0 \u00a0[[[0], [2], [4]]], [[[0], [10], [12]]],\u00a0 \u00a0 \u00a0[[[0], [5], [7]]], [[[0], [13], [15]]],\u00a0 \u00a0 \u00a0[[[0], [6], [8]]], [[[0], [14], [16]]]]\nThe output tensor has shape [2, 2, 4, 1] and value:\nx = [[[[1], \u00a0 [2], \u00a0[3], \u00a0[4]],\u00a0 \u00a0 \u00a0 [[5], \u00a0 [6], \u00a0[7], \u00a0[8]]],\u00a0 \u00a0 \u00a0[[[9], \u00a0[10], [11], \u00a0[12]],\u00a0 \u00a0 \u00a0 [[13], [14], [15], \u00a0[16]]]]", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.compat.v1.bincount": {"description": "Counts the number of occurrences of each value in an integer array.", "Args": {"arr": "An int32 tensor of non-negative values.", "weights": "If non-None, must be the same shape as arr. For each value in\narr, the bin will be incremented by the corresponding weight instead of\n1.", "minlength": "If given, ensures the output has length at least minlength,\npadding with zeros at the end if necessary.", "maxlength": "If given, skips values in arr that are equal or greater than\nmaxlength, ensuring that the output has length at most maxlength.", "dtype": "If weights is None, determines the type of the output bins."}, "Returns": "A vector with the same dtype as weights or the given dtype. The bin\nvalues."}, "tf.compat.v1.boolean_mask": {"description": "Apply boolean mask to tensor.", "Args": {"tensor": "N-D Tensor.", "mask": "K-D boolean Tensor, K <= N and K must be known statically.", "name": "A name for this operation (optional).", "axis": "A 0-D int Tensor representing the axis in tensor to mask from. By\ndefault, axis is 0 which will mask from the first dimension. Otherwise K +\naxis <= N."}, "Returns": "(N-K+1)-dimensional tensor populated by entries in tensor corresponding\nto True values in mask.", "Raises": {"ValueError": "If shapes do not conform."}}, "tf.compat.v1.case": {"description": "Create a case operation.", "Args": {"pred_fn_pairs": "Dict or list of pairs of a boolean scalar tensor and a\ncallable which returns a list of tensors.", "default": "Optional callable that returns a list of tensors.", "exclusive": "True iff at most one predicate is allowed to evaluate to True.", "strict": "A boolean that enables/disables 'strict' mode; see above.", "name": "A name for this operation (optional)."}, "Returns": "The tensors returned by the first pair whose predicate evaluated to True, or\nthose returned by default if none does.", "Raises": {"TypeError": "If fns[i] is not callable for any i, or default is not\ncallable."}}, "tf.compat.v1.clip_by_average_norm": {"description": "Clips tensor values to a maximum average L2-norm. (deprecated)", "Args": {"t": "A Tensor.", "clip_norm": "A 0-D (scalar) Tensor > 0. A maximum clipping value.", "name": "A name for the operation (optional)."}, "Returns": "A clipped Tensor."}, "tf.compat.v1.colocate_with": {"description": "DEPRECATED FUNCTION"}, "tf.compat.v1.cond": {"description": "Return true_fn() if the predicate pred is true else false_fn(). (deprecated arguments)", "Args": {"pred": "A scalar determining whether to return the result of true_fn or\nfalse_fn.", "true_fn": "The callable to be performed if pred is true.", "false_fn": "The callable to be performed if pred is false.", "strict": "A boolean that enables/disables 'strict' mode; see above.", "name": "Optional name prefix for the returned tensors."}, "Returns": "Tensors returned by the call to either true_fn or false_fn. If the\ncallables return a singleton list, the element is extracted from the list.", "Raises": {"TypeError": "if true_fn or false_fn is not callable.", "ValueError": "if true_fn and false_fn do not return the same number of\ntensors, or return tensors of different types."}}, "tf.compat.v1.confusion_matrix": {"description": "Computes the confusion matrix from predictions and labels.", "Args": {"labels": "1-D Tensor of real labels for the classification task.", "predictions": "1-D Tensor of predictions for a given classification.", "num_classes": "The possible number of labels the classification task can have.\nIf this value is not provided, it will be calculated using both\npredictions and labels array.", "dtype": "Data type of the confusion matrix.", "name": "Scope name.", "weights": "An optional Tensor whose shape matches predictions."}, "Returns": "A Tensor of type dtype with shape [n, n] representing the confusion\nmatrix, where n is the number of possible labels in the classification\ntask.", "Raises": {"ValueError": "If both predictions and labels are not 1-D vectors and have\nmismatched shapes, or if weights is not None and its shape doesn't\nmatch predictions."}}, "tf.compat.v1.constant": {"description": "Creates a constant tensor.", "Args": {"value": "A constant value (or list) of output type dtype.", "dtype": "The type of the elements of the resulting tensor.", "shape": "Optional dimensions of resulting tensor.", "name": "Optional name for the tensor.", "verify_shape": "Boolean that enables verification of a shape of values."}, "Returns": "A Constant Tensor.", "Raises": {"TypeError": "if shape is incorrectly specified or unsupported."}}, "tf.compat.v1.container": {"description": "Wrapper for Graph.container() using the default graph.", "Args": {"container_name": "The container string to use in the context."}, "Returns": "A context manager that specifies the default container to use for newly\ncreated stateful ops."}, "tf.compat.v1.control_flow_v2_enabled": {"description": "Returns True if v2 control flow is enabled."}, "tf.compat.v1.convert_to_tensor": {"description": "Converts the given value to a Tensor.", "Args": {"value": "An object whose type has a registered Tensor conversion function.", "dtype": "Optional element type for the returned tensor. If missing, the type\nis inferred from the type of value.", "name": "Optional name to use if a new Tensor is created.", "preferred_dtype": "Optional element type for the returned tensor, used when\ndtype is None. In some cases, a caller may not have a dtype in mind when\nconverting to a tensor, so preferred_dtype can be used as a soft\npreference.  If the conversion to preferred_dtype is not possible, this\nargument has no effect.", "dtype_hint": "same meaning as preferred_dtype, and overrides it."}, "Returns": "A Tensor based on value.", "Raises": {"TypeError": "If no conversion function is registered for value to dtype.", "RuntimeError": "If a registered conversion function returns an invalid value.", "ValueError": "If the value is a tensor not of given dtype in graph mode."}}, "tf.compat.v1.convert_to_tensor_or_indexed_slices": {"description": "Converts the given object to a Tensor or an IndexedSlices.", "Args": {"value": "An IndexedSlices, SparseTensor, or an object that can be consumed\nby convert_to_tensor().", "dtype": "(Optional.) The required DType of the returned Tensor or\nIndexedSlices.", "name": "(Optional.) A name to use if a new Tensor is created."}, "Returns": "A Tensor, IndexedSlices, or SparseTensor based on value.", "Raises": {"ValueError": "If dtype does not match the element type of value."}}, "tf.compat.v1.convert_to_tensor_or_sparse_tensor": {"description": "Converts value to a SparseTensor or Tensor.", "Args": {"value": "A SparseTensor, SparseTensorValue, or an object whose type has a\nregistered Tensor conversion function.", "dtype": "Optional element type for the returned tensor. If missing, the type\nis inferred from the type of value.", "name": "Optional name to use if a new Tensor is created."}, "Returns": "A SparseTensor or Tensor based on value.", "Raises": {"RuntimeError": "If result type is incompatible with dtype."}}, "tf.compat.v1.count_nonzero": {"description": "Computes number of nonzero elements across dimensions of a tensor. (deprecated arguments) (deprecated arguments)", "Args": {"input_tensor": "The tensor to reduce. Should be of numeric type, bool, or\nstring.", "axis": "The dimensions to reduce. If None (the default), reduces all\ndimensions. Must be in the range [-rank(input_tensor),\nrank(input_tensor)).", "keepdims": "If true, retains reduced dimensions with length 1.", "dtype": "The output dtype; defaults to tf.int64.", "name": "A name for the operation (optional).", "reduction_indices": "The old (deprecated) name for axis.", "keep_dims": "Deprecated alias for keepdims.", "input": "Overrides input_tensor. For compatibility."}, "Returns": "The reduced tensor (number of nonzero values)."}, "tf.compat.v1.count_up_to": {"description": "Increments &#39;ref&#39; until it reaches &#39;limit&#39;. (deprecated)", "Args": {"ref": "A Variable. Must be one of the following types: int32, int64.\nShould be from a scalar Variable node.", "limit": "An int.\nIf incrementing ref would bring it above limit, instead generates an\n'OutOfRange' error.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as ref.\nA copy of the input before increment. If nothing else modifies the\ninput, the values produced will all be distinct."}, "tf.compat.v1.create_partitioned_variables": {"description": "Create a list of partitioned variables according to the given slicing. (deprecated)", "Args": {"shape": "List of integers.  The shape of the full variable.", "slicing": "List of integers.  How to partition the variable.\nMust be of the same length as shape.  Each value\nindicate how many slices to create in the corresponding\ndimension.  Presently only one of the values can be more than 1;\nthat is, the variable can only be sliced along one dimension.\nFor convenience, The requested number of partitions does not have to\ndivide the corresponding dimension evenly.  If it does not, the\nshapes of the partitions are incremented by 1 starting from partition\n0 until all slack is absorbed.  The adjustment rules may change in the\nfuture, but as you can save/restore these variables with different\nslicing specifications this should not be a problem.", "initializer": "A Tensor of shape shape or a variable initializer\nfunction.  If a function, it will be called once for each slice,\npassing the shape and data type of the slice as parameters.  The\nfunction must return a tensor with the same shape as the slice.", "dtype": "Type of the variables. Ignored if initializer is a Tensor.", "trainable": "If True also add all the variables to the graph collection\nGraphKeys.TRAINABLE_VARIABLES.", "collections": "List of graph collections keys to add the variables to.\nDefaults to [GraphKeys.GLOBAL_VARIABLES].", "name": "Optional name for the full variable.  Defaults to\n\"PartitionedVariable\" and gets uniquified automatically.", "reuse": "Boolean or None; if True and name is set, it would reuse\npreviously created variables. if False it will create new variables.\nif None, it would inherit the parent scope reuse."}, "Returns": "A list of Variables corresponding to the slicing.", "Raises": {"ValueError": "If any of the arguments is malformed."}}, "tf.compat.v1.decode_csv": {"description": "Convert CSV records to tensors. Each column maps to one tensor.", "Args": {"records": "A Tensor of type string.\nEach string is a record/row in the csv and all records should have\nthe same format.", "record_defaults": "A list of Tensor objects with specific types.\nAcceptable types are float32, float64, int32, int64, string.\nOne tensor per column of the input record, with either a\nscalar default value for that column or an empty vector if the column is\nrequired.", "field_delim": "An optional string. Defaults to \",\".\nchar delimiter to separate fields in a record.", "use_quote_delim": "An optional bool. Defaults to True.\nIf false, treats double quotation marks as regular\ncharacters inside of the string fields (ignoring RFC 4180, Section 2,\nBullet 5).", "name": "A name for the operation (optional).", "na_value": "Additional string to recognize as NA/NaN.", "select_cols": "Optional sorted list of column indices to select. If specified,\nonly this subset of columns will be parsed and returned."}, "Returns": "A list of Tensor objects. Has the same type as record_defaults.\nEach tensor will have the same shape as records.", "Raises": {"ValueError": "If any of the arguments is malformed."}}, "tf.compat.v1.decode_raw": {"description": "Convert raw byte strings into tensors. (deprecated arguments)", "Args": {"input_bytes": "Each element of the input Tensor is converted to an array of bytes.", "out_type": "DType of the output. Acceptable types are half, float, double,\nint32, uint16, uint8, int16, int8, int64.", "little_endian": "Whether the input_bytes data is in little-endian format. Data will be\nconverted into host byte order if necessary.", "name": "A name for the operation (optional).", "bytes": "Deprecated parameter. Use input_bytes instead."}, "Returns": "A Tensor object storing the decoded bytes."}, "tf.compat.v1.delete_session_tensor": {"description": "Delete the tensor for the given tensor handle.", "Args": {"handle": "The string representation of a persistent tensor handle.", "name": "Optional name prefix for the return tensor."}, "Returns": "A pair of graph elements. The first is a placeholder for feeding a\ntensor handle and the second is a deletion operation."}, "tf.compat.v1.depth_to_space": {"description": "DepthToSpace for tensors of type T.", "Args": {"input": "A Tensor.", "block_size": "An int that is >= 2.\nThe size of the spatial block, same as in Space2Depth.", "data_format": "An optional string from: \"NHWC\", \"NCHW\", \"NCHW_VECT_C\". Defaults to \"NHWC\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.compat.v1.device": {"description": "Wrapper for Graph.device() using the default graph.", "Args": {"device_name_or_function": "The device name or function to use in the context."}, "Returns": "A context manager that specifies the default device to use for newly\ncreated ops.", "Raises": {"RuntimeError": "If eager execution is enabled and a function is passed in."}}, "tf.compat.v1.disable_control_flow_v2": {"description": "Opts out of control flow v2."}, "tf.compat.v1.disable_eager_execution": {"description": "Disables eager execution."}, "tf.compat.v1.disable_resource_variables": {"description": "Opts out of resource variables. (deprecated)"}, "tf.compat.v1.disable_tensor_equality": {"description": "Compare Tensors by their id and be hashable."}, "tf.compat.v1.disable_v2_behavior": {"description": "Disables TensorFlow 2.x behaviors."}, "tf.compat.v1.disable_v2_tensorshape": {"description": "Disables the V2 TensorShape behavior and reverts to V1 behavior."}, "tf.compat.v1.div": {"description": "Divides x / y elementwise (using Python 2 division operator semantics). (deprecated)", "Args": {"x": "Tensor numerator of real numeric type.", "y": "Tensor denominator of real numeric type.", "name": "A name for the operation (optional)."}, "Returns": "x / y returns the quotient of x and y."}, "tf.compat.v1.enable_control_flow_v2": {"description": "Use control flow v2."}, "tf.compat.v1.enable_eager_execution": {"description": "Enables eager execution for the lifetime of this program.", "Args": {"config": "(Optional.) A tf.compat.v1.ConfigProto to use to configure the\nenvironment in which operations are executed. Note that\ntf.compat.v1.ConfigProto is also used to configure graph execution (via\ntf.compat.v1.Session) and many options within tf.compat.v1.ConfigProto\nare not implemented (or are irrelevant) when eager execution is enabled.", "device_policy": "(Optional.) Policy controlling how operations requiring\ninputs on a specific device (e.g., a GPU 0) handle inputs on a different\ndevice  (e.g. GPU 1 or CPU). When set to None, an appropriate value will\nbe picked automatically. The value picked may change between TensorFlow\nreleases.\nValid values:\n\ntf.contrib.eager.DEVICE_PLACEMENT_EXPLICIT: raises an error if the\nplacement is not correct.\ntf.contrib.eager.DEVICE_PLACEMENT_WARN: copies the tensors which are not\non the right device but logs a warning.\ntf.contrib.eager.DEVICE_PLACEMENT_SILENT: silently copies the tensors.\nNote that this may hide performance problems as there is no notification\nprovided when operations are blocked on the tensor being copied between\ndevices.\ntf.contrib.eager.DEVICE_PLACEMENT_SILENT_FOR_INT32: silently copies\nint32 tensors, raising errors on the other ones.", "execution_mode": "(Optional.) Policy controlling how operations dispatched are\nactually executed. When set to None, an appropriate value will be picked\nautomatically. The value picked may change between TensorFlow releases.\nValid values:\ntf.contrib.eager.SYNC: executes each operation synchronously.\ntf.contrib.eager.ASYNC: executes each operation asynchronously. These\noperations may return \"non-ready\" handles."}, "Raises": {"ValueError": "If eager execution is enabled after creating/executing a\nTensorFlow graph, or if options provided conflict with a previous call\nto this function."}}, "tf.compat.v1.enable_resource_variables": {"description": "Creates resource variables by default."}, "tf.compat.v1.enable_tensor_equality": {"description": "Compare Tensors with element-wise comparison and thus be unhashable."}, "tf.compat.v1.enable_v2_behavior": {"description": "Enables TensorFlow 2.x behaviors."}, "tf.compat.v1.enable_v2_tensorshape": {"description": "In TensorFlow 2.0, iterating over a TensorShape instance returns values."}, "tf.compat.v1.executing_eagerly": {"description": "Checks whether the current thread has eager execution enabled.", "Returns": "True if the current thread has eager execution enabled."}, "tf.compat.v1.executing_eagerly_outside_functions": {"description": "Returns True if executing eagerly, even if inside a graph function.", "Returns": "boolean, whether the outermost context is in eager mode."}, "tf.compat.v1.expand_dims": {"description": "Returns a tensor with a length 1 axis inserted at index axis. (deprecated arguments)", "Args": {"input": "A Tensor.", "axis": "0-D (scalar). Specifies the dimension index at which to expand the\nshape of input. Must be in the range [-rank(input) - 1, rank(input)].", "name": "The name of the output Tensor (optional).", "dim": "0-D (scalar). Equivalent to axis, to be deprecated."}, "Returns": "A Tensor with the same data as input, but its shape has an additional\ndimension of size 1 added.", "Raises": {"ValueError": "if either both or neither of dim and axis are specified."}}, "tf.compat.v1.extract_image_patches": {"description": "Extract patches from images and put them in the &#34;depth&#34; output dimension.", "Args": {"images": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, int8, int16, int32, int64, uint8, uint16, uint32, uint64, complex64, complex128, bool.\n4-D Tensor with shape [batch, in_rows, in_cols, depth].", "ksizes": "A list of ints that has length >= 4.\nThe size of the sliding window for each dimension of images.", "strides": "A list of ints that has length >= 4.\nHow far the centers of two consecutive patches are in\nthe images. Must be: [1, stride_rows, stride_cols, 1].", "rates": "A list of ints that has length >= 4.\nMust be: [1, rate_rows, rate_cols, 1]. This is the\ninput stride, specifying how far two consecutive patch samples are in the\ninput. Equivalent to extracting patches with\npatch_sizes_eff = patch_sizes + (patch_sizes - 1) * (rates - 1), followed by\nsubsampling them spatially by a factor of rates. This is equivalent to\nrate in dilated (a.k.a. Atrous) convolutions.", "padding": "A string from: \"SAME\", \"VALID\".\nThe type of padding algorithm to use.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as images."}, "tf.compat.v1.fixed_size_partitioner": {"description": "Partitioner to specify a fixed number of shards along given axis.", "Args": {"num_shards": "int, number of shards to partition variable.", "axis": "int, axis to partition on."}, "Returns": "A partition function usable as the partitioner argument to\nvariable_scope and get_variable."}, "tf.compat.v1.floor_div": {"description": "Returns x // y element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, uint8, int8, uint16, int16, int32, uint32, uint64, int64, complex64, complex128.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.compat.v1.foldl": {"description": "foldl on the list of tensors unpacked from elems on dimension 0.", "Args": {"fn": "The callable to be performed.", "elems": "A tensor or (possibly nested) sequence of tensors, each of which will\nbe unpacked along their first dimension.  The nested sequence of the\nresulting slices will be the first argument to fn.", "initializer": "(optional) A tensor or (possibly nested) sequence of tensors,\nas the initial value for the accumulator.", "parallel_iterations": "(optional) The number of iterations allowed to run in\nparallel.", "back_prop": "(optional) True enables support for back propagation.", "swap_memory": "(optional) True enables GPU-CPU memory swapping.", "name": "(optional) Name prefix for the returned tensors."}, "Returns": "A tensor or (possibly nested) sequence of tensors, resulting from applying\nfn consecutively to the list of tensors unpacked from elems, from first\nto last.", "Raises": {"TypeError": "if fn is not callable."}}, "tf.compat.v1.foldr": {"description": "foldr on the list of tensors unpacked from elems on dimension 0.", "Args": {"fn": "The callable to be performed.", "elems": "A tensor or (possibly nested) sequence of tensors, each of which will\nbe unpacked along their first dimension.  The nested sequence of the\nresulting slices will be the first argument to fn.", "initializer": "(optional) A tensor or (possibly nested) sequence of tensors,\nas the initial value for the accumulator.", "parallel_iterations": "(optional) The number of iterations allowed to run in\nparallel.", "back_prop": "(optional) True enables support for back propagation.", "swap_memory": "(optional) True enables GPU-CPU memory swapping.", "name": "(optional) Name prefix for the returned tensors."}, "Returns": "A tensor or (possibly nested) sequence of tensors, resulting from applying\nfn consecutively to the list of tensors unpacked from elems, from last\nto first.", "Raises": {"TypeError": "if fn is not callable."}}, "tf.compat.v1.gather": {"description": "Gather slices from params axis axis according to indices. (deprecated arguments)", "Args": {"params": "The Tensor from which to gather values. Must be at least rank\naxis + 1.", "indices": "The index Tensor.  Must be one of the following types: int32,\nint64. The values must be in range [0, params.shape[axis]).", "validate_indices": "Deprecated, does nothing. Indices are always validated on\nCPU, never validated on GPU.\nCaution: On CPU, if an out of bound index is found, an error is raised.\nOn GPU, if an out of bound index is found, a 0 is stored in the\ncorresponding output value.", "axis": "A Tensor. Must be one of the following types: int32, int64. The\naxis in params to gather indices from. Must be greater than or equal\nto batch_dims.  Defaults to the first non-batch dimension. Supports\nnegative indexes.", "batch_dims": "An integer.  The number of batch dimensions.  Must be less\nthan or equal to rank(indices).", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as params."}, "tf.compat.v1.gather_nd": {"description": "Gather slices from params into a Tensor with shape specified by indices.", "Args": {"params": "A Tensor. The tensor from which to gather values.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nIndex tensor.", "name": "A name for the operation (optional).", "batch_dims": "An integer or a scalar 'Tensor'. The number of batch dimensions."}, "Returns": "A Tensor. Has the same type as params."}, "tf.compat.v1.get_collection": {"description": "Wrapper for Graph.get_collection() using the default graph.", "Args": {"key": "The key for the collection. For example, the GraphKeys class contains\nmany standard names for collections.", "scope": "(Optional.) If supplied, the resulting list is filtered to include\nonly items whose name attribute matches using re.match. Items without\na name attribute are never returned if a scope is supplied and the\nchoice or re.match means that a scope without special tokens filters\nby prefix."}, "Returns": "The list of values in the collection with the given name, or\nan empty list if no value has been added to that collection. The\nlist contains the values in the order under which they were\ncollected."}, "tf.compat.v1.get_collection_ref": {"description": "Wrapper for Graph.get_collection_ref() using the default graph.", "Args": {"key": "The key for the collection. For example, the GraphKeys class contains\nmany standard names for collections."}, "Returns": "The list of values in the collection with the given name, or an empty\nlist if no value has been added to that collection.  Note that this returns\nthe collection list itself, which can be modified in place to change the\ncollection."}, "tf.compat.v1.get_default_graph": {"description": "Returns the default graph for the current thread.", "Returns": "The default Graph being used in the current thread."}, "tf.compat.v1.get_default_session": {"description": "Returns the default session for the current thread.", "Returns": "The default Session being used in the current thread."}, "tf.compat.v1.get_local_variable": {"description": "Gets an existing *local* variable or creates a new one.", "Args": {"name": "The name of the new or existing variable.", "shape": "Shape of the new or existing variable.", "dtype": "Type of the new or existing variable (defaults to DT_FLOAT).", "initializer": "Initializer for the variable if one is created. Can either be\nan initializer object or a Tensor. If it's a Tensor, its shape must be known\nunless validate_shape is False.", "regularizer": "A (Tensor -> Tensor or None) function; the result of\napplying it on a newly created variable will be added to the collection\ntf.GraphKeys.REGULARIZATION_LOSSES and can be used for regularization.", "collections": "List of graph collections keys to add the Variable to.\nDefaults to [GraphKeys.LOCAL_VARIABLES] (see tf.Variable).", "caching_device": "Optional device string or function describing where the\nVariable should be cached for reading.  Defaults to the Variable's\ndevice.  If not None, caches on another device.  Typical use is to\ncache on the device where the Ops using the Variable reside, to\ndeduplicate copying through Switch and other conditional statements.", "partitioner": "Optional callable that accepts a fully defined TensorShape\nand dtype of the Variable to be created, and returns a list of\npartitions for each axis (currently only one axis can be partitioned).", "validate_shape": "If False, allows the variable to be initialized with a\nvalue of unknown shape. If True, the default, the shape of initial_value\nmust be known. For this to be used the initializer must be a Tensor and\nnot an initializer object.", "use_resource": "If False, creates a regular Variable. If true, creates an\nexperimental ResourceVariable instead with well-defined semantics.\nDefaults to False (will later change to True). When eager execution is\nenabled this argument is always forced to be True.", "custom_getter": "Callable that takes as a first argument the true getter, and\nallows overwriting the internal get_variable method.\nThe signature of custom_getter should match that of this method,\nbut the most future-proof version will allow for changes:\ndef custom_getter(getter, *args, **kwargs).  Direct access to\nall get_variable parameters is also allowed:\ndef custom_getter(getter, name, *args, **kwargs).  A simple identity\ncustom getter that simply creates variables with modified names is:\ndef custom_getter(getter, name, *args, **kwargs):\u00a0 return getter(name + '_suffix', *args, **kwargs)", "constraint": "An optional projection function to be applied to the variable\nafter being updated by an Optimizer (e.g. used to implement norm\nconstraints or value constraints for layer weights). The function must\ntake as input the unprojected Tensor representing the value of the\nvariable and return the Tensor for the projected value\n(which must have the same shape). Constraints are not safe to\nuse when doing asynchronous distributed training.", "synchronization": "Indicates when a distributed a variable will be\naggregated. Accepted values are constants defined in the class\ntf.VariableSynchronization. By default the synchronization is set to\nAUTO and the current DistributionStrategy chooses\nwhen to synchronize.", "aggregation": "Indicates how a distributed variable will be aggregated.\nAccepted values are constants defined in the class\ntf.VariableAggregation."}, "Returns": "The created or existing Variable (or PartitionedVariable, if a\npartitioner was used).", "Raises": {"ValueError": "when creating a new variable and shape is not declared,\nwhen violating reuse during variable creation, or when initializer dtype\nand dtype don't match. Reuse is set inside variable_scope."}}, "tf.compat.v1.get_seed": {"description": "Returns the local seeds an operation should use given an op-specific seed.", "Args": {"op_seed": "integer."}, "Returns": "A tuple of two integers that should be used for the local seed of this\noperation."}, "tf.compat.v1.get_session_handle": {"description": "Return the handle of data.", "Args": {"data": "A tensor to be stored in the session.", "name": "Optional name prefix for the return tensor."}, "Returns": "A scalar string tensor representing a unique handle for data.", "Raises": {"TypeError": "if data is not a Tensor."}}, "tf.compat.v1.get_session_tensor": {"description": "Get the tensor of type dtype by feeding a tensor handle.", "Args": {"handle": "The string representation of a persistent tensor handle.", "dtype": "The type of the output tensor.", "name": "Optional name prefix for the return tensor."}, "Returns": "A pair of tensors. The first is a placeholder for feeding a\ntensor handle and the second is the tensor in the session state\nkeyed by the tensor handle."}, "tf.compat.v1.get_variable": {"description": "Gets an existing variable with these parameters or create a new one.", "Args": {"name": "The name of the new or existing variable.", "shape": "Shape of the new or existing variable.", "dtype": "Type of the new or existing variable (defaults to DT_FLOAT).", "initializer": "Initializer for the variable if one is created. Can either be\nan initializer object or a Tensor. If it's a Tensor, its shape must be known\nunless validate_shape is False.", "regularizer": "A (Tensor -> Tensor or None) function; the result of\napplying it on a newly created variable will be added to the collection\ntf.GraphKeys.REGULARIZATION_LOSSES and can be used for regularization.", "trainable": "If True also add the variable to the graph collection\nGraphKeys.TRAINABLE_VARIABLES (see tf.Variable).", "collections": "List of graph collections keys to add the Variable to.\nDefaults to [GraphKeys.GLOBAL_VARIABLES] (see tf.Variable).", "caching_device": "Optional device string or function describing where the\nVariable should be cached for reading.  Defaults to the Variable's\ndevice.  If not None, caches on another device.  Typical use is to\ncache on the device where the Ops using the Variable reside, to\ndeduplicate copying through Switch and other conditional statements.", "partitioner": "Optional callable that accepts a fully defined TensorShape\nand dtype of the Variable to be created, and returns a list of\npartitions for each axis (currently only one axis can be partitioned).", "validate_shape": "If False, allows the variable to be initialized with a\nvalue of unknown shape. If True, the default, the shape of initial_value\nmust be known. For this to be used the initializer must be a Tensor and\nnot an initializer object.", "use_resource": "If False, creates a regular Variable. If true, creates an\nexperimental ResourceVariable instead with well-defined semantics.\nDefaults to False (will later change to True). When eager execution is\nenabled this argument is always forced to be True.", "custom_getter": "Callable that takes as a first argument the true getter, and\nallows overwriting the internal get_variable method.\nThe signature of custom_getter should match that of this method,\nbut the most future-proof version will allow for changes:\ndef custom_getter(getter, *args, **kwargs).  Direct access to\nall get_variable parameters is also allowed:\ndef custom_getter(getter, name, *args, **kwargs).  A simple identity\ncustom getter that simply creates variables with modified names is:\ndef custom_getter(getter, name, *args, **kwargs):\u00a0 return getter(name + '_suffix', *args, **kwargs)", "constraint": "An optional projection function to be applied to the variable\nafter being updated by an Optimizer (e.g. used to implement norm\nconstraints or value constraints for layer weights). The function must\ntake as input the unprojected Tensor representing the value of the\nvariable and return the Tensor for the projected value\n(which must have the same shape). Constraints are not safe to\nuse when doing asynchronous distributed training.", "synchronization": "Indicates when a distributed a variable will be\naggregated. Accepted values are constants defined in the class\ntf.VariableSynchronization. By default the synchronization is set to\nAUTO and the current DistributionStrategy chooses\nwhen to synchronize.", "aggregation": "Indicates how a distributed variable will be aggregated.\nAccepted values are constants defined in the class\ntf.VariableAggregation."}, "Returns": "The created or existing Variable (or PartitionedVariable, if a\npartitioner was used).", "Raises": {"ValueError": "when creating a new variable and shape is not declared,\nwhen violating reuse during variable creation, or when initializer dtype\nand dtype don't match. Reuse is set inside variable_scope."}}, "tf.compat.v1.get_variable_scope": {"description": "Returns the current variable scope."}, "tf.compat.v1.global_variables": {"description": "Returns global variables.", "Args": {"scope": "(Optional.) A string. If supplied, the resulting list is filtered to\ninclude only items whose name attribute matches scope using\nre.match. Items without a name attribute are never returned if a scope\nis supplied. The choice of re.match means that a scope without special\ntokens filters by prefix."}, "Returns": "A list of Variable objects."}, "tf.compat.v1.global_variables_initializer": {"description": "Returns an Op that initializes global variables.", "Returns": "An Op that initializes global variables in the graph."}, "tf.compat.v1.gradients": {"description": "Constructs symbolic derivatives of sum of ys w.r.t. x in xs.", "Args": {"ys": "A Tensor or list of tensors to be differentiated.", "xs": "A Tensor or list of tensors to be used for differentiation.", "grad_ys": "Optional. A Tensor or list of tensors the same size as\nys and holding the gradients computed for each y in ys.", "name": "Optional name to use for grouping all the gradient ops together.\ndefaults to 'gradients'.", "colocate_gradients_with_ops": "If True, try colocating gradients with\nthe corresponding op.", "gate_gradients": "If True, add a tuple around the gradients returned\nfor an operations.  This avoids some race conditions.", "aggregation_method": "Specifies the method used to combine gradient terms.\nAccepted values are constants defined in the class AggregationMethod.", "stop_gradients": "Optional. A Tensor or list of tensors not to differentiate\nthrough.", "unconnected_gradients": "Optional. Specifies the gradient value returned when\nthe given input tensors are unconnected. Accepted values are constants\ndefined in the class tf.UnconnectedGradients and the default value is\nnone."}, "Returns": "A list of Tensor of length len(xs) where each tensor is the sum(dy/dx)\nfor y in ys and for x in xs.", "Raises": {"LookupError": "if one of the operations between x and y does not\nhave a registered gradient function.", "ValueError": "if the arguments are invalid.", "RuntimeError": "if called in Eager mode."}}, "tf.compat.v1.hessians": {"description": "Constructs the Hessian of sum of ys with respect to x in xs.", "Args": {"ys": "A Tensor or list of tensors to be differentiated.", "xs": "A Tensor or list of tensors to be used for differentiation.", "name": "Optional name to use for grouping all the gradient ops together.\ndefaults to 'hessians'.", "colocate_gradients_with_ops": "See gradients() documentation for details.", "gate_gradients": "See gradients() documentation for details.", "aggregation_method": "See gradients() documentation for details."}, "Returns": "A list of Hessian matrices of sum(ys) for each x in xs.", "Raises": {"LookupError": "if one of the operations between xs and ys does not\nhave a registered gradient function."}}, "tf.compat.v1.initialize_all_tables": {"description": "Returns an Op that initializes all tables of the default graph. (deprecated)", "Args": {"name": "Optional name for the initialization op."}, "Returns": "An Op that initializes all tables.  Note that if there are\nnot tables the returned Op is a NoOp."}, "tf.compat.v1.initialize_all_variables": {"description": "See tf.compat.v1.global_variables_initializer. (deprecated)"}, "tf.compat.v1.initialize_local_variables": {"description": "See tf.compat.v1.local_variables_initializer. (deprecated)"}, "tf.compat.v1.initialize_variables": {"description": "See tf.compat.v1.variables_initializer. (deprecated)"}, "tf.compat.v1.is_variable_initialized": {"description": "Tests if a variable has been initialized.", "Args": {"variable": "A Variable."}, "Returns": "Returns a scalar boolean Tensor, True if the variable has been\ninitialized, False otherwise."}, "tf.compat.v1.load_file_system_library": {"description": "Loads a TensorFlow plugin, containing file system implementation. (deprecated)", "Args": {"library_filename": "Path to the plugin.\nRelative or absolute filesystem path to a dynamic library file."}, "Returns": "None.", "Raises": {"RuntimeError": "when unable to load the library."}}, "tf.compat.v1.local_variables": {"description": "Returns local variables.", "Args": {"scope": "(Optional.) A string. If supplied, the resulting list is filtered to\ninclude only items whose name attribute matches scope using\nre.match. Items without a name attribute are never returned if a scope\nis supplied. The choice of re.match means that a scope without special\ntokens filters by prefix."}, "Returns": "A list of local Variable objects."}, "tf.compat.v1.local_variables_initializer": {"description": "Returns an Op that initializes all local variables.", "Returns": "An Op that initializes all local variables in the graph."}, "tf.compat.v1.make_template": {"description": "Given an arbitrary function, wrap it so that it does variable sharing.", "Args": {"name_": "A name for the scope created by this template. If necessary, the name\nwill be made unique by appending _N to the name.", "func_": "The function to wrap.", "create_scope_now_": "Boolean controlling whether the scope should be created\nwhen the template is constructed or when the template is called. Default\nis False, meaning the scope is created when the template is called.", "unique_name_": "When used, it overrides name_ and is not made unique. If a\ntemplate of the same scope/unique_name already exists and reuse is false,\nan error is raised. Defaults to None.", "custom_getter_": "Optional custom getter for variables used in func_. See\nthe tf.compat.v1.get_variable custom_getter documentation for more\ninformation.", "**kwargs": "Keyword arguments to apply to func_."}, "Returns": "A function to encapsulate a set of variables which should be created once\nand reused. An enclosing scope will be created either when make_template\nis called or when the result is called, depending on the value of\ncreate_scope_now_. Regardless of the value, the first time the template\nis called it will enter the scope with no reuse, and call func_ to create\nvariables, which are guaranteed to be unique. All subsequent calls will\nre-enter the scope and reuse those variables.", "Raises": {"ValueError": "if name_ is None."}}, "tf.compat.v1.map_fn": {"description": "Transforms elems by applying fn to each element unstacked on axis 0. (deprecated arguments)", "Args": {"fn": "The callable to be performed.  It accepts one argument, which will have\nthe same (possibly nested) structure as elems.  Its output must have the\nsame structure as fn_output_signature if one is provided; otherwise it\nmust have the same structure as elems.", "elems": "A tensor or (possibly nested) sequence of tensors, each of which will\nbe unstacked along their first dimension.  fn will be applied to the\nnested sequence of the resulting slices.  elems may include ragged and\nsparse tensors. elems must consist of at least one tensor.", "dtype": "Deprecated: Equivalent to fn_output_signature.", "parallel_iterations": "(optional) The number of iterations allowed to run in\nparallel. When graph building, the default value is 10. While executing\neagerly, the default value is set to 1.", "back_prop": "(optional) False disables support for back propagation.", "swap_memory": "(optional) True enables GPU-CPU memory swapping.", "infer_shape": "(optional) False disables tests for consistent output shapes.", "name": "(optional) Name prefix for the returned tensors.", "fn_output_signature": "The output signature of fn. Must be specified if\nfn's input and output signatures are different (i.e., if their\nstructures, dtypes, or tensor types do not match).\nfn_output_signature can be specified using any of the following:\n\nA tf.DType or tf.TensorSpec (to describe a tf.Tensor)\nA tf.RaggedTensorSpec (to describe a tf.RaggedTensor)\nA tf.SparseTensorSpec (to describe a tf.sparse.SparseTensor)\nA (possibly nested) tuple, list, or dict containing the above types."}, "Returns": "A tensor or (possibly nested) sequence of tensors.  Each tensor stacks the\nresults of applying fn to tensors unstacked from elems along the first\ndimension, from first to last.  The result may include ragged and sparse\ntensors.", "Raises": {"TypeError": "if fn is not callable or the structure of the output of\nfn and fn_output_signature do not match.", "ValueError": "if the lengths of the output of fn and fn_output_signature\ndo not match, or if the elems does not contain any tensor."}}, "tf.compat.v1.min_max_variable_partitioner": {"description": "Partitioner to allocate minimum size per slice.", "Args": {"max_partitions": "Upper bound on the number of partitions. Defaults to 1.", "axis": "Axis along which to partition the variable. Defaults to 0.", "min_slice_size": "Minimum size of the variable slice per partition. Defaults\nto 256K.", "bytes_per_string_element": "If the Variable is of type string, this provides\nan estimate of how large each scalar in the Variable is."}, "Returns": "A partition function usable as the partitioner argument to\nvariable_scope and get_variable."}, "tf.compat.v1.model_variables": {"description": "Returns all variables in the MODEL_VARIABLES collection.", "Args": {"scope": "(Optional.) A string. If supplied, the resulting list is filtered to\ninclude only items whose name attribute matches scope using\nre.match. Items without a name attribute are never returned if a scope\nis supplied. The choice of re.match means that a scope without special\ntokens filters by prefix."}, "Returns": "A list of local Variable objects."}, "tf.compat.v1.moving_average_variables": {"description": "Returns all variables that maintain their moving averages.", "Args": {"scope": "(Optional.) A string. If supplied, the resulting list is filtered to\ninclude only items whose name attribute matches scope using\nre.match. Items without a name attribute are never returned if a scope\nis supplied. The choice of re.match means that a scope without special\ntokens filters by prefix."}, "Returns": "A list of Variable objects."}, "tf.compat.v1.multinomial": {"description": "Draws samples from a multinomial distribution. (deprecated)", "Args": {"logits": "2-D Tensor with shape [batch_size, num_classes].  Each slice\n[i, :] represents the unnormalized log-probabilities for all classes.", "num_samples": "0-D.  Number of independent samples to draw for each row slice.", "seed": "A Python integer. Used to create a random seed for the distribution.\nSee tf.random.set_seed for behavior.", "name": "Optional name for the operation.", "output_dtype": "The integer type of the output: int32 or int64. Defaults\nto int64."}, "Returns": "The drawn samples of shape [batch_size, num_samples]."}, "tf.compat.v1.no_regularizer": {"description": "Use this function to prevent regularization of variables."}, "tf.compat.v1.norm": {"description": "Computes the norm of vectors, matrices, and tensors. (deprecated arguments)", "Args": {"tensor": "Tensor of types float32, float64, complex64, complex128", "ord": "Order of the norm. Supported values are 'fro', 'euclidean',\n1, 2, np.inf and any positive real number yielding the corresponding\np-norm. Default is 'euclidean' which is equivalent to Frobenius norm if\ntensor is a matrix and equivalent to 2-norm for vectors.\nSome restrictions apply:\n  a) The Frobenius norm fro is not defined for vectors,\n  b) If axis is a 2-tuple (matrix norm), only 'euclidean', 'fro', 1,\n     2, np.inf are supported.\nSee the description of axis on how to compute norms for a batch of\nvectors or matrices stored in a tensor.", "axis": "If axis is None (the default), the input is considered a vector\nand a single vector norm is computed over the entire set of values in the\ntensor, i.e. norm(tensor, ord=ord) is equivalent to\nnorm(reshape(tensor, [-1]), ord=ord).\nIf axis is a Python integer, the input is considered a batch of vectors,\nand axis determines the axis in tensor over which to compute vector\nnorms.\nIf axis is a 2-tuple of Python integers it is considered a batch of\nmatrices and axis determines the axes in tensor over which to compute\na matrix norm.\nNegative indices are supported. Example: If you are passing a tensor that\ncan be either a matrix or a batch of matrices at runtime, pass\naxis=[-2,-1] instead of axis=None to make sure that matrix norms are\ncomputed.", "keepdims": "If True, the axis indicated in axis are kept with size 1.\nOtherwise, the dimensions in axis are removed from the output shape.", "name": "The name of the op.", "keep_dims": "Deprecated alias for keepdims."}, "Returns": "output\n\n\nA Tensor of the same type as tensor, containing the vector or\nmatrix norms. If keepdims is True then the rank of output is equal to\nthe rank of tensor. Otherwise, if axis is none the output is a scalar,\nif axis is an integer, the rank of output is one less than the rank\nof tensor, if axis is a 2-tuple the rank of output is two less\nthan the rank of tensor.", "Raises": {"ValueError": "If ord or axis is invalid."}}, "tf.compat.v1.ones_like": {"description": "Creates a tensor with all elements set to 1.", "Args": {"tensor": "A Tensor.", "dtype": "A type for the returned Tensor. Must be float32, float64,\nint8, uint8, int16, uint16, int32, int64, complex64,\ncomplex128 or bool.", "name": "A name for the operation (optional).", "optimize": "if true, attempt to statically determine the shape of 'tensor' and\nencode it as a constant."}, "Returns": "A Tensor with all elements set to 1."}, "tf.compat.v1.op_scope": {"description": "DEPRECATED. Same as name_scope above, just different argument order."}, "tf.compat.v1.pad": {"description": "Pads a tensor.", "Args": {"tensor": "A Tensor.", "paddings": "A Tensor of type int32.", "mode": "One of \"CONSTANT\", \"REFLECT\", or \"SYMMETRIC\" (case-insensitive)", "name": "A name for the operation (optional).", "constant_values": "In \"CONSTANT\" mode, the scalar pad value to use. Must be\nsame type as tensor."}, "Returns": "A Tensor. Has the same type as tensor.", "Raises": {"ValueError": "When mode is not one of \"CONSTANT\", \"REFLECT\", or \"SYMMETRIC\"."}}, "tf.compat.v1.parse_example": {"description": "Parses Example protos into a dict of tensors.", "Args": {"serialized": "A vector (1-D Tensor) of strings, a batch of binary\nserialized Example protos.", "features": "A dict mapping feature keys to FixedLenFeature,\nVarLenFeature, SparseFeature, and RaggedFeature values.", "example_names": "A vector (1-D Tensor) of strings (optional), the names of\nthe serialized protos in the batch.", "name": "A name for this operation (optional)."}, "Returns": "A dict mapping feature keys to Tensor, SparseTensor, and\nRaggedTensor values.", "Raises": {"ValueError": "if any feature is invalid."}}, "tf.compat.v1.parse_single_example": {"description": "Parses a single Example proto.", "Args": {"serialized": "A scalar string Tensor, a single serialized Example.", "features": "A dict mapping feature keys to FixedLenFeature or\nVarLenFeature values.", "name": "A name for this operation (optional).", "example_names": "(Optional) A scalar string Tensor, the associated name."}, "Returns": "A dict mapping feature keys to Tensor and SparseTensor values.", "Raises": {"ValueError": "if any feature is invalid."}}, "tf.compat.v1.placeholder": {"description": "Inserts a placeholder for a tensor that will be always fed.", "Args": {"dtype": "The type of elements in the tensor to be fed.", "shape": "The shape of the tensor to be fed (optional). If the shape is not\nspecified, you can feed a tensor of any shape.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor that may be used as a handle for feeding a value, but not\nevaluated directly.", "Raises": {"RuntimeError": "if eager execution is enabled"}}, "tf.compat.v1.placeholder_with_default": {"description": "A placeholder op that passes through input when its output is not fed.", "Args": {"input": "A Tensor. The default value to produce when output is not fed.", "shape": "A tf.TensorShape or list of ints. The (possibly partial) shape of\nthe tensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.compat.v1.py_func": {"description": "Wraps a python function and uses it as a TensorFlow op.", "Args": {"func": "A Python function, which accepts ndarray objects as arguments and\nreturns a list of ndarray objects (or a single ndarray). This function\nmust accept as many arguments as there are tensors in inp, and these\nargument types will match the corresponding tf.Tensor objects in inp.\nThe returns ndarrays must match the number and types defined Tout.\nImportant Note: Input and output numpy ndarrays of func are not\n  guaranteed to be copies. In some cases their underlying memory will be\n  shared with the corresponding TensorFlow tensors. In-place modification\n  or storing func input or return values in python datastructures\n  without explicit (np.)copy can have non-deterministic consequences.", "inp": "A list of Tensor objects.", "Tout": "A list or tuple of tensorflow data types or a single tensorflow data\ntype if there is only one, indicating what func returns.", "stateful": "(Boolean.) If True, the function should be considered stateful. If\na function is stateless, when given the same input it will return the same\noutput and have no observable side effects. Optimizations such as common\nsubexpression elimination are only performed on stateless operations.", "name": "A name for the operation (optional)."}, "Returns": "A list of Tensor or a single Tensor which func computes."}, "tf.compat.v1.quantize_v2": {"description": "Please use tf.quantization.quantize instead."}, "tf.compat.v1.random_normal_initializer": {"description": "Initializer that generates tensors with a normal distribution.", "Args": {"mean": "a python scalar or a scalar tensor. Mean of the random values to\ngenerate.", "stddev": "a python scalar or a scalar tensor. Standard deviation of the random\nvalues to generate.", "seed": "A Python integer. Used to create random seeds. See\ntf.compat.v1.set_random_seed for behavior.", "dtype": "Default data type, used if no dtype argument is provided when\ncalling the initializer. Only floating point types are supported."}}, "tf.compat.v1.random_poisson": {"description": "Draws shape samples from each of the given Poisson distribution(s).", "Args": {"lam": "A Tensor or Python value or N-D array of type dtype.\nlam provides the rate parameter(s) describing the poisson\ndistribution(s) to sample.", "shape": "A 1-D integer Tensor or Python array. The shape of the output samples\nto be drawn per \"rate\"-parameterized distribution.", "dtype": "The type of the output: float16, float32, float64, int32 or\nint64.", "seed": "A Python integer. Used to create a random seed for the distributions.\nSee\ntf.random.set_seed\nfor behavior.", "name": "Optional name for the operation."}, "Returns": "samples\n\n\na Tensor of shape tf.concat([shape, tf.shape(lam)], axis=0)\nwith values of type dtype."}, "tf.compat.v1.random_uniform_initializer": {"description": "Initializer that generates tensors with a uniform distribution.", "Args": {"minval": "A python scalar or a scalar tensor. Lower bound of the range of\nrandom values to generate.", "maxval": "A python scalar or a scalar tensor. Upper bound of the range of\nrandom values to generate.  Defaults to 1 for float types.", "seed": "A Python integer. Used to create random seeds. See\ntf.compat.v1.set_random_seed for behavior.", "dtype": "Default data type, used if no dtype argument is provided when\ncalling the initializer."}}, "tf.compat.v1.reduce_all": {"description": "Computes tf.math.logical_and of elements across dimensions of a tensor. (deprecated arguments)", "Args": {"input_tensor": "The boolean tensor to reduce.", "axis": "The dimensions to reduce. If None (the default), reduces all\ndimensions. Must be in the range [-rank(input_tensor),\nrank(input_tensor)).", "keepdims": "If true, retains reduced dimensions with length 1.", "name": "A name for the operation (optional).", "reduction_indices": "The old (deprecated) name for axis.", "keep_dims": "Deprecated alias for keepdims."}, "Returns": "The reduced tensor."}, "tf.compat.v1.reduce_any": {"description": "Computes tf.math.logical_or of elements across dimensions of a tensor. (deprecated arguments)", "Args": {"input_tensor": "The boolean tensor to reduce.", "axis": "The dimensions to reduce. If None (the default), reduces all\ndimensions. Must be in the range [-rank(input_tensor),\nrank(input_tensor)).", "keepdims": "If true, retains reduced dimensions with length 1.", "name": "A name for the operation (optional).", "reduction_indices": "The old (deprecated) name for axis.", "keep_dims": "Deprecated alias for keepdims."}, "Returns": "The reduced tensor."}, "tf.compat.v1.reduce_join": {"description": "Joins all strings into a single string, or joins along an axis.", "Args": {"inputs": "A tf.string tensor.", "axis": "Which axis to join along. The default behavior is to join all\nelements, producing a scalar.", "keepdims": "If true, retains reduced dimensions with length 1.", "separator": "a string added between each string being joined.", "name": "A name for the operation (optional)."}, "Returns": "A tf.string tensor."}, "tf.compat.v1.reduce_logsumexp": {"description": "Computes log(sum(exp(elements across dimensions of a tensor))). (deprecated arguments)", "Args": {"input_tensor": "The tensor to reduce. Should have numeric type.", "axis": "The dimensions to reduce. If None (the default), reduces all\ndimensions. Must be in the range [-rank(input_tensor),\nrank(input_tensor)).", "keepdims": "If true, retains reduced dimensions with length 1.", "name": "A name for the operation (optional).", "reduction_indices": "The old (deprecated) name for axis.", "keep_dims": "Deprecated alias for keepdims."}, "Returns": "The reduced tensor."}, "tf.compat.v1.reduce_max": {"description": "Computes tf.math.maximum of elements across dimensions of a tensor. (deprecated arguments)", "Args": {"input_tensor": "The tensor to reduce. Should have real numeric type.", "axis": "The dimensions to reduce. If None (the default), reduces all\ndimensions. Must be in the range [-rank(input_tensor),\nrank(input_tensor)).", "keepdims": "If true, retains reduced dimensions with length 1.", "name": "A name for the operation (optional).", "reduction_indices": "The old (deprecated) name for axis.", "keep_dims": "Deprecated alias for keepdims."}, "Returns": "The reduced tensor."}, "tf.compat.v1.reduce_mean": {"description": "Computes the mean of elements across dimensions of a tensor.", "Args": {"input_tensor": "The tensor to reduce. Should have numeric type.", "axis": "The dimensions to reduce. If None (the default), reduces all\ndimensions. Must be in the range [-rank(input_tensor),\nrank(input_tensor)).", "keepdims": "If true, retains reduced dimensions with length 1.", "name": "A name for the operation (optional).", "reduction_indices": "The old (deprecated) name for axis.", "keep_dims": "Deprecated alias for keepdims."}, "Returns": "The reduced tensor."}, "tf.compat.v1.reduce_min": {"description": "Computes the tf.math.minimum of elements across dimensions of a tensor. (deprecated arguments)", "Args": {"input_tensor": "The tensor to reduce. Should have real numeric type.", "axis": "The dimensions to reduce. If None (the default), reduces all\ndimensions. Must be in the range [-rank(input_tensor),\nrank(input_tensor)).", "keepdims": "If true, retains reduced dimensions with length 1.", "name": "A name for the operation (optional).", "reduction_indices": "The old (deprecated) name for axis.", "keep_dims": "Deprecated alias for keepdims."}, "Returns": "The reduced tensor."}, "tf.compat.v1.reduce_prod": {"description": "Computes tf.math.multiply of elements across dimensions of a tensor. (deprecated arguments)", "Args": {"input_tensor": "The tensor to reduce. Should have numeric type.", "axis": "The dimensions to reduce. If None (the default), reduces all\ndimensions. Must be in the range [-rank(input_tensor),\nrank(input_tensor)).", "keepdims": "If true, retains reduced dimensions with length 1.", "name": "A name for the operation (optional).", "reduction_indices": "The old (deprecated) name for axis.", "keep_dims": "Deprecated alias for keepdims."}, "Returns": "The reduced tensor."}, "tf.compat.v1.reduce_sum": {"description": "Computes the sum of elements across dimensions of a tensor. (deprecated arguments)", "Args": {"input_tensor": "The tensor to reduce. Should have numeric type.", "axis": "The dimensions to reduce. If None (the default), reduces all\ndimensions. Must be in the range [-rank(input_tensor),\nrank(input_tensor)).", "keepdims": "If true, retains reduced dimensions with length 1.", "name": "A name for the operation (optional).", "reduction_indices": "The old (deprecated) name for axis.", "keep_dims": "Deprecated alias for keepdims."}, "Returns": "The reduced tensor, of the same dtype as the input_tensor."}, "tf.compat.v1.report_uninitialized_variables": {"description": "Adds ops to list the names of uninitialized variables.", "Args": {"var_list": "List of Variable objects to check. Defaults to the value of\nglobal_variables() + local_variables()", "name": "Optional name of the Operation."}, "Returns": "A 1-D tensor containing names of the uninitialized variables, or an empty\n1-D tensor if there are no variables or no uninitialized variables."}, "tf.compat.v1.reset_default_graph": {"description": "Clears the default graph stack and resets the global default graph.", "Raises": {"AssertionError": "If this function is called within a nested graph."}}, "tf.compat.v1.resource_variables_enabled": {"description": "Returns True if resource variables are enabled."}, "tf.compat.v1.reverse_sequence": {"description": "Reverses variable length slices. (deprecated arguments) (deprecated arguments)", "Args": {"input": "A Tensor. The input to reverse.", "seq_lengths": "A Tensor. Must be one of the following types: int32,\nint64. 1-D with length input.dims(batch_axis) and max(seq_lengths) <=\ninput.dims(seq_axis)", "seq_axis": "An int. The dimension which is partially reversed.", "batch_axis": "An optional int. Defaults to 0. The dimension along which\nreversal is performed.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.compat.v1.scalar_mul": {"description": "Multiplies a scalar times a Tensor or IndexedSlices object.", "Args": {"scalar": "A 0-D scalar Tensor. Must have known shape.", "x": "A Tensor or IndexedSlices to be scaled.", "name": "A name for the operation (optional)."}, "Returns": "scalar * x of the same type (Tensor or IndexedSlices) as x.", "Raises": {"ValueError": "if scalar is not a 0-D scalar."}}, "tf.compat.v1.scan": {"description": "scan on the list of tensors unpacked from elems on dimension 0.", "Args": {"fn": "The callable to be performed.  It accepts two arguments.  The first will\nhave the same structure as initializer if one is provided, otherwise it\nwill have the same structure as elems.  The second will have the same\n(possibly nested) structure as elems.  Its output must have the same\nstructure as initializer if one is provided, otherwise it must have the\nsame structure as elems.", "elems": "A tensor or (possibly nested) sequence of tensors, each of which will\nbe unpacked along their first dimension.  The nested sequence of the\nresulting slices will be the first argument to fn.", "initializer": "(optional) A tensor or (possibly nested) sequence of tensors,\ninitial value for the accumulator, and the expected output type of fn.", "parallel_iterations": "(optional) The number of iterations allowed to run in\nparallel.", "back_prop": "(optional) True enables support for back propagation.", "swap_memory": "(optional) True enables GPU-CPU memory swapping.", "infer_shape": "(optional) False disables tests for consistent output shapes.", "reverse": "(optional) True scans the tensor last to first (instead of first to\nlast).", "name": "(optional) Name prefix for the returned tensors."}, "Returns": "A tensor or (possibly nested) sequence of tensors.  Each tensor packs the\nresults of applying fn to tensors unpacked from elems along the first\ndimension, and the previous accumulator value(s), from first to last (or\nlast to first, if reverse=True).", "Raises": {"TypeError": "if fn is not callable or the structure of the output of\nfn and initializer do not match.", "ValueError": "if the lengths of the output of fn and initializer\ndo not match."}}, "tf.compat.v1.scatter_add": {"description": "Adds sparse updates to the variable referenced by resource.", "Args": {"ref": "A Variable.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA tensor of indices into the first dimension of ref.", "updates": "A Tensor. Must have the same type as ref.\nA tensor of updated values to store in ref.", "use_locking": "An optional bool. Defaults to False.\nIf True, the assignment will be protected by a lock;\notherwise the behavior is undefined, but may exhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "Same as ref.  Returned as a convenience for operations that want\nto use the updated values after the update is done."}, "tf.compat.v1.scatter_div": {"description": "Divides a variable reference by sparse updates.", "Args": {"ref": "A mutable Tensor. Must be one of the following types: float32,\nfloat64, int32, uint8, int16, int8, complex64, int64,\nqint8, quint8, qint32, bfloat16, uint16, complex128, half,\nuint32, uint64. Should be from a Variable node.", "indices": "A Tensor. Must be one of the following types: int32, int64. A\ntensor of indices into the first dimension of ref.", "updates": "A Tensor. Must have the same type as ref. A tensor of values\nthat ref is divided by.", "use_locking": "An optional bool. Defaults to False. If True, the operation\nwill be protected by a lock; otherwise the behavior is undefined, but may\nexhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as ref."}, "tf.compat.v1.scatter_max": {"description": "Reduces sparse updates into a variable reference using the max operation.", "Args": {"ref": "A mutable Tensor. Must be one of the following types: half,\nbfloat16, float32, float64, int32, int64. Should be from a\nVariable node.", "indices": "A Tensor. Must be one of the following types: int32, int64. A\ntensor of indices into the first dimension of ref.", "updates": "A Tensor. Must have the same type as ref. A tensor of updated\nvalues to reduce into ref.", "use_locking": "An optional bool. Defaults to False. If True, the update\nwill be protected by a lock; otherwise the behavior is undefined, but may\nexhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as ref."}, "tf.compat.v1.scatter_min": {"description": "Reduces sparse updates into a variable reference using the min operation.", "Args": {"ref": "A mutable Tensor. Must be one of the following types: half,\nbfloat16, float32, float64, int32, int64. Should be from a\nVariable node.", "indices": "A Tensor. Must be one of the following types: int32, int64. A\ntensor of indices into the first dimension of ref.", "updates": "A Tensor. Must have the same type as ref. A tensor of updated\nvalues to reduce into ref.", "use_locking": "An optional bool. Defaults to False. If True, the update\nwill be protected by a lock; otherwise the behavior is undefined, but may\nexhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as ref."}, "tf.compat.v1.scatter_mul": {"description": "Multiplies sparse updates into a variable reference.", "Args": {"ref": "A mutable Tensor. Must be one of the following types: float32,\nfloat64, int32, uint8, int16, int8, complex64, int64,\nqint8, quint8, qint32, bfloat16, uint16, complex128, half,\nuint32, uint64. Should be from a Variable node.", "indices": "A Tensor. Must be one of the following types: int32, int64. A\ntensor of indices into the first dimension of ref.", "updates": "A Tensor. Must have the same type as ref. A tensor of updated\nvalues to multiply to ref.", "use_locking": "An optional bool. Defaults to False. If True, the operation\nwill be protected by a lock; otherwise the behavior is undefined, but may\nexhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as ref."}, "tf.compat.v1.scatter_nd_add": {"description": "Applies sparse addition to individual values or slices in a Variable.", "Args": {"ref": "A mutable Tensor. Must be one of the following types: float32,\nfloat64, int32, uint8, int16, int8, complex64, int64,\nqint8, quint8, qint32, bfloat16, uint16, complex128, half,\nuint32, uint64. A mutable Tensor. Should be from a Variable node.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA tensor of indices into ref.", "updates": "A Tensor. Must have the same type as ref.\nA tensor of updated values to add to ref.", "use_locking": "An optional bool. Defaults to False.\nIf True, the assignment will be protected by a lock;\notherwise the behavior is undefined, but may exhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as ref."}, "tf.compat.v1.scatter_nd_sub": {"description": "Applies sparse subtraction to individual values or slices in a Variable.", "Args": {"ref": "A mutable Tensor. Must be one of the following types: float32,\nfloat64, int32, uint8, int16, int8, complex64, int64,\nqint8, quint8, qint32, bfloat16, uint16, complex128, half,\nuint32, uint64. A mutable Tensor. Should be from a Variable node.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA tensor of indices into ref.", "updates": "A Tensor. Must have the same type as ref.\nA tensor of updated values to add to ref.", "use_locking": "An optional bool. Defaults to False.\nAn optional bool. Defaults to True. If True, the assignment will\nbe protected by a lock; otherwise the behavior is undefined,\nbut may exhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as ref."}, "tf.compat.v1.scatter_nd_update": {"description": "Applies sparse updates to individual values or slices in a Variable.", "Args": {"ref": "A Variable.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA tensor of indices into ref.", "updates": "A Tensor. Must have the same type as ref.\nA Tensor. Must have the same type as ref. A tensor of updated\nvalues to add to ref.", "use_locking": "An optional bool. Defaults to True.\nAn optional bool. Defaults to True. If True, the assignment will\nbe protected by a lock; otherwise the behavior is undefined,\nbut may exhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "The value of the variable after the update."}, "tf.compat.v1.scatter_sub": {"description": "Subtracts sparse updates to a variable reference.", "Args": {"ref": "A mutable Tensor. Must be one of the following types: float32,\nfloat64, int32, uint8, int16, int8, complex64, int64,\nqint8, quint8, qint32, bfloat16, uint16, complex128, half,\nuint32, uint64. Should be from a Variable node.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA tensor of indices into the first dimension of ref.", "updates": "A Tensor. Must have the same type as ref.\nA tensor of updated values to subtract from ref.", "use_locking": "An optional bool. Defaults to False.\nIf True, the subtraction will be protected by a lock;\notherwise the behavior is undefined, but may exhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as ref."}, "tf.compat.v1.scatter_update": {"description": "Applies sparse updates to a variable reference.", "Args": {"ref": "A Variable.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA tensor of indices into the first dimension of ref.", "updates": "A Tensor. Must have the same type as ref.\nA tensor of updated values to store in ref.", "use_locking": "An optional bool. Defaults to True.\nIf True, the assignment will be protected by a lock;\notherwise the behavior is undefined, but may exhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "Same as ref.  Returned as a convenience for operations that want\nto use the updated values after the update is done."}, "tf.compat.v1.serialize_many_sparse": {"description": "Serialize N-minibatch SparseTensor into an [N, 3] Tensor.", "Args": {"sp_input": "The input rank R SparseTensor.", "name": "A name prefix for the returned tensors (optional).", "out_type": "The dtype to use for serialization."}, "Returns": "A matrix (2-D Tensor) with N rows and 3 columns. Each column\nrepresents serialized SparseTensor's indices, values, and shape\n(respectively).", "Raises": {"TypeError": "If sp_input is not a SparseTensor."}}, "tf.compat.v1.serialize_sparse": {"description": "Serialize a SparseTensor into a 3-vector (1-D Tensor) object.", "Args": {"sp_input": "The input SparseTensor.", "name": "A name prefix for the returned tensors (optional).", "out_type": "The dtype to use for serialization."}, "Returns": "A 3-vector (1-D Tensor), with each column representing the serialized\nSparseTensor's indices, values, and shape (respectively).", "Raises": {"TypeError": "If sp_input is not a SparseTensor."}}, "tf.compat.v1.set_random_seed": {"description": "Sets the graph-level random seed for the default graph.", "Args": {"seed": "integer."}}, "tf.compat.v1.setdiff1d": {"description": "Computes the difference between two lists of numbers or strings.", "Args": {"x": "A Tensor. 1-D. Values to keep.", "y": "A Tensor. Must have the same type as x. 1-D. Values to remove.", "out_idx": "An optional tf.DType from: tf.int32, tf.int64. Defaults to tf.int32.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (out, idx)."}, "tf.compat.v1.shape": {"description": "Returns the shape of a tensor.", "Args": {"input": "A Tensor or SparseTensor.", "name": "A name for the operation (optional).", "out_type": "(Optional) The specified output type of the operation (int32\nor int64). Defaults to tf.int32."}, "Returns": "A Tensor of type out_type."}, "tf.compat.v1.size": {"description": "Returns the size of a tensor.", "Args": {"input": "A Tensor or SparseTensor.", "name": "A name for the operation (optional).", "out_type": "(Optional) The specified non-quantized numeric output type of the\noperation. Defaults to tf.int32."}, "Returns": "A Tensor of type out_type. Defaults to tf.int32."}, "tf.compat.v1.space_to_batch": {"description": "SpaceToBatch for 4-D tensors of type T.", "Args": {"input": "A Tensor. 4-D with shape [batch, height, width, depth].", "paddings": "A Tensor. Must be one of the following types: int32, int64.\n2-D tensor of non-negative integers with shape [2, 2]. It specifies\n  the padding of the input with zeros across the spatial dimensions as follows:\n\u00a0 paddings = [[pad_top, pad_bottom], [pad_left, pad_right]]\nThe effective spatial dimensions of the zero-padded input tensor will be:\n\u00a0 height_pad = pad_top + height + pad_bottom\u00a0 width_pad = pad_left + width + pad_right", "block_size": "An int that is >= 2.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.compat.v1.space_to_depth": {"description": "SpaceToDepth for tensors of type T.", "Args": {"input": "A Tensor.", "block_size": "An int that is >= 2. The size of the spatial block.", "data_format": "An optional string from: \"NHWC\", \"NCHW\", \"NCHW_VECT_C\". Defaults to \"NHWC\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.compat.v1.sparse_add": {"description": "Adds two tensors, at least one of each is a SparseTensor. (deprecated arguments)", "Args": {"a": "The first operand; SparseTensor or Tensor.", "b": "The second operand; SparseTensor or Tensor. At least one operand\nmust be sparse.", "threshold": "An optional 0-D Tensor (defaults to 0). The magnitude\nthreshold that determines if an output value/index pair takes space. Its\ndtype should match that of the values if they are real; if the latter are\ncomplex64/complex128, then the dtype should be float32/float64,\ncorrespondingly.", "thresh": "Deprecated alias for threshold."}, "Returns": "A SparseTensor or a Tensor, representing the sum.", "Raises": {"TypeError": "If both a and b are Tensors.  Use tf.add() instead."}}, "tf.compat.v1.sparse_concat": {"description": "Concatenates a list of SparseTensor along the specified dimension. (deprecated arguments)", "Args": {"axis": "Dimension to concatenate along. Must be in range [-rank, rank),\nwhere rank is the number of dimensions in each input SparseTensor.", "sp_inputs": "List of SparseTensor to concatenate.", "name": "A name prefix for the returned tensors (optional).", "expand_nonconcat_dim": "Whether to allow the expansion in the non-concat\ndimensions. Defaulted to False.", "concat_dim": "The old (deprecated) name for axis.", "expand_nonconcat_dims": "alias for expand_nonconcat_dim"}, "Returns": "A SparseTensor with the concatenated output.", "Raises": {"TypeError": "If sp_inputs is not a list of SparseTensor."}}, "tf.compat.v1.sparse_matmul": {"description": "Multiply matrix &#34;a&#34; by matrix &#34;b&#34;.", "Args": {"a": "A Tensor. Must be one of the following types: float32, bfloat16.", "b": "A Tensor. Must be one of the following types: float32, bfloat16.", "transpose_a": "An optional bool. Defaults to False.", "transpose_b": "An optional bool. Defaults to False.", "a_is_sparse": "An optional bool. Defaults to False.", "b_is_sparse": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type float32."}, "tf.compat.v1.sparse_merge": {"description": "Combines a batch of feature ids and values into a single SparseTensor. (deprecated)", "Args": {"sp_ids": "A single SparseTensor with values property of type int32\nor int64 or a Python list of such SparseTensors or a list thereof.", "sp_values": "A SparseTensor of any type.", "vocab_size": "A scalar int64 Tensor (or Python int) containing the new size\nof the last dimension, all(0 <= sp_ids.values < vocab_size).\nOr a list thereof with all(0 <= sp_ids[i].values < vocab_size[i]) for\nall i.", "name": "A name prefix for the returned tensors (optional)", "already_sorted": "A boolean to specify whether the per-batch values in\nsp_values are already sorted. If so skip sorting, False by default\n(optional)."}, "Returns": "A SparseTensor compactly representing a batch of feature ids and values,\nuseful for passing to functions that expect such a SparseTensor.", "Raises": {"TypeError": "If sp_values is not a SparseTensor. Or if sp_ids is neither\na SparseTensor nor a list thereof. Or if vocab_size is not a\nTensor or a Python int and sp_ids is a SparseTensor. Or if\nvocab_size is not a or list thereof and sp_ids is a list.", "ValueError": "If sp_ids and vocab_size are lists of different lengths."}}, "tf.compat.v1.sparse_placeholder": {"description": "Inserts a placeholder for a sparse tensor that will be always fed.", "Args": {"dtype": "The type of values elements in the tensor to be fed.", "shape": "The shape of the tensor to be fed (optional). If the shape is not\nspecified, you can feed a sparse tensor of any shape.", "name": "A name for prefixing the operations (optional)."}, "Returns": "A SparseTensor that may be used as a handle for feeding a value, but not\nevaluated directly.", "Raises": {"RuntimeError": "if eager execution is enabled"}}, "tf.compat.v1.sparse_reduce_max": {"description": "Computes tf.sparse.maximum of elements across dimensions of a SparseTensor. (deprecated arguments) (deprecated arguments)", "Args": {"sp_input": "The SparseTensor to reduce. Should have numeric type.", "axis": "The dimensions to reduce; list or scalar. If None (the\ndefault), reduces all dimensions.", "keepdims": "If true, retain reduced dimensions with length 1.", "reduction_axes": "Deprecated name of axis.", "keep_dims": "Deprecated alias for keepdims."}, "Returns": "The reduced Tensor."}, "tf.compat.v1.sparse_reduce_max_sparse": {"description": "Computes the max of elements across dimensions of a SparseTensor. (deprecated arguments)", "Args": {"sp_input": "The SparseTensor to reduce. Should have numeric type.", "axis": "The dimensions to reduce; list or scalar. If None (the\ndefault), reduces all dimensions.", "keepdims": "If true, retain reduced dimensions with length 1.", "reduction_axes": "Deprecated name of axis.", "keep_dims": "Deprecated alias for keepdims."}, "Returns": "The reduced SparseTensor."}, "tf.compat.v1.sparse_reduce_sum": {"description": "Computes tf.sparse.add of elements across dimensions of a SparseTensor. (deprecated arguments) (deprecated arguments)", "Args": {"sp_input": "The SparseTensor to reduce. Should have numeric type.", "axis": "The dimensions to reduce; list or scalar. If None (the\ndefault), reduces all dimensions.", "keepdims": "If true, retain reduced dimensions with length 1.", "reduction_axes": "Deprecated name of axis.", "keep_dims": "Deprecated alias for keepdims."}, "Returns": "The reduced Tensor."}, "tf.compat.v1.sparse_reduce_sum_sparse": {"description": "Computes the sum of elements across dimensions of a SparseTensor. (deprecated arguments)", "Args": {"sp_input": "The SparseTensor to reduce. Should have numeric type.", "axis": "The dimensions to reduce; list or scalar. If None (the\ndefault), reduces all dimensions.", "keepdims": "If true, retain reduced dimensions with length 1.", "reduction_axes": "Deprecated name of axis.", "keep_dims": "Deprecated alias for keepdims."}, "Returns": "The reduced SparseTensor."}, "tf.compat.v1.sparse_segment_mean": {"description": "Computes the mean along sparse segments of a tensor.", "Args": {"data": "A Tensor with data that will be assembled in the output.", "indices": "A 1-D Tensor with indices into data. Has same rank as\nsegment_ids.", "segment_ids": "A 1-D Tensor with indices into the output Tensor. Values\nshould be sorted and can be repeated.", "name": "A name for the operation (optional).", "num_segments": "An optional int32 scalar. Indicates the size of the output\nTensor."}, "Returns": "A tensor of the shape as data, except for dimension 0 which\nhas size k, the number of segments specified via num_segments or\ninferred for the last element in segments_ids."}, "tf.compat.v1.sparse_segment_sqrt_n": {"description": "Computes the sum along sparse segments of a tensor divided by the sqrt(N).", "Args": {"data": "A Tensor with data that will be assembled in the output.", "indices": "A 1-D Tensor with indices into data. Has same rank as\nsegment_ids.", "segment_ids": "A 1-D Tensor with indices into the output Tensor. Values\nshould be sorted and can be repeated.", "name": "A name for the operation (optional).", "num_segments": "An optional int32 scalar. Indicates the size of the output\nTensor."}, "Returns": "A tensor of the shape as data, except for dimension 0 which\nhas size k, the number of segments specified via num_segments or\ninferred for the last element in segments_ids."}, "tf.compat.v1.sparse_segment_sum": {"description": "Computes the sum along sparse segments of a tensor.", "Args": {"data": "A Tensor with data that will be assembled in the output.", "indices": "A 1-D Tensor with indices into data. Has same rank as\nsegment_ids.", "segment_ids": "A 1-D Tensor with indices into the output Tensor. Values\nshould be sorted and can be repeated.", "name": "A name for the operation (optional).", "num_segments": "An optional int32 scalar. Indicates the size of the output\nTensor."}, "Returns": "A tensor of the shape as data, except for dimension 0 which\nhas size k, the number of segments specified via num_segments or\ninferred for the last element in segments_ids."}, "tf.compat.v1.sparse_split": {"description": "Split a SparseTensor into num_split tensors along axis. (deprecated arguments)", "Args": {"keyword_required": "Python 2 standin for * (temporary for argument reorder)", "sp_input": "The SparseTensor to split.", "num_split": "A Python integer. The number of ways to split.", "axis": "A 0-D int32 Tensor. The dimension along which to split. Must be in\nrange [-rank, rank), where rank is the number of dimensions in the input\nSparseTensor.", "name": "A name for the operation (optional).", "split_dim": "Deprecated old name for axis."}, "Returns": "num_split SparseTensor objects resulting from splitting value.", "Raises": {"TypeError": "If sp_input is not a SparseTensor.", "ValueError": "If the deprecated split_dim and axis are both non None."}}, "tf.compat.v1.sparse_to_dense": {"description": "Converts a sparse representation into a dense tensor. (deprecated)", "Args": {"sparse_indices": "A 0-D, 1-D, or 2-D Tensor of type int32 or int64.\nsparse_indices[i] contains the complete index where sparse_values[i]\nwill be placed.", "output_shape": "A 1-D Tensor of the same type as sparse_indices.  Shape\nof the dense output tensor.", "sparse_values": "A 0-D or 1-D Tensor.  Values corresponding to each row of\nsparse_indices, or a scalar value to be used for all sparse indices.", "default_value": "A 0-D Tensor of the same type as sparse_values.  Value\nto set for indices not specified in sparse_indices.  Defaults to zero.", "validate_indices": "A boolean value.  If True, indices are checked to make\nsure they are sorted in lexicographic order and that there are no repeats.", "name": "A name for the operation (optional)."}, "Returns": "Dense Tensor of shape output_shape.  Has the same type as\nsparse_values."}, "tf.compat.v1.squeeze": {"description": "Removes dimensions of size 1 from the shape of a tensor. (deprecated arguments)", "Args": {"input": "A Tensor. The input to squeeze.", "axis": "An optional list of ints. Defaults to []. If specified, only\nsqueezes the dimensions listed. The dimension index starts at 0. It is an\nerror to squeeze a dimension that is not 1. Must be in the range\n[-rank(input), rank(input)). Must be specified if input is a\nRaggedTensor.", "name": "A name for the operation (optional).", "squeeze_dims": "Deprecated keyword argument that is now axis."}, "Returns": "A Tensor. Has the same type as input.\nContains the same data as input, but has one or more dimensions of\nsize 1 removed.", "Raises": {"ValueError": "When both squeeze_dims and axis are specified."}}, "tf.compat.v1.string_split": {"description": "Split elements of source based on delimiter. (deprecated arguments)", "Args": {"source": "1-D string Tensor, the strings to split.", "sep": "0-D string Tensor, the delimiter character, the string should\nbe length 0 or 1. Default is ' '.", "skip_empty": "A bool. If True, skip the empty strings from the result.", "delimiter": "deprecated alias for sep.", "result_type": "The tensor type for the result: one of \"RaggedTensor\" or\n\"SparseTensor\".", "name": "A name for the operation (optional)."}, "Raises": {"ValueError": "If delimiter is not a string."}, "Returns": "A SparseTensor or RaggedTensor of rank 2, the strings split according\nto the delimiter.  The first column of the indices corresponds to the row\nin source and the second column corresponds to the index of the split\ncomponent in this row."}, "tf.compat.v1.string_to_hash_bucket": {"description": "Converts each string in the input Tensor to its hash mod by a number of buckets.", "Args": {"string_tensor": "A Tensor of type string.", "num_buckets": "An int that is >= 1. The number of buckets.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int64."}, "tf.compat.v1.string_to_number": {"description": "Converts each string in the input Tensor to the specified numeric type.", "Args": {"string_tensor": "A Tensor of type string.", "out_type": "An optional tf.DType from: tf.float32, tf.float64, tf.int32, tf.int64. Defaults to tf.float32.\nThe numeric type to interpret each string in string_tensor as.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type out_type."}, "tf.compat.v1.substr": {"description": "Return substrings from Tensor of strings.", "Raises": {}, "Args": {"input": "A Tensor of type string. Tensor of strings", "pos": "A Tensor. Must be one of the following types: int32, int64.\nScalar defining the position of first character in each substring", "len": "A Tensor. Must have the same type as pos.\nScalar defining the number of characters to include in each substring", "unit": "An optional string from: \"BYTE\", \"UTF8_CHAR\". Defaults to \"BYTE\".\nThe unit that is used to create the substring.  One of: \"BYTE\" (for\ndefining position and length by bytes) or \"UTF8_CHAR\" (for the UTF-8\nencoded Unicode code points).  The default is \"BYTE\". Results are undefined if\nunit=UTF8_CHAR and the input strings do not contain structurally valid\nUTF-8.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.compat.v1.tables_initializer": {"description": "Returns an Op that initializes all tables of the default graph.", "Args": {"name": "Optional name for the initialization op."}, "Returns": "An Op that initializes all tables.  Note that if there are\nnot tables the returned Op is a NoOp."}, "tf.compat.v1.to_bfloat16": {"description": "Casts a tensor to type bfloat16. (deprecated)", "Args": {"x": "A Tensor or SparseTensor or IndexedSlices.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor or SparseTensor or IndexedSlices with same shape as x with\ntype bfloat16.", "Raises": {"TypeError": "If x cannot be cast to the bfloat16."}}, "tf.compat.v1.to_complex128": {"description": "Casts a tensor to type complex128. (deprecated)", "Args": {"x": "A Tensor or SparseTensor or IndexedSlices.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor or SparseTensor or IndexedSlices with same shape as x with\ntype complex128.", "Raises": {"TypeError": "If x cannot be cast to the complex128."}}, "tf.compat.v1.to_complex64": {"description": "Casts a tensor to type complex64. (deprecated)", "Args": {"x": "A Tensor or SparseTensor or IndexedSlices.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor or SparseTensor or IndexedSlices with same shape as x with\ntype complex64.", "Raises": {"TypeError": "If x cannot be cast to the complex64."}}, "tf.compat.v1.to_double": {"description": "Casts a tensor to type float64. (deprecated)", "Args": {"x": "A Tensor or SparseTensor or IndexedSlices.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor or SparseTensor or IndexedSlices with same shape as x with\ntype float64.", "Raises": {"TypeError": "If x cannot be cast to the float64."}}, "tf.compat.v1.to_float": {"description": "Casts a tensor to type float32. (deprecated)", "Args": {"x": "A Tensor or SparseTensor or IndexedSlices.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor or SparseTensor or IndexedSlices with same shape as x with\ntype float32.", "Raises": {"TypeError": "If x cannot be cast to the float32."}}, "tf.compat.v1.to_int32": {"description": "Casts a tensor to type int32. (deprecated)", "Args": {"x": "A Tensor or SparseTensor or IndexedSlices.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor or SparseTensor or IndexedSlices with same shape as x with\ntype int32.", "Raises": {"TypeError": "If x cannot be cast to the int32."}}, "tf.compat.v1.to_int64": {"description": "Casts a tensor to type int64. (deprecated)", "Args": {"x": "A Tensor or SparseTensor or IndexedSlices.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor or SparseTensor or IndexedSlices with same shape as x with\ntype int64.", "Raises": {"TypeError": "If x cannot be cast to the int64."}}, "tf.compat.v1.trainable_variables": {"description": "Returns all variables created with trainable=True.", "Args": {"scope": "(Optional.) A string. If supplied, the resulting list is filtered to\ninclude only items whose name attribute matches scope using\nre.match. Items without a name attribute are never returned if a scope\nis supplied. The choice of re.match means that a scope without special\ntokens filters by prefix."}, "Returns": "A list of Variable objects."}, "tf.compat.v1.transpose": {"description": "Transposes a.", "Args": {"a": "A Tensor.", "perm": "A permutation of the dimensions of a.", "name": "A name for the operation (optional).", "conjugate": "Optional bool. Setting it to True is mathematically equivalent\nto tf.math.conj(tf.transpose(input))."}, "Returns": "A transposed Tensor."}, "tf.compat.v1.truncated_normal_initializer": {"description": "Initializer that generates a truncated normal distribution.", "Args": {"mean": "a python scalar or a scalar tensor. Mean of the random values to\ngenerate.", "stddev": "a python scalar or a scalar tensor. Standard deviation of the random\nvalues to generate.", "seed": "A Python integer. Used to create random seeds. See\ntf.compat.v1.set_random_seed for behavior.", "dtype": "Default data type, used if no dtype argument is provided when\ncalling the initializer. Only floating point types are supported."}}, "tf.compat.v1.tuple": {"description": "Group tensors together.", "Args": {"tensors": "A list of Tensors or IndexedSlices, some entries can be None.", "name": "(optional) A name to use as a name_scope for the operation.", "control_inputs": "List of additional ops to finish before returning."}, "Returns": "Same as tensors.", "Raises": {"ValueError": "If tensors does not contain any Tensor or IndexedSlices.", "TypeError": "If control_inputs is not a list of Operation or Tensor\nobjects."}}, "tf.compat.v1.uniform_unit_scaling_initializer": {"description": "Initializer that generates tensors without scaling variance.", "Args": {"factor": "Float.  A multiplicative factor by which the values will be scaled.", "seed": "A Python integer. Used to create random seeds. See\ntf.compat.v1.set_random_seed for behavior.", "dtype": "Default data type, used if no dtype argument is provided when\ncalling the initializer. Only floating point types are supported."}}, "tf.compat.v1.variable_axis_size_partitioner": {"description": "Get a partitioner for VariableScope to keep shards below max_shard_bytes.", "Args": {"max_shard_bytes": "The maximum size any given shard is allowed to be.", "axis": "The axis to partition along.  Default: outermost axis.", "bytes_per_string_element": "If the Variable is of type string, this provides\nan estimate of how large each scalar in the Variable is.", "max_shards": "The maximum number of shards in int created taking precedence\nover max_shard_bytes."}, "Returns": "A partition function usable as the partitioner argument to\nvariable_scope and get_variable.", "Raises": {"ValueError": "If any of the byte counts are non-positive."}}, "tf.compat.v1.variable_creator_scope": {"description": "Scope which defines a variable creation function to be used by variable().", "Args": {"variable_creator": "the passed creator"}}, "tf.compat.v1.variable_op_scope": {"description": "Deprecated: context manager for defining an op that creates variables."}, "tf.compat.v1.variable_scope": {"description": "A context manager for defining ops that creates variables (layers).", "Args": {"name_or_scope": "string or VariableScope: the scope to open.", "default_name": "The default name to use if the name_or_scope argument is\nNone, this name will be uniquified. If name_or_scope is provided it\nwon't be used and therefore it is not required and can be None.", "values": "The list of Tensor arguments that are passed to the op function.", "initializer": "default initializer for variables within this scope.", "regularizer": "default regularizer for variables within this scope.", "caching_device": "default caching device for variables within this scope.", "partitioner": "default partitioner for variables within this scope.", "custom_getter": "default custom getter for variables within this scope.", "reuse": "True, None, or tf.compat.v1.AUTO_REUSE; if True, we go into\nreuse mode for this scope as well as all sub-scopes; if\ntf.compat.v1.AUTO_REUSE, we create variables if they do not exist, and\nreturn them otherwise; if None, we inherit the parent scope's reuse\nflag. When eager execution is enabled, new variables are always created\nunless an EagerVariableStore or template is currently active.", "dtype": "type of variables created in this scope (defaults to the type in\nthe passed scope, or inherited from parent scope).", "use_resource": "If False, all variables will be regular Variables. If True,\nexperimental ResourceVariables with well-defined semantics will be used\ninstead. Defaults to False (will later change to True). When eager\nexecution is enabled this argument is always forced to be True.", "constraint": "An optional projection function to be applied to the variable\nafter being updated by an Optimizer (e.g. used to implement norm\nconstraints or value constraints for layer weights). The function must\ntake as input the unprojected Tensor representing the value of the\nvariable and return the Tensor for the projected value (which must have\nthe same shape). Constraints are not safe to use when doing asynchronous\ndistributed training.", "auxiliary_name_scope": "If True, we create an auxiliary name scope with\nthe scope. If False, we don't create it. Note that the argument is not\ninherited, and it only takes effect for once when creating. You should\nonly use it for re-entering a premade variable scope."}, "Raises": {"ValueError": "when trying to reuse within a create scope, or create within\na reuse scope.", "TypeError": "when the types of some arguments are not appropriate."}}, "tf.compat.v1.variables_initializer": {"description": "Returns an Op that initializes a list of variables.", "Args": {"var_list": "List of Variable objects to initialize.", "name": "Optional name for the returned operation."}, "Returns": "An Op that run the initializers of all the specified variables."}, "tf.compat.v1.verify_tensor_all_finite": {"description": "Assert that the tensor does not contain any NaN&#39;s or Inf&#39;s.", "Args": {"t": "Tensor to check.", "msg": "Message to log on failure.", "name": "A name for this operation (optional).", "x": "Alias for t.", "message": "Alias for msg."}, "Returns": "Same tensor as t."}, "tf.compat.v1.where": {"description": "Return the elements, either from x or y, depending on the condition.", "Args": {"condition": "A Tensor of type bool", "x": "A Tensor which may have the same shape as condition. If condition is\nrank 1, x may have higher rank, but its first dimension must match the\nsize of condition.", "y": "A tensor with the same shape and type as x.", "name": "A name of the operation (optional)"}, "Returns": "A Tensor with the same type and shape as x, y if they are non-None.\nOtherwise, a Tensor with shape (num_true, rank(condition)).", "Raises": {"ValueError": "When exactly one of x or y is non-None."}}, "tf.compat.v1.while_loop": {"description": "Repeat body while the condition cond is true.", "Args": {"cond": "A callable that represents the termination condition of the loop.", "body": "A callable that represents the loop body.", "loop_vars": "A (possibly nested) tuple, namedtuple or list of numpy array,\nTensor, and TensorArray objects.", "shape_invariants": "The shape invariants for the loop variables.", "parallel_iterations": "The number of iterations allowed to run in parallel. It\nmust be a positive integer.", "back_prop": "Whether backprop is enabled for this while loop.", "swap_memory": "Whether GPU-CPU memory swap is enabled for this loop.", "name": "Optional name prefix for the returned tensors.", "maximum_iterations": "Optional maximum number of iterations of the while loop\nto run.  If provided, the cond output is AND-ed with an additional\ncondition ensuring the number of iterations executed is no greater than\nmaximum_iterations.", "return_same_structure": "If True, output has same structure as loop_vars. If\neager execution is enabled, this is ignored (and always treated as True)."}, "Returns": "The output tensors for the loop variables after the loop.\nIf return_same_structure is True, the return value has the same\nstructure as loop_vars.\nIf return_same_structure is False, the return value is a Tensor,\nTensorArray or IndexedSlice if the length of loop_vars is 1, or a list\notherwise.", "Raises": {"TypeError": "if cond or body is not callable.", "ValueError": "if loop_vars is empty."}}, "tf.compat.v1.wrap_function": {"description": "Wraps the TF 1.x function fn into a graph function.", "Args": {"fn": "python function to be wrapped", "signature": "the placeholder and python arguments to be passed to the wrapped\nfunction", "name": "Optional. The name of the function."}, "Returns": "the wrapped graph function."}, "tf.compat.v1.zeros_like": {"description": "Creates a tensor with all elements set to zero.", "Args": {"tensor": "A Tensor.", "dtype": "A type for the returned Tensor. Must be float16, float32,\nfloat64, int8, uint8, int16, uint16, int32, int64,\ncomplex64, complex128, bool or string. (optional)", "name": "A name for the operation (optional).", "optimize": "if True, attempt to statically determine the shape of tensor\nand encode it as a constant. (optional, defaults to True)"}, "Returns": "A Tensor with all elements set to zero."}}, "tf.compat.v1.app": {"tf.compat.v1.app.run": {"description": "Runs the program with an optional &#39;main&#39; function and &#39;argv&#39; list."}}, "tf.compat.v1.audio": {}, "tf.compat.v1.autograph": {"tf.compat.v1.autograph.to_code": {"description": "Returns the source code generated by AutoGraph, as a string.", "Args": {"entity": "Python callable or class.", "recursive": "Whether to recursively convert any functions that the converted\nfunction may call.", "arg_values": "Deprecated.", "arg_types": "Deprecated.", "indentation": "Deprecated.", "experimental_optional_features": "None, a tuple of, or a single\ntf.autograph.experimental.Feature value."}, "Returns": "The converted code as string."}, "tf.compat.v1.autograph.to_graph": {"description": "Converts a Python entity into a TensorFlow graph.", "Args": {"entity": "Python callable or class to convert.", "recursive": "Whether to recursively convert any functions that the converted\nfunction may call.", "arg_values": "Deprecated.", "arg_types": "Deprecated.", "experimental_optional_features": "None, a tuple of, or a single\ntf.autograph.experimental.Feature value."}, "Returns": "Same as entity, the converted Python function or class.", "Raises": {"ValueError": "If the entity could not be converted."}}}, "tf.compat.v1.bitwise": {}, "tf.compat.v1.compat": {}, "tf.compat.v1.config": {}, "tf.compat.v1.config.optimizer": {}, "tf.compat.v1.config.threading": {}, "tf.compat.v1.data": {"tf.compat.v1.data.Dataset": {"description": "Represents a potentially large set of elements.", "Args": {"variant_tensor": "A DT_VARIANT tensor that represents the dataset."}, "Attributes": {"element_spec": "The type specification of an element of this dataset.\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])dataset.element_specTensorSpec(shape=(), dtype=tf.int32, name=None)\nFor more information,\nread this guide.", "output_classes": "Returns the class of each component of an element of this dataset. (deprecated)\nDeprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\nInstructions for updating:\nUse tf.compat.v1.data.get_output_classes(dataset).", "output_shapes": "Returns the shape of each component of an element of this dataset. (deprecated)Deprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\nInstructions for updating:\nUse tf.compat.v1.data.get_output_shapes(dataset).", "output_types": "Returns the type of each component of an element of this dataset. (deprecated)Deprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\nInstructions for updating:\nUse tf.compat.v1.data.get_output_types(dataset)."}}, "tf.compat.v1.data.FixedLengthRecordDataset": {"description": "A Dataset of fixed-length records from one or more binary files.", "Args": {"filenames": "A tf.string tensor or tf.data.Dataset containing one or\nmore filenames.", "record_bytes": "A tf.int64 scalar representing the number of bytes in each\nrecord.", "header_bytes": "(Optional.) A tf.int64 scalar representing the number of\nbytes to skip at the start of a file.", "footer_bytes": "(Optional.) A tf.int64 scalar representing the number of\nbytes to ignore at the end of a file.", "buffer_size": "(Optional.) A tf.int64 scalar representing the number of\nbytes to buffer when reading.", "compression_type": "(Optional.) A tf.string scalar evaluating to one of\n\"\" (no compression), \"ZLIB\", or \"GZIP\".", "num_parallel_reads": "(Optional.) A tf.int64 scalar representing the\nnumber of files to read in parallel. If greater than one, the records of\nfiles read in parallel are outputted in an interleaved order. If your\ninput pipeline is I/O bottlenecked, consider setting this parameter to a\nvalue greater than one to parallelize the I/O. If None, files will be\nread sequentially.", "name": "(Optional.) A name for the tf.data operation."}, "Attributes": {"element_spec": "The type specification of an element of this dataset.\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])dataset.element_specTensorSpec(shape=(), dtype=tf.int32, name=None)\nFor more information,\nread this guide.", "output_classes": "Returns the class of each component of an element of this dataset. (deprecated)\nDeprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\nInstructions for updating:\nUse tf.compat.v1.data.get_output_classes(dataset).", "output_shapes": "Returns the shape of each component of an element of this dataset. (deprecated)Deprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\nInstructions for updating:\nUse tf.compat.v1.data.get_output_shapes(dataset).", "output_types": "Returns the type of each component of an element of this dataset. (deprecated)Deprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\nInstructions for updating:\nUse tf.compat.v1.data.get_output_types(dataset)."}}, "tf.compat.v1.data.Iterator": {"description": "Represents the state of iterating through a Dataset.", "Args": {"iterator_resource": "A tf.resource scalar tf.Tensor representing the\niterator.", "initializer": "A tf.Operation that should be run to initialize this\niterator.", "output_types": "A (nested) structure of tf.DType objects corresponding to\neach component of an element of this iterator.", "output_shapes": "A (nested) structure of tf.TensorShape objects\ncorresponding to each component of an element of this iterator.", "output_classes": "A (nested) structure of Python type objects\ncorresponding to each component of an element of this iterator."}, "Raises": {"TypeError": "If output_types, output_shapes, or output_classes is not\nspecified."}, "Attributes": {"element_spec": "The type specification of an element of this iterator.\nFor more information,\nread this guide.", "initializer": "A tf.Operation that should be run to initialize this iterator.", "output_classes": "Returns the class of each component of an element of this iterator. (deprecated)\nDeprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\nInstructions for updating:\nUse tf.compat.v1.data.get_output_classes(iterator).\nThe expected values are tf.Tensor and tf.sparse.SparseTensor.", "output_shapes": "Returns the shape of each component of an element of this iterator. (deprecated)\nDeprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\nInstructions for updating:\nUse tf.compat.v1.data.get_output_shapes(iterator).", "output_types": "Returns the type of each component of an element of this iterator. (deprecated)Deprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\nInstructions for updating:\nUse tf.compat.v1.data.get_output_types(iterator)."}}, "tf.compat.v1.data.TFRecordDataset": {"description": "A Dataset comprising records from one or more TFRecord files.", "Args": {"filenames": "A tf.string tensor or tf.data.Dataset containing one or\nmore filenames.", "compression_type": "(Optional.) A tf.string scalar evaluating to one of\n\"\" (no compression), \"ZLIB\", or \"GZIP\".", "buffer_size": "(Optional.) A tf.int64 scalar representing the number of\nbytes in the read buffer. If your input pipeline is I/O bottlenecked,\nconsider setting this parameter to a value 1-100 MBs. If None, a\nsensible default for both local and remote file systems is used.", "num_parallel_reads": "(Optional.) A tf.int64 scalar representing the\nnumber of files to read in parallel. If greater than one, the records of\nfiles read in parallel are outputted in an interleaved order. If your\ninput pipeline is I/O bottlenecked, consider setting this parameter to a\nvalue greater than one to parallelize the I/O. If None, files will be\nread sequentially.", "name": "(Optional.) A name for the tf.data operation."}, "Raises": {"TypeError": "If any argument does not have the expected type.", "ValueError": "If any argument does not have the expected shape."}, "Attributes": {"element_spec": "The type specification of an element of this dataset.\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])dataset.element_specTensorSpec(shape=(), dtype=tf.int32, name=None)\nFor more information,\nread this guide.", "output_classes": "Returns the class of each component of an element of this dataset. (deprecated)\nDeprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\nInstructions for updating:\nUse tf.compat.v1.data.get_output_classes(dataset).", "output_shapes": "Returns the shape of each component of an element of this dataset. (deprecated)Deprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\nInstructions for updating:\nUse tf.compat.v1.data.get_output_shapes(dataset).", "output_types": "Returns the type of each component of an element of this dataset. (deprecated)Deprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\nInstructions for updating:\nUse tf.compat.v1.data.get_output_types(dataset)."}}, "tf.compat.v1.data.TextLineDataset": {"description": "A Dataset comprising lines from one or more text files.", "Args": {"filenames": "A tf.data.Dataset whose elements are tf.string scalars, a\ntf.string tensor, or a value that can be converted to a tf.string\ntensor (such as a list of Python strings).", "compression_type": "(Optional.) A tf.string scalar evaluating to one of\n\"\" (no compression), \"ZLIB\", or \"GZIP\".", "buffer_size": "(Optional.) A tf.int64 scalar denoting the number of bytes\nto buffer. A value of 0 results in the default buffering values chosen\nbased on the compression type.", "num_parallel_reads": "(Optional.) A tf.int64 scalar representing the\nnumber of files to read in parallel. If greater than one, the records of\nfiles read in parallel are outputted in an interleaved order. If your\ninput pipeline is I/O bottlenecked, consider setting this parameter to a\nvalue greater than one to parallelize the I/O. If None, files will be\nread sequentially.", "name": "(Optional.) A name for the tf.data operation."}, "Attributes": {"element_spec": "The type specification of an element of this dataset.\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])dataset.element_specTensorSpec(shape=(), dtype=tf.int32, name=None)\nFor more information,\nread this guide.", "output_classes": "Returns the class of each component of an element of this dataset. (deprecated)\nDeprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\nInstructions for updating:\nUse tf.compat.v1.data.get_output_classes(dataset).", "output_shapes": "Returns the shape of each component of an element of this dataset. (deprecated)Deprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\nInstructions for updating:\nUse tf.compat.v1.data.get_output_shapes(dataset).", "output_types": "Returns the type of each component of an element of this dataset. (deprecated)Deprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\nInstructions for updating:\nUse tf.compat.v1.data.get_output_types(dataset)."}}, "tf.compat.v1.data.get_output_classes": {"description": "Returns the output classes for elements of the input dataset / iterator.", "Args": {"dataset_or_iterator": "A tf.data.Dataset or tf.data.Iterator."}, "Returns": "A (nested) structure of Python type objects matching the structure of the\ndataset / iterator elements and specifying the class of the individual\ncomponents."}, "tf.compat.v1.data.get_output_shapes": {"description": "Returns the output shapes for elements of the input dataset / iterator.", "Args": {"dataset_or_iterator": "A tf.data.Dataset or tf.data.Iterator."}, "Returns": "A (nested) structure of tf.TensorShape objects matching the structure of\nthe dataset / iterator elements and specifying the shape of the individual\ncomponents."}, "tf.compat.v1.data.get_output_types": {"description": "Returns the output shapes for elements of the input dataset / iterator.", "Args": {"dataset_or_iterator": "A tf.data.Dataset or tf.data.Iterator."}, "Returns": "A (nested) structure of tf.DType objects matching the structure of\ndataset / iterator elements and specifying the shape of the individual\ncomponents."}, "tf.compat.v1.data.make_initializable_iterator": {"description": "Creates an iterator for elements of dataset.", "Args": {"dataset": "A tf.data.Dataset.", "shared_name": "(Optional.) If non-empty, the returned iterator will be shared\nunder the given name across multiple sessions that share the same devices\n(e.g. when using a remote server)."}, "Returns": "A tf.data.Iterator for elements of dataset.", "Raises": {"RuntimeError": "If eager execution is enabled."}}, "tf.compat.v1.data.make_one_shot_iterator": {"description": "Creates an iterator for elements of dataset.", "Args": {"dataset": "A tf.data.Dataset."}, "Returns": "A tf.data.Iterator for elements of dataset."}}, "tf.compat.v1.data.experimental.service": {}, "tf.compat.v1.debugging": {"tf.compat.v1.debugging.assert_shapes": {"description": "Assert tensor shapes and dimension size relationships between tensors.", "Args": {"shapes": "A list of (Tensor, shape) tuples, wherein shape is the\nexpected shape of Tensor. See the example code above. The shape must\nbe an iterable. Each element of the iterable can be either a concrete\ninteger value or a string that abstractly represents the dimension.\nFor example,\n\n('N', 'Q') specifies a 2D shape wherein the first and second\ndimensions of shape may or may not be equal.\n('N', 'N', 'Q') specifies a 3D shape wherein the first and second\ndimensions are equal.\n(1, 'N') specifies a 2D shape wherein the first dimension is\nexactly 1 and the second dimension can be any value.\nNote that the abstract dimension letters take effect across different\ntuple elements of the list. For example,\ntf.debugging.assert_shapes([(x, ('N', 'A')), (y, ('N', 'B'))] asserts\nthat both x and y are rank-2 tensors and their first dimensions are\nequal (N).\nshape can also be a tf.TensorShape.", "data": "The tensors to print out if the condition is False.  Defaults to error\nmessage and first few entries of the violating tensor.", "summarize": "Print this many entries of the tensor.", "message": "A string to prefix to the default message.", "name": "A name for this operation (optional).  Defaults to \"assert_shapes\"."}, "Returns": "Op raising InvalidArgumentError unless all shape constraints are\nsatisfied.\nIf static checks determine all constraints are satisfied, a no_op is\nreturned.", "Raises": {"ValueError": "If static checks determine any shape constraint is violated."}}}, "tf.compat.v1.distribute": {"tf.compat.v1.distribute.MirroredStrategy": {"description": "Synchronous training across multiple replicas on one machine.", "Args": {"devices": "a list of device strings such as ['/gpu:0', '/gpu:1'].  If\nNone, all available GPUs are used. If no GPUs are found, CPU is used.", "cross_device_ops": "optional, a descedant of CrossDeviceOps. If this is not\nset, NcclAllReduce() will be used by default.  One would customize this\nif NCCL isn't available or if a special implementation that exploits\nthe particular hardware is available."}, "Attributes": {"cluster_resolver": "Returns the cluster resolver associated with this strategy.\nIn general, when using a multi-worker tf.distribute strategy such as\ntf.distribute.experimental.MultiWorkerMirroredStrategy or\ntf.distribute.TPUStrategy(), there is a\ntf.distribute.cluster_resolver.ClusterResolver associated with the\nstrategy used, and such an instance is returned by this property.\nStrategies that intend to have an associated\ntf.distribute.cluster_resolver.ClusterResolver must set the\nrelevant attribute, or override this property; otherwise, None is returned\nby default. Those strategies should also provide information regarding what\nis returned by this property.\nSingle-worker strategies usually do not have a\ntf.distribute.cluster_resolver.ClusterResolver, and in those cases this\nproperty will return None.\nThe tf.distribute.cluster_resolver.ClusterResolver may be useful when the\nuser needs to access information such as the cluster spec, task type or task\nid. For example,\nos.environ['TF_CONFIG'] = json.dumps({\u00a0 'cluster': {\u00a0 \u00a0 \u00a0 'worker': [\"localhost:12345\", \"localhost:23456\"],\u00a0 \u00a0 \u00a0 'ps': [\"localhost:34567\"]\u00a0 },\u00a0 'task': {'type': 'worker', 'index': 0}})# This implicitly uses TF_CONFIG for the cluster and current task info.strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()...if strategy.cluster_resolver.task_type == 'worker':\u00a0 # Perform something that's only applicable on workers. Since we set this\u00a0 # as a worker above, this block will run on this particular instance.elif strategy.cluster_resolver.task_type == 'ps':\u00a0 # Perform something that's only applicable on parameter servers. Since we\u00a0 # set this as a worker above, this block will not run on this particular\u00a0 # instance.\nFor more information, please see\ntf.distribute.cluster_resolver.ClusterResolver's API docstring.", "extended": "tf.distribute.StrategyExtended with additional methods.", "num_replicas_in_sync": "Returns number of replicas over which gradients are aggregated."}}, "tf.compat.v1.distribute.OneDeviceStrategy": {"description": "A distribution strategy for running on a single device.", "Args": {"device": "Device string identifier for the device on which the variables\nshould be placed. See class docs for more details on how the device is\nused. Examples: \"/cpu:0\", \"/gpu:0\", \"/device:CPU:0\", \"/device:GPU:0\""}, "Attributes": {"cluster_resolver": "Returns the cluster resolver associated with this strategy.\nIn general, when using a multi-worker tf.distribute strategy such as\ntf.distribute.experimental.MultiWorkerMirroredStrategy or\ntf.distribute.TPUStrategy(), there is a\ntf.distribute.cluster_resolver.ClusterResolver associated with the\nstrategy used, and such an instance is returned by this property.\nStrategies that intend to have an associated\ntf.distribute.cluster_resolver.ClusterResolver must set the\nrelevant attribute, or override this property; otherwise, None is returned\nby default. Those strategies should also provide information regarding what\nis returned by this property.\nSingle-worker strategies usually do not have a\ntf.distribute.cluster_resolver.ClusterResolver, and in those cases this\nproperty will return None.\nThe tf.distribute.cluster_resolver.ClusterResolver may be useful when the\nuser needs to access information such as the cluster spec, task type or task\nid. For example,\nos.environ['TF_CONFIG'] = json.dumps({\u00a0 'cluster': {\u00a0 \u00a0 \u00a0 'worker': [\"localhost:12345\", \"localhost:23456\"],\u00a0 \u00a0 \u00a0 'ps': [\"localhost:34567\"]\u00a0 },\u00a0 'task': {'type': 'worker', 'index': 0}})# This implicitly uses TF_CONFIG for the cluster and current task info.strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()...if strategy.cluster_resolver.task_type == 'worker':\u00a0 # Perform something that's only applicable on workers. Since we set this\u00a0 # as a worker above, this block will run on this particular instance.elif strategy.cluster_resolver.task_type == 'ps':\u00a0 # Perform something that's only applicable on parameter servers. Since we\u00a0 # set this as a worker above, this block will not run on this particular\u00a0 # instance.\nFor more information, please see\ntf.distribute.cluster_resolver.ClusterResolver's API docstring.", "extended": "tf.distribute.StrategyExtended with additional methods.", "num_replicas_in_sync": "Returns number of replicas over which gradients are aggregated."}}, "tf.compat.v1.distribute.ReplicaContext": {"description": "A class with a collection of APIs that can be called in a replica context.", "Args": {"strategy": "A tf.distribute.Strategy.", "replica_id_in_sync_group": "An integer, a Tensor or None. Prefer an\ninteger whenever possible to avoid issues with nested tf.function. It\naccepts a Tensor only to be compatible with tpu.replicate."}, "Attributes": {"devices": "Returns the devices this replica is to be executed on, as a tuple of strings. (deprecated)\nDeprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\nInstructions for updating:\nPlease avoid relying on devices property.Note: For tf.distribute.MirroredStrategy and\ntf.distribute.experimental.MultiWorkerMirroredStrategy, this returns a\nnested\nlist of device strings, e.g, [[\"GPU:0\"]].", "num_replicas_in_sync": "Returns number of replicas that are kept in sync.", "replica_id_in_sync_group": "Returns the id of the replica.\nThis identifies the replica among all replicas that are kept in sync. The\nvalue of the replica id can range from 0 to\ntf.distribute.ReplicaContext.num_replicas_in_sync - 1.\nNote: This is not guaranteed to be the same ID as the XLA replica ID use\nfor low-level operations such as collective_permute.", "strategy": "The current tf.distribute.Strategy object."}}, "tf.compat.v1.distribute.Strategy": {"description": "A list of devices with a state & compute distribution policy.", "Attributes": {"cluster_resolver": "Returns the cluster resolver associated with this strategy.\nIn general, when using a multi-worker tf.distribute strategy such as\ntf.distribute.experimental.MultiWorkerMirroredStrategy or\ntf.distribute.TPUStrategy(), there is a\ntf.distribute.cluster_resolver.ClusterResolver associated with the\nstrategy used, and such an instance is returned by this property.\nStrategies that intend to have an associated\ntf.distribute.cluster_resolver.ClusterResolver must set the\nrelevant attribute, or override this property; otherwise, None is returned\nby default. Those strategies should also provide information regarding what\nis returned by this property.\nSingle-worker strategies usually do not have a\ntf.distribute.cluster_resolver.ClusterResolver, and in those cases this\nproperty will return None.\nThe tf.distribute.cluster_resolver.ClusterResolver may be useful when the\nuser needs to access information such as the cluster spec, task type or task\nid. For example,\nos.environ['TF_CONFIG'] = json.dumps({\u00a0 'cluster': {\u00a0 \u00a0 \u00a0 'worker': [\"localhost:12345\", \"localhost:23456\"],\u00a0 \u00a0 \u00a0 'ps': [\"localhost:34567\"]\u00a0 },\u00a0 'task': {'type': 'worker', 'index': 0}})# This implicitly uses TF_CONFIG for the cluster and current task info.strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()...if strategy.cluster_resolver.task_type == 'worker':\u00a0 # Perform something that's only applicable on workers. Since we set this\u00a0 # as a worker above, this block will run on this particular instance.elif strategy.cluster_resolver.task_type == 'ps':\u00a0 # Perform something that's only applicable on parameter servers. Since we\u00a0 # set this as a worker above, this block will not run on this particular\u00a0 # instance.\nFor more information, please see\ntf.distribute.cluster_resolver.ClusterResolver's API docstring.", "extended": "tf.distribute.StrategyExtended with additional methods.", "num_replicas_in_sync": "Returns number of replicas over which gradients are aggregated."}}, "tf.compat.v1.distribute.StrategyExtended": {"description": "Additional APIs for algorithms that need to be distribution-aware.", "Attributes": {"experimental_between_graph": "Whether the strategy uses between-graph replication or not.\nThis is expected to return a constant value that will not be changed\nthroughout its life cycle.", "experimental_require_static_shapes": "Returns True if static shape is required; False otherwise.", "experimental_should_init": "Whether initialization is needed.", "parameter_devices": "Returns the tuple of all devices used to place variables.", "should_checkpoint": "Whether checkpointing is needed.", "should_save_summary": "Whether saving summaries is needed.", "worker_devices": "Returns the tuple of all devices used to for compute replica execution."}}, "tf.compat.v1.distribute.get_loss_reduction": {"description": "tf.distribute.ReduceOp corresponding to the last loss reduction.", "Returns": "tf.distribute.ReduceOp corresponding to the last loss reduction for\nestimator and v1 optimizer use case. tf.distribute.ReduceOp.SUM otherwise."}}, "tf.compat.v1.distribute.cluster_resolver": {}, "tf.compat.v1.distributions": {"tf.compat.v1.distributions.Bernoulli": {"description": "Bernoulli distribution.", "Args": {"logits": "An N-D Tensor representing the log-odds of a 1 event. Each\nentry in the Tensor parametrizes an independent Bernoulli distribution\nwhere the probability of an event is sigmoid(logits). Only one of\nlogits or probs should be passed in.", "probs": "An N-D Tensor representing the probability of a 1\nevent. Each entry in the Tensor parameterizes an independent\nBernoulli distribution. Only one of logits or probs should be passed\nin.", "dtype": "The type of the event samples. Default: int32.", "validate_args": "Python bool, default False. When True distribution\nparameters are checked for validity despite possibly degrading runtime\nperformance. When False invalid inputs may silently render incorrect\noutputs.", "allow_nan_stats": "Python bool, default True. When True,\nstatistics (e.g., mean, mode, variance) use the value \"NaN\" to\nindicate the result is undefined. When False, an exception is raised\nif one or more of the statistic's batch members are undefined.", "name": "Python str name prefixed to Ops created by this class."}, "Raises": {"ValueError": "If p and logits are passed, or if neither are passed."}, "Attributes": {"allow_nan_stats": "Python bool describing behavior when a stat is undefined.\nStats return +/- infinity when it makes sense. E.g., the variance of a\nCauchy distribution is infinity. However, sometimes the statistic is\nundefined, e.g., if a distribution's pdf does not achieve a maximum within\nthe support of the distribution, the mode is undefined. If the mean is\nundefined, then by definition the variance is undefined. E.g. the mean for\nStudent's T for df = 1 is undefined (no clear way to say it is either + or -\ninfinity), so the variance = E[(X - mean)**2] is also undefined.", "batch_shape": "Shape of a single sample from a single event index as a TensorShape.\nMay be partially defined or unknown.\nThe batch dimensions are indexes into independent, non-identical\nparameterizations of this distribution.", "dtype": "The DType of Tensors handled by this Distribution.", "event_shape": "Shape of a single sample from a single batch as a TensorShape.\nMay be partially defined or unknown.", "logits": "Log-odds of a 1 outcome (vs 0).", "name": "Name prepended to all ops created by this Distribution.", "parameters": "Dictionary of parameters used to instantiate this Distribution.", "probs": "Probability of a 1 outcome (vs 0).", "reparameterization_type": "Describes how samples from the distribution are reparameterized.\nCurrently this is one of the static instances\ndistributions.FULLY_REPARAMETERIZED\nor distributions.NOT_REPARAMETERIZED.", "validate_args": "Python bool indicating possibly expensive checks are enabled."}}, "tf.compat.v1.distributions.Beta": {"description": "Beta distribution.", "Args": {"concentration1": "Positive floating-point Tensor indicating mean\nnumber of successes; aka \"alpha\". Implies self.dtype and\nself.batch_shape, i.e.,\nconcentration1.shape = [N1, N2, ..., Nm] = self.batch_shape.", "concentration0": "Positive floating-point Tensor indicating mean\nnumber of failures; aka \"beta\". Otherwise has same semantics as\nconcentration1.", "validate_args": "Python bool, default False. When True distribution\nparameters are checked for validity despite possibly degrading runtime\nperformance. When False invalid inputs may silently render incorrect\noutputs.", "allow_nan_stats": "Python bool, default True. When True, statistics\n(e.g., mean, mode, variance) use the value \"NaN\" to indicate the\nresult is undefined. When False, an exception is raised if one or\nmore of the statistic's batch members are undefined.", "name": "Python str name prefixed to Ops created by this class."}, "Attributes": {"allow_nan_stats": "Python bool describing behavior when a stat is undefined.\nStats return +/- infinity when it makes sense. E.g., the variance of a\nCauchy distribution is infinity. However, sometimes the statistic is\nundefined, e.g., if a distribution's pdf does not achieve a maximum within\nthe support of the distribution, the mode is undefined. If the mean is\nundefined, then by definition the variance is undefined. E.g. the mean for\nStudent's T for df = 1 is undefined (no clear way to say it is either + or -\ninfinity), so the variance = E[(X - mean)**2] is also undefined.", "batch_shape": "Shape of a single sample from a single event index as a TensorShape.\nMay be partially defined or unknown.\nThe batch dimensions are indexes into independent, non-identical\nparameterizations of this distribution.", "concentration0": "Concentration parameter associated with a 0 outcome.", "concentration1": "Concentration parameter associated with a 1 outcome.", "dtype": "The DType of Tensors handled by this Distribution.", "event_shape": "Shape of a single sample from a single batch as a TensorShape.\nMay be partially defined or unknown.", "name": "Name prepended to all ops created by this Distribution.", "parameters": "Dictionary of parameters used to instantiate this Distribution.", "reparameterization_type": "Describes how samples from the distribution are reparameterized.\nCurrently this is one of the static instances\ndistributions.FULLY_REPARAMETERIZED\nor distributions.NOT_REPARAMETERIZED.", "total_concentration": "Sum of concentration parameters.", "validate_args": "Python bool indicating possibly expensive checks are enabled."}}, "tf.compat.v1.distributions.Categorical": {"description": "Categorical distribution.", "Args": {"logits": "An N-D Tensor, N >= 1, representing the log probabilities\nof a set of Categorical distributions. The first N - 1 dimensions\nindex into a batch of independent distributions and the last dimension\nrepresents a vector of logits for each class. Only one of logits or\nprobs should be passed in.", "probs": "An N-D Tensor, N >= 1, representing the probabilities\nof a set of Categorical distributions. The first N - 1 dimensions\nindex into a batch of independent distributions and the last dimension\nrepresents a vector of probabilities for each class. Only one of\nlogits or probs should be passed in.", "dtype": "The type of the event samples (default: int32).", "validate_args": "Python bool, default False. When True distribution\nparameters are checked for validity despite possibly degrading runtime\nperformance. When False invalid inputs may silently render incorrect\noutputs.", "allow_nan_stats": "Python bool, default True. When True, statistics\n(e.g., mean, mode, variance) use the value \"NaN\" to indicate the\nresult is undefined. When False, an exception is raised if one or\nmore of the statistic's batch members are undefined.", "name": "Python str name prefixed to Ops created by this class."}, "Attributes": {"allow_nan_stats": "Python bool describing behavior when a stat is undefined.\nStats return +/- infinity when it makes sense. E.g., the variance of a\nCauchy distribution is infinity. However, sometimes the statistic is\nundefined, e.g., if a distribution's pdf does not achieve a maximum within\nthe support of the distribution, the mode is undefined. If the mean is\nundefined, then by definition the variance is undefined. E.g. the mean for\nStudent's T for df = 1 is undefined (no clear way to say it is either + or -\ninfinity), so the variance = E[(X - mean)**2] is also undefined.", "batch_shape": "Shape of a single sample from a single event index as a TensorShape.\nMay be partially defined or unknown.\nThe batch dimensions are indexes into independent, non-identical\nparameterizations of this distribution.", "dtype": "The DType of Tensors handled by this Distribution.", "event_shape": "Shape of a single sample from a single batch as a TensorShape.\nMay be partially defined or unknown.", "event_size": "Scalar int32 tensor: the number of classes.", "logits": "Vector of coordinatewise logits.", "name": "Name prepended to all ops created by this Distribution.", "parameters": "Dictionary of parameters used to instantiate this Distribution.", "probs": "Vector of coordinatewise probabilities.", "reparameterization_type": "Describes how samples from the distribution are reparameterized.\nCurrently this is one of the static instances\ndistributions.FULLY_REPARAMETERIZED\nor distributions.NOT_REPARAMETERIZED.", "validate_args": "Python bool indicating possibly expensive checks are enabled."}}, "tf.compat.v1.distributions.Dirichlet": {"description": "Dirichlet distribution.", "Args": {"concentration": "Positive floating-point Tensor indicating mean number\nof class occurrences; aka \"alpha\". Implies self.dtype, and\nself.batch_shape, self.event_shape, i.e., if\nconcentration.shape = [N1, N2, ..., Nm, k] then\nbatch_shape = [N1, N2, ..., Nm] and\nevent_shape = [k].", "validate_args": "Python bool, default False. When True distribution\nparameters are checked for validity despite possibly degrading runtime\nperformance. When False invalid inputs may silently render incorrect\noutputs.", "allow_nan_stats": "Python bool, default True. When True, statistics\n(e.g., mean, mode, variance) use the value \"NaN\" to indicate the\nresult is undefined. When False, an exception is raised if one or\nmore of the statistic's batch members are undefined.", "name": "Python str name prefixed to Ops created by this class."}, "Attributes": {"allow_nan_stats": "Python bool describing behavior when a stat is undefined.\nStats return +/- infinity when it makes sense. E.g., the variance of a\nCauchy distribution is infinity. However, sometimes the statistic is\nundefined, e.g., if a distribution's pdf does not achieve a maximum within\nthe support of the distribution, the mode is undefined. If the mean is\nundefined, then by definition the variance is undefined. E.g. the mean for\nStudent's T for df = 1 is undefined (no clear way to say it is either + or -\ninfinity), so the variance = E[(X - mean)**2] is also undefined.", "batch_shape": "Shape of a single sample from a single event index as a TensorShape.\nMay be partially defined or unknown.\nThe batch dimensions are indexes into independent, non-identical\nparameterizations of this distribution.", "concentration": "Concentration parameter; expected counts for that coordinate.", "dtype": "The DType of Tensors handled by this Distribution.", "event_shape": "Shape of a single sample from a single batch as a TensorShape.\nMay be partially defined or unknown.", "name": "Name prepended to all ops created by this Distribution.", "parameters": "Dictionary of parameters used to instantiate this Distribution.", "reparameterization_type": "Describes how samples from the distribution are reparameterized.\nCurrently this is one of the static instances\ndistributions.FULLY_REPARAMETERIZED\nor distributions.NOT_REPARAMETERIZED.", "total_concentration": "Sum of last dim of concentration parameter.", "validate_args": "Python bool indicating possibly expensive checks are enabled."}}, "tf.compat.v1.distributions.DirichletMultinomial": {"description": "Dirichlet-Multinomial compound distribution.", "Args": {"total_count": "Non-negative floating point tensor, whose dtype is the same\nas concentration. The shape is broadcastable to [N1,..., Nm] with\nm >= 0. Defines this as a batch of N1 x ... x Nm different\nDirichlet multinomial distributions. Its components should be equal to\ninteger values.", "concentration": "Positive floating point tensor, whose dtype is the\nsame as n with shape broadcastable to [N1,..., Nm, K] m >= 0.\nDefines this as a batch of N1 x ... x Nm different K class Dirichlet\nmultinomial distributions.", "validate_args": "Python bool, default False. When True distribution\nparameters are checked for validity despite possibly degrading runtime\nperformance. When False invalid inputs may silently render incorrect\noutputs.", "allow_nan_stats": "Python bool, default True. When True, statistics\n(e.g., mean, mode, variance) use the value \"NaN\" to indicate the\nresult is undefined. When False, an exception is raised if one or\nmore of the statistic's batch members are undefined.", "name": "Python str name prefixed to Ops created by this class."}, "Attributes": {"allow_nan_stats": "Python bool describing behavior when a stat is undefined.\nStats return +/- infinity when it makes sense. E.g., the variance of a\nCauchy distribution is infinity. However, sometimes the statistic is\nundefined, e.g., if a distribution's pdf does not achieve a maximum within\nthe support of the distribution, the mode is undefined. If the mean is\nundefined, then by definition the variance is undefined. E.g. the mean for\nStudent's T for df = 1 is undefined (no clear way to say it is either + or -\ninfinity), so the variance = E[(X - mean)**2] is also undefined.", "batch_shape": "Shape of a single sample from a single event index as a TensorShape.\nMay be partially defined or unknown.\nThe batch dimensions are indexes into independent, non-identical\nparameterizations of this distribution.", "concentration": "Concentration parameter; expected prior counts for that coordinate.", "dtype": "The DType of Tensors handled by this Distribution.", "event_shape": "Shape of a single sample from a single batch as a TensorShape.\nMay be partially defined or unknown.", "name": "Name prepended to all ops created by this Distribution.", "parameters": "Dictionary of parameters used to instantiate this Distribution.", "reparameterization_type": "Describes how samples from the distribution are reparameterized.\nCurrently this is one of the static instances\ndistributions.FULLY_REPARAMETERIZED\nor distributions.NOT_REPARAMETERIZED.", "total_concentration": "Sum of last dim of concentration parameter.", "total_count": "Number of trials used to construct a sample.", "validate_args": "Python bool indicating possibly expensive checks are enabled."}}, "tf.compat.v1.distributions.Distribution": {"description": "A generic probability distribution base class.", "Args": {"dtype": "The type of the event samples. None implies no type-enforcement.", "reparameterization_type": "Instance of ReparameterizationType.\nIf distributions.FULLY_REPARAMETERIZED, this\nDistribution can be reparameterized in terms of some standard\ndistribution with a function whose Jacobian is constant for the support\nof the standard distribution. If distributions.NOT_REPARAMETERIZED,\nthen no such reparameterization is available.", "validate_args": "Python bool, default False. When True distribution\nparameters are checked for validity despite possibly degrading runtime\nperformance. When False invalid inputs may silently render incorrect\noutputs.", "allow_nan_stats": "Python bool, default True. When True, statistics\n(e.g., mean, mode, variance) use the value \"NaN\" to indicate the\nresult is undefined. When False, an exception is raised if one or\nmore of the statistic's batch members are undefined.", "parameters": "Python dict of parameters used to instantiate this\nDistribution.", "graph_parents": "Python list of graph prerequisites of this\nDistribution.", "name": "Python str name prefixed to Ops created by this class. Default:\nsubclass name."}, "Raises": {"ValueError": "if any member of graph_parents is None or not a Tensor."}, "Attributes": {"allow_nan_stats": "Python bool describing behavior when a stat is undefined.\nStats return +/- infinity when it makes sense. E.g., the variance of a\nCauchy distribution is infinity. However, sometimes the statistic is\nundefined, e.g., if a distribution's pdf does not achieve a maximum within\nthe support of the distribution, the mode is undefined. If the mean is\nundefined, then by definition the variance is undefined. E.g. the mean for\nStudent's T for df = 1 is undefined (no clear way to say it is either + or -\ninfinity), so the variance = E[(X - mean)**2] is also undefined.", "batch_shape": "Shape of a single sample from a single event index as a TensorShape.\nMay be partially defined or unknown.\nThe batch dimensions are indexes into independent, non-identical\nparameterizations of this distribution.", "dtype": "The DType of Tensors handled by this Distribution.", "event_shape": "Shape of a single sample from a single batch as a TensorShape.\nMay be partially defined or unknown.", "name": "Name prepended to all ops created by this Distribution.", "parameters": "Dictionary of parameters used to instantiate this Distribution.", "reparameterization_type": "Describes how samples from the distribution are reparameterized.\nCurrently this is one of the static instances\ndistributions.FULLY_REPARAMETERIZED\nor distributions.NOT_REPARAMETERIZED.", "validate_args": "Python bool indicating possibly expensive checks are enabled."}}, "tf.compat.v1.distributions.Exponential": {"description": "Exponential distribution.", "Args": {"rate": "Floating point tensor, equivalent to 1 / mean. Must contain only\npositive values.", "validate_args": "Python bool, default False. When True distribution\nparameters are checked for validity despite possibly degrading runtime\nperformance. When False invalid inputs may silently render incorrect\noutputs.", "allow_nan_stats": "Python bool, default True. When True, statistics\n(e.g., mean, mode, variance) use the value \"NaN\" to indicate the\nresult is undefined. When False, an exception is raised if one or\nmore of the statistic's batch members are undefined.", "name": "Python str name prefixed to Ops created by this class."}, "Attributes": {"allow_nan_stats": "Python bool describing behavior when a stat is undefined.\nStats return +/- infinity when it makes sense. E.g., the variance of a\nCauchy distribution is infinity. However, sometimes the statistic is\nundefined, e.g., if a distribution's pdf does not achieve a maximum within\nthe support of the distribution, the mode is undefined. If the mean is\nundefined, then by definition the variance is undefined. E.g. the mean for\nStudent's T for df = 1 is undefined (no clear way to say it is either + or -\ninfinity), so the variance = E[(X - mean)**2] is also undefined.", "batch_shape": "Shape of a single sample from a single event index as a TensorShape.\nMay be partially defined or unknown.\nThe batch dimensions are indexes into independent, non-identical\nparameterizations of this distribution.", "concentration": "Concentration parameter.", "dtype": "The DType of Tensors handled by this Distribution.", "event_shape": "Shape of a single sample from a single batch as a TensorShape.\nMay be partially defined or unknown.", "name": "Name prepended to all ops created by this Distribution.", "parameters": "Dictionary of parameters used to instantiate this Distribution.", "rate": "Rate parameter.", "reparameterization_type": "Describes how samples from the distribution are reparameterized.\nCurrently this is one of the static instances\ndistributions.FULLY_REPARAMETERIZED\nor distributions.NOT_REPARAMETERIZED.", "validate_args": "Python bool indicating possibly expensive checks are enabled."}}, "tf.compat.v1.distributions.Gamma": {"description": "Gamma distribution.", "Args": {"concentration": "Floating point tensor, the concentration params of the\ndistribution(s). Must contain only positive values.", "rate": "Floating point tensor, the inverse scale params of the\ndistribution(s). Must contain only positive values.", "validate_args": "Python bool, default False. When True distribution\nparameters are checked for validity despite possibly degrading runtime\nperformance. When False invalid inputs may silently render incorrect\noutputs.", "allow_nan_stats": "Python bool, default True. When True, statistics\n(e.g., mean, mode, variance) use the value \"NaN\" to indicate the\nresult is undefined. When False, an exception is raised if one or\nmore of the statistic's batch members are undefined.", "name": "Python str name prefixed to Ops created by this class."}, "Raises": {"TypeError": "if concentration and rate are different dtypes."}, "Attributes": {"allow_nan_stats": "Python bool describing behavior when a stat is undefined.\nStats return +/- infinity when it makes sense. E.g., the variance of a\nCauchy distribution is infinity. However, sometimes the statistic is\nundefined, e.g., if a distribution's pdf does not achieve a maximum within\nthe support of the distribution, the mode is undefined. If the mean is\nundefined, then by definition the variance is undefined. E.g. the mean for\nStudent's T for df = 1 is undefined (no clear way to say it is either + or -\ninfinity), so the variance = E[(X - mean)**2] is also undefined.", "batch_shape": "Shape of a single sample from a single event index as a TensorShape.\nMay be partially defined or unknown.\nThe batch dimensions are indexes into independent, non-identical\nparameterizations of this distribution.", "concentration": "Concentration parameter.", "dtype": "The DType of Tensors handled by this Distribution.", "event_shape": "Shape of a single sample from a single batch as a TensorShape.\nMay be partially defined or unknown.", "name": "Name prepended to all ops created by this Distribution.", "parameters": "Dictionary of parameters used to instantiate this Distribution.", "rate": "Rate parameter.", "reparameterization_type": "Describes how samples from the distribution are reparameterized.\nCurrently this is one of the static instances\ndistributions.FULLY_REPARAMETERIZED\nor distributions.NOT_REPARAMETERIZED.", "validate_args": "Python bool indicating possibly expensive checks are enabled."}}, "tf.compat.v1.distributions.Laplace": {"description": "The Laplace distribution with location loc and scale parameters.", "Args": {"loc": "Floating point tensor which characterizes the location (center)\nof the distribution.", "scale": "Positive floating point tensor which characterizes the spread of\nthe distribution.", "validate_args": "Python bool, default False. When True distribution\nparameters are checked for validity despite possibly degrading runtime\nperformance. When False invalid inputs may silently render incorrect\noutputs.", "allow_nan_stats": "Python bool, default True. When True,\nstatistics (e.g., mean, mode, variance) use the value \"NaN\" to\nindicate the result is undefined. When False, an exception is raised\nif one or more of the statistic's batch members are undefined.", "name": "Python str name prefixed to Ops created by this class."}, "Raises": {"TypeError": "if loc and scale are of different dtype."}, "Attributes": {"allow_nan_stats": "Python bool describing behavior when a stat is undefined.\nStats return +/- infinity when it makes sense. E.g., the variance of a\nCauchy distribution is infinity. However, sometimes the statistic is\nundefined, e.g., if a distribution's pdf does not achieve a maximum within\nthe support of the distribution, the mode is undefined. If the mean is\nundefined, then by definition the variance is undefined. E.g. the mean for\nStudent's T for df = 1 is undefined (no clear way to say it is either + or -\ninfinity), so the variance = E[(X - mean)**2] is also undefined.", "batch_shape": "Shape of a single sample from a single event index as a TensorShape.\nMay be partially defined or unknown.\nThe batch dimensions are indexes into independent, non-identical\nparameterizations of this distribution.", "dtype": "The DType of Tensors handled by this Distribution.", "event_shape": "Shape of a single sample from a single batch as a TensorShape.\nMay be partially defined or unknown.", "loc": "Distribution parameter for the location.", "name": "Name prepended to all ops created by this Distribution.", "parameters": "Dictionary of parameters used to instantiate this Distribution.", "reparameterization_type": "Describes how samples from the distribution are reparameterized.\nCurrently this is one of the static instances\ndistributions.FULLY_REPARAMETERIZED\nor distributions.NOT_REPARAMETERIZED.", "scale": "Distribution parameter for scale.", "validate_args": "Python bool indicating possibly expensive checks are enabled."}}, "tf.compat.v1.distributions.Multinomial": {"description": "Multinomial distribution.", "Args": {"total_count": "Non-negative floating point tensor with shape broadcastable\nto [N1,..., Nm] with m >= 0. Defines this as a batch of\nN1 x ... x Nm different Multinomial distributions. Its components\nshould be equal to integer values.", "logits": "Floating point tensor representing unnormalized log-probabilities\nof a positive event with shape broadcastable to\n[N1,..., Nm, K] m >= 0, and the same dtype as total_count. Defines\nthis as a batch of N1 x ... x Nm different K class Multinomial\ndistributions. Only one of logits or probs should be passed in.", "probs": "Positive floating point tensor with shape broadcastable to\n[N1,..., Nm, K] m >= 0 and same dtype as total_count. Defines\nthis as a batch of N1 x ... x Nm different K class Multinomial\ndistributions. probs's components in the last portion of its shape\nshould sum to 1. Only one of logits or probs should be passed in.", "validate_args": "Python bool, default False. When True distribution\nparameters are checked for validity despite possibly degrading runtime\nperformance. When False invalid inputs may silently render incorrect\noutputs.", "allow_nan_stats": "Python bool, default True. When True, statistics\n(e.g., mean, mode, variance) use the value \"NaN\" to indicate the\nresult is undefined. When False, an exception is raised if one or\nmore of the statistic's batch members are undefined.", "name": "Python str name prefixed to Ops created by this class."}, "Attributes": {"allow_nan_stats": "Python bool describing behavior when a stat is undefined.\nStats return +/- infinity when it makes sense. E.g., the variance of a\nCauchy distribution is infinity. However, sometimes the statistic is\nundefined, e.g., if a distribution's pdf does not achieve a maximum within\nthe support of the distribution, the mode is undefined. If the mean is\nundefined, then by definition the variance is undefined. E.g. the mean for\nStudent's T for df = 1 is undefined (no clear way to say it is either + or -\ninfinity), so the variance = E[(X - mean)**2] is also undefined.", "batch_shape": "Shape of a single sample from a single event index as a TensorShape.\nMay be partially defined or unknown.\nThe batch dimensions are indexes into independent, non-identical\nparameterizations of this distribution.", "dtype": "The DType of Tensors handled by this Distribution.", "event_shape": "Shape of a single sample from a single batch as a TensorShape.\nMay be partially defined or unknown.", "logits": "Vector of coordinatewise logits.", "name": "Name prepended to all ops created by this Distribution.", "parameters": "Dictionary of parameters used to instantiate this Distribution.", "probs": "Probability of drawing a 1 in that coordinate.", "reparameterization_type": "Describes how samples from the distribution are reparameterized.\nCurrently this is one of the static instances\ndistributions.FULLY_REPARAMETERIZED\nor distributions.NOT_REPARAMETERIZED.", "total_count": "Number of trials used to construct a sample.", "validate_args": "Python bool indicating possibly expensive checks are enabled."}}, "tf.compat.v1.distributions.Normal": {"description": "The Normal distribution with location loc and scale parameters.", "Args": {"loc": "Floating point tensor; the means of the distribution(s).", "scale": "Floating point tensor; the stddevs of the distribution(s).\nMust contain only positive values.", "validate_args": "Python bool, default False. When True distribution\nparameters are checked for validity despite possibly degrading runtime\nperformance. When False invalid inputs may silently render incorrect\noutputs.", "allow_nan_stats": "Python bool, default True. When True,\nstatistics (e.g., mean, mode, variance) use the value \"NaN\" to\nindicate the result is undefined. When False, an exception is raised\nif one or more of the statistic's batch members are undefined.", "name": "Python str name prefixed to Ops created by this class."}, "Raises": {"TypeError": "if loc and scale have different dtype."}, "Attributes": {"allow_nan_stats": "Python bool describing behavior when a stat is undefined.\nStats return +/- infinity when it makes sense. E.g., the variance of a\nCauchy distribution is infinity. However, sometimes the statistic is\nundefined, e.g., if a distribution's pdf does not achieve a maximum within\nthe support of the distribution, the mode is undefined. If the mean is\nundefined, then by definition the variance is undefined. E.g. the mean for\nStudent's T for df = 1 is undefined (no clear way to say it is either + or -\ninfinity), so the variance = E[(X - mean)**2] is also undefined.", "batch_shape": "Shape of a single sample from a single event index as a TensorShape.\nMay be partially defined or unknown.\nThe batch dimensions are indexes into independent, non-identical\nparameterizations of this distribution.", "dtype": "The DType of Tensors handled by this Distribution.", "event_shape": "Shape of a single sample from a single batch as a TensorShape.\nMay be partially defined or unknown.", "loc": "Distribution parameter for the mean.", "name": "Name prepended to all ops created by this Distribution.", "parameters": "Dictionary of parameters used to instantiate this Distribution.", "reparameterization_type": "Describes how samples from the distribution are reparameterized.\nCurrently this is one of the static instances\ndistributions.FULLY_REPARAMETERIZED\nor distributions.NOT_REPARAMETERIZED.", "scale": "Distribution parameter for standard deviation.", "validate_args": "Python bool indicating possibly expensive checks are enabled."}}, "tf.compat.v1.distributions.RegisterKL": {"description": "Decorator to register a KL divergence implementation function.", "Args": {"dist_cls_a": "the class of the first argument of the KL divergence.", "dist_cls_b": "the class of the second argument of the KL divergence."}}, "tf.compat.v1.distributions.ReparameterizationType": {"description": "Instances of this class represent how sampling is reparameterized."}, "tf.compat.v1.distributions.StudentT": {"description": "Student&#39;s t-distribution.", "Args": {"df": "Floating-point Tensor. The degrees of freedom of the\ndistribution(s). df must contain only positive values.", "loc": "Floating-point Tensor. The mean(s) of the distribution(s).", "scale": "Floating-point Tensor. The scaling factor(s) for the\ndistribution(s). Note that scale is not technically the standard\ndeviation of this distribution but has semantics more similar to\nstandard deviation than variance.", "validate_args": "Python bool, default False. When True distribution\nparameters are checked for validity despite possibly degrading runtime\nperformance. When False invalid inputs may silently render incorrect\noutputs.", "allow_nan_stats": "Python bool, default True. When True,\nstatistics (e.g., mean, mode, variance) use the value \"NaN\" to\nindicate the result is undefined. When False, an exception is raised\nif one or more of the statistic's batch members are undefined.", "name": "Python str name prefixed to Ops created by this class."}, "Raises": {"TypeError": "if loc and scale are different dtypes."}, "Attributes": {"allow_nan_stats": "Python bool describing behavior when a stat is undefined.\nStats return +/- infinity when it makes sense. E.g., the variance of a\nCauchy distribution is infinity. However, sometimes the statistic is\nundefined, e.g., if a distribution's pdf does not achieve a maximum within\nthe support of the distribution, the mode is undefined. If the mean is\nundefined, then by definition the variance is undefined. E.g. the mean for\nStudent's T for df = 1 is undefined (no clear way to say it is either + or -\ninfinity), so the variance = E[(X - mean)**2] is also undefined.", "batch_shape": "Shape of a single sample from a single event index as a TensorShape.\nMay be partially defined or unknown.\nThe batch dimensions are indexes into independent, non-identical\nparameterizations of this distribution.", "df": "Degrees of freedom in these Student's t distribution(s).", "dtype": "The DType of Tensors handled by this Distribution.", "event_shape": "Shape of a single sample from a single batch as a TensorShape.\nMay be partially defined or unknown.", "loc": "Locations of these Student's t distribution(s).", "name": "Name prepended to all ops created by this Distribution.", "parameters": "Dictionary of parameters used to instantiate this Distribution.", "reparameterization_type": "Describes how samples from the distribution are reparameterized.\nCurrently this is one of the static instances\ndistributions.FULLY_REPARAMETERIZED\nor distributions.NOT_REPARAMETERIZED.", "scale": "Scaling factors of these Student's t distribution(s).", "validate_args": "Python bool indicating possibly expensive checks are enabled."}}, "tf.compat.v1.distributions.Uniform": {"description": "Uniform distribution with low and high parameters.", "Args": {"low": "Floating point tensor, lower boundary of the output interval. Must\nhave low < high.", "high": "Floating point tensor, upper boundary of the output interval. Must\nhave low < high.", "validate_args": "Python bool, default False. When True distribution\nparameters are checked for validity despite possibly degrading runtime\nperformance. When False invalid inputs may silently render incorrect\noutputs.", "allow_nan_stats": "Python bool, default True. When True, statistics\n(e.g., mean, mode, variance) use the value \"NaN\" to indicate the\nresult is undefined. When False, an exception is raised if one or\nmore of the statistic's batch members are undefined.", "name": "Python str name prefixed to Ops created by this class."}, "Raises": {"InvalidArgumentError": "if low >= high and validate_args=False."}, "Attributes": {"allow_nan_stats": "Python bool describing behavior when a stat is undefined.\nStats return +/- infinity when it makes sense. E.g., the variance of a\nCauchy distribution is infinity. However, sometimes the statistic is\nundefined, e.g., if a distribution's pdf does not achieve a maximum within\nthe support of the distribution, the mode is undefined. If the mean is\nundefined, then by definition the variance is undefined. E.g. the mean for\nStudent's T for df = 1 is undefined (no clear way to say it is either + or -\ninfinity), so the variance = E[(X - mean)**2] is also undefined.", "batch_shape": "Shape of a single sample from a single event index as a TensorShape.\nMay be partially defined or unknown.\nThe batch dimensions are indexes into independent, non-identical\nparameterizations of this distribution.", "dtype": "The DType of Tensors handled by this Distribution.", "event_shape": "Shape of a single sample from a single batch as a TensorShape.\nMay be partially defined or unknown.", "high": "Upper boundary of the output interval.", "low": "Lower boundary of the output interval.", "name": "Name prepended to all ops created by this Distribution.", "parameters": "Dictionary of parameters used to instantiate this Distribution.", "reparameterization_type": "Describes how samples from the distribution are reparameterized.\nCurrently this is one of the static instances\ndistributions.FULLY_REPARAMETERIZED\nor distributions.NOT_REPARAMETERIZED.", "validate_args": "Python bool indicating possibly expensive checks are enabled."}}, "tf.compat.v1.distributions.kl_divergence": {"description": "Get the KL-divergence KL(distribution_a || distribution_b). (deprecated)", "Args": {"distribution_a": "The first distribution.", "distribution_b": "The second distribution.", "allow_nan_stats": "Python bool, default True. When True,\nstatistics (e.g., mean, mode, variance) use the value \"NaN\" to\nindicate the result is undefined. When False, an exception is raised\nif one or more of the statistic's batch members are undefined.", "name": "Python str name prefixed to Ops created by this class."}, "Returns": "A Tensor with the batchwise KL-divergence between distribution_a\nand distribution_b.", "Raises": {"NotImplementedError": "If no KL method is defined for distribution types\nof distribution_a and distribution_b."}}}, "tf.compat.v1.dtypes": {}, "tf.compat.v1.errors": {"tf.compat.v1.errors.error_code_from_exception_type": {}, "tf.compat.v1.errors.exception_type_from_error_code": {}, "tf.compat.v1.errors.raise_exception_on_not_ok_status": {"description": "Context manager to check for C API status."}}, "tf.compat.v1.estimator": {"tf.compat.v1.estimator.BaselineClassifier": {"description": "A classifier that can establish a simple baseline.", "Args": {"model_fn": "Model function. Follows the signature:\n\nfeatures -- This is the first item returned from the input_fn\npassed to train, evaluate, and predict. This should be a\nsingle tf.Tensor or dict of same.\nlabels -- This is the second item returned from the input_fn\npassed to train, evaluate, and predict. This should be a\nsingle tf.Tensor or dict of same (for multi-head models). If\nmode is tf.estimator.ModeKeys.PREDICT, labels=None will be\npassed. If the model_fn's signature does not accept mode, the\nmodel_fn must still be able to handle labels=None.\nmode -- Optional. Specifies if this is training, evaluation or\nprediction. See tf.estimator.ModeKeys.\nparams -- Optional dict of hyperparameters.  Will receive what is\npassed to Estimator in params parameter. This allows to configure\nEstimators from hyper parameter tuning.\nconfig -- Optional estimator.RunConfig object. Will receive what\nis passed to Estimator as its config parameter, or a default\nvalue. Allows setting up things in your model_fn based on\nconfiguration such as num_ps_replicas, or model_dir.\nReturns -- tf.estimator.EstimatorSpec", "model_dir": "Directory to save model parameters, graph and etc. This can\nalso be used to load checkpoints from the directory into an estimator to\ncontinue training a previously saved model. If PathLike object, the\npath will be resolved. If None, the model_dir in config will be used\nif set. If both are set, they must be same. If both are None, a\ntemporary directory will be used.", "config": "estimator.RunConfig configuration object.", "params": "dict of hyper parameters that will be passed into model_fn.\nKeys are names of parameters, values are basic python types.", "warm_start_from": "Optional string filepath to a checkpoint or SavedModel to\nwarm-start from, or a tf.estimator.WarmStartSettings object to fully\nconfigure warm-starting.  If None, only TRAINABLE variables are\nwarm-started.  If the string filepath is provided instead of a\ntf.estimator.WarmStartSettings, then all variables are warm-started,\nand it is assumed that vocabularies and tf.Tensor names are unchanged."}, "Raises": {"ValueError": "if this is called via a subclass and if that class overrides\na member of Estimator."}, "Attributes": {"config": "", "model_dir": "", "model_fn": "Returns the model_fn which is bound to self.params.", "params": ""}}, "tf.compat.v1.estimator.BaselineEstimator": {"description": "An estimator that can establish a simple baseline.", "Args": {"model_fn": "Model function. Follows the signature:\n\nfeatures -- This is the first item returned from the input_fn\npassed to train, evaluate, and predict. This should be a\nsingle tf.Tensor or dict of same.\nlabels -- This is the second item returned from the input_fn\npassed to train, evaluate, and predict. This should be a\nsingle tf.Tensor or dict of same (for multi-head models). If\nmode is tf.estimator.ModeKeys.PREDICT, labels=None will be\npassed. If the model_fn's signature does not accept mode, the\nmodel_fn must still be able to handle labels=None.\nmode -- Optional. Specifies if this is training, evaluation or\nprediction. See tf.estimator.ModeKeys.\nparams -- Optional dict of hyperparameters.  Will receive what is\npassed to Estimator in params parameter. This allows to configure\nEstimators from hyper parameter tuning.\nconfig -- Optional estimator.RunConfig object. Will receive what\nis passed to Estimator as its config parameter, or a default\nvalue. Allows setting up things in your model_fn based on\nconfiguration such as num_ps_replicas, or model_dir.\nReturns -- tf.estimator.EstimatorSpec", "model_dir": "Directory to save model parameters, graph and etc. This can\nalso be used to load checkpoints from the directory into an estimator to\ncontinue training a previously saved model. If PathLike object, the\npath will be resolved. If None, the model_dir in config will be used\nif set. If both are set, they must be same. If both are None, a\ntemporary directory will be used.", "config": "estimator.RunConfig configuration object.", "params": "dict of hyper parameters that will be passed into model_fn.\nKeys are names of parameters, values are basic python types.", "warm_start_from": "Optional string filepath to a checkpoint or SavedModel to\nwarm-start from, or a tf.estimator.WarmStartSettings object to fully\nconfigure warm-starting.  If None, only TRAINABLE variables are\nwarm-started.  If the string filepath is provided instead of a\ntf.estimator.WarmStartSettings, then all variables are warm-started,\nand it is assumed that vocabularies and tf.Tensor names are unchanged."}, "Raises": {"ValueError": "if this is called via a subclass and if that class overrides\na member of Estimator."}, "Attributes": {"config": "", "model_dir": "", "model_fn": "Returns the model_fn which is bound to self.params.", "params": ""}}, "tf.compat.v1.estimator.BaselineRegressor": {"description": "A regressor that can establish a simple baseline.", "Args": {"model_fn": "Model function. Follows the signature:\n\nfeatures -- This is the first item returned from the input_fn\npassed to train, evaluate, and predict. This should be a\nsingle tf.Tensor or dict of same.\nlabels -- This is the second item returned from the input_fn\npassed to train, evaluate, and predict. This should be a\nsingle tf.Tensor or dict of same (for multi-head models). If\nmode is tf.estimator.ModeKeys.PREDICT, labels=None will be\npassed. If the model_fn's signature does not accept mode, the\nmodel_fn must still be able to handle labels=None.\nmode -- Optional. Specifies if this is training, evaluation or\nprediction. See tf.estimator.ModeKeys.\nparams -- Optional dict of hyperparameters.  Will receive what is\npassed to Estimator in params parameter. This allows to configure\nEstimators from hyper parameter tuning.\nconfig -- Optional estimator.RunConfig object. Will receive what\nis passed to Estimator as its config parameter, or a default\nvalue. Allows setting up things in your model_fn based on\nconfiguration such as num_ps_replicas, or model_dir.\nReturns -- tf.estimator.EstimatorSpec", "model_dir": "Directory to save model parameters, graph and etc. This can\nalso be used to load checkpoints from the directory into an estimator to\ncontinue training a previously saved model. If PathLike object, the\npath will be resolved. If None, the model_dir in config will be used\nif set. If both are set, they must be same. If both are None, a\ntemporary directory will be used.", "config": "estimator.RunConfig configuration object.", "params": "dict of hyper parameters that will be passed into model_fn.\nKeys are names of parameters, values are basic python types.", "warm_start_from": "Optional string filepath to a checkpoint or SavedModel to\nwarm-start from, or a tf.estimator.WarmStartSettings object to fully\nconfigure warm-starting.  If None, only TRAINABLE variables are\nwarm-started.  If the string filepath is provided instead of a\ntf.estimator.WarmStartSettings, then all variables are warm-started,\nand it is assumed that vocabularies and tf.Tensor names are unchanged."}, "Raises": {"ValueError": "if this is called via a subclass and if that class overrides\na member of Estimator."}, "Attributes": {"config": "", "model_dir": "", "model_fn": "Returns the model_fn which is bound to self.params.", "params": ""}}, "tf.compat.v1.estimator.DNNClassifier": {"description": "A classifier for TensorFlow DNN models.", "Args": {"model_fn": "Model function. Follows the signature:\n\nfeatures -- This is the first item returned from the input_fn\npassed to train, evaluate, and predict. This should be a\nsingle tf.Tensor or dict of same.\nlabels -- This is the second item returned from the input_fn\npassed to train, evaluate, and predict. This should be a\nsingle tf.Tensor or dict of same (for multi-head models). If\nmode is tf.estimator.ModeKeys.PREDICT, labels=None will be\npassed. If the model_fn's signature does not accept mode, the\nmodel_fn must still be able to handle labels=None.\nmode -- Optional. Specifies if this is training, evaluation or\nprediction. See tf.estimator.ModeKeys.\nparams -- Optional dict of hyperparameters.  Will receive what is\npassed to Estimator in params parameter. This allows to configure\nEstimators from hyper parameter tuning.\nconfig -- Optional estimator.RunConfig object. Will receive what\nis passed to Estimator as its config parameter, or a default\nvalue. Allows setting up things in your model_fn based on\nconfiguration such as num_ps_replicas, or model_dir.\nReturns -- tf.estimator.EstimatorSpec", "model_dir": "Directory to save model parameters, graph and etc. This can\nalso be used to load checkpoints from the directory into an estimator to\ncontinue training a previously saved model. If PathLike object, the\npath will be resolved. If None, the model_dir in config will be used\nif set. If both are set, they must be same. If both are None, a\ntemporary directory will be used.", "config": "estimator.RunConfig configuration object.", "params": "dict of hyper parameters that will be passed into model_fn.\nKeys are names of parameters, values are basic python types.", "warm_start_from": "Optional string filepath to a checkpoint or SavedModel to\nwarm-start from, or a tf.estimator.WarmStartSettings object to fully\nconfigure warm-starting.  If None, only TRAINABLE variables are\nwarm-started.  If the string filepath is provided instead of a\ntf.estimator.WarmStartSettings, then all variables are warm-started,\nand it is assumed that vocabularies and tf.Tensor names are unchanged."}, "Raises": {"ValueError": "if this is called via a subclass and if that class overrides\na member of Estimator."}, "Attributes": {"config": "", "model_dir": "", "model_fn": "Returns the model_fn which is bound to self.params.", "params": ""}}, "tf.compat.v1.estimator.DNNEstimator": {"description": "An estimator for TensorFlow DNN models with user-specified head.", "Args": {"model_fn": "Model function. Follows the signature:\n\nfeatures -- This is the first item returned from the input_fn\npassed to train, evaluate, and predict. This should be a\nsingle tf.Tensor or dict of same.\nlabels -- This is the second item returned from the input_fn\npassed to train, evaluate, and predict. This should be a\nsingle tf.Tensor or dict of same (for multi-head models). If\nmode is tf.estimator.ModeKeys.PREDICT, labels=None will be\npassed. If the model_fn's signature does not accept mode, the\nmodel_fn must still be able to handle labels=None.\nmode -- Optional. Specifies if this is training, evaluation or\nprediction. See tf.estimator.ModeKeys.\nparams -- Optional dict of hyperparameters.  Will receive what is\npassed to Estimator in params parameter. This allows to configure\nEstimators from hyper parameter tuning.\nconfig -- Optional estimator.RunConfig object. Will receive what\nis passed to Estimator as its config parameter, or a default\nvalue. Allows setting up things in your model_fn based on\nconfiguration such as num_ps_replicas, or model_dir.\nReturns -- tf.estimator.EstimatorSpec", "model_dir": "Directory to save model parameters, graph and etc. This can\nalso be used to load checkpoints from the directory into an estimator to\ncontinue training a previously saved model. If PathLike object, the\npath will be resolved. If None, the model_dir in config will be used\nif set. If both are set, they must be same. If both are None, a\ntemporary directory will be used.", "config": "estimator.RunConfig configuration object.", "params": "dict of hyper parameters that will be passed into model_fn.\nKeys are names of parameters, values are basic python types.", "warm_start_from": "Optional string filepath to a checkpoint or SavedModel to\nwarm-start from, or a tf.estimator.WarmStartSettings object to fully\nconfigure warm-starting.  If None, only TRAINABLE variables are\nwarm-started.  If the string filepath is provided instead of a\ntf.estimator.WarmStartSettings, then all variables are warm-started,\nand it is assumed that vocabularies and tf.Tensor names are unchanged."}, "Raises": {"ValueError": "if this is called via a subclass and if that class overrides\na member of Estimator."}, "Attributes": {"config": "", "model_dir": "", "model_fn": "Returns the model_fn which is bound to self.params.", "params": ""}}, "tf.compat.v1.estimator.DNNLinearCombinedClassifier": {"description": "An estimator for TensorFlow Linear and DNN joined classification models.", "Args": {"model_fn": "Model function. Follows the signature:\n\nfeatures -- This is the first item returned from the input_fn\npassed to train, evaluate, and predict. This should be a\nsingle tf.Tensor or dict of same.\nlabels -- This is the second item returned from the input_fn\npassed to train, evaluate, and predict. This should be a\nsingle tf.Tensor or dict of same (for multi-head models). If\nmode is tf.estimator.ModeKeys.PREDICT, labels=None will be\npassed. If the model_fn's signature does not accept mode, the\nmodel_fn must still be able to handle labels=None.\nmode -- Optional. Specifies if this is training, evaluation or\nprediction. See tf.estimator.ModeKeys.\nparams -- Optional dict of hyperparameters.  Will receive what is\npassed to Estimator in params parameter. This allows to configure\nEstimators from hyper parameter tuning.\nconfig -- Optional estimator.RunConfig object. Will receive what\nis passed to Estimator as its config parameter, or a default\nvalue. Allows setting up things in your model_fn based on\nconfiguration such as num_ps_replicas, or model_dir.\nReturns -- tf.estimator.EstimatorSpec", "model_dir": "Directory to save model parameters, graph and etc. This can\nalso be used to load checkpoints from the directory into an estimator to\ncontinue training a previously saved model. If PathLike object, the\npath will be resolved. If None, the model_dir in config will be used\nif set. If both are set, they must be same. If both are None, a\ntemporary directory will be used.", "config": "estimator.RunConfig configuration object.", "params": "dict of hyper parameters that will be passed into model_fn.\nKeys are names of parameters, values are basic python types.", "warm_start_from": "Optional string filepath to a checkpoint or SavedModel to\nwarm-start from, or a tf.estimator.WarmStartSettings object to fully\nconfigure warm-starting.  If None, only TRAINABLE variables are\nwarm-started.  If the string filepath is provided instead of a\ntf.estimator.WarmStartSettings, then all variables are warm-started,\nand it is assumed that vocabularies and tf.Tensor names are unchanged."}, "Raises": {"ValueError": "if this is called via a subclass and if that class overrides\na member of Estimator."}, "Attributes": {"config": "", "model_dir": "", "model_fn": "Returns the model_fn which is bound to self.params.", "params": ""}}, "tf.compat.v1.estimator.DNNLinearCombinedEstimator": {"description": "An estimator for TensorFlow Linear and DNN joined models with custom head.", "Args": {"model_fn": "Model function. Follows the signature:\n\nfeatures -- This is the first item returned from the input_fn\npassed to train, evaluate, and predict. This should be a\nsingle tf.Tensor or dict of same.\nlabels -- This is the second item returned from the input_fn\npassed to train, evaluate, and predict. This should be a\nsingle tf.Tensor or dict of same (for multi-head models). If\nmode is tf.estimator.ModeKeys.PREDICT, labels=None will be\npassed. If the model_fn's signature does not accept mode, the\nmodel_fn must still be able to handle labels=None.\nmode -- Optional. Specifies if this is training, evaluation or\nprediction. See tf.estimator.ModeKeys.\nparams -- Optional dict of hyperparameters.  Will receive what is\npassed to Estimator in params parameter. This allows to configure\nEstimators from hyper parameter tuning.\nconfig -- Optional estimator.RunConfig object. Will receive what\nis passed to Estimator as its config parameter, or a default\nvalue. Allows setting up things in your model_fn based on\nconfiguration such as num_ps_replicas, or model_dir.\nReturns -- tf.estimator.EstimatorSpec", "model_dir": "Directory to save model parameters, graph and etc. This can\nalso be used to load checkpoints from the directory into an estimator to\ncontinue training a previously saved model. If PathLike object, the\npath will be resolved. If None, the model_dir in config will be used\nif set. If both are set, they must be same. If both are None, a\ntemporary directory will be used.", "config": "estimator.RunConfig configuration object.", "params": "dict of hyper parameters that will be passed into model_fn.\nKeys are names of parameters, values are basic python types.", "warm_start_from": "Optional string filepath to a checkpoint or SavedModel to\nwarm-start from, or a tf.estimator.WarmStartSettings object to fully\nconfigure warm-starting.  If None, only TRAINABLE variables are\nwarm-started.  If the string filepath is provided instead of a\ntf.estimator.WarmStartSettings, then all variables are warm-started,\nand it is assumed that vocabularies and tf.Tensor names are unchanged."}, "Raises": {"ValueError": "if this is called via a subclass and if that class overrides\na member of Estimator."}, "Attributes": {"config": "", "model_dir": "", "model_fn": "Returns the model_fn which is bound to self.params.", "params": ""}}, "tf.compat.v1.estimator.DNNLinearCombinedRegressor": {"description": "An estimator for TensorFlow Linear and DNN joined models for regression.", "Args": {"model_fn": "Model function. Follows the signature:\n\nfeatures -- This is the first item returned from the input_fn\npassed to train, evaluate, and predict. This should be a\nsingle tf.Tensor or dict of same.\nlabels -- This is the second item returned from the input_fn\npassed to train, evaluate, and predict. This should be a\nsingle tf.Tensor or dict of same (for multi-head models). If\nmode is tf.estimator.ModeKeys.PREDICT, labels=None will be\npassed. If the model_fn's signature does not accept mode, the\nmodel_fn must still be able to handle labels=None.\nmode -- Optional. Specifies if this is training, evaluation or\nprediction. See tf.estimator.ModeKeys.\nparams -- Optional dict of hyperparameters.  Will receive what is\npassed to Estimator in params parameter. This allows to configure\nEstimators from hyper parameter tuning.\nconfig -- Optional estimator.RunConfig object. Will receive what\nis passed to Estimator as its config parameter, or a default\nvalue. Allows setting up things in your model_fn based on\nconfiguration such as num_ps_replicas, or model_dir.\nReturns -- tf.estimator.EstimatorSpec", "model_dir": "Directory to save model parameters, graph and etc. This can\nalso be used to load checkpoints from the directory into an estimator to\ncontinue training a previously saved model. If PathLike object, the\npath will be resolved. If None, the model_dir in config will be used\nif set. If both are set, they must be same. If both are None, a\ntemporary directory will be used.", "config": "estimator.RunConfig configuration object.", "params": "dict of hyper parameters that will be passed into model_fn.\nKeys are names of parameters, values are basic python types.", "warm_start_from": "Optional string filepath to a checkpoint or SavedModel to\nwarm-start from, or a tf.estimator.WarmStartSettings object to fully\nconfigure warm-starting.  If None, only TRAINABLE variables are\nwarm-started.  If the string filepath is provided instead of a\ntf.estimator.WarmStartSettings, then all variables are warm-started,\nand it is assumed that vocabularies and tf.Tensor names are unchanged."}, "Raises": {"ValueError": "if this is called via a subclass and if that class overrides\na member of Estimator."}, "Attributes": {"config": "", "model_dir": "", "model_fn": "Returns the model_fn which is bound to self.params.", "params": ""}}, "tf.compat.v1.estimator.DNNRegressor": {"description": "A regressor for TensorFlow DNN models.", "Args": {"model_fn": "Model function. Follows the signature:\n\nfeatures -- This is the first item returned from the input_fn\npassed to train, evaluate, and predict. This should be a\nsingle tf.Tensor or dict of same.\nlabels -- This is the second item returned from the input_fn\npassed to train, evaluate, and predict. This should be a\nsingle tf.Tensor or dict of same (for multi-head models). If\nmode is tf.estimator.ModeKeys.PREDICT, labels=None will be\npassed. If the model_fn's signature does not accept mode, the\nmodel_fn must still be able to handle labels=None.\nmode -- Optional. Specifies if this is training, evaluation or\nprediction. See tf.estimator.ModeKeys.\nparams -- Optional dict of hyperparameters.  Will receive what is\npassed to Estimator in params parameter. This allows to configure\nEstimators from hyper parameter tuning.\nconfig -- Optional estimator.RunConfig object. Will receive what\nis passed to Estimator as its config parameter, or a default\nvalue. Allows setting up things in your model_fn based on\nconfiguration such as num_ps_replicas, or model_dir.\nReturns -- tf.estimator.EstimatorSpec", "model_dir": "Directory to save model parameters, graph and etc. This can\nalso be used to load checkpoints from the directory into an estimator to\ncontinue training a previously saved model. If PathLike object, the\npath will be resolved. If None, the model_dir in config will be used\nif set. If both are set, they must be same. If both are None, a\ntemporary directory will be used.", "config": "estimator.RunConfig configuration object.", "params": "dict of hyper parameters that will be passed into model_fn.\nKeys are names of parameters, values are basic python types.", "warm_start_from": "Optional string filepath to a checkpoint or SavedModel to\nwarm-start from, or a tf.estimator.WarmStartSettings object to fully\nconfigure warm-starting.  If None, only TRAINABLE variables are\nwarm-started.  If the string filepath is provided instead of a\ntf.estimator.WarmStartSettings, then all variables are warm-started,\nand it is assumed that vocabularies and tf.Tensor names are unchanged."}, "Raises": {"ValueError": "if this is called via a subclass and if that class overrides\na member of Estimator."}, "Attributes": {"config": "", "model_dir": "", "model_fn": "Returns the model_fn which is bound to self.params.", "params": ""}}, "tf.compat.v1.estimator.Estimator": {"description": "Estimator class to train and evaluate TensorFlow models.", "Args": {"model_fn": "Model function. Follows the signature:\n\nfeatures -- This is the first item returned from the input_fn\npassed to train, evaluate, and predict. This should be a\nsingle tf.Tensor or dict of same.\nlabels -- This is the second item returned from the input_fn\npassed to train, evaluate, and predict. This should be a\nsingle tf.Tensor or dict of same (for multi-head models). If\nmode is tf.estimator.ModeKeys.PREDICT, labels=None will be\npassed. If the model_fn's signature does not accept mode, the\nmodel_fn must still be able to handle labels=None.\nmode -- Optional. Specifies if this is training, evaluation or\nprediction. See tf.estimator.ModeKeys.\nparams -- Optional dict of hyperparameters.  Will receive what is\npassed to Estimator in params parameter. This allows to configure\nEstimators from hyper parameter tuning.\nconfig -- Optional estimator.RunConfig object. Will receive what\nis passed to Estimator as its config parameter, or a default\nvalue. Allows setting up things in your model_fn based on\nconfiguration such as num_ps_replicas, or model_dir.\nReturns -- tf.estimator.EstimatorSpec", "model_dir": "Directory to save model parameters, graph and etc. This can\nalso be used to load checkpoints from the directory into an estimator to\ncontinue training a previously saved model. If PathLike object, the\npath will be resolved. If None, the model_dir in config will be used\nif set. If both are set, they must be same. If both are None, a\ntemporary directory will be used.", "config": "estimator.RunConfig configuration object.", "params": "dict of hyper parameters that will be passed into model_fn.\nKeys are names of parameters, values are basic python types.", "warm_start_from": "Optional string filepath to a checkpoint or SavedModel to\nwarm-start from, or a tf.estimator.WarmStartSettings object to fully\nconfigure warm-starting.  If None, only TRAINABLE variables are\nwarm-started.  If the string filepath is provided instead of a\ntf.estimator.WarmStartSettings, then all variables are warm-started,\nand it is assumed that vocabularies and tf.Tensor names are unchanged."}, "Raises": {"ValueError": "if this is called via a subclass and if that class overrides\na member of Estimator."}, "Attributes": {"config": "", "model_dir": "", "model_fn": "Returns the model_fn which is bound to self.params.", "params": ""}}, "tf.compat.v1.estimator.LinearClassifier": {"description": "Linear classifier model.", "Args": {"model_fn": "Model function. Follows the signature:\n\nfeatures -- This is the first item returned from the input_fn\npassed to train, evaluate, and predict. This should be a\nsingle tf.Tensor or dict of same.\nlabels -- This is the second item returned from the input_fn\npassed to train, evaluate, and predict. This should be a\nsingle tf.Tensor or dict of same (for multi-head models). If\nmode is tf.estimator.ModeKeys.PREDICT, labels=None will be\npassed. If the model_fn's signature does not accept mode, the\nmodel_fn must still be able to handle labels=None.\nmode -- Optional. Specifies if this is training, evaluation or\nprediction. See tf.estimator.ModeKeys.\nparams -- Optional dict of hyperparameters.  Will receive what is\npassed to Estimator in params parameter. This allows to configure\nEstimators from hyper parameter tuning.\nconfig -- Optional estimator.RunConfig object. Will receive what\nis passed to Estimator as its config parameter, or a default\nvalue. Allows setting up things in your model_fn based on\nconfiguration such as num_ps_replicas, or model_dir.\nReturns -- tf.estimator.EstimatorSpec", "model_dir": "Directory to save model parameters, graph and etc. This can\nalso be used to load checkpoints from the directory into an estimator to\ncontinue training a previously saved model. If PathLike object, the\npath will be resolved. If None, the model_dir in config will be used\nif set. If both are set, they must be same. If both are None, a\ntemporary directory will be used.", "config": "estimator.RunConfig configuration object.", "params": "dict of hyper parameters that will be passed into model_fn.\nKeys are names of parameters, values are basic python types.", "warm_start_from": "Optional string filepath to a checkpoint or SavedModel to\nwarm-start from, or a tf.estimator.WarmStartSettings object to fully\nconfigure warm-starting.  If None, only TRAINABLE variables are\nwarm-started.  If the string filepath is provided instead of a\ntf.estimator.WarmStartSettings, then all variables are warm-started,\nand it is assumed that vocabularies and tf.Tensor names are unchanged."}, "Raises": {"ValueError": "if this is called via a subclass and if that class overrides\na member of Estimator."}, "Attributes": {"config": "", "model_dir": "", "model_fn": "Returns the model_fn which is bound to self.params.", "params": ""}}, "tf.compat.v1.estimator.LinearEstimator": {"description": "An estimator for TensorFlow linear models with user-specified head.", "Args": {"head": "A _Head instance constructed with a method such as\ntf.contrib.estimator.multi_label_head.", "feature_columns": "An iterable containing all the feature columns used by\nthe model. All items in the set should be instances of classes derived\nfrom FeatureColumn.", "model_dir": "Directory to save model parameters, graph and etc. This can\nalso be used to load checkpoints from the directory into a estimator to\ncontinue training a previously saved model.", "optimizer": "An instance of tf.Optimizer used to train the model. Can also\nbe a string (one of 'Adagrad', 'Adam', 'Ftrl', 'RMSProp', 'SGD'), or\ncallable. Defaults to FTRL optimizer.", "config": "RunConfig object to configure the runtime settings.", "partitioner": "Optional. Partitioner for input layer.", "sparse_combiner": "A string specifying how to reduce if a categorical column\nis multivalent.  One of \"mean\", \"sqrtn\", and \"sum\" -- these are\neffectively different ways to do example-level normalization, which can\nbe useful for bag-of-words features. for more details, see\ntf.feature_column.linear_model.", "warm_start_from": "A string filepath to a checkpoint to warm-start from, or\na WarmStartSettings object to fully configure warm-starting.  If the\nstring filepath is provided instead of a WarmStartSettings, then all\nweights and biases are warm-started, and it is assumed that vocabularies\nand Tensor names are unchanged."}, "Attributes": {"config": "", "model_dir": "", "model_fn": "Returns the model_fn which is bound to self.params.", "params": ""}}, "tf.compat.v1.estimator.LinearRegressor": {"description": "An estimator for TensorFlow Linear regression problems.", "Args": {"model_fn": "Model function. Follows the signature:\n\nfeatures -- This is the first item returned from the input_fn\npassed to train, evaluate, and predict. This should be a\nsingle tf.Tensor or dict of same.\nlabels -- This is the second item returned from the input_fn\npassed to train, evaluate, and predict. This should be a\nsingle tf.Tensor or dict of same (for multi-head models). If\nmode is tf.estimator.ModeKeys.PREDICT, labels=None will be\npassed. If the model_fn's signature does not accept mode, the\nmodel_fn must still be able to handle labels=None.\nmode -- Optional. Specifies if this is training, evaluation or\nprediction. See tf.estimator.ModeKeys.\nparams -- Optional dict of hyperparameters.  Will receive what is\npassed to Estimator in params parameter. This allows to configure\nEstimators from hyper parameter tuning.\nconfig -- Optional estimator.RunConfig object. Will receive what\nis passed to Estimator as its config parameter, or a default\nvalue. Allows setting up things in your model_fn based on\nconfiguration such as num_ps_replicas, or model_dir.\nReturns -- tf.estimator.EstimatorSpec", "model_dir": "Directory to save model parameters, graph and etc. This can\nalso be used to load checkpoints from the directory into an estimator to\ncontinue training a previously saved model. If PathLike object, the\npath will be resolved. If None, the model_dir in config will be used\nif set. If both are set, they must be same. If both are None, a\ntemporary directory will be used.", "config": "estimator.RunConfig configuration object.", "params": "dict of hyper parameters that will be passed into model_fn.\nKeys are names of parameters, values are basic python types.", "warm_start_from": "Optional string filepath to a checkpoint or SavedModel to\nwarm-start from, or a tf.estimator.WarmStartSettings object to fully\nconfigure warm-starting.  If None, only TRAINABLE variables are\nwarm-started.  If the string filepath is provided instead of a\ntf.estimator.WarmStartSettings, then all variables are warm-started,\nand it is assumed that vocabularies and tf.Tensor names are unchanged."}, "Raises": {"ValueError": "if this is called via a subclass and if that class overrides\na member of Estimator."}, "Attributes": {"config": "", "model_dir": "", "model_fn": "Returns the model_fn which is bound to self.params.", "params": ""}}, "tf.compat.v1.estimator.classifier_parse_example_spec": {"description": "Generates parsing spec for tf.parse_example to be used with classifiers.", "Args": {"feature_columns": "An iterable containing all feature columns. All items\nshould be instances of classes derived from FeatureColumn.", "label_key": "A string identifying the label. It means tf.Example stores labels\nwith this key.", "label_dtype": "A tf.dtype identifies the type of labels. By default it is\ntf.int64. If user defines a label_vocabulary, this should be set as\ntf.string. tf.float32 labels are only supported for binary\nclassification.", "label_default": "used as label if label_key does not exist in given\ntf.Example. An example usage: let's say label_key is 'clicked' and\n  tf.Example contains clicked data only for positive examples in following\nformat key:clicked, value:1. This means that if there is no data with\n  key 'clicked' it should count as negative example by setting\n  label_deafault=0. Type of this value should be compatible with\n  label_dtype.", "weight_column": "A string or a NumericColumn created by\ntf.feature_column.numeric_column defining feature column representing\nweights. It is used to down weight or boost examples during training. It\nwill be multiplied by the loss of the example. If it is a string, it is\nused as a key to fetch weight tensor from the features. If it is a\nNumericColumn, raw tensor is fetched by key weight_column.key, then\nweight_column.normalizer_fn is applied on it to get weight tensor."}, "Returns": "A dict mapping each feature key to a FixedLenFeature or VarLenFeature\nvalue.", "Raises": {"ValueError": "if label_key is None."}}, "tf.compat.v1.estimator.regressor_parse_example_spec": {"description": "Generates parsing spec for tf.parse_example to be used with regressors.", "Args": {"feature_columns": "An iterable containing all feature columns. All items\nshould be instances of classes derived from _FeatureColumn.", "label_key": "A string identifying the label. It means tf.Example stores labels\nwith this key.", "label_dtype": "A tf.dtype identifies the type of labels. By default it is\ntf.float32.", "label_default": "used as label if label_key does not exist in given\ntf.Example. By default default_value is none, which means\ntf.parse_example will error out if there is any missing label.", "label_dimension": "Number of regression targets per example. This is the size\nof the last dimension of the labels and logits Tensor objects\n(typically, these have shape [batch_size, label_dimension]).", "weight_column": "A string or a NumericColumn created by\ntf.feature_column.numeric_column defining feature column representing\nweights. It is used to down weight or boost examples during training. It\nwill be multiplied by the loss of the example. If it is a string, it is\nused as a key to fetch weight tensor from the features. If it is a\nNumericColumn, raw tensor is fetched by key weight_column.key, then\nweight_column.normalizer_fn is applied on it to get weight tensor."}, "Returns": "A dict mapping each feature key to a FixedLenFeature or VarLenFeature\nvalue.", "Raises": {"ValueError": "if label_key is None."}}}, "tf.compat.v1.estimator.export": {}, "tf.compat.v1.estimator.inputs": {"tf.compat.v1.estimator.inputs.numpy_input_fn": {"description": "Returns input function that would feed dict of numpy arrays into the model.", "Args": {"x": "numpy array object or dict of numpy array objects. If an array, the array\nwill be treated as a single feature.", "y": "numpy array object or dict of numpy array object. None if absent.", "batch_size": "Integer, size of batches to return.", "num_epochs": "Integer, number of epochs to iterate over data. If None will\nrun forever.", "shuffle": "Boolean, if True shuffles the queue. Avoid shuffle at prediction\ntime.", "queue_capacity": "Integer, size of queue to accumulate.", "num_threads": "Integer, number of threads used for reading and enqueueing. In\norder to have predicted and repeatable order of reading and enqueueing,\nsuch as in prediction and evaluation mode, num_threads should be 1."}, "Returns": "Function, that has signature of ()->(dict of features, targets)", "Raises": {"ValueError": "if 'shuffle' is not provided or a bool.", "TypeError": "x is not a dict or array."}}, "tf.compat.v1.estimator.inputs.pandas_input_fn": {"description": "Returns input function that would feed Pandas DataFrame into the model.", "Args": {"x": "pandas DataFrame object.", "y": "pandas Series object or DataFrame. None if absent.", "batch_size": "int, size of batches to return.", "num_epochs": "int, number of epochs to iterate over data. If not None, read\nattempts that would exceed this value will raise OutOfRangeError.", "shuffle": "bool, whether to read the records in random order.", "queue_capacity": "int, size of the read queue. If None, it will be set\nroughly to the size of x.", "num_threads": "Integer, number of threads used for reading and enqueueing. In\norder to have predicted and repeatable order of reading and enqueueing,\nsuch as in prediction and evaluation mode, num_threads should be 1.", "target_column": "str, name to give the target column y. This parameter is\nnot used when y is a DataFrame."}, "Returns": "Function, that has signature of ()->(dict of features, target)", "Raises": {"ValueError": "if 'shuffle' is not provided or a bool."}}}, "tf.compat.v1.estimator.tpu": {"tf.compat.v1.estimator.tpu.InputPipelineConfig": {"description": "Please see the definition of these values in TPUConfig.", "Class Variables": {"BROADCAST": "4", "PER_HOST_V1": "2", "PER_HOST_V2": "3", "PER_SHARD_V1": "1", "SLICED": "5"}}, "tf.compat.v1.estimator.tpu.RunConfig": {"description": "RunConfig with TPU support.", "Args": {"tpu_config": "the TPUConfig that specifies TPU-specific configuration.", "evaluation_master": "a string. The address of the master to use for eval.\nDefaults to master if not set.", "master": "a string. The address of the master to use for training.", "cluster": "a ClusterResolver", "**kwargs": "keyword config parameters."}, "Raises": {"ValueError": "if cluster is not None and the provided session_config has a\ncluster_def already."}, "Attributes": {"checkpoint_save_graph_def": "", "cluster": "", "cluster_spec": "", "device_fn": "Returns the device_fn.\nIf device_fn is not None, it overrides the default\ndevice function used in Estimator.\nOtherwise the default one is used.", "eval_distribute": "Optional tf.distribute.Strategy for evaluation.", "evaluation_master": "", "experimental_max_worker_delay_secs": "", "global_id_in_cluster": "The global id in the training cluster.\nAll global ids in the training cluster are assigned from an increasing\nsequence of consecutive integers. The first id is 0.\nNote: Task id (the property field task_id) is tracking the index of the\nnode among all nodes with the SAME task type. For example, given the cluster\ndefinition as follows:\u00a0 cluster = {'chief': ['host0:2222'],\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0'ps': ['host1:2222', 'host2:2222'],\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0'worker': ['host3:2222', 'host4:2222', 'host5:2222']}\nNodes with task type worker can have id 0, 1, 2.  Nodes with task type\nps can have id, 0, 1. So, task_id is not unique, but the pair\n(task_type, task_id) can uniquely determine a node in the cluster.\nGlobal id, i.e., this field, is tracking the index of the node among ALL\nnodes in the cluster. It is uniquely assigned.  For example, for the cluster\nspec given above, the global ids are assigned as:\n\u00a0 task_type \u00a0| task_id \u00a0| \u00a0global_id\u00a0 --------------------------------\u00a0 chief \u00a0 \u00a0 \u00a0| 0 \u00a0 \u00a0 \u00a0 \u00a0| \u00a00\u00a0 worker \u00a0 \u00a0 | 0 \u00a0 \u00a0 \u00a0 \u00a0| \u00a01\u00a0 worker \u00a0 \u00a0 | 1 \u00a0 \u00a0 \u00a0 \u00a0| \u00a02\u00a0 worker \u00a0 \u00a0 | 2 \u00a0 \u00a0 \u00a0 \u00a0| \u00a03\u00a0 ps \u00a0 \u00a0 \u00a0 \u00a0 | 0 \u00a0 \u00a0 \u00a0 \u00a0| \u00a04\u00a0 ps \u00a0 \u00a0 \u00a0 \u00a0 | 1 \u00a0 \u00a0 \u00a0 \u00a0| \u00a05", "is_chief": "", "keep_checkpoint_every_n_hours": "", "keep_checkpoint_max": "", "log_step_count_steps": "", "master": "", "model_dir": "", "num_ps_replicas": "", "num_worker_replicas": "", "protocol": "Returns the optional protocol value.", "save_checkpoints_secs": "", "save_checkpoints_steps": "", "save_summary_steps": "", "service": "Returns the platform defined (in TF_CONFIG) service dict.", "session_config": "", "session_creation_timeout_secs": "", "task_id": "", "task_type": "", "tf_random_seed": "", "tpu_config": "", "train_distribute": "Optional tf.distribute.Strategy for training."}}, "tf.compat.v1.estimator.tpu.TPUConfig": {"description": "TPU related configuration required by TPUEstimator.", "Args": {"iterations_per_loop": "This is the number of train steps running in TPU system\nbefore returning to CPU host for each Session.run. This means global\nstep is increased iterations_per_loop times in one Session.run. It is\nrecommended to be set as number of global steps for next checkpoint. Note\nthat in evaluation don't use this value, instead we run total eval steps\non TPU for a single Session.run.\n[Experimental]: iterations_per_loop can be specified as a time interval.\n  To specify N seconds in one Session.run, one can specify it as Ns\n  and substitute the N with the N with the number of desired seconds.\n  Alternatively, the unit of time can also be specified in minutes or\n  hours, e.g. 3600s or 60m or 1h.", "num_shards": "(Deprecated, ignored by TPUEstimator). The number of model\nreplicas in the system. For non-model-parallelism case, this number equals\nthe total number of TPU cores. For model-parallelism, the total number of\nTPU cores equals num_cores_per_replica * num_shards.", "num_cores_per_replica": "Defaults to None, which disables model parallelism.\nAn integer which describes the number of TPU cores per model replica. This\nis required by model-parallelism which enables partitioning the model to\nmultiple cores. Currently num_cores_per_replica must be 1, 2, 4, or 8.", "per_host_input_for_training": "If True, for PER_HOST_V1, the input_fn is\ninvoked once on each host, and the number of hosts must be smaller or\nequal to the number of replicas. For PER_HOST_V2, the input_fn is\ninvoked once for each host (if the number of hosts is less than the number\nof replicas) or replica (if the number of replicas is less than the number\nof hosts. With the per-core input pipeline configuration, it is invoked\nonce for each core. With a global batch size train_batch_size in\nTPUEstimator constructor, the batch size for each shard is\ntrain_batch_size // #hosts in the True or PER_HOST_V1 mode. In\nPER_HOST_V2 mode, it is train_batch_size // #cores. In BROADCAST\nmode, input_fn is only invoked once on host 0 and the tensors are\nbroadcasted to all other replicas. The batch size equals to\ntrain_batch_size. With the per-core input pipeline configuration, the\nshard batch size is also train_batch_size // #cores.\nNote: per_host_input_for_training==PER_SHARD_V1 only supports mode.TRAIN.", "tpu_job_name": "The name of the TPU job. Typically, this name is auto-inferred\nwithin TPUEstimator, however when using ClusterSpec propagation in more\nesoteric cluster configurations, you may need to specify the job name as a\nstring.", "initial_infeed_sleep_secs": "The number of seconds the infeed thread should\nwait before enqueueing the first batch. This helps avoid timeouts for\nmodels that require a long compilation time.", "input_partition_dims": "A nested list to describe the partition dims for all\nthe tensors from input_fn(). The structure of input_partition_dims must\nmatch the structure of features and labels from input_fn(). The total\nnumber of partitions must match\nnum_cores_per_replica. For example, if input_fn() returns two tensors:\n  images with shape [N, H, W, C] and labels [N]. input_partition_dims =\n  [[1, 2, 2, 1], None] will split the images to 4 pieces and feed into 4\n  TPU cores. labels tensor are directly broadcasted to all the TPU cores\n  since the partition dims is None.\nCurrent limitations: This feature is only supported with the PER_HOST_V2\n  input mode.", "eval_training_input_configuration": "If SLICED, input_fn is only invoked\nonce on host 0 and the tensors are broadcasted to all other replicas.\nUnlike per_host_input_for_training=BROADCAST, each replica will only get a\nslice of the data instead of a whole copy. If PER_HOST_V1, the behaviour\nis determined by per_host_input_for_training.", "experimental_host_call_every_n_steps": "Within a training loop, this argument\nsets how often host calls are performed during training. Host calls will\nbe evaluated every n steps within a training loop where n is the value of\nthis argument.", "experimental_allow_per_host_v2_parallel_get_next": "When enabled, allows\nconcurrent execution of dataset get next calls when using PER_HOST_V2\ninput. May result in a performance increase for models with a small step\ntime, but as a consequence TPUEstimator may non-deterministically\ndistribute batches to different cores, rather than guaranteeing round\nrobin behavior.", "experimental_feed_hook": "This is a class which user can provide to the TPU\nestimator to override the default TPUInfeedOutfeedSessionHook implementation\nand add customized implementatioin to handle infeed outfeed logic. If\ngiven class is None, TPU estimator uses default TPUInfeedOutfeedSessionHook\nimplementation in tpu_estimator.py. If not None, TPU estimator uses this\ncustomized tpu infeed outfeed session hook class rather to override the\ndefault one."}, "Raises": {"ValueError": "If num_cores_per_replica is not 1, 2, 4, 8, ..., 128."}, "Attributes": {"iterations_per_loop": "A namedtuple alias for field number 0", "num_shards": "A namedtuple alias for field number 1", "num_cores_per_replica": "A namedtuple alias for field number 2", "per_host_input_for_training": "A namedtuple alias for field number 3", "tpu_job_name": "A namedtuple alias for field number 4", "initial_infeed_sleep_secs": "A namedtuple alias for field number 5", "input_partition_dims": "A namedtuple alias for field number 6", "eval_training_input_configuration": "A namedtuple alias for field number 7", "experimental_host_call_every_n_steps": "A namedtuple alias for field number 8", "experimental_allow_per_host_v2_parallel_get_next": "A namedtuple alias for field number 9", "experimental_feed_hook": "A namedtuple alias for field number 10"}}, "tf.compat.v1.estimator.tpu.TPUEstimator": {"description": "Estimator with TPU support.", "Args": {"model_fn": "Model function as required by Estimator which returns\nEstimatorSpec or TPUEstimatorSpec. training_hooks, 'evaluation_hooks',\nand prediction_hooks must not capure any TPU Tensor inside the\nmodel_fn.", "model_dir": "Directory to save model parameters, graph and etc. This can\nalso be used to load checkpoints from the directory into a estimator to\ncontinue training a previously saved model. If None, the model_dir in\nconfig will be used if set. If both are set, they must be same. If\nboth are None, a temporary directory will be used.", "config": "An tpu_config.RunConfig configuration object. Cannot be None.", "params": "An optional dict of hyper parameters that will be passed into\ninput_fn and model_fn.  Keys are names of parameters, values are\nbasic python types. There are reserved keys for TPUEstimator,\nincluding 'batch_size'.", "use_tpu": "A bool indicating whether TPU support is enabled. Currently, -\nTPU training and evaluation respect this bit, but eval_on_tpu can\noverride execution of eval. See below.", "train_batch_size": "An int representing the global training batch size.\nTPUEstimator transforms this global batch size to a per-shard batch\nsize, as params['batch_size'], when calling input_fn and model_fn.\nCannot be None if use_tpu is True. Must be divisible by total\nnumber of replicas.", "eval_batch_size": "An int representing evaluation batch size. Must be\ndivisible by total number of replicas.", "predict_batch_size": "An int representing the prediction batch size. Must be\ndivisible by total number of replicas.", "batch_axis": "A python tuple of int values describing how each tensor\nproduced by the Estimator input_fn should be split across the TPU\ncompute shards. For example, if your input_fn produced (images, labels)\nwhere the images tensor is in HWCN format, your shard dimensions would\nbe [3, 0], where 3 corresponds to the N dimension of your images\nTensor, and 0 corresponds to the dimension along which to split the\nlabels to match up with the corresponding images. If None is supplied,\nand per_host_input_for_training is True, batches will be sharded based\non the major dimension. If tpu_config.per_host_input_for_training is\nFalse or PER_HOST_V2, batch_axis is ignored.", "eval_on_tpu": "If False, evaluation runs on CPU or GPU. In this case, the\nmodel_fn must return EstimatorSpec when called with mode as EVAL.", "export_to_tpu": "If True, export_saved_model() exports a metagraph for\nserving on TPU. Note that unsupported export modes such as EVAL will be\nignored. For those modes, only a CPU model will be exported. Currently,\nexport_to_tpu only supports PREDICT.", "export_to_cpu": "If True, export_saved_model() exports a metagraph for\nserving on CPU.", "warm_start_from": "Optional string filepath to a checkpoint or SavedModel to\nwarm-start from, or a tf.estimator.WarmStartSettings object to fully\nconfigure warm-starting.  If the string filepath is provided instead of\na WarmStartSettings, then all variables are warm-started, and it is\nassumed that vocabularies and Tensor names are unchanged.", "embedding_config_spec": "Optional EmbeddingConfigSpec instance to support\nusing TPU embedding.", "export_saved_model_api_version": "an integer: 1 or 2. 1 corresponds to V1,\n2 corresponds to V2. (Defaults to V1). With\nV1, export_saved_model() adds rewrite() and TPUPartitionedCallOp() for\nuser; while in v2, user is expected to add rewrite(),\nTPUPartitionedCallOp() etc in their model_fn."}, "Raises": {"ValueError": "params has reserved keys already."}, "Attributes": {"config": "", "model_dir": "", "model_fn": "Returns the model_fn which is bound to self.params.", "params": ""}}, "tf.compat.v1.estimator.tpu.TPUEstimatorSpec": {"description": "Ops and objects returned from a model_fn and passed to TPUEstimator.", "Attributes": {"mode": "A namedtuple alias for field number 0", "predictions": "A namedtuple alias for field number 1", "loss": "A namedtuple alias for field number 2", "train_op": "A namedtuple alias for field number 3", "eval_metrics": "A namedtuple alias for field number 4", "export_outputs": "A namedtuple alias for field number 5", "scaffold_fn": "A namedtuple alias for field number 6", "host_call": "A namedtuple alias for field number 7", "training_hooks": "A namedtuple alias for field number 8", "evaluation_hooks": "A namedtuple alias for field number 9", "prediction_hooks": "A namedtuple alias for field number 10"}}}, "tf.compat.v1.feature_column": {"tf.compat.v1.feature_column.categorical_column_with_vocabulary_file": {"description": "A CategoricalColumn with a vocabulary file.", "Args": {"key": "A unique string identifying the input feature. It is used as the\ncolumn name and the dictionary key for feature parsing configs, feature\nTensor objects, and feature columns.", "vocabulary_file": "The vocabulary file name.", "vocabulary_size": "Number of the elements in the vocabulary. This must be no\ngreater than length of vocabulary_file, if less than length, later\nvalues are ignored. If None, it is set to the length of vocabulary_file.", "num_oov_buckets": "Non-negative integer, the number of out-of-vocabulary\nbuckets. All out-of-vocabulary inputs will be assigned IDs in the range\n[vocabulary_size, vocabulary_size+num_oov_buckets) based on a hash of\nthe input value. A positive num_oov_buckets can not be specified with\ndefault_value.", "default_value": "The integer ID value to return for out-of-vocabulary feature\nvalues, defaults to -1. This can not be specified with a positive\nnum_oov_buckets.", "dtype": "The type of features. Only string and integer types are supported."}, "Returns": "A CategoricalColumn with a vocabulary file.", "Raises": {"ValueError": "dtype is neither string nor integer."}}, "tf.compat.v1.feature_column.input_layer": {"description": "Returns a dense Tensor as input layer based on given feature_columns.", "Args": {"features": "A mapping from key to tensors. _FeatureColumns look up via these\nkeys. For example numeric_column('price') will look at 'price' key in\nthis dict. Values can be a SparseTensor or a Tensor depends on\ncorresponding _FeatureColumn.", "feature_columns": "An iterable containing the FeatureColumns to use as inputs\nto your model. All items should be instances of classes derived from\n_DenseColumn such as numeric_column, embedding_column,\nbucketized_column, indicator_column. If you have categorical features,\nyou can wrap them with an embedding_column or indicator_column.", "weight_collections": "A list of collection names to which the Variable will be\nadded. Note that variables will also be added to collections\ntf.GraphKeys.GLOBAL_VARIABLES and ops.GraphKeys.MODEL_VARIABLES.", "trainable": "If True also add the variable to the graph collection\nGraphKeys.TRAINABLE_VARIABLES (see tf.Variable).", "cols_to_vars": "If not None, must be a dictionary that will be filled with a\nmapping from _FeatureColumn to list of Variables.  For example, after\nthe call, we might have cols_to_vars =\n{_EmbeddingColumn(\n  categorical_column=_HashedCategoricalColumn(\n    key='sparse_feature', hash_bucket_size=5, dtype=tf.string),\n  dimension=10): [", "cols_to_output_tensors": "If not None, must be a dictionary that will be\nfilled with a mapping from '_FeatureColumn' to the associated\noutput Tensors."}, "Returns": "A Tensor which represents input layer of a model. Its shape\nis (batch_size, first_layer_dimension) and its dtype is float32.\nfirst_layer_dimension is determined based on given feature_columns.", "Raises": {"ValueError": "if an item in feature_columns is not a _DenseColumn."}}, "tf.compat.v1.feature_column.linear_model": {"description": "Returns a linear prediction Tensor based on given feature_columns.", "Args": {"features": "A mapping from key to tensors. _FeatureColumns look up via these\nkeys. For example numeric_column('price') will look at 'price' key in\nthis dict. Values are Tensor or SparseTensor depending on\ncorresponding _FeatureColumn.", "feature_columns": "An iterable containing the FeatureColumns to use as inputs\nto your model. All items should be instances of classes derived from\n_FeatureColumns.", "units": "An integer, dimensionality of the output space. Default value is 1.", "sparse_combiner": "A string specifying how to reduce if a categorical column\nis multivalent. Except numeric_column, almost all columns passed to\nlinear_model are considered as categorical columns.  It combines each\ncategorical column independently. Currently \"mean\", \"sqrtn\" and \"sum\" are\nsupported, with \"sum\" the default for linear model. \"sqrtn\" often achieves\ngood accuracy, in particular with bag-of-words columns.\n\n\"sum\": do not normalize features in the column\n\"mean\": do l1 normalization on features in the column\n\"sqrtn\": do l2 normalization on features in the column", "weight_collections": "A list of collection names to which the Variable will be\nadded. Note that, variables will also be added to collections\ntf.GraphKeys.GLOBAL_VARIABLES and ops.GraphKeys.MODEL_VARIABLES.", "trainable": "If True also add the variable to the graph collection\nGraphKeys.TRAINABLE_VARIABLES (see tf.Variable).", "cols_to_vars": "If not None, must be a dictionary that will be filled with a\nmapping from _FeatureColumn to associated list of Variables.  For\nexample, after the call, we might have cols_to_vars = {\n_NumericColumn(\nkey='numeric_feature1', shape=(1,):\n[],\n'bias': [],\n_NumericColumn(\nkey='numeric_feature2', shape=(2,)):\n[]}\nIf a column creates no variables, its value will be an empty list. Note\nthat cols_to_vars will also contain a string key 'bias' that maps to a\nlist of Variables."}, "Returns": "A Tensor which represents predictions/logits of a linear model. Its shape\nis (batch_size, units) and its dtype is float32.", "Raises": {"ValueError": "if an item in feature_columns is neither a _DenseColumn\nnor _CategoricalColumn."}}, "tf.compat.v1.feature_column.make_parse_example_spec": {"description": "Creates parsing spec dictionary from input feature_columns.", "Args": {"feature_columns": "An iterable containing all feature columns. All items\nshould be instances of classes derived from _FeatureColumn."}, "Returns": "A dict mapping each feature key to a FixedLenFeature or VarLenFeature\nvalue.", "Raises": {"ValueError": "If any of the given feature_columns is not a _FeatureColumn\ninstance."}}, "tf.compat.v1.feature_column.shared_embedding_columns": {"description": "List of dense columns that convert from sparse, categorical input.", "Args": {"categorical_columns": "List of categorical columns created by a\ncategorical_column_with_* function. These columns produce the sparse IDs\nthat are inputs to the embedding lookup. All columns must be of the same\ntype and have the same arguments except key. E.g. they can be\ncategorical_column_with_vocabulary_file with the same vocabulary_file.\nSome or all columns could also be weighted_categorical_column.", "dimension": "An integer specifying dimension of the embedding, must be > 0.", "combiner": "A string specifying how to reduce if there are multiple entries in\na single row. Currently 'mean', 'sqrtn' and 'sum' are supported, with\n'mean' the default. 'sqrtn' often achieves good accuracy, in particular\nwith bag-of-words columns. Each of this can be thought as example level\nnormalizations on the column. For more information, see\ntf.embedding_lookup_sparse.", "initializer": "A variable initializer function to be used in embedding\nvariable initialization. If not specified, defaults to\ntruncated_normal_initializer with mean 0.0 and\nstandard deviation 1/sqrt(dimension).", "shared_embedding_collection_name": "Optional name of the collection where\nshared embedding weights are added. If not given, a reasonable name will\nbe chosen based on the names of categorical_columns. This is also used\nin variable_scope when creating shared embedding weights.", "ckpt_to_load_from": "String representing checkpoint name/pattern from which to\nrestore column weights. Required if tensor_name_in_ckpt is not None.", "tensor_name_in_ckpt": "Name of the Tensor in ckpt_to_load_from from which\nto restore the column weights. Required if ckpt_to_load_from is not\nNone.", "max_norm": "If not None, each embedding is clipped if its l2-norm is larger\nthan this value, before combining.", "trainable": "Whether or not the embedding is trainable. Default is True.", "use_safe_embedding_lookup": "If true, uses safe_embedding_lookup_sparse\ninstead of embedding_lookup_sparse. safe_embedding_lookup_sparse ensures\nthere are no empty rows and all weights and ids are positive at the\nexpense of extra compute cost. This only applies to rank 2 (NxM) shaped\ninput tensors. Defaults to true, consider turning off if the above checks\nare not needed. Note that having empty rows will not trigger any error\nthough the output result might be 0 or omitted."}, "Returns": "A list of dense columns that converts from sparse input. The order of\nresults follows the ordering of categorical_columns.", "Raises": {"ValueError": "if initializer is specified and is not callable.", "RuntimeError": "if eager execution is enabled."}}}, "tf.compat.v1.flags": {"tf.compat.v1.flags.ArgumentParser": {"description": "Base class used to parse and convert arguments.", "Class Variables": {"syntactic_help": "''"}}, "tf.compat.v1.flags.ArgumentSerializer": {"description": "Base class for generating string representations of a flag value."}, "tf.compat.v1.flags.BaseListParser": {"description": "Base class for a parser of lists of strings.", "Class Variables": {"syntactic_help": "''"}}, "tf.compat.v1.flags.BooleanFlag": {"description": "Basic boolean flag.", "Attributes": {"value": ""}}, "tf.compat.v1.flags.BooleanParser": {"description": "Parser of boolean values.", "Class Variables": {"syntactic_help": "''"}}, "tf.compat.v1.flags.CantOpenFlagFileError": {"description": "Raised when flagfile fails to open."}, "tf.compat.v1.flags.CsvListSerializer": {"description": "Base class for generating string representations of a flag value."}, "tf.compat.v1.flags.DEFINE": {"description": "Registers a generic Flag object.", "Args": {"parser": "ArgumentParser, used to parse the flag arguments.", "name": "str, the flag name.", "default": "The default value of the flag.", "help": "str, the help message.", "flag_values": "FlagValues, the FlagValues instance with which the flag will be\nregistered. This should almost never need to be overridden.", "serializer": "ArgumentSerializer, the flag serializer instance.", "module_name": "str, the name of the Python module declaring this flag. If not\nprovided, it will be computed using the stack trace of this call.", "required": "bool, is this a required flag. This must be used as a keyword\nargument.", "**args": "dict, the extra keyword args that are passed to Flag init."}, "Returns": "a handle to defined flag."}, "tf.compat.v1.flags.DEFINE_alias": {"description": "Defines an alias flag for an existing one.", "Args": {"name": "str, the flag name.", "original_name": "str, the original flag name.", "flag_values": "FlagValues, the FlagValues instance with which the flag will be\nregistered. This should almost never need to be overridden.", "module_name": "A string, the name of the module that defines this flag."}, "Returns": "a handle to defined flag.", "Raises": {"flags.FlagError": "UnrecognizedFlagError: if the referenced flag doesn't exist.\nDuplicateFlagError: if the alias name has been used by some existing flag."}}, "tf.compat.v1.flags.DEFINE_bool": {"description": "Registers a boolean flag.", "Args": {"name": "str, the flag name.", "default": "bool|str|None, the default value of the flag.", "help": "str, the help message.", "flag_values": "FlagValues, the FlagValues instance with which the flag will be\nregistered. This should almost never need to be overridden.", "module_name": "str, the name of the Python module declaring this flag. If not\nprovided, it will be computed using the stack trace of this call.", "required": "bool, is this a required flag. This must be used as a keyword\nargument.", "**args": "dict, the extra keyword args that are passed to Flag init."}, "Returns": "a handle to defined flag."}, "tf.compat.v1.flags.DEFINE_enum": {"description": "Registers a flag whose value can be any string from enum_values.", "Args": {"name": "str, the flag name.", "default": "str|None, the default value of the flag.", "enum_values": "[str], a non-empty list of strings with the possible values for\nthe flag.", "help": "str, the help message.", "flag_values": "FlagValues, the FlagValues instance with which the flag will be\nregistered. This should almost never need to be overridden.", "module_name": "str, the name of the Python module declaring this flag. If not\nprovided, it will be computed using the stack trace of this call.", "required": "bool, is this a required flag. This must be used as a keyword\nargument.", "**args": "dict, the extra keyword args that are passed to Flag init."}, "Returns": "a handle to defined flag."}, "tf.compat.v1.flags.DEFINE_enum_class": {"description": "Registers a flag whose value can be the name of enum members.", "Args": {"name": "str, the flag name.", "default": "Enum|str|None, the default value of the flag.", "enum_class": "class, the Enum class with all the possible values for the flag.", "help": "str, the help message.", "flag_values": "FlagValues, the FlagValues instance with which the flag will be\nregistered. This should almost never need to be overridden.", "module_name": "str, the name of the Python module declaring this flag. If not\nprovided, it will be computed using the stack trace of this call.", "case_sensitive": "bool, whether to map strings to members of the enum_class\nwithout considering case.", "required": "bool, is this a required flag. This must be used as a keyword\nargument.", "**args": "dict, the extra keyword args that are passed to Flag init."}, "Returns": "a handle to defined flag."}, "tf.compat.v1.flags.DEFINE_flag": {"description": "Registers a &#39;Flag&#39; object with a &#39;FlagValues&#39; object.", "Args": {"flag": "Flag, a flag that is key to the module.", "flag_values": "FlagValues, the FlagValues instance with which the flag will be\nregistered. This should almost never need to be overridden.", "module_name": "str, the name of the Python module declaring this flag. If not\nprovided, it will be computed using the stack trace of this call.", "required": "bool, is this a required flag. This must be used as a keyword\nargument."}, "Returns": "a handle to defined flag."}, "tf.compat.v1.flags.DEFINE_float": {"description": "Registers a flag whose value must be a float.", "Args": {"name": "str, the flag name.", "default": "float|str|None, the default value of the flag.", "help": "str, the help message.", "lower_bound": "float, min value of the flag.", "upper_bound": "float, max value of the flag.", "flag_values": "FlagValues, the FlagValues instance with which the flag will be\nregistered. This should almost never need to be overridden.", "required": "bool, is this a required flag. This must be used as a keyword\nargument.", "**args": "dict, the extra keyword args that are passed to DEFINE."}, "Returns": "a handle to defined flag."}, "tf.compat.v1.flags.DEFINE_integer": {"description": "Registers a flag whose value must be an integer.", "Args": {"name": "str, the flag name.", "default": "int|str|None, the default value of the flag.", "help": "str, the help message.", "lower_bound": "int, min value of the flag.", "upper_bound": "int, max value of the flag.", "flag_values": "FlagValues, the FlagValues instance with which the flag will be\nregistered. This should almost never need to be overridden.", "required": "bool, is this a required flag. This must be used as a keyword\nargument.", "**args": "dict, the extra keyword args that are passed to DEFINE."}, "Returns": "a handle to defined flag."}, "tf.compat.v1.flags.DEFINE_list": {"description": "Registers a flag whose value is a comma-separated list of strings.", "Args": {"name": "str, the flag name.", "default": "list|str|None, the default value of the flag.", "help": "str, the help message.", "flag_values": "FlagValues, the FlagValues instance with which the flag will be\nregistered. This should almost never need to be overridden.", "required": "bool, is this a required flag. This must be used as a keyword\nargument.", "**args": "Dictionary with extra keyword args that are passed to the Flag\ninit."}, "Returns": "a handle to defined flag."}, "tf.compat.v1.flags.DEFINE_multi": {"description": "Registers a generic MultiFlag that parses its args with a given parser.", "Args": {"parser": "ArgumentParser, used to parse the flag arguments.", "serializer": "ArgumentSerializer, the flag serializer instance.", "name": "str, the flag name.", "default": "Union[Iterable[T], Text, None], the default value of the flag. If\nthe value is text, it will be parsed as if it was provided from the\ncommand line. If the value is a non-string iterable, it will be iterated\nover to create a shallow copy of the values. If it is None, it is left\nas-is.", "help": "str, the help message.", "flag_values": "FlagValues, the FlagValues instance with which the flag will be\nregistered. This should almost never need to be overridden.", "module_name": "A string, the name of the Python module declaring this flag. If\nnot provided, it will be computed using the stack trace of this call.", "required": "bool, is this a required flag. This must be used as a keyword\nargument.", "**args": "Dictionary with extra keyword args that are passed to the Flag\ninit."}, "Returns": "a handle to defined flag."}, "tf.compat.v1.flags.DEFINE_multi_enum": {"description": "Registers a flag whose value can be a list strings from enum_values.", "Args": {"name": "str, the flag name.", "default": "Union[Iterable[Text], Text, None], the default value of the flag;\nsee DEFINE_multi.", "enum_values": "[str], a non-empty list of strings with the possible values for\nthe flag.", "help": "str, the help message.", "flag_values": "FlagValues, the FlagValues instance with which the flag will be\nregistered. This should almost never need to be overridden.", "case_sensitive": "Whether or not the enum is to be case-sensitive.", "required": "bool, is this a required flag. This must be used as a keyword\nargument.", "**args": "Dictionary with extra keyword args that are passed to the Flag\ninit."}, "Returns": "a handle to defined flag."}, "tf.compat.v1.flags.DEFINE_multi_enum_class": {"description": "Registers a flag whose value can be a list of enum members.", "Args": {"name": "str, the flag name.", "default": "Union[Iterable[Enum], Iterable[Text], Enum, Text, None], the\ndefault value of the flag; see DEFINE_multi; only differences are\ndocumented here. If the value is a single Enum, it is treated as a\nsingle-item list of that Enum value. If it is an iterable, text values\nwithin the iterable will be converted to the equivalent Enum objects.", "enum_class": "class, the Enum class with all the possible values for the flag.\nhelp: str, the help message.", "flag_values": "FlagValues, the FlagValues instance with which the flag will be\nregistered. This should almost never need to be overridden.", "module_name": "A string, the name of the Python module declaring this flag. If\nnot provided, it will be computed using the stack trace of this call.", "case_sensitive": "bool, whether to map strings to members of the enum_class\nwithout considering case.", "required": "bool, is this a required flag. This must be used as a keyword\nargument.", "**args": "Dictionary with extra keyword args that are passed to the Flag\ninit."}, "Returns": "a handle to defined flag."}, "tf.compat.v1.flags.DEFINE_multi_float": {"description": "Registers a flag whose value can be a list of arbitrary floats.", "Args": {"name": "str, the flag name.", "default": "Union[Iterable[float], Text, None], the default value of the flag;\nsee DEFINE_multi.", "help": "str, the help message.", "lower_bound": "float, min values of the flag.", "upper_bound": "float, max values of the flag.", "flag_values": "FlagValues, the FlagValues instance with which the flag will be\nregistered. This should almost never need to be overridden.", "required": "bool, is this a required flag. This must be used as a keyword\nargument.", "**args": "Dictionary with extra keyword args that are passed to the Flag\ninit."}, "Returns": "a handle to defined flag."}, "tf.compat.v1.flags.DEFINE_multi_integer": {"description": "Registers a flag whose value can be a list of arbitrary integers.", "Args": {"name": "str, the flag name.", "default": "Union[Iterable[int], Text, None], the default value of the flag;\nsee DEFINE_multi.", "help": "str, the help message.", "lower_bound": "int, min values of the flag.", "upper_bound": "int, max values of the flag.", "flag_values": "FlagValues, the FlagValues instance with which the flag will be\nregistered. This should almost never need to be overridden.", "required": "bool, is this a required flag. This must be used as a keyword\nargument.", "**args": "Dictionary with extra keyword args that are passed to the Flag\ninit."}, "Returns": "a handle to defined flag."}, "tf.compat.v1.flags.DEFINE_multi_string": {"description": "Registers a flag whose value can be a list of any strings.", "Args": {"name": "str, the flag name.", "default": "Union[Iterable[Text], Text, None], the default value of the flag;\nsee DEFINE_multi.", "help": "str, the help message.", "flag_values": "FlagValues, the FlagValues instance with which the flag will be\nregistered. This should almost never need to be overridden.", "required": "bool, is this a required flag. This must be used as a keyword\nargument.", "**args": "Dictionary with extra keyword args that are passed to the Flag\ninit."}, "Returns": "a handle to defined flag."}, "tf.compat.v1.flags.DEFINE_spaceseplist": {"description": "Registers a flag whose value is a whitespace-separated list of strings.", "Args": {"name": "str, the flag name.", "default": "list|str|None, the default value of the flag.", "help": "str, the help message.", "comma_compat": "bool - Whether to support comma as an additional separator. If\nfalse then only whitespace is supported.  This is intended only for\nbackwards compatibility with flags that used to be comma-separated.", "flag_values": "FlagValues, the FlagValues instance with which the flag will be\nregistered. This should almost never need to be overridden.", "required": "bool, is this a required flag. This must be used as a keyword\nargument.", "**args": "Dictionary with extra keyword args that are passed to the Flag\ninit."}, "Returns": "a handle to defined flag."}, "tf.compat.v1.flags.DEFINE_string": {"description": "Registers a flag whose value can be any string."}, "tf.compat.v1.flags.DuplicateFlagError": {"description": "Raised if there is a flag naming conflict."}, "tf.compat.v1.flags.EnumClassFlag": {"description": "Basic enum flag; its value is an enum class&#39;s member.", "Attributes": {"value": ""}}, "tf.compat.v1.flags.EnumClassListSerializer": {"description": "A serializer for MultiEnumClass flags.", "Args": {"list_sep": "String to be used as a separator when serializing", "**kwargs": "Keyword arguments to the EnumClassSerializer used to serialize\nindividual values."}}, "tf.compat.v1.flags.EnumClassParser": {"description": "Parser of an Enum class member.", "Args": {"enum_class": "class, the Enum class with all possible flag values.", "case_sensitive": "bool, whether or not the enum is to be case-sensitive. If\nFalse, all member names must be unique when case is ignored."}, "Raises": {"TypeError": "When enum_class is not a subclass of Enum.", "ValueError": "When enum_class is empty."}, "Attributes": {"member_names": "The accepted enum names, in lowercase if not case sensitive."}, "Class Variables": {"syntactic_help": "''"}}, "tf.compat.v1.flags.EnumClassSerializer": {"description": "Class for generating string representations of an enum class flag value.", "Args": {"lowercase": "If True, enum member names are lowercased during serialization."}}, "tf.compat.v1.flags.EnumFlag": {"description": "Basic enum flag; its value can be any string from list of enum_values.", "Attributes": {"value": ""}}, "tf.compat.v1.flags.EnumParser": {"description": "Parser of a string enum value (a string value from a given set).", "Args": {"enum_values": "[str], a non-empty list of string values in the enum.", "case_sensitive": "bool, whether or not the enum is to be case-sensitive."}, "Raises": {"ValueError": "When enum_values is empty."}, "Class Variables": {"syntactic_help": "''"}}, "tf.compat.v1.flags.Error": {"description": "The base class for all flags errors."}, "tf.compat.v1.flags.FLAGS": {"description": "Registry of &#39;Flag&#39; objects."}, "tf.compat.v1.flags.Flag": {"description": "Information about a command-line flag.", "Attributes": {"value": ""}}, "tf.compat.v1.flags.FlagHolder": {"description": "Holds a defined flag.", "Args": {"flag_values": "The container the flag is registered to.", "flag": "The flag object for this flag.", "ensure_non_none_value": "Is the value of the flag allowed to be None."}, "Attributes": {"default": "Returns the default value of the flag.", "name": "", "present": "Returns True if the flag was parsed from command-line flags.", "value": "Returns the value of the flag.\nIf _ensure_non_none_value is True, then return value is not None."}}, "tf.compat.v1.flags.FlagNameConflictsWithMethodError": {"description": "Raised when a flag name conflicts with FlagValues methods."}, "tf.compat.v1.flags.FlagValues": {"description": "Registry of &#39;Flag&#39; objects."}, "tf.compat.v1.flags.FloatParser": {"description": "Parser of floating point values.", "Class Variables": {"number_article": "'a'", "number_name": "'number'", "syntactic_help": "'a number'"}}, "tf.compat.v1.flags.IllegalFlagValueError": {"description": "Raised when the flag command line argument is illegal."}, "tf.compat.v1.flags.IntegerParser": {"description": "Parser of an integer value.", "Class Variables": {"number_article": "'an'", "number_name": "'integer'", "syntactic_help": "'an integer'"}}, "tf.compat.v1.flags.ListParser": {"description": "Parser for a comma-separated list of strings.", "Class Variables": {"syntactic_help": "''"}}, "tf.compat.v1.flags.ListSerializer": {"description": "Base class for generating string representations of a flag value."}, "tf.compat.v1.flags.MultiEnumClassFlag": {"description": "A multi_enum_class flag.", "Attributes": {"value": ""}}, "tf.compat.v1.flags.MultiFlag": {"description": "A flag that can appear multiple time on the command-line.", "Attributes": {"value": ""}}, "tf.compat.v1.flags.UnparsedFlagAccessError": {"description": "Raised when accessing the flag value from unparsed FlagValues."}, "tf.compat.v1.flags.UnrecognizedFlagError": {"description": "Raised when a flag is unrecognized.", "Attributes": {"flagname": "str, the name of the unrecognized flag.", "flagvalue": "The value of the flag, empty if the flag is not defined."}}, "tf.compat.v1.flags.ValidationError": {"description": "Raised when flag validator constraint is not satisfied."}, "tf.compat.v1.flags.WhitespaceSeparatedListParser": {"description": "Parser for a whitespace-separated list of strings.", "Args": {"comma_compat": "bool, whether to support comma as an additional separator.\nIf False then only whitespace is supported.  This is intended only for\nbackwards compatibility with flags that used to be comma-separated."}, "Class Variables": {"syntactic_help": "''"}}, "tf.compat.v1.flags.adopt_module_key_flags": {"description": "Declares that all flags key to a module are key to the current module.", "Args": {"module": "module, the module object from which all key flags will be declared\nas key flags to the current module.", "flag_values": "FlagValues, the FlagValues instance in which the flags will be\ndeclared as key flags. This should almost never need to be overridden."}, "Raises": {"Error": "Raised when given an argument that is a module name (a string),\ninstead of a module object."}}, "tf.compat.v1.flags.declare_key_flag": {"description": "Declares one flag as key to the current module.", "Args": {"flag_name": "str, the name of an already declared flag. (Redeclaring flags as\nkey, including flags implicitly key because they were declared in this\nmodule, is a no-op.)", "flag_values": "FlagValues, the FlagValues instance in which the flag will be\ndeclared as a key flag. This should almost never need to be overridden."}, "Raises": {"ValueError": "Raised if flag_name not defined as a Python flag."}}, "tf.compat.v1.flags.disclaim_key_flags": {"description": "Declares that the current module will not define any more key flags."}, "tf.compat.v1.flags.doc_to_help": {"description": "Takes a __doc__ string and reformats it as help."}, "tf.compat.v1.flags.flag_dict_to_args": {"description": "Convert a dict of values into process call parameters.", "Args": {"flag_map": "dict, a mapping where the keys are flag names (strings).\nvalues are treated according to their type:\n\nIf value is None, then only the name is emitted.\nIf value is True, then only the name is emitted.\nIf value is False, then only the name prepended with 'no' is emitted.\nIf value is a string then --name=value is emitted.\nIf value is a collection, this will emit --name=value1,value2,value3,\nunless the flag name is in multi_flags, in which case this will emit\n--name=value1 --name=value2 --name=value3.\nEverything else is converted to string an passed as such.", "multi_flags": "set, names (strings) of flags that should be treated as\nmulti-flags."}}, "tf.compat.v1.flags.get_help_width": {"description": "Returns the integer width of help lines that is used in TextWrap."}, "tf.compat.v1.flags.mark_bool_flags_as_mutual_exclusive": {"description": "Ensures that only one flag among flag_names is True.", "Args": {"flag_names": "[str], names of the flags.", "required": "bool. If true, exactly one flag must be True. Otherwise, at most\none flag can be True, and it is valid for all flags to be False.", "flag_values": "flags.FlagValues, optional FlagValues instance where the flags\nare defined."}}, "tf.compat.v1.flags.mark_flag_as_required": {"description": "Ensures that flag is not None during program execution.", "Args": {"flag_name": "str, name of the flag", "flag_values": "flags.FlagValues, optional FlagValues instance where the flag\nis defined."}, "Raises": {"AttributeError": "Raised when flag_name is not registered as a valid flag\nname."}}, "tf.compat.v1.flags.mark_flags_as_mutual_exclusive": {"description": "Ensures that only one flag among flag_names is not None.", "Args": {"flag_names": "[str], names of the flags.", "required": "bool. If true, exactly one of the flags must have a value other\nthan None. Otherwise, at most one of the flags can have a value other\nthan None, and it is valid for all of the flags to be None.", "flag_values": "flags.FlagValues, optional FlagValues instance where the flags\nare defined."}}, "tf.compat.v1.flags.mark_flags_as_required": {"description": "Ensures that flags are not None during program execution.", "Args": {"flag_names": "Sequence[str], names of the flags.", "flag_values": "flags.FlagValues, optional FlagValues instance where the flags\nare defined."}, "Raises": {"AttributeError": "If any of flag name has not already been defined as a flag."}}, "tf.compat.v1.flags.multi_flags_validator": {"description": "A function decorator for defining a multi-flag validator.", "Args": {"flag_names": "[str], a list of the flag names to be checked.", "message": "str, error text to be shown to the user if checker returns False.\nIf checker raises flags.ValidationError, message from the raised\nerror will be shown.", "flag_values": "flags.FlagValues, optional FlagValues instance to validate\nagainst."}, "Returns": "A function decorator that registers its function argument as a validator.", "Raises": {"AttributeError": "Raised when a flag is not registered as a valid flag name."}}, "tf.compat.v1.flags.register_multi_flags_validator": {"description": "Adds a constraint to multiple flags.", "Args": {"flag_names": "[str], a list of the flag names to be checked.", "multi_flags_checker": "callable, a function to validate the flag.\ninput - dict, with keys() being flag_names, and value for each key\n    being the value of the corresponding flag (string, boolean, etc).\noutput - bool, True if validator constraint is satisfied.\n    If constraint is not satisfied, it should either return False or\n    raise flags.ValidationError.", "message": "str, error text to be shown to the user if checker returns False.\nIf checker raises flags.ValidationError, message from the raised\nerror will be shown.", "flag_values": "flags.FlagValues, optional FlagValues instance to validate\nagainst."}, "Raises": {"AttributeError": "Raised when a flag is not registered as a valid flag name."}}, "tf.compat.v1.flags.register_validator": {"description": "Adds a constraint, which will be enforced during program execution."}, "tf.compat.v1.flags.text_wrap": {"description": "Wraps a given text to a maximum line length and returns it.", "Args": {"text": "str, text to wrap.", "length": "int, maximum length of a line, includes indentation.\nIf this is None then use get_help_width()", "indent": "str, indent for all but first line.", "firstline_indent": "str, indent for first line; if None, fall back to indent."}, "Returns": "str, the wrapped text.", "Raises": {"ValueError": "Raised if indent or firstline_indent not shorter than length."}}, "tf.compat.v1.flags.validator": {"description": "A function decorator for defining a flag validator.", "Args": {"flag_name": "str, name of the flag to be checked.", "message": "str, error text to be shown to the user if checker returns False.\nIf checker raises flags.ValidationError, message from the raised\nerror will be shown.", "flag_values": "flags.FlagValues, optional FlagValues instance to validate\nagainst."}, "Returns": "A function decorator that registers its function argument as a validator.", "Raises": {"AttributeError": "Raised when flag_name is not registered as a valid flag\nname."}}}, "tf.compat.v1.gfile": {"tf.compat.v1.gfile.Copy": {"description": "Copies data from src to dst.", "Args": {"src": "string, name of the file whose contents need to be copied", "dst": "string, name of the file to which to copy to", "overwrite": "boolean, if false it's an error for dst to be occupied by an\nexisting file."}, "Raises": {"errors.OpError": "If the operation fails."}}, "tf.compat.v1.gfile.DeleteRecursively": {"description": "Deletes everything under dirname recursively.", "Args": {"dirname": "string, a path to a directory"}, "Raises": {"errors.OpError": "If the operation fails."}}, "tf.compat.v1.gfile.Exists": {"description": "Determines whether a path exists or not.", "Args": {"path": "string, a path"}, "Returns": "True if the path exists, whether it's a file or a directory.\nFalse if the path does not exist and there are no filesystem errors.", "Raises": {"errors.OpError": "Propagates any errors reported by the FileSystem API."}}, "tf.compat.v1.gfile.FastGFile": {"description": "File I/O wrappers without thread locking.", "Attributes": {"mode": "Returns the mode in which the file was opened.", "name": "Returns the file name."}}, "tf.compat.v1.gfile.Glob": {"description": "Returns a list of files that match the given pattern(s).", "Args": {"filename": "string or iterable of strings. The glob pattern(s)."}, "Returns": "A list of strings containing filenames that match the given pattern(s).", "Raises": {}}, "tf.compat.v1.gfile.IsDirectory": {"description": "Returns whether the path is a directory or not.", "Args": {"dirname": "string, path to a potential directory"}, "Returns": "True, if the path is a directory; False otherwise"}, "tf.compat.v1.gfile.ListDirectory": {"description": "Returns a list of entries contained within a directory.", "Args": {"dirname": "string, path to a directory"}, "Returns": "[filename1, filename2, ... filenameN] as strings", "Raises": {}}, "tf.compat.v1.gfile.MakeDirs": {"description": "Creates a directory and all parent/intermediate directories.", "Args": {"dirname": "string, name of the directory to be created"}, "Raises": {"errors.OpError": "If the operation fails."}}, "tf.compat.v1.gfile.MkDir": {"description": "Creates a directory with the name dirname.", "Args": {"dirname": "string, name of the directory to be created"}, "Raises": {"errors.OpError": "If the operation fails."}}, "tf.compat.v1.gfile.Remove": {"description": "Deletes the file located at &#39;filename&#39;.", "Args": {"filename": "string, a filename"}, "Raises": {"errors.OpError": "Propagates any errors reported by the FileSystem API.  E.g.,\nNotFoundError if the file does not exist."}}, "tf.compat.v1.gfile.Rename": {"description": "Rename or move a file / directory.", "Args": {"oldname": "string, pathname for a file", "newname": "string, pathname to which the file needs to be moved", "overwrite": "boolean, if false it's an error for newname to be occupied by\nan existing file."}, "Raises": {"errors.OpError": "If the operation fails."}}, "tf.compat.v1.gfile.Stat": {"description": "Returns file statistics for a given path.", "Args": {"filename": "string, path to a file"}, "Returns": "FileStatistics struct that contains information about the path", "Raises": {"errors.OpError": "If the operation fails."}}, "tf.compat.v1.gfile.Walk": {"description": "Recursive directory tree generator for directories.", "Args": {"top": "string, a Directory name", "in_order": "bool, Traverse in order if True, post order if False.  Errors that\nhappen while listing directories are ignored."}}}, "tf.compat.v1.graph_util": {"tf.compat.v1.graph_util.convert_variables_to_constants": {"description": "Replaces all the variables in a graph with constants of the same values. (deprecated)", "Args": {"sess": "Active TensorFlow session containing the variables.", "input_graph_def": "GraphDef object holding the network.", "output_node_names": "List of name strings for the result nodes of the graph.", "variable_names_whitelist": "The set of variable names to convert (by default,\nall variables are converted).", "variable_names_blacklist": "The set of variable names to omit converting\nto constants."}, "Returns": "GraphDef containing a simplified version of the original.", "Raises": {"RuntimeError": "if a DT_RESOURCE op is found whose ancestor Variables are both\ndenylisted AND whitelisted for freezing."}}, "tf.compat.v1.graph_util.extract_sub_graph": {"description": "Extract the subgraph that can reach any of the nodes in &#39;dest_nodes&#39;. (deprecated)", "Args": {"graph_def": "A graph_pb2.GraphDef proto.", "dest_nodes": "An iterable of strings specifying the destination node names."}, "Returns": "The GraphDef of the sub-graph.", "Raises": {"TypeError": "If 'graph_def' is not a graph_pb2.GraphDef proto."}}, "tf.compat.v1.graph_util.must_run_on_cpu": {"description": "Returns True if the given node_def must run on CPU, otherwise False. (deprecated)", "Args": {"node": "The node to be assigned to a device. Could be either an ops.Operation\nor NodeDef.", "pin_variables_on_cpu": "If True, this function will return False if node_def\nrepresents a variable-related op."}, "Returns": "True if the given node must run on CPU, otherwise False."}, "tf.compat.v1.graph_util.remove_training_nodes": {"description": "Prunes out nodes that aren&#39;t needed for inference. (deprecated)", "Args": {"input_graph": "Model to analyze and prune.", "protected_nodes": "An optional list of names of nodes to be kept\nunconditionally. This is for example useful to preserve Identity output\nnodes."}, "Returns": "A list of nodes with the unnecessary ones removed."}, "tf.compat.v1.graph_util.tensor_shape_from_node_def_name": {"description": "Convenience function to get a shape from a NodeDef&#39;s input string. (deprecated)"}}, "tf.compat.v1.image": {"tf.compat.v1.image.ResizeMethod": {"description": "See v1.image.resize for details."}, "tf.compat.v1.image.crop_and_resize": {"description": "Extracts crops from the input image tensor and resizes them.", "Args": {"image": "A Tensor. Must be one of the following types: uint8, uint16, int8, int16, int32, int64, half, float32, float64.\nA 4-D tensor of shape [batch, image_height, image_width, depth].\nBoth image_height and image_width need to be positive.", "boxes": "A Tensor of type float32.\nA 2-D tensor of shape [num_boxes, 4]. The i-th row of the tensor\nspecifies the coordinates of a box in the box_ind[i] image and is specified\nin normalized coordinates [y1, x1, y2, x2]. A normalized coordinate value of\ny is mapped to the image coordinate at y * (image_height - 1), so as the\n[0, 1] interval of normalized image height is mapped to\n[0, image_height - 1] in image height coordinates. We do allow y1 > y2, in\nwhich case the sampled crop is an up-down flipped version of the original\nimage. The width dimension is treated similarly. Normalized coordinates\noutside the [0, 1] range are allowed, in which case we use\nextrapolation_value to extrapolate the input image values.", "box_ind": "A Tensor of type int32.\nA 1-D tensor of shape [num_boxes] with int32 values in [0, batch).\nThe value of box_ind[i] specifies the image that the i-th box refers to.", "crop_size": "A Tensor of type int32.\nA 1-D tensor of 2 elements, size = [crop_height, crop_width]. All\ncropped image patches are resized to this size. The aspect ratio of the image\ncontent is not preserved. Both crop_height and crop_width need to be\npositive.", "method": "An optional string from: \"bilinear\", \"nearest\". Defaults to \"bilinear\".\nA string specifying the sampling method for resizing. It can be either\n\"bilinear\" or \"nearest\" and default to \"bilinear\". Currently two sampling\nmethods are supported: Bilinear and Nearest Neighbor.", "extrapolation_value": "An optional float. Defaults to 0.\nValue used for extrapolation, when applicable.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type float32."}, "tf.compat.v1.image.draw_bounding_boxes": {"description": "Draw bounding boxes on a batch of images.", "Args": {"images": "A Tensor. Must be one of the following types: float32, half.\n4-D with shape [batch, height, width, depth]. A batch of images.", "boxes": "A Tensor of type float32. 3-D with shape [batch,\nnum_bounding_boxes, 4] containing bounding boxes.", "name": "A name for the operation (optional).", "colors": "A Tensor of type float32. 2-D. A list of RGBA colors to cycle\nthrough for the boxes."}, "Returns": "A Tensor. Has the same type as images."}, "tf.compat.v1.image.extract_glimpse": {"description": "Extracts a glimpse from the input tensor.", "Args": {"input": "A Tensor of type float32. A 4-D float tensor of shape\n[batch_size, height, width, channels].", "size": "A Tensor of type int32. A 1-D tensor of 2 elements containing the\nsize of the glimpses to extract.  The glimpse height must be specified\nfirst, following by the glimpse width.", "offsets": "A Tensor of type float32. A 2-D integer tensor of shape\n[batch_size, 2] containing the y, x locations of the center of each\nwindow.", "centered": "An optional bool. Defaults to True. indicates if the offset\ncoordinates are centered relative to the image, in which case the (0, 0)\noffset is relative to the center of the input images. If false, the (0,0)\noffset corresponds to the upper left corner of the input images.", "normalized": "An optional bool. Defaults to True. indicates if the offset\ncoordinates are normalized.", "uniform_noise": "An optional bool. Defaults to True. indicates if the\nnoise should be generated using a uniform distribution or a Gaussian\ndistribution.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type float32."}, "tf.compat.v1.image.resize": {"description": "Resize images to size using the specified method.", "Args": {"images": "4-D Tensor of shape [batch, height, width, channels] or 3-D Tensor\nof shape [height, width, channels].", "size": "A 1-D int32 Tensor of 2 elements: new_height, new_width.  The new\nsize for the images.", "method": "ResizeMethod.  Defaults to tf.image.ResizeMethod.BILINEAR.", "align_corners": "bool.  If True, the centers of the 4 corner pixels of the\ninput and output tensors are aligned, preserving the values at the corner\npixels. Defaults to False.", "preserve_aspect_ratio": "Whether to preserve the aspect ratio. If this is set,\nthen images will be resized to a size that fits in size while\npreserving the aspect ratio of the original image. Scales up the image if\nsize is bigger than the current size of the image. Defaults to False.", "name": "A name for this operation (optional)."}, "Raises": {"ValueError": "if an unsupported resize method is specified."}, "Returns": "If images was 4-D, a 4-D float Tensor of shape\n[batch, new_height, new_width, channels].\nIf images was 3-D, a 3-D float Tensor of shape\n[new_height, new_width, channels]."}, "tf.compat.v1.image.resize_area": {"description": "Resize images to size using area interpolation.", "Args": {"images": "A Tensor. Must be one of the following types: int8, uint8, int16, uint16, int32, int64, half, float32, float64, bfloat16.\n4-D with shape [batch, height, width, channels].", "size": "A 1-D int32 Tensor of 2 elements: new_height, new_width.  The\nnew size for the images.", "align_corners": "An optional bool. Defaults to False.\nIf true, the centers of the 4 corner pixels of the input and output tensors are\naligned, preserving the values at the corner pixels. Defaults to false.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type float32."}, "tf.compat.v1.image.resize_bicubic": {}, "tf.compat.v1.image.resize_bilinear": {}, "tf.compat.v1.image.resize_image_with_pad": {"description": "Resizes and pads an image to a target width and height.", "Args": {"image": "4-D Tensor of shape [batch, height, width, channels] or 3-D Tensor\nof shape [height, width, channels].", "target_height": "Target height.", "target_width": "Target width.", "method": "Method to use for resizing image. See resize_images()", "align_corners": "bool.  If True, the centers of the 4 corner pixels of the\ninput and output tensors are aligned, preserving the values at the corner\npixels. Defaults to False."}, "Raises": {"ValueError": "if target_height or target_width are zero or negative."}, "Returns": "Resized and padded image.\nIf images was 4-D, a 4-D float Tensor of shape\n[batch, new_height, new_width, channels].\nIf images was 3-D, a 3-D float Tensor of shape\n[new_height, new_width, channels]."}, "tf.compat.v1.image.resize_nearest_neighbor": {}, "tf.compat.v1.image.sample_distorted_bounding_box": {"description": "Generate a single randomly distorted bounding box for an image. (deprecated)", "Args": {"image_size": "A Tensor. Must be one of the following types: uint8, int8,\nint16, int32, int64. 1-D, containing [height, width, channels].", "bounding_boxes": "A Tensor of type float32. 3-D with shape [batch, N, 4]\ndescribing the N bounding boxes associated with the image.", "seed": "An optional int. Defaults to 0. If either seed or seed2 are\nset to non-zero, the random number generator is seeded by the given\nseed.  Otherwise, it is seeded by a random seed.", "seed2": "An optional int. Defaults to 0. A second seed to avoid seed\ncollision.", "min_object_covered": "A Tensor of type float32. Defaults to 0.1. The\ncropped area of the image must contain at least this fraction of any\nbounding box supplied. The value of this parameter should be non-negative.\nIn the case of 0, the cropped area does not need to overlap any of the\nbounding boxes supplied.", "aspect_ratio_range": "An optional list of floats. Defaults to [0.75,\n1.33]. The cropped area of the image must have an aspect ratio = width /\nheight within this range.", "area_range": "An optional list of floats. Defaults to [0.05, 1]. The\ncropped area of the image must contain a fraction of the supplied image\nwithin this range.", "max_attempts": "An optional int. Defaults to 100. Number of attempts at\ngenerating a cropped region of the image of the specified constraints.\nAfter max_attempts failures, return the entire image.", "use_image_if_no_bounding_boxes": "An optional bool. Defaults to False.\nControls behavior if no bounding boxes supplied. If true, assume an\nimplicit bounding box covering the whole input. If false, raise an error.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (begin, size, bboxes).", "Raises": {"ValueError": "If no seed is specified and op determinism is enabled."}}}, "tf.compat.v1.initializers": {"tf.compat.v1.initializers.he_normal": {"description": "He normal initializer.", "Args": {"seed": "A Python integer. Used to seed the random generator."}, "Returns": "An initializer."}, "tf.compat.v1.initializers.he_uniform": {"description": "He uniform variance scaling initializer.", "Args": {"seed": "A Python integer. Used to seed the random generator."}, "Returns": "An initializer."}, "tf.compat.v1.initializers.lecun_normal": {"description": "LeCun normal initializer.", "Args": {"seed": "A Python integer. Used to seed the random generator."}, "Returns": "An initializer."}, "tf.compat.v1.initializers.lecun_uniform": {"description": "LeCun uniform initializer.", "Args": {"seed": "A Python integer. Used to seed the random generator."}, "Returns": "An initializer."}}, "tf.compat.v1.io": {"tf.compat.v1.io.TFRecordCompressionType": {"description": "The type of compression for the record."}, "tf.compat.v1.io.tf_record_iterator": {"description": "An iterator that read the records from a TFRecords file. (deprecated)", "Args": {"path": "The path to the TFRecords file.", "options": "(optional) A TFRecordOptions object."}, "Returns": "An iterator of serialized TFRecords.", "Raises": {"IOError": "If path cannot be opened for reading."}}}, "tf.compat.v1.io.gfile": {}, "tf.compat.v1.keras": {}, "tf.compat.v1.keras.activations": {}, "tf.compat.v1.keras.applications": {}, "tf.compat.v1.keras.applications.densenet": {}, "tf.compat.v1.keras.applications.efficientnet": {}, "tf.compat.v1.keras.applications.efficientnet_v2": {}, "tf.compat.v1.keras.applications.imagenet_utils": {}, "tf.compat.v1.keras.applications.inception_resnet_v2": {}, "tf.compat.v1.keras.applications.inception_v3": {}, "tf.compat.v1.keras.applications.mobilenet": {}, "tf.compat.v1.keras.applications.mobilenet_v2": {}, "tf.compat.v1.keras.applications.mobilenet_v3": {}, "tf.compat.v1.keras.applications.nasnet": {}, "tf.compat.v1.keras.applications.regnet": {}, "tf.compat.v1.keras.applications.resnet": {}, "tf.compat.v1.keras.applications.resnet50": {}, "tf.compat.v1.keras.applications.resnet_rs": {}, "tf.compat.v1.keras.applications.resnet_v2": {}, "tf.compat.v1.keras.applications.vgg16": {}, "tf.compat.v1.keras.applications.vgg19": {}, "tf.compat.v1.keras.applications.xception": {}, "tf.compat.v1.keras.backend": {"tf.compat.v1.keras.backend.get_session": {"description": "Returns the TF session to be used by the backend.", "Args": {"op_input_list": "An option sequence of tensors or ops, which will be used\nto determine the current graph. Otherwise the default graph will be\nused."}, "Returns": "A TensorFlow session."}, "tf.compat.v1.keras.backend.name_scope": {"description": "A context manager for use when defining a Python op.", "Args": {"name": "The name argument that is passed to the op function.", "default_name": "The default name to use if the name argument is None.", "values": "The list of Tensor arguments that are passed to the op function."}, "Raises": {"TypeError": "if default_name is passed in but not a string."}, "Attributes": {"name": ""}}, "tf.compat.v1.keras.backend.set_session": {"description": "Sets the global TensorFlow session.", "Args": {"session": "A TF Session."}}}, "tf.compat.v1.keras.callbacks": {"tf.compat.v1.keras.callbacks.TensorBoard": {"description": "Enable visualizations for TensorBoard.", "Args": {"log_dir": "the path of the directory where to save the log files to be\nparsed by TensorBoard.", "histogram_freq": "frequency (in epochs) at which to compute activation and\nweight histograms for the layers of the model. If set to 0, histograms\nwon't be computed. Validation data (or split) must be specified for\nhistogram visualizations.", "write_graph": "whether to visualize the graph in TensorBoard. The log file\ncan become quite large when write_graph is set to True.", "write_grads": "whether to visualize gradient histograms in TensorBoard.\nhistogram_freq must be greater than 0.", "batch_size": "size of batch of inputs to feed to the network for histograms\ncomputation.", "write_images": "whether to write model weights to visualize as image in\nTensorBoard.", "embeddings_freq": "frequency (in epochs) at which selected embedding layers\nwill be saved. If set to 0, embeddings won't be computed. Data to be\nvisualized in TensorBoard's Embedding tab must be passed as\nembeddings_data.", "embeddings_layer_names": "a list of names of layers to keep eye on. If None\nor empty list all the embedding layer will be watched.", "embeddings_metadata": "a dictionary which maps layer name to a file name in\nwhich metadata for this embedding layer is saved.\n  Here are details\n    about metadata files format. In case if the same metadata file is\n    used for all embedding layers, string can be passed.", "embeddings_data": "data to be embedded at layers specified in\nembeddings_layer_names. Numpy array (if the model has a single input)\nor list of Numpy arrays (if the model has multiple inputs). Learn more\nabout embeddings in this guide.", "update_freq": "'batch' or 'epoch' or integer. When using 'batch',\nwrites the losses and metrics to TensorBoard after each batch. The same\napplies for 'epoch'. If using an integer, let's say 1000, the\ncallback will write the metrics and losses to TensorBoard every 1000\nsamples. Note that writing too frequently to TensorBoard can slow down\nyour training.", "profile_batch": "Profile the batch to sample compute characteristics. By\ndefault, it will profile the second batch. Set profile_batch=0 to\ndisable profiling."}, "Raises": {"ValueError": "If histogram_freq is set and no validation data is provided."}}}, "tf.compat.v1.keras.constraints": {}, "tf.compat.v1.keras.datasets": {}, "tf.compat.v1.keras.datasets.boston_housing": {}, "tf.compat.v1.keras.datasets.cifar10": {}, "tf.compat.v1.keras.datasets.cifar100": {}, "tf.compat.v1.keras.datasets.fashion_mnist": {}, "tf.compat.v1.keras.datasets.imdb": {}, "tf.compat.v1.keras.datasets.mnist": {}, "tf.compat.v1.keras.datasets.reuters": {}, "tf.compat.v1.keras.estimator": {"tf.compat.v1.keras.estimator.model_to_estimator": {"description": "Constructs an Estimator instance from given keras model.", "Args": {"keras_model": "A compiled Keras model object. This argument is mutually\nexclusive with keras_model_path. Estimator's model_fn uses the\nstructure of the model to clone the model. Defaults to None.", "keras_model_path": "Path to a compiled Keras model saved on disk, in HDF5\nformat, which can be generated with the save() method of a Keras model.\nThis argument is mutually exclusive with keras_model.\nDefaults to None.", "custom_objects": "Dictionary for cloning customized objects. This is\nused with classes that is not part of this pip package. For example, if\nuser maintains a relu6 class that inherits from tf.keras.layers.Layer,\nthen pass custom_objects={'relu6': relu6}. Defaults to None.", "model_dir": "Directory to save Estimator model parameters, graph, summary\nfiles for TensorBoard, etc. If unset a directory will be created with\ntempfile.mkdtemp", "config": "RunConfig to config Estimator. Allows setting up things in\nmodel_fn based on configuration such as num_ps_replicas, or\nmodel_dir. Defaults to None. If both config.model_dir and the\nmodel_dir argument (above) are specified the model_dir argument\ntakes precedence.", "checkpoint_format": "Sets the format of the checkpoint saved by the estimator\nwhen training. May be saver or checkpoint, depending on whether to\nsave checkpoints from tf.train.Saver or tf.train.Checkpoint. This\nargument currently defaults to saver. When 2.0 is released, the default\nwill be checkpoint. Estimators use name-based tf.train.Saver\ncheckpoints, while Keras models use object-based checkpoints from\ntf.train.Checkpoint. Currently, saving object-based checkpoints from\nmodel_to_estimator is only supported by Functional and Sequential\nmodels. Defaults to 'saver'.", "metric_names_map": "Optional dictionary mapping Keras model output metric\nnames to custom names. This can be used to override the default Keras\nmodel output metrics names in a multi IO model use case and provide custom\nnames for the eval_metric_ops in Estimator.\nThe Keras model metric names can be obtained using model.metrics_names\nexcluding any loss metrics such as total loss and output losses.\nFor example, if your Keras model has two outputs out_1 and out_2,\nwith mse loss and acc metric, then model.metrics_names will be\n['loss', 'out_1_loss', 'out_2_loss', 'out_1_acc', 'out_2_acc'].\nThe model metric names excluding the loss metrics will be\n['out_1_acc', 'out_2_acc'].", "export_outputs": "Optional dictionary. This can be used to override the\ndefault Keras model output exports in a multi IO model use case and\nprovide custom names for the export_outputs in\ntf.estimator.EstimatorSpec. Default is None, which is equivalent to\n{'serving_default': tf.estimator.export.PredictOutput}. If not None,\nthe keys must match the keys of model.output_names.\nA dict {name: output} where:\n\nname: An arbitrary name for this output.\noutput: an ExportOutput class such as ClassificationOutput,\nRegressionOutput, or PredictOutput. Single-headed models only need\nto specify one entry in this dictionary. Multi-headed models should\nspecify one entry for each head, one of which must be named using\ntf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY\nIf no entry is provided, a default PredictOutput mapping to\npredictions will be created."}, "Returns": "An Estimator from given keras model.", "Raises": {"ValueError": "If an invalid checkpoint_format was given."}}}, "tf.compat.v1.keras.initializers": {"tf.compat.v1.keras.initializers.Constant": {"description": "Initializer that generates tensors with constant values.", "Args": {"value": "A Python scalar, list or tuple of values, or a N-dimensional numpy\narray. All elements of the initialized variable will be set to the\ncorresponding value in the value argument.", "dtype": "Default data type, used if no dtype argument is provided when\ncalling the initializer.", "verify_shape": "Boolean that enables verification of the shape of value. If\nTrue, the initializer will throw an error if the shape of value is not\ncompatible with the shape of the initialized tensor."}, "Raises": {"TypeError": "If the input value is not one of the expected types."}}, "tf.compat.v1.keras.initializers.Identity": {"description": "Initializer that generates the identity matrix.", "Args": {"gain": "Multiplicative factor to apply to the identity matrix.", "dtype": "Default data type, used if no dtype argument is provided when\ncalling the initializer. Only floating point types are supported."}}, "tf.compat.v1.keras.initializers.Ones": {"description": "Initializer that generates tensors initialized to 1."}, "tf.compat.v1.keras.initializers.Orthogonal": {"description": "Initializer that generates an orthogonal matrix.", "Args": {"gain": "multiplicative factor to apply to the orthogonal matrix", "seed": "A Python integer. Used to create random seeds. See\ntf.compat.v1.set_random_seed for behavior.", "dtype": "Default data type, used if no dtype argument is provided when\ncalling the initializer. Only floating point types are supported."}}, "tf.compat.v1.keras.initializers.RandomNormal": {"description": "Initializer that generates a normal distribution.", "Args": {"mean": "a python scalar or a scalar tensor. Mean of the random values to\ngenerate.", "stddev": "a python scalar or a scalar tensor. Standard deviation of the random\nvalues to generate.", "seed": "A Python integer. Used to create random seeds. See\ntf.compat.v1.set_random_seed for behavior.", "dtype": "Default data type, used if no dtype argument is provided when\ncalling the initializer. Only floating point types are supported."}}, "tf.compat.v1.keras.initializers.RandomUniform": {"description": "Initializer that generates tensors with a uniform distribution.", "Args": {"minval": "A python scalar or a scalar tensor. Lower bound of the range of\nrandom values to generate.", "maxval": "A python scalar or a scalar tensor. Upper bound of the range of\nrandom values to generate.  Defaults to 1 for float types.", "seed": "A Python integer. Used to create random seeds. See\ntf.compat.v1.set_random_seed for behavior.", "dtype": "Default data type, used if no dtype argument is provided when\ncalling the initializer."}}, "tf.compat.v1.keras.initializers.TruncatedNormal": {"description": "Initializer that generates a truncated normal distribution.", "Args": {"mean": "a python scalar or a scalar tensor. Mean of the random values to\ngenerate.", "stddev": "a python scalar or a scalar tensor. Standard deviation of the\nrandom values to generate.", "seed": "A Python integer. Used to create random seeds. See\ntf.compat.v1.set_random_seed for behavior.", "dtype": "Default data type, used if no dtype argument is provided when\ncalling the initializer. Only floating point types are supported."}}, "tf.compat.v1.keras.initializers.VarianceScaling": {"description": "Initializer capable of adapting its scale to the shape of weights tensors.", "Args": {"scale": "Scaling factor (positive float).", "mode": "One of \"fan_in\", \"fan_out\", \"fan_avg\".", "distribution": "Random distribution to use. One of \"normal\", \"uniform\".", "seed": "A Python integer. Used to create random seeds. See\ntf.compat.v1.set_random_seed for behavior.", "dtype": "Default data type, used if no dtype argument is provided when\ncalling the initializer. Only floating point types are supported."}, "Raises": {"ValueError": "In case of an invalid value for the \"scale\", mode\" or\n\"distribution\" arguments."}}, "tf.compat.v1.keras.initializers.Zeros": {"description": "Initializer that generates tensors initialized to 0."}, "tf.compat.v1.keras.initializers.glorot_normal": {"description": "The Glorot normal initializer, also called Xavier normal initializer.", "Args": {"seed": "A Python integer. Used to create random seeds. See\ntf.compat.v1.set_random_seed for behavior.", "dtype": "Default data type, used if no dtype argument is provided when\ncalling the initializer. Only floating point types are supported."}}, "tf.compat.v1.keras.initializers.glorot_uniform": {"description": "The Glorot uniform initializer, also called Xavier uniform initializer.", "Args": {"seed": "A Python integer. Used to create random seeds. See\ntf.compat.v1.set_random_seed for behavior.", "dtype": "Default data type, used if no dtype argument is provided when\ncalling the initializer. Only floating point types are supported."}}, "tf.compat.v1.keras.initializers.he_normal": {"description": "Initializer capable of adapting its scale to the shape of weights tensors.", "Args": {"scale": "Scaling factor (positive float).", "mode": "One of \"fan_in\", \"fan_out\", \"fan_avg\".", "distribution": "Random distribution to use. One of \"normal\", \"uniform\".", "seed": "A Python integer. Used to create random seeds. See\ntf.compat.v1.set_random_seed for behavior.", "dtype": "Default data type, used if no dtype argument is provided when\ncalling the initializer. Only floating point types are supported."}, "Raises": {"ValueError": "In case of an invalid value for the \"scale\", mode\" or\n\"distribution\" arguments."}}, "tf.compat.v1.keras.initializers.he_uniform": {"description": "Initializer capable of adapting its scale to the shape of weights tensors.", "Args": {"scale": "Scaling factor (positive float).", "mode": "One of \"fan_in\", \"fan_out\", \"fan_avg\".", "distribution": "Random distribution to use. One of \"normal\", \"uniform\".", "seed": "A Python integer. Used to create random seeds. See\ntf.compat.v1.set_random_seed for behavior.", "dtype": "Default data type, used if no dtype argument is provided when\ncalling the initializer. Only floating point types are supported."}, "Raises": {"ValueError": "In case of an invalid value for the \"scale\", mode\" or\n\"distribution\" arguments."}}, "tf.compat.v1.keras.initializers.lecun_normal": {"description": "Initializer capable of adapting its scale to the shape of weights tensors.", "Args": {"scale": "Scaling factor (positive float).", "mode": "One of \"fan_in\", \"fan_out\", \"fan_avg\".", "distribution": "Random distribution to use. One of \"normal\", \"uniform\".", "seed": "A Python integer. Used to create random seeds. See\ntf.compat.v1.set_random_seed for behavior.", "dtype": "Default data type, used if no dtype argument is provided when\ncalling the initializer. Only floating point types are supported."}, "Raises": {"ValueError": "In case of an invalid value for the \"scale\", mode\" or\n\"distribution\" arguments."}}, "tf.compat.v1.keras.initializers.lecun_uniform": {"description": "Initializer capable of adapting its scale to the shape of weights tensors.", "Args": {"scale": "Scaling factor (positive float).", "mode": "One of \"fan_in\", \"fan_out\", \"fan_avg\".", "distribution": "Random distribution to use. One of \"normal\", \"uniform\".", "seed": "A Python integer. Used to create random seeds. See\ntf.compat.v1.set_random_seed for behavior.", "dtype": "Default data type, used if no dtype argument is provided when\ncalling the initializer. Only floating point types are supported."}, "Raises": {"ValueError": "In case of an invalid value for the \"scale\", mode\" or\n\"distribution\" arguments."}}}, "tf.compat.v1.keras.layers": {"tf.compat.v1.keras.layers.BatchNormalization": {"description": "Layer that normalizes its inputs.", "Args": {"axis": "Integer or a list of integers, the axis that should be normalized\n(typically the features axis). For instance, after a Conv2D layer with\ndata_format=\"channels_first\", set axis=1 in BatchNormalization.", "momentum": "Momentum for the moving average.", "epsilon": "Small float added to variance to avoid dividing by zero.", "center": "If True, add offset of beta to normalized tensor. If False, beta\nis ignored.", "scale": "If True, multiply by gamma. If False, gamma is not used. When the\nnext layer is linear (also e.g. nn.relu), this can be disabled since the\nscaling will be done by the next layer.", "beta_initializer": "Initializer for the beta weight.", "gamma_initializer": "Initializer for the gamma weight.", "moving_mean_initializer": "Initializer for the moving mean.", "moving_variance_initializer": "Initializer for the moving variance.", "beta_regularizer": "Optional regularizer for the beta weight.", "gamma_regularizer": "Optional regularizer for the gamma weight.", "beta_constraint": "Optional constraint for the beta weight.", "gamma_constraint": "Optional constraint for the gamma weight.", "renorm": "Whether to use Batch Renormalization. This adds extra variables during\n  training. The inference is the same for either value of this parameter.", "renorm_clipping": "A dictionary that may map keys 'rmax', 'rmin', 'dmax' to\nscalar Tensors used to clip the renorm correction. The correction (r,\nd) is used as corrected_value = normalized_value * r + d, with r\nclipped to [rmin, rmax], and d to [-dmax, dmax]. Missing rmax, rmin,\ndmax are set to inf, 0, inf, respectively.", "renorm_momentum": "Momentum used to update the moving means and standard\ndeviations with renorm. Unlike momentum, this affects training and\nshould be neither too small (which would add noise) nor too large (which\nwould give stale estimates). Note that momentum is still applied to get\nthe means and variances for inference.", "fused": "if True, use a faster, fused implementation, or raise a ValueError\nif the fused implementation cannot be used. If None, use the faster\nimplementation if possible. If False, do not used the fused\nimplementation.\nNote that in TensorFlow 1.x, the meaning of fused=True is different: if\n  False, the layer uses the system-recommended implementation.", "trainable": "Boolean, if True the variables will be marked as trainable.", "virtual_batch_size": "An int. By default, virtual_batch_size is None,\nwhich means batch normalization is performed across the whole batch. When\nvirtual_batch_size is not None, instead perform \"Ghost Batch\nNormalization\", which creates virtual sub-batches which are each\nnormalized separately (with shared gamma, beta, and moving statistics).\nMust divide the actual batch size during execution.", "adjustment": "A function taking the Tensor containing the (dynamic) shape of\nthe input tensor and returning a pair (scale, bias) to apply to the\nnormalized values (before gamma and beta), only during training. For\nexample, if axis=-1,\n  adjustment = lambda shape: (\n    tf.random.uniform(shape[-1:], 0.93, 1.07),\n    tf.random.uniform(shape[-1:], -0.1, 0.1)) will scale the normalized\n      value by up to 7% up or down, then shift the result by up to 0.1\n      (with independent scaling and bias for each feature but shared\n      across all examples), and finally apply gamma and/or beta. If\n      None, no adjustment is applied. Cannot be specified if\n      virtual_batch_size is specified."}}, "tf.compat.v1.keras.layers.CuDNNGRU": {"description": "Fast GRU implementation backed by cuDNN.", "Args": {"units": "Positive integer, dimensionality of the output space.", "kernel_initializer": "Initializer for the kernel weights matrix, used for\nthe linear transformation of the inputs.", "recurrent_initializer": "Initializer for the recurrent_kernel weights\nmatrix, used for the linear transformation of the recurrent state.", "bias_initializer": "Initializer for the bias vector.", "kernel_regularizer": "Regularizer function applied to the kernel weights\nmatrix.", "recurrent_regularizer": "Regularizer function applied to the\nrecurrent_kernel weights matrix.", "bias_regularizer": "Regularizer function applied to the bias vector.", "activity_regularizer": "Regularizer function applied to the output of the\nlayer (its \"activation\").", "kernel_constraint": "Constraint function applied to the kernel weights\nmatrix.", "recurrent_constraint": "Constraint function applied to the\nrecurrent_kernel weights matrix.", "bias_constraint": "Constraint function applied to the bias vector.", "return_sequences": "Boolean. Whether to return the last output in the output\nsequence, or the full sequence.", "return_state": "Boolean. Whether to return the last state in addition to the\noutput.", "go_backwards": "Boolean (default False). If True, process the input sequence\nbackwards and return the reversed sequence.", "stateful": "Boolean (default False). If True, the last state for each sample\nat index i in a batch will be used as initial state for the sample of\nindex i in the following batch."}, "Attributes": {"cell": "", "states": ""}}, "tf.compat.v1.keras.layers.CuDNNLSTM": {"description": "Fast LSTM implementation backed by cuDNN.", "Args": {"units": "Positive integer, dimensionality of the output space.", "kernel_initializer": "Initializer for the kernel weights matrix, used for\nthe linear transformation of the inputs.", "unit_forget_bias": "Boolean. If True, add 1 to the bias of the forget gate\nat initialization. Setting it to true will also force\nbias_initializer=\"zeros\". This is recommended in Jozefowicz et\nal.", "recurrent_initializer": "Initializer for the recurrent_kernel weights\nmatrix, used for the linear transformation of the recurrent state.", "bias_initializer": "Initializer for the bias vector.", "kernel_regularizer": "Regularizer function applied to the kernel weights\nmatrix.", "recurrent_regularizer": "Regularizer function applied to the\nrecurrent_kernel weights matrix.", "bias_regularizer": "Regularizer function applied to the bias vector.", "activity_regularizer": "Regularizer function applied to the output of the\nlayer (its \"activation\").", "kernel_constraint": "Constraint function applied to the kernel weights\nmatrix.", "recurrent_constraint": "Constraint function applied to the\nrecurrent_kernel weights matrix.", "bias_constraint": "Constraint function applied to the bias vector.", "return_sequences": "Boolean. Whether to return the last output. in the\noutput sequence, or the full sequence.", "return_state": "Boolean. Whether to return the last state in addition to the\noutput.", "go_backwards": "Boolean (default False). If True, process the input sequence\nbackwards and return the reversed sequence.", "stateful": "Boolean (default False). If True, the last state for each sample\nat index i in a batch will be used as initial state for the sample of\nindex i in the following batch."}, "Attributes": {"cell": "", "states": ""}}, "tf.compat.v1.keras.layers.DenseFeatures": {"description": "A layer that produces a dense Tensor based on given feature_columns.", "Args": {"feature_columns": "An iterable containing the FeatureColumns to use as\ninputs to your model. All items should be instances of classes derived\nfrom DenseColumn such as numeric_column, embedding_column,\nbucketized_column, indicator_column. If you have categorical\nfeatures, you can wrap them with an embedding_column or\nindicator_column.", "trainable": "Boolean, whether the layer's variables will be updated via\ngradient descent during training.", "name": "Name to give to the DenseFeatures.", "partitioner": "Partitioner for input layer. Defaults to None.", "**kwargs": "Keyword arguments to construct a layer."}, "Raises": {"ValueError": "if an item in feature_columns is not a DenseColumn."}}, "tf.compat.v1.keras.layers.GRU": {"description": "Gated Recurrent Unit - Cho et al. 2014.", "Args": {"units": "Positive integer, dimensionality of the output space.", "activation": "Activation function to use.\nDefault: hyperbolic tangent (tanh).\nIf you pass None, no activation is applied\n(ie. \"linear\" activation: a(x) = x).", "recurrent_activation": "Activation function to use\nfor the recurrent step.\nDefault: hard sigmoid (hard_sigmoid).\nIf you pass None, no activation is applied\n(ie. \"linear\" activation: a(x) = x).", "use_bias": "Boolean, whether the layer uses a bias vector.", "kernel_initializer": "Initializer for the kernel weights matrix,\nused for the linear transformation of the inputs.", "recurrent_initializer": "Initializer for the recurrent_kernel\nweights matrix, used for the linear transformation of the recurrent state.", "bias_initializer": "Initializer for the bias vector.", "kernel_regularizer": "Regularizer function applied to\nthe kernel weights matrix.", "recurrent_regularizer": "Regularizer function applied to\nthe recurrent_kernel weights matrix.", "bias_regularizer": "Regularizer function applied to the bias vector.", "activity_regularizer": "Regularizer function applied to\nthe output of the layer (its \"activation\")..", "kernel_constraint": "Constraint function applied to\nthe kernel weights matrix.", "recurrent_constraint": "Constraint function applied to\nthe recurrent_kernel weights matrix.", "bias_constraint": "Constraint function applied to the bias vector.", "dropout": "Float between 0 and 1.\nFraction of the units to drop for\nthe linear transformation of the inputs.", "recurrent_dropout": "Float between 0 and 1.\nFraction of the units to drop for\nthe linear transformation of the recurrent state.", "return_sequences": "Boolean. Whether to return the last output\nin the output sequence, or the full sequence.", "return_state": "Boolean. Whether to return the last state\nin addition to the output.", "go_backwards": "Boolean (default False).\nIf True, process the input sequence backwards and return the\nreversed sequence.", "stateful": "Boolean (default False). If True, the last state\nfor each sample at index i in a batch will be used as initial\nstate for the sample of index i in the following batch.", "unroll": "Boolean (default False).\nIf True, the network will be unrolled,\nelse a symbolic loop will be used.\nUnrolling can speed-up a RNN,\nalthough it tends to be more memory-intensive.\nUnrolling is only suitable for short sequences.", "time_major": "The shape format of the inputs and outputs tensors.\nIf True, the inputs and outputs will be in shape\n(timesteps, batch, ...), whereas in the False case, it will be\n(batch, timesteps, ...). Using time_major = True is a bit more\nefficient because it avoids transposes at the beginning and end of the\nRNN calculation. However, most TensorFlow data is batch-major, so by\ndefault this function accepts input and emits output in batch-major\nform.", "reset_after": "GRU convention (whether to apply reset gate after or\nbefore matrix multiplication). False = \"before\" (default),\nTrue = \"after\" (cuDNN compatible)."}, "Attributes": {"activation": "", "bias_constraint": "", "bias_initializer": "", "bias_regularizer": "", "dropout": "", "implementation": "", "kernel_constraint": "", "kernel_initializer": "", "kernel_regularizer": "", "recurrent_activation": "", "recurrent_constraint": "", "recurrent_dropout": "", "recurrent_initializer": "", "recurrent_regularizer": "", "reset_after": "", "states": "", "units": "", "use_bias": ""}}, "tf.compat.v1.keras.layers.GRUCell": {"description": "Cell class for the GRU layer.", "Args": {"units": "Positive integer, dimensionality of the output space.", "activation": "Activation function to use.\nDefault: hyperbolic tangent (tanh).\nIf you pass None, no activation is applied\n(ie. \"linear\" activation: a(x) = x).", "recurrent_activation": "Activation function to use\nfor the recurrent step.\nDefault: hard sigmoid (hard_sigmoid).\nIf you pass None, no activation is applied\n(ie. \"linear\" activation: a(x) = x).", "use_bias": "Boolean, whether the layer uses a bias vector.", "kernel_initializer": "Initializer for the kernel weights matrix,\nused for the linear transformation of the inputs.", "recurrent_initializer": "Initializer for the recurrent_kernel\nweights matrix,\nused for the linear transformation of the recurrent state.", "bias_initializer": "Initializer for the bias vector.", "kernel_regularizer": "Regularizer function applied to\nthe kernel weights matrix.", "recurrent_regularizer": "Regularizer function applied to\nthe recurrent_kernel weights matrix.", "bias_regularizer": "Regularizer function applied to the bias vector.", "kernel_constraint": "Constraint function applied to\nthe kernel weights matrix.", "recurrent_constraint": "Constraint function applied to\nthe recurrent_kernel weights matrix.", "bias_constraint": "Constraint function applied to the bias vector.", "dropout": "Float between 0 and 1.\nFraction of the units to drop for the linear transformation of the inputs.", "recurrent_dropout": "Float between 0 and 1.\nFraction of the units to drop for\nthe linear transformation of the recurrent state.", "reset_after": "GRU convention (whether to apply reset gate after or\nbefore matrix multiplication). False = \"before\" (default),\nTrue = \"after\" (cuDNN compatible)."}}, "tf.compat.v1.keras.layers.LSTM": {"description": "Long Short-Term Memory layer - Hochreiter 1997.", "Args": {"units": "Positive integer, dimensionality of the output space.", "activation": "Activation function to use.\nDefault: hyperbolic tangent (tanh).\nIf you pass None, no activation is applied\n(ie. \"linear\" activation: a(x) = x).", "recurrent_activation": "Activation function to use\nfor the recurrent step.\nDefault: hard sigmoid (hard_sigmoid).\nIf you pass None, no activation is applied\n(ie. \"linear\" activation: a(x) = x).", "use_bias": "Boolean, whether the layer uses a bias vector.", "kernel_initializer": "Initializer for the kernel weights matrix,\nused for the linear transformation of the inputs..", "recurrent_initializer": "Initializer for the recurrent_kernel\nweights matrix,\nused for the linear transformation of the recurrent state.", "bias_initializer": "Initializer for the bias vector.", "unit_forget_bias": "Boolean.\nIf True, add 1 to the bias of the forget gate at initialization.\nSetting it to true will also force bias_initializer=\"zeros\".\nThis is recommended in Jozefowicz et al., 2015.", "kernel_regularizer": "Regularizer function applied to\nthe kernel weights matrix.", "recurrent_regularizer": "Regularizer function applied to\nthe recurrent_kernel weights matrix.", "bias_regularizer": "Regularizer function applied to the bias vector.", "activity_regularizer": "Regularizer function applied to\nthe output of the layer (its \"activation\").", "kernel_constraint": "Constraint function applied to\nthe kernel weights matrix.", "recurrent_constraint": "Constraint function applied to\nthe recurrent_kernel weights matrix.", "bias_constraint": "Constraint function applied to the bias vector.", "dropout": "Float between 0 and 1.\nFraction of the units to drop for\nthe linear transformation of the inputs.", "recurrent_dropout": "Float between 0 and 1.\nFraction of the units to drop for\nthe linear transformation of the recurrent state.", "return_sequences": "Boolean. Whether to return the last output\nin the output sequence, or the full sequence.", "return_state": "Boolean. Whether to return the last state\nin addition to the output.", "go_backwards": "Boolean (default False).\nIf True, process the input sequence backwards and return the\nreversed sequence.", "stateful": "Boolean (default False). If True, the last state\nfor each sample at index i in a batch will be used as initial\nstate for the sample of index i in the following batch.", "unroll": "Boolean (default False).\nIf True, the network will be unrolled,\nelse a symbolic loop will be used.\nUnrolling can speed-up a RNN,\nalthough it tends to be more memory-intensive.\nUnrolling is only suitable for short sequences.", "time_major": "The shape format of the inputs and outputs tensors.\nIf True, the inputs and outputs will be in shape\n(timesteps, batch, ...), whereas in the False case, it will be\n(batch, timesteps, ...). Using time_major = True is a bit more\nefficient because it avoids transposes at the beginning and end of the\nRNN calculation. However, most TensorFlow data is batch-major, so by\ndefault this function accepts input and emits output in batch-major\nform."}, "Attributes": {"activation": "", "bias_constraint": "", "bias_initializer": "", "bias_regularizer": "", "dropout": "", "implementation": "", "kernel_constraint": "", "kernel_initializer": "", "kernel_regularizer": "", "recurrent_activation": "", "recurrent_constraint": "", "recurrent_dropout": "", "recurrent_initializer": "", "recurrent_regularizer": "", "states": "", "unit_forget_bias": "", "units": "", "use_bias": ""}}, "tf.compat.v1.keras.layers.LSTMCell": {"description": "Cell class for the LSTM layer.", "Args": {"units": "Positive integer, dimensionality of the output space.", "activation": "Activation function to use.\nDefault: hyperbolic tangent (tanh).\nIf you pass None, no activation is applied\n(ie. \"linear\" activation: a(x) = x).", "recurrent_activation": "Activation function to use\nfor the recurrent step.\nDefault: hard sigmoid (hard_sigmoid).\nIf you pass None, no activation is applied\n(ie. \"linear\" activation: a(x) = x).", "use_bias": "Boolean, whether the layer uses a bias vector.", "kernel_initializer": "Initializer for the kernel weights matrix,\nused for the linear transformation of the inputs.", "recurrent_initializer": "Initializer for the recurrent_kernel\nweights matrix,\nused for the linear transformation of the recurrent state.", "bias_initializer": "Initializer for the bias vector.", "unit_forget_bias": "Boolean.\nIf True, add 1 to the bias of the forget gate at initialization.\nSetting it to true will also force bias_initializer=\"zeros\".\nThis is recommended in Jozefowicz et al., 2015", "kernel_regularizer": "Regularizer function applied to\nthe kernel weights matrix.", "recurrent_regularizer": "Regularizer function applied to\nthe recurrent_kernel weights matrix.", "bias_regularizer": "Regularizer function applied to the bias vector.", "kernel_constraint": "Constraint function applied to\nthe kernel weights matrix.", "recurrent_constraint": "Constraint function applied to\nthe recurrent_kernel weights matrix.", "bias_constraint": "Constraint function applied to the bias vector.", "dropout": "Float between 0 and 1.\nFraction of the units to drop for\nthe linear transformation of the inputs.", "recurrent_dropout": "Float between 0 and 1.\nFraction of the units to drop for\nthe linear transformation of the recurrent state."}}, "tf.compat.v1.keras.layers.disable_v2_dtype_behavior": {"description": "Disables the V2 dtype behavior for Keras layers."}, "tf.compat.v1.keras.layers.enable_v2_dtype_behavior": {"description": "Enable the V2 dtype behavior for Keras layers."}}, "tf.compat.v1.keras.layers.experimental.preprocessing": {}, "tf.compat.v1.keras.losses": {}, "tf.compat.v1.keras.metrics": {}, "tf.compat.v1.keras.mixed_precision": {}, "tf.compat.v1.keras.models": {}, "tf.compat.v1.keras.optimizers": {}, "tf.compat.v1.keras.optimizers.legacy": {}, "tf.compat.v1.keras.optimizers.schedules": {}, "tf.compat.v1.keras.preprocessing": {}, "tf.compat.v1.keras.preprocessing.image": {}, "tf.compat.v1.keras.preprocessing.sequence": {}, "tf.compat.v1.keras.preprocessing.text": {}, "tf.compat.v1.keras.regularizers": {}, "tf.compat.v1.keras.utils": {"tf.compat.v1.keras.utils.DeterministicRandomTestTool": {"description": "DeterministicRandomTestTool is a testing tool.", "Attributes": {"operation_seed": ""}}, "tf.compat.v1.keras.utils.get_or_create_layer": {"description": "Use this method to track nested keras models in a shim-decorated method.", "Args": {"name": "A name to give the nested layer to track.", "create_layer_method": "a Callable that takes no args and returns the nested\nlayer."}, "Returns": "The created layer."}, "tf.compat.v1.keras.utils.track_tf1_style_variables": {"description": "Wrap layer & module methods in this decorator to capture tf1-style weights.", "Args": {"method": "The method to decorate. This should belong to a custom tf.Module,\ntf.keras.layers.Layer, or tf.keras.Model."}, "Returns": "The decorated method."}}, "tf.compat.v1.keras.wrappers": {}, "tf.compat.v1.keras.wrappers.scikit_learn": {}, "tf.compat.v1.layers": {"tf.compat.v1.layers.AveragePooling1D": {"description": "Average Pooling layer for 1D inputs.", "Args": {"pool_size": "An integer or tuple/list of a single integer,\nrepresenting the size of the pooling window.", "strides": "An integer or tuple/list of a single integer, specifying the\nstrides of the pooling operation.", "padding": "A string. The padding method, either 'valid' or 'same'.\nCase-insensitive.", "data_format": "A string, one of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs.\nchannels_last corresponds to inputs with shape\n(batch, length, channels) while channels_first corresponds to\ninputs with shape (batch, channels, length).", "name": "A string, the name of the layer."}, "Attributes": {"graph": "", "scope_name": ""}}, "tf.compat.v1.layers.AveragePooling2D": {"description": "Average pooling layer for 2D inputs (e.g. images).", "Args": {"pool_size": "An integer or tuple/list of 2 integers: (pool_height, pool_width)\nspecifying the size of the pooling window.\nCan be a single integer to specify the same value for\nall spatial dimensions.", "strides": "An integer or tuple/list of 2 integers,\nspecifying the strides of the pooling operation.\nCan be a single integer to specify the same value for\nall spatial dimensions.", "padding": "A string. The padding method, either 'valid' or 'same'.\nCase-insensitive.", "data_format": "A string. The ordering of the dimensions in the inputs.\nchannels_last (default) and channels_first are supported.\nchannels_last corresponds to inputs with shape\n(batch, height, width, channels) while channels_first corresponds to\ninputs with shape (batch, channels, height, width).", "name": "A string, the name of the layer."}, "Attributes": {"graph": "", "scope_name": ""}}, "tf.compat.v1.layers.AveragePooling3D": {"description": "Average pooling layer for 3D inputs (e.g. volumes).", "Args": {"pool_size": "An integer or tuple/list of 3 integers:\n(pool_depth, pool_height, pool_width)\nspecifying the size of the pooling window.\nCan be a single integer to specify the same value for\nall spatial dimensions.", "strides": "An integer or tuple/list of 3 integers,\nspecifying the strides of the pooling operation.\nCan be a single integer to specify the same value for\nall spatial dimensions.", "padding": "A string. The padding method, either 'valid' or 'same'.\nCase-insensitive.", "data_format": "A string. The ordering of the dimensions in the inputs.\nchannels_last (default) and channels_first are supported.\nchannels_last corresponds to inputs with shape\n(batch, depth, height, width, channels) while channels_first\ncorresponds to inputs with shape\n(batch, channels, depth, height, width).", "name": "A string, the name of the layer."}, "Attributes": {"graph": "", "scope_name": ""}}, "tf.compat.v1.layers.BatchNormalization": {"description": "Batch Normalization layer from (Ioffe et al., 2015).", "Args": {"axis": "An int or list of int, the axis or axes that should be normalized,\ntypically the features axis/axes. For instance, after a Conv2D layer\nwith data_format=\"channels_first\", set axis=1. If a list of axes is\nprovided, each axis in axis will be normalized\n  simultaneously. Default is -1 which uses the last axis. Note: when\n    using multi-axis batch norm, the beta, gamma, moving_mean, and\n    moving_variance variables are the same rank as the input Tensor,\n    with dimension size 1 in all reduced (non-axis) dimensions).", "momentum": "Momentum for the moving average.", "epsilon": "Small float added to variance to avoid dividing by zero.", "center": "If True, add offset of beta to normalized tensor. If False, beta\nis ignored.", "scale": "If True, multiply by gamma. If False, gamma is not used. When the\nnext layer is linear (also e.g. nn.relu), this can be disabled since the\nscaling can be done by the next layer.", "beta_initializer": "Initializer for the beta weight.", "gamma_initializer": "Initializer for the gamma weight.", "moving_mean_initializer": "Initializer for the moving mean.", "moving_variance_initializer": "Initializer for the moving variance.", "beta_regularizer": "Optional regularizer for the beta weight.", "gamma_regularizer": "Optional regularizer for the gamma weight.", "beta_constraint": "An optional projection function to be applied to the beta\nweight after being updated by an Optimizer (e.g. used to implement norm\nconstraints or value constraints for layer weights). The function must\ntake as input the unprojected variable and must return the projected\nvariable (which must have the same shape). Constraints are not safe to use\nwhen doing asynchronous distributed training.", "gamma_constraint": "An optional projection function to be applied to the\ngamma weight after being updated by an Optimizer.", "renorm": "Whether to use Batch Renormalization (Ioffe, 2017). This adds extra\nvariables during training. The inference is the same for either value of\nthis parameter.", "renorm_clipping": "A dictionary that may map keys 'rmax', 'rmin', 'dmax' to\nscalar Tensors used to clip the renorm correction. The correction (r,\nd) is used as corrected_value = normalized_value * r + d, with r\nclipped to [rmin, rmax], and d to [-dmax, dmax]. Missing rmax, rmin,\ndmax are set to inf, 0, inf, respectively.", "renorm_momentum": "Momentum used to update the moving means and standard\ndeviations with renorm. Unlike momentum, this affects training and\nshould be neither too small (which would add noise) nor too large (which\nwould give stale estimates). Note that momentum is still applied to get\nthe means and variances for inference.", "fused": "if None or True, use a faster, fused implementation if possible.\nIf False, use the system recommended implementation.", "trainable": "Boolean, if True also add variables to the graph collection\nGraphKeys.TRAINABLE_VARIABLES (see tf.Variable).", "virtual_batch_size": "An int. By default, virtual_batch_size is None,\nwhich means batch normalization is performed across the whole batch. When\nvirtual_batch_size is not None, instead perform \"Ghost Batch\nNormalization\", which creates virtual sub-batches which are each\nnormalized separately (with shared gamma, beta, and moving statistics).\nMust divide the actual batch size during execution.", "adjustment": "A function taking the Tensor containing the (dynamic) shape of\nthe input tensor and returning a pair (scale, bias) to apply to the\nnormalized values (before gamma and beta), only during training. For\nexample, if axis==-1,\n  adjustment = lambda shape: (\n    tf.random.uniform(shape[-1:], 0.93, 1.07),\n    tf.random.uniform(shape[-1:], -0.1, 0.1)) will scale the normalized\n      value by up to 7% up or down, then shift the result by up to 0.1\n      (with independent scaling and bias for each feature but shared\n      across all examples), and finally apply gamma and/or beta. If\n      None, no adjustment is applied. Cannot be specified if\n      virtual_batch_size is specified.", "name": "A string, the name of the layer."}, "Attributes": {"graph": "", "scope_name": ""}}, "tf.compat.v1.layers.Conv1D": {"description": "1D convolution layer (e.g. temporal convolution).", "Args": {"filters": "Integer, the dimensionality of the output space (i.e. the number\nof filters in the convolution).", "kernel_size": "An integer or tuple/list of a single integer, specifying the\nlength of the 1D convolution window.", "strides": "An integer or tuple/list of a single integer,\nspecifying the stride length of the convolution.\nSpecifying any stride value != 1 is incompatible with specifying\nany dilation_rate value != 1.", "padding": "One of \"valid\" or \"same\" (case-insensitive).\n\"valid\" means no padding. \"same\" results in padding evenly to\nthe left/right or up/down of the input such that output has the same\nheight/width dimension as the input.", "data_format": "A string, one of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs.\nchannels_last corresponds to inputs with shape\n(batch, length, channels) while channels_first corresponds to\ninputs with shape (batch, channels, length).", "dilation_rate": "An integer or tuple/list of a single integer, specifying\nthe dilation rate to use for dilated convolution.\nCurrently, specifying any dilation_rate value != 1 is\nincompatible with specifying any strides value != 1.", "activation": "Activation function. Set it to None to maintain a\nlinear activation.", "use_bias": "Boolean, whether the layer uses a bias.", "kernel_initializer": "An initializer for the convolution kernel.", "bias_initializer": "An initializer for the bias vector. If None, the default\ninitializer will be used.", "kernel_regularizer": "Optional regularizer for the convolution kernel.", "bias_regularizer": "Optional regularizer for the bias vector.", "activity_regularizer": "Optional regularizer function for the output.", "kernel_constraint": "Optional projection function to be applied to the\nkernel after being updated by an Optimizer (e.g. used to implement\nnorm constraints or value constraints for layer weights). The function\nmust take as input the unprojected variable and must return the\nprojected variable (which must have the same shape). Constraints are\nnot safe to use when doing asynchronous distributed training.", "bias_constraint": "Optional projection function to be applied to the\nbias after being updated by an Optimizer.", "trainable": "Boolean, if True also add variables to the graph collection\nGraphKeys.TRAINABLE_VARIABLES (see tf.Variable).", "name": "A string, the name of the layer."}, "Attributes": {"graph": "", "scope_name": ""}}, "tf.compat.v1.layers.Conv2D": {"description": "2D convolution layer (e.g. spatial convolution over images).", "Args": {"filters": "Integer, the dimensionality of the output space (i.e. the number\nof filters in the convolution).", "kernel_size": "An integer or tuple/list of 2 integers, specifying the\nheight and width of the 2D convolution window.\nCan be a single integer to specify the same value for\nall spatial dimensions.", "strides": "An integer or tuple/list of 2 integers,\nspecifying the strides of the convolution along the height and width.\nCan be a single integer to specify the same value for\nall spatial dimensions.\nSpecifying any stride value != 1 is incompatible with specifying\nany dilation_rate value != 1.", "padding": "One of \"valid\" or \"same\" (case-insensitive).\n\"valid\" means no padding. \"same\" results in padding evenly to\nthe left/right or up/down of the input such that output has the same\nheight/width dimension as the input.", "data_format": "A string, one of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs.\nchannels_last corresponds to inputs with shape\n(batch, height, width, channels) while channels_first corresponds to\ninputs with shape (batch, channels, height, width).", "dilation_rate": "An integer or tuple/list of 2 integers, specifying\nthe dilation rate to use for dilated convolution.\nCan be a single integer to specify the same value for\nall spatial dimensions.\nCurrently, specifying any dilation_rate value != 1 is\nincompatible with specifying any stride value != 1.", "activation": "Activation function. Set it to None to maintain a\nlinear activation.", "use_bias": "Boolean, whether the layer uses a bias.", "kernel_initializer": "An initializer for the convolution kernel.", "bias_initializer": "An initializer for the bias vector. If None, the default\ninitializer will be used.", "kernel_regularizer": "Optional regularizer for the convolution kernel.", "bias_regularizer": "Optional regularizer for the bias vector.", "activity_regularizer": "Optional regularizer function for the output.", "kernel_constraint": "Optional projection function to be applied to the\nkernel after being updated by an Optimizer (e.g. used to implement\nnorm constraints or value constraints for layer weights). The function\nmust take as input the unprojected variable and must return the\nprojected variable (which must have the same shape). Constraints are\nnot safe to use when doing asynchronous distributed training.", "bias_constraint": "Optional projection function to be applied to the\nbias after being updated by an Optimizer.", "trainable": "Boolean, if True also add variables to the graph collection\nGraphKeys.TRAINABLE_VARIABLES (see tf.Variable).", "name": "A string, the name of the layer."}, "Attributes": {"graph": "", "scope_name": ""}}, "tf.compat.v1.layers.Conv2DTranspose": {"description": "Transposed 2D convolution layer (sometimes called 2D Deconvolution).", "Args": {"filters": "Integer, the dimensionality of the output space (i.e. the number\nof filters in the convolution).", "kernel_size": "A tuple or list of 2 positive integers specifying the spatial\ndimensions of the filters. Can be a single integer to specify the same\nvalue for all spatial dimensions.", "strides": "A tuple or list of 2 positive integers specifying the strides\nof the convolution. Can be a single integer to specify the same value for\nall spatial dimensions.", "padding": "one of \"valid\" or \"same\" (case-insensitive).\n\"valid\" means no padding. \"same\" results in padding evenly to\nthe left/right or up/down of the input such that output has the same\nheight/width dimension as the input.", "data_format": "A string, one of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs.\nchannels_last corresponds to inputs with shape\n(batch, height, width, channels) while channels_first corresponds to\ninputs with shape (batch, channels, height, width).", "activation": "Activation function. Set it to None to maintain a\nlinear activation.", "use_bias": "Boolean, whether the layer uses a bias.", "kernel_initializer": "An initializer for the convolution kernel.", "bias_initializer": "An initializer for the bias vector. If None, the default\ninitializer will be used.", "kernel_regularizer": "Optional regularizer for the convolution kernel.", "bias_regularizer": "Optional regularizer for the bias vector.", "activity_regularizer": "Optional regularizer function for the output.", "kernel_constraint": "Optional projection function to be applied to the\nkernel after being updated by an Optimizer (e.g. used to implement\nnorm constraints or value constraints for layer weights). The function\nmust take as input the unprojected variable and must return the\nprojected variable (which must have the same shape). Constraints are\nnot safe to use when doing asynchronous distributed training.", "bias_constraint": "Optional projection function to be applied to the\nbias after being updated by an Optimizer.", "trainable": "Boolean, if True also add variables to the graph collection\nGraphKeys.TRAINABLE_VARIABLES (see tf.Variable).", "name": "A string, the name of the layer."}, "Attributes": {"graph": "", "scope_name": ""}}, "tf.compat.v1.layers.Conv3D": {"description": "3D convolution layer (e.g. spatial convolution over volumes).", "Args": {"filters": "Integer, the dimensionality of the output space (i.e. the number\nof filters in the convolution).", "kernel_size": "An integer or tuple/list of 3 integers, specifying the\ndepth, height and width of the 3D convolution window.\nCan be a single integer to specify the same value for\nall spatial dimensions.", "strides": "An integer or tuple/list of 3 integers,\nspecifying the strides of the convolution along the depth,\nheight and width.\nCan be a single integer to specify the same value for\nall spatial dimensions.\nSpecifying any stride value != 1 is incompatible with specifying\nany dilation_rate value != 1.", "padding": "One of \"valid\" or \"same\" (case-insensitive).\n\"valid\" means no padding. \"same\" results in padding evenly to\nthe left/right or up/down of the input such that output has the same\nheight/width dimension as the input.", "data_format": "A string, one of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs.\nchannels_last corresponds to inputs with shape\n(batch, depth, height, width, channels) while channels_first\ncorresponds to inputs with shape\n(batch, channels, depth, height, width).", "dilation_rate": "An integer or tuple/list of 3 integers, specifying\nthe dilation rate to use for dilated convolution.\nCan be a single integer to specify the same value for\nall spatial dimensions.\nCurrently, specifying any dilation_rate value != 1 is\nincompatible with specifying any stride value != 1.", "activation": "Activation function. Set it to None to maintain a\nlinear activation.", "use_bias": "Boolean, whether the layer uses a bias.", "kernel_initializer": "An initializer for the convolution kernel.", "bias_initializer": "An initializer for the bias vector. If None, the default\ninitializer will be used.", "kernel_regularizer": "Optional regularizer for the convolution kernel.", "bias_regularizer": "Optional regularizer for the bias vector.", "activity_regularizer": "Optional regularizer function for the output.", "kernel_constraint": "Optional projection function to be applied to the\nkernel after being updated by an Optimizer (e.g. used to implement\nnorm constraints or value constraints for layer weights). The function\nmust take as input the unprojected variable and must return the\nprojected variable (which must have the same shape). Constraints are\nnot safe to use when doing asynchronous distributed training.", "bias_constraint": "Optional projection function to be applied to the\nbias after being updated by an Optimizer.", "trainable": "Boolean, if True also add variables to the graph collection\nGraphKeys.TRAINABLE_VARIABLES (see tf.Variable).", "name": "A string, the name of the layer."}, "Attributes": {"graph": "", "scope_name": ""}}, "tf.compat.v1.layers.Conv3DTranspose": {"description": "Transposed 3D convolution layer (sometimes called 3D Deconvolution).", "Args": {"filters": "Integer, the dimensionality of the output space (i.e. the number\nof filters in the convolution).", "kernel_size": "An integer or tuple/list of 3 integers, specifying the\ndepth, height and width of the 3D convolution window.\nCan be a single integer to specify the same value for all spatial\ndimensions.", "strides": "An integer or tuple/list of 3 integers, specifying the strides\nof the convolution along the depth, height and width.\nCan be a single integer to specify the same value for all spatial\ndimensions.", "padding": "One of \"valid\" or \"same\" (case-insensitive).\n\"valid\" means no padding. \"same\" results in padding evenly to\nthe left/right or up/down of the input such that output has the same\nheight/width dimension as the input.", "data_format": "A string, one of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs.\nchannels_last corresponds to inputs with shape\n(batch, depth, height, width, channels) while channels_first\ncorresponds to inputs with shape\n(batch, channels, depth, height, width).", "activation": "Activation function. Set it to None to maintain a\nlinear activation.", "use_bias": "Boolean, whether the layer uses a bias.", "kernel_initializer": "An initializer for the convolution kernel.", "bias_initializer": "An initializer for the bias vector. If None, the default\ninitializer will be used.", "kernel_regularizer": "Optional regularizer for the convolution kernel.", "bias_regularizer": "Optional regularizer for the bias vector.", "activity_regularizer": "Optional regularizer function for the output.", "kernel_constraint": "Optional projection function to be applied to the\nkernel after being updated by an Optimizer (e.g. used to implement\nnorm constraints or value constraints for layer weights). The function\nmust take as input the unprojected variable and must return the\nprojected variable (which must have the same shape). Constraints are\nnot safe to use when doing asynchronous distributed training.", "bias_constraint": "Optional projection function to be applied to the\nbias after being updated by an Optimizer.", "trainable": "Boolean, if True also add variables to the graph collection\nGraphKeys.TRAINABLE_VARIABLES (see tf.Variable).", "name": "A string, the name of the layer."}, "Attributes": {"graph": "", "scope_name": ""}}, "tf.compat.v1.layers.Dense": {"description": "Densely-connected layer class.", "Args": {"units": "Integer or Long, dimensionality of the output space.", "activation": "Activation function (callable). Set it to None to maintain a\nlinear activation.", "use_bias": "Boolean, whether the layer uses a bias.", "kernel_initializer": "Initializer function for the weight matrix.\nIf None (default), weights are initialized using the default\ninitializer used by tf.compat.v1.get_variable.", "bias_initializer": "Initializer function for the bias.", "kernel_regularizer": "Regularizer function for the weight matrix.", "bias_regularizer": "Regularizer function for the bias.", "activity_regularizer": "Regularizer function for the output.", "kernel_constraint": "An optional projection function to be applied to the\nkernel after being updated by an Optimizer (e.g. used to implement\nnorm constraints or value constraints for layer weights). The function\nmust take as input the unprojected variable and must return the\nprojected variable (which must have the same shape). Constraints are\nnot safe to use when doing asynchronous distributed training.", "bias_constraint": "An optional projection function to be applied to the\nbias after being updated by an Optimizer.", "trainable": "Boolean, if True also add variables to the graph collection\nGraphKeys.TRAINABLE_VARIABLES (see tf.Variable).", "name": "String, the name of the layer. Layers with the same name will\nshare weights, but to avoid mistakes we require reuse=True in such cases.", "_reuse": "Boolean, whether to reuse the weights of a previous layer\nby the same name."}, "Attributes": {"graph": "", "scope_name": ""}}, "tf.compat.v1.layers.Dropout": {"description": "Applies Dropout to the input.", "Args": {"rate": "The dropout rate, between 0 and 1. E.g. rate=0.1 would drop out\n10% of input units.", "noise_shape": "1D tensor of type int32 representing the shape of the\nbinary dropout mask that will be multiplied with the input.\nFor instance, if your inputs have shape\n(batch_size, timesteps, features), and you want the dropout mask\nto be the same for all timesteps, you can use\nnoise_shape=[batch_size, 1, features].", "seed": "A Python integer. Used to create random seeds. See\ntf.compat.v1.set_random_seed.\nfor behavior.", "name": "The name of the layer (string)."}, "Attributes": {"graph": "", "scope_name": ""}}, "tf.compat.v1.layers.Flatten": {"description": "Flattens an input tensor while preserving the batch axis (axis 0).", "Args": {"data_format": "A string, one of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs.\nchannels_last corresponds to inputs with shape\n(batch, ..., channels) while channels_first corresponds to\ninputs with shape (batch, channels, ...)."}, "Attributes": {"graph": "", "scope_name": ""}}, "tf.compat.v1.layers.Layer": {"description": "Base layer class.", "Args": {"trainable": "Boolean, whether the layer's variables should be trainable.", "name": "String name of the layer.", "dtype": "Default dtype of the layer's weights (default of None means use the\ntype of the first input)."}, "Attributes": {"graph": "", "scope_name": ""}}, "tf.compat.v1.layers.MaxPooling1D": {"description": "Max Pooling layer for 1D inputs.", "Args": {"pool_size": "An integer or tuple/list of a single integer,\nrepresenting the size of the pooling window.", "strides": "An integer or tuple/list of a single integer, specifying the\nstrides of the pooling operation.", "padding": "A string. The padding method, either 'valid' or 'same'.\nCase-insensitive.", "data_format": "A string, one of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs.\nchannels_last corresponds to inputs with shape\n(batch, length, channels) while channels_first corresponds to\ninputs with shape (batch, channels, length).", "name": "A string, the name of the layer."}, "Attributes": {"graph": "", "scope_name": ""}}, "tf.compat.v1.layers.MaxPooling2D": {"description": "Max pooling layer for 2D inputs (e.g. images).", "Args": {"pool_size": "An integer or tuple/list of 2 integers: (pool_height, pool_width)\nspecifying the size of the pooling window.\nCan be a single integer to specify the same value for\nall spatial dimensions.", "strides": "An integer or tuple/list of 2 integers,\nspecifying the strides of the pooling operation.\nCan be a single integer to specify the same value for\nall spatial dimensions.", "padding": "A string. The padding method, either 'valid' or 'same'.\nCase-insensitive.", "data_format": "A string. The ordering of the dimensions in the inputs.\nchannels_last (default) and channels_first are supported.\nchannels_last corresponds to inputs with shape\n(batch, height, width, channels) while channels_first corresponds to\ninputs with shape (batch, channels, height, width).", "name": "A string, the name of the layer."}, "Attributes": {"graph": "", "scope_name": ""}}, "tf.compat.v1.layers.MaxPooling3D": {"description": "Max pooling layer for 3D inputs (e.g. volumes).", "Args": {"pool_size": "An integer or tuple/list of 3 integers:\n(pool_depth, pool_height, pool_width)\nspecifying the size of the pooling window.\nCan be a single integer to specify the same value for\nall spatial dimensions.", "strides": "An integer or tuple/list of 3 integers,\nspecifying the strides of the pooling operation.\nCan be a single integer to specify the same value for\nall spatial dimensions.", "padding": "A string. The padding method, either 'valid' or 'same'.\nCase-insensitive.", "data_format": "A string. The ordering of the dimensions in the inputs.\nchannels_last (default) and channels_first are supported.\nchannels_last corresponds to inputs with shape\n(batch, depth, height, width, channels) while channels_first\ncorresponds to inputs with shape\n(batch, channels, depth, height, width).", "name": "A string, the name of the layer."}, "Attributes": {"graph": "", "scope_name": ""}}, "tf.compat.v1.layers.SeparableConv1D": {"description": "Depthwise separable 1D convolution.", "Args": {"filters": "Integer, the dimensionality of the output space (i.e. the number\nof filters in the convolution).", "kernel_size": "A single integer specifying the spatial\ndimensions of the filters.", "strides": "A single integer specifying the strides\nof the convolution.\nSpecifying any stride value != 1 is incompatible with specifying\nany dilation_rate value != 1.", "padding": "One of \"valid\" or \"same\" (case-insensitive).\n\"valid\" means no padding. \"same\" results in padding evenly to\nthe left/right or up/down of the input such that output has the same\nheight/width dimension as the input.", "data_format": "A string, one of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs.\nchannels_last corresponds to inputs with shape\n(batch, length, channels) while channels_first corresponds to\ninputs with shape (batch, channels, length).", "dilation_rate": "A single integer, specifying\nthe dilation rate to use for dilated convolution.\nCurrently, specifying any dilation_rate value != 1 is\nincompatible with specifying any stride value != 1.", "depth_multiplier": "The number of depthwise convolution output channels for\neach input channel. The total number of depthwise convolution output\nchannels will be equal to num_filters_in * depth_multiplier.", "activation": "Activation function. Set it to None to maintain a\nlinear activation.", "use_bias": "Boolean, whether the layer uses a bias.", "depthwise_initializer": "An initializer for the depthwise convolution kernel.", "pointwise_initializer": "An initializer for the pointwise convolution kernel.", "bias_initializer": "An initializer for the bias vector. If None, the default\ninitializer will be used.", "depthwise_regularizer": "Optional regularizer for the depthwise\nconvolution kernel.", "pointwise_regularizer": "Optional regularizer for the pointwise\nconvolution kernel.", "bias_regularizer": "Optional regularizer for the bias vector.", "activity_regularizer": "Optional regularizer function for the output.", "depthwise_constraint": "Optional projection function to be applied to the\ndepthwise kernel after being updated by an Optimizer (e.g. used for\nnorm constraints or value constraints for layer weights). The function\nmust take as input the unprojected variable and must return the\nprojected variable (which must have the same shape). Constraints are\nnot safe to use when doing asynchronous distributed training.", "pointwise_constraint": "Optional projection function to be applied to the\npointwise kernel after being updated by an Optimizer.", "bias_constraint": "Optional projection function to be applied to the\nbias after being updated by an Optimizer.", "trainable": "Boolean, if True also add variables to the graph collection\nGraphKeys.TRAINABLE_VARIABLES (see tf.Variable).", "name": "A string, the name of the layer."}, "Attributes": {"graph": "", "scope_name": ""}}, "tf.compat.v1.layers.SeparableConv2D": {"description": "Depthwise separable 2D convolution.", "Args": {"filters": "Integer, the dimensionality of the output space (i.e. the number\nof filters in the convolution).", "kernel_size": "A tuple or list of 2 integers specifying the spatial\ndimensions of the filters. Can be a single integer to specify the same\nvalue for all spatial dimensions.", "strides": "A tuple or list of 2 positive integers specifying the strides\nof the convolution. Can be a single integer to specify the same value for\nall spatial dimensions.\nSpecifying any stride value != 1 is incompatible with specifying\nany dilation_rate value != 1.", "padding": "One of \"valid\" or \"same\" (case-insensitive).\n\"valid\" means no padding. \"same\" results in padding evenly to\nthe left/right or up/down of the input such that output has the same\nheight/width dimension as the input.", "data_format": "A string, one of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs.\nchannels_last corresponds to inputs with shape\n(batch, height, width, channels) while channels_first corresponds to\ninputs with shape (batch, channels, height, width).", "dilation_rate": "An integer or tuple/list of 2 integers, specifying\nthe dilation rate to use for dilated convolution.\nCan be a single integer to specify the same value for\nall spatial dimensions.\nCurrently, specifying any dilation_rate value != 1 is\nincompatible with specifying any stride value != 1.", "depth_multiplier": "The number of depthwise convolution output channels for\neach input channel. The total number of depthwise convolution output\nchannels will be equal to num_filters_in * depth_multiplier.", "activation": "Activation function. Set it to None to maintain a\nlinear activation.", "use_bias": "Boolean, whether the layer uses a bias.", "depthwise_initializer": "An initializer for the depthwise convolution kernel.", "pointwise_initializer": "An initializer for the pointwise convolution kernel.", "bias_initializer": "An initializer for the bias vector. If None, the default\ninitializer will be used.", "depthwise_regularizer": "Optional regularizer for the depthwise\nconvolution kernel.", "pointwise_regularizer": "Optional regularizer for the pointwise\nconvolution kernel.", "bias_regularizer": "Optional regularizer for the bias vector.", "activity_regularizer": "Optional regularizer function for the output.", "depthwise_constraint": "Optional projection function to be applied to the\ndepthwise kernel after being updated by an Optimizer (e.g. used for\nnorm constraints or value constraints for layer weights). The function\nmust take as input the unprojected variable and must return the\nprojected variable (which must have the same shape). Constraints are\nnot safe to use when doing asynchronous distributed training.", "pointwise_constraint": "Optional projection function to be applied to the\npointwise kernel after being updated by an Optimizer.", "bias_constraint": "Optional projection function to be applied to the\nbias after being updated by an Optimizer.", "trainable": "Boolean, if True also add variables to the graph collection\nGraphKeys.TRAINABLE_VARIABLES (see tf.Variable).", "name": "A string, the name of the layer."}, "Attributes": {"graph": "", "scope_name": ""}}, "tf.compat.v1.layers.average_pooling1d": {"description": "Average Pooling layer for 1D inputs.", "Args": {"inputs": "The tensor over which to pool. Must have rank 3.", "pool_size": "An integer or tuple/list of a single integer,\nrepresenting the size of the pooling window.", "strides": "An integer or tuple/list of a single integer, specifying the\nstrides of the pooling operation.", "padding": "A string. The padding method, either 'valid' or 'same'.\nCase-insensitive.", "data_format": "A string, one of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs.\nchannels_last corresponds to inputs with shape\n(batch, length, channels) while channels_first corresponds to\ninputs with shape (batch, channels, length).", "name": "A string, the name of the layer."}, "Returns": "The output tensor, of rank 3.", "Raises": {"ValueError": "if eager execution is enabled."}}, "tf.compat.v1.layers.average_pooling2d": {"description": "Average pooling layer for 2D inputs (e.g. images).", "Args": {"inputs": "The tensor over which to pool. Must have rank 4.", "pool_size": "An integer or tuple/list of 2 integers: (pool_height, pool_width)\nspecifying the size of the pooling window.\nCan be a single integer to specify the same value for\nall spatial dimensions.", "strides": "An integer or tuple/list of 2 integers,\nspecifying the strides of the pooling operation.\nCan be a single integer to specify the same value for\nall spatial dimensions.", "padding": "A string. The padding method, either 'valid' or 'same'.\nCase-insensitive.", "data_format": "A string. The ordering of the dimensions in the inputs.\nchannels_last (default) and channels_first are supported.\nchannels_last corresponds to inputs with shape\n(batch, height, width, channels) while channels_first corresponds to\ninputs with shape (batch, channels, height, width).", "name": "A string, the name of the layer."}, "Returns": "Output tensor.", "Raises": {"ValueError": "if eager execution is enabled."}}, "tf.compat.v1.layers.average_pooling3d": {"description": "Average pooling layer for 3D inputs (e.g. volumes).", "Args": {"inputs": "The tensor over which to pool. Must have rank 5.", "pool_size": "An integer or tuple/list of 3 integers:\n(pool_depth, pool_height, pool_width)\nspecifying the size of the pooling window.\nCan be a single integer to specify the same value for\nall spatial dimensions.", "strides": "An integer or tuple/list of 3 integers,\nspecifying the strides of the pooling operation.\nCan be a single integer to specify the same value for\nall spatial dimensions.", "padding": "A string. The padding method, either 'valid' or 'same'.\nCase-insensitive.", "data_format": "A string. The ordering of the dimensions in the inputs.\nchannels_last (default) and channels_first are supported.\nchannels_last corresponds to inputs with shape\n(batch, depth, height, width, channels) while channels_first\ncorresponds to inputs with shape\n(batch, channels, depth, height, width).", "name": "A string, the name of the layer."}, "Returns": "Output tensor.", "Raises": {"ValueError": "if eager execution is enabled."}}, "tf.compat.v1.layers.batch_normalization": {"description": "Functional interface for the batch normalization layer from_config(Ioffe et al., 2015).", "Args": {"inputs": "Tensor input.", "axis": "An int, the axis that should be normalized (typically the features\naxis). For instance, after a Convolution2D layer with\ndata_format=\"channels_first\", set axis=1 in BatchNormalization.", "momentum": "Momentum for the moving average.", "epsilon": "Small float added to variance to avoid dividing by zero.", "center": "If True, add offset of beta to normalized tensor. If False, beta\nis ignored.", "scale": "If True, multiply by gamma. If False, gamma is not used. When the\nnext layer is linear (also e.g. nn.relu), this can be disabled since the\nscaling can be done by the next layer.", "beta_initializer": "Initializer for the beta weight.", "gamma_initializer": "Initializer for the gamma weight.", "moving_mean_initializer": "Initializer for the moving mean.", "moving_variance_initializer": "Initializer for the moving variance.", "beta_regularizer": "Optional regularizer for the beta weight.", "gamma_regularizer": "Optional regularizer for the gamma weight.", "beta_constraint": "An optional projection function to be applied to the beta\nweight after being updated by an Optimizer (e.g. used to implement norm\nconstraints or value constraints for layer weights). The function must\ntake as input the unprojected variable and must return the projected\nvariable (which must have the same shape). Constraints are not safe to use\nwhen doing asynchronous distributed training.", "gamma_constraint": "An optional projection function to be applied to the\ngamma weight after being updated by an Optimizer.", "training": "Either a Python boolean, or a TensorFlow boolean scalar tensor\n(e.g. a placeholder). Whether to return the output in training mode\n(normalized with statistics of the current batch) or in inference mode\n(normalized with moving statistics). NOTE: make sure to set this\n  parameter correctly, or else your training/inference will not work\n  properly.", "trainable": "Boolean, if True also add variables to the graph collection\nGraphKeys.TRAINABLE_VARIABLES (see tf.Variable).", "name": "String, the name of the layer.", "reuse": "Boolean, whether to reuse the weights of a previous layer by the same\nname.", "renorm": "Whether to use Batch Renormalization (Ioffe, 2017). This adds extra\nvariables during training. The inference is the same for either value of\nthis parameter.", "renorm_clipping": "A dictionary that may map keys 'rmax', 'rmin', 'dmax' to\nscalar Tensors used to clip the renorm correction. The correction (r,\nd) is used as corrected_value = normalized_value * r + d, with r\nclipped to [rmin, rmax], and d to [-dmax, dmax]. Missing rmax, rmin,\ndmax are set to inf, 0, inf, respectively.", "renorm_momentum": "Momentum used to update the moving means and standard\ndeviations with renorm. Unlike momentum, this affects training and\nshould be neither too small (which would add noise) nor too large (which\nwould give stale estimates). Note that momentum is still applied to get\nthe means and variances for inference.", "fused": "if None or True, use a faster, fused implementation if possible.\nIf False, use the system recommended implementation.", "virtual_batch_size": "An int. By default, virtual_batch_size is None,\nwhich means batch normalization is performed across the whole batch. When\nvirtual_batch_size is not None, instead perform \"Ghost Batch\nNormalization\", which creates virtual sub-batches which are each\nnormalized separately (with shared gamma, beta, and moving statistics).\nMust divide the actual batch size during execution.", "adjustment": "A function taking the Tensor containing the (dynamic) shape of\nthe input tensor and returning a pair (scale, bias) to apply to the\nnormalized values (before gamma and beta), only during training. For\nexample, if axis==-1,\n  adjustment = lambda shape: (\n    tf.random.uniform(shape[-1:], 0.93, 1.07),\n    tf.random.uniform(shape[-1:], -0.1, 0.1)) will scale the normalized\n      value by up to 7% up or down, then shift the result by up to 0.1\n      (with independent scaling and bias for each feature but shared\n      across all examples), and finally apply gamma and/or beta. If\n      None, no adjustment is applied. Cannot be specified if\n      virtual_batch_size is specified."}, "Returns": "Output tensor.", "Raises": {"ValueError": "if eager execution is enabled."}}, "tf.compat.v1.layers.conv1d": {"description": "Functional interface for 1D convolution layer (e.g. temporal convolution).", "Args": {"inputs": "Tensor input.", "filters": "Integer, the dimensionality of the output space (i.e. the number\nof filters in the convolution).", "kernel_size": "An integer or tuple/list of a single integer, specifying the\nlength of the 1D convolution window.", "strides": "An integer or tuple/list of a single integer,\nspecifying the stride length of the convolution.\nSpecifying any stride value != 1 is incompatible with specifying\nany dilation_rate value != 1.", "padding": "One of \"valid\" or \"same\" (case-insensitive).\n\"valid\" means no padding. \"same\" results in padding evenly to\nthe left/right or up/down of the input such that output has the same\nheight/width dimension as the input.", "data_format": "A string, one of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs.\nchannels_last corresponds to inputs with shape\n(batch, length, channels) while channels_first corresponds to\ninputs with shape (batch, channels, length).", "dilation_rate": "An integer or tuple/list of a single integer, specifying\nthe dilation rate to use for dilated convolution.\nCurrently, specifying any dilation_rate value != 1 is\nincompatible with specifying any strides value != 1.", "activation": "Activation function. Set it to None to maintain a\nlinear activation.", "use_bias": "Boolean, whether the layer uses a bias.", "kernel_initializer": "An initializer for the convolution kernel.", "bias_initializer": "An initializer for the bias vector. If None, the default\ninitializer will be used.", "kernel_regularizer": "Optional regularizer for the convolution kernel.", "bias_regularizer": "Optional regularizer for the bias vector.", "activity_regularizer": "Optional regularizer function for the output.", "kernel_constraint": "Optional projection function to be applied to the\nkernel after being updated by an Optimizer (e.g. used to implement\nnorm constraints or value constraints for layer weights). The function\nmust take as input the unprojected variable and must return the\nprojected variable (which must have the same shape). Constraints are\nnot safe to use when doing asynchronous distributed training.", "bias_constraint": "Optional projection function to be applied to the\nbias after being updated by an Optimizer.", "trainable": "Boolean, if True also add variables to the graph collection\nGraphKeys.TRAINABLE_VARIABLES (see tf.Variable).", "name": "A string, the name of the layer.", "reuse": "Boolean, whether to reuse the weights of a previous layer\nby the same name."}, "Returns": "Output tensor.", "Raises": {"ValueError": "if eager execution is enabled."}}, "tf.compat.v1.layers.conv2d": {"description": "Functional interface for the 2D convolution layer.", "Args": {"inputs": "Tensor input.", "filters": "Integer, the dimensionality of the output space (i.e. the number\nof filters in the convolution).", "kernel_size": "An integer or tuple/list of 2 integers, specifying the\nheight and width of the 2D convolution window.\nCan be a single integer to specify the same value for\nall spatial dimensions.", "strides": "An integer or tuple/list of 2 integers,\nspecifying the strides of the convolution along the height and width.\nCan be a single integer to specify the same value for\nall spatial dimensions.\nSpecifying any stride value != 1 is incompatible with specifying\nany dilation_rate value != 1.", "padding": "One of \"valid\" or \"same\" (case-insensitive).\n\"valid\" means no padding. \"same\" results in padding evenly to\nthe left/right or up/down of the input such that output has the same\nheight/width dimension as the input.", "data_format": "A string, one of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs.\nchannels_last corresponds to inputs with shape\n(batch, height, width, channels) while channels_first corresponds to\ninputs with shape (batch, channels, height, width).", "dilation_rate": "An integer or tuple/list of 2 integers, specifying\nthe dilation rate to use for dilated convolution.\nCan be a single integer to specify the same value for\nall spatial dimensions.\nCurrently, specifying any dilation_rate value != 1 is\nincompatible with specifying any stride value != 1.", "activation": "Activation function. Set it to None to maintain a\nlinear activation.", "use_bias": "Boolean, whether the layer uses a bias.", "kernel_initializer": "An initializer for the convolution kernel.", "bias_initializer": "An initializer for the bias vector. If None, the default\ninitializer will be used.", "kernel_regularizer": "Optional regularizer for the convolution kernel.", "bias_regularizer": "Optional regularizer for the bias vector.", "activity_regularizer": "Optional regularizer function for the output.", "kernel_constraint": "Optional projection function to be applied to the\nkernel after being updated by an Optimizer (e.g. used to implement\nnorm constraints or value constraints for layer weights). The function\nmust take as input the unprojected variable and must return the\nprojected variable (which must have the same shape). Constraints are\nnot safe to use when doing asynchronous distributed training.", "bias_constraint": "Optional projection function to be applied to the\nbias after being updated by an Optimizer.", "trainable": "Boolean, if True also add variables to the graph collection\nGraphKeys.TRAINABLE_VARIABLES (see tf.Variable).", "name": "A string, the name of the layer.", "reuse": "Boolean, whether to reuse the weights of a previous layer\nby the same name."}, "Returns": "Output tensor.", "Raises": {"ValueError": "if eager execution is enabled."}}, "tf.compat.v1.layers.conv2d_transpose": {"description": "Functional interface for transposed 2D convolution layer.", "Args": {"inputs": "Input tensor.", "filters": "Integer, the dimensionality of the output space (i.e. the number\nof filters in the convolution).", "kernel_size": "A tuple or list of 2 positive integers specifying the spatial\ndimensions of the filters. Can be a single integer to specify the same\nvalue for all spatial dimensions.", "strides": "A tuple or list of 2 positive integers specifying the strides\nof the convolution. Can be a single integer to specify the same value for\nall spatial dimensions.", "padding": "one of \"valid\" or \"same\" (case-insensitive).\n\"valid\" means no padding. \"same\" results in padding evenly to\nthe left/right or up/down of the input such that output has the same\nheight/width dimension as the input.", "data_format": "A string, one of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs.\nchannels_last corresponds to inputs with shape\n(batch, height, width, channels) while channels_first corresponds to\ninputs with shape (batch, channels, height, width).", "activation": "Activation function. Set it to None to maintain a\nlinear activation.", "use_bias": "Boolean, whether the layer uses a bias.", "kernel_initializer": "An initializer for the convolution kernel.", "bias_initializer": "An initializer for the bias vector. If None, the default\ninitializer will be used.", "kernel_regularizer": "Optional regularizer for the convolution kernel.", "bias_regularizer": "Optional regularizer for the bias vector.", "activity_regularizer": "Optional regularizer function for the output.", "kernel_constraint": "Optional projection function to be applied to the\nkernel after being updated by an Optimizer (e.g. used to implement\nnorm constraints or value constraints for layer weights). The function\nmust take as input the unprojected variable and must return the\nprojected variable (which must have the same shape). Constraints are\nnot safe to use when doing asynchronous distributed training.", "bias_constraint": "Optional projection function to be applied to the\nbias after being updated by an Optimizer.", "trainable": "Boolean, if True also add variables to the graph collection\nGraphKeys.TRAINABLE_VARIABLES (see tf.Variable).", "name": "A string, the name of the layer.", "reuse": "Boolean, whether to reuse the weights of a previous layer\nby the same name."}, "Returns": "Output tensor.", "Raises": {"ValueError": "if eager execution is enabled."}}, "tf.compat.v1.layers.conv3d": {"description": "Functional interface for the 3D convolution layer.", "Args": {"inputs": "Tensor input.", "filters": "Integer, the dimensionality of the output space (i.e. the number\nof filters in the convolution).", "kernel_size": "An integer or tuple/list of 3 integers, specifying the\ndepth, height and width of the 3D convolution window.\nCan be a single integer to specify the same value for\nall spatial dimensions.", "strides": "An integer or tuple/list of 3 integers,\nspecifying the strides of the convolution along the depth,\nheight and width.\nCan be a single integer to specify the same value for\nall spatial dimensions.\nSpecifying any stride value != 1 is incompatible with specifying\nany dilation_rate value != 1.", "padding": "One of \"valid\" or \"same\" (case-insensitive).\n\"valid\" means no padding. \"same\" results in padding evenly to\nthe left/right or up/down of the input such that output has the same\nheight/width dimension as the input.", "data_format": "A string, one of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs.\nchannels_last corresponds to inputs with shape\n(batch, depth, height, width, channels) while channels_first\ncorresponds to inputs with shape\n(batch, channels, depth, height, width).", "dilation_rate": "An integer or tuple/list of 3 integers, specifying\nthe dilation rate to use for dilated convolution.\nCan be a single integer to specify the same value for\nall spatial dimensions.\nCurrently, specifying any dilation_rate value != 1 is\nincompatible with specifying any stride value != 1.", "activation": "Activation function. Set it to None to maintain a\nlinear activation.", "use_bias": "Boolean, whether the layer uses a bias.", "kernel_initializer": "An initializer for the convolution kernel.", "bias_initializer": "An initializer for the bias vector. If None, the default\ninitializer will be used.", "kernel_regularizer": "Optional regularizer for the convolution kernel.", "bias_regularizer": "Optional regularizer for the bias vector.", "activity_regularizer": "Optional regularizer function for the output.", "kernel_constraint": "Optional projection function to be applied to the\nkernel after being updated by an Optimizer (e.g. used to implement\nnorm constraints or value constraints for layer weights). The function\nmust take as input the unprojected variable and must return the\nprojected variable (which must have the same shape). Constraints are\nnot safe to use when doing asynchronous distributed training.", "bias_constraint": "Optional projection function to be applied to the\nbias after being updated by an Optimizer.", "trainable": "Boolean, if True also add variables to the graph collection\nGraphKeys.TRAINABLE_VARIABLES (see tf.Variable).", "name": "A string, the name of the layer.", "reuse": "Boolean, whether to reuse the weights of a previous layer\nby the same name."}, "Returns": "Output tensor.", "Raises": {"ValueError": "if eager execution is enabled."}}, "tf.compat.v1.layers.conv3d_transpose": {"description": "Functional interface for transposed 3D convolution layer.", "Args": {"inputs": "Input tensor.", "filters": "Integer, the dimensionality of the output space (i.e. the number\nof filters in the convolution).", "kernel_size": "A tuple or list of 3 positive integers specifying the spatial\ndimensions of the filters. Can be a single integer to specify the same\nvalue for all spatial dimensions.", "strides": "A tuple or list of 3 positive integers specifying the strides\nof the convolution. Can be a single integer to specify the same value for\nall spatial dimensions.", "padding": "one of \"valid\" or \"same\" (case-insensitive).\n\"valid\" means no padding. \"same\" results in padding evenly to\nthe left/right or up/down of the input such that output has the same\nheight/width dimension as the input.", "data_format": "A string, one of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs.\nchannels_last corresponds to inputs with shape\n(batch, depth, height, width, channels) while channels_first\ncorresponds to inputs with shape\n(batch, channels, depth, height, width).", "activation": "Activation function. Set it to None to maintain a\nlinear activation.", "use_bias": "Boolean, whether the layer uses a bias.", "kernel_initializer": "An initializer for the convolution kernel.", "bias_initializer": "An initializer for the bias vector. If None, the default\ninitializer will be used.", "kernel_regularizer": "Optional regularizer for the convolution kernel.", "bias_regularizer": "Optional regularizer for the bias vector.", "activity_regularizer": "Optional regularizer function for the output.", "kernel_constraint": "Optional projection function to be applied to the\nkernel after being updated by an Optimizer (e.g. used to implement\nnorm constraints or value constraints for layer weights). The function\nmust take as input the unprojected variable and must return the\nprojected variable (which must have the same shape). Constraints are\nnot safe to use when doing asynchronous distributed training.", "bias_constraint": "Optional projection function to be applied to the\nbias after being updated by an Optimizer.", "trainable": "Boolean, if True also add variables to the graph collection\nGraphKeys.TRAINABLE_VARIABLES (see tf.Variable).", "name": "A string, the name of the layer.", "reuse": "Boolean, whether to reuse the weights of a previous layer\nby the same name."}, "Returns": "Output tensor.", "Raises": {"ValueError": "if eager execution is enabled."}}, "tf.compat.v1.layers.dense": {"description": "Functional interface for the densely-connected layer.", "Args": {"inputs": "Tensor input.", "units": "Integer or Long, dimensionality of the output space.", "activation": "Activation function (callable). Set it to None to maintain a\nlinear activation.", "use_bias": "Boolean, whether the layer uses a bias.", "kernel_initializer": "Initializer function for the weight matrix.\nIf None (default), weights are initialized using the default\ninitializer used by tf.compat.v1.get_variable.", "bias_initializer": "Initializer function for the bias.", "kernel_regularizer": "Regularizer function for the weight matrix.", "bias_regularizer": "Regularizer function for the bias.", "activity_regularizer": "Regularizer function for the output.", "kernel_constraint": "An optional projection function to be applied to the\nkernel after being updated by an Optimizer (e.g. used to implement\nnorm constraints or value constraints for layer weights). The function\nmust take as input the unprojected variable and must return the\nprojected variable (which must have the same shape). Constraints are\nnot safe to use when doing asynchronous distributed training.", "bias_constraint": "An optional projection function to be applied to the\nbias after being updated by an Optimizer.", "trainable": "Boolean, if True also add variables to the graph collection\nGraphKeys.TRAINABLE_VARIABLES (see tf.Variable).", "name": "String, the name of the layer.", "reuse": "Boolean, whether to reuse the weights of a previous layer\nby the same name."}, "Returns": "Output tensor the same shape as inputs except the last dimension is of\nsize units.", "Raises": {"ValueError": "if eager execution is enabled."}}, "tf.compat.v1.layers.dropout": {"description": "Applies Dropout to the input.", "Args": {"inputs": "Tensor input.", "rate": "The dropout rate, between 0 and 1. E.g. \"rate=0.1\" would drop out\n10% of input units.", "noise_shape": "1D tensor of type int32 representing the shape of the\nbinary dropout mask that will be multiplied with the input.\nFor instance, if your inputs have shape\n(batch_size, timesteps, features), and you want the dropout mask\nto be the same for all timesteps, you can use\nnoise_shape=[batch_size, 1, features].", "seed": "A Python integer. Used to create random seeds. See\ntf.compat.v1.set_random_seed\nfor behavior.", "training": "Either a Python boolean, or a TensorFlow boolean scalar tensor\n(e.g. a placeholder). Whether to return the output in training mode\n(apply dropout) or in inference mode (return the input untouched).", "name": "The name of the layer (string)."}, "Returns": "Output tensor.", "Raises": {"ValueError": "if eager execution is enabled."}}, "tf.compat.v1.layers.flatten": {"description": "Flattens an input tensor while preserving the batch axis (axis 0).", "Args": {"inputs": "Tensor input.", "name": "The name of the layer (string).", "data_format": "A string, one of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs.\nchannels_last corresponds to inputs with shape\n(batch, height, width, channels) while channels_first corresponds to\ninputs with shape (batch, channels, height, width)."}, "Returns": "Reshaped tensor."}, "tf.compat.v1.layers.max_pooling1d": {"description": "Max Pooling layer for 1D inputs.", "Args": {"inputs": "The tensor over which to pool. Must have rank 3.", "pool_size": "An integer or tuple/list of a single integer,\nrepresenting the size of the pooling window.", "strides": "An integer or tuple/list of a single integer, specifying the\nstrides of the pooling operation.", "padding": "A string. The padding method, either 'valid' or 'same'.\nCase-insensitive.", "data_format": "A string, one of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs.\nchannels_last corresponds to inputs with shape\n(batch, length, channels) while channels_first corresponds to\ninputs with shape (batch, channels, length).", "name": "A string, the name of the layer."}, "Returns": "The output tensor, of rank 3.", "Raises": {"ValueError": "if eager execution is enabled."}}, "tf.compat.v1.layers.max_pooling2d": {"description": "Max pooling layer for 2D inputs (e.g. images).", "Args": {"inputs": "The tensor over which to pool. Must have rank 4.", "pool_size": "An integer or tuple/list of 2 integers: (pool_height, pool_width)\nspecifying the size of the pooling window.\nCan be a single integer to specify the same value for\nall spatial dimensions.", "strides": "An integer or tuple/list of 2 integers,\nspecifying the strides of the pooling operation.\nCan be a single integer to specify the same value for\nall spatial dimensions.", "padding": "A string. The padding method, either 'valid' or 'same'.\nCase-insensitive.", "data_format": "A string. The ordering of the dimensions in the inputs.\nchannels_last (default) and channels_first are supported.\nchannels_last corresponds to inputs with shape\n(batch, height, width, channels) while channels_first corresponds to\ninputs with shape (batch, channels, height, width).", "name": "A string, the name of the layer."}, "Returns": "Output tensor.", "Raises": {"ValueError": "if eager execution is enabled."}}, "tf.compat.v1.layers.max_pooling3d": {"description": "Max pooling layer for 3D inputs (e.g.", "Args": {"inputs": "The tensor over which to pool. Must have rank 5.", "pool_size": "An integer or tuple/list of 3 integers: (pool_depth, pool_height,\npool_width) specifying the size of the pooling window. Can be a single\ninteger to specify the same value for all spatial dimensions.", "strides": "An integer or tuple/list of 3 integers, specifying the strides of\nthe pooling operation. Can be a single integer to specify the same value\nfor all spatial dimensions.", "padding": "A string. The padding method, either 'valid' or 'same'.\nCase-insensitive.", "data_format": "A string. The ordering of the dimensions in the inputs.\nchannels_last (default) and channels_first are supported.\nchannels_last corresponds to inputs with shape (batch, depth, height,\nwidth, channels) while channels_first corresponds to inputs with shape\n(batch, channels, depth, height, width).", "name": "A string, the name of the layer."}, "Returns": "Output tensor.", "Raises": {"ValueError": "if eager execution is enabled."}}, "tf.compat.v1.layers.separable_conv1d": {"description": "Functional interface for the depthwise separable 1D convolution layer.", "Args": {"inputs": "Input tensor.", "filters": "Integer, the dimensionality of the output space (i.e. the number\nof filters in the convolution).", "kernel_size": "A single integer specifying the spatial\ndimensions of the filters.", "strides": "A single integer specifying the strides\nof the convolution.\nSpecifying any stride value != 1 is incompatible with specifying\nany dilation_rate value != 1.", "padding": "One of \"valid\" or \"same\" (case-insensitive).\n\"valid\" means no padding. \"same\" results in padding evenly to\nthe left/right or up/down of the input such that output has the same\nheight/width dimension as the input.", "data_format": "A string, one of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs.\nchannels_last corresponds to inputs with shape\n(batch, length, channels) while channels_first corresponds to\ninputs with shape (batch, channels, length).", "dilation_rate": "A single integer, specifying\nthe dilation rate to use for dilated convolution.\nCurrently, specifying any dilation_rate value != 1 is\nincompatible with specifying any stride value != 1.", "depth_multiplier": "The number of depthwise convolution output channels for\neach input channel. The total number of depthwise convolution output\nchannels will be equal to num_filters_in * depth_multiplier.", "activation": "Activation function. Set it to None to maintain a\nlinear activation.", "use_bias": "Boolean, whether the layer uses a bias.", "depthwise_initializer": "An initializer for the depthwise convolution kernel.", "pointwise_initializer": "An initializer for the pointwise convolution kernel.", "bias_initializer": "An initializer for the bias vector. If None, the default\ninitializer will be used.", "depthwise_regularizer": "Optional regularizer for the depthwise\nconvolution kernel.", "pointwise_regularizer": "Optional regularizer for the pointwise\nconvolution kernel.", "bias_regularizer": "Optional regularizer for the bias vector.", "activity_regularizer": "Optional regularizer function for the output.", "depthwise_constraint": "Optional projection function to be applied to the\ndepthwise kernel after being updated by an Optimizer (e.g. used for\nnorm constraints or value constraints for layer weights). The function\nmust take as input the unprojected variable and must return the\nprojected variable (which must have the same shape). Constraints are\nnot safe to use when doing asynchronous distributed training.", "pointwise_constraint": "Optional projection function to be applied to the\npointwise kernel after being updated by an Optimizer.", "bias_constraint": "Optional projection function to be applied to the\nbias after being updated by an Optimizer.", "trainable": "Boolean, if True also add variables to the graph collection\nGraphKeys.TRAINABLE_VARIABLES (see tf.Variable).", "name": "A string, the name of the layer.", "reuse": "Boolean, whether to reuse the weights of a previous layer\nby the same name."}, "Returns": "Output tensor.", "Raises": {"ValueError": "if eager execution is enabled."}}, "tf.compat.v1.layers.separable_conv2d": {"description": "Functional interface for the depthwise separable 2D convolution layer.", "Args": {"inputs": "Input tensor.", "filters": "Integer, the dimensionality of the output space (i.e. the number\nof filters in the convolution).", "kernel_size": "A tuple or list of 2 integers specifying the spatial\ndimensions of the filters. Can be a single integer to specify the same\nvalue for all spatial dimensions.", "strides": "A tuple or list of 2 positive integers specifying the strides\nof the convolution. Can be a single integer to specify the same value for\nall spatial dimensions.\nSpecifying any stride value != 1 is incompatible with specifying\nany dilation_rate value != 1.", "padding": "One of \"valid\" or \"same\" (case-insensitive).\n\"valid\" means no padding. \"same\" results in padding evenly to\nthe left/right or up/down of the input such that output has the same\nheight/width dimension as the input.", "data_format": "A string, one of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs.\nchannels_last corresponds to inputs with shape\n(batch, height, width, channels) while channels_first corresponds to\ninputs with shape (batch, channels, height, width).", "dilation_rate": "An integer or tuple/list of 2 integers, specifying\nthe dilation rate to use for dilated convolution.\nCan be a single integer to specify the same value for\nall spatial dimensions.\nCurrently, specifying any dilation_rate value != 1 is\nincompatible with specifying any stride value != 1.", "depth_multiplier": "The number of depthwise convolution output channels for\neach input channel. The total number of depthwise convolution output\nchannels will be equal to num_filters_in * depth_multiplier.", "activation": "Activation function. Set it to None to maintain a\nlinear activation.", "use_bias": "Boolean, whether the layer uses a bias.", "depthwise_initializer": "An initializer for the depthwise convolution kernel.", "pointwise_initializer": "An initializer for the pointwise convolution kernel.", "bias_initializer": "An initializer for the bias vector. If None, the default\ninitializer will be used.", "depthwise_regularizer": "Optional regularizer for the depthwise\nconvolution kernel.", "pointwise_regularizer": "Optional regularizer for the pointwise\nconvolution kernel.", "bias_regularizer": "Optional regularizer for the bias vector.", "activity_regularizer": "Optional regularizer function for the output.", "depthwise_constraint": "Optional projection function to be applied to the\ndepthwise kernel after being updated by an Optimizer (e.g. used for\nnorm constraints or value constraints for layer weights). The function\nmust take as input the unprojected variable and must return the\nprojected variable (which must have the same shape). Constraints are\nnot safe to use when doing asynchronous distributed training.", "pointwise_constraint": "Optional projection function to be applied to the\npointwise kernel after being updated by an Optimizer.", "bias_constraint": "Optional projection function to be applied to the\nbias after being updated by an Optimizer.", "trainable": "Boolean, if True also add variables to the graph collection\nGraphKeys.TRAINABLE_VARIABLES (see tf.Variable).", "name": "A string, the name of the layer.", "reuse": "Boolean, whether to reuse the weights of a previous layer\nby the same name."}, "Returns": "Output tensor.", "Raises": {"ValueError": "if eager execution is enabled."}}}, "tf.compat.v1.linalg": {}, "tf.compat.v1.lite": {"tf.compat.v1.lite.OpHint": {"description": "A class that helps build tflite function invocations. (deprecated)", "Args": {"function_name": "Name of the function (the custom op name in tflite)", "level": "OpHint level.", "children_inputs_mappings": "Children OpHint inputs/outputs mapping.\nchildren_inputs_mappings should like below:\n\"parent_first_child_input\":\n    [{\"parent_input_index\": num, \"child_input_index\": num}, ...]\n\"parent_last_child_output\":\n    [{\"parent_output_index\": num, \"child_output_index\": num}, ...]\n\"internal_children_input_output\":\n    [{\"child_input_index\": num, \"child_output_index\": num}, ...]", "**kwargs": "Keyword arguments of any constant attributes for the function."}, "Class Variables": {"AGGREGATE_FIRST": "'first'", "AGGREGATE_LAST": "'last'", "AGGREGATE_STACK": "'stack'", "CHILDREN_INPUTS_MAPPINGS": "'_tflite_children_ophint_inputs_mapping'", "FUNCTION_AGGREGATE_ATTR": "'_tflite_function_aggregate'", "FUNCTION_INPUT_INDEX_ATTR": "'_tflite_function_input_index'", "FUNCTION_LEVEL_ATTR": "'_tflite_ophint_level'", "FUNCTION_NAME_ATTR": "'_tflite_function_name'", "FUNCTION_OUTPUT_INDEX_ATTR": "'_tflite_function_output_index'", "FUNCTION_SORT_INDEX_ATTR": "'_tflite_function_sort_index'", "FUNCTION_UUID_ATTR": "'_tflite_function_uuid'", "TFLITE_INPUT_INDICES": "'_tflite_input_indices'"}}, "tf.compat.v1.lite.OpHint.OpHintArgumentTracker": {"description": "Conceptually tracks indices of arguments of &#34;OpHint functions&#34;.", "Args": {"function_name": "Name of the function that this tracks arguments for.", "unique_function_id": "UUID of function that this tracks arguments for.", "node_name_prefix": "How identities that are created are named.", "attr_name": "Name of attribute to use to store the index for this hint.\ni.e. FUNCTION_INPUT_INDEX or FUNCTION_OUTPUT_INDEX", "level": "Hierarchical level of the Ophint node, a number.", "children_inputs_mappings": "Inputs/Outputs mapping for children hints."}}, "tf.compat.v1.lite.TFLiteConverter": {"description": "Convert a TensorFlow model into output_format.", "Args": {"graph_def": "Frozen TensorFlow GraphDef.", "input_tensors": "List of input tensors. Type and shape are computed using\nfoo.shape and foo.dtype.", "output_tensors": "List of output tensors (only .name is used from this).", "input_arrays_with_shape": "Tuple of strings representing input tensor names\nand list of integers representing input shapes\n(e.g., [(\"foo\" : [1, 16, 16, 3])]). Use only when graph cannot be loaded\n  into TensorFlow and when input_tensors and output_tensors are\n  None. (default None)", "output_arrays": "List of output tensors to freeze graph with. Use only when\ngraph cannot be loaded into TensorFlow and when input_tensors and\noutput_tensors are None. (default None)", "experimental_debug_info_func": "An experimental function to retrieve the\ngraph debug info for a set of nodes from the graph_def."}, "Raises": {"ValueError": "Invalid arguments."}, "Attributes": {"optimizations": "Experimental flag, subject to change. Set of optimizations to\napply. e.g {tf.lite.Optimize.DEFAULT}. (default None, must be None or a\nset of values of type tf.lite.Optimize)", "representative_dataset": "A generator function used for integer quantization\nwhere each generated sample has the same order, type and shape as the\ninputs to the model. Usually, this is a small subset of a few hundred\nsamples randomly chosen, in no particular order, from the training or\nevaluation dataset. This is an optional attribute, but required for full\ninteger quantization, i.e, if tf.int8 is the only supported type in\ntarget_spec.supported_types. Refer to tf.lite.RepresentativeDataset.\n(default None)", "target_spec": "Experimental flag, subject to change. Specifications of target\ndevice, including supported ops set, supported types and a set of user's\ndefined TensorFlow operators required in the TensorFlow Lite runtime.\nRefer to tf.lite.TargetSpec.", "inference_type": "Data type of numeric arrays, excluding the input layer.\n(default tf.float32, must be in {tf.float32, tf.int8, tf.uint8})", "inference_input_type": "Data type of the numeric arrays in the input layer. If\ninference_input_type is in {tf.int8, tf.uint8}, then\nquantized_input_stats must be provided. (default is the value assigned\nto inference_type, must be in {tf.float32, tf.int8, tf.uint8})", "inference_output_type": "Data type of the numeric arrays in the output layer.\n(default is the value assigned to inference_type, must be in\n{tf.float32, tf.int8, tf.uint8})", "quantized_input_stats": "Map of input tensor names to a tuple of floats\nrepresenting the mean and standard deviation of the training data.\n(e.g., {\"foo\" : (0., 1.)}). Required if inference_input_type is tf.int8\n  or tf.uint8. (default None)", "default_ranges_stats": "Tuple of integers (min, max) representing range values\nfor all numeric arrays without a specified range. Intended for\nexperimenting with quantization via \"dummy quantization\". (default None)", "allow_custom_ops": "Boolean indicating whether to allow custom operations.\nWhen False any unknown operation is an error. When True, custom ops are\ncreated for any op that is unknown. The developer will need to provide\nthese to the TensorFlow Lite runtime with a custom resolver. (default\nFalse)", "drop_control_dependency": "Boolean indicating whether to drop control\ndependencies silently. This is due to TFLite not supporting control\ndependencies. (default True)", "reorder_across_fake_quant": "Boolean indicating whether to reorder FakeQuant\nnodes in unexpected locations. Used when the location of the FakeQuant\nnodes is preventing graph transformations necessary to convert the graph.\nResults in a graph that differs from the quantized training graph,\npotentially causing differing arithmetic behavior. (default False)", "change_concat_input_ranges": "Boolean to change behavior of min/max ranges for\ninputs and outputs of the concat operator for quantized models. Changes\nthe ranges of concat operator overlap when true. (default False)", "output_format": "Output file format. (default\ntf.compat.v1.lite.constants.TFLITE, must be in\n{tf.compat.v1.lite.constants.TFLITE,\ntf.compat.v1.lite.constants.GRAPHVIZ_DOT})", "dump_graphviz_dir": "Full filepath of folder to dump the graphs at various\nstages of processing GraphViz .dot files. Preferred over\noutput_format=tf.compat.v1.lite.constants.GRAPHVIZ_DOT in order to keep\nthe requirements of the output file. (default None)", "dump_graphviz_video": "Boolean indicating whether to dump the GraphViz .dot\nfiles after every graph transformation. Requires the dump_graphviz_dir\nflag to be specified. (default False)", "conversion_summary_dir": "Full path of the directory to store conversion logs.\n(default None)", "exclude_conversion_metadata": "Whether not to embed the conversion metadata\ninto the converted model. (default False)", "target_ops": "Deprecated. Please use target_spec.supported_ops instead.", "post_training_quantize": "Deprecated. Please use optimizations instead and\nset it to {tf.lite.Optimize.DEFAULT}. (default False)", "experimental_new_converter": "Experimental flag, subject to change. Enables\nMLIR-based conversion. (default True)", "experimental_new_quantizer": "Experimental flag, subject to change. Enables\nMLIR-based quantization conversion instead of Flatbuffer-based conversion.\n(default True)"}}, "tf.compat.v1.lite.TocoConverter": {"description": "Convert a TensorFlow model into output_format."}, "tf.compat.v1.lite.toco_convert": {"description": "Convert a TensorFlow GraphDef to TFLite. (deprecated)", "Returns": "The converted TensorFlow Lite model in a bytes array.", "Raises": {}}}, "tf.compat.v1.lite.constants": {}, "tf.compat.v1.lite.experimental.authoring": {}, "tf.compat.v1.logging": {"tf.compat.v1.logging.TaskLevelStatusMessage": {}, "tf.compat.v1.logging.debug": {}, "tf.compat.v1.logging.error": {}, "tf.compat.v1.logging.fatal": {}, "tf.compat.v1.logging.flush": {}, "tf.compat.v1.logging.get_verbosity": {"description": "Return how much logging output will be produced."}, "tf.compat.v1.logging.info": {}, "tf.compat.v1.logging.log": {}, "tf.compat.v1.logging.log_every_n": {"description": "Log &#39;msg % args&#39; at level &#39;level&#39; once per &#39;n&#39; times.", "Args": {"level": "The level at which to log.", "msg": "The message to be logged.", "n": "The number of times this should be called before it is logged.", "*args": "The args to be substituted into the msg."}}, "tf.compat.v1.logging.log_first_n": {"description": "Log &#39;msg % args&#39; at level &#39;level&#39; only first &#39;n&#39; times.", "Args": {"level": "The level at which to log.", "msg": "The message to be logged.", "n": "The number of times this should be called before it is logged.", "*args": "The args to be substituted into the msg."}}, "tf.compat.v1.logging.log_if": {"description": "Log &#39;msg % args&#39; at level &#39;level&#39; only if condition is fulfilled."}, "tf.compat.v1.logging.set_verbosity": {"description": "Sets the threshold for what messages will be logged."}, "tf.compat.v1.logging.vlog": {}, "tf.compat.v1.logging.warn": {}, "tf.compat.v1.logging.warning": {}}, "tf.compat.v1.lookup": {"tf.compat.v1.lookup.StaticHashTable": {"description": "A generic hash table that is immutable once initialized.", "Args": {"initializer": "The table initializer to use. See HashTable kernel for\nsupported key and value types.", "default_value": "The value to use if a key is missing in the table.", "name": "A name for the operation (optional).", "experimental_is_anonymous": "Whether to use anonymous mode for the\ntable (default is False). In anonymous mode, the table\nresource can only be accessed via a resource handle. It can't\nbe looked up by a name. When all resource handles pointing to\nthat resource are gone, the resource will be deleted\nautomatically."}, "Attributes": {"default_value": "The default value of the table.", "initializer": "", "key_dtype": "The table key dtype.", "name": "The name of the table.", "resource_handle": "Returns the resource handle associated with this Resource.", "value_dtype": "The table value dtype."}}, "tf.compat.v1.lookup.StaticVocabularyTable": {"description": "String to Id table that assigns out-of-vocabulary keys to hash buckets.", "Args": {"initializer": "A TableInitializerBase object that contains the data used\nto initialize the table. If None, then we only use out-of-vocab buckets.", "num_oov_buckets": "Number of buckets to use for out-of-vocabulary keys. Must\nbe greater than zero. If out-of-vocab buckets are not required, use\nStaticHashTable instead.", "lookup_key_dtype": "Data type of keys passed to lookup. Defaults to\ninitializer.key_dtype if initializer is specified, otherwise\ntf.string. Must be string or integer, and must be castable to\ninitializer.key_dtype.", "name": "A name for the operation (optional).", "experimental_is_anonymous": "Whether to use anonymous mode for the\ntable (default is False). In anonymous mode, the table\nresource can only be accessed via a resource handle. It can't\nbe looked up by a name. When all resource handles pointing to\nthat resource are gone, the resource will be deleted\nautomatically."}, "Raises": {"ValueError": "when num_oov_buckets is not positive.", "TypeError": "when lookup_key_dtype or initializer.key_dtype are not\ninteger or string. Also when initializer.value_dtype != int64."}, "Attributes": {"initializer": "", "key_dtype": "The table key dtype.", "name": "The name of the table.", "resource_handle": "Returns the resource handle associated with this Resource.", "value_dtype": "The table value dtype."}}}, "tf.compat.v1.losses": {"tf.compat.v1.losses.Reduction": {"description": "Types of loss reduction.", "Class Variables": {"MEAN": "'weighted_mean'", "NONE": "'none'", "SUM": "'weighted_sum'", "SUM_BY_NONZERO_WEIGHTS": "'weighted_sum_by_nonzero_weights'", "SUM_OVER_BATCH_SIZE": "'weighted_sum_over_batch_size'", "SUM_OVER_NONZERO_WEIGHTS": "'weighted_sum_by_nonzero_weights'"}}, "tf.compat.v1.losses.absolute_difference": {"description": "Adds an Absolute Difference loss to the training procedure.", "Args": {"labels": "The ground truth output tensor, same dimensions as 'predictions'.", "predictions": "The predicted outputs.", "weights": "Optional Tensor whose rank is either 0, or the same rank as\nlabels, and must be broadcastable to labels (i.e., all dimensions must\nbe either 1, or the same as the corresponding losses dimension).", "scope": "The scope for the operations performed in computing the loss.", "loss_collection": "collection to which this loss will be added.", "reduction": "Type of reduction to apply to loss."}, "Returns": "Weighted loss float Tensor. If reduction is NONE, this has the same\nshape as labels; otherwise, it is scalar.", "Raises": {"ValueError": "If the shape of predictions doesn't match that of\nlabels or if the shape of weights is invalid or if labels\nor predictions is None."}}, "tf.compat.v1.losses.add_loss": {"description": "Adds a externally defined loss to the collection of losses.", "Args": {"loss": "A loss Tensor.", "loss_collection": "Optional collection to add the loss to."}}, "tf.compat.v1.losses.compute_weighted_loss": {"description": "Computes the weighted loss.", "Args": {"losses": "Tensor of shape [batch_size, d1, ... dN].", "weights": "Optional Tensor whose rank is either 0, or the same rank as\nlosses, and must be broadcastable to losses (i.e., all dimensions must\nbe either 1, or the same as the corresponding losses dimension).", "scope": "the scope for the operations performed in computing the loss.", "loss_collection": "the loss will be added to these collections.", "reduction": "Type of reduction to apply to loss."}, "Returns": "Weighted loss Tensor of the same type as losses. If reduction is\nNONE, this has the same shape as losses; otherwise, it is scalar.", "Raises": {"ValueError": "If weights is None or the shape is not compatible with\nlosses, or if the number of dimensions (rank) of either losses or\nweights is missing."}}, "tf.compat.v1.losses.cosine_distance": {"description": "Adds a cosine-distance loss to the training procedure. (deprecated arguments)", "Args": {"labels": "Tensor whose shape matches 'predictions'", "predictions": "An arbitrary matrix.", "axis": "The dimension along which the cosine distance is computed.", "weights": "Optional Tensor whose rank is either 0, or the same rank as\nlabels, and must be broadcastable to labels (i.e., all dimensions must\nbe either 1, or the same as the corresponding losses dimension).", "scope": "The scope for the operations performed in computing the loss.", "loss_collection": "collection to which this loss will be added.", "reduction": "Type of reduction to apply to loss.", "dim": "The old (deprecated) name for axis."}, "Returns": "Weighted loss float Tensor. If reduction is NONE, this has the same\nshape as labels; otherwise, it is scalar.", "Raises": {"ValueError": "If predictions shape doesn't match labels shape, or\naxis, labels, predictions or weights is None."}}, "tf.compat.v1.losses.get_losses": {"description": "Gets the list of losses from the loss_collection.", "Args": {"scope": "An optional scope name for filtering the losses to return.", "loss_collection": "Optional losses collection."}, "Returns": "a list of loss tensors."}, "tf.compat.v1.losses.get_regularization_loss": {"description": "Gets the total regularization loss.", "Args": {"scope": "An optional scope name for filtering the losses to return.", "name": "The name of the returned tensor."}, "Returns": "A scalar regularization loss."}, "tf.compat.v1.losses.get_regularization_losses": {"description": "Gets the list of regularization losses.", "Args": {"scope": "An optional scope name for filtering the losses to return."}, "Returns": "A list of regularization losses as Tensors."}, "tf.compat.v1.losses.get_total_loss": {"description": "Returns a tensor whose value represents the total loss.", "Args": {"add_regularization_losses": "A boolean indicating whether or not to use the\nregularization losses in the sum.", "name": "The name of the returned tensor.", "scope": "An optional scope name for filtering the losses to return. Note that\nthis filters the losses added with tf.add_loss() as well as the\nregularization losses to that scope."}, "Returns": "A Tensor whose value represents the total loss.", "Raises": {"ValueError": "if losses is not iterable."}}, "tf.compat.v1.losses.hinge_loss": {"description": "Adds a hinge loss to the training procedure.", "Args": {"labels": "The ground truth output tensor. Its shape should match the shape of\nlogits. The values of the tensor are expected to be 0.0 or 1.0. Internally\nthe {0,1} labels are converted to {-1,1} when calculating the hinge loss.", "logits": "The logits, a float tensor. Note that logits are assumed to be\nunbounded and 0-centered. A value > 0 (resp. < 0) is considered a positive\n(resp. negative) binary prediction.", "weights": "Optional Tensor whose rank is either 0, or the same rank as\nlabels, and must be broadcastable to labels (i.e., all dimensions must\nbe either 1, or the same as the corresponding losses dimension).", "scope": "The scope for the operations performed in computing the loss.", "loss_collection": "collection to which the loss will be added.", "reduction": "Type of reduction to apply to loss."}, "Returns": "Weighted loss float Tensor. If reduction is NONE, this has the same\nshape as labels; otherwise, it is scalar.", "Raises": {"ValueError": "If the shapes of logits and labels don't match or\nif labels or logits is None."}}, "tf.compat.v1.losses.huber_loss": {"description": "Adds a [Huber Loss](https://en.wikipedia.org/wiki/Huber_loss) term to the training procedure.", "Args": {"labels": "The ground truth output tensor, same dimensions as 'predictions'.", "predictions": "The predicted outputs.", "weights": "Optional Tensor whose rank is either 0, or the same rank as\nlabels, and must be broadcastable to labels (i.e., all dimensions must\nbe either 1, or the same as the corresponding losses dimension).", "delta": "float, the point where the huber loss function changes from a\nquadratic to linear.", "scope": "The scope for the operations performed in computing the loss.", "loss_collection": "collection to which the loss will be added.", "reduction": "Type of reduction to apply to loss."}, "Returns": "Weighted loss float Tensor. If reduction is NONE, this has the same\nshape as labels; otherwise, it is scalar.", "Raises": {"ValueError": "If the shape of predictions doesn't match that of labels or\n if the shape of weights is invalid.  Also if labels or\npredictions is None."}}, "tf.compat.v1.losses.log_loss": {"description": "Adds a Log Loss term to the training procedure.", "Args": {"labels": "The ground truth output tensor, same dimensions as 'predictions'.", "predictions": "The predicted outputs.", "weights": "Optional Tensor whose rank is either 0, or the same rank as\nlabels, and must be broadcastable to labels (i.e., all dimensions must\nbe either 1, or the same as the corresponding losses dimension).", "epsilon": "A small increment to add to avoid taking a log of zero.", "scope": "The scope for the operations performed in computing the loss.", "loss_collection": "collection to which the loss will be added.", "reduction": "Type of reduction to apply to loss."}, "Returns": "Weighted loss float Tensor. If reduction is NONE, this has the same\nshape as labels; otherwise, it is scalar.", "Raises": {"ValueError": "If the shape of predictions doesn't match that of labels or\nif the shape of weights is invalid.  Also if labels or predictions\nis None."}}, "tf.compat.v1.losses.mean_pairwise_squared_error": {"description": "Adds a pairwise-errors-squared loss to the training procedure.", "Args": {"labels": "The ground truth output tensor, whose shape must match the shape of\npredictions.", "predictions": "The predicted outputs, a tensor of size\n[batch_size, d0, .. dN] where N+1 is the total number of dimensions in\npredictions.", "weights": "Coefficients for the loss a scalar, a tensor of shape\n[batch_size] or a tensor whose shape matches predictions.", "scope": "The scope for the operations performed in computing the loss.", "loss_collection": "collection to which the loss will be added."}, "Returns": "A scalar Tensor that returns the weighted loss.", "Raises": {"ValueError": "If the shape of predictions doesn't match that of labels or\nif the shape of weights is invalid.  Also if labels or predictions\nis None."}}, "tf.compat.v1.losses.mean_squared_error": {"description": "Adds a Sum-of-Squares loss to the training procedure.", "Args": {"labels": "The ground truth output tensor, same dimensions as 'predictions'.", "predictions": "The predicted outputs.", "weights": "Optional Tensor whose rank is either 0, or the same rank as\nlabels, and must be broadcastable to labels (i.e., all dimensions must\nbe either 1, or the same as the corresponding losses dimension).", "scope": "The scope for the operations performed in computing the loss.", "loss_collection": "collection to which the loss will be added.", "reduction": "Type of reduction to apply to loss."}, "Returns": "Weighted loss float Tensor. If reduction is NONE, this has the same\nshape as labels; otherwise, it is scalar.", "Raises": {"ValueError": "If the shape of predictions doesn't match that of labels or\nif the shape of weights is invalid.  Also if labels or predictions\nis None."}}, "tf.compat.v1.losses.sigmoid_cross_entropy": {"description": "Creates a cross-entropy loss using tf.nn.sigmoid_cross_entropy_with_logits.", "Args": {"multi_class_labels": "[batch_size, num_classes] target integer labels in\n{0, 1}.", "logits": "Float [batch_size, num_classes] logits outputs of the network.", "weights": "Optional Tensor whose rank is either 0, or the same rank as\nmulti_class_labels, and must be broadcastable to multi_class_labels\n(i.e., all dimensions must be either 1, or the same as the\ncorresponding losses dimension).", "label_smoothing": "If greater than 0 then smooth the labels.", "scope": "The scope for the operations performed in computing the loss.", "loss_collection": "collection to which the loss will be added.", "reduction": "Type of reduction to apply to loss."}, "Returns": "Weighted loss Tensor of the same type as logits. If reduction is\nNONE, this has the same shape as logits; otherwise, it is scalar.", "Raises": {"ValueError": "If the shape of logits doesn't match that of\nmulti_class_labels or if the shape of weights is invalid, or if\nweights is None.  Also if multi_class_labels or logits is None."}}, "tf.compat.v1.losses.softmax_cross_entropy": {"description": "Creates a cross-entropy loss using tf.nn.softmax_cross_entropy_with_logits_v2.", "Args": {"onehot_labels": "One-hot-encoded labels.", "logits": "Logits outputs of the network.", "weights": "Optional Tensor that is broadcastable to loss.", "label_smoothing": "If greater than 0 then smooth the labels.", "scope": "the scope for the operations performed in computing the loss.", "loss_collection": "collection to which the loss will be added.", "reduction": "Type of reduction to apply to loss."}, "Returns": "Weighted loss Tensor of the same type as logits. If reduction is\nNONE, this has shape [batch_size]; otherwise, it is scalar.", "Raises": {"ValueError": "If the shape of logits doesn't match that of onehot_labels\nor if the shape of weights is invalid or if weights is None.  Also if\nonehot_labels or logits is None."}}, "tf.compat.v1.losses.sparse_softmax_cross_entropy": {"description": "Cross-entropy loss using tf.nn.sparse_softmax_cross_entropy_with_logits.", "Args": {"labels": "Tensor of shape [d_0, d_1, ..., d_{r-1}] (where r is rank of\nlabels and result) and dtype int32 or int64. Each entry in labels\nmust be an index in [0, num_classes). Other values will raise an\nexception when this op is run on CPU, and return NaN for corresponding\nloss and gradient rows on GPU.", "logits": "Unscaled log probabilities of shape\n[d_0, d_1, ..., d_{r-1}, num_classes] and dtype float16, float32 or\nfloat64.", "weights": "Coefficients for the loss. This must be scalar or broadcastable to\nlabels (i.e. same rank and each dimension is either 1 or the same).", "scope": "the scope for the operations performed in computing the loss.", "loss_collection": "collection to which the loss will be added.", "reduction": "Type of reduction to apply to loss."}, "Returns": "Weighted loss Tensor of the same type as logits. If reduction is\nNONE, this has the same shape as labels; otherwise, it is scalar.", "Raises": {"ValueError": "If the shapes of logits, labels, and weights are\nincompatible, or if any of them are None."}}}, "tf.compat.v1.manip": {}, "tf.compat.v1.math": {"tf.compat.v1.math.in_top_k": {"description": "Says whether the targets are in the top K predictions.", "Args": {"predictions": "A Tensor of type float32.\nA batch_size x classes tensor.", "targets": "A Tensor. Must be one of the following types: int32, int64.\nA batch_size vector of class ids.", "k": "An int. Number of top elements to look at for computing precision.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type bool. Computed Precision at k as a bool Tensor."}, "tf.compat.v1.math.log_softmax": {"description": "Computes log softmax activations. (deprecated arguments)", "Args": {"logits": "A non-empty Tensor. Must be one of the following types: half,\nfloat32, float64.", "axis": "The dimension softmax would be performed on. The default is -1 which\nindicates the last dimension.", "name": "A name for the operation (optional).", "dim": "Deprecated alias for axis."}, "Returns": "A Tensor. Has the same type as logits. Same shape as logits.", "Raises": {"InvalidArgumentError": "if logits is empty or axis is beyond the last\ndimension of logits."}}, "tf.compat.v1.math.softmax": {"description": "Computes softmax activations.", "Args": {"logits": "A non-empty Tensor. Must be one of the following types: half,\nfloat32, float64.", "axis": "The dimension softmax would be performed on. The default is -1 which\nindicates the last dimension.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type and shape as logits.", "Raises": {"InvalidArgumentError": "if logits is empty or axis is beyond the last\ndimension of logits."}}}, "tf.compat.v1.math.special": {}, "tf.compat.v1.metrics": {"tf.compat.v1.metrics.accuracy": {"description": "Calculates how often predictions matches labels.", "Args": {"labels": "The ground truth values, a Tensor whose shape matches\npredictions.", "predictions": "The predicted values, a Tensor of any shape.", "weights": "Optional Tensor whose rank is either 0, or the same rank as\nlabels, and must be broadcastable to labels (i.e., all dimensions must\nbe either 1, or the same as the corresponding labels dimension).", "metrics_collections": "An optional list of collections that accuracy should\nbe added to.", "updates_collections": "An optional list of collections that update_op should\nbe added to.", "name": "An optional variable_scope name."}, "Returns": "accuracy\n\n\nA Tensor representing the accuracy, the value of total divided\nby count.", "Raises": {"ValueError": "If predictions and labels have mismatched shapes, or if\nweights is not None and its shape doesn't match predictions, or if\neither metrics_collections or updates_collections are not a list or\ntuple.", "RuntimeError": "If eager execution is enabled."}}, "tf.compat.v1.metrics.auc": {"description": "Computes the approximate AUC via a Riemann sum. (deprecated)", "Args": {"labels": "A Tensor whose shape matches predictions. Will be cast to\nbool.", "predictions": "A floating point Tensor of arbitrary shape and whose values\nare in the range [0, 1].", "weights": "Optional Tensor whose rank is either 0, or the same rank as\nlabels, and must be broadcastable to labels (i.e., all dimensions must\nbe either 1, or the same as the corresponding labels dimension).", "num_thresholds": "The number of thresholds to use when discretizing the roc\ncurve.", "metrics_collections": "An optional list of collections that auc should be\nadded to.", "updates_collections": "An optional list of collections that update_op should\nbe added to.", "curve": "Specifies the name of the curve to be computed, 'ROC' [default] or\n'PR' for the Precision-Recall-curve.", "name": "An optional variable_scope name.", "summation_method": "Specifies the Riemann summation method used\n(https://en.wikipedia.org/wiki/Riemann_sum): 'trapezoidal' [default] that\napplies the trapezoidal rule; 'careful_interpolation', a variant of it\ndiffering only by a more correct interpolation scheme for PR-AUC -\ninterpolating (true/false) positives but not the ratio that is precision;\n'minoring' that applies left summation for increasing intervals and right\nsummation for decreasing intervals; 'majoring' that does the opposite.\nNote that 'careful_interpolation' is strictly preferred to 'trapezoidal'\n(to be deprecated soon) as it applies the same method for ROC, and a\nbetter one (see Davis & Goadrich 2006 for details) for the PR curve.", "thresholds": "An optional list of floating point values to use as the\nthresholds for discretizing the curve. If set, the num_thresholds\nparameter is ignored. Values should be in [0, 1]. Endpoint thresholds\nequal to {-epsilon, 1+epsilon} for a small positive epsilon value will be\nautomatically included with these to correctly handle predictions equal to\n exactly 0 or 1."}, "Returns": "auc\n\n\nA scalar Tensor representing the current area-under-curve.", "Raises": {"ValueError": "If predictions and labels have mismatched shapes, or if\nweights is not None and its shape doesn't match predictions, or if\neither metrics_collections or updates_collections are not a list or\ntuple.", "RuntimeError": "If eager execution is enabled."}}, "tf.compat.v1.metrics.average_precision_at_k": {"description": "Computes average precision@k of predictions with respect to sparse labels.", "Args": {"labels": "int64 Tensor or SparseTensor with shape\n[D1, ... DN, num_labels] or [D1, ... DN], where the latter implies\nnum_labels=1. N >= 1 and num_labels is the number of target classes for\nthe associated prediction. Commonly, N=1 and labels has shape\n[batch_size, num_labels]. [D1, ... DN] must match predictions. Values\nshould be in range [0, num_classes), where num_classes is the last\ndimension of predictions. Values outside this range are ignored.", "predictions": "Float Tensor with shape [D1, ... DN, num_classes] where\nN >= 1. Commonly, N=1 and predictions has shape\n[batch size, num_classes]. The final dimension contains the logit values\nfor each class. [D1, ... DN] must match labels.", "k": "Integer, k for @k metric. This will calculate an average precision for\nrange [1,k], as documented above.", "weights": "Tensor whose rank is either 0, or n-1, where n is the rank of\nlabels. If the latter, it must be broadcastable to labels (i.e., all\ndimensions must be either 1, or the same as the corresponding labels\ndimension).", "metrics_collections": "An optional list of collections that values should\nbe added to.", "updates_collections": "An optional list of collections that updates should\nbe added to.", "name": "Name of new update operation, and namespace for other dependent ops."}, "Returns": "mean_average_precision\n\n\nScalar float64 Tensor with the mean average\nprecision values.", "Raises": {"ValueError": "if k is invalid.", "RuntimeError": "If eager execution is enabled."}}, "tf.compat.v1.metrics.false_negatives": {"description": "Computes the total number of false negatives.", "Args": {"labels": "The ground truth values, a Tensor whose dimensions must match\npredictions. Will be cast to bool.", "predictions": "The predicted values, a Tensor of arbitrary dimensions. Will\nbe cast to bool.", "weights": "Optional Tensor whose rank is either 0, or the same rank as\nlabels, and must be broadcastable to labels (i.e., all dimensions must\nbe either 1, or the same as the corresponding labels dimension).", "metrics_collections": "An optional list of collections that the metric\nvalue variable should be added to.", "updates_collections": "An optional list of collections that the metric update\nops should be added to.", "name": "An optional variable_scope name."}, "Returns": "value_tensor\n\n\nA Tensor representing the current value of the metric.", "Raises": {"ValueError": "If weights is not None and its shape doesn't match values,\nor if either metrics_collections or updates_collections are not a list\nor tuple.", "RuntimeError": "If eager execution is enabled."}}, "tf.compat.v1.metrics.false_negatives_at_thresholds": {"description": "Computes false negatives at provided threshold values.", "Args": {"labels": "A Tensor whose shape matches predictions. Will be cast to\nbool.", "predictions": "A floating point Tensor of arbitrary shape and whose values\nare in the range [0, 1].", "thresholds": "A python list or tuple of float thresholds in [0, 1].", "weights": "Optional Tensor whose rank is either 0, or the same rank as\nlabels, and must be broadcastable to labels (i.e., all dimensions must\nbe either 1, or the same as the corresponding labels dimension).", "metrics_collections": "An optional list of collections that false_negatives\nshould be added to.", "updates_collections": "An optional list of collections that update_op should\nbe added to.", "name": "An optional variable_scope name."}, "Returns": "false_negatives\n\n\n A float Tensor of shape [len(thresholds)].", "Raises": {"ValueError": "If predictions and labels have mismatched shapes, or if\nweights is not None and its shape doesn't match predictions, or if\neither metrics_collections or updates_collections are not a list or\ntuple.", "RuntimeError": "If eager execution is enabled."}}, "tf.compat.v1.metrics.false_positives": {"description": "Sum the weights of false positives.", "Args": {"labels": "The ground truth values, a Tensor whose dimensions must match\npredictions. Will be cast to bool.", "predictions": "The predicted values, a Tensor of arbitrary dimensions. Will\nbe cast to bool.", "weights": "Optional Tensor whose rank is either 0, or the same rank as\nlabels, and must be broadcastable to labels (i.e., all dimensions must\nbe either 1, or the same as the corresponding labels dimension).", "metrics_collections": "An optional list of collections that the metric\nvalue variable should be added to.", "updates_collections": "An optional list of collections that the metric update\nops should be added to.", "name": "An optional variable_scope name."}, "Returns": "value_tensor\n\n\nA Tensor representing the current value of the metric.", "Raises": {"ValueError": "If predictions and labels have mismatched shapes, or if\nweights is not None and its shape doesn't match predictions, or if\neither metrics_collections or updates_collections are not a list or\ntuple.", "RuntimeError": "If eager execution is enabled."}}, "tf.compat.v1.metrics.false_positives_at_thresholds": {"description": "Computes false positives at provided threshold values.", "Args": {"labels": "A Tensor whose shape matches predictions. Will be cast to\nbool.", "predictions": "A floating point Tensor of arbitrary shape and whose values\nare in the range [0, 1].", "thresholds": "A python list or tuple of float thresholds in [0, 1].", "weights": "Optional Tensor whose rank is either 0, or the same rank as\nlabels, and must be broadcastable to labels (i.e., all dimensions must\nbe either 1, or the same as the corresponding labels dimension).", "metrics_collections": "An optional list of collections that false_positives\nshould be added to.", "updates_collections": "An optional list of collections that update_op should\nbe added to.", "name": "An optional variable_scope name."}, "Returns": "false_positives\n\n\n A float Tensor of shape [len(thresholds)].", "Raises": {"ValueError": "If predictions and labels have mismatched shapes, or if\nweights is not None and its shape doesn't match predictions, or if\neither metrics_collections or updates_collections are not a list or\ntuple.", "RuntimeError": "If eager execution is enabled."}}, "tf.compat.v1.metrics.mean": {"description": "Computes the (weighted) mean of the given values.", "Args": {"values": "A Tensor of arbitrary dimensions.", "weights": "Optional Tensor whose rank is either 0, or the same rank as\nvalues, and must be broadcastable to values (i.e., all dimensions must\nbe either 1, or the same as the corresponding values dimension).", "metrics_collections": "An optional list of collections that mean\nshould be added to.", "updates_collections": "An optional list of collections that update_op\nshould be added to.", "name": "An optional variable_scope name."}, "Returns": "mean\n\n\nA Tensor representing the current mean, the value of total divided\nby count.", "Raises": {"ValueError": "If weights is not None and its shape doesn't match values,\nor if either metrics_collections or updates_collections are not a list\nor tuple.", "RuntimeError": "If eager execution is enabled."}}, "tf.compat.v1.metrics.mean_absolute_error": {"description": "Computes the mean absolute error between the labels and predictions.", "Args": {"labels": "A Tensor of the same shape as predictions.", "predictions": "A Tensor of arbitrary shape.", "weights": "Optional Tensor whose rank is either 0, or the same rank as\nlabels, and must be broadcastable to labels (i.e., all dimensions must\nbe either 1, or the same as the corresponding labels dimension).", "metrics_collections": "An optional list of collections that\nmean_absolute_error should be added to.", "updates_collections": "An optional list of collections that update_op should\nbe added to.", "name": "An optional variable_scope name."}, "Returns": "mean_absolute_error\n\n\nA Tensor representing the current mean, the value of\ntotal divided by count.", "Raises": {"ValueError": "If predictions and labels have mismatched shapes, or if\nweights is not None and its shape doesn't match predictions, or if\neither metrics_collections or updates_collections are not a list or\ntuple.", "RuntimeError": "If eager execution is enabled."}}, "tf.compat.v1.metrics.mean_cosine_distance": {"description": "Computes the cosine distance between the labels and predictions.", "Args": {"labels": "A Tensor of arbitrary shape.", "predictions": "A Tensor of the same shape as labels.", "dim": "The dimension along which the cosine distance is computed.", "weights": "Optional Tensor whose rank is either 0, or the same rank as\nlabels, and must be broadcastable to labels (i.e., all dimensions must\nbe either 1, or the same as the corresponding labels dimension). Also,\ndimension dim must be 1.", "metrics_collections": "An optional list of collections that the metric\nvalue variable should be added to.", "updates_collections": "An optional list of collections that the metric update\nops should be added to.", "name": "An optional variable_scope name."}, "Returns": "mean_distance\n\n\nA Tensor representing the current mean, the value of\ntotal divided by count.", "Raises": {"ValueError": "If predictions and labels have mismatched shapes, or if\nweights is not None and its shape doesn't match predictions, or if\neither metrics_collections or updates_collections are not a list or\ntuple.", "RuntimeError": "If eager execution is enabled."}}, "tf.compat.v1.metrics.mean_iou": {"description": "Calculate per-step mean Intersection-Over-Union (mIOU).", "Args": {"labels": "A Tensor of ground truth labels with shape [batch size] and of\ntype int32 or int64. The tensor will be flattened if its rank > 1.", "predictions": "A Tensor of prediction results for semantic labels, whose\nshape is [batch size] and type int32 or int64. The tensor will be\nflattened if its rank > 1.", "num_classes": "The possible number of labels the prediction task can\nhave. This value must be provided, since a confusion matrix of\ndimension = [num_classes, num_classes] will be allocated.", "weights": "Optional Tensor whose rank is either 0, or the same rank as\nlabels, and must be broadcastable to labels (i.e., all dimensions must\nbe either 1, or the same as the corresponding labels dimension).", "metrics_collections": "An optional list of collections that mean_iou\nshould be added to.", "updates_collections": "An optional list of collections update_op should be\nadded to.", "name": "An optional variable_scope name."}, "Returns": "mean_iou\n\n\nA Tensor representing the mean intersection-over-union.", "Raises": {"ValueError": "If predictions and labels have mismatched shapes, or if\nweights is not None and its shape doesn't match predictions, or if\neither metrics_collections or updates_collections are not a list or\ntuple.", "RuntimeError": "If eager execution is enabled."}}, "tf.compat.v1.metrics.mean_per_class_accuracy": {"description": "Calculates the mean of the per-class accuracies.", "Args": {"labels": "A Tensor of ground truth labels with shape [batch size] and of\ntype int32 or int64. The tensor will be flattened if its rank > 1.", "predictions": "A Tensor of prediction results for semantic labels, whose\nshape is [batch size] and type int32 or int64. The tensor will be\nflattened if its rank > 1.", "num_classes": "The possible number of labels the prediction task can\nhave. This value must be provided, since two variables with shape =\n[num_classes] will be allocated.", "weights": "Optional Tensor whose rank is either 0, or the same rank as\nlabels, and must be broadcastable to labels (i.e., all dimensions must\nbe either 1, or the same as the corresponding labels dimension)."}, "Returns": "mean_accuracy\n\n\nA Tensor representing the mean per class accuracy.", "Raises": {"ValueError": "If predictions and labels have mismatched shapes, or if\nweights is not None and its shape doesn't match predictions, or if\neither metrics_collections or updates_collections are not a list or\ntuple.", "RuntimeError": "If eager execution is enabled."}}, "tf.compat.v1.metrics.mean_relative_error": {"description": "Computes the mean relative error by normalizing with the given values.", "Args": {"labels": "A Tensor of the same shape as predictions.", "predictions": "A Tensor of arbitrary shape.", "normalizer": "A Tensor of the same shape as predictions.", "weights": "Optional Tensor whose rank is either 0, or the same rank as\nlabels, and must be broadcastable to labels (i.e., all dimensions must\nbe either 1, or the same as the corresponding labels dimension).", "metrics_collections": "An optional list of collections that\nmean_relative_error should be added to.", "updates_collections": "An optional list of collections that update_op should\nbe added to.", "name": "An optional variable_scope name."}, "Returns": "mean_relative_error\n\n\nA Tensor representing the current mean, the value of\ntotal divided by count.", "Raises": {"ValueError": "If predictions and labels have mismatched shapes, or if\nweights is not None and its shape doesn't match predictions, or if\neither metrics_collections or updates_collections are not a list or\ntuple.", "RuntimeError": "If eager execution is enabled."}}, "tf.compat.v1.metrics.mean_squared_error": {"description": "Computes the mean squared error between the labels and predictions.", "Args": {"labels": "A Tensor of the same shape as predictions.", "predictions": "A Tensor of arbitrary shape.", "weights": "Optional Tensor whose rank is either 0, or the same rank as\nlabels, and must be broadcastable to labels (i.e., all dimensions must\nbe either 1, or the same as the corresponding labels dimension).", "metrics_collections": "An optional list of collections that\nmean_squared_error should be added to.", "updates_collections": "An optional list of collections that update_op should\nbe added to.", "name": "An optional variable_scope name."}, "Returns": "mean_squared_error\n\n\nA Tensor representing the current mean, the value of\ntotal divided by count.", "Raises": {"ValueError": "If predictions and labels have mismatched shapes, or if\nweights is not None and its shape doesn't match predictions, or if\neither metrics_collections or updates_collections are not a list or\ntuple.", "RuntimeError": "If eager execution is enabled."}}, "tf.compat.v1.metrics.mean_tensor": {"description": "Computes the element-wise (weighted) mean of the given tensors.", "Args": {"values": "A Tensor of arbitrary dimensions.", "weights": "Optional Tensor whose rank is either 0, or the same rank as\nvalues, and must be broadcastable to values (i.e., all dimensions must\nbe either 1, or the same as the corresponding values dimension).", "metrics_collections": "An optional list of collections that mean\nshould be added to.", "updates_collections": "An optional list of collections that update_op\nshould be added to.", "name": "An optional variable_scope name."}, "Returns": "mean\n\n\nA float Tensor representing the current mean, the value of total\ndivided by count.", "Raises": {"ValueError": "If weights is not None and its shape doesn't match values,\nor if either metrics_collections or updates_collections are not a list\nor tuple.", "RuntimeError": "If eager execution is enabled."}}, "tf.compat.v1.metrics.percentage_below": {"description": "Computes the percentage of values less than the given threshold.", "Args": {"values": "A numeric Tensor of arbitrary size.", "threshold": "A scalar threshold.", "weights": "Optional Tensor whose rank is either 0, or the same rank as\nvalues, and must be broadcastable to values (i.e., all dimensions must\nbe either 1, or the same as the corresponding values dimension).", "metrics_collections": "An optional list of collections that the metric\nvalue variable should be added to.", "updates_collections": "An optional list of collections that the metric update\nops should be added to.", "name": "An optional variable_scope name."}, "Returns": "percentage\n\n\nA Tensor representing the current mean, the value of total\ndivided by count.", "Raises": {"ValueError": "If weights is not None and its shape doesn't match values,\nor if either metrics_collections or updates_collections are not a list\nor tuple.", "RuntimeError": "If eager execution is enabled."}}, "tf.compat.v1.metrics.precision": {"description": "Computes the precision of the predictions with respect to the labels.", "Args": {"labels": "The ground truth values, a Tensor whose dimensions must match\npredictions. Will be cast to bool.", "predictions": "The predicted values, a Tensor of arbitrary dimensions. Will\nbe cast to bool.", "weights": "Optional Tensor whose rank is either 0, or the same rank as\nlabels, and must be broadcastable to labels (i.e., all dimensions must\nbe either 1, or the same as the corresponding labels dimension).", "metrics_collections": "An optional list of collections that precision should\nbe added to.", "updates_collections": "An optional list of collections that update_op should\nbe added to.", "name": "An optional variable_scope name."}, "Returns": "precision\n\n\nScalar float Tensor with the value of true_positives\ndivided by the sum of true_positives and false_positives.", "Raises": {"ValueError": "If predictions and labels have mismatched shapes, or if\nweights is not None and its shape doesn't match predictions, or if\neither metrics_collections or updates_collections are not a list or\ntuple.", "RuntimeError": "If eager execution is enabled."}}, "tf.compat.v1.metrics.precision_at_k": {"description": "Computes precision@k of the predictions with respect to sparse labels.", "Args": {"labels": "int64 Tensor or SparseTensor with shape\n[D1, ... DN, num_labels] or [D1, ... DN], where the latter implies\nnum_labels=1. N >= 1 and num_labels is the number of target classes for\nthe associated prediction. Commonly, N=1 and labels has shape\n[batch_size, num_labels]. [D1, ... DN] must match predictions. Values\nshould be in range [0, num_classes), where num_classes is the last\ndimension of predictions. Values outside this range are ignored.", "predictions": "Float Tensor with shape [D1, ... DN, num_classes] where\nN >= 1. Commonly, N=1 and predictions has shape [batch size, num_classes].\nThe final dimension contains the logit values for each class. [D1, ... DN]\nmust match labels.", "k": "Integer, k for @k metric.", "class_id": "Integer class ID for which we want binary metrics. This should be\nin range [0, num_classes], where num_classes is the last dimension of\npredictions. If class_id is outside this range, the method returns\nNAN.", "weights": "Tensor whose rank is either 0, or n-1, where n is the rank of\nlabels. If the latter, it must be broadcastable to labels (i.e., all\ndimensions must be either 1, or the same as the corresponding labels\ndimension).", "metrics_collections": "An optional list of collections that values should\nbe added to.", "updates_collections": "An optional list of collections that updates should\nbe added to.", "name": "Name of new update operation, and namespace for other dependent ops."}, "Returns": "precision\n\n\nScalar float64 Tensor with the value of true_positives\ndivided by the sum of true_positives and false_positives.", "Raises": {"ValueError": "If weights is not None and its shape doesn't match\npredictions, or if either metrics_collections or updates_collections\nare not a list or tuple.", "RuntimeError": "If eager execution is enabled."}}, "tf.compat.v1.metrics.precision_at_thresholds": {"description": "Computes precision values for different thresholds on predictions.", "Args": {"labels": "The ground truth values, a Tensor whose dimensions must match\npredictions. Will be cast to bool.", "predictions": "A floating point Tensor of arbitrary shape and whose values\nare in the range [0, 1].", "thresholds": "A python list or tuple of float thresholds in [0, 1].", "weights": "Optional Tensor whose rank is either 0, or the same rank as\nlabels, and must be broadcastable to labels (i.e., all dimensions must\nbe either 1, or the same as the corresponding labels dimension).", "metrics_collections": "An optional list of collections that auc should be\nadded to.", "updates_collections": "An optional list of collections that update_op should\nbe added to.", "name": "An optional variable_scope name."}, "Returns": "precision\n\n\nA float Tensor of shape [len(thresholds)].", "Raises": {"ValueError": "If predictions and labels have mismatched shapes, or if\nweights is not None and its shape doesn't match predictions, or if\neither metrics_collections or updates_collections are not a list or\ntuple.", "RuntimeError": "If eager execution is enabled."}}, "tf.compat.v1.metrics.precision_at_top_k": {"description": "Computes precision@k of the predictions with respect to sparse labels.", "Args": {"labels": "int64 Tensor or SparseTensor with shape\n[D1, ... DN, num_labels] or [D1, ... DN], where the latter implies\nnum_labels=1. N >= 1 and num_labels is the number of target classes for\nthe associated prediction. Commonly, N=1 and labels has shape\n[batch_size, num_labels]. [D1, ... DN] must match predictions. Values\nshould be in range [0, num_classes), where num_classes is the last\ndimension of predictions. Values outside this range are ignored.", "predictions_idx": "Integer Tensor with shape [D1, ... DN, k] where\nN >= 1. Commonly, N=1 and predictions has shape [batch size, k].\nThe final dimension contains the top k predicted class indices.\n[D1, ... DN] must match labels.", "k": "Integer, k for @k metric. Only used for the default op name.", "class_id": "Integer class ID for which we want binary metrics. This should be\nin range [0, num_classes], where num_classes is the last dimension of\npredictions. If class_id is outside this range, the method returns\nNAN.", "weights": "Tensor whose rank is either 0, or n-1, where n is the rank of\nlabels. If the latter, it must be broadcastable to labels (i.e., all\ndimensions must be either 1, or the same as the corresponding labels\ndimension).", "metrics_collections": "An optional list of collections that values should\nbe added to.", "updates_collections": "An optional list of collections that updates should\nbe added to.", "name": "Name of new update operation, and namespace for other dependent ops."}, "Returns": "precision\n\n\nScalar float64 Tensor with the value of true_positives\ndivided by the sum of true_positives and false_positives.", "Raises": {"ValueError": "If weights is not None and its shape doesn't match\npredictions, or if either metrics_collections or updates_collections\nare not a list or tuple.", "RuntimeError": "If eager execution is enabled."}}, "tf.compat.v1.metrics.recall": {"description": "Computes the recall of the predictions with respect to the labels.", "Args": {"labels": "The ground truth values, a Tensor whose dimensions must match\npredictions. Will be cast to bool.", "predictions": "The predicted values, a Tensor of arbitrary dimensions. Will\nbe cast to bool.", "weights": "Optional Tensor whose rank is either 0, or the same rank as\nlabels, and must be broadcastable to labels (i.e., all dimensions must\nbe either 1, or the same as the corresponding labels dimension).", "metrics_collections": "An optional list of collections that recall should\nbe added to.", "updates_collections": "An optional list of collections that update_op should\nbe added to.", "name": "An optional variable_scope name."}, "Returns": "recall\n\n\nScalar float Tensor with the value of true_positives divided\nby the sum of true_positives and false_negatives.", "Raises": {"ValueError": "If predictions and labels have mismatched shapes, or if\nweights is not None and its shape doesn't match predictions, or if\neither metrics_collections or updates_collections are not a list or\ntuple.", "RuntimeError": "If eager execution is enabled."}}, "tf.compat.v1.metrics.recall_at_k": {"description": "Computes recall@k of the predictions with respect to sparse labels.", "Args": {"labels": "int64 Tensor or SparseTensor with shape\n[D1, ... DN, num_labels] or [D1, ... DN], where the latter implies\nnum_labels=1. N >= 1 and num_labels is the number of target classes for\nthe associated prediction. Commonly, N=1 and labels has shape\n[batch_size, num_labels]. [D1, ... DN] must match predictions. Values\nshould be in range [0, num_classes), where num_classes is the last\ndimension of predictions. Values outside this range always count\ntowards false_negative_at_<k>.", "predictions": "Float Tensor with shape [D1, ... DN, num_classes] where\nN >= 1. Commonly, N=1 and predictions has shape [batch size, num_classes].\nThe final dimension contains the logit values for each class. [D1, ... DN]\nmust match labels.", "k": "Integer, k for @k metric.", "class_id": "Integer class ID for which we want binary metrics. This should be\nin range [0, num_classes), where num_classes is the last dimension of\npredictions. If class_id is outside this range, the method returns NAN.", "weights": "Tensor whose rank is either 0, or n-1, where n is the rank of\nlabels. If the latter, it must be broadcastable to labels (i.e., all\ndimensions must be either 1, or the same as the corresponding labels\ndimension).", "metrics_collections": "An optional list of collections that values should\nbe added to.", "updates_collections": "An optional list of collections that updates should\nbe added to.", "name": "Name of new update operation, and namespace for other dependent ops."}, "Returns": "recall\n\n\nScalar float64 Tensor with the value of true_positives divided\nby the sum of true_positives and false_negatives.", "Raises": {"ValueError": "If weights is not None and its shape doesn't match\npredictions, or if either metrics_collections or updates_collections\nare not a list or tuple.", "RuntimeError": "If eager execution is enabled."}}, "tf.compat.v1.metrics.recall_at_thresholds": {"description": "Computes various recall values for different thresholds on predictions.", "Args": {"labels": "The ground truth values, a Tensor whose dimensions must match\npredictions. Will be cast to bool.", "predictions": "A floating point Tensor of arbitrary shape and whose values\nare in the range [0, 1].", "thresholds": "A python list or tuple of float thresholds in [0, 1].", "weights": "Optional Tensor whose rank is either 0, or the same rank as\nlabels, and must be broadcastable to labels (i.e., all dimensions must\nbe either 1, or the same as the corresponding labels dimension).", "metrics_collections": "An optional list of collections that recall should be\nadded to.", "updates_collections": "An optional list of collections that update_op should\nbe added to.", "name": "An optional variable_scope name."}, "Returns": "recall\n\n\nA float Tensor of shape [len(thresholds)].", "Raises": {"ValueError": "If predictions and labels have mismatched shapes, or if\nweights is not None and its shape doesn't match predictions, or if\neither metrics_collections or updates_collections are not a list or\ntuple.", "RuntimeError": "If eager execution is enabled."}}, "tf.compat.v1.metrics.recall_at_top_k": {"description": "Computes recall@k of top-k predictions with respect to sparse labels.", "Args": {"labels": "int64 Tensor or SparseTensor with shape\n[D1, ... DN, num_labels] or [D1, ... DN], where the latter implies\nnum_labels=1. N >= 1 and num_labels is the number of target classes for\nthe associated prediction. Commonly, N=1 and labels has shape\n[batch_size, num_labels]. [D1, ... DN] must match predictions. Values\nshould be in range [0, num_classes), where num_classes is the last\ndimension of predictions. Values outside this range always count\ntowards false_negative_at_<k>.", "predictions_idx": "Integer Tensor with shape [D1, ... DN, k] where N >= 1.\nCommonly, N=1 and predictions has shape [batch size, k]. The final\ndimension contains the top k predicted class indices. [D1, ... DN] must\nmatch labels.", "k": "Integer, k for @k metric. Only used for the default op name.", "class_id": "Integer class ID for which we want binary metrics. This should be\nin range [0, num_classes), where num_classes is the last dimension of\npredictions. If class_id is outside this range, the method returns NAN.", "weights": "Tensor whose rank is either 0, or n-1, where n is the rank of\nlabels. If the latter, it must be broadcastable to labels (i.e., all\ndimensions must be either 1, or the same as the corresponding labels\ndimension).", "metrics_collections": "An optional list of collections that values should\nbe added to.", "updates_collections": "An optional list of collections that updates should\nbe added to.", "name": "Name of new update operation, and namespace for other dependent ops."}, "Returns": "recall\n\n\nScalar float64 Tensor with the value of true_positives divided\nby the sum of true_positives and false_negatives.", "Raises": {"ValueError": "If weights is not None and its shape doesn't match\npredictions, or if either metrics_collections or updates_collections\nare not a list or tuple."}}, "tf.compat.v1.metrics.root_mean_squared_error": {"description": "Computes the root mean squared error between the labels and predictions.", "Args": {"labels": "A Tensor of the same shape as predictions.", "predictions": "A Tensor of arbitrary shape.", "weights": "Optional Tensor whose rank is either 0, or the same rank as\nlabels, and must be broadcastable to labels (i.e., all dimensions must\nbe either 1, or the same as the corresponding labels dimension).", "metrics_collections": "An optional list of collections that\nroot_mean_squared_error should be added to.", "updates_collections": "An optional list of collections that update_op should\nbe added to.", "name": "An optional variable_scope name."}, "Returns": "root_mean_squared_error\n\n\nA Tensor representing the current mean, the value\nof total divided by count.", "Raises": {"ValueError": "If predictions and labels have mismatched shapes, or if\nweights is not None and its shape doesn't match predictions, or if\neither metrics_collections or updates_collections are not a list or\ntuple.", "RuntimeError": "If eager execution is enabled."}}, "tf.compat.v1.metrics.sensitivity_at_specificity": {"description": "Computes the specificity at a given sensitivity.", "Args": {"labels": "The ground truth values, a Tensor whose dimensions must match\npredictions. Will be cast to bool.", "predictions": "A floating point Tensor of arbitrary shape and whose values\nare in the range [0, 1].", "specificity": "A scalar value in range [0, 1].", "weights": "Optional Tensor whose rank is either 0, or the same rank as\nlabels, and must be broadcastable to labels (i.e., all dimensions must\nbe either 1, or the same as the corresponding labels dimension).", "num_thresholds": "The number of thresholds to use for matching the given\nspecificity.", "metrics_collections": "An optional list of collections that sensitivity\nshould be added to.", "updates_collections": "An optional list of collections that update_op should\nbe added to.", "name": "An optional variable_scope name."}, "Returns": "sensitivity\n\n\nA scalar Tensor representing the sensitivity at the given\nspecificity value.", "Raises": {"ValueError": "If predictions and labels have mismatched shapes, if\nweights is not None and its shape doesn't match predictions, or if\nspecificity is not between 0 and 1, or if either metrics_collections\nor updates_collections are not a list or tuple.", "RuntimeError": "If eager execution is enabled."}}, "tf.compat.v1.metrics.sparse_average_precision_at_k": {"description": "Renamed to average_precision_at_k, please use that method instead. (deprecated)"}, "tf.compat.v1.metrics.sparse_precision_at_k": {"description": "Renamed to precision_at_k, please use that method instead. (deprecated)"}, "tf.compat.v1.metrics.specificity_at_sensitivity": {"description": "Computes the specificity at a given sensitivity.", "Args": {"labels": "The ground truth values, a Tensor whose dimensions must match\npredictions. Will be cast to bool.", "predictions": "A floating point Tensor of arbitrary shape and whose values\nare in the range [0, 1].", "sensitivity": "A scalar value in range [0, 1].", "weights": "Optional Tensor whose rank is either 0, or the same rank as\nlabels, and must be broadcastable to labels (i.e., all dimensions must\nbe either 1, or the same as the corresponding labels dimension).", "num_thresholds": "The number of thresholds to use for matching the given\nsensitivity.", "metrics_collections": "An optional list of collections that specificity\nshould be added to.", "updates_collections": "An optional list of collections that update_op should\nbe added to.", "name": "An optional variable_scope name."}, "Returns": "specificity\n\n\nA scalar Tensor representing the specificity at the given\nsensitivity value.", "Raises": {"ValueError": "If predictions and labels have mismatched shapes, if\nweights is not None and its shape doesn't match predictions, or if\nsensitivity is not between 0 and 1, or if either metrics_collections\nor updates_collections are not a list or tuple.", "RuntimeError": "If eager execution is enabled."}}, "tf.compat.v1.metrics.true_negatives": {"description": "Sum the weights of true_negatives.", "Args": {"labels": "The ground truth values, a Tensor whose dimensions must match\npredictions. Will be cast to bool.", "predictions": "The predicted values, a Tensor of arbitrary dimensions. Will\nbe cast to bool.", "weights": "Optional Tensor whose rank is either 0, or the same rank as\nlabels, and must be broadcastable to labels (i.e., all dimensions must\nbe either 1, or the same as the corresponding labels dimension).", "metrics_collections": "An optional list of collections that the metric\nvalue variable should be added to.", "updates_collections": "An optional list of collections that the metric update\nops should be added to.", "name": "An optional variable_scope name."}, "Returns": "value_tensor\n\n\nA Tensor representing the current value of the metric.", "Raises": {"ValueError": "If predictions and labels have mismatched shapes, or if\nweights is not None and its shape doesn't match predictions, or if\neither metrics_collections or updates_collections are not a list or\ntuple.", "RuntimeError": "If eager execution is enabled."}}, "tf.compat.v1.metrics.true_negatives_at_thresholds": {"description": "Computes true negatives at provided threshold values.", "Args": {"labels": "A Tensor whose shape matches predictions. Will be cast to\nbool.", "predictions": "A floating point Tensor of arbitrary shape and whose values\nare in the range [0, 1].", "thresholds": "A python list or tuple of float thresholds in [0, 1].", "weights": "Optional Tensor whose rank is either 0, or the same rank as\nlabels, and must be broadcastable to labels (i.e., all dimensions must\nbe either 1, or the same as the corresponding labels dimension).", "metrics_collections": "An optional list of collections that true_negatives\nshould be added to.", "updates_collections": "An optional list of collections that update_op should\nbe added to.", "name": "An optional variable_scope name."}, "Returns": "true_negatives\n\n\n A float Tensor of shape [len(thresholds)].", "Raises": {"ValueError": "If predictions and labels have mismatched shapes, or if\nweights is not None and its shape doesn't match predictions, or if\neither metrics_collections or updates_collections are not a list or\ntuple.", "RuntimeError": "If eager execution is enabled."}}, "tf.compat.v1.metrics.true_positives": {"description": "Sum the weights of true_positives.", "Args": {"labels": "The ground truth values, a Tensor whose dimensions must match\npredictions. Will be cast to bool.", "predictions": "The predicted values, a Tensor of arbitrary dimensions. Will\nbe cast to bool.", "weights": "Optional Tensor whose rank is either 0, or the same rank as\nlabels, and must be broadcastable to labels (i.e., all dimensions must\nbe either 1, or the same as the corresponding labels dimension).", "metrics_collections": "An optional list of collections that the metric\nvalue variable should be added to.", "updates_collections": "An optional list of collections that the metric update\nops should be added to.", "name": "An optional variable_scope name."}, "Returns": "value_tensor\n\n\nA Tensor representing the current value of the metric.", "Raises": {"ValueError": "If predictions and labels have mismatched shapes, or if\nweights is not None and its shape doesn't match predictions, or if\neither metrics_collections or updates_collections are not a list or\ntuple.", "RuntimeError": "If eager execution is enabled."}}, "tf.compat.v1.metrics.true_positives_at_thresholds": {"description": "Computes true positives at provided threshold values.", "Args": {"labels": "A Tensor whose shape matches predictions. Will be cast to\nbool.", "predictions": "A floating point Tensor of arbitrary shape and whose values\nare in the range [0, 1].", "thresholds": "A python list or tuple of float thresholds in [0, 1].", "weights": "Optional Tensor whose rank is either 0, or the same rank as\nlabels, and must be broadcastable to labels (i.e., all dimensions must\nbe either 1, or the same as the corresponding labels dimension).", "metrics_collections": "An optional list of collections that true_positives\nshould be added to.", "updates_collections": "An optional list of collections that update_op should\nbe added to.", "name": "An optional variable_scope name."}, "Returns": "true_positives\n\n\n A float Tensor of shape [len(thresholds)].", "Raises": {"ValueError": "If predictions and labels have mismatched shapes, or if\nweights is not None and its shape doesn't match predictions, or if\neither metrics_collections or updates_collections are not a list or\ntuple.", "RuntimeError": "If eager execution is enabled."}}}, "tf.compat.v1.mixed_precision": {"tf.compat.v1.mixed_precision.DynamicLossScale": {"description": "Loss scale that dynamically adjusts itself.", "Args": {"initial_loss_scale": "A Python float.  The loss scale to use at the\nbeginning. It's better to start this at a very high number, because a\nloss scale that is too high gets lowered far more quickly than a loss\nscale that is too low gets raised. The default is 2 ** 15, which is\napproximately half the maximum float16 value.", "increment_period": "Increases loss scale every increment_period\nconsecutive steps that finite gradients are encountered. If a nonfinite\ngradient is encountered, the count is reset back to zero.", "multiplier": "The multiplier to use when increasing or decreasing the loss\nscale."}, "Attributes": {"increment_period": "", "initial_loss_scale": "", "multiplier": ""}}, "tf.compat.v1.mixed_precision.FixedLossScale": {"description": "Loss scale with a fixed value.", "Args": {"loss_scale_value": "A Python float. Its ideal value varies depending on\nmodels to run. Choosing a too small loss_scale might affect model\nquality; a too big loss_scale might cause inf or nan. There is no single\nright loss_scale to apply. There is no harm choosing a relatively big\nnumber as long as no nan or inf is encountered in training."}, "Raises": {"ValueError": "If loss_scale_value is less than 1."}}, "tf.compat.v1.mixed_precision.LossScale": {"description": "Base class for all TF1 loss scales."}, "tf.compat.v1.mixed_precision.MixedPrecisionLossScaleOptimizer": {"description": "An optimizer that applies loss scaling.", "Args": {"use_locking": "Bool. If True apply use locks to prevent concurrent updates\nto variables.", "name": "A non-empty string.  The name to use for accumulators created\nfor the optimizer."}, "Raises": {"ValueError": "If name is malformed."}, "Class Variables": {"GATE_GRAPH": "2", "GATE_NONE": "0", "GATE_OP": "1"}}, "tf.compat.v1.mixed_precision.disable_mixed_precision_graph_rewrite": {"description": "Disables the mixed precision graph rewrite."}, "tf.compat.v1.mixed_precision.enable_mixed_precision_graph_rewrite": {"description": "Enable mixed precision via a graph rewrite.", "Raises": {}, "Args": {"opt": "An instance of a tf.keras.optimizers.Optimizer or a\ntf.train.Optimizer.", "loss_scale": "Either an int/float, the string \"dynamic\", or an instance of\na tf.mixed_precision.experimental.LossScale. The loss scale to use. It\nis recommended to keep this as its default value of \"dynamic\", which\nwill adjust the scaling automatically to prevent Inf or NaN values."}, "Returns": "A version of opt that will use loss scaling to prevent underflow."}}, "tf.compat.v1.mlir": {}, "tf.compat.v1.nest": {}, "tf.compat.v1.nn": {"tf.compat.v1.nn.avg_pool": {"description": "Performs the average pooling on the input.", "Args": {"value": "A 4-D Tensor of shape [batch, height, width, channels] and type\nfloat32, float64, qint8, quint8, or qint32.", "ksize": "An int or list of ints that has length 1, 2 or 4. The size of\nthe window for each dimension of the input tensor.", "strides": "An int or list of ints that has length 1, 2 or 4. The\nstride of the sliding window for each dimension of the input tensor.", "padding": "A string, either 'VALID' or 'SAME'. The padding algorithm.\nSee the \"returns\" section of tf.nn.convolution for details.", "data_format": "A string. 'NHWC' and 'NCHW' are supported.", "name": "Optional name for the operation.", "input": "Alias for value."}, "Returns": "A Tensor with the same type as value.  The average pooled output tensor."}, "tf.compat.v1.nn.batch_norm_with_global_normalization": {"description": "Batch normalization.", "Args": {"t": "A 4D input Tensor.", "m": "A 1D mean Tensor with size matching the last dimension of t.\nThis is the first output from tf.nn.moments,\nor a saved moving average thereof.", "v": "A 1D variance Tensor with size matching the last dimension of t.\nThis is the second output from tf.nn.moments,\nor a saved moving average thereof.", "beta": "A 1D beta Tensor with size matching the last dimension of t.\nAn offset to be added to the normalized tensor.", "gamma": "A 1D gamma Tensor with size matching the last dimension of t.\nIf \"scale_after_normalization\" is true, this tensor will be multiplied\nwith the normalized tensor.", "variance_epsilon": "A small float number to avoid dividing by 0.", "scale_after_normalization": "A bool indicating whether the resulted tensor\nneeds to be multiplied with gamma.", "name": "A name for this operation (optional).", "input": "Alias for t.", "mean": "Alias for m.", "variance": "Alias for v."}, "Returns": "A batch-normalized t."}, "tf.compat.v1.nn.bidirectional_dynamic_rnn": {"description": "Creates a dynamic version of bidirectional recurrent neural network. (deprecated)", "Args": {"cell_fw": "An instance of RNNCell, to be used for forward direction.", "cell_bw": "An instance of RNNCell, to be used for backward direction.", "inputs": "The RNN inputs.\nIf time_major == False (default), this must be a tensor of shape:\n  [batch_size, max_time, ...], or a nested tuple of such elements.\nIf time_major == True, this must be a tensor of shape: [max_time,\n  batch_size, ...], or a nested tuple of such elements.", "sequence_length": "(optional) An int32/int64 vector, size [batch_size],\ncontaining the actual lengths for each of the sequences in the batch. If\nnot provided, all batch entries are assumed to be full sequences; and time\nreversal is applied from time 0 to max_time for each sequence.", "initial_state_fw": "(optional) An initial state for the forward RNN. This must\nbe a tensor of appropriate type and shape [batch_size,\ncell_fw.state_size]. If cell_fw.state_size is a tuple, this should be a\ntuple of tensors having shapes [batch_size, s] for s in\ncell_fw.state_size.", "initial_state_bw": "(optional) Same as for initial_state_fw, but using the\ncorresponding properties of cell_bw.", "dtype": "(optional) The data type for the initial states and expected output.\nRequired if initial_states are not provided or RNN states have a\nheterogeneous dtype.", "parallel_iterations": "(Default: 32).  The number of iterations to run in\nparallel.  Those operations which do not have any temporal dependency and\ncan be run in parallel, will be.  This parameter trades off time for\nspace.  Values >> 1 use more memory but take less time, while smaller\nvalues use less memory but computations take longer.", "swap_memory": "Transparently swap the tensors produced in forward inference\nbut needed for back prop from GPU to CPU.  This allows training RNNs which\nwould typically not fit on a single GPU, with very minimal (or no)\nperformance penalty.", "time_major": "The shape format of the inputs and outputs Tensors. If true,\nthese Tensors must be shaped [max_time, batch_size, depth]. If false,\nthese Tensors must be shaped [batch_size, max_time, depth]. Using\ntime_major = True is a bit more efficient because it avoids transposes\nat the beginning and end of the RNN calculation.  However, most TensorFlow\ndata is batch-major, so by default this function accepts input and emits\noutput in batch-major form.", "scope": "VariableScope for the created subgraph; defaults to\n\"bidirectional_rnn\""}, "Returns": "A tuple (outputs, output_states) where:\noutputs: A tuple (output_fw, output_bw) containing the forward and\n  the backward rnn output Tensor.\n  If time_major == False (default),\n    output_fw will be a Tensor shaped:\n    [batch_size, max_time, cell_fw.output_size]\n    and output_bw will be a Tensor shaped:\n    [batch_size, max_time, cell_bw.output_size].\n  If time_major == True,\n    output_fw will be a Tensor shaped:\n    [max_time, batch_size, cell_fw.output_size]\n    and output_bw will be a Tensor shaped:\n    [max_time, batch_size, cell_bw.output_size].\n  It returns a tuple instead of a single concatenated Tensor, unlike\n  in the bidirectional_rnn. If the concatenated one is preferred,\n  the forward and backward outputs can be concatenated as\n  tf.concat(outputs, 2).\noutput_states: A tuple (output_state_fw, output_state_bw) containing\n  the forward and the backward final states of bidirectional rnn.", "Raises": {"TypeError": "If cell_fw or cell_bw is not an instance of RNNCell."}}, "tf.compat.v1.nn.conv1d": {"description": "Computes a 1-D convolution of input with rank &gt;=3 and a 3-D filter. (deprecated argument values) (deprecated argument values)", "Args": {"value": "A Tensor of rank at least 3. Must be of type float16, float32, or\nfloat64.", "filters": "A Tensor of rank at least 3.  Must have the same type as value.", "stride": "An int or list of ints that has length 1 or 3.  The number of\nentries by which the filter is moved right at each step.", "padding": "'SAME' or 'VALID'", "use_cudnn_on_gpu": "An optional bool.  Defaults to True.", "data_format": "An optional string from \"NWC\", \"NCW\".  Defaults to \"NWC\",\nthe data is stored in the order of batch_shape + [in_width,\nin_channels].  The \"NCW\" format stores data as batch_shape +\n[in_channels, in_width].", "name": "A name for the operation (optional).", "input": "Alias for value.", "dilations": "An int or list of ints that has length 1 or 3 which\ndefaults to 1. The dilation factor for each dimension of input. If set to\nk > 1, there will be k-1 skipped cells between each filter element on that\ndimension. Dilations in the batch and depth dimensions must be 1."}, "Returns": "A Tensor.  Has the same type as input.", "Raises": {"ValueError": "if data_format is invalid."}}, "tf.compat.v1.nn.conv2d": {"description": "Computes a 2-D convolution given 4-D input and filter tensors.", "Args": {"input": "A Tensor. Must be one of the following types:\nhalf, bfloat16, float32, float64.\nA 4-D tensor. The dimension order is interpreted according to the value\nof data_format, see below for details.", "filter": "A Tensor. Must have the same type as input.\nA 4-D tensor of shape\n[filter_height, filter_width, in_channels, out_channels]", "strides": "An int or list of ints that has length 1, 2 or 4.  The\nstride of the sliding window for each dimension of input. If a single\nvalue is given it is replicated in the H and W dimension. By default\nthe N and C dimensions are set to 1. The dimension order is determined\nby the value of data_format, see below for details.", "padding": "Either the string \"SAME\" or \"VALID\" indicating the type of\npadding algorithm to use, or a list indicating the explicit paddings at\nthe start and end of each dimension. When explicit padding is used and\ndata_format is \"NHWC\", this should be in the form [[0, 0], [pad_top,\npad_bottom], [pad_left, pad_right], [0, 0]]. When explicit padding used\nand data_format is \"NCHW\", this should be in the form [[0, 0], [0, 0],\n[pad_top, pad_bottom], [pad_left, pad_right]].", "use_cudnn_on_gpu": "An optional bool. Defaults to True.", "data_format": "An optional string from: \"NHWC\", \"NCHW\".\nDefaults to \"NHWC\".\nSpecify the data format of the input and output data. With the\ndefault format \"NHWC\", the data is stored in the order of:\n    [batch, height, width, channels].\nAlternatively, the format could be \"NCHW\", the data storage order of:\n    [batch, channels, height, width].", "dilations": "An int or list of ints that has length 1, 2 or 4,\ndefaults to 1. The dilation factor for each dimension ofinput. If a\nsingle value is given it is replicated in the H and W dimension. By\ndefault the N and C dimensions are set to 1. If set to k > 1, there\nwill be k-1 skipped cells between each filter element on that dimension.\nThe dimension order is determined by the value of data_format, see above\nfor details. Dilations in the batch and depth dimensions if a 4-d tensor\nmust be 1.", "name": "A name for the operation (optional).", "filters": "Alias for filter."}, "Returns": "A Tensor. Has the same type as input."}, "tf.compat.v1.nn.conv2d_backprop_filter": {"description": "Computes the gradients of convolution with respect to the filter.", "Args": {"input": "A Tensor. Must be one of the following types:\nhalf, bfloat16, float32, float64.\n4-D with shape [batch, in_height, in_width, in_channels].", "filter_sizes": "A Tensor of type int32.\nAn integer vector representing the tensor shape of filter,\nwhere filter is a 4-D\n[filter_height, filter_width, in_channels, out_channels] tensor.", "out_backprop": "A Tensor. Must have the same type as input.\n4-D with shape [batch, out_height, out_width, out_channels].\nGradients w.r.t. the output of the convolution.", "strides": "A list of ints.\nThe stride of the sliding window for each dimension of the input\nof the convolution. Must be in the same order as the dimension specified\nwith format.", "padding": "Either the string \"SAME\" or \"VALID\" indicating the type of\npadding algorithm to use, or a list indicating the explicit paddings at\nthe start and end of each dimension. When explicit padding is used and\ndata_format is \"NHWC\", this should be in the form [[0, 0], [pad_top,\npad_bottom], [pad_left, pad_right], [0, 0]]. When explicit padding used\nand data_format is \"NCHW\", this should be in the form [[0, 0], [0, 0],\n[pad_top, pad_bottom], [pad_left, pad_right]].", "use_cudnn_on_gpu": "An optional bool. Defaults to True.", "data_format": "An optional string from: \"NHWC\", \"NCHW\".\nDefaults to \"NHWC\".\nSpecify the data format of the input and output data. With the\ndefault format \"NHWC\", the data is stored in the order of:\n    [batch, in_height, in_width, in_channels].\nAlternatively, the format could be \"NCHW\", the data storage order of:\n    [batch, in_channels, in_height, in_width].", "dilations": "An optional list of ints. Defaults to [1, 1, 1, 1].\n1-D tensor of length 4.  The dilation factor for each dimension of\ninput. If set to k > 1, there will be k-1 skipped cells between each\nfilter element on that dimension. The dimension order is determined by\nthe value of data_format, see above for details. Dilations in the batch\nand depth dimensions must be 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.compat.v1.nn.conv2d_backprop_input": {"description": "Computes the gradients of convolution with respect to the input.", "Args": {"input_sizes": "A Tensor of type int32.\nAn integer vector representing the shape of input,\nwhere input is a 4-D [batch, height, width, channels] tensor.", "filter": "A Tensor. Must be one of the following types:\nhalf, bfloat16, float32, float64.\n4-D with shape\n[filter_height, filter_width, in_channels, out_channels].", "out_backprop": "A Tensor. Must have the same type as filter.\n4-D with shape [batch, out_height, out_width, out_channels].\nGradients w.r.t. the output of the convolution.", "strides": "A list of ints.\nThe stride of the sliding window for each dimension of the input\nof the convolution. Must be in the same order as the dimension specified\nwith format.", "padding": "Either the string \"SAME\" or \"VALID\" indicating the type of\npadding algorithm to use, or a list indicating the explicit paddings at\nthe start and end of each dimension. When explicit padding is used and\ndata_format is \"NHWC\", this should be in the form [[0, 0], [pad_top,\npad_bottom], [pad_left, pad_right], [0, 0]]. When explicit padding used\nand data_format is \"NCHW\", this should be in the form [[0, 0], [0, 0],\n[pad_top, pad_bottom], [pad_left, pad_right]].", "use_cudnn_on_gpu": "An optional bool. Defaults to True.", "data_format": "An optional string from: \"NHWC\", \"NCHW\".\nDefaults to \"NHWC\".\nSpecify the data format of the input and output data. With the\ndefault format \"NHWC\", the data is stored in the order of:\n    [batch, in_height, in_width, in_channels].\nAlternatively, the format could be \"NCHW\", the data storage order of:\n    [batch, in_channels, in_height, in_width].", "dilations": "An optional list of ints. Defaults to [1, 1, 1, 1].\n1-D tensor of length 4.  The dilation factor for each dimension of\ninput. If set to k > 1, there will be k-1 skipped cells between each\nfilter element on that dimension. The dimension order is determined by\nthe value of data_format, see above for details. Dilations in the batch\nand depth dimensions must be 1.", "name": "A name for the operation (optional).", "filters": "Alias for filter."}, "Returns": "A Tensor. Has the same type as filter."}, "tf.compat.v1.nn.conv2d_transpose": {"description": "The transpose of conv2d.", "Args": {"value": "A 4-D Tensor of type float and shape\n[batch, height, width, in_channels] for NHWC data format or\n[batch, in_channels, height, width] for NCHW data format.", "filter": "A 4-D Tensor with the same type as value and shape\n[height, width, output_channels, in_channels].  filter's\nin_channels dimension must match that of value.", "output_shape": "A 1-D Tensor representing the output shape of the\ndeconvolution op.", "strides": "An int or list of ints that has length 1, 2 or 4.  The\nstride of the sliding window for each dimension of input. If a single\nvalue is given it is replicated in the H and W dimension. By default\nthe N and C dimensions are set to 0. The dimension order is determined\nby the value of data_format, see below for details.", "padding": "A string, either 'VALID' or 'SAME'. The padding algorithm.\nSee the \"returns\" section of tf.nn.convolution for details.", "data_format": "A string. 'NHWC' and 'NCHW' are supported.", "name": "Optional name for the returned tensor.", "input": "Alias for value.", "filters": "Alias for filter.", "dilations": "An int or list of ints that has length 1, 2 or 4,\ndefaults to 1. The dilation factor for each dimension ofinput. If a\nsingle value is given it is replicated in the H and W dimension. By\ndefault the N and C dimensions are set to 1. If set to k > 1, there\nwill be k-1 skipped cells between each filter element on that dimension.\nThe dimension order is determined by the value of data_format, see above\nfor details. Dilations in the batch and depth dimensions if a 4-d tensor\nmust be 1."}, "Returns": "A Tensor with the same type as value.", "Raises": {"ValueError": "If input/output depth does not match filter's shape, or if\npadding is other than 'VALID' or 'SAME'."}}, "tf.compat.v1.nn.conv3d": {"description": "Computes a 3-D convolution given 5-D input and filter tensors.", "Args": {"input": "A Tensor. Must be one of the following types: half, bfloat16, float32, float64.\nShape [batch, in_depth, in_height, in_width, in_channels].", "filter": "A Tensor. Must have the same type as input.\nShape [filter_depth, filter_height, filter_width, in_channels,\nout_channels]. in_channels must match between input and filter.", "strides": "A list of ints that has length >= 5.\n1-D tensor of length 5. The stride of the sliding window for each\ndimension of input. Must have strides[0] = strides[4] = 1.", "padding": "A string from: \"SAME\", \"VALID\".\nThe type of padding algorithm to use.", "data_format": "An optional string from: \"NDHWC\", \"NCDHW\". Defaults to \"NDHWC\".\nThe data format of the input and output data. With the\ndefault format \"NDHWC\", the data is stored in the order of:\n    [batch, in_depth, in_height, in_width, in_channels].\nAlternatively, the format could be \"NCDHW\", the data storage order is:\n    [batch, in_channels, in_depth, in_height, in_width].", "dilations": "An optional list of ints. Defaults to [1, 1, 1, 1, 1].\n1-D tensor of length 5.  The dilation factor for each dimension of\ninput. If set to k > 1, there will be k-1 skipped cells between each\nfilter element on that dimension. The dimension order is determined by the\nvalue of data_format, see above for details. Dilations in the batch and\ndepth dimensions must be 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.compat.v1.nn.conv3d_backprop_filter": {"description": "Computes the gradients of 3-D convolution with respect to the filter.", "Args": {"input": "A Tensor. Must be one of the following types: half, bfloat16, float32, float64.\nShape [batch, depth, rows, cols, in_channels].", "filter_sizes": "A Tensor of type int32.\nAn integer vector representing the tensor shape of filter,\nwhere filter is a 5-D\n[filter_depth, filter_height, filter_width, in_channels, out_channels]\ntensor.", "out_backprop": "A Tensor. Must have the same type as input.\nBackprop signal of shape [batch, out_depth, out_rows, out_cols,\nout_channels].", "strides": "A list of ints that has length >= 5.\n1-D tensor of length 5. The stride of the sliding window for each\ndimension of input. Must have strides[0] = strides[4] = 1.", "padding": "A string from: \"SAME\", \"VALID\".\nThe type of padding algorithm to use.", "data_format": "An optional string from: \"NDHWC\", \"NCDHW\". Defaults to \"NDHWC\".\nThe data format of the input and output data. With the\ndefault format \"NDHWC\", the data is stored in the order of:\n    [batch, in_depth, in_height, in_width, in_channels].\nAlternatively, the format could be \"NCDHW\", the data storage order is:\n    [batch, in_channels, in_depth, in_height, in_width].", "dilations": "An optional list of ints. Defaults to [1, 1, 1, 1, 1].\n1-D tensor of length 5.  The dilation factor for each dimension of\ninput. If set to k > 1, there will be k-1 skipped cells between each\nfilter element on that dimension. The dimension order is determined by the\nvalue of data_format, see above for details. Dilations in the batch and\ndepth dimensions must be 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.compat.v1.nn.conv3d_transpose": {"description": "The transpose of conv3d.", "Args": {"value": "A 5-D Tensor of type float and shape\n[batch, depth, height, width, in_channels].", "filter": "A 5-D Tensor with the same type as value and shape\n[depth, height, width, output_channels, in_channels].  filter's\nin_channels dimension must match that of value.", "output_shape": "A 1-D Tensor representing the output shape of the\ndeconvolution op.", "strides": "A list of ints. The stride of the sliding window for each\ndimension of the input tensor.", "padding": "A string, either 'VALID' or 'SAME'. The padding algorithm.\nSee the \"returns\" section of tf.nn.convolution for details.", "data_format": "A string, either 'NDHWC' or 'NCDHW' specifying the layout\nof the input and output tensors. Defaults to 'NDHWC'.", "name": "Optional name for the returned tensor.", "input": "Alias of value.", "filters": "Alias of filter.", "dilations": "An int or list of ints that has length 1, 3 or 5,\ndefaults to 1. The dilation factor for each dimension ofinput. If a\nsingle value is given it is replicated in the D, H and W dimension.\nBy default the N and C dimensions are set to 1. If set to k > 1, there\nwill be k-1 skipped cells between each filter element on that dimension.\nThe dimension order is determined by the value of data_format, see above\nfor details. Dilations in the batch and depth dimensions if a 5-d tensor\nmust be 1."}, "Returns": "A Tensor with the same type as value.", "Raises": {"ValueError": "If input/output depth does not match filter's shape, or if\npadding is other than 'VALID' or 'SAME'."}}, "tf.compat.v1.nn.convolution": {"description": "Computes sums of N-D convolutions (actually cross-correlation).", "Args": {"input": "An (N+2)-D Tensor of type T, of shape\n[batch_size] + input_spatial_shape + [in_channels] if data_format does\nnot start with \"NC\" (default), or\n[batch_size, in_channels] + input_spatial_shape if data_format starts\nwith \"NC\".", "filter": "An (N+2)-D Tensor with the same type as input and shape\nspatial_filter_shape + [in_channels, out_channels].", "padding": "A string, either \"VALID\" or \"SAME\". The padding algorithm.\n\"valid\" means no padding. \"same\" results in padding evenly to\nthe left/right or up/down of the input such that output has the same\nheight/width dimension as the input when the strides are 1. See\nhere\nfor more information.", "strides": "Optional.  Sequence of N ints >= 1.  Specifies the output stride.\nDefaults to [1]*N.  If any value of strides is > 1, then all values of\ndilation_rate must be 1.", "dilation_rate": "Optional.  Sequence of N ints >= 1.  Specifies the filter\nupsampling/input downsampling rate.  In the literature, the same parameter\nis sometimes called input stride or dilation.  The effective filter\nsize used for the convolution will be spatial_filter_shape +\n(spatial_filter_shape - 1) * (rate - 1), obtained by inserting\n(dilation_rate[i]-1) zeros between consecutive elements of the original\nfilter in each spatial dimension i.  If any value of dilation_rate is > 1,\nthen all values of strides must be 1.", "name": "Optional name for the returned tensor.", "data_format": "A string or None.  Specifies whether the channel dimension of\nthe input and output is the last dimension (default, or if data_format\ndoes not start with \"NC\"), or the second dimension (if data_format\nstarts with \"NC\").  For N=1, the valid values are \"NWC\" (default) and\n\"NCW\".  For N=2, the valid values are \"NHWC\" (default) and \"NCHW\".\nFor N=3, the valid values are \"NDHWC\" (default) and \"NCDHW\"."}, "Returns": "A Tensor with the same type as input of shape\n`[batch_size] + output_spatial_shape + [out_channels]`\nif data_format is None or does not start with \"NC\", or\n`[batch_size, out_channels] + output_spatial_shape`\nif data_format starts with \"NC\",\nwhere output_spatial_shape depends on the value of padding.\nIf padding == \"SAME\":\n  output_spatial_shape[i] = ceil(input_spatial_shape[i] / strides[i])\nIf padding == \"VALID\":\n  output_spatial_shape[i] =\n    ceil((input_spatial_shape[i] -\n          (spatial_filter_shape[i]-1) * dilation_rate[i])\n         / strides[i]).", "Raises": {"ValueError": "If input/output depth does not match filter shape, if padding\nis other than \"VALID\" or \"SAME\", or if data_format is invalid."}}, "tf.compat.v1.nn.crelu": {"description": "Computes Concatenated ReLU.", "Args": {"features": "A Tensor with type float, double, int32, int64, uint8,\nint16, or int8.", "name": "A name for the operation (optional).", "axis": "The axis that the output values are concatenated along. Default is -1."}, "Returns": "A Tensor with the same type as features."}, "tf.compat.v1.nn.ctc_beam_search_decoder": {"description": "Performs beam search decoding on the logits given in input.", "Args": {"inputs": "3-D float Tensor, size [max_time x batch_size x num_classes].\nThe logits.", "sequence_length": "1-D int32 vector containing sequence lengths, having size\n[batch_size].", "beam_width": "An int scalar >= 0 (beam search beam width).", "top_paths": "An int scalar >= 0, <= beam_width (controls output size).", "merge_repeated": "Boolean.  Default: True."}, "Returns": "A tuple (decoded, log_probabilities) where"}, "tf.compat.v1.nn.ctc_loss": {"description": "Computes the CTC (Connectionist Temporal Classification) Loss.", "Args": {"labels": "An int32 SparseTensor.\nlabels.indices[i, :] == [b, t] means labels.values[i] stores the id\n  for (batch b, time t). labels.values[i] must take on values in [0,\n  num_labels). See core/ops/ctc_ops.cc for more details.", "inputs": "3-D float Tensor.\nIf time_major == False, this will be a Tensor shaped: [batch_size,\n  max_time, num_classes].\nIf time_major == True (default), this will be a Tensor shaped:\n  [max_time, batch_size, num_classes]. The logits.", "sequence_length": "1-D int32 vector, size [batch_size]. The sequence\nlengths.", "preprocess_collapse_repeated": "Boolean.  Default: False. If True, repeated\nlabels are collapsed prior to the CTC calculation.", "ctc_merge_repeated": "Boolean.  Default: True.", "ignore_longer_outputs_than_inputs": "Boolean. Default: False. If True,\nsequences with longer outputs than inputs will be ignored.", "time_major": "The shape format of the inputs Tensors. If True, these\nTensors must be shaped [max_time, batch_size, num_classes]. If False,\nthese Tensors must be shaped [batch_size, max_time, num_classes].\nUsing time_major = True (default) is a bit more efficient because it\navoids transposes at the beginning of the ctc_loss calculation.  However,\nmost TensorFlow data is batch-major, so by this function also accepts\ninputs in batch-major form.", "logits": "Alias for inputs."}, "Returns": "A 1-D float Tensor, size [batch], containing the negative log\nprobabilities.", "Raises": {"TypeError": "if labels is not a SparseTensor."}}, "tf.compat.v1.nn.ctc_loss_v2": {"description": "Computes CTC (Connectionist Temporal Classification) loss.", "Args": {"labels": "tensor of shape [batch_size, max_label_seq_length] or SparseTensor", "logits": "tensor of shape [frames, batch_size, num_labels], if\nlogits_time_major == False, shape is [batch_size, frames, num_labels].", "label_length": "tensor of shape [batch_size], None if labels is SparseTensor\nLength of reference label sequence in labels.", "logit_length": "tensor of shape [batch_size] Length of input sequence in\nlogits.", "logits_time_major": "(optional) If True (default), logits is shaped [time,\nbatch, logits]. If False, shape is [batch, time, logits]", "unique": "(optional) Unique label indices as computed by\nctc_unique_labels(labels).  If supplied, enable a faster, memory efficient\nimplementation on TPU.", "blank_index": "(optional) Set the class index to use for the blank label.\nNegative values will start from num_classes, ie, -1 will reproduce the\nctc_loss behavior of using num_classes - 1 for the blank symbol. There is\nsome memory/performance overhead to switching from the default of 0 as an\nadditional shifted copy of the logits may be created.", "name": "A name for this Op. Defaults to \"ctc_loss_dense\"."}, "Returns": "loss\n\n\ntensor of shape [batch_size], negative log probabilities."}, "tf.compat.v1.nn.depthwise_conv2d": {"description": "Depthwise 2-D convolution.", "Args": {"input": "4-D with shape according to data_format.", "filter": "4-D with shape\n[filter_height, filter_width, in_channels, channel_multiplier].", "strides": "1-D of size 4.  The stride of the sliding window for each\ndimension of input.", "padding": "Controls how to pad the image before applying the convolution. Can\nbe the string \"SAME\" or \"VALID\" indicating the type of padding\nalgorithm to use, or a list indicating the explicit paddings at the start\nand end of each dimension. When explicit padding is used and data_format\nis \"NHWC\", this should be in the form [[0, 0], [pad_top, pad_bottom],\n[pad_left, pad_right], [0, 0]]. When explicit padding used and\ndata_format is \"NCHW\", this should be in the form [[0, 0], [0, 0],\n[pad_top, pad_bottom], [pad_left, pad_right]].", "rate": "1-D of size 2. The dilation rate in which we sample input values\nacross the height and width dimensions in atrous convolution. If it is\ngreater than 1, then all values of strides must be 1.", "name": "A name for this operation (optional).", "data_format": "The data format for input. Either \"NHWC\" (default) or \"NCHW\".", "dilations": "Alias of rate."}, "Returns": "A 4-D Tensor with shape according to data_format.  E.g., for\n\"NHWC\" format, shape is\n[batch, out_height, out_width, in_channels * channel_multiplier]."}, "tf.compat.v1.nn.depthwise_conv2d_native": {"description": "Computes a 2-D depthwise convolution.", "Args": {"input": "A Tensor. Must be one of the following types: half, bfloat16,\nfloat32, float64.", "filter": "A Tensor. Must have the same type as input.", "strides": "A list of ints. 1-D of length 4.  The stride of the sliding\nwindow for each dimension of input.", "padding": "Controls how to pad the image before applying the convolution. Can\nbe the string \"SAME\" or \"VALID\" indicating the type of padding\nalgorithm to use, or a list indicating the explicit paddings at the start\nand end of each dimension. When explicit padding is used and data_format\nis \"NHWC\", this should be in the form [[0, 0], [pad_top, pad_bottom],\n[pad_left, pad_right], [0, 0]]. When explicit padding used and\ndata_format is \"NCHW\", this should be in the form [[0, 0], [0, 0],\n[pad_top, pad_bottom], [pad_left, pad_right]].", "data_format": "An optional string from: \"NHWC\", \"NCHW\". Defaults to\n\"NHWC\". Specify the data format of the input and output data. With the\ndefault format \"NHWC\", the data is stored in the order of: [batch, height,\n  width, channels].\nAlternatively, the format could be \"NCHW\", the data storage order of:\n  [batch, channels, height, width].", "dilations": "An optional list of ints. Defaults to [1, 1, 1, 1]. 1-D\ntensor of length 4.  The dilation factor for each dimension of input. If\nset to k > 1, there will be k-1 skipped cells between each filter element\non that dimension. The dimension order is determined by the value of\ndata_format, see above for details. Dilations in the batch and depth\ndimensions must be 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.compat.v1.nn.dilation2d": {"description": "Computes the grayscale dilation of 4-D input and 3-D filter tensors.", "Args": {"input": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.\n4-D with shape [batch, in_height, in_width, depth].", "filter": "A Tensor. Must have the same type as input.\n3-D with shape [filter_height, filter_width, depth].", "strides": "A list of ints that has length >= 4.\nThe stride of the sliding window for each dimension of the input\ntensor. Must be: [1, stride_height, stride_width, 1].", "rates": "A list of ints that has length >= 4.\nThe input stride for atrous morphological dilation. Must be:\n[1, rate_height, rate_width, 1].", "padding": "A string from: \"SAME\", \"VALID\".\nThe type of padding algorithm to use.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.compat.v1.nn.dropout": {"description": "Computes dropout. (deprecated arguments)", "Args": {"x": "A floating point tensor.", "keep_prob": "(deprecated) A deprecated alias for (1-rate).", "noise_shape": "A 1-D integer Tensor, representing the\nshape for randomly generated keep/drop flags.", "seed": "A Python integer. Used to create random seeds. See\ntf.random.set_seed for behavior.", "name": "A name for this operation (optional).", "rate": "A scalar Tensor with the same type as x. The probability that each\nelement of x is discarded."}, "Returns": "A Tensor of the same shape of x.", "Raises": {"ValueError": "If rate is not in [0, 1) or if x is not a floating\npoint tensor."}}, "tf.compat.v1.nn.dynamic_rnn": {"description": "Creates a recurrent neural network specified by RNNCell cell. (deprecated)", "Args": {"cell": "An instance of RNNCell.", "inputs": "The RNN inputs.\nIf time_major == False (default), this must be a Tensor of shape:\n  [batch_size, max_time, ...], or a nested tuple of such elements.\nIf time_major == True, this must be a Tensor of shape: [max_time,\n  batch_size, ...], or a nested tuple of such elements. This may also be\n  a (possibly nested) tuple of Tensors satisfying this property.  The\n  first two dimensions must match across all the inputs, but otherwise the\n  ranks and other shape components may differ. In this case, input to\n  cell at each time-step will replicate the structure of these tuples,\n  except for the time dimension (from which the time is taken). The input\n  to cell at each time step will be a Tensor or (possibly nested)\n  tuple of Tensors each with dimensions [batch_size, ...].", "sequence_length": "(optional) An int32/int64 vector sized [batch_size]. Used\nto copy-through state and zero-out outputs when past a batch element's\nsequence length.  This parameter enables users to extract the last valid\nstate and properly padded outputs, so it is provided for correctness.", "initial_state": "(optional) An initial state for the RNN. If cell.state_size\nis an integer, this must be a Tensor of appropriate type and shape\n[batch_size, cell.state_size]. If cell.state_size is a tuple, this\nshould be a tuple of tensors having shapes [batch_size, s] for s in\ncell.state_size.", "dtype": "(optional) The data type for the initial state and expected output.\nRequired if initial_state is not provided or RNN state has a heterogeneous\ndtype.", "parallel_iterations": "(Default: 32).  The number of iterations to run in\nparallel.  Those operations which do not have any temporal dependency and\ncan be run in parallel, will be.  This parameter trades off time for\nspace.  Values >> 1 use more memory but take less time, while smaller\nvalues use less memory but computations take longer.", "swap_memory": "Transparently swap the tensors produced in forward inference\nbut needed for back prop from GPU to CPU.  This allows training RNNs which\nwould typically not fit on a single GPU, with very minimal (or no)\nperformance penalty.", "time_major": "The shape format of the inputs and outputs Tensors. If true,\nthese Tensors must be shaped [max_time, batch_size, depth]. If false,\nthese Tensors must be shaped [batch_size, max_time, depth]. Using\ntime_major = True is a bit more efficient because it avoids transposes\nat the beginning and end of the RNN calculation.  However, most TensorFlow\ndata is batch-major, so by default this function accepts input and emits\noutput in batch-major form.", "scope": "VariableScope for the created subgraph; defaults to \"rnn\"."}, "Returns": "A pair (outputs, state) where:", "Raises": {"TypeError": "If cell is not an instance of RNNCell.", "ValueError": "If inputs is None or an empty list."}}, "tf.compat.v1.nn.embedding_lookup": {"description": "Looks up embeddings for the given ids from a list of tensors.", "Args": {"params": "A single tensor representing the complete embedding tensor, or a\nlist of P tensors all of same shape except for the first dimension,\nrepresenting sharded embedding tensors.  Alternatively, a\nPartitionedVariable, created by partitioning along dimension 0. Each\nelement must be appropriately sized for the given partition_strategy.", "ids": "A Tensor or a 'RaggedTensor' with type int32 or int64 containing\nthe ids to be looked up in params.", "partition_strategy": "A string specifying the partitioning strategy, relevant\nif len(params) > 1. Currently \"div\" and \"mod\" are supported. Default\nis \"mod\".", "name": "A name for the operation (optional).", "validate_indices": "DEPRECATED. If this operation is assigned to CPU, values\nin indices are always validated to be within range.  If assigned to GPU,\nout-of-bound indices result in safe but unspecified behavior, which may\ninclude raising an error.", "max_norm": "If not None, each embedding is clipped if its l2-norm is larger\nthan this value."}, "Returns": "A Tensor or a 'RaggedTensor', depending on the input, with the same type\nas the tensors in params.", "Raises": {"ValueError": "If params is empty."}}, "tf.compat.v1.nn.embedding_lookup_sparse": {"description": "Looks up embeddings for the given ids and weights from a list of tensors.", "Args": {"params": "A single tensor representing the complete embedding tensor, or a\nlist tensors all of same shape except for the first dimension,\nrepresenting sharded embedding tensors. Alternatively, a\nPartitionedVariable, created by partitioning along dimension 0. Each\nelement must be appropriately sized for the given partition_strategy.", "sp_ids": "N x M SparseTensor of int64 ids where N is typically batch size\nand M is arbitrary.", "sp_weights": "either a SparseTensor of float / double weights, or None to\nindicate all weights should be taken to be 1. If specified, sp_weights\nmust have exactly the same shape and indices as sp_ids.", "partition_strategy": "A string specifying the partitioning strategy, relevant\nif len(params) > 1. Currently \"div\" and \"mod\" are supported. Default\nis \"mod\". See tf.nn.embedding_lookup for more details.", "name": "Optional name for the op.", "combiner": "A string specifying the reduction op. Currently \"mean\", \"sqrtn\"\nand \"sum\" are supported. \"sum\" computes the weighted sum of the embedding\nresults for each row. \"mean\" is the weighted sum divided by the total\nweight. \"sqrtn\" is the weighted sum divided by the square root of the sum\nof the squares of the weights. Defaults to mean.", "max_norm": "If not None, each embedding is clipped if its l2-norm is larger\nthan this value, before combining."}, "Returns": "A dense tensor representing the combined embeddings for the\nsparse ids. For each row in the dense tensor represented by sp_ids, the op\nlooks up the embeddings for all ids in that row, multiplies them by the\ncorresponding weight, and combines these embeddings as specified.\nIn other words, if\nshape(combined params) = [p0, p1, ..., pm]\nand\nshape(sp_ids) = shape(sp_weights) = [d0, d1]\nthen\nshape(output) = [d0, p1, ..., pm].\nFor instance, if params is a 10x20 matrix, and sp_ids / sp_weights are\n\u00a0 [0, 0]: id 1, weight 2.0\u00a0 [0, 1]: id 3, weight 0.5\u00a0 [1, 0]: id 0, weight 1.0\u00a0 [2, 3]: id 1, weight 3.0\nwith combiner=\"mean\", then the output will be a 3x20 matrix where\n\u00a0 output[0, :] = (params[1, :] * 2.0 + params[3, :] * 0.5) / (2.0 + 0.5)\u00a0 output[1, :] = (params[0, :] * 1.0) / 1.0\u00a0 output[2, :] = (params[1, :] * 3.0) / 3.0", "Raises": {"TypeError": "If sp_ids is not a SparseTensor, or if sp_weights is\nneither None nor SparseTensor.", "ValueError": "If combiner is not one of {\"mean\", \"sqrtn\", \"sum\"}."}}, "tf.compat.v1.nn.erosion2d": {"description": "Computes the grayscale erosion of 4-D value and 3-D kernel tensors.", "Args": {"value": "A Tensor. 4-D with shape [batch, in_height, in_width, depth].", "kernel": "A Tensor. Must have the same type as value.\n3-D with shape [kernel_height, kernel_width, depth].", "strides": "A list of ints that has length >= 4.\n1-D of length 4. The stride of the sliding window for each dimension of\nthe input tensor. Must be: [1, stride_height, stride_width, 1].", "rates": "A list of ints that has length >= 4.\n1-D of length 4. The input stride for atrous morphological dilation.\nMust be: [1, rate_height, rate_width, 1].", "padding": "A string from: \"SAME\", \"VALID\".\nThe type of padding algorithm to use.", "name": "A name for the operation (optional). If not specified \"erosion2d\"\nis used."}, "Returns": "A Tensor. Has the same type as value.\n4-D with shape [batch, out_height, out_width, depth].", "Raises": {"ValueError": "If the value depth does not match kernel' shape, or if\npadding is other than 'VALID' or 'SAME'."}}, "tf.compat.v1.nn.fractional_avg_pool": {"description": "Performs fractional average pooling on the input. (deprecated)", "Args": {"value": "A Tensor. 4-D with shape [batch, height, width, channels].", "pooling_ratio": "A list of floats that has length >= 4.  Pooling ratio for\neach dimension of value, currently only supports row and col dimension\nand should be >= 1.0. For example, a valid pooling ratio looks like [1.0,\n1.44, 1.73, 1.0]. The first and last elements must be 1.0 because we don't\nallow pooling on batch and channels dimensions.  1.44 and 1.73 are pooling\nratio on height and width dimensions respectively.", "pseudo_random": "An optional bool.  Defaults to False. When set to True,\ngenerates the pooling sequence in a pseudorandom fashion, otherwise, in a\nrandom fashion. Check paper (Graham, 2015) for difference between\npseudorandom and random.", "overlapping": "An optional bool.  Defaults to False.  When set to True,\nit means when pooling, the values at the boundary of adjacent pooling\ncells are used by both cells. For example:\nindex  0  1  2  3  4\nvalue  20 5  16 3  7\nIf the pooling sequence is [0, 2, 4], then 16, at index 2 will be used\ntwice.  The result would be [20, 16] for fractional avg pooling.", "deterministic": "An optional bool.  Deprecated; use fractional_avg_pool_v2\ninstead.", "seed": "An optional int.  Defaults to 0.  If set to be non-zero, the\nrandom number generator is seeded by the given seed.  Otherwise it is\nseeded by a random seed.", "seed2": "An optional int.  Deprecated; use fractional_avg_pool_v2 instead.", "name": "A name for the operation (optional)."}}, "tf.compat.v1.nn.fractional_max_pool": {"description": "Performs fractional max pooling on the input. (deprecated)", "Args": {"value": "A Tensor. 4-D with shape [batch, height, width, channels].", "pooling_ratio": "A list of floats that has length >= 4.  Pooling ratio for\neach dimension of value, currently only supports row and col dimension\nand should be >= 1.0. For example, a valid pooling ratio looks like [1.0,\n1.44, 1.73, 1.0]. The first and last elements must be 1.0 because we don't\nallow pooling on batch and channels dimensions.  1.44 and 1.73 are pooling\nratio on height and width dimensions respectively.", "pseudo_random": "An optional bool.  Defaults to False. When set to True,\ngenerates the pooling sequence in a pseudorandom fashion, otherwise, in a\nrandom fashion. Check (Graham, 2015) for difference between\npseudorandom and random.", "overlapping": "An optional bool.  Defaults to False.  When set to True,\nit means when pooling, the values at the boundary of adjacent pooling\ncells are used by both cells. For example:\nindex  0  1  2  3  4\nvalue  20 5  16 3  7\nIf the pooling sequence is [0, 2, 4], then 16, at index 2 will be used\ntwice.  The result would be [20, 16] for fractional max pooling.", "deterministic": "An optional bool.  Deprecated; use fractional_max_pool_v2\ninstead.", "seed": "An optional int.  Defaults to 0.  If set to be non-zero, the\nrandom number generator is seeded by the given seed.  Otherwise it is\nseeded by a random seed.", "seed2": "An optional int.  Deprecated; use fractional_max_pool_v2 instead.", "name": "A name for the operation (optional)."}, "Raises": {"ValueError": "If op determinism is enabled and either the seeds are not set or\nthe \"deterministic\" argument is False."}}, "tf.compat.v1.nn.fused_batch_norm": {"description": "Batch normalization.", "Args": {"x": "Input Tensor of 4 or 5 dimensions.", "scale": "A Tensor of 1 dimension for scaling.", "offset": "A Tensor of 1 dimension for bias.", "mean": "A Tensor of 1 dimension for population mean. The shape and meaning\nof this argument depends on the value of is_training and\nexponential_avg_factor as follows:\nis_trainingFalse (inference):\n  Mean must be a Tensor of the same shape as scale containing the\n  estimated population mean computed during training.\nis_trainingTrue and exponential_avg_factor == 1.0:\n  Mean must be None.\nis_trainingTrue and exponential_avg_factor != 1.0:\n  Mean must be a Tensor of the same shape as scale containing the\n  exponential running mean.", "variance": "A Tensor of 1 dimension for population variance. The shape and\nmeaning of this argument depends on the value of is_training and\nexponential_avg_factor as follows:\nis_trainingFalse (inference):\n  Variance must be a Tensor of the same shape as scale containing\n  the estimated population variance computed during training.\nis_training==True and exponential_avg_factor == 1.0:\n  Variance must be None.\nis_training==True and exponential_avg_factor != 1.0:\n  Variance must be a Tensor of the same shape as scale containing\n  the exponential running variance.", "epsilon": "A small float number added to the variance of x.", "data_format": "The data format for x. Support \"NHWC\" (default) or \"NCHW\" for\n4D tenors and \"NDHWC\" or \"NCDHW\" for 5D tensors.", "is_training": "A bool value to specify if the operation is used for\ntraining or inference.", "name": "A name for this operation (optional).", "exponential_avg_factor": "A float number (usually between 0 and 1) used\nfor controlling the decay of the running\npopulation average of mean and variance.\nIf set to 1.0, the current batch average is\nreturned."}, "Returns": "y\n\n\nA 4D or 5D Tensor for the normalized, scaled, offsetted x."}, "tf.compat.v1.nn.max_pool": {"description": "Performs the max pooling on the input.", "Args": {"value": "A 4-D Tensor of the format specified by data_format.", "ksize": "An int or list of ints that has length 1, 2 or 4.\nThe size of the window for each dimension of the input tensor.", "strides": "An int or list of ints that has length 1, 2 or 4.\nThe stride of the sliding window for each dimension of the input tensor.", "padding": "Either the string \"SAME\" or \"VALID\" indicating the type of\npadding algorithm to use, or a list indicating the explicit paddings at\nthe start and end of each dimension. When explicit padding is used and\ndata_format is \"NHWC\", this should be in the form [[0, 0], [pad_top,\npad_bottom], [pad_left, pad_right], [0, 0]]. When explicit padding used\nand data_format is \"NCHW\", this should be in the form [[0, 0], [0, 0],\n[pad_top, pad_bottom], [pad_left, pad_right]]. When using explicit\npadding, the size of the paddings cannot be greater than the sliding\nwindow size.", "data_format": "A string. 'NHWC', 'NCHW' and 'NCHW_VECT_C' are supported.", "name": "Optional name for the operation.", "input": "Alias for value."}, "Returns": "A Tensor of format specified by data_format.\nThe max pooled output tensor."}, "tf.compat.v1.nn.max_pool_with_argmax": {"description": "Performs max pooling on the input and outputs both max values and indices.", "Args": {"input": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.\n4-D with shape [batch, height, width, channels].  Input to pool over.", "ksize": "A list of ints that has length >= 4.\nThe size of the window for each dimension of the input tensor.", "strides": "A list of ints that has length >= 4.\nThe stride of the sliding window for each dimension of the\ninput tensor.", "padding": "A string from: \"SAME\", \"VALID\".\nThe type of padding algorithm to use.", "Targmax": "An optional tf.DType from: tf.int32, tf.int64. Defaults to tf.int64.", "include_batch_in_index": "An optional bool. Defaults to False.\nWhether to include batch dimension in flattened index of argmax.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output, argmax)."}, "tf.compat.v1.nn.moments": {"description": "Calculate the mean and variance of x.", "Args": {"x": "A Tensor.", "axes": "Array of ints.  Axes along which to compute mean and\nvariance.", "shift": "Not used in the current implementation", "name": "Name used to scope the operations that compute the moments.", "keep_dims": "produce moments with the same dimensionality as the input.", "keepdims": "Alias to keep_dims."}, "Returns": "Two Tensor objects: mean and variance."}, "tf.compat.v1.nn.nce_loss": {"description": "Computes and returns the noise-contrastive estimation training loss.", "Args": {"weights": "A Tensor of shape [num_classes, dim], or a list of Tensor\nobjects whose concatenation along dimension 0 has shape\n[num_classes, dim].  The (possibly-partitioned) class embeddings.", "biases": "A Tensor of shape [num_classes].  The class biases.", "labels": "A Tensor of type int64 and shape [batch_size,\nnum_true]. The target classes.", "inputs": "A Tensor of shape [batch_size, dim].  The forward\nactivations of the input network.", "num_sampled": "An int.  The number of negative classes to randomly sample\nper batch. This single sample of negative classes is evaluated for each\nelement in the batch.", "num_classes": "An int. The number of possible classes.", "num_true": "An int.  The number of target classes per training example.", "sampled_values": "a tuple of (sampled_candidates, true_expected_count,\nsampled_expected_count) returned by a *_candidate_sampler function.\n(if None, we default to log_uniform_candidate_sampler)", "remove_accidental_hits": "A bool.  Whether to remove \"accidental hits\"\nwhere a sampled class equals one of the target classes.  If set to\nTrue, this is a \"Sampled Logistic\" loss instead of NCE, and we are\nlearning to generate log-odds instead of log probabilities. See\nour Candidate Sampling Algorithms Reference\n(pdf).\nDefault is False.", "partition_strategy": "A string specifying the partitioning strategy, relevant\nif len(weights) > 1. Currently \"div\" and \"mod\" are supported.\nDefault is \"mod\". See tf.nn.embedding_lookup for more details.", "name": "A name for the operation (optional)."}, "Returns": "A batch_size 1-D tensor of per-example NCE losses."}, "tf.compat.v1.nn.pool": {"description": "Performs an N-D pooling operation.", "Args": {"input": "Tensor of rank N+2, of shape\n[batch_size] + input_spatial_shape + [num_channels] if data_format does\nnot start with \"NC\" (default), or\n[batch_size, num_channels] + input_spatial_shape if data_format starts\nwith \"NC\".  Pooling happens over the spatial dimensions only.", "window_shape": "Sequence of N ints >= 1.", "pooling_type": "Specifies pooling operation, must be \"AVG\" or \"MAX\".", "padding": "The padding algorithm, must be \"SAME\" or \"VALID\".\nSee the \"returns\" section of tf.nn.convolution for details.", "dilation_rate": "Optional.  Dilation rate.  List of N ints >= 1.\nDefaults to [1]*N.  If any value of dilation_rate is > 1, then all\nvalues of strides must be 1.", "strides": "Optional.  Sequence of N ints >= 1.  Defaults to [1]*N.\nIf any value of strides is > 1, then all values of dilation_rate must be\n1.", "name": "Optional. Name of the op.", "data_format": "A string or None.  Specifies whether the channel dimension of\nthe input and output is the last dimension (default, or if data_format\ndoes not start with \"NC\"), or the second dimension (if data_format\nstarts with \"NC\").  For N=1, the valid values are \"NWC\" (default) and\n\"NCW\".  For N=2, the valid values are \"NHWC\" (default) and \"NCHW\".\nFor N=3, the valid values are \"NDHWC\" (default) and \"NCDHW\".", "dilations": "Alias for dilation_rate"}, "Returns": "Tensor of rank N+2, of shape\n  [batch_size] + output_spatial_shape + [num_channels]\nif data_format is None or does not start with \"NC\", or\n[batch_size, num_channels] + output_spatial_shape\nif data_format starts with \"NC\",\nwhere output_spatial_shape depends on the value of padding:\nIf padding = \"SAME\":\n  output_spatial_shape[i] = ceil(input_spatial_shape[i] / strides[i])\nIf padding = \"VALID\":\n  output_spatial_shape[i] =\n    ceil((input_spatial_shape[i] - (window_shape[i] - 1) * dilation_rate[i])\n         / strides[i]).", "Raises": {"ValueError": "if arguments are invalid."}}, "tf.compat.v1.nn.quantized_avg_pool": {"description": "Produces the average pool of the input tensor for quantized types.", "Args": {"input": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.\n4-D with shape [batch, height, width, channels].", "min_input": "A Tensor of type float32.\nThe float value that the lowest quantized input value represents.", "max_input": "A Tensor of type float32.\nThe float value that the highest quantized input value represents.", "ksize": "A list of ints.\nThe size of the window for each dimension of the input tensor.\nThe length must be 4 to match the number of dimensions of the input.", "strides": "A list of ints.\nThe stride of the sliding window for each dimension of the input\ntensor.  The length must be 4 to match the number of dimensions of the input.", "padding": "A string from: \"SAME\", \"VALID\".\nThe type of padding algorithm to use.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output, min_output, max_output)."}, "tf.compat.v1.nn.quantized_conv2d": {"description": "Computes a 2D convolution given quantized 4D input and filter tensors.", "Args": {"input": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.", "filter": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.\nfilter's input_depth dimension must match input's depth dimensions.", "min_input": "A Tensor of type float32.\nThe float value that the lowest quantized input value represents.", "max_input": "A Tensor of type float32.\nThe float value that the highest quantized input value represents.", "min_filter": "A Tensor of type float32.\nThe float value that the lowest quantized filter value represents.", "max_filter": "A Tensor of type float32.\nThe float value that the highest quantized filter value represents.", "strides": "A list of ints.\nThe stride of the sliding window for each dimension of the input\ntensor.", "padding": "A string from: \"SAME\", \"VALID\".\nThe type of padding algorithm to use.", "out_type": "An optional tf.DType from: tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16. Defaults to tf.qint32.", "dilations": "An optional list of ints. Defaults to [1, 1, 1, 1].\n1-D tensor of length 4.  The dilation factor for each dimension of\ninput. If set to k > 1, there will be k-1 skipped cells between each\nfilter element on that dimension. The dimension order is determined by the\nvalue of data_format, see above for details. Dilations in the batch and\ndepth dimensions must be 1.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output, min_output, max_output)."}, "tf.compat.v1.nn.quantized_max_pool": {"description": "Produces the max pool of the input tensor for quantized types.", "Args": {"input": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.\nThe 4D (batch x rows x cols x depth) Tensor to MaxReduce over.", "min_input": "A Tensor of type float32.\nThe float value that the lowest quantized input value represents.", "max_input": "A Tensor of type float32.\nThe float value that the highest quantized input value represents.", "ksize": "A list of ints.\nThe size of the window for each dimension of the input tensor.\nThe length must be 4 to match the number of dimensions of the input.", "strides": "A list of ints.\nThe stride of the sliding window for each dimension of the input\ntensor. The length must be 4 to match the number of dimensions of the input.", "padding": "A string from: \"SAME\", \"VALID\".\nThe type of padding algorithm to use.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output, min_output, max_output)."}, "tf.compat.v1.nn.quantized_relu_x": {"description": "Computes Quantized Rectified Linear X: min(max(features, 0), max_value)", "Args": {"features": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.", "max_value": "A Tensor of type float32.", "min_features": "A Tensor of type float32.\nThe float value that the lowest quantized value represents.", "max_features": "A Tensor of type float32.\nThe float value that the highest quantized value represents.", "out_type": "An optional tf.DType from: tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16. Defaults to tf.quint8.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (activations, min_activations, max_activations)."}, "tf.compat.v1.nn.raw_rnn": {"description": "Creates an RNN specified by RNNCell cell and loop function loop_fn.", "Args": {"cell": "An instance of RNNCell.", "loop_fn": "A callable that takes inputs (time, cell_output, cell_state,\nloop_state) and returns the tuple (finished, next_input,\nnext_cell_state, emit_output, next_loop_state). Here time is an int32\nscalar Tensor, cell_output is a Tensor or (possibly nested) tuple of\ntensors as determined by cell.output_size, and cell_state is a\nTensor or (possibly nested) tuple of tensors, as determined by the\nloop_fn on its first call (and should match cell.state_size).\nThe outputs are: finished, a boolean Tensor of\nshape [batch_size], next_input: the next input to feed to cell,\nnext_cell_state: the next state to feed to cell,\nand emit_output: the output to store for this iteration.  Note that\n  emit_output should be a Tensor or (possibly nested) tuple of tensors\n  which is aggregated in the emit_ta inside the while_loop. For the\n  first call to loop_fn, the emit_output corresponds to the\n  emit_structure which is then used to determine the size of the\n  zero_tensor for the emit_ta (defaults to cell.output_size). For\n  the subsequent calls to the loop_fn, the emit_output corresponds to\n  the actual output tensor that is to be aggregated in the emit_ta. The\n  parameter cell_state and output next_cell_state may be either a\n  single or (possibly nested) tuple of tensors.  The parameter\n  loop_state and output next_loop_state may be either a single or\n  (possibly nested) tuple of Tensor and TensorArray objects.  This\n  last parameter may be ignored by loop_fn and the return value may be\n  None.  If it is not None, then the loop_state will be propagated\n  through the RNN loop, for use purely by loop_fn to keep track of its\n  own state. The next_loop_state parameter returned may be None.  The\n  first call to loop_fn will be time = 0, cell_output = None,\ncell_state = None, and loop_state = None.  For this call: The\n  next_cell_state value should be the value with which to initialize the\n  cell's state.  It may be a final state from a previous RNN or it may be\n  the output of cell.zero_state().  It should be a (possibly nested)\n  tuple structure of tensors. If cell.state_size is an integer, this\n  must be a Tensor of appropriate type and shape [batch_size,\n  cell.state_size]. If cell.state_size is a TensorShape, this must be\n  a Tensor of appropriate type and shape [batch_size] +\n  cell.state_size. If cell.state_size is a (possibly nested) tuple of\n  ints or TensorShape, this will be a tuple having the corresponding\n  shapes. The emit_output value may be either None or a (possibly\n  nested) tuple structure of tensors, e.g., (tf.zeros(shape_0,\n  dtype=dtype_0), tf.zeros(shape_1, dtype=dtype_1)). If this first\n  emit_output return value is None, then the emit_ta result of\n  raw_rnn will have the same structure and dtypes as cell.output_size.\n  Otherwise emit_ta will have the same structure, shapes (prepended with\n  a batch_size dimension), and dtypes as emit_output.  The actual\n  values returned for emit_output at this initializing call are ignored.\n  Note, this emit structure must be consistent across all time steps.", "parallel_iterations": "(Default: 32).  The number of iterations to run in\nparallel.  Those operations which do not have any temporal dependency and\ncan be run in parallel, will be.  This parameter trades off time for\nspace.  Values >> 1 use more memory but take less time, while smaller\nvalues use less memory but computations take longer.", "swap_memory": "Transparently swap the tensors produced in forward inference\nbut needed for back prop from GPU to CPU.  This allows training RNNs which\nwould typically not fit on a single GPU, with very minimal (or no)\nperformance penalty.", "scope": "VariableScope for the created subgraph; defaults to \"rnn\"."}, "Returns": "A tuple (emit_ta, final_state, final_loop_state) where:\nemit_ta: The RNN output TensorArray.\n   If loop_fn returns a (possibly nested) set of Tensors for\n   emit_output during initialization, (inputs time = 0,\n   cell_output = None, and loop_state = None), then emit_ta will\n   have the same structure, dtypes, and shapes as emit_output instead.\n   If loop_fn returns emit_output = None during this call,\n   the structure of cell.output_size is used:\n   If cell.output_size is a (possibly nested) tuple of integers\n   or TensorShape objects, then emit_ta will be a tuple having the\n   same structure as cell.output_size, containing TensorArrays whose\n   elements' shapes correspond to the shape data in cell.output_size.\nfinal_state: The final cell state.  If cell.state_size is an int, this\n  will be shaped [batch_size, cell.state_size].  If it is a\n  TensorShape, this will be shaped [batch_size] + cell.state_size.\n  If it is a (possibly nested) tuple of ints or TensorShape, this will\n  be a tuple having the corresponding shapes.\nfinal_loop_state: The final loop state as returned by loop_fn.", "Raises": {"TypeError": "If cell is not an instance of RNNCell, or loop_fn is not\na callable."}}, "tf.compat.v1.nn.relu_layer": {"description": "Computes Relu(x * weight &#43; biases).", "Args": {"x": "a 2D tensor.  Dimensions typically: batch, in_units", "weights": "a 2D tensor.  Dimensions typically: in_units, out_units", "biases": "a 1D tensor.  Dimensions: out_units", "name": "A name for the operation (optional).  If not specified\n\"nn_relu_layer\" is used."}, "Returns": "A 2-D Tensor computing relu(matmul(x, weights) + biases).\nDimensions typically: batch, out_units."}, "tf.compat.v1.nn.safe_embedding_lookup_sparse": {"description": "Lookup embedding results, accounting for invalid IDs and empty features.", "Args": {"embedding_weights": "A single tensor representing the complete embedding\ntensor, or a list tensors all of same shape except for the first\ndimension, representing sharded embedding tensors. Alternatively, a\nPartitionedVariable, created by partitioning along dimension 0. Each\nelement must be appropriately sized for the given partition_strategy.", "sparse_ids": "SparseTensor of shape [d_0, d_1, ..., d_n] containing the\nids. d_0 is typically batch size.", "sparse_weights": "SparseTensor of same shape as sparse_ids, containing\nfloat weights corresponding to sparse_ids, or None if all weights are\nbe assumed to be 1.0.", "combiner": "A string specifying how to combine embedding results for each\nentry. Currently \"mean\", \"sqrtn\" and \"sum\" are supported, with \"mean\" the\ndefault.", "default_id": "The id to use for an entry with no features.", "name": "A name for this operation (optional).", "partition_strategy": "A string specifying the partitioning strategy. Currently\n\"div\" and \"mod\" are supported. Default is \"div\".", "max_norm": "If not None, all embeddings are l2-normalized to max_norm before\ncombining."}, "Returns": "A dense tensor representing the combined embeddings for the\nsparse ids. For each row in the dense tensor represented by sp_ids, the op\nlooks up the embeddings for all ids in that row, multiplies them by the\ncorresponding weight, and combines these embeddings as specified.\nIn other words, if\nshape(combined embedding_weights) = [p0, p1, ..., pm]\nand\nshape(sparse_ids) = shape(sparse_weights) = [d0, d1, ..., dn]\nthen\nshape(output) = [d0, d1, ... dn-1, p1, ..., pm].\nFor instance, if params is a 10x20 matrix, and sp_ids / sp_weights are\n\u00a0 [0, 0]: id 1, weight 2.0\u00a0 [0, 1]: id 3, weight 0.5\u00a0 [1, 0]: id -1, weight 1.0\u00a0 [2, 3]: id 1, weight 3.0\ndefault_id is 0.\nwith combiner=\"mean\", then the output will be a 3x20 matrix where\n\u00a0 output[0, :] = (params[1, :] * 2.0 + params[3, :] * 0.5) / (2.0 + 0.5)\u00a0 output[1, :] = (params[0, :] * 1.0) / 1.0\u00a0 output[2, :] = (params[1, :] * 3.0) / 3.0", "Raises": {"ValueError": "if embedding_weights is empty."}}, "tf.compat.v1.nn.sampled_softmax_loss": {"description": "Computes and returns the sampled softmax training loss.", "Args": {"weights": "A Tensor of shape [num_classes, dim], or a list of Tensor\nobjects whose concatenation along dimension 0 has shape\n[num_classes, dim].  The (possibly-sharded) class embeddings.", "biases": "A Tensor of shape [num_classes].  The class biases.", "labels": "A Tensor of type int64 and shape [batch_size,\nnum_true]. The target classes.  Note that this format differs from\nthe labels argument of nn.softmax_cross_entropy_with_logits.", "inputs": "A Tensor of shape [batch_size, dim].  The forward\nactivations of the input network.", "num_sampled": "An int.  The number of classes to randomly sample per batch.", "num_classes": "An int. The number of possible classes.", "num_true": "An int.  The number of target classes per training example.", "sampled_values": "a tuple of (sampled_candidates, true_expected_count,\nsampled_expected_count) returned by a *_candidate_sampler function.\n(if None, we default to log_uniform_candidate_sampler)", "remove_accidental_hits": "A bool.  whether to remove \"accidental hits\"\nwhere a sampled class equals one of the target classes.  Default is\nTrue.", "partition_strategy": "A string specifying the partitioning strategy, relevant\nif len(weights) > 1. Currently \"div\" and \"mod\" are supported.\nDefault is \"mod\". See tf.nn.embedding_lookup for more details.", "name": "A name for the operation (optional).", "seed": "random seed for candidate sampling. Default to None, which doesn't set\nthe op-level random seed for candidate sampling."}, "Returns": "A batch_size 1-D tensor of per-example sampled softmax losses."}, "tf.compat.v1.nn.separable_conv2d": {"description": "2-D convolution with separable filters.", "Args": {"input": "4-D Tensor with shape according to data_format.", "depthwise_filter": "4-D Tensor with shape\n[filter_height, filter_width, in_channels, channel_multiplier].\nContains in_channels convolutional filters of depth 1.", "pointwise_filter": "4-D Tensor with shape\n[1, 1, channel_multiplier * in_channels, out_channels].  Pointwise\nfilter to mix channels after depthwise_filter has convolved spatially.", "strides": "1-D of size 4.  The strides for the depthwise convolution for\neach dimension of input.", "padding": "Controls how to pad the image before applying the depthwise\nconvolution. Can be the string \"SAME\" or \"VALID\" indicating the type\nof padding algorithm to use, or a Python list indicating the explicit\npaddings at the start and end of each dimension. When explicit padding is\nused and data_format is \"NHWC\", this should be in the form [[0, 0],\n[pad_top, pad_bottom], [pad_left, pad_right], [0, 0]]. When explicit\npadding used and data_format is \"NCHW\", this should be in the form\n[[0, 0], [0, 0], [pad_top, pad_bottom], [pad_left, pad_right]].", "rate": "1-D of size 2. The dilation rate in which we sample input values\nacross the height and width dimensions in atrous convolution. If it is\ngreater than 1, then all values of strides must be 1.", "name": "A name for this operation (optional).", "data_format": "The data format for input. Either \"NHWC\" (default) or \"NCHW\".", "dilations": "Alias of rate."}, "Returns": "A 4-D Tensor with shape according to 'data_format'. For\nexample, with data_format=\"NHWC\", shape is [batch, out_height,\nout_width, out_channels]."}, "tf.compat.v1.nn.sigmoid_cross_entropy_with_logits": {"description": "Computes sigmoid cross entropy given logits.", "Args": {"labels": "A Tensor of the same type and shape as logits. Between 0 and 1,\ninclusive.", "logits": "A Tensor of type float32 or float64. Any real number.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of the same shape as logits with the componentwise\nlogistic losses.", "Raises": {"ValueError": "If logits and labels do not have the same shape."}}, "tf.compat.v1.nn.softmax_cross_entropy_with_logits": {"description": "Computes softmax cross entropy between logits and labels. (deprecated)", "Args": {"_sentinel": "Used to prevent positional parameters. Internal, do not use.", "labels": "Each vector along the class dimension should hold a valid\nprobability distribution e.g. for the case in which labels are of shape\n[batch_size, num_classes], each row of labels[i] must be a valid\nprobability distribution.", "logits": "Per-label activations, typically a linear output. These activation\nenergies are interpreted as unnormalized log probabilities.", "dim": "The class dimension. Defaulted to -1 which is the last dimension.", "name": "A name for the operation (optional).", "axis": "Alias for dim."}, "Returns": "A Tensor that contains the softmax cross entropy loss. Its type is the\nsame as logits and its shape is the same as labels except that it does\nnot have the last dimension of labels."}, "tf.compat.v1.nn.softmax_cross_entropy_with_logits_v2": {"description": "Computes softmax cross entropy between logits and labels. (deprecated arguments)", "Args": {"labels": "Each vector along the class dimension should hold a valid\nprobability distribution e.g. for the case in which labels are of shape\n[batch_size, num_classes], each row of labels[i] must be a valid\nprobability distribution.", "logits": "Unscaled log probabilities.", "axis": "The class dimension. Defaulted to -1 which is the last dimension.", "name": "A name for the operation (optional).", "dim": "Deprecated alias for axis."}, "Returns": "A Tensor that contains the softmax cross entropy loss. Its type is the\nsame as logits and its shape is the same as labels except that it does\nnot have the last dimension of labels."}, "tf.compat.v1.nn.sparse_softmax_cross_entropy_with_logits": {"description": "Computes sparse softmax cross entropy between logits and labels.", "Args": {"_sentinel": "Used to prevent positional parameters. Internal, do not use.", "labels": "Tensor of shape [d_0, d_1, ..., d_{r-1}] (where r is rank of\nlabels and result) and dtype int32 or int64. Each entry in labels\nmust be an index in [0, num_classes). Other values will raise an\nexception when this op is run on CPU, and return NaN for corresponding\nloss and gradient rows on GPU.", "logits": "Per-label activations (typically a linear output) of shape\n[d_0, d_1, ..., d_{r-1}, num_classes] and dtype float16, float32, or\nfloat64. These activation energies are interpreted as unnormalized log\nprobabilities.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of the same shape as labels and of the same type as logits\nwith the softmax cross entropy loss.", "Raises": {"ValueError": "If logits are scalars (need to have rank >= 1) or if the rank\nof the labels is not equal to the rank of the logits minus one."}}, "tf.compat.v1.nn.static_bidirectional_rnn": {"description": "Creates a bidirectional recurrent neural network. (deprecated)", "Args": {"cell_fw": "An instance of RNNCell, to be used for forward direction.", "cell_bw": "An instance of RNNCell, to be used for backward direction.", "inputs": "A length T list of inputs, each a tensor of shape [batch_size,\ninput_size], or a nested tuple of such elements.", "initial_state_fw": "(optional) An initial state for the forward RNN. This must\nbe a tensor of appropriate type and shape [batch_size,\ncell_fw.state_size]. If cell_fw.state_size is a tuple, this should be a\ntuple of tensors having shapes [batch_size, s] for s in\ncell_fw.state_size.", "initial_state_bw": "(optional) Same as for initial_state_fw, but using the\ncorresponding properties of cell_bw.", "dtype": "(optional) The data type for the initial state.  Required if either\nof the initial states are not provided.", "sequence_length": "(optional) An int32/int64 vector, size [batch_size],\ncontaining the actual lengths for each of the sequences.", "scope": "VariableScope for the created subgraph; defaults to\n\"bidirectional_rnn\""}, "Returns": "A tuple (outputs, output_state_fw, output_state_bw) where:\noutputs is a length T list of outputs (one for each input), which\n  are depth-concatenated forward and backward outputs.\noutput_state_fw is the final state of the forward rnn.\noutput_state_bw is the final state of the backward rnn.", "Raises": {"TypeError": "If cell_fw or cell_bw is not an instance of RNNCell.", "ValueError": "If inputs is None or an empty list."}}, "tf.compat.v1.nn.static_rnn": {"description": "Creates a recurrent neural network specified by RNNCell cell. (deprecated)", "Args": {"cell": "An instance of RNNCell.", "inputs": "A length T list of inputs, each a Tensor of shape [batch_size,\ninput_size], or a nested tuple of such elements.", "initial_state": "(optional) An initial state for the RNN. If cell.state_size\nis an integer, this must be a Tensor of appropriate type and shape\n[batch_size, cell.state_size]. If cell.state_size is a tuple, this\nshould be a tuple of tensors having shapes [batch_size, s] for s in\ncell.state_size.", "dtype": "(optional) The data type for the initial state and expected output.\nRequired if initial_state is not provided or RNN state has a heterogeneous\ndtype.", "sequence_length": "Specifies the length of each sequence in inputs. An int32\nor int64 vector (tensor) size [batch_size], values in [0, T).", "scope": "VariableScope for the created subgraph; defaults to \"rnn\"."}, "Returns": "A pair (outputs, state) where:\n\noutputs is a length T list of outputs (one for each input), or a nested\ntuple of such elements.\nstate is the final state", "Raises": {"TypeError": "If cell is not an instance of RNNCell.", "ValueError": "If inputs is None or an empty list, or if the input depth\n(column size) cannot be inferred from inputs via shape inference."}}, "tf.compat.v1.nn.static_state_saving_rnn": {"description": "RNN that accepts a state saver for time-truncated RNN calculation. (deprecated)", "Args": {"cell": "An instance of RNNCell.", "inputs": "A length T list of inputs, each a Tensor of shape [batch_size,\ninput_size].", "state_saver": "A state saver object with methods state and save_state.", "state_name": "Python string or tuple of strings.  The name to use with the\nstate_saver. If the cell returns tuples of states (i.e., cell.state_size\nis a tuple) then state_name should be a tuple of strings having the same\nlength as cell.state_size.  Otherwise it should be a single string.", "sequence_length": "(optional) An int32/int64 vector size [batch_size]. See the\ndocumentation for rnn() for more details about sequence_length.", "scope": "VariableScope for the created subgraph; defaults to \"rnn\"."}, "Returns": "A pair (outputs, state) where:\noutputs is a length T list of outputs (one for each input)\nstates is the final state", "Raises": {"TypeError": "If cell is not an instance of RNNCell.", "ValueError": "If inputs is None or an empty list, or if the arity and\ntype of state_name does not match that of cell.state_size."}}, "tf.compat.v1.nn.sufficient_statistics": {"description": "Calculate the sufficient statistics for the mean and variance of x.", "Args": {"x": "A Tensor.", "axes": "Array of ints. Axes along which to compute mean and variance. As in\nPython, the axes can also be negative numbers. A negative axis is\ninterpreted as counting from the end of the rank, i.e., axis +\nrank(values)-th dimension.", "shift": "A Tensor containing the value by which to shift the data for\nnumerical stability, or None if no shift is to be performed. A shift\nclose to the true mean provides the most numerically stable results.", "keep_dims": "produce statistics with the same dimensionality as the input.", "name": "Name used to scope the operations that compute the sufficient stats.", "keepdims": "Alias for keep_dims."}, "Returns": "Four Tensor objects of the same type as x:\n\nthe count (number of elements to average over).\nthe (possibly shifted) sum of the elements in the array.\nthe (possibly shifted) sum of squares of the elements in the array.\nthe shift by which the mean must be corrected or None if shift is None."}, "tf.compat.v1.nn.weighted_cross_entropy_with_logits": {"description": "Computes a weighted cross entropy. (deprecated arguments)", "Args": {"labels": "A Tensor of the same type and shape as logits.", "logits": "A Tensor of type float32 or float64.", "pos_weight": "A coefficient to use on the positive examples.", "name": "A name for the operation (optional).", "targets": "Deprecated alias for labels."}, "Returns": "A Tensor of the same shape as logits with the componentwise\nweighted logistic losses.", "Raises": {"ValueError": "If logits and labels do not have the same shape."}}, "tf.compat.v1.nn.weighted_moments": {"description": "Returns the frequency-weighted mean and variance of x.", "Args": {"x": "A tensor.", "axes": "1-d tensor of int32 values; these are the axes along which\nto compute mean and variance.", "frequency_weights": "A tensor of positive weights which can be\nbroadcast with x.", "name": "Name used to scope the operation.", "keep_dims": "Produce moments with the same dimensionality as the input.", "keepdims": "Alias of keep_dims."}, "Returns": "Two tensors: weighted_mean and weighted_variance."}, "tf.compat.v1.nn.xw_plus_b": {"description": "Computes matmul(x, weights) &#43; biases.", "Args": {"x": "a 2D tensor.  Dimensions typically: batch, in_units", "weights": "a 2D tensor.  Dimensions typically: in_units, out_units", "biases": "a 1D tensor.  Dimensions: out_units", "name": "A name for the operation (optional).  If not specified\n\"xw_plus_b\" is used."}, "Returns": "A 2-D Tensor computing matmul(x, weights) + biases.\nDimensions typically: batch, out_units."}}, "tf.compat.v1.nn.rnn_cell": {"tf.compat.v1.nn.rnn_cell.BasicLSTMCell": {"description": "DEPRECATED: Please use tf.compat.v1.nn.rnn_cell.LSTMCell instead.", "Args": {"num_units": "int, The number of units in the LSTM cell.", "forget_bias": "float, The bias added to forget gates (see above). Must set\nto 0.0 manually when restoring from CudnnLSTM-trained checkpoints.", "state_is_tuple": "If True, accepted and returned states are 2-tuples of the\nc_state and m_state.  If False, they are concatenated along the\ncolumn axis.  The latter behavior will soon be deprecated.", "activation": "Activation function of the inner states.  Default: tanh. It\ncould also be string that is within Keras activation function names.", "reuse": "(optional) Python boolean describing whether to reuse variables in\nan existing scope.  If not True, and the existing scope already has\nthe given variables, an error is raised.", "name": "String, the name of the layer. Layers with the same name will share\nweights, but to avoid mistakes we require reuse=True in such cases.", "dtype": "Default dtype of the layer (default of None means use the type of\nthe first input). Required when build is called before call.", "**kwargs": "Dict, keyword named properties for common layer attributes, like\ntrainable etc when constructing the cell from configs of get_config().\nWhen restoring from CudnnLSTM-trained checkpoints, must use\nCudnnCompatibleLSTMCell instead."}, "Attributes": {"graph": "", "output_size": "Integer or TensorShape: size of outputs produced by this cell.", "scope_name": "", "state_size": "size(s) of state(s) used by this cell.\nIt can be represented by an Integer, a TensorShape or a tuple of Integers\nor TensorShapes."}}, "tf.compat.v1.nn.rnn_cell.BasicRNNCell": {"description": "The most basic RNN cell.", "Args": {"num_units": "int, The number of units in the RNN cell.", "activation": "Nonlinearity to use.  Default: tanh. It could also be string\nthat is within Keras activation function names.", "reuse": "(optional) Python boolean describing whether to reuse variables in an\nexisting scope.  If not True, and the existing scope already has the\ngiven variables, an error is raised.", "name": "String, the name of the layer. Layers with the same name will share\nweights, but to avoid mistakes we require reuse=True in such cases.", "dtype": "Default dtype of the layer (default of None means use the type of\nthe first input). Required when build is called before call.", "**kwargs": "Dict, keyword named properties for common layer attributes, like\ntrainable etc when constructing the cell from configs of get_config()."}, "Attributes": {"graph": "", "output_size": "Integer or TensorShape: size of outputs produced by this cell.", "scope_name": "", "state_size": "size(s) of state(s) used by this cell.\nIt can be represented by an Integer, a TensorShape or a tuple of Integers\nor TensorShapes."}}, "tf.compat.v1.nn.rnn_cell.DeviceWrapper": {"description": "Operator that ensures an RNNCell runs on a particular device.", "Args": {"cell": "An instance of RNNCell.", "device": "A device string or function, for passing to tf.device.", "**kwargs": "dict of keyword arguments for base layer."}, "Attributes": {"graph": "", "output_size": "Integer or TensorShape: size of outputs produced by this cell.", "scope_name": "", "state_size": "size(s) of state(s) used by this cell.\nIt can be represented by an Integer, a TensorShape or a tuple of Integers\nor TensorShapes."}}, "tf.compat.v1.nn.rnn_cell.DropoutWrapper": {"description": "Operator adding dropout to inputs and outputs of the given cell.", "Args": {"cell": "an RNNCell, a projection to output_size is added to it.", "input_keep_prob": "unit Tensor or float between 0 and 1, input keep\nprobability; if it is constant and 1, no input dropout will be added.", "output_keep_prob": "unit Tensor or float between 0 and 1, output keep\nprobability; if it is constant and 1, no output dropout will be added.", "state_keep_prob": "unit Tensor or float between 0 and 1, output keep\nprobability; if it is constant and 1, no output dropout will be added.\nState dropout is performed on the outgoing states of the cell. Note\nthe state components to which dropout is applied when state_keep_prob\nis in (0, 1) are also determined by the argument\ndropout_state_filter_visitor (e.g. by default dropout is never applied\nto the c component of an LSTMStateTuple).", "variational_recurrent": "Python bool.  If True, then the same dropout\npattern is applied across all time steps per run call. If this parameter\nis set, input_size must be provided.", "input_size": "(optional) (possibly nested tuple of) TensorShape objects\ncontaining the depth(s) of the input tensors expected to be passed in to\nthe DropoutWrapper.  Required and used iff variational_recurrent\n= True and input_keep_prob < 1.", "dtype": "(optional) The dtype of the input, state, and output tensors.\nRequired and used iff variational_recurrent = True.", "seed": "(optional) integer, the randomness seed.", "dropout_state_filter_visitor": "(optional), default: (see below).  Function\nthat takes any hierarchical level of the state and returns a scalar or\ndepth=1 structure of Python booleans describing which terms in the state\nshould be dropped out.  In addition, if the function returns True,\ndropout is applied across this sublevel.  If the function returns\nFalse, dropout is not applied across this entire sublevel.\nDefault behavior: perform dropout on all terms except the memory (c)\n  state of LSTMCellState objects, and don't try to apply dropout to\nTensorArray objects: def dropout_state_filter_visitor(s):\n  if isinstance(s, LSTMCellState): # Never perform dropout on the c\n    state. return LSTMCellState(c=False, h=True)\n  elif isinstance(s, TensorArray): return False return True", "**kwargs": "dict of keyword arguments for base layer."}, "Raises": {"TypeError": "if cell is not an RNNCell, or keep_state_fn is provided\nbut not callable.", "ValueError": "if any of the keep_probs are not between 0 and 1."}, "Attributes": {"graph": "", "output_size": "Integer or TensorShape: size of outputs produced by this cell.", "scope_name": "", "state_size": "size(s) of state(s) used by this cell.\nIt can be represented by an Integer, a TensorShape or a tuple of Integers\nor TensorShapes.", "wrapped_cell": ""}}, "tf.compat.v1.nn.rnn_cell.GRUCell": {"description": "Gated Recurrent Unit cell.", "Args": {"num_units": "int, The number of units in the GRU cell.", "activation": "Nonlinearity to use.  Default: tanh.", "reuse": "(optional) Python boolean describing whether to reuse variables in an\nexisting scope.  If not True, and the existing scope already has the\ngiven variables, an error is raised.", "kernel_initializer": "(optional) The initializer to use for the weight and\nprojection matrices.", "bias_initializer": "(optional) The initializer to use for the bias.", "name": "String, the name of the layer. Layers with the same name will share\nweights, but to avoid mistakes we require reuse=True in such cases.", "dtype": "Default dtype of the layer (default of None means use the type of\nthe first input). Required when build is called before call.", "**kwargs": "Dict, keyword named properties for common layer attributes, like\n  trainable etc when constructing the cell from configs of get_config().\nReferences:\nLearning Phrase Representations using RNN Encoder Decoder for Statistical\nMachine Translation:\n  Cho et al., 2014\n  (pdf)"}, "Attributes": {"graph": "", "output_size": "Integer or TensorShape: size of outputs produced by this cell.", "scope_name": "", "state_size": "size(s) of state(s) used by this cell.\nIt can be represented by an Integer, a TensorShape or a tuple of Integers\nor TensorShapes."}}, "tf.compat.v1.nn.rnn_cell.LSTMCell": {"description": "Long short-term memory unit (LSTM) recurrent network cell.", "Args": {"num_units": "int, The number of units in the LSTM cell.", "use_peepholes": "bool, set True to enable diagonal/peephole connections.", "cell_clip": "(optional) A float value, if provided the cell state is clipped\nby this value prior to the cell output activation.", "initializer": "(optional) The initializer to use for the weight and\nprojection matrices.", "num_proj": "(optional) int, The output dimensionality for the projection\nmatrices.  If None, no projection is performed.", "proj_clip": "(optional) A float value.  If num_proj > 0 and proj_clip is\nprovided, then the projected values are clipped elementwise to within\n[-proj_clip, proj_clip].", "num_unit_shards": "Deprecated, will be removed by Jan. 2017. Use a\nvariable_scope partitioner instead.", "num_proj_shards": "Deprecated, will be removed by Jan. 2017. Use a\nvariable_scope partitioner instead.", "forget_bias": "Biases of the forget gate are initialized by default to 1 in\norder to reduce the scale of forgetting at the beginning of the\ntraining. Must set it manually to 0.0 when restoring from CudnnLSTM\ntrained checkpoints.", "state_is_tuple": "If True, accepted and returned states are 2-tuples of the\nc_state and m_state.  If False, they are concatenated along the\ncolumn axis.  This latter behavior will soon be deprecated.", "activation": "Activation function of the inner states.  Default: tanh. It\ncould also be string that is within Keras activation function names.", "reuse": "(optional) Python boolean describing whether to reuse variables in\nan existing scope.  If not True, and the existing scope already has\nthe given variables, an error is raised.", "name": "String, the name of the layer. Layers with the same name will share\nweights, but to avoid mistakes we require reuse=True in such cases.", "dtype": "Default dtype of the layer (default of None means use the type of\nthe first input). Required when build is called before call.", "**kwargs": "Dict, keyword named properties for common layer attributes, like\ntrainable etc when constructing the cell from configs of get_config().\nWhen restoring from CudnnLSTM-trained checkpoints, use\nCudnnCompatibleLSTMCell instead."}, "Attributes": {"graph": "", "output_size": "Integer or TensorShape: size of outputs produced by this cell.", "scope_name": "", "state_size": "size(s) of state(s) used by this cell.\nIt can be represented by an Integer, a TensorShape or a tuple of Integers\nor TensorShapes."}}, "tf.compat.v1.nn.rnn_cell.LSTMStateTuple": {"description": "Tuple used by LSTM Cells for state_size, zero_state, and output state.", "Attributes": {"c": "A namedtuple alias for field number 0", "h": "A namedtuple alias for field number 1", "dtype": ""}}, "tf.compat.v1.nn.rnn_cell.MultiRNNCell": {"description": "RNN cell composed sequentially of multiple simple cells.", "Args": {"cells": "list of RNNCells that will be composed in this order.", "state_is_tuple": "If True, accepted and returned states are n-tuples, where\nn = len(cells).  If False, the states are all concatenated along the\ncolumn axis.  This latter behavior will soon be deprecated."}, "Raises": {"ValueError": "if cells is empty (not allowed), or at least one of the cells\nreturns a state tuple but the flag state_is_tuple is False."}, "Attributes": {"graph": "", "output_size": "Integer or TensorShape: size of outputs produced by this cell.", "scope_name": "", "state_size": "size(s) of state(s) used by this cell.\nIt can be represented by an Integer, a TensorShape or a tuple of Integers\nor TensorShapes."}}, "tf.compat.v1.nn.rnn_cell.RNNCell": {"description": "Abstract object representing an RNN cell.", "Attributes": {"graph": "", "output_size": "Integer or TensorShape: size of outputs produced by this cell.", "scope_name": "", "state_size": "size(s) of state(s) used by this cell.\nIt can be represented by an Integer, a TensorShape or a tuple of Integers\nor TensorShapes."}}, "tf.compat.v1.nn.rnn_cell.ResidualWrapper": {"description": "RNNCell wrapper that ensures cell inputs are added to the outputs.", "Args": {"cell": "An instance of RNNCell.", "residual_fn": "(Optional) The function to map raw cell inputs and raw cell\noutputs to the actual cell outputs of the residual network.\nDefaults to calling nest.map_structure on (lambda i, o: i + o), inputs\n  and outputs.", "**kwargs": "dict of keyword arguments for base layer."}, "Attributes": {"graph": "", "output_size": "Integer or TensorShape: size of outputs produced by this cell.", "scope_name": "", "state_size": "size(s) of state(s) used by this cell.\nIt can be represented by an Integer, a TensorShape or a tuple of Integers\nor TensorShapes."}}}, "tf.compat.v1.profiler": {"tf.compat.v1.profiler.AdviceProto": {"description": "A ProtocolMessage"}, "tf.compat.v1.profiler.AdviceProto.Checker": {"description": "A ProtocolMessage"}, "tf.compat.v1.profiler.AdviceProto.CheckersEntry": {"description": "A ProtocolMessage"}, "tf.compat.v1.profiler.GraphNodeProto": {"description": "A ProtocolMessage"}, "tf.compat.v1.profiler.GraphNodeProto.InputShapesEntry": {"description": "A ProtocolMessage"}, "tf.compat.v1.profiler.MultiGraphNodeProto": {"description": "A ProtocolMessage"}, "tf.compat.v1.profiler.OpLogProto": {"description": "A ProtocolMessage"}, "tf.compat.v1.profiler.OpLogProto.IdToStringEntry": {"description": "A ProtocolMessage"}, "tf.compat.v1.profiler.ProfileOptionBuilder": {"description": "Option Builder for Profiling API.", "Args": {"options": "Optional initial option dict to start with."}}, "tf.compat.v1.profiler.Profiler": {"description": "TensorFlow multi-step profiler.", "Args": {"graph": "tf.Graph. If None and eager execution is not enabled, use default\ngraph.", "op_log": "optional. tensorflow::tfprof::OpLogProto proto. Used to define\nextra op types."}}, "tf.compat.v1.profiler.advise": {"description": "Auto profile and advise.", "Args": {"graph": "tf.Graph. If None and eager execution is not enabled, use default\ngraph.", "run_meta": "optional tensorflow.RunMetadata proto. It is necessary to to\nsupport run time information profiling, such as time and memory.", "options": "see ALL_ADVICE example above. Default checks everything."}, "Returns": "Returns AdviceProto proto"}, "tf.compat.v1.profiler.profile": {"description": "Profile model.", "Args": {"graph": "tf.Graph. If None and eager execution is not enabled, use default\ngraph.", "run_meta": "optional tensorflow.RunMetadata proto. It is necessary to to\nsupport run time information profiling, such as time and memory.", "op_log": "tensorflow.tfprof.OpLogProto proto. User can assign \"types\" to graph\nnodes with op_log. \"types\" allow user to flexibly group and account\nprofiles using options['accounted_type_regexes'].", "cmd": "string. Either 'op', 'scope', 'graph' or 'code'. 'op' view organizes\nprofile using operation type. (e.g. MatMul) 'scope' view organizes profile\nusing graph node name scope. 'graph' view organizes profile using graph\nnode inputs/outputs. 'code' view organizes profile using Python call\nstack.", "options": "A dict of options. See core/profiler/g3doc/options.md."}, "Returns": "If cmd is 'scope' or 'graph', returns GraphNodeProto proto.\nIf cmd is 'op' or 'code', returns MultiGraphNodeProto proto.\nSide effect: stdout/file/timeline.json depending on options['output']"}, "tf.compat.v1.profiler.write_op_log": {"description": "Log provided &#39;op_log&#39;, and add additional model information below.", "Args": {"graph": "tf.Graph. If None and eager execution is not enabled, use\ndefault graph.", "log_dir": "directory to write the log file.", "op_log": "(Optional) OpLogProto proto to be written. If not provided, an new\none is created.", "run_meta": "(Optional) RunMetadata proto that helps flops computation using\nrun time shape information.", "add_trace": "Whether to add python code trace information.\nUsed to support \"code\" view."}}}, "tf.compat.v1.python_io": {}, "tf.compat.v1.quantization": {}, "tf.compat.v1.queue": {}, "tf.compat.v1.ragged": {"tf.compat.v1.ragged.RaggedTensorValue": {"description": "Represents the value of a RaggedTensor.", "Args": {"values": "A numpy array of any type and shape; or a RaggedTensorValue.", "row_splits": "A 1-D int32 or int64 numpy array."}, "Attributes": {"dtype": "The numpy dtype of values in this tensor.", "flat_values": "The innermost values array for this ragged tensor value.", "nested_row_splits": "The row_splits for all ragged dimensions in this ragged tensor value.", "ragged_rank": "The number of ragged dimensions in this ragged tensor value.", "row_splits": "The split indices for the ragged tensor value.", "shape": "A tuple indicating the shape of this RaggedTensorValue.", "values": "The concatenated values for all rows in this tensor."}}, "tf.compat.v1.ragged.constant_value": {"description": "Constructs a RaggedTensorValue from a nested Python list.", "Args": {"pylist": "A nested list, tuple or np.ndarray.  Any nested element that\nis not a list or tuple must be a scalar value compatible with dtype.", "dtype": "numpy.dtype.  The type of elements for the returned RaggedTensor.\nIf not specified, then a default is chosen based on the scalar values in\npylist.", "ragged_rank": "An integer specifying the ragged rank of the returned\nRaggedTensorValue.  Must be nonnegative and less than K. Defaults to\nmax(0, K - 1) if inner_shape is not specified.  Defaults to `max(0, K\n\n1 - len(inner_shape))ifinner_shapeis specified.\n</td>\n</tr><tr>\n<td>inner_shape</td>\n<td>\nA tuple of integers specifying the shape for individual inner\nvalues in the returnedRaggedTensorValue.  Defaults to()ifragged_rankis not specified.  Ifragged_rankis specified, then a\ndefault is chosen based on the contents ofpylist.\n</td>\n</tr><tr>\n<td>row_splits_dtype</td>\n<td>\ndata type for the constructedRaggedTensorValue's\nrow_splits.  One ofnumpy.int32ornumpy.int64`."}, "Returns": "A tf.RaggedTensorValue or numpy.array with rank K and the specified\nragged_rank, containing the values from pylist.", "Raises": {"ValueError": "If the scalar values in pylist have inconsistent nesting\ndepth; or if ragged_rank or inner_shape are incompatible with pylist."}}, "tf.compat.v1.ragged.placeholder": {"description": "Creates a placeholder for a tf.RaggedTensor that will always be fed.", "Args": {"dtype": "The data type for the RaggedTensor.", "ragged_rank": "The ragged rank for the RaggedTensor", "value_shape": "The shape for individual flat values in the RaggedTensor.", "name": "A name for the operation (optional)."}, "Returns": "A RaggedTensor that may be used as a handle for feeding a value, but\nnot evaluated directly.", "Raises": {"RuntimeError": "if eager execution is enabled"}}}, "tf.compat.v1.random": {"tf.compat.v1.random.stateless_multinomial": {"description": "Draws deterministic pseudorandom samples from a multinomial distribution. (deprecated)", "Args": {"logits": "2-D Tensor with shape [batch_size, num_classes].  Each slice\n[i, :] represents the unnormalized log-probabilities for all classes.", "num_samples": "0-D.  Number of independent samples to draw for each row slice.", "seed": "A shape [2] Tensor, the seed to the random number generator. Must have\ndtype int32 or int64. (When using XLA, only int32 is allowed.)", "output_dtype": "The integer type of the output: int32 or int64. Defaults\nto int64.", "name": "Optional name for the operation."}, "Returns": "The drawn samples of shape [batch_size, num_samples]."}}, "tf.compat.v1.raw_ops": {}, "tf.compat.v1.resource_loader": {"tf.compat.v1.resource_loader.get_data_files_path": {"description": "Get a direct path to the data files colocated with the script.", "Returns": "The directory where files specified in data attribute of py_test\nand py_binary are stored."}, "tf.compat.v1.resource_loader.get_path_to_datafile": {"description": "Get the path to the specified file in the data dependencies.", "Args": {"path": "a string resource path relative to tensorflow/"}, "Returns": "The path to the specified file present in the data attribute of py_test\nor py_binary.", "Raises": {"IOError": "If the path is not found, or the resource can't be opened."}}, "tf.compat.v1.resource_loader.get_root_dir_with_all_resources": {"description": "Get a root directory containing all the data attributes in the build rule.", "Returns": "The path to the specified file present in the data attribute of py_test\nor py_binary. Falls back to returning the same as get_data_files_path if it\nfails to detect a bazel runfiles directory."}, "tf.compat.v1.resource_loader.load_resource": {"description": "Load the resource at given path, where path is relative to tensorflow/.", "Args": {"path": "a string resource path relative to tensorflow/."}, "Returns": "The contents of that resource.", "Raises": {"IOError": "If the path is not found, or the resource can't be opened."}}, "tf.compat.v1.resource_loader.readahead_file_path": {"description": "Readahead files not implemented; simply returns given path."}}, "tf.compat.v1.saved_model": {"tf.compat.v1.saved_model.Builder": {"description": "Builds the SavedModel protocol buffer and saves variables and assets."}, "tf.compat.v1.saved_model.build_signature_def": {"description": "Utility function to build a SignatureDef protocol buffer.", "Args": {"inputs": "Inputs of the SignatureDef defined as a proto map of string to\ntensor info.", "outputs": "Outputs of the SignatureDef defined as a proto map of string to\ntensor info.", "method_name": "Method name of the SignatureDef as a string."}, "Returns": "A SignatureDef protocol buffer constructed based on the supplied arguments."}, "tf.compat.v1.saved_model.build_tensor_info": {"description": "Utility function to build TensorInfo proto from a Tensor. (deprecated)", "Args": {"tensor": "Tensor or SparseTensor whose name, dtype and shape are used to\nbuild the TensorInfo. For SparseTensors, the names of the three\nconstituent Tensors are used."}, "Returns": "A TensorInfo protocol buffer constructed based on the supplied argument.", "Raises": {"RuntimeError": "If eager execution is enabled."}}, "tf.compat.v1.saved_model.classification_signature_def": {"description": "Creates classification signature from given examples and predictions.", "Args": {"examples": "A string Tensor, expected to accept serialized tf.Examples.", "classes": "A string Tensor.  Note that the ClassificationResponse message\nrequires that class labels are strings, not integers or anything else.", "scores": "a float Tensor."}, "Returns": "A classification-flavored signature_def.", "Raises": {"ValueError": "If examples is None."}}, "tf.compat.v1.saved_model.contains_saved_model": {"description": "Checks whether the provided export directory could contain a SavedModel.", "Args": {"export_dir": "Absolute string path to possible export location. For example,\n'/my/foo/model'."}, "Returns": "True if the export directory contains SavedModel files, False otherwise."}, "tf.compat.v1.saved_model.get_tensor_from_tensor_info": {"description": "Returns the Tensor or CompositeTensor described by a TensorInfo proto. (deprecated)", "Args": {"tensor_info": "A TensorInfo proto describing a Tensor or SparseTensor or\nCompositeTensor.", "graph": "The tf.Graph in which tensors are looked up. If None, the\ncurrent default graph is used.", "import_scope": "If not None, names in tensor_info are prefixed with this\nstring before lookup."}, "Returns": "The Tensor or SparseTensor or CompositeTensor in graph described by\ntensor_info.", "Raises": {"KeyError": "If tensor_info does not correspond to a tensor in graph.", "ValueError": "If tensor_info is malformed."}}, "tf.compat.v1.saved_model.is_valid_signature": {"description": "Determine whether a SignatureDef can be served by TensorFlow Serving."}, "tf.compat.v1.saved_model.load": {"description": "Loads the model from a SavedModel as specified by tags. (deprecated)", "Args": {"sess": "The TensorFlow session to restore the variables.", "tags": "Set of string tags to identify the required MetaGraphDef. These should\ncorrespond to the tags used when saving the variables using the\nSavedModel save() API.", "export_dir": "Directory in which the SavedModel protocol buffer and variables\nto be loaded are located.", "import_scope": "Optional string -- if specified, prepend this string\nfollowed by '/' to all loaded tensor names. This scope is applied to\ntensor instances loaded into the passed session, but it is not written\nthrough to the static MetaGraphDef protocol buffer that is returned.", "**saver_kwargs": "Optional keyword arguments passed through to Saver."}, "Returns": "The MetaGraphDef protocol buffer loaded in the provided session. This\ncan be used to further extract signature-defs, collection-defs, etc.", "Raises": {"RuntimeError": "MetaGraphDef associated with the tags cannot be found."}}, "tf.compat.v1.saved_model.main_op_with_restore": {"description": "Returns a main op to init variables, tables and restore the graph. (deprecated)", "Args": {"restore_op_name": "Name of the op to use to restore the graph."}, "Returns": "The set of ops to be run as part of the main op upon the load operation."}, "tf.compat.v1.saved_model.predict_signature_def": {"description": "Creates prediction signature from given inputs and outputs.", "Args": {"inputs": "dict of string to Tensor.", "outputs": "dict of string to Tensor."}, "Returns": "A prediction-flavored signature_def.", "Raises": {"ValueError": "If inputs or outputs is None."}}, "tf.compat.v1.saved_model.regression_signature_def": {"description": "Creates regression signature from given examples and predictions.", "Args": {"examples": "A string Tensor, expected to accept serialized tf.Examples.", "predictions": "A float Tensor."}, "Returns": "A regression-flavored signature_def.", "Raises": {"ValueError": "If examples is None."}}, "tf.compat.v1.saved_model.simple_save": {"description": "Convenience function to build a SavedModel suitable for serving. (deprecated)", "Args": {"session": "The TensorFlow session from which to save the meta graph and\nvariables.", "export_dir": "The path to which the SavedModel will be stored.", "inputs": "dict mapping string input names to tensors. These are added\nto the SignatureDef as the inputs.", "outputs": "dict mapping string output names to tensors. These are added\nto the SignatureDef as the outputs.", "legacy_init_op": "Legacy support for op or group of ops to execute after the\nrestore op upon a load."}}}, "tf.compat.v1.saved_model.builder": {}, "tf.compat.v1.saved_model.constants": {}, "tf.compat.v1.saved_model.loader": {}, "tf.compat.v1.saved_model.main_op": {"tf.compat.v1.saved_model.main_op.main_op": {"description": "Returns a main op to init variables and tables. (deprecated)", "Returns": "The set of ops to be run as part of the main op upon the load operation."}}, "tf.compat.v1.saved_model.signature_constants": {}, "tf.compat.v1.saved_model.signature_def_utils": {"tf.compat.v1.saved_model.signature_def_utils.MethodNameUpdater": {"description": "Updates the method name(s) of the SavedModel stored in the given path.", "Args": {"export_dir": "Directory containing the SavedModel files."}, "Raises": {"IOError": "If the saved model file does not exist, or cannot be successfully\nparsed."}}}, "tf.compat.v1.saved_model.tag_constants": {}, "tf.compat.v1.saved_model.utils": {}, "tf.compat.v1.sets": {}, "tf.compat.v1.signal": {}, "tf.compat.v1.sparse": {}, "tf.compat.v1.spectral": {}, "tf.compat.v1.strings": {"tf.compat.v1.strings.length": {"description": "Computes the length of each string given in the input tensor.", "Args": {"input": "A Tensor of type string. The strings for which to compute the\nlength for each element.", "name": "A name for the operation (optional).", "unit": "An optional string from: \"BYTE\", \"UTF8_CHAR\". Defaults to\n\"BYTE\". The unit that is counted to compute string length.  One of:\n  \"BYTE\" (for the number of bytes in each string) or \"UTF8_CHAR\" (for\n  the number of UTF-8 encoded Unicode code points in each string). Results\n  are undefined if unit=UTF8_CHAR and the input strings do not contain\n  structurally valid UTF-8."}, "Returns": "A Tensor of type int32, containing the length of the input string in\nthe same element of the input tensor."}, "tf.compat.v1.strings.split": {"description": "Split elements of input based on sep.", "Args": {"input": "A string Tensor of rank N, the strings to split.  If\nrank(input) is not known statically, then it is assumed to be 1.", "sep": "0-D string Tensor, the delimiter character.", "maxsplit": "An int. If maxsplit > 0, limit of the split of the result.", "result_type": "The tensor type for the result: one of \"RaggedTensor\" or\n\"SparseTensor\".", "source": "alias for \"input\" argument.", "name": "A name for the operation (optional)."}, "Raises": {"ValueError": "If sep is not a string."}, "Returns": "A SparseTensor or RaggedTensor of rank N+1, the strings split\naccording to the delimiter."}, "tf.compat.v1.strings.substr": {"description": "Return substrings from Tensor of strings.", "Raises": {}, "Args": {"input": "A Tensor of type string. Tensor of strings", "pos": "A Tensor. Must be one of the following types: int32, int64.\nScalar defining the position of first character in each substring", "len": "A Tensor. Must have the same type as pos.\nScalar defining the number of characters to include in each substring", "unit": "An optional string from: \"BYTE\", \"UTF8_CHAR\". Defaults to \"BYTE\".\nThe unit that is used to create the substring.  One of: \"BYTE\" (for\ndefining position and length by bytes) or \"UTF8_CHAR\" (for the UTF-8\nencoded Unicode code points).  The default is \"BYTE\". Results are undefined if\nunit=UTF8_CHAR and the input strings do not contain structurally valid\nUTF-8.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}}, "tf.compat.v1.summary": {"tf.compat.v1.summary.FileWriter": {"description": "Writes Summary protocol buffers to event files.", "Args": {"logdir": "A string. Directory where event file will be written.", "graph": "A Graph object, such as sess.graph.", "max_queue": "Integer. Size of the queue for pending events and summaries.", "flush_secs": "Number. How often, in seconds, to flush the\npending events and summaries to disk.", "graph_def": "DEPRECATED: Use the graph argument instead.", "filename_suffix": "A string. Every event file's name is suffixed with\nsuffix.", "session": "A tf.compat.v1.Session object. See details above."}, "Raises": {"RuntimeError": "If called with eager execution enabled."}}, "tf.compat.v1.summary.FileWriterCache": {"description": "Cache for file writers."}, "tf.compat.v1.summary.SummaryDescription": {"description": "A ProtocolMessage"}, "tf.compat.v1.summary.TaggedRunMetadata": {"description": "A ProtocolMessage"}, "tf.compat.v1.summary.all_v2_summary_ops": {"description": "Returns all V2-style summary ops defined in the current default graph.", "Returns": "List of summary ops, or None if called under eager execution."}, "tf.compat.v1.summary.audio": {"description": "Outputs a Summary protocol buffer with audio.", "Args": {"name": "A name for the generated node. Will also serve as a series name in\nTensorBoard.", "tensor": "A 3-D float32 Tensor of shape [batch_size, frames, channels]\nor a 2-D float32 Tensor of shape [batch_size, frames].", "sample_rate": "A Scalar float32 Tensor indicating the sample rate of the\nsignal in hertz.", "max_outputs": "Max number of batch elements to generate audio for.", "collections": "Optional list of ops.GraphKeys.  The collections to add the\nsummary to.  Defaults to [_ops.GraphKeys.SUMMARIES]", "family": "Optional; if provided, used as the prefix of the summary tag name,\nwhich controls the tab name used for display on Tensorboard."}, "Returns": "A scalar Tensor of type string. The serialized Summary protocol\nbuffer."}, "tf.compat.v1.summary.get_summary_description": {"description": "Given a TensorSummary node_def, retrieve its SummaryDescription.", "Args": {"node_def": "the node_def_pb2.NodeDef of a TensorSummary op"}, "Returns": "a summary_pb2.SummaryDescription", "Raises": {"ValueError": "if the node is not a summary op."}}, "tf.compat.v1.summary.histogram": {"description": "Outputs a Summary protocol buffer with a histogram.", "Args": {"name": "A name for the generated node. Will also serve as a series name in\nTensorBoard.", "values": "A real numeric Tensor. Any shape. Values to use to\nbuild the histogram.", "collections": "Optional list of graph collections keys. The new summary op is\nadded to these collections. Defaults to [GraphKeys.SUMMARIES].", "family": "Optional; if provided, used as the prefix of the summary tag name,\nwhich controls the tab name used for display on Tensorboard."}, "Returns": "A scalar Tensor of type string. The serialized Summary protocol\nbuffer."}, "tf.compat.v1.summary.image": {"description": "Outputs a Summary protocol buffer with images.", "Args": {"name": "A name for the generated node. Will also serve as a series name in\nTensorBoard.", "tensor": "A 4-D uint8 or float32 Tensor of shape [batch_size, height,\nwidth, channels] where channels is 1, 3, or 4.", "max_outputs": "Max number of batch elements to generate images for.", "collections": "Optional list of ops.GraphKeys.  The collections to add the\nsummary to.  Defaults to [_ops.GraphKeys.SUMMARIES]", "family": "Optional; if provided, used as the prefix of the summary tag name,\nwhich controls the tab name used for display on Tensorboard."}, "Returns": "A scalar Tensor of type string. The serialized Summary protocol\nbuffer."}, "tf.compat.v1.summary.initialize": {"description": "Initializes summary writing for graph execution mode.", "Args": {"graph": "A tf.Graph or tf.compat.v1.GraphDef to output to the writer.\nThis function will not write the default graph by default. When\nwriting to an event log file, the associated step will be zero.", "session": "So this method can call tf.Session.run. This defaults\nto tf.compat.v1.get_default_session."}, "Raises": {"RuntimeError": "If  the current thread has no default\ntf.contrib.summary.SummaryWriter.", "ValueError": "If session wasn't passed and no default session."}}, "tf.compat.v1.summary.merge": {"description": "Merges summaries.", "Args": {"inputs": "A list of string Tensor objects containing serialized Summary\nprotocol buffers.", "collections": "Optional list of graph collections keys. The new summary op is\nadded to these collections. Defaults to [].", "name": "A name for the operation (optional)."}, "Returns": "A scalar Tensor of type string. The serialized Summary protocol\nbuffer resulting from the merging.", "Raises": {"RuntimeError": "If called with eager mode enabled."}}, "tf.compat.v1.summary.merge_all": {"description": "Merges all summaries collected in the default graph.", "Args": {"key": "GraphKey used to collect the summaries.  Defaults to\nGraphKeys.SUMMARIES.", "scope": "Optional scope used to filter the summary ops, using re.match.", "name": "A name for the operation (optional)."}, "Returns": "If no summaries were collected, returns None.  Otherwise returns a scalar\nTensor of type string containing the serialized Summary protocol\nbuffer resulting from the merging.", "Raises": {"RuntimeError": "If called with eager execution enabled."}}, "tf.compat.v1.summary.scalar": {"description": "Outputs a Summary protocol buffer containing a single scalar value.", "Args": {"name": "A name for the generated node. Will also serve as the series name in\nTensorBoard.", "tensor": "A real numeric Tensor containing a single value.", "collections": "Optional list of graph collections keys. The new summary op is\nadded to these collections. Defaults to [GraphKeys.SUMMARIES].", "family": "Optional; if provided, used as the prefix of the summary tag name,\nwhich controls the tab name used for display on Tensorboard."}, "Returns": "A scalar Tensor of type string. Which contains a Summary protobuf.", "Raises": {"ValueError": "If tensor has the wrong shape or type."}}, "tf.compat.v1.summary.tensor_summary": {"description": "Outputs a Summary protocol buffer with a serialized tensor.proto.", "Args": {"name": "A name for the generated node. If display_name is not set, it will\nalso serve as the tag name in TensorBoard. (In that case, the tag\nname will inherit tf name scopes.)", "tensor": "A tensor of any type and shape to serialize.", "summary_description": "A long description of the summary sequence. Markdown\nis supported.", "collections": "Optional list of graph collections keys. The new summary op is\nadded to these collections. Defaults to [GraphKeys.SUMMARIES].", "summary_metadata": "Optional SummaryMetadata proto (which describes which\nplugins may use the summary value).", "family": "Optional; if provided, used as the prefix of the summary tag,\nwhich controls the name used for display on TensorBoard when\ndisplay_name is not set.", "display_name": "A string used to name this data in TensorBoard. If this is\nnot set, then the node name will be used instead."}, "Returns": "A scalar Tensor of type string. The serialized Summary protocol\nbuffer."}, "tf.compat.v1.summary.text": {"description": "Summarizes textual data.", "Args": {"name": "A name for the generated node. Will also serve as a series name in\nTensorBoard.", "tensor": "a string-type Tensor to summarize.", "collections": "Optional list of ops.GraphKeys.  The collections to add the\nsummary to.  Defaults to [_ops.GraphKeys.SUMMARIES]"}, "Returns": "A TensorSummary op that is configured so that TensorBoard will recognize\nthat it contains textual data. The TensorSummary is a scalar Tensor of\ntype string which contains Summary protobufs.", "Raises": {"ValueError": "If tensor has the wrong type."}}}, "tf.compat.v1.sysconfig": {}, "tf.compat.v1.test": {"tf.compat.v1.test.StubOutForTesting": {"description": "Support class for stubbing methods out for unit testing."}, "tf.compat.v1.test.assert_equal_graph_def": {"description": "Asserts that two GraphDefs are (mostly) the same.", "Args": {"actual": "The GraphDef we have.", "expected": "The GraphDef we expected.", "checkpoint_v2": "boolean determining whether to ignore randomized attribute\nvalues that appear in V2 checkpoints.", "hash_table_shared_name": "boolean determining whether to ignore randomized\nshared_names that appear in HashTableV2 op defs."}, "Raises": {"AssertionError": "If the GraphDefs do not match.", "TypeError": "If either argument is not a GraphDef."}}, "tf.compat.v1.test.compute_gradient": {"description": "Computes and returns the theoretical and numerical Jacobian. (deprecated)", "Args": {"x": "a tensor or list of tensors", "x_shape": "the dimensions of x as a tuple or an array of ints. If x is a list,\nthen this is the list of shapes.", "y": "a tensor", "y_shape": "the dimensions of y as a tuple or an array of ints.", "x_init_value": "(optional) a numpy array of the same shape as \"x\"\nrepresenting the initial value of x. If x is a list, this should be a list\nof numpy arrays.  If this is none, the function will pick a random tensor\nas the initial value.", "delta": "(optional) the amount of perturbation.", "init_targets": "list of targets to run to initialize model params.", "extra_feed_dict": "dict that allows fixing specified tensor values\nduring the Jacobian calculation."}, "Returns": "Two 2-d numpy arrays representing the theoretical and numerical\nJacobian for dy/dx. Each has \"x_size\" rows and \"y_size\" columns\nwhere \"x_size\" is the number of elements in x and \"y_size\" is the\nnumber of elements in y. If x is a list, returns a list of two numpy arrays."}, "tf.compat.v1.test.compute_gradient_error": {"description": "Computes the gradient error. (deprecated)", "Args": {"x": "a tensor or list of tensors", "x_shape": "the dimensions of x as a tuple or an array of ints. If x is a list,\nthen this is the list of shapes.", "y": "a tensor", "y_shape": "the dimensions of y as a tuple or an array of ints.", "x_init_value": "(optional) a numpy array of the same shape as \"x\"\nrepresenting the initial value of x. If x is a list, this should be a list\nof numpy arrays.  If this is none, the function will pick a random tensor\nas the initial value.", "delta": "(optional) the amount of perturbation.", "init_targets": "list of targets to run to initialize model params.", "extra_feed_dict": "dict that allows fixing specified tensor values\nduring the Jacobian calculation."}, "Returns": "The maximum error in between the two Jacobians."}, "tf.compat.v1.test.get_temp_dir": {"description": "Returns a temporary directory for use during tests.", "Returns": "The temporary directory."}, "tf.compat.v1.test.test_src_dir_path": {"description": "Creates an absolute test srcdir path given a relative path.", "Args": {"relative_path": "a path relative to tensorflow root.\ne.g. \"core/platform\"."}, "Returns": "An absolute path to the linked in runfiles."}}, "tf.compat.v1.tpu": {"tf.compat.v1.tpu.CrossShardOptimizer": {"description": "An optimizer that averages gradients across TPU shards.", "Args": {"opt": "An existing Optimizer to encapsulate.", "reduction": "The reduction to apply to the shard losses.", "name": "Optional name prefix for the operations created when applying\ngradients. Defaults to \"CrossShardOptimizer\".", "group_assignment": "Optional 2d int32 lists with shape\n[num_groups, num_replicas_per_group] which describles how to apply\noptimizer to subgroups."}, "Raises": {"ValueError": "If reduction is not a valid cross-shard reduction."}, "Class Variables": {"GATE_GRAPH": "2", "GATE_NONE": "0", "GATE_OP": "1"}}, "tf.compat.v1.tpu.PaddingSpec": {"description": "Represents the type of padding policies for tpu.replicate."}, "tf.compat.v1.tpu.batch_parallel": {"description": "Shards computation along the batch dimension for parallel execution.", "Args": {"computation": "A Python function that builds a computation to apply to each\nshard of the input.", "inputs": "A list of input tensors or None (equivalent to an empty list). The\n0-th dimension of each Tensor must have size divisible by num_shards.", "num_shards": "The number of shards.", "infeed_queue": "If not None, the InfeedQueue from which to append a tuple\nof arguments as inputs to computation.", "device_assignment": "If not None, a DeviceAssignment describing the\nmapping between logical cores in the computation with physical cores in\nthe TPU topology. Uses a default device assignment if None. The\nDeviceAssignment may be omitted if each shard of the computation uses\nonly one core, and there is either only one shard, or the number of shards\nis equal to the number of cores in the TPU system.", "name": "(Deprecated) Does nothing.", "xla_options": "An instance of tpu.XLAOptions which indicates the options\npassed to XLA compiler. Use None for default options."}, "Returns": "A list of output tensors.", "Raises": {"ValueError": "If num_shards <= 0"}}, "tf.compat.v1.tpu.bfloat16_scope": {"description": "Scope class for bfloat16 variables so that the model uses custom getter."}, "tf.compat.v1.tpu.core": {"description": "Returns the device name for a core in a replicated TPU computation.", "Args": {"num": "the virtual core number within each replica to which operators should\nbe assigned."}, "Returns": "A device name, suitable for passing to tf.device()."}, "tf.compat.v1.tpu.cross_replica_sum": {"description": "Sum the input tensor across replicas according to group_assignment.", "Args": {"x": "The local tensor to the sum.", "group_assignment": "Optional 2d int32 lists with shape [num_groups,\nnum_replicas_per_group]. group_assignment[i] represents the replica ids\nin the ith subgroup.", "name": "Optional op name."}, "Returns": "A Tensor which is summed across replicas."}, "tf.compat.v1.tpu.initialize_system": {"description": "Initializes a distributed TPU system for use with TensorFlow.", "Args": {"embedding_config": "If not None, a TPUEmbeddingConfiguration proto\ndescribing the desired configuration of the hardware embedding lookup\ntables. If embedding_config is None, no hardware embeddings can be used.", "job": "The job (the XXX in TensorFlow device specification /job:XXX) that\ncontains the TPU devices that will be initialized. If job=None it is\nassumed there is only one job in the TensorFlow flock, and an error will\nbe returned if this assumption does not hold.", "compilation_failure_closes_chips": "Set the configuration whether\nwe want to close TPU chips when there is a compilation failure.", "tpu_cancellation_closes_chips": "Set the configuration whether\nwe want to close TPU chips when a TPU execution is cancelled. If the value\nis None, the behavior will be determined by the command line flag\ntpu_cancellation_closes_chips for the TPU worker. WARNING: this argument\nonly applies to TFRT TPU runtime."}, "Returns": "A serialized TopologyProto that describes the TPU system. Note:\nthe topology must be evaluated using Session.run before it can be used."}, "tf.compat.v1.tpu.outside_compilation": {"description": "Builds part of a computation outside any current TPU replicate scope.", "Args": {"computation": "A Python function that builds the computation to\nplace on the host.", "*args": "the positional arguments for the computation.", "**kwargs": "the keyword arguments for the computation."}, "Returns": "The Tensors returned by computation."}, "tf.compat.v1.tpu.replicate": {"description": "Builds a graph operator that runs a replicated TPU computation.", "Args": {"computation": "A Python function that builds the computation to replicate.", "inputs": "A list of lists of input tensors or None (equivalent to\n[[]]), indexed by [replica_num][input_num]. All replicas must\nhave the same number of inputs. Each input can be a nested structure\ncontaining values that are convertible to tensors. Note that passing an\nN-dimension list of compatible values will result in a N-dimension list of\nscalar tensors rather than a single Rank-N tensors. If you need different\nbehavior, convert part of inputs to tensors with tf.convert_to_tensor.", "infeed_queue": "If not None, the InfeedQueue from which to append a tuple\nof arguments as inputs to computation.", "device_assignment": "If not None, a DeviceAssignment describing the\nmapping between logical cores in the computation with physical cores in\nthe TPU topology. Uses a default device assignment if None. The\nDeviceAssignment may be omitted if each replica of the computation uses\nonly one core, and there is either only one replica, or the number of\nreplicas is equal to the number of cores in the TPU system.", "name": "(Deprecated) Does nothing.", "maximum_shapes": "A nested structure of tf.TensorShape representing the shape\nto which the respective component of each input element in each replica\nshould be padded. Any unknown dimensions (e.g.\ntf.compat.v1.Dimension(None) in a tf.TensorShape or -1 in a tensor-like\nobject) will be padded to the maximum size of that dimension over all\nreplicas. The structure of maximum_shapes needs to be the same as\ninputs[0].", "padding_spec": "An enum specified by tpu.PaddingSpec. This describes the\npadding policy when the inputs to tpu.replicate is dynamic.\nOne usage is to enable automatic bucketizing on the inputs by setting the\nvalue to tpu.PaddingSpec.POWER_OF_TWO, which can help to reduce the\nrecompilation in the XLA side.", "xla_options": "An instance of tpu.XLAOptions which indicates the options\npassed to XLA compiler. Use None for default options."}, "Returns": "A list of outputs, indexed by [replica_num] each output can be a nested\nstructure same as what computation() returns with a few exceptions.\nExceptions include:\n1) None output: a NoOp would be returned which control-depends on\n     computation.\n  2) Single value output: A tuple containing the value would be returned.\n  3) Operation-only outputs: a NoOp would be returned which\n     control-depends on computation.", "Raises": {"ValueError": "If the structure of inputs per replica does not match\nthe structure of maximum_shapes."}}, "tf.compat.v1.tpu.rewrite": {"description": "Rewrites computation for execution on a TPU system.", "Args": {"computation": "A Python function that builds a computation to apply to the\ninput. If the function takes n inputs, 'inputs' should be a list of n\ntensors.\ncomputation may return a list of operations and tensors. Tensors must\ncome before operations in the returned list.  The return value of\nrewrite is a list of tensors corresponding to the tensors from the\noutput of computation.\nAll Operations constructed during computation will be executed when\nevaluating any of the returned output tensors, not just the ones returned.", "inputs": "A list of input tensors or None (equivalent to an empty list).\nEach input can be a nested structure containing values that are\nconvertible to tensors. Note that passing an N-dimension list of\ncompatible values will result in a N-dimension list of scalar tensors\nrather than a single Rank-N tensors. If you need different behavior,\nconvert part of inputs to tensors with tf.convert_to_tensor.", "infeed_queue": "If not None, the InfeedQueue from which to append a tuple\nof arguments as inputs to computation.", "device_assignment": "if not None, a DeviceAssignment describing the\nmapping between logical cores in the computation with physical cores in\nthe TPU topology. May be omitted for a single-core computation, in which\ncase the core attached to task 0, TPU device 0 is used.", "name": "(Deprecated) Does nothing.", "xla_options": "An instance of tpu.XLAOptions which indicates the options\npassed to XLA compiler. Use None for default options."}, "Returns": "Same data structure as if computation(*inputs) is called directly with some\nexceptions for correctness. Exceptions include:\n1) None output: a NoOp would be returned which control-depends on\n     computation.\n  2) Single value output: A tuple containing the value would be returned.\n  3) Operation-only outputs: a NoOp would be returned which\n     control-depends on computation."}, "tf.compat.v1.tpu.shard": {"description": "Shards computation for parallel execution.", "Args": {"computation": "A Python function that builds a computation to apply to each\nshard of the input.", "inputs": "A list of input tensors or None (equivalent to an empty list). Each\ninput tensor has a corresponding shard axes, given by input_shard_axes,\nwhich must have size divisible by num_shards.", "num_shards": "The number of shards.", "input_shard_axes": "A list of dimensions along which to shard inputs, or\nNone. None means \"shard all inputs along dimension 0\". If not None,\nthere must be one dimension per input.", "outputs_from_all_shards": "Boolean or list of boolean. For each output, if\nTrue, outputs from all shards are concatenated along the corresponding\noutput_shard_axes entry. Otherwise, each output is taken\nfrom an arbitrary shard. If the argument is a boolean, the argument's\nvalue is used for each output.", "output_shard_axes": "A list of dimensions along which to concatenate the\noutputs of computation, or None. None means \"concatenate all outputs\nalong dimension 0\". If not None, there must be one dimension per output.\nIgnored if outputs_from_all_shards is False.", "infeed_queue": "If not None, the InfeedQueue to use to augment the inputs\nof computation.", "device_assignment": "If not None, a DeviceAssignment describing the\nmapping between logical cores in the computation with physical cores in\nthe TPU topology. Uses a default device assignment if None. The\nDeviceAssignment may be omitted if each shard of the computation uses\nonly one core, and there is either only one shard, or the number of shards\nis equal to the number of cores in the TPU system.", "name": "(Deprecated) Does nothing.", "xla_options": "An instance of tpu.XLAOptions which indicates the options\npassed to XLA compiler. Use None for default options."}, "Returns": "A list of output tensors.", "Raises": {"ValueError": "If len(output_shard_axes) != len(outputs from computation)"}}, "tf.compat.v1.tpu.shutdown_system": {"description": "Shuts down a running a distributed TPU system.", "Args": {"job": "The job (the XXX in TensorFlow device specification /job:XXX) that\ncontains the TPU devices that will be shutdown. If job=None it is\nassumed there is only one job in the TensorFlow flock, and an error will\nbe returned if this assumption does not hold."}}}, "tf.compat.v1.tpu.experimental.embedding": {}, "tf.compat.v1.train": {"tf.compat.v1.train.AdadeltaOptimizer": {"description": "Optimizer that implements the Adadelta algorithm.", "Args": {"learning_rate": "A Tensor or a floating point value. The learning rate.\nTo match the exact form in the original paper use 1.0.", "rho": "A Tensor or a floating point value. The decay rate.", "epsilon": "A Tensor or a floating point value.  A constant epsilon used\nto better conditioning the grad update.", "use_locking": "If True use locks for update operations.", "name": "Optional name prefix for the operations created when applying\ngradients.  Defaults to \"Adadelta\"."}, "Class Variables": {"GATE_GRAPH": "2", "GATE_NONE": "0", "GATE_OP": "1"}}, "tf.compat.v1.train.AdagradDAOptimizer": {"description": "Adagrad Dual Averaging algorithm for sparse linear models.", "Args": {"learning_rate": "A Tensor or a floating point value.  The learning rate.", "global_step": "A Tensor containing the current training step number.", "initial_gradient_squared_accumulator_value": "A floating point value.\nStarting value for the accumulators, must be positive.", "l1_regularization_strength": "A float value, must be greater than or\nequal to zero.", "l2_regularization_strength": "A float value, must be greater than or\nequal to zero.", "use_locking": "If True use locks for update operations.", "name": "Optional name prefix for the operations created when applying\ngradients.  Defaults to \"AdagradDA\"."}, "Raises": {"ValueError": "If the initial_gradient_squared_accumulator_value is\ninvalid."}, "Class Variables": {"GATE_GRAPH": "2", "GATE_NONE": "0", "GATE_OP": "1"}}, "tf.compat.v1.train.AdagradOptimizer": {"description": "Optimizer that implements the Adagrad algorithm.", "Args": {"learning_rate": "A Tensor or a floating point value.  The learning rate.", "initial_accumulator_value": "A floating point value.\nStarting value for the accumulators, must be positive.", "use_locking": "If True use locks for update operations.", "name": "Optional name prefix for the operations created when applying\ngradients.  Defaults to \"Adagrad\"."}, "Raises": {"ValueError": "If the initial_accumulator_value is invalid."}, "Class Variables": {"GATE_GRAPH": "2", "GATE_NONE": "0", "GATE_OP": "1"}}, "tf.compat.v1.train.AdamOptimizer": {"description": "Optimizer that implements the Adam algorithm.", "Args": {"learning_rate": "A Tensor or a floating point value.  The learning rate.", "beta1": "A float value or a constant float tensor. The exponential decay\nrate for the 1st moment estimates.", "beta2": "A float value or a constant float tensor. The exponential decay\nrate for the 2nd moment estimates.", "epsilon": "A small constant for numerical stability. This epsilon is\n\"epsilon hat\" in the Kingma and Ba paper (in the formula just before\nSection 2.1), not the epsilon in Algorithm 1 of the paper.", "use_locking": "If True use locks for update operations.", "name": "Optional name for the operations created when applying gradients.\nDefaults to \"Adam\"."}, "Class Variables": {"GATE_GRAPH": "2", "GATE_NONE": "0", "GATE_OP": "1"}}, "tf.compat.v1.train.Checkpoint": {"description": "Groups trackable objects, saving and restoring them.", "Args": {"**kwargs": "Keyword arguments are set as attributes of this object, and are\nsaved with the checkpoint. Values must be trackable objects."}, "Raises": {"ValueError": "If objects in kwargs are not trackable."}, "Attributes": {"save_counter": "Incremented when save() is called. Used to number\ncheckpoints."}}, "tf.compat.v1.train.ChiefSessionCreator": {"description": "Creates a tf.compat.v1.Session for a chief.", "Args": {"scaffold": "A Scaffold used for gathering or building supportive ops. If\nnot specified a default one is created. It's used to finalize the graph.", "master": "String representation of the TensorFlow master to use.", "config": "ConfigProto proto used to configure the session.", "checkpoint_dir": "A string.  Optional path to a directory where to restore\nvariables.", "checkpoint_filename_with_path": "Full file name path to the checkpoint file."}}, "tf.compat.v1.train.FtrlOptimizer": {"description": "Optimizer that implements the FTRL algorithm.", "Args": {"learning_rate": "A float value or a constant float Tensor.", "learning_rate_power": "A float value, must be less or equal to zero.\nControls how the learning rate decreases during training. Use zero for\na fixed learning rate. See section 3.1 in (McMahan et al., 2013).", "initial_accumulator_value": "The starting value for accumulators.\nOnly zero or positive values are allowed.", "l1_regularization_strength": "A float value, must be greater than or\nequal to zero.", "l2_regularization_strength": "A float value, must be greater than or\nequal to zero.", "use_locking": "If True use locks for update operations.", "name": "Optional name prefix for the operations created when applying\ngradients.  Defaults to \"Ftrl\".", "accum_name": "The suffix for the variable that keeps the gradient squared\naccumulator.  If not present, defaults to name.", "linear_name": "The suffix for the variable that keeps the linear gradient\naccumulator.  If not present, defaults to name + \"1\".", "l2_shrinkage_regularization_strength": "A float value, must be greater than\nor equal to zero. This differs from L2 above in that the L2 above is a\nstabilization penalty, whereas this L2 shrinkage is a magnitude penalty.\nThe FTRL formulation can be written as:\nw{t+1} = argminw(\\hat{g}{1:t}w + L1||w||_1 + L2||w||_2^2), where\n\\hat{g} = g + (2L2_shrinkagew), and g is the gradient of the loss\nfunction w.r.t. the weights w.\nSpecifically, in the absence of L1 regularization, it is equivalent to\nthe following update rule:\nw_{t+1} = w_t - lr_t / (beta + 2L2lr_t) * g_t -\n          2L2_shrinkagelr_t / (beta + 2L2lr_t) * w_t\nwhere lr_t is the learning rate at t.\nWhen input is sparse shrinkage will only happen on the active weights.", "beta": "A float value; corresponds to the beta parameter in the paper."}, "Raises": {"ValueError": "If one of the arguments is invalid."}, "Class Variables": {"GATE_GRAPH": "2", "GATE_NONE": "0", "GATE_OP": "1"}}, "tf.compat.v1.train.GradientDescentOptimizer": {"description": "Optimizer that implements the gradient descent algorithm.", "Args": {"learning_rate": "A Tensor or a floating point value.  The learning\nrate to use.", "use_locking": "If True use locks for update operations.", "name": "Optional name prefix for the operations created when applying\ngradients. Defaults to \"GradientDescent\"."}, "Class Variables": {"GATE_GRAPH": "2", "GATE_NONE": "0", "GATE_OP": "1"}}, "tf.compat.v1.train.LooperThread": {"description": "A thread that runs code repeatedly, optionally on a timer.", "Args": {"coord": "A Coordinator.", "timer_interval_secs": "Time boundaries at which to call Run(), or None\nif it should be called back to back.", "target": "Optional callable object that will be executed in the thread.", "args": "Optional arguments to pass to target when calling it.", "kwargs": "Optional keyword arguments to pass to target when calling it."}, "Raises": {"ValueError": "If one of the arguments is invalid."}, "Attributes": {"daemon": "A boolean value indicating whether this thread is a daemon thread.\nThis must be set before start() is called, otherwise RuntimeError is\nraised. Its initial value is inherited from the creating thread; the\nmain thread is not a daemon thread and therefore all threads created in\nthe main thread default to daemon = False.\nThe entire Python program exits when only daemon threads are left.", "ident": "Thread identifier of this thread or None if it has not been started.\nThis is a nonzero integer. See the get_ident() function. Thread\nidentifiers may be recycled when a thread exits and another thread is\ncreated. The identifier is available even after the thread has exited.", "name": "A string used for identification purposes only.\nIt has no semantics. Multiple threads may be given the same name. The\ninitial name is set by the constructor.", "native_id": "Native integral thread ID of this thread, or None if it has not been started.\nThis is a non-negative integer. See the get_native_id() function.\nThis represents the Thread ID as reported by the kernel."}}, "tf.compat.v1.train.MomentumOptimizer": {"description": "Optimizer that implements the Momentum algorithm.", "Args": {"learning_rate": "A Tensor or a floating point value.  The learning rate.", "momentum": "A Tensor or a floating point value.  The momentum.", "use_locking": "If True use locks for update operations.", "name": "Optional name prefix for the operations created when applying\ngradients.  Defaults to \"Momentum\".", "use_nesterov": "If True use Nesterov Momentum.\nSee (Sutskever et al., 2013).\nThis implementation always computes gradients at the value of the\nvariable(s) passed to the optimizer. Using Nesterov Momentum makes the\nvariable(s) track the values called theta_t + mu*v_t in the paper.\nThis implementation is an approximation of the original formula, valid\nfor high values of momentum. It will compute the \"adjusted gradient\"\nin NAG by assuming that the new gradient will be estimated by the\ncurrent average gradient plus the product of momentum and the change\nin the average gradient."}, "Class Variables": {"GATE_GRAPH": "2", "GATE_NONE": "0", "GATE_OP": "1"}}, "tf.compat.v1.train.MonitoredSession": {"description": "Session-like object that handles initialization, recovery and hooks.", "Args": {"session_creator": "A factory object to create session. Typically a\nChiefSessionCreator which is the default one.", "hooks": "An iterable of `SessionRunHook' objects."}, "Returns": "A MonitoredSession object.", "Attributes": {"graph": "The graph that was launched in this session."}}, "tf.compat.v1.train.MonitoredSession.StepContext": {"description": "Control flow instrument for the step_fn from run_step_fn().", "Args": {"session": "An instance of tf.compat.v1.Session.", "run_with_hooks_fn": "A function for running fetches and hooks."}, "Attributes": {"session": ""}}, "tf.compat.v1.train.MonitoredTrainingSession": {"description": "Creates a MonitoredSession for training.", "Args": {"master": "String the TensorFlow master to use.", "is_chief": "If True, it will take care of initialization and recovery the\nunderlying TensorFlow session. If False, it will wait on a chief to\ninitialize or recover the TensorFlow session.", "checkpoint_dir": "A string.  Optional path to a directory where to restore\nvariables.", "scaffold": "A Scaffold used for gathering or building supportive ops. If not\nspecified, a default one is created. It's used to finalize the graph.", "hooks": "Optional list of SessionRunHook objects.", "chief_only_hooks": "list of SessionRunHook objects. Activate these hooks if\nis_chief==True, ignore otherwise.", "save_checkpoint_secs": "The frequency, in seconds, that a checkpoint is saved\nusing a default checkpoint saver. If both save_checkpoint_steps and\nsave_checkpoint_secs are set to None, then the default checkpoint\nsaver isn't used. If both are provided, then only save_checkpoint_secs\nis used. Default 600.", "save_summaries_steps": "The frequency, in number of global steps, that the\nsummaries are written to disk using a default summary saver. If both\nsave_summaries_steps and save_summaries_secs are set to None, then\nthe default summary saver isn't used. Default 100.", "save_summaries_secs": "The frequency, in secs, that the summaries are written\nto disk using a default summary saver.  If both save_summaries_steps and\nsave_summaries_secs are set to None, then the default summary saver\nisn't used. Default not enabled.", "config": "an instance of tf.compat.v1.ConfigProto proto used to configure\nthe session. It's the config argument of constructor of\ntf.compat.v1.Session.", "stop_grace_period_secs": "Number of seconds given to threads to stop after\nclose() has been called.", "log_step_count_steps": "The frequency, in number of global steps, that the\nglobal step/sec is logged.", "max_wait_secs": "Maximum time workers should wait for the session to become\navailable. This should be kept relatively short to help detect incorrect\ncode, but sometimes may need to be increased if the chief takes a while to\nstart up.", "save_checkpoint_steps": "The frequency, in number of global steps, that a\ncheckpoint is saved using a default checkpoint saver. If both\nsave_checkpoint_steps and save_checkpoint_secs are set to None, then\nthe default checkpoint saver isn't used. If both are provided, then only\nsave_checkpoint_secs is used. Default not enabled.", "summary_dir": "A string.  Optional path to a directory where to save\nsummaries. If None, checkpoint_dir is used instead.", "save_graph_def": "Whether to save the GraphDef and MetaGraphDef to\ncheckpoint_dir. The GraphDef is saved after the session is created as\ngraph.pbtxt. MetaGraphDefs are saved out for every checkpoint as\nmodel.ckpt-*.meta."}, "Returns": "A MonitoredSession object."}, "tf.compat.v1.train.NewCheckpointReader": {"description": "A function that returns a CheckPointReader.", "Args": {"filepattern": "The filename."}, "Returns": "A CheckpointReader object."}, "tf.compat.v1.train.Optimizer": {"description": "Base class for optimizers.", "Args": {"use_locking": "Bool. If True apply use locks to prevent concurrent updates\nto variables.", "name": "A non-empty string.  The name to use for accumulators created\nfor the optimizer."}, "Raises": {"ValueError": "If name is malformed."}, "Class Variables": {"GATE_GRAPH": "2", "GATE_NONE": "0", "GATE_OP": "1"}}, "tf.compat.v1.train.ProximalAdagradOptimizer": {"description": "Optimizer that implements the Proximal Adagrad algorithm.", "Args": {"learning_rate": "A Tensor or a floating point value.  The learning rate.", "initial_accumulator_value": "A floating point value.\nStarting value for the accumulators, must be positive.", "l1_regularization_strength": "A float value, must be greater than or\nequal to zero.", "l2_regularization_strength": "A float value, must be greater than or\nequal to zero.", "use_locking": "If True use locks for update operations.", "name": "Optional name prefix for the operations created when applying\ngradients.  Defaults to \"Adagrad\"."}, "Raises": {"ValueError": "If the initial_accumulator_value is invalid."}, "Class Variables": {"GATE_GRAPH": "2", "GATE_NONE": "0", "GATE_OP": "1"}}, "tf.compat.v1.train.ProximalGradientDescentOptimizer": {"description": "Optimizer that implements the proximal gradient descent algorithm.", "Args": {"learning_rate": "A Tensor or a floating point value.  The learning\nrate to use.", "l1_regularization_strength": "A float value, must be greater than or\nequal to zero.", "l2_regularization_strength": "A float value, must be greater than or\nequal to zero.", "use_locking": "If True use locks for update operations.", "name": "Optional name prefix for the operations created when applying\ngradients. Defaults to \"GradientDescent\"."}, "Class Variables": {"GATE_GRAPH": "2", "GATE_NONE": "0", "GATE_OP": "1"}}, "tf.compat.v1.train.QueueRunner": {"description": "Holds a list of enqueue operations for a queue, each to be run in a thread.", "Args": {"queue": "A Queue.", "enqueue_ops": "List of enqueue ops to run in threads later.", "close_op": "Op to close the queue. Pending enqueue ops are preserved.", "cancel_op": "Op to close the queue and cancel pending enqueue ops.", "queue_closed_exception_types": "Optional tuple of Exception types that\nindicate that the queue has been closed when raised during an enqueue\noperation.  Defaults to (tf.errors.OutOfRangeError,).  Another common\ncase includes (tf.errors.OutOfRangeError, tf.errors.CancelledError),\nwhen some of the enqueue ops may dequeue from other Queues.", "queue_runner_def": "Optional QueueRunnerDef protocol buffer. If specified,\nrecreates the QueueRunner from its contents. queue_runner_def and the\nother arguments are mutually exclusive.", "import_scope": "Optional string. Name scope to add. Only used when\ninitializing from protocol buffer."}, "Raises": {"ValueError": "If queue or enqueue_ops are not provided when not\nrestoring from queue_runner_def.", "RuntimeError": "If eager execution is enabled."}, "Attributes": {"cancel_op": "", "close_op": "", "enqueue_ops": "", "exceptions_raised": "Exceptions raised but not handled by the QueueRunner threads.\nExceptions raised in queue runner threads are handled in one of two ways\ndepending on whether or not a Coordinator was passed to\ncreate_threads():\n\nWith a Coordinator, exceptions are reported to the coordinator and\nforgotten by the QueueRunner.\nWithout a Coordinator, exceptions are captured by the QueueRunner and\nmade available in this exceptions_raised property.", "name": "The string name of the underlying Queue.", "queue": "", "queue_closed_exception_types": ""}}, "tf.compat.v1.train.RMSPropOptimizer": {"description": "Optimizer that implements the RMSProp algorithm (Tielemans et al.", "Args": {"learning_rate": "A Tensor or a floating point value.  The learning rate.", "decay": "Discounting factor for the history/coming gradient", "momentum": "A scalar tensor.", "epsilon": "Small value to avoid zero denominator.", "use_locking": "If True use locks for update operation.", "centered": "If True, gradients are normalized by the estimated variance of\nthe gradient; if False, by the uncentered second moment. Setting this to\nTrue may help with training, but is slightly more expensive in terms of\ncomputation and memory. Defaults to False.", "name": "Optional name prefix for the operations created when applying\ngradients. Defaults to \"RMSProp\"."}, "Class Variables": {"GATE_GRAPH": "2", "GATE_NONE": "0", "GATE_OP": "1"}}, "tf.compat.v1.train.Saver": {"description": "Saves and restores variables.", "Args": {"var_list": "A list of Variable/SaveableObject, or a dictionary mapping\nnames to SaveableObjects. If None, defaults to the list of all\nsaveable objects.", "reshape": "If True, allows restoring parameters from a checkpoint where\nthe variables have a different shape.", "sharded": "If True, shard the checkpoints, one per device.", "max_to_keep": "Maximum number of recent checkpoints to keep. Defaults to 5.", "keep_checkpoint_every_n_hours": "How often to keep checkpoints. Defaults to\n10,000 hours.", "name": "String.  Optional name to use as a prefix when adding operations.", "restore_sequentially": "A Bool, which if true, causes restore of different\nvariables to happen sequentially within each device.  This can lower\nmemory usage when restoring very large models.", "saver_def": "Optional SaverDef proto to use instead of running the\nbuilder. This is only useful for specialty code that wants to recreate a\nSaver object for a previously built Graph that had a Saver. The\nsaver_def proto should be the one returned by the as_saver_def()\ncall of the Saver that was created for that Graph.", "builder": "Optional SaverBuilder to use if a saver_def was not provided.\nDefaults to BulkSaverBuilder().", "defer_build": "If True, defer adding the save and restore ops to the\nbuild() call. In that case build() should be called before\nfinalizing the graph or using the saver.", "allow_empty": "If False (default) raise an error if there are no variables\nin the graph. Otherwise, construct the saver anyway and make it a no-op.", "write_version": "controls what format to use when saving checkpoints.  It\nalso affects certain filepath matching logic.  The V2 format is the\nrecommended choice: it is much more optimized than V1 in terms of memory\n  required and latency incurred during restore.  Regardless of this\n  flag, the Saver is able to restore from both V2 and V1 checkpoints.", "pad_step_number": "if True, pads the global step number in the checkpoint\nfilepaths to some fixed width (8 by default).  This is turned off by\ndefault.", "save_relative_paths": "If True, will write relative paths to the\ncheckpoint state file. This is needed if the user wants to copy the\ncheckpoint directory and reload from the copied directory.", "filename": "If known at graph construction time, filename used for variable\nloading/saving."}, "Raises": {"TypeError": "If var_list is invalid.", "ValueError": "If any of the keys or values in var_list are not unique.", "RuntimeError": "If eager execution is enabled andvar_list does not specify\na list of variables to save."}, "Attributes": {"last_checkpoints": "List of not-yet-deleted checkpoint filenames.\nYou can pass any of the returned values to restore()."}}, "tf.compat.v1.train.SaverDef": {"description": "A ProtocolMessage", "Class Variables": {"CheckpointFormatVersion": "Instance of google.protobuf.internal.enum_type_wrapper.EnumTypeWrapper", "LEGACY": "0", "V1": "1", "V2": "2"}}, "tf.compat.v1.train.Scaffold": {"description": "Structure to create or gather pieces commonly needed to train a model.", "Args": {"init_op": "Optional op for initializing variables.", "init_feed_dict": "Optional session feed dictionary to use when running the\ninit_op.", "init_fn": "Optional function to use to initialize the model after running\nthe init_op.  Will be called as init_fn(scaffold, session).", "ready_op": "Optional op to verify that the variables are initialized.  Must\nreturn an empty 1D string tensor when the variables are initialized, or\na non-empty 1D string tensor listing the names of the non-initialized\nvariables.", "ready_for_local_init_op": "Optional op to verify that the global variables\nare initialized and local_init_op can be run. Must return an empty 1D\nstring tensor when the global variables are initialized, or a non-empty\n1D string tensor listing the names of the non-initialized global\nvariables.", "local_init_op": "Optional op to initialize local variables.", "summary_op": "Optional op to gather all summaries.  Must return a scalar\nstring tensor containing a serialized Summary proto.", "saver": "Optional tf.compat.v1.train.Saver object to use to save and\nrestore variables.  May also be a tf.train.Checkpoint object, in which\ncase object-based checkpoints are saved. This will also load some\nobject-based checkpoints saved from elsewhere, but that loading may be\nfragile since it uses fixed keys rather than performing a full\ngraph-based match. For example if a variable has two paths from the\nCheckpoint object because two Model objects share the Layer object\nthat owns it, removing one Model may change the keys and break\ncheckpoint loading through this API, whereas a graph-based match would\nmatch the variable through the other Model.", "copy_from_scaffold": "Optional scaffold object to copy fields from. Its\nfields will be overwritten by the provided fields in this function.", "local_init_feed_dict": "Optional session feed dictionary to use when running\nthe local_init_op."}, "Attributes": {"init_feed_dict": "", "init_fn": "", "init_op": "", "local_init_feed_dict": "", "local_init_op": "", "ready_for_local_init_op": "", "ready_op": "", "saver": "", "summary_op": ""}}, "tf.compat.v1.train.SessionCreator": {"description": "A factory for tf.Session."}, "tf.compat.v1.train.SessionManager": {"description": "Training helper that restores from checkpoint and creates session.", "Args": {"local_init_op": "An Operation run immediately after session creation.\nUsually used to initialize tables and local variables.", "ready_op": "An Operation to check if the model is initialized.", "ready_for_local_init_op": "An Operation to check if the model is ready\nto run local_init_op.", "graph": "The Graph that the model will use.", "recovery_wait_secs": "Seconds between checks for the model to be ready.", "local_init_run_options": "RunOptions to be passed to session.run when\nexecuting the local_init_op.", "local_init_feed_dict": "Optional session feed dictionary to use when running\nthe local_init_op."}, "Raises": {"ValueError": "If ready_for_local_init_op is not None but local_init_op is\nNone"}}, "tf.compat.v1.train.SingularMonitoredSession": {"description": "Session-like object that handles initialization, restoring, and hooks.", "Args": {}, "Attributes": {"graph": "The graph that was launched in this session."}}, "tf.compat.v1.train.Supervisor": {"description": "A training helper that checkpoints models and computes summaries.", "Args": {"graph": "A Graph.  The graph that the model will use.  Defaults to the\ndefault Graph.  The supervisor may add operations to the graph before\ncreating a session, but the graph should not be modified by the caller\nafter passing it to the supervisor.", "ready_op": "1-D string Tensor.  This tensor is evaluated by supervisors in\nprepare_or_wait_for_session() to check if the model is ready to use.\nThe model is considered ready if it returns an empty array.  Defaults to\nthe tensor returned from tf.compat.v1.report_uninitialized_variables()\nIf None, the model is not checked for readiness.", "ready_for_local_init_op": "1-D string Tensor.  This tensor is evaluated by\nsupervisors in prepare_or_wait_for_session() to check if the model is\nready to run the local_init_op. The model is considered ready if it\nreturns an empty array. Defaults to None. If None, the model is not\nchecked for readiness before running local_init_op.", "is_chief": "If True, create a chief supervisor in charge of initializing and\nrestoring the model.  If False, create a supervisor that relies on a\nchief supervisor for inits and restore.", "init_op": "Operation.  Used by chief supervisors to initialize the model\nwhen it can not be recovered.  Defaults to an Operation that\ninitializes all global variables.  If None, no initialization is done\nautomatically unless you pass a value for init_fn, see below.", "init_feed_dict": "A dictionary that maps Tensor objects to feed values.\nThis feed dictionary will be used when init_op is evaluated.", "local_init_op": "Operation. Used by all supervisors to run initializations\nthat should run for every new supervisor instance. By default these are\ntable initializers and initializers for local variables. If None, no\nfurther per supervisor-instance initialization is done automatically.", "logdir": "A string.  Optional path to a directory where to checkpoint the\nmodel and log events for the visualizer.  Used by chief supervisors. The\ndirectory will be created if it does not exist.", "summary_op": "An Operation that returns a Summary for the event logs. Used\nby chief supervisors if a logdir was specified.  Defaults to the\noperation returned from summary.merge_all().  If None, summaries are\nnot computed automatically.", "saver": "A Saver object.  Used by chief supervisors if a logdir was\nspecified.  Defaults to the saved returned by Saver(). If None, the\nmodel is not saved automatically.", "global_step": "An integer Tensor of size 1 that counts steps.  The value\nfrom 'global_step' is used in summaries and checkpoint filenames.\nDefault to the op named 'global_step' in the graph if it exists, is of\nrank 1, size 1, and of type tf.int32 or tf.int64.  If None the global\nstep is not recorded in summaries and checkpoint files.  Used by chief\nsupervisors if a logdir was specified.", "save_summaries_secs": "Number of seconds between the computation of\nsummaries for the event log.  Defaults to 120 seconds.  Pass 0 to\ndisable summaries.", "save_model_secs": "Number of seconds between the creation of model\ncheckpoints.  Defaults to 600 seconds.  Pass 0 to disable checkpoints.", "recovery_wait_secs": "Number of seconds between checks that the model is\nready.  Used by supervisors when waiting for a chief supervisor to\ninitialize or restore the model.  Defaults to 30 seconds.", "stop_grace_secs": "Grace period, in seconds, given to running threads to\nstop when stop() is called.  Defaults to 120 seconds.", "checkpoint_basename": "The basename for checkpoint saving.", "session_manager": "SessionManager, which manages Session creation and\nrecovery. If it is None, a default SessionManager will be created\nwith the set of arguments passed in for backwards compatibility.", "summary_writer": "SummaryWriter to use or USE_DEFAULT.  Can be None to\nindicate that no summaries should be written.", "init_fn": "Optional callable used to initialize the model. Called after the\noptional init_op is called.  The callable must accept one argument,\nthe session being initialized.", "local_init_run_options": "RunOptions to be passed as the SessionManager\nlocal_init_run_options parameter."}, "Raises": {"RuntimeError": "If called with eager execution enabled."}, "Attributes": {"coord": "Return the Coordinator used by the Supervisor.\nThe Coordinator can be useful if you want to run multiple threads\nduring your training.", "global_step": "Return the global_step Tensor used by the supervisor.", "init_feed_dict": "Return the feed dictionary used when evaluating the init_op.", "init_op": "Return the Init Op used by the supervisor.", "is_chief": "Return True if this is a chief supervisor.", "ready_for_local_init_op": "", "ready_op": "Return the Ready Op used by the supervisor.", "save_model_secs": "Return the delay between checkpoints.", "save_path": "Return the save path used by the supervisor.", "save_summaries_secs": "Return the delay between summary computations.", "saver": "Return the Saver used by the supervisor.", "session_manager": "Return the SessionManager used by the Supervisor.", "summary_op": "Return the Summary Tensor used by the chief supervisor.", "summary_writer": "Return the SummaryWriter used by the chief supervisor."}, "Class Variables": {"USE_DEFAULT": "0"}}, "tf.compat.v1.train.SyncReplicasOptimizer": {"description": "Class to synchronize, aggregate gradients and pass them to the optimizer.", "Args": {"opt": "The actual optimizer that will be used to compute and apply the\ngradients. Must be one of the Optimizer classes.", "replicas_to_aggregate": "number of replicas to aggregate for each variable\nupdate.", "total_num_replicas": "Total number of tasks/workers/replicas, could be\ndifferent from replicas_to_aggregate.\nIf total_num_replicas > replicas_to_aggregate: it is backup_replicas +\nreplicas_to_aggregate.\nIf total_num_replicas < replicas_to_aggregate: Replicas compute\nmultiple batches per update to variables.", "variable_averages": "Optional ExponentialMovingAverage object, used to\nmaintain moving averages for the variables passed in\nvariables_to_average.", "variables_to_average": "a list of variables that need to be averaged. Only\nneeded if variable_averages is passed in.", "use_locking": "If True use locks for update operation.", "name": "string. Optional name of the returned operation."}, "Class Variables": {"GATE_GRAPH": "2", "GATE_NONE": "0", "GATE_OP": "1"}}, "tf.compat.v1.train.WorkerSessionCreator": {"description": "Creates a tf.compat.v1.Session for a worker.", "Args": {"scaffold": "A Scaffold used for gathering or building supportive ops. If\nnot specified a default one is created. It's used to finalize the graph.", "master": "String representation of the TensorFlow master to use.", "config": "ConfigProto proto used to configure the session.", "max_wait_secs": "Maximum time to wait for the session to become available."}}, "tf.compat.v1.train.add_queue_runner": {"description": "Adds a QueueRunner to a collection in the graph. (deprecated)", "Args": {"qr": "A QueueRunner.", "collection": "A GraphKey specifying the graph collection to add\nthe queue runner to.  Defaults to GraphKeys.QUEUE_RUNNERS."}}, "tf.compat.v1.train.assert_global_step": {"description": "Asserts global_step_tensor is a scalar int Variable or Tensor.", "Args": {"global_step_tensor": "Tensor to test."}}, "tf.compat.v1.train.basic_train_loop": {"description": "Basic loop to train a model.", "Args": {"supervisor": "tf.compat.v1.train.Supervisor to run the training services.", "train_step_fn": "Callable to execute one training step.  Called repeatedly as\ntrain_step_fn(session, *args **kwargs).", "args": "Optional positional arguments passed to train_step_fn.", "kwargs": "Optional keyword arguments passed to train_step_fn.", "master": "Master to use to create the training session.  Defaults to \"\"\nwhich causes the session to be created in the local process."}}, "tf.compat.v1.train.batch": {"description": "Creates batches of tensors in tensors. (deprecated)", "Args": {"tensors": "The list or dictionary of tensors to enqueue.", "batch_size": "The new batch size pulled from the queue.", "num_threads": "The number of threads enqueuing tensors.  The batching will\nbe nondeterministic if num_threads > 1.", "capacity": "An integer. The maximum number of elements in the queue.", "enqueue_many": "Whether each tensor in tensors is a single example.", "shapes": "(Optional) The shapes for each example.  Defaults to the\ninferred shapes for tensors.", "dynamic_pad": "Boolean.  Allow variable dimensions in input shapes.\nThe given dimensions are padded upon dequeue so that tensors within a\nbatch have the same shapes.", "allow_smaller_final_batch": "(Optional) Boolean. If True, allow the final\nbatch to be smaller if there are insufficient items left in the queue.", "shared_name": "(Optional). If set, this queue will be shared under the given\nname across multiple sessions.", "name": "(Optional) A name for the operations."}, "Returns": "A list or dictionary of tensors with the same types as tensors (except if\nthe input is a list of one element, then it returns a tensor, not a list).", "Raises": {"ValueError": "If the shapes are not specified, and cannot be\ninferred from the elements of tensors."}}, "tf.compat.v1.train.batch_join": {"description": "Runs a list of tensors to fill a queue to create batches of examples. (deprecated)", "Args": {"tensors_list": "A list of tuples or dictionaries of tensors to enqueue.", "batch_size": "An integer. The new batch size pulled from the queue.", "capacity": "An integer. The maximum number of elements in the queue.", "enqueue_many": "Whether each tensor in tensor_list_list is a single\nexample.", "shapes": "(Optional) The shapes for each example.  Defaults to the\ninferred shapes for tensor_list_list[i].", "dynamic_pad": "Boolean.  Allow variable dimensions in input shapes.\nThe given dimensions are padded upon dequeue so that tensors within a\nbatch have the same shapes.", "allow_smaller_final_batch": "(Optional) Boolean. If True, allow the final\nbatch to be smaller if there are insufficient items left in the queue.", "shared_name": "(Optional) If set, this queue will be shared under the given\nname across multiple sessions.", "name": "(Optional) A name for the operations."}, "Returns": "A list or dictionary of tensors with the same number and types as\ntensors_list[i].", "Raises": {"ValueError": "If the shapes are not specified, and cannot be\ninferred from the elements of tensor_list_list."}}, "tf.compat.v1.train.checkpoint_exists": {"description": "Checks whether a V1 or V2 checkpoint exists with the specified prefix. (deprecated)", "Args": {"checkpoint_prefix": "the prefix of a V1 or V2 checkpoint, with V2 taking\npriority.  Typically the result of Saver.save() or that of\ntf.train.latest_checkpoint(), regardless of sharded/non-sharded or\nV1/V2."}, "Returns": "A bool, true if a checkpoint referred to by checkpoint_prefix exists."}, "tf.compat.v1.train.cosine_decay": {"description": "Applies cosine decay to the learning rate.", "Args": {"learning_rate": "A scalar float32 or float64 Tensor or a Python number.\nThe initial learning rate.", "global_step": "A scalar int32 or int64 Tensor or a Python number. Global\nstep to use for the decay computation.", "decay_steps": "A scalar int32 or int64 Tensor or a Python number. Number\nof steps to decay over.", "alpha": "A scalar float32 or float64 Tensor or a Python number. Minimum\nlearning rate value as a fraction of learning_rate.", "name": "String. Optional name of the operation.  Defaults to 'CosineDecay'."}, "Returns": "A scalar Tensor of the same type as learning_rate.  The decayed\nlearning rate.", "Raises": {"ValueError": "if global_step is not supplied."}}, "tf.compat.v1.train.cosine_decay_restarts": {"description": "Applies cosine decay with restarts to the learning rate.", "Args": {"learning_rate": "A scalar float32 or float64 Tensor or a Python number.\nThe initial learning rate.", "global_step": "A scalar int32 or int64 Tensor or a Python number. Global\nstep to use for the decay computation.", "first_decay_steps": "A scalar int32 or int64 Tensor or a Python number.\nNumber of steps to decay over.", "t_mul": "A scalar float32 or float64 Tensor or a Python number. Used to\nderive the number of iterations in the i-th period", "m_mul": "A scalar float32 or float64 Tensor or a Python number.\nUsed to derive the initial learning rate of the i-th period:", "alpha": "A scalar float32 or float64 Tensor or a Python number. Minimum\nlearning rate value as a fraction of the learning_rate.", "name": "String. Optional name of the operation.  Defaults to 'SGDRDecay'."}, "Returns": "A scalar Tensor of the same type as learning_rate.  The decayed\nlearning rate.", "Raises": {"ValueError": "if global_step is not supplied."}}, "tf.compat.v1.train.create_global_step": {"description": "Create global step tensor in graph.", "Args": {"graph": "The graph in which to create the global step tensor. If missing, use\ndefault graph."}, "Returns": "Global step tensor.", "Raises": {"ValueError": "if global step tensor is already defined."}}, "tf.compat.v1.train.do_quantize_training_on_graphdef": {"description": "A general quantization scheme is being developed in tf.contrib.quantize. (deprecated)", "Args": {"input_graph": "A GraphDef.", "num_bits": "The number of bits for quantize training."}, "Returns": "The graph with quantize training done."}, "tf.compat.v1.train.exponential_decay": {"description": "Applies exponential decay to the learning rate.", "Args": {"learning_rate": "A scalar float32 or float64 Tensor or a Python number.\nThe initial learning rate.", "global_step": "A scalar int32 or int64 Tensor or a Python number. Global\nstep to use for the decay computation.  Must not be negative.", "decay_steps": "A scalar int32 or int64 Tensor or a Python number. Must\nbe positive.  See the decay computation above.", "decay_rate": "A scalar float32 or float64 Tensor or a Python number.\nThe decay rate.", "staircase": "Boolean.  If True decay the learning rate at discrete intervals", "name": "String.  Optional name of the operation.  Defaults to\n'ExponentialDecay'."}, "Returns": "A scalar Tensor of the same type as learning_rate.  The decayed\nlearning rate.", "Raises": {"ValueError": "if global_step is not supplied."}}, "tf.compat.v1.train.export_meta_graph": {"description": "Returns MetaGraphDef proto.", "Args": {"filename": "Optional filename including the path for writing the generated\nMetaGraphDef protocol buffer.", "meta_info_def": "MetaInfoDef protocol buffer.", "graph_def": "GraphDef protocol buffer.", "saver_def": "SaverDef protocol buffer.", "collection_list": "List of string keys to collect.", "as_text": "If True, writes the MetaGraphDef as an ASCII proto.", "graph": "The Graph to export. If None, use the default graph.", "export_scope": "Optional string. Name scope under which to extract the\nsubgraph. The scope name will be striped from the node definitions for\neasy import later into new name scopes. If None, the whole graph is\nexported. graph_def and export_scope cannot both be specified.", "clear_devices": "Whether or not to clear the device field for an Operation\nor Tensor during export.", "clear_extraneous_savers": "Remove any Saver-related information from the graph\n(both Save/Restore ops and SaverDefs) that are not associated with the\nprovided SaverDef.", "strip_default_attrs": "Boolean. If True, default-valued attributes will be\nremoved from the NodeDefs. For a detailed guide, see\nStripping Default-Valued Attributes.", "save_debug_info": "If True, save the GraphDebugInfo to a separate file,\nwhich in the same directory of filename and with _debug added before the\nfile extend.", "**kwargs": "Optional keyed arguments."}, "Returns": "A MetaGraphDef proto.", "Raises": {"ValueError": "When the GraphDef is larger than 2GB.", "RuntimeError": "If called with eager execution enabled."}}, "tf.compat.v1.train.generate_checkpoint_state_proto": {"description": "Generates a checkpoint state proto.", "Args": {"save_dir": "Directory where the model was saved.", "model_checkpoint_path": "The checkpoint file.", "all_model_checkpoint_paths": "List of strings.  Paths to all not-yet-deleted\ncheckpoints, sorted from oldest to newest.  If this is a non-empty list,\nthe last element must be equal to model_checkpoint_path.  These paths\nare also saved in the CheckpointState proto.", "all_model_checkpoint_timestamps": "A list of floats, indicating the number of\nseconds since the Epoch when each checkpoint was generated.", "last_preserved_timestamp": "A float, indicating the number of seconds since\nthe Epoch when the last preserved checkpoint was written, e.g. due to a\nkeep_checkpoint_every_n_hours parameter (see\ntf.train.CheckpointManager for an implementation)."}, "Returns": "CheckpointState proto with model_checkpoint_path and\nall_model_checkpoint_paths updated to either absolute paths or\nrelative paths to the current save_dir.", "Raises": {"ValueError": "If all_model_checkpoint_timestamps was provided but its length\ndoes not match all_model_checkpoint_paths."}}, "tf.compat.v1.train.get_checkpoint_mtimes": {"description": "Returns the mtimes (modification timestamps) of the checkpoints. (deprecated)", "Args": {"checkpoint_prefixes": "a list of checkpoint paths, typically the results of\nSaver.save() or those of tf.train.latest_checkpoint(), regardless of\nsharded/non-sharded or V1/V2."}, "Returns": "A list of mtimes (in microseconds) of the found checkpoints."}, "tf.compat.v1.train.get_global_step": {"description": "Get the global step tensor.", "Args": {"graph": "The graph to find the global step in. If missing, use default graph."}, "Returns": "The global step variable, or None if none was found.", "Raises": {"TypeError": "If the global step tensor has a non-integer type, or if it is not\na Variable."}}, "tf.compat.v1.train.get_or_create_global_step": {"description": "Returns and create (if necessary) the global step tensor.", "Args": {"graph": "The graph in which to create the global step tensor. If missing, use\ndefault graph."}, "Returns": "The global step tensor."}, "tf.compat.v1.train.global_step": {"description": "Small helper to get the global step.", "Args": {"sess": "A TensorFlow Session object.", "global_step_tensor": "Tensor or the name of the operation that contains\nthe global step."}, "Returns": "The global step value."}, "tf.compat.v1.train.import_meta_graph": {"description": "Recreates a Graph saved in a MetaGraphDef proto.", "Args": {"meta_graph_or_file": "MetaGraphDef protocol buffer or filename (including\nthe path) containing a MetaGraphDef.", "clear_devices": "Whether or not to clear the device field for an Operation\nor Tensor during import.", "import_scope": "Optional string. Name scope to add. Only used when\ninitializing from protocol buffer.", "**kwargs": "Optional keyed arguments."}, "Returns": "A saver constructed from saver_def in MetaGraphDef or None.\nA None value is returned if no variables exist in the MetaGraphDef\n(i.e., there are no variables to restore).", "Raises": {"RuntimeError": "If called with eager execution enabled."}}, "tf.compat.v1.train.init_from_checkpoint": {"description": "Replaces tf.Variable initializers so they load from a checkpoint file.", "Args": {"ckpt_dir_or_file": "Directory with checkpoints file or path to checkpoint.", "assignment_map": "Dict, or a list of key-value pairs, where keys are names\nof the variables in the checkpoint and values are current variables or\nnames of current variables (in default graph)."}, "Raises": {"ValueError": "If missing variables in current graph, or if missing\ncheckpoints or tensors in checkpoints."}}, "tf.compat.v1.train.input_producer": {"description": "Output the rows of input_tensor to a queue for an input pipeline. (deprecated)", "Args": {"input_tensor": "A tensor with the rows to produce. Must be at least\none-dimensional. Must either have a fully-defined shape, or\nelement_shape must be defined.", "element_shape": "(Optional.) A TensorShape representing the shape of a\nrow of input_tensor, if it cannot be inferred.", "num_epochs": "(Optional.) An integer. If specified input_producer produces\neach row of input_tensor num_epochs times before generating an\nOutOfRange error. If not specified, input_producer can cycle through\nthe rows of input_tensor an unlimited number of times.", "shuffle": "(Optional.) A boolean. If true, the rows are randomly shuffled\nwithin each epoch.", "seed": "(Optional.) An integer. The seed to use if shuffle is true.", "capacity": "(Optional.) The capacity of the queue to be used for buffering\nthe input.", "shared_name": "(Optional.) If set, this queue will be shared under the given\nname across multiple sessions.", "summary_name": "(Optional.) If set, a scalar summary for the current queue\nsize will be generated, using this name as part of the tag.", "name": "(Optional.) A name for queue.", "cancel_op": "(Optional.) Cancel op for the queue"}, "Returns": "A queue with the output rows.  A QueueRunner for the queue is\nadded to the current QUEUE_RUNNER collection of the current\ngraph.", "Raises": {"ValueError": "If the shape of the input cannot be inferred from the arguments.", "RuntimeError": "If called with eager execution enabled."}}, "tf.compat.v1.train.inverse_time_decay": {"description": "Applies inverse time decay to the initial learning rate.", "Args": {"learning_rate": "A scalar float32 or float64 Tensor or a Python number.\nThe initial learning rate.", "global_step": "A Python number. Global step to use for the decay computation.\nMust not be negative.", "decay_steps": "How often to apply decay.", "decay_rate": "A Python number.  The decay rate.", "staircase": "Whether to apply decay in a discrete staircase, as opposed to\ncontinuous, fashion.", "name": "String.  Optional name of the operation.  Defaults to\n'InverseTimeDecay'."}, "Returns": "A scalar Tensor of the same type as learning_rate.  The decayed\nlearning rate.", "Raises": {"ValueError": "if global_step is not supplied."}}, "tf.compat.v1.train.limit_epochs": {"description": "Returns tensor num_epochs times and then raises an OutOfRange error. (deprecated)", "Args": {"tensor": "Any Tensor.", "num_epochs": "A positive integer (optional).  If specified, limits the number\nof steps the output tensor may be evaluated.", "name": "A name for the operations (optional)."}, "Returns": "tensor or OutOfRange.", "Raises": {"ValueError": "if num_epochs is invalid."}}, "tf.compat.v1.train.linear_cosine_decay": {"description": "Applies linear cosine decay to the learning rate.", "Args": {"learning_rate": "A scalar float32 or float64 Tensor or a Python number.\nThe initial learning rate.", "global_step": "A scalar int32 or int64 Tensor or a Python number. Global\nstep to use for the decay computation.", "decay_steps": "A scalar int32 or int64 Tensor or a Python number. Number\nof steps to decay over.", "num_periods": "Number of periods in the cosine part of the decay. See\ncomputation above.", "alpha": "See computation above.", "beta": "See computation above.", "name": "String.  Optional name of the operation.  Defaults to\n'LinearCosineDecay'."}, "Returns": "A scalar Tensor of the same type as learning_rate.  The decayed\nlearning rate.", "Raises": {"ValueError": "if global_step is not supplied."}}, "tf.compat.v1.train.maybe_batch": {"description": "Conditionally creates batches of tensors based on keep_input. (deprecated)", "Args": {"tensors": "The list or dictionary of tensors to enqueue.", "keep_input": "A bool Tensor.  This tensor controls whether the input is\nadded to the queue or not.  If it is a scalar and evaluates True, then\ntensors are all added to the queue. If it is a vector and enqueue_many\nis True, then each example is added to the queue only if the\ncorresponding value in keep_input is True. This tensor essentially\nacts as a filtering mechanism.", "batch_size": "The new batch size pulled from the queue.", "num_threads": "The number of threads enqueuing tensors.  The batching will\nbe nondeterministic if num_threads > 1.", "capacity": "An integer. The maximum number of elements in the queue.", "enqueue_many": "Whether each tensor in tensors is a single example.", "shapes": "(Optional) The shapes for each example.  Defaults to the\ninferred shapes for tensors.", "dynamic_pad": "Boolean.  Allow variable dimensions in input shapes.\nThe given dimensions are padded upon dequeue so that tensors within a\nbatch have the same shapes.", "allow_smaller_final_batch": "(Optional) Boolean. If True, allow the final\nbatch to be smaller if there are insufficient items left in the queue.", "shared_name": "(Optional). If set, this queue will be shared under the given\nname across multiple sessions.", "name": "(Optional) A name for the operations."}, "Returns": "A list or dictionary of tensors with the same types as tensors.", "Raises": {"ValueError": "If the shapes are not specified, and cannot be\ninferred from the elements of tensors."}}, "tf.compat.v1.train.maybe_batch_join": {"description": "Runs a list of tensors to conditionally fill a queue to create batches. (deprecated)", "Args": {"tensors_list": "A list of tuples or dictionaries of tensors to enqueue.", "keep_input": "A bool Tensor.  This tensor controls whether the input is\nadded to the queue or not.  If it is a scalar and evaluates True, then\ntensors are all added to the queue. If it is a vector and enqueue_many\nis True, then each example is added to the queue only if the\ncorresponding value in keep_input is True. This tensor essentially\nacts as a filtering mechanism.", "batch_size": "An integer. The new batch size pulled from the queue.", "capacity": "An integer. The maximum number of elements in the queue.", "enqueue_many": "Whether each tensor in tensor_list_list is a single\nexample.", "shapes": "(Optional) The shapes for each example.  Defaults to the\ninferred shapes for tensor_list_list[i].", "dynamic_pad": "Boolean.  Allow variable dimensions in input shapes.\nThe given dimensions are padded upon dequeue so that tensors within a\nbatch have the same shapes.", "allow_smaller_final_batch": "(Optional) Boolean. If True, allow the final\nbatch to be smaller if there are insufficient items left in the queue.", "shared_name": "(Optional) If set, this queue will be shared under the given\nname across multiple sessions.", "name": "(Optional) A name for the operations."}, "Returns": "A list or dictionary of tensors with the same number and types as\ntensors_list[i].", "Raises": {"ValueError": "If the shapes are not specified, and cannot be\ninferred from the elements of tensor_list_list."}}, "tf.compat.v1.train.maybe_shuffle_batch": {"description": "Creates batches by randomly shuffling conditionally-enqueued tensors. (deprecated)", "Args": {"tensors": "The list or dictionary of tensors to enqueue.", "batch_size": "The new batch size pulled from the queue.", "capacity": "An integer. The maximum number of elements in the queue.", "min_after_dequeue": "Minimum number elements in the queue after a\ndequeue, used to ensure a level of mixing of elements.", "keep_input": "A bool Tensor.  This tensor controls whether the input is\nadded to the queue or not.  If it is a scalar and evaluates True, then\ntensors are all added to the queue. If it is a vector and enqueue_many\nis True, then each example is added to the queue only if the\ncorresponding value in keep_input is True. This tensor essentially\nacts as a filtering mechanism.", "num_threads": "The number of threads enqueuing tensor_list.", "seed": "Seed for the random shuffling within the queue.", "enqueue_many": "Whether each tensor in tensor_list is a single example.", "shapes": "(Optional) The shapes for each example.  Defaults to the\ninferred shapes for tensor_list.", "allow_smaller_final_batch": "(Optional) Boolean. If True, allow the final\nbatch to be smaller if there are insufficient items left in the queue.", "shared_name": "(Optional) If set, this queue will be shared under the given\nname across multiple sessions.", "name": "(Optional) A name for the operations."}, "Returns": "A list or dictionary of tensors with the types as tensors.", "Raises": {"ValueError": "If the shapes are not specified, and cannot be\ninferred from the elements of tensors."}}, "tf.compat.v1.train.maybe_shuffle_batch_join": {"description": "Create batches by randomly shuffling conditionally-enqueued tensors. (deprecated)", "Args": {"tensors_list": "A list of tuples or dictionaries of tensors to enqueue.", "batch_size": "An integer. The new batch size pulled from the queue.", "capacity": "An integer. The maximum number of elements in the queue.", "min_after_dequeue": "Minimum number elements in the queue after a\ndequeue, used to ensure a level of mixing of elements.", "keep_input": "A bool Tensor.  This tensor controls whether the input is\nadded to the queue or not.  If it is a scalar and evaluates True, then\ntensors are all added to the queue. If it is a vector and enqueue_many\nis True, then each example is added to the queue only if the\ncorresponding value in keep_input is True. This tensor essentially\nacts as a filtering mechanism.", "seed": "Seed for the random shuffling within the queue.", "enqueue_many": "Whether each tensor in tensor_list_list is a single\nexample.", "shapes": "(Optional) The shapes for each example.  Defaults to the\ninferred shapes for tensors_list[i].", "allow_smaller_final_batch": "(Optional) Boolean. If True, allow the final\nbatch to be smaller if there are insufficient items left in the queue.", "shared_name": "(optional). If set, this queue will be shared under the given\nname across multiple sessions.", "name": "(Optional) A name for the operations."}, "Returns": "A list or dictionary of tensors with the same number and types as\ntensors_list[i].", "Raises": {"ValueError": "If the shapes are not specified, and cannot be\ninferred from the elements of tensors_list."}}, "tf.compat.v1.train.natural_exp_decay": {"description": "Applies natural exponential decay to the initial learning rate.", "Args": {"learning_rate": "A scalar float32 or float64 Tensor or a Python number.\nThe initial learning rate.", "global_step": "A Python number. Global step to use for the decay computation.\nMust not be negative.", "decay_steps": "How often to apply decay.", "decay_rate": "A Python number.  The decay rate.", "staircase": "Whether to apply decay in a discrete staircase, as opposed to\ncontinuous, fashion.", "name": "String.  Optional name of the operation.  Defaults to\n'ExponentialTimeDecay'."}, "Returns": "A scalar Tensor of the same type as learning_rate.  The decayed\nlearning rate.", "Raises": {"ValueError": "if global_step is not supplied."}}, "tf.compat.v1.train.noisy_linear_cosine_decay": {"description": "Applies noisy linear cosine decay to the learning rate.", "Args": {"learning_rate": "A scalar float32 or float64 Tensor or a Python number.\nThe initial learning rate.", "global_step": "A scalar int32 or int64 Tensor or a Python number. Global\nstep to use for the decay computation.", "decay_steps": "A scalar int32 or int64 Tensor or a Python number. Number\nof steps to decay over.", "initial_variance": "initial variance for the noise. See computation above.", "variance_decay": "decay for the noise's variance. See computation above.", "num_periods": "Number of periods in the cosine part of the decay. See\ncomputation above.", "alpha": "See computation above.", "beta": "See computation above.", "name": "String.  Optional name of the operation.  Defaults to\n'NoisyLinearCosineDecay'."}, "Returns": "A scalar Tensor of the same type as learning_rate.  The decayed\nlearning rate.", "Raises": {"ValueError": "if global_step is not supplied."}}, "tf.compat.v1.train.piecewise_constant": {"description": "Piecewise constant from boundaries and interval values.", "Args": {"x": "A 0-D scalar Tensor. Must be one of the following types: float32,\nfloat64, uint8, int8, int16, int32, int64.", "boundaries": "A list of Tensors or ints or floats with strictly\nincreasing entries, and with all elements having the same type as x.", "values": "A list of Tensors or floats or ints that specifies the values\nfor the intervals defined by boundaries. It should have one more element\nthan boundaries, and all elements should have the same type.", "name": "A string. Optional name of the operation. Defaults to\n'PiecewiseConstant'."}, "Returns": "A 0-D Tensor. Its value is values[0] when x <= boundaries[0],\nvalues[1] when x > boundaries[0] and x <= boundaries[1], ...,\nand values[-1] when x > boundaries[-1].", "Raises": {"ValueError": "if types of x and boundaries do not match, or types of all\nvalues do not match or\nthe number of elements in the lists does not match."}}, "tf.compat.v1.train.polynomial_decay": {"description": "Applies a polynomial decay to the learning rate.", "Args": {"learning_rate": "A scalar float32 or float64 Tensor or a Python number.\nThe initial learning rate.", "global_step": "A scalar int32 or int64 Tensor or a Python number. Global\nstep to use for the decay computation.  Must not be negative.", "decay_steps": "A scalar int32 or int64 Tensor or a Python number. Must\nbe positive.  See the decay computation above.", "end_learning_rate": "A scalar float32 or float64 Tensor or a Python\nnumber.  The minimal end learning rate.", "power": "A scalar float32 or float64 Tensor or a Python number.  The\npower of the polynomial. Defaults to linear, 1.0.", "cycle": "A boolean, whether or not it should cycle beyond decay_steps.", "name": "String.  Optional name of the operation. Defaults to\n'PolynomialDecay'."}, "Returns": "A scalar Tensor of the same type as learning_rate.  The decayed\nlearning rate.", "Raises": {"ValueError": "if global_step is not supplied."}}, "tf.compat.v1.train.range_input_producer": {"description": "Produces the integers from 0 to limit-1 in a queue. (deprecated)", "Args": {"limit": "An int32 scalar tensor.", "num_epochs": "An integer (optional). If specified, range_input_producer\nproduces each integer num_epochs times before generating an\nOutOfRange error. If not specified, range_input_producer can cycle\nthrough the integers an unlimited number of times.", "shuffle": "Boolean. If true, the integers are randomly shuffled within each\nepoch.", "seed": "An integer (optional). Seed used if shuffle == True.", "capacity": "An integer. Sets the queue capacity.", "shared_name": "(optional). If set, this queue will be shared under the given\nname across multiple sessions.", "name": "A name for the operations (optional)."}, "Returns": "A Queue with the output integers.  A QueueRunner for the Queue\nis added to the current Graph's QUEUE_RUNNER collection."}, "tf.compat.v1.train.remove_checkpoint": {"description": "Removes a checkpoint given by checkpoint_prefix. (deprecated)", "Args": {"checkpoint_prefix": "The prefix of a V1 or V2 checkpoint. Typically the result\nof Saver.save() or that of tf.train.latest_checkpoint(), regardless of\nsharded/non-sharded or V1/V2.", "checkpoint_format_version": "SaverDef.CheckpointFormatVersion, defaults to\nSaverDef.V2.", "meta_graph_suffix": "Suffix for MetaGraphDef file. Defaults to 'meta'."}}, "tf.compat.v1.train.replica_device_setter": {"description": "Return a device function to use when building a Graph for replicas.", "Args": {"ps_tasks": "Number of tasks in the ps job.  Ignored if cluster is\nprovided.", "ps_device": "String.  Device of the ps job.  If empty no ps job is used.\nDefaults to ps.", "worker_device": "String.  Device of the worker job.  If empty no worker\njob is used.", "merge_devices": "Boolean. If True, merges or only sets a device if the\ndevice constraint is completely unset. merges device specification rather\nthan overriding them.", "cluster": "ClusterDef proto or ClusterSpec.", "ps_ops": "List of strings representing Operation types that need to be\nplaced on ps devices.  If None, defaults to STANDARD_PS_OPS.", "ps_strategy": "A callable invoked for every ps Operation (i.e. matched by\nps_ops), that takes the Operation and returns the ps task index to\nuse.  If None, defaults to a round-robin strategy across all ps\ndevices."}, "Returns": "A function to pass to tf.device().", "Raises": {}}, "tf.compat.v1.train.sdca_fprint": {"description": "Computes fingerprints of the input strings.", "Args": {"input": "A Tensor of type string.\nvector of strings to compute fingerprints on.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int64."}, "tf.compat.v1.train.sdca_optimizer": {"description": "Distributed version of Stochastic Dual Coordinate Ascent (SDCA) optimizer for", "Args": {"sparse_example_indices": "A list of Tensor objects with type int64.\na list of vectors which contain example indices.", "sparse_feature_indices": "A list with the same length as sparse_example_indices of Tensor objects with type int64.\na list of vectors which contain feature indices.", "sparse_feature_values": "A list of Tensor objects with type float32.\na list of vectors which contains feature value\nassociated with each feature group.", "dense_features": "A list of Tensor objects with type float32.\na list of matrices which contains the dense feature values.", "example_weights": "A Tensor of type float32.\na vector which contains the weight associated with each\nexample.", "example_labels": "A Tensor of type float32.\na vector which contains the label/target associated with each\nexample.", "sparse_indices": "A list with the same length as sparse_example_indices of Tensor objects with type int64.\na list of vectors where each value is the indices which has\ncorresponding weights in sparse_weights. This field maybe omitted for the\ndense approach.", "sparse_weights": "A list with the same length as sparse_example_indices of Tensor objects with type float32.\na list of vectors where each value is the weight associated with\na sparse feature group.", "dense_weights": "A list with the same length as dense_features of Tensor objects with type float32.\na list of vectors where the values are the weights associated\nwith a dense feature group.", "example_state_data": "A Tensor of type float32.\na list of vectors containing the example state data.", "loss_type": "A string from: \"logistic_loss\", \"squared_loss\", \"hinge_loss\", \"smooth_hinge_loss\", \"poisson_loss\".\nType of the primal loss. Currently SdcaSolver supports logistic,\nsquared and hinge losses.", "l1": "A float. Symmetric l1 regularization strength.", "l2": "A float. Symmetric l2 regularization strength.", "num_loss_partitions": "An int that is >= 1.\nNumber of partitions of the global loss function.", "num_inner_iterations": "An int that is >= 1.\nNumber of iterations per mini-batch.", "adaptative": "An optional bool. Defaults to True.\nWhether to use Adaptive SDCA for the inner loop.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (out_example_state_data, out_delta_sparse_weights, out_delta_dense_weights)."}, "tf.compat.v1.train.sdca_shrink_l1": {"description": "Applies L1 regularization shrink step on the parameters.", "Args": {"weights": "A list of Tensor objects with type mutable float32.\na list of vectors where each value is the weight associated with a\nfeature group.", "l1": "A float. Symmetric l1 regularization strength.", "l2": "A float.\nSymmetric l2 regularization strength. Should be a positive float.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.compat.v1.train.shuffle_batch": {"description": "Creates batches by randomly shuffling tensors. (deprecated)", "Args": {"tensors": "The list or dictionary of tensors to enqueue.", "batch_size": "The new batch size pulled from the queue.", "capacity": "An integer. The maximum number of elements in the queue.", "min_after_dequeue": "Minimum number elements in the queue after a\ndequeue, used to ensure a level of mixing of elements.", "num_threads": "The number of threads enqueuing tensor_list.", "seed": "Seed for the random shuffling within the queue.", "enqueue_many": "Whether each tensor in tensor_list is a single example.", "shapes": "(Optional) The shapes for each example.  Defaults to the\ninferred shapes for tensor_list.", "allow_smaller_final_batch": "(Optional) Boolean. If True, allow the final\nbatch to be smaller if there are insufficient items left in the queue.", "shared_name": "(Optional) If set, this queue will be shared under the given\nname across multiple sessions.", "name": "(Optional) A name for the operations."}, "Returns": "A list or dictionary of tensors with the types as tensors.", "Raises": {"ValueError": "If the shapes are not specified, and cannot be\ninferred from the elements of tensors."}}, "tf.compat.v1.train.shuffle_batch_join": {"description": "Create batches by randomly shuffling tensors. (deprecated)", "Args": {"tensors_list": "A list of tuples or dictionaries of tensors to enqueue.", "batch_size": "An integer. The new batch size pulled from the queue.", "capacity": "An integer. The maximum number of elements in the queue.", "min_after_dequeue": "Minimum number elements in the queue after a\ndequeue, used to ensure a level of mixing of elements.", "seed": "Seed for the random shuffling within the queue.", "enqueue_many": "Whether each tensor in tensor_list_list is a single\nexample.", "shapes": "(Optional) The shapes for each example.  Defaults to the\ninferred shapes for tensors_list[i].", "allow_smaller_final_batch": "(Optional) Boolean. If True, allow the final\nbatch to be smaller if there are insufficient items left in the queue.", "shared_name": "(optional). If set, this queue will be shared under the given\nname across multiple sessions.", "name": "(Optional) A name for the operations."}, "Returns": "A list or dictionary of tensors with the same number and types as\ntensors_list[i].", "Raises": {"ValueError": "If the shapes are not specified, and cannot be\ninferred from the elements of tensors_list."}}, "tf.compat.v1.train.slice_input_producer": {"description": "Produces a slice of each Tensor in tensor_list. (deprecated)", "Args": {"tensor_list": "A list of Tensor objects. Every Tensor in\ntensor_list must have the same size in the first dimension.", "num_epochs": "An integer (optional). If specified, slice_input_producer\nproduces each slice num_epochs times before generating\nan OutOfRange error. If not specified, slice_input_producer can cycle\nthrough the slices an unlimited number of times.", "shuffle": "Boolean. If true, the integers are randomly shuffled within each\nepoch.", "seed": "An integer (optional). Seed used if shuffle == True.", "capacity": "An integer. Sets the queue capacity.", "shared_name": "(optional). If set, this queue will be shared under the given\nname across multiple sessions.", "name": "A name for the operations (optional)."}, "Returns": "A list of tensors, one for each element of tensor_list.  If the tensor\nin tensor_list has shape [N, a, b, .., z], then the corresponding output\ntensor will have shape [a, b, ..., z].", "Raises": {"ValueError": "if slice_input_producer produces nothing from tensor_list."}}, "tf.compat.v1.train.start_queue_runners": {"description": "Starts all queue runners collected in the graph. (deprecated)", "Args": {"sess": "Session used to run the queue ops.  Defaults to the\ndefault session.", "coord": "Optional Coordinator for coordinating the started threads.", "daemon": "Whether the threads should be marked as daemons, meaning\nthey don't block program exit.", "start": "Set to False to only create the threads, not start them.", "collection": "A GraphKey specifying the graph collection to\nget the queue runners from.  Defaults to GraphKeys.QUEUE_RUNNERS."}, "Raises": {"RuntimeError": "If called with eager execution enabled.", "ValueError": "If called without a default tf.compat.v1.Session registered."}, "Returns": "A list of threads."}, "tf.compat.v1.train.string_input_producer": {"description": "Output strings (e.g. filenames) to a queue for an input pipeline. (deprecated)", "Args": {"string_tensor": "A 1-D string tensor with the strings to produce.", "num_epochs": "An integer (optional). If specified, string_input_producer\nproduces each string from string_tensor num_epochs times before\ngenerating an OutOfRange error. If not specified,\nstring_input_producer can cycle through the strings in string_tensor\nan unlimited number of times.", "shuffle": "Boolean. If true, the strings are randomly shuffled within each\nepoch.", "seed": "An integer (optional). Seed used if shuffle == True.", "capacity": "An integer. Sets the queue capacity.", "shared_name": "(optional). If set, this queue will be shared under the given\nname across multiple sessions. All sessions open to the device which has\nthis queue will be able to access it via the shared_name. Using this in\na distributed setting means each name will only be seen by one of the\nsessions which has access to this operation.", "name": "A name for the operations (optional).", "cancel_op": "Cancel op for the queue (optional)."}, "Returns": "A queue with the output strings.  A QueueRunner for the Queue\nis added to the current Graph's QUEUE_RUNNER collection.", "Raises": {"ValueError": "If the string_tensor is a null Python list.  At runtime,\nwill fail with an assertion if string_tensor becomes a null tensor."}}, "tf.compat.v1.train.summary_iterator": {"description": "Returns a iterator for reading Event protocol buffers from an event file.", "Args": {"path": "The path to an event file created by a SummaryWriter."}, "Returns": "A iterator that yields Event protocol buffers"}, "tf.compat.v1.train.update_checkpoint_state": {"description": "Updates the content of the &#39;checkpoint&#39; file. (deprecated)", "Args": {"save_dir": "Directory where the model was saved.", "model_checkpoint_path": "The checkpoint file.", "all_model_checkpoint_paths": "List of strings.  Paths to all not-yet-deleted\ncheckpoints, sorted from oldest to newest.  If this is a non-empty list,\nthe last element must be equal to model_checkpoint_path.  These paths\nare also saved in the CheckpointState proto.", "latest_filename": "Optional name of the checkpoint file.  Default to\n'checkpoint'.", "all_model_checkpoint_timestamps": "Optional list of timestamps (floats,\nseconds since the Epoch) indicating when the checkpoints in\nall_model_checkpoint_paths were created.", "last_preserved_timestamp": "A float, indicating the number of seconds since\nthe Epoch when the last preserved checkpoint was written, e.g. due to a\nkeep_checkpoint_every_n_hours parameter (see\ntf.train.CheckpointManager for an implementation)."}, "Raises": {"RuntimeError": "If any of the model checkpoint paths conflict with the file\ncontaining CheckpointSate."}}, "tf.compat.v1.train.warm_start": {"description": "Warm-starts a model using the given settings.", "Args": {"ckpt_to_initialize_from": "[Required] A string specifying the directory with\ncheckpoint file(s) or path to checkpoint from which to warm-start the\nmodel parameters.", "vars_to_warm_start": "[Optional] One of the following:\n\nA regular expression (string) that captures which variables to\nwarm-start (see tf.compat.v1.get_collection).  This expression will only\nconsider variables in the TRAINABLE_VARIABLES collection -- if you need\nto warm-start non_TRAINABLE vars (such as optimizer accumulators or\nbatch norm statistics), please use the below option.\nA list of strings, each a regex scope provided to\ntf.compat.v1.get_collection with GLOBAL_VARIABLES (please see\ntf.compat.v1.get_collection).  For backwards compatibility reasons,\nthis is separate from the single-string argument type.\nA list of Variables to warm-start.  If you do not have access to the\nVariable objects at the call site, please use the above option.\nNone, in which case only TRAINABLE variables specified in\nvar_name_to_vocab_info will be warm-started.\n\nDefaults to '.*', which warm-starts all variables in the\nTRAINABLE_VARIABLES collection.  Note that this excludes variables such\nas accumulators and moving statistics from batch norm.", "var_name_to_vocab_info": "[Optional] Dict of variable names (strings) to\ntf.estimator.VocabInfo. The variable names should be \"full\" variables,\nnot the names of the partitions.  If not explicitly provided, the variable\nis assumed to have no (changes to) vocabulary.", "var_name_to_prev_var_name": "[Optional] Dict of variable names (strings) to\nname of the previously-trained variable in ckpt_to_initialize_from. If\nnot explicitly provided, the name of the variable is assumed to be same\nbetween previous checkpoint and current model.  Note that this has no\neffect on the set of variables that is warm-started, and only controls\nname mapping (use vars_to_warm_start for controlling what variables to\nwarm-start)."}, "Raises": {"ValueError": "If the WarmStartSettings contains prev_var_name or VocabInfo\nconfiguration for variable names that are not used.  This is to ensure\na stronger check for variable configuration than relying on users to\nexamine the logs."}}}, "tf.compat.v1.train.queue_runner": {}, "tf.compat.v1.types": {}, "tf.compat.v1.user_ops": {"tf.compat.v1.user_ops.my_fact": {"description": "Example of overriding the generated code for an Op."}}, "tf.compat.v1.version": {}, "tf.compat.v1.xla": {}, "tf.config": {"tf.config.LogicalDevice": {"description": "Abstraction for a logical device initialized by the runtime.", "Attributes": {"name": "A namedtuple alias for field number 0", "device_type": "A namedtuple alias for field number 1"}}, "tf.config.LogicalDeviceConfiguration": {"description": "Configuration class for a logical devices.", "Attributes": {"memory_limit": "A namedtuple alias for field number 0", "experimental_priority": "A namedtuple alias for field number 1"}}, "tf.config.PhysicalDevice": {"description": "Abstraction for a locally visible physical device.", "Attributes": {"name": "A namedtuple alias for field number 0", "device_type": "A namedtuple alias for field number 1"}}, "tf.config.experimental_connect_to_cluster": {"description": "Connects to the given cluster.", "Args": {"cluster_spec_or_resolver": "A ClusterSpec or ClusterResolver describing\nthe cluster.", "job_name": "The name of the local job.", "task_index": "The local task index.", "protocol": "The communication protocol, such as \"grpc\". If unspecified, will\nuse the default from python/platform/remote_utils.py.", "make_master_device_default": "If True and a cluster resolver is passed, will\nautomatically enter the master task device scope, which indicates the\nmaster becomes the default device to run ops. It won't do anything if\na cluster spec is passed. Will throw an error if the caller is currently\nalready in some device scope.", "cluster_device_filters": "an instance of\ntf.train.experimental/ClusterDeviceFilters that specify device filters\nto the remote tasks in cluster."}}, "tf.config.experimental_connect_to_host": {"description": "Connects to a single machine to enable remote execution on it.", "Args": {"remote_host": "a single or a list the remote server addr in host-port format.", "job_name": "The job name under which the new server will be accessible."}, "Raises": {"ValueError": "if remote_host is None."}}, "tf.config.experimental_functions_run_eagerly": {"description": "Returns the value of the experimental_run_functions_eagerly setting. (deprecated)"}, "tf.config.experimental_run_functions_eagerly": {"description": "Enables / disables eager execution of tf.functions. (deprecated)", "Args": {"run_eagerly": "Boolean. Whether to run functions eagerly."}}, "tf.config.functions_run_eagerly": {"description": "Returns the value of the run_functions_eagerly setting."}, "tf.config.get_logical_device_configuration": {"description": "Get the virtual device configuration for a tf.config.PhysicalDevice.", "Args": {"device": "PhysicalDevice to query"}, "Returns": "List of tf.config.LogicalDeviceConfiguration objects or\nNone if no virtual device configuration has been set for this physical\ndevice."}, "tf.config.get_soft_device_placement": {"description": "Return status of soft device placement flag.", "Returns": "A boolean indicating if soft placement is enabled."}, "tf.config.get_visible_devices": {"description": "Get the list of visible physical devices.", "Args": {"device_type": "(optional string) Only include devices matching this device\ntype. For example \"CPU\" or \"GPU\"."}, "Returns": "List of visible PhysicalDevices"}, "tf.config.list_logical_devices": {"description": "Return a list of logical devices created by runtime.", "Args": {"device_type": "(optional string) Only include devices matching this device\ntype. For example \"CPU\" or \"GPU\"."}, "Returns": "List of initialized LogicalDevices"}, "tf.config.list_physical_devices": {"description": "Return a list of physical devices visible to the host runtime.", "Args": {"device_type": "(optional string) Only include devices matching this device\ntype. For example \"CPU\" or \"GPU\"."}, "Returns": "List of discovered tf.config.PhysicalDevice objects"}, "tf.config.run_functions_eagerly": {"description": "Enables / disables eager execution of tf.functions.", "Args": {"run_eagerly": "Boolean. Whether to run functions eagerly."}}, "tf.config.set_logical_device_configuration": {"description": "Set the logical device configuration for a tf.config.PhysicalDevice.", "Args": {"device": "The PhysicalDevice to configure.", "logical_devices": "(optional) List of tf.config.LogicalDeviceConfiguration\nobjects to allocate for the specified PhysicalDevice. If None, the\ndefault configuration will be used."}, "Raises": {"ValueError": "If argument validation fails.", "RuntimeError": "Runtime is already initialized."}}, "tf.config.set_soft_device_placement": {"description": "Enable or disable soft device placement.", "Args": {"enabled": "A boolean indicating whether to enable soft placement."}}, "tf.config.set_visible_devices": {"description": "Set the list of visible devices.", "Args": {"devices": "List of PhysicalDevices to make visible", "device_type": "(optional) Only configure devices matching this device type.\nFor example \"CPU\" or \"GPU\". Other devices will be left unaltered."}, "Raises": {"ValueError": "If argument validation fails.", "RuntimeError": "Runtime is already initialized."}}}, "tf.config.optimizer": {"tf.config.optimizer.get_experimental_options": {"description": "Get experimental optimizer options.", "Returns": "Dictionary of configured experimental optimizer options"}, "tf.config.optimizer.get_jit": {"description": "Returns JIT compilation configuration for code inside tf.function."}, "tf.config.optimizer.set_experimental_options": {"description": "Set experimental optimizer options.", "Args": {"options": "Dictionary of experimental optimizer options to configure.\nValid keys:\n\nlayout_optimizer: Optimize tensor layouts e.g. This will try to use NCHW\nlayout on GPU which is faster.\nconstant_folding: Fold constants Statically infer the value of tensors\nwhen possible, and materialize the result using constants.\nshape_optimization: Simplify computations made on shapes.\nremapping: Remap subgraphs onto more efficient implementations.\narithmetic_optimization: Simplify arithmetic ops with common\nsub-expression elimination and arithmetic simplification.\ndependency_optimization: Control dependency optimizations. Remove\nredundant control dependencies, which may enable other optimization.\nThis optimizer is also essential for pruning Identity and NoOp nodes.\nloop_optimization: Loop optimizations.\nfunction_optimization: Function optimizations and inlining.\ndebug_stripper: Strips debug-related nodes from the graph.\ndisable_model_pruning: Disable removal of unnecessary ops from the graph\nscoped_allocator_optimization: Try to allocate some independent Op\noutputs contiguously in order to merge or eliminate downstream Ops.\npin_to_host_optimization: Force small ops onto the CPU.\nimplementation_selector: Enable the swap of kernel implementations based\non the device placement.\nauto_mixed_precision: Change certain float32 ops to float16 on Volta\nGPUs and above. Without the use of loss scaling, this can cause\nnumerical underflow (see\nkeras.mixed_precision.experimental.LossScaleOptimizer).\ndisable_meta_optimizer: Disable the entire meta optimizer.\nmin_graph_nodes: The minimum number of nodes in a graph to optimizer.\nFor smaller graphs, optimization is skipped."}}, "tf.config.optimizer.set_jit": {"description": "Configure JIT compilation. (deprecated argument values)", "Args": {"enabled": "JIT compilation configuration.\nPossible values:\n\n\"autoclustering\" (True is a deprecated alias): perform\nautoclustering\n(automatically identify and compile clusters of nodes) on all graphs\nusing\nXLA.\nFalse: do not automatically compile any graphs."}}}, "tf.config.threading": {"tf.config.threading.get_inter_op_parallelism_threads": {"description": "Get number of threads used for parallelism between independent operations.", "Returns": "Number of parallel threads"}, "tf.config.threading.get_intra_op_parallelism_threads": {"description": "Get number of threads used within an individual op for parallelism.", "Returns": "Number of parallel threads"}, "tf.config.threading.set_inter_op_parallelism_threads": {"description": "Set number of threads used for parallelism between independent operations.", "Args": {"num_threads": "Number of parallel threads"}}, "tf.config.threading.set_intra_op_parallelism_threads": {"description": "Set number of threads used within an individual op for parallelism.", "Args": {"num_threads": "Number of parallel threads"}}}, "tf.data": {"tf.data.Dataset": {"description": "Represents a potentially large set of elements.", "Args": {"variant_tensor": "A DT_VARIANT tensor that represents the dataset."}, "Attributes": {"element_spec": "The type specification of an element of this dataset.\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])dataset.element_specTensorSpec(shape=(), dtype=tf.int32, name=None)\nFor more information,\nread this guide."}}, "tf.data.DatasetSpec": {"description": "Type specification for tf.data.Dataset.", "Attributes": {"element_spec": "The inner element spec.", "value_type": "The Python type for values that are compatible with this TypeSpec.\nIn particular, all values that are compatible with this TypeSpec must be an\ninstance of this type."}}, "tf.data.FixedLengthRecordDataset": {"description": "A Dataset of fixed-length records from one or more binary files.", "Args": {"filenames": "A tf.string tensor or tf.data.Dataset containing one or\nmore filenames.", "record_bytes": "A tf.int64 scalar representing the number of bytes in each\nrecord.", "header_bytes": "(Optional.) A tf.int64 scalar representing the number of\nbytes to skip at the start of a file.", "footer_bytes": "(Optional.) A tf.int64 scalar representing the number of\nbytes to ignore at the end of a file.", "buffer_size": "(Optional.) A tf.int64 scalar representing the number of\nbytes to buffer when reading.", "compression_type": "(Optional.) A tf.string scalar evaluating to one of\n\"\" (no compression), \"ZLIB\", or \"GZIP\".", "num_parallel_reads": "(Optional.) A tf.int64 scalar representing the\nnumber of files to read in parallel. If greater than one, the records of\nfiles read in parallel are outputted in an interleaved order. If your\ninput pipeline is I/O bottlenecked, consider setting this parameter to a\nvalue greater than one to parallelize the I/O. If None, files will be\nread sequentially.", "name": "(Optional.) A name for the tf.data operation."}, "Attributes": {"element_spec": "The type specification of an element of this dataset.\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])dataset.element_specTensorSpec(shape=(), dtype=tf.int32, name=None)\nFor more information,\nread this guide."}}, "tf.data.Iterator": {"description": "Represents an iterator of a tf.data.Dataset.", "Attributes": {"element_spec": "The type specification of an element of this iterator.\ndataset = tf.data.Dataset.from_tensors(42)iterator = iter(dataset)iterator.element_spectf.TensorSpec(shape=(), dtype=tf.int32, name=None)\nFor more information,\nread this guide."}}, "tf.data.IteratorSpec": {"description": "Type specification for tf.data.Iterator.", "Attributes": {"element_spec": "A (nested) structure of tf.TypeSpec objects that represents\nthe type specification of the iterator elements.", "value_type": "The Python type for values that are compatible with this TypeSpec.\nIn particular, all values that are compatible with this TypeSpec must be an\ninstance of this type."}}, "tf.data.Options": {"description": "Represents options for tf.data.Dataset.", "Attributes": {"autotune": "The autotuning options associated with the dataset. See tf.data.experimental.AutotuneOptions for more details.", "deterministic": "Whether the outputs need to be produced in deterministic order. If None, defaults to True.", "experimental_deterministic": "DEPRECATED. Use deterministic instead.", "experimental_distribute": "The distribution strategy options associated with the dataset. See tf.data.experimental.DistributeOptions for more details.", "experimental_external_state_policy": "This option can be used to override the default policy for how to handle external state when serializing a dataset or checkpointing its iterator. There are three settings available - IGNORE: External state is ignored without a warning; WARN: External state is ignored and a warning is logged; FAIL: External state results in an error.", "experimental_optimization": "The optimization options associated with the dataset. See tf.data.experimental.OptimizationOptions for more details.", "experimental_slack": "Whether to introduce 'slack' in the last prefetch of the input pipeline, if it exists. This may reduce CPU contention with accelerator host-side activity at the start of a step. The slack frequency is determined by the number of devices attached to this input pipeline. If None, defaults to False.", "experimental_threading": "DEPRECATED. Use threading instead.", "threading": "The threading options associated with the dataset. See tf.data.ThreadingOptions for more details."}}, "tf.data.TFRecordDataset": {"description": "A Dataset comprising records from one or more TFRecord files.", "Args": {"filenames": "A tf.string tensor or tf.data.Dataset containing one or\nmore filenames.", "compression_type": "(Optional.) A tf.string scalar evaluating to one of\n\"\" (no compression), \"ZLIB\", or \"GZIP\".", "buffer_size": "(Optional.) A tf.int64 scalar representing the number of\nbytes in the read buffer. If your input pipeline is I/O bottlenecked,\nconsider setting this parameter to a value 1-100 MBs. If None, a\nsensible default for both local and remote file systems is used.", "num_parallel_reads": "(Optional.) A tf.int64 scalar representing the\nnumber of files to read in parallel. If greater than one, the records of\nfiles read in parallel are outputted in an interleaved order. If your\ninput pipeline is I/O bottlenecked, consider setting this parameter to a\nvalue greater than one to parallelize the I/O. If None, files will be\nread sequentially.", "name": "(Optional.) A name for the tf.data operation."}, "Raises": {"TypeError": "If any argument does not have the expected type.", "ValueError": "If any argument does not have the expected shape."}, "Attributes": {"element_spec": "The type specification of an element of this dataset.\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\ndataset.element_spec\nTensorSpec(shape=(), dtype=tf.int32, name=None)\n\nFor more information,\nread this guide."}}, "tf.data.TextLineDataset": {"description": "Creates a Dataset comprising lines from one or more text files.", "Args": {"filenames": "A tf.data.Dataset whose elements are tf.string scalars, a\ntf.string tensor, or a value that can be converted to a tf.string\ntensor (such as a list of Python strings).", "compression_type": "(Optional.) A tf.string scalar evaluating to one of\n\"\" (no compression), \"ZLIB\", or \"GZIP\".", "buffer_size": "(Optional.) A tf.int64 scalar denoting the number of bytes\nto buffer. A value of 0 results in the default buffering values chosen\nbased on the compression type.", "num_parallel_reads": "(Optional.) A tf.int64 scalar representing the\nnumber of files to read in parallel. If greater than one, the records of\nfiles read in parallel are outputted in an interleaved order. If your\ninput pipeline is I/O bottlenecked, consider setting this parameter to a\nvalue greater than one to parallelize the I/O. If None, files will be\nread sequentially.", "name": "(Optional.) A name for the tf.data operation."}, "Attributes": {"element_spec": "The type specification of an element of this dataset.\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])dataset.element_specTensorSpec(shape=(), dtype=tf.int32, name=None)\nFor more information,\nread this guide."}}, "tf.data.ThreadingOptions": {"description": "Represents options for dataset threading.", "Attributes": {"max_intra_op_parallelism": "If set, it overrides the maximum degree of intra-op parallelism.", "private_threadpool_size": "If set, the dataset will use a private threadpool of the given size. The value 0 can be used to indicate that the threadpool size should be determined at runtime based on the number of available CPU cores."}}}, "tf.data.experimental.service": {"tf.data.experimental.service.CrossTrainerCache": {"description": "Options related to the tf.data service cross trainer cache.", "Args": {"trainer_id": "Each training job has a unique ID. Once a job has consumed\ndata, the data remains in the cache and is re-used by jobs with different\ntrainer_ids. Requests with the same trainer_id do not re-use data."}, "Raises": {}}, "tf.data.experimental.service.DispatchServer": {"description": "An in-process tf.data service dispatch server.", "Args": {"config": "(Optional.) A tf.data.experimental.service.DispatcherConfig\nconfigration. If None, the dispatcher will use default\nconfiguration values.", "start": "(Optional.) Boolean, indicating whether to start the server after\ncreating it. Defaults to True."}, "Attributes": {"target": "Returns a target that can be used to connect to the server.\ndispatcher = tf.data.experimental.service.DispatchServer()dataset = tf.data.Dataset.range(10)dataset = dataset.apply(tf.data.experimental.service.distribute(\u00a0 \u00a0 processing_mode=\"parallel_epochs\", service=dispatcher.target))\nThe returned string will be in the form protocol://address, e.g.\n\"grpc://localhost:5050\"."}}, "tf.data.experimental.service.DispatcherConfig": {"description": "Configuration class for tf.data service dispatchers.", "Attributes": {"port": "A namedtuple alias for field number 0", "protocol": "A namedtuple alias for field number 1", "work_dir": "A namedtuple alias for field number 2", "fault_tolerant_mode": "A namedtuple alias for field number 3", "worker_addresses": "A namedtuple alias for field number 4", "job_gc_check_interval_ms": "A namedtuple alias for field number 5", "job_gc_timeout_ms": "A namedtuple alias for field number 6"}}, "tf.data.experimental.service.ShardingPolicy": {"description": "Specifies how to shard data among tf.data service workers.", "Class Variables": {"DATA": "<ShardingPolicy.DATA: 3>", "DYNAMIC": "<ShardingPolicy.DYNAMIC: 1>", "FILE": "<ShardingPolicy.FILE: 2>", "FILE_OR_DATA": "<ShardingPolicy.FILE_OR_DATA: 4>", "HINT": "<ShardingPolicy.HINT: 5>", "OFF": "<ShardingPolicy.OFF: 0>"}}, "tf.data.experimental.service.WorkerConfig": {"description": "Configuration class for tf.data service dispatchers.", "Attributes": {"dispatcher_address": "A namedtuple alias for field number 0", "worker_address": "A namedtuple alias for field number 1", "port": "A namedtuple alias for field number 2", "protocol": "A namedtuple alias for field number 3", "heartbeat_interval_ms": "A namedtuple alias for field number 4", "dispatcher_timeout_ms": "A namedtuple alias for field number 5"}}, "tf.data.experimental.service.WorkerServer": {"description": "An in-process tf.data service worker server.", "Args": {"config": "A tf.data.experimental.service.WorkerConfig configration.", "start": "(Optional.) Boolean, indicating whether to start the server after\ncreating it. Defaults to True."}}, "tf.data.experimental.service.distribute": {"description": "A transformation that moves dataset processing to the tf.data service.", "Args": {"processing_mode": "A tf.data.experimental.service.ShardingPolicy specifying\nhow to shard the dataset among tf.data workers. See\ntf.data.experimental.service.ShardingPolicy for details. For backwards\ncompatibility, processing_mode may also be set to the strings\n\"parallel_epochs\" or \"distributed_epoch\", which are respectively\nequivalent to ShardingPolicy.OFF and ShardingPolicy.DYNAMIC.", "service": "A string or a tuple indicating how to connect to the tf.data\nservice. If it's a string, it should be in the format\n[<protocol>://]<address>, where <address> identifies the dispatcher\n  address and <protocol> can optionally be used to override the default\n  protocol to use. If it's a tuple, it should be (protocol, address).", "job_name": "(Optional.) The name of the job. If provided, it must be a\nnon-empty string. This argument makes it possible for multiple datasets to\nshare the same job. The default behavior is that the dataset creates\nanonymous, exclusively owned jobs.", "consumer_index": "(Optional.) The index of the consumer in the range from 0\nto num_consumers. Must be specified alongside num_consumers. When\nspecified, consumers will read from the job in a strict round-robin order,\ninstead of the default first-come-first-served order.", "num_consumers": "(Optional.) The number of consumers which will consume from\nthe job. Must be specified alongside consumer_index. When specified,\nconsumers will read from the job in a strict round-robin order, instead of\nthe default first-come-first-served order. When num_consumers is\nspecified, the dataset must have infinite cardinality to prevent a\nproducer from running out of data early and causing consumers to go out of\nsync.", "max_outstanding_requests": "(Optional.) A limit on how many elements may be\nrequested at the same time. You can use this option to control the amount\nof memory used, since distribute won't use more than element_size *\nmax_outstanding_requests of memory.", "data_transfer_protocol": "(Optional.) The protocol to use for transferring\ndata with the tf.data service. By default, data is transferred using gRPC.", "compression": "How to compress the dataset's elements before transferring them\nover the network. \"AUTO\" leaves the decision of how to compress up to the\ntf.data service runtime. None indicates not to compress.", "target_workers": "(Optional.) Which workers to read from. If \"AUTO\", tf.data\nruntime decides which workers to read from. If \"ANY\", reads from any\ntf.data service workers. If \"LOCAL\", only reads from local in-processs\ntf.data service workers. \"AUTO\" works well for most cases, while users\ncan specify other targets. For example, \"LOCAL\" helps avoid RPCs and\ndata copy if every TF worker colocates with a tf.data service worker.\nConsumers of a shared job must use the same target_workers. Defaults to\n\"AUTO\"."}, "Returns": "Dataset\n\n\nA Dataset of the elements produced by the data service."}, "tf.data.experimental.service.from_dataset_id": {"description": "Creates a dataset which reads data from the tf.data service.", "Args": {"processing_mode": "A tf.data.experimental.service.ShardingPolicy specifying\nhow to shard the dataset among tf.data workers. See\ntf.data.experimental.service.ShardingPolicy for details. For backwards\ncompatibility, processing_mode may also be set to the strings\n\"parallel_epochs\" or \"distributed_epoch\", which are respectively\nequivalent to ShardingPolicy.OFF and ShardingPolicy.DYNAMIC.", "service": "A string or a tuple indicating how to connect to the tf.data\nservice. If it's a string, it should be in the format\n[<protocol>://]<address>, where <address> identifies the dispatcher\n  address and <protocol> can optionally be used to override the default\n  protocol to use. If it's a tuple, it should be (protocol, address).", "dataset_id": "The id of the dataset to read from. This id is returned by\nregister_dataset when the dataset is registered with the tf.data\nservice.", "element_spec": "A nested structure of tf.TypeSpecs representing the type of\nelements produced by the dataset. This argument is only required inside a\ntf.function. Use tf.data.Dataset.element_spec to get the element spec\nfor a given dataset.", "job_name": "(Optional.) The name of the job. If provided, it must be a\nnon-empty string. This argument makes it possible for multiple datasets to\nshare the same job. The default behavior is that the dataset creates\nanonymous, exclusively owned jobs.", "consumer_index": "(Optional.) The index of the consumer in the range from 0\nto num_consumers. Must be specified alongside num_consumers. When\nspecified, consumers will read from the job in a strict round-robin order,\ninstead of the default first-come-first-served order.", "num_consumers": "(Optional.) The number of consumers which will consume from\nthe job. Must be specified alongside consumer_index. When specified,\nconsumers will read from the job in a strict round-robin order, instead of\nthe default first-come-first-served order. When num_consumers is\nspecified, the dataset must have infinite cardinality to prevent a\nproducer from running out of data early and causing consumers to go out of\nsync.", "max_outstanding_requests": "(Optional.) A limit on how many elements may be\nrequested at the same time. You can use this option to control the amount\nof memory used, since distribute won't use more than element_size *\nmax_outstanding_requests of memory.", "data_transfer_protocol": "(Optional.) The protocol to use for transferring\ndata with the tf.data service. By default, data is transferred using gRPC.", "target_workers": "(Optional.) Which workers to read from. If \"AUTO\", tf.data\nruntime decides which workers to read from. If \"ANY\", reads from any\ntf.data service workers. If \"LOCAL\", only reads from local in-processs\ntf.data service workers. \"AUTO\" works well for most cases, while users\ncan specify other targets. For example, \"LOCAL\" helps avoid RPCs and\ndata copy if every TF worker colocates with a tf.data service worker.\nConsumers of a shared job must use the same target_workers. Defaults to\n\"AUTO\"."}, "Returns": "A tf.data.Dataset which reads from the tf.data service."}, "tf.data.experimental.service.register_dataset": {"description": "Registers a dataset with the tf.data service.", "Args": {"service": "A string or a tuple indicating how to connect to the tf.data\nservice. If it's a string, it should be in the format\n[<protocol>://]<address>, where <address> identifies the dispatcher\n  address and <protocol> can optionally be used to override the default\n  protocol to use. If it's a tuple, it should be (protocol, address).", "dataset": "A tf.data.Dataset to register with the tf.data service.", "compression": "(Optional.) How to compress the dataset's elements before\ntransferring them over the network. \"AUTO\" leaves the decision of how to\ncompress up to the tf.data service runtime. None indicates not to\ncompress."}, "Returns": "A scalar int64 tensor of the registered dataset's id."}}, "tf.debugging": {"tf.debugging.Assert": {"description": "Asserts that the given condition is true.", "Args": {"condition": "The condition to evaluate.", "data": "The tensors to print out when condition is false.", "summarize": "Print this many entries of each tensor.", "name": "A name for this operation (optional)."}, "Returns": "assert_op\n\n\nAn Operation that, when executed, raises a\ntf.errors.InvalidArgumentError if condition is not true.", "Raises": {}}, "tf.debugging.assert_all_finite": {"description": "Assert that the tensor does not contain any NaN&#39;s or Inf&#39;s.", "Args": {"x": "Tensor to check.", "message": "Message to log on failure.", "name": "A name for this operation (optional)."}, "Returns": "Same tensor as x."}, "tf.debugging.assert_equal": {"description": "Assert the condition x == y holds element-wise.", "Args": {"x": "Numeric Tensor.", "y": "Numeric Tensor, same dtype as and broadcastable to x.", "message": "A string to prefix to the default message.", "summarize": "Print this many entries of each tensor.", "name": "A name for this operation (optional).  Defaults to \"assert_equal\"."}, "Returns": "Op that raises InvalidArgumentError if x == y is False. This can be\nused with tf.control_dependencies inside of tf.functions to block\nfollowup computation until the check has executed.", "Raises": {"InvalidArgumentError": "if the check can be performed immediately and\nx == y is False. The check can be performed immediately during eager\nexecution or if x and y are statically known."}}, "tf.debugging.assert_greater": {"description": "Assert the condition x &gt; y holds element-wise.", "Args": {"x": "Numeric Tensor.", "y": "Numeric Tensor, same dtype as and broadcastable to x.", "message": "A string to prefix to the default message.", "summarize": "Print this many entries of each tensor.", "name": "A name for this operation (optional).  Defaults to \"assert_greater\"."}, "Returns": "Op that raises InvalidArgumentError if x > y is False. This can be\nused with tf.control_dependencies inside of tf.functions to block\nfollowup computation until the check has executed.", "Raises": {"InvalidArgumentError": "if the check can be performed immediately and\nx > y is False. The check can be performed immediately during eager\nexecution or if x and y are statically known."}}, "tf.debugging.assert_greater_equal": {"description": "Assert the condition x &gt;= y holds element-wise.", "Args": {"x": "Numeric Tensor.", "y": "Numeric Tensor, same dtype as and broadcastable to x.", "message": "A string to prefix to the default message.", "summarize": "Print this many entries of each tensor.", "name": "A name for this operation (optional).  Defaults to\n\"assert_greater_equal\"."}, "Returns": "Op that raises InvalidArgumentError if x >= y is False. This can be\nused with tf.control_dependencies inside of tf.functions to block\nfollowup computation until the check has executed.", "Raises": {"InvalidArgumentError": "if the check can be performed immediately and\nx >= y is False. The check can be performed immediately during eager\nexecution or if x and y are statically known."}}, "tf.debugging.assert_integer": {"description": "Assert that x is of integer dtype.", "Args": {"x": "A Tensor.", "message": "A string to prefix to the default message.", "name": "A name for this operation (optional). Defaults to \"assert_integer\"."}, "Raises": {"TypeError": "If x.dtype is not a non-quantized integer type."}}, "tf.debugging.assert_less": {"description": "Assert the condition x &lt; y holds element-wise.", "Args": {"x": "Numeric Tensor.", "y": "Numeric Tensor, same dtype as and broadcastable to x.", "message": "A string to prefix to the default message.", "summarize": "Print this many entries of each tensor.", "name": "A name for this operation (optional).  Defaults to \"assert_less\"."}, "Returns": "Op that raises InvalidArgumentError if x < y is False.\nThis can be used with tf.control_dependencies inside of tf.functions\nto block followup computation until the check has executed.", "Raises": {"InvalidArgumentError": "if the check can be performed immediately and\nx < y is False. The check can be performed immediately during eager\nexecution or if x and y are statically known."}}, "tf.debugging.assert_less_equal": {"description": "Assert the condition x &lt;= y holds element-wise.", "Args": {"x": "Numeric Tensor.", "y": "Numeric Tensor, same dtype as and broadcastable to x.", "message": "A string to prefix to the default message.", "summarize": "Print this many entries of each tensor.", "name": "A name for this operation (optional). Defaults to \"assert_less_equal\"."}, "Returns": "Op that raises InvalidArgumentError if x <= y is False. This can be\nused with tf.control_dependencies inside of tf.functions to block\nfollowup computation until the check has executed.", "Raises": {"InvalidArgumentError": "if the check can be performed immediately and\nx <= y is False. The check can be performed immediately during eager\nexecution or if x and y are statically known."}}, "tf.debugging.assert_near": {"description": "Assert the condition x and y are close element-wise.", "Args": {"x": "Float or complex Tensor.", "y": "Float or complex Tensor, same dtype as and broadcastable to x.", "rtol": "Tensor.  Same dtype as, and broadcastable to, x.\nThe relative tolerance.  Default is 10 * eps.", "atol": "Tensor.  Same dtype as, and broadcastable to, x.\nThe absolute tolerance.  Default is 10 * eps.", "message": "A string to prefix to the default message.", "summarize": "Print this many entries of each tensor.", "name": "A name for this operation (optional).  Defaults to \"assert_near\"."}, "Returns": "Op that raises InvalidArgumentError if x and y are not close enough.\nThis can be used with tf.control_dependencies inside of tf.functions\nto block followup computation until the check has executed.", "Raises": {"InvalidArgumentError": "if the check can be performed immediately and\nx != y is False for any pair of elements in x and y. The check can\nbe performed immediately during eager execution or if x and y are\nstatically known."}}, "tf.debugging.assert_negative": {"description": "Assert the condition x &lt; 0 holds element-wise.", "Args": {"x": "Numeric Tensor.", "message": "A string to prefix to the default message.", "summarize": "Print this many entries of each tensor.", "name": "A name for this operation (optional).  Defaults to \"assert_negative\"."}, "Returns": "Op raising InvalidArgumentError unless x is all negative. This can be\nused with tf.control_dependencies inside of tf.functions to block\nfollowup computation until the check has executed.", "Raises": {"InvalidArgumentError": "if the check can be performed immediately and\nx[i] < 0 is False. The check can be performed immediately during eager\nexecution or if x is statically known."}}, "tf.debugging.assert_non_negative": {"description": "Assert the condition x &gt;= 0 holds element-wise.", "Args": {"x": "Numeric Tensor.", "message": "A string to prefix to the default message.", "summarize": "Print this many entries of each tensor.", "name": "A name for this operation (optional).  Defaults to\n\"assert_non_negative\"."}, "Returns": "Op raising InvalidArgumentError unless x is all non-negative. This can\nbe used with tf.control_dependencies inside of tf.functions to block\nfollowup computation until the check has executed.", "Raises": {"InvalidArgumentError": "if the check can be performed immediately and\nx[i] >= 0 is False. The check can be performed immediately during eager\nexecution or if x is statically known."}}, "tf.debugging.assert_non_positive": {"description": "Assert the condition x &lt;= 0 holds element-wise.", "Args": {"x": "Numeric Tensor.", "message": "A string to prefix to the default message.", "summarize": "Print this many entries of each tensor.", "name": "A name for this operation (optional).  Defaults to\n\"assert_non_positive\"."}, "Returns": "Op raising InvalidArgumentError unless x is all non-positive. This can\nbe used with tf.control_dependencies inside of tf.functions to block\nfollowup computation until the check has executed.", "Raises": {"InvalidArgumentError": "if the check can be performed immediately and\nx[i] <= 0 is False. The check can be performed immediately during eager\nexecution or if x is statically known."}}, "tf.debugging.assert_none_equal": {"description": "Assert the condition x != y holds for all elements.", "Args": {"x": "Numeric Tensor.", "y": "Numeric Tensor, same dtype as and broadcastable to x.", "summarize": "Print this many entries of each tensor.", "message": "A string to prefix to the default message.", "name": "A name for this operation (optional).  Defaults to\n\"assert_none_equal\"."}, "Returns": "Op that raises InvalidArgumentError if x != y is ever False. This can\nbe used with tf.control_dependencies inside of tf.functions to block\nfollowup computation until the check has executed.", "Raises": {"InvalidArgumentError": "if the check can be performed immediately and\nx != y is False for any pair of elements in x and y. The check can\nbe performed immediately during eager execution or if x and y are\nstatically known."}}, "tf.debugging.assert_positive": {"description": "Assert the condition x &gt; 0 holds element-wise.", "Args": {"x": "Numeric Tensor.", "message": "A string to prefix to the default message.", "summarize": "Print this many entries of each tensor.", "name": "A name for this operation (optional). Defaults to \"assert_positive\"."}, "Returns": "Op raising InvalidArgumentError unless x is all positive. This can be\nused with tf.control_dependencies inside of tf.functions to block\nfollowup computation until the check has executed.", "Raises": {"InvalidArgumentError": "if the check can be performed immediately and\nx[i] > 0 is False. The check can be performed immediately during eager\nexecution or if x is statically known."}}, "tf.debugging.assert_proper_iterable": {"description": "Static assert that values is a &#34;proper&#34; iterable.", "Args": {"values": "Object to be checked."}, "Raises": {"TypeError": "If values is not iterable or is one of\nTensor, SparseTensor, np.array, tf.compat.bytes_or_text_types."}}, "tf.debugging.assert_rank": {"description": "Assert that x has rank equal to rank.", "Args": {"x": "Tensor.", "rank": "Scalar integer Tensor.", "message": "A string to prefix to the default message.", "name": "A name for this operation (optional). Defaults to\n\"assert_rank\"."}, "Returns": "Op raising InvalidArgumentError unless x has specified rank.\nIf static checks determine x has correct rank, a no_op is returned.\nThis can be used with tf.control_dependencies inside of tf.functions\nto block followup computation until the check has executed.", "Raises": {"InvalidArgumentError": "if the check can be performed immediately and\nx does not have rank rank. The check can be performed immediately\nduring eager execution or if the shape of x is statically known."}}, "tf.debugging.assert_rank_at_least": {"description": "Assert that x has rank of at least rank.", "Args": {"x": "Tensor.", "rank": "Scalar integer Tensor.", "message": "A string to prefix to the default message.", "name": "A name for this operation (optional).  Defaults to\n\"assert_rank_at_least\"."}, "Returns": "Op raising InvalidArgumentError unless x has specified rank or higher.\nIf static checks determine x has correct rank, a no_op is returned.\nThis can be used with tf.control_dependencies inside of tf.functions\nto block followup computation until the check has executed.", "Raises": {"InvalidArgumentError": "x does not have rank at least rank, but the rank\ncannot be statically determined.", "ValueError": "If static checks determine x has mismatched rank."}}, "tf.debugging.assert_rank_in": {"description": "Assert that x has a rank in ranks.", "Args": {"x": "Tensor.", "ranks": "Iterable of scalar Tensor objects.", "message": "A string to prefix to the default message.", "name": "A name for this operation (optional). Defaults to \"assert_rank_in\"."}, "Returns": "Op raising InvalidArgumentError unless rank of x is in ranks.\nIf static checks determine x has matching rank, a no_op is returned.\nThis can be used with tf.control_dependencies inside of tf.functions\nto block followup computation until the check has executed.", "Raises": {"InvalidArgumentError": "x does not have rank in ranks, but the rank cannot\nbe statically determined.", "ValueError": "If static checks determine x has mismatched rank."}}, "tf.debugging.assert_same_float_dtype": {"description": "Validate and return float type based on tensors and dtype.", "Args": {"tensors": "Tensors of input values. Can include None elements, which will be\nignored.", "dtype": "Expected type."}, "Returns": "Validated type.", "Raises": {"ValueError": "if neither tensors nor dtype is supplied, or result is not\nfloat, or the common type of the inputs is not a floating point type."}}, "tf.debugging.assert_scalar": {"description": "Asserts that the given tensor is a scalar.", "Args": {"tensor": "A Tensor.", "message": "A string to prefix to the default message.", "name": "A name for this operation. Defaults to \"assert_scalar\""}, "Raises": {"ValueError": "If the tensor is not scalar (rank 0), or if its shape is\nunknown."}}, "tf.debugging.assert_shapes": {"description": "Assert tensor shapes and dimension size relationships between tensors.", "Args": {"shapes": "dictionary with (Tensor to shape) items, or a list of\n(Tensor, shape) tuples. A shape must be an iterable.", "data": "The tensors to print out if the condition is False.  Defaults to error\nmessage and first few entries of the violating tensor.", "summarize": "Print this many entries of the tensor.", "message": "A string to prefix to the default message.", "name": "A name for this operation (optional).  Defaults to \"assert_shapes\"."}, "Raises": {"ValueError": "If static checks determine any shape constraint is violated."}}, "tf.debugging.assert_type": {"description": "Asserts that the given Tensor is of the specified type.", "Args": {"tensor": "A Tensor, SparseTensor or tf.Variable .", "tf_type": "A tensorflow type (dtypes.float32, tf.int64, dtypes.bool,\netc).", "message": "A string to prefix to the default message.", "name": "A name for this operation. Defaults to \"assert_type\""}, "Raises": {"TypeError": "If the tensor's data type doesn't match tf_type."}}, "tf.debugging.check_numerics": {"description": "Checks a tensor for NaN and Inf values.", "Args": {"tensor": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "message": "A string. Prefix of the error message.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as tensor."}, "tf.debugging.disable_check_numerics": {"description": "Disable the eager/graph unified numerics checking mechanism."}, "tf.debugging.disable_traceback_filtering": {"description": "Disable filtering out TensorFlow-internal frames in exception stack traces."}, "tf.debugging.enable_check_numerics": {"description": "Enable tensor numerics checking in an eager/graph unified fashion.", "Args": {"stack_height_limit": "Limit to the height of the printed stack trace.\nApplicable only to ops in tf.functions (graphs).", "path_length_limit": "Limit to the file path included in the printed stack\ntrace. Applicable only to ops in tf.functions (graphs)."}}, "tf.debugging.enable_traceback_filtering": {"description": "Enable filtering out TensorFlow-internal frames in exception stack traces.", "Raises": {"RuntimeError": "If Python version is not at least 3.7."}}, "tf.debugging.get_log_device_placement": {"description": "Get if device placements are logged.", "Returns": "If device placements are logged."}, "tf.debugging.is_numeric_tensor": {"description": "Returns True if the elements of tensor are numbers."}, "tf.debugging.is_traceback_filtering_enabled": {"description": "Check whether traceback filtering is currently enabled.", "Returns": "True if traceback filtering is enabled\n(e.g. if tf.debugging.enable_traceback_filtering() was called),\nand False otherwise (e.g. if tf.debugging.disable_traceback_filtering()\nwas called)."}, "tf.debugging.set_log_device_placement": {"description": "Turns logging for device placement decisions on or off.", "Args": {"enabled": "Whether to enabled device placement logging."}}}, "tf.distribute": {"tf.distribute.CrossDeviceOps": {"description": "Base class for cross-device reduction and broadcasting algorithms."}, "tf.distribute.DistributedDataset": {"description": "Represents a dataset distributed among devices and machines.", "Attributes": {"element_spec": "The type specification of an element of this tf.distribute.DistributedDataset.\nglobal_batch_size = 16strategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])dataset = tf.data.Dataset.from_tensors(([1.],[2])).repeat(100).batch(global_batch_size)dist_dataset = strategy.experimental_distribute_dataset(dataset)dist_dataset.element_spec(PerReplicaSpec(TensorSpec(shape=(None, 1), dtype=tf.float32, name=None),                TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)), PerReplicaSpec(TensorSpec(shape=(None, 1), dtype=tf.int32, name=None),                TensorSpec(shape=(None, 1), dtype=tf.int32, name=None)))"}}, "tf.distribute.DistributedIterator": {"description": "An iterator over tf.distribute.DistributedDataset.", "Attributes": {"element_spec": "The type specification of an element of tf.distribute.DistributedIterator.\nglobal_batch_size = 16strategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])dataset = tf.data.Dataset.from_tensors(([1.],[2])).repeat(100).batch(global_batch_size)distributed_iterator = iter(strategy.experimental_distribute_dataset(dataset))distributed_iterator.element_spec(PerReplicaSpec(TensorSpec(shape=(None, 1), dtype=tf.float32, name=None),                TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)), PerReplicaSpec(TensorSpec(shape=(None, 1), dtype=tf.int32, name=None),                TensorSpec(shape=(None, 1), dtype=tf.int32, name=None)))"}}, "tf.distribute.DistributedValues": {"description": "Base class for representing distributed values."}, "tf.distribute.HierarchicalCopyAllReduce": {"description": "Hierarchical copy all-reduce implementation of CrossDeviceOps.", "Args": {"num_packs": "a non-negative integer. The number of packs to split values\ninto. If zero, no packing will be done."}, "Raises": {}}, "tf.distribute.InputContext": {"description": "A class wrapping information needed by an input function.", "Args": {"num_input_pipelines": "the number of input pipelines in a cluster.", "input_pipeline_id": "the current input pipeline id, should be an int in\n[0,num_input_pipelines).", "num_replicas_in_sync": "the number of replicas that are in sync."}, "Attributes": {"input_pipeline_id": "Returns the input pipeline ID.", "num_input_pipelines": "Returns the number of input pipelines.", "num_replicas_in_sync": "Returns the number of compute replicas in sync."}}, "tf.distribute.InputOptions": {"description": "Run options for experimental_distribute_dataset(s_from_function).", "Attributes": {"experimental_fetch_to_device": "Boolean. If True, dataset\nelements will be prefetched to accelerator device memory. When False,\ndataset elements are prefetched to host device memory. Must be False when\nusing TPUEmbedding API. experimental_fetch_to_device can only be used\nwith experimental_replication_mode=PER_WORKER. Default behavior is same as\nsetting it to True.", "experimental_replication_mode": "Replication mode for the input function.\nCurrently, the InputReplicationMode.PER_REPLICA is only supported with\ntf.distribute.MirroredStrategy.\nexperimental_distribute_datasets_from_function.\nThe default value is InputReplicationMode.PER_WORKER.", "experimental_place_dataset_on_device": "Boolean. Default to False. When True,\ndataset will be placed on the device, otherwise it will remain on the\nhost. experimental_place_dataset_on_device=True can only be used with\nexperimental_replication_mode=PER_REPLICA", "experimental_per_replica_buffer_size": "Integer. Default to 1. Indicates the\nprefetch buffer size in the replica device memory. Users can set it\nto 0 to completely disable prefetching behavior, or a number greater than\n1 to enable larger buffer size. Note that this option is still\nvalid with experimental_fetch_to_device=False."}}, "tf.distribute.InputReplicationMode": {"description": "Replication mode for input function.", "Class Variables": {"PER_REPLICA": "<InputReplicationMode.PER_REPLICA: 'PER_REPLICA'>", "PER_WORKER": "<InputReplicationMode.PER_WORKER: 'PER_WORKER'>"}}, "tf.distribute.MirroredStrategy": {"description": "Synchronous training across multiple replicas on one machine.", "Args": {"devices": "a list of device strings such as ['/gpu:0', '/gpu:1'].  If\nNone, all available GPUs are used. If no GPUs are found, CPU is used.", "cross_device_ops": "optional, a descedant of CrossDeviceOps. If this is not\nset, NcclAllReduce() will be used by default.  One would customize this\nif NCCL isn't available or if a special implementation that exploits\nthe particular hardware is available."}, "Attributes": {"cluster_resolver": "Returns the cluster resolver associated with this strategy.\nIn general, when using a multi-worker tf.distribute strategy such as\ntf.distribute.experimental.MultiWorkerMirroredStrategy or\ntf.distribute.TPUStrategy(), there is a\ntf.distribute.cluster_resolver.ClusterResolver associated with the\nstrategy used, and such an instance is returned by this property.\nStrategies that intend to have an associated\ntf.distribute.cluster_resolver.ClusterResolver must set the\nrelevant attribute, or override this property; otherwise, None is returned\nby default. Those strategies should also provide information regarding what\nis returned by this property.\nSingle-worker strategies usually do not have a\ntf.distribute.cluster_resolver.ClusterResolver, and in those cases this\nproperty will return None.\nThe tf.distribute.cluster_resolver.ClusterResolver may be useful when the\nuser needs to access information such as the cluster spec, task type or task\nid. For example,\nos.environ['TF_CONFIG'] = json.dumps({\u00a0 'cluster': {\u00a0 \u00a0 \u00a0 'worker': [\"localhost:12345\", \"localhost:23456\"],\u00a0 \u00a0 \u00a0 'ps': [\"localhost:34567\"]\u00a0 },\u00a0 'task': {'type': 'worker', 'index': 0}})# This implicitly uses TF_CONFIG for the cluster and current task info.strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()...if strategy.cluster_resolver.task_type == 'worker':\u00a0 # Perform something that's only applicable on workers. Since we set this\u00a0 # as a worker above, this block will run on this particular instance.elif strategy.cluster_resolver.task_type == 'ps':\u00a0 # Perform something that's only applicable on parameter servers. Since we\u00a0 # set this as a worker above, this block will not run on this particular\u00a0 # instance.\nFor more information, please see\ntf.distribute.cluster_resolver.ClusterResolver's API docstring.", "extended": "tf.distribute.StrategyExtended with additional methods.", "num_replicas_in_sync": "Returns number of replicas over which gradients are aggregated."}}, "tf.distribute.MultiWorkerMirroredStrategy": {"description": "A distribution strategy for synchronous training on multiple workers.", "Args": {"cluster_resolver": "optional\ntf.distribute.cluster_resolver.ClusterResolver. If None,\ntf.distribute.cluster_resolver.TFConfigClusterResolver is used.", "communication_options": "optional\ntf.distribute.experimental.CommunicationOptions. This configures the\ndefault options for cross device communications. It can be overridden by\noptions provided to the communication APIs like\ntf.distribute.ReplicaContext.all_reduce. See\ntf.distribute.experimental.CommunicationOptions for details."}, "Attributes": {"cluster_resolver": "Returns the cluster resolver associated with this strategy.\nAs a multi-worker strategy, tf.distribute.MultiWorkerMirroredStrategy\nprovides the associated tf.distribute.cluster_resolver.ClusterResolver. If\nthe user provides one in __init__, that instance is returned; if the user\ndoes not, a default TFConfigClusterResolver is provided.", "extended": "tf.distribute.StrategyExtended with additional methods.", "num_replicas_in_sync": "Returns number of replicas over which gradients are aggregated."}}, "tf.distribute.NcclAllReduce": {"description": "NCCL all-reduce implementation of CrossDeviceOps.", "Args": {"num_packs": "a non-negative integer. The number of packs to split values\ninto. If zero, no packing will be done."}, "Raises": {"ValueError": "if num_packs is negative."}}, "tf.distribute.OneDeviceStrategy": {"description": "A distribution strategy for running on a single device.", "Args": {"device": "Device string identifier for the device on which the variables\nshould be placed. See class docs for more details on how the device is\nused. Examples: \"/cpu:0\", \"/gpu:0\", \"/device:CPU:0\", \"/device:GPU:0\""}, "Attributes": {"cluster_resolver": "Returns the cluster resolver associated with this strategy.\nIn general, when using a multi-worker tf.distribute strategy such as\ntf.distribute.experimental.MultiWorkerMirroredStrategy or\ntf.distribute.TPUStrategy(), there is a\ntf.distribute.cluster_resolver.ClusterResolver associated with the\nstrategy used, and such an instance is returned by this property.\nStrategies that intend to have an associated\ntf.distribute.cluster_resolver.ClusterResolver must set the\nrelevant attribute, or override this property; otherwise, None is returned\nby default. Those strategies should also provide information regarding what\nis returned by this property.\nSingle-worker strategies usually do not have a\ntf.distribute.cluster_resolver.ClusterResolver, and in those cases this\nproperty will return None.\nThe tf.distribute.cluster_resolver.ClusterResolver may be useful when the\nuser needs to access information such as the cluster spec, task type or task\nid. For example,\nos.environ['TF_CONFIG'] = json.dumps({\u00a0 'cluster': {\u00a0 \u00a0 \u00a0 'worker': [\"localhost:12345\", \"localhost:23456\"],\u00a0 \u00a0 \u00a0 'ps': [\"localhost:34567\"]\u00a0 },\u00a0 'task': {'type': 'worker', 'index': 0}})# This implicitly uses TF_CONFIG for the cluster and current task info.strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()...if strategy.cluster_resolver.task_type == 'worker':\u00a0 # Perform something that's only applicable on workers. Since we set this\u00a0 # as a worker above, this block will run on this particular instance.elif strategy.cluster_resolver.task_type == 'ps':\u00a0 # Perform something that's only applicable on parameter servers. Since we\u00a0 # set this as a worker above, this block will not run on this particular\u00a0 # instance.\nFor more information, please see\ntf.distribute.cluster_resolver.ClusterResolver's API docstring.", "extended": "tf.distribute.StrategyExtended with additional methods.", "num_replicas_in_sync": "Returns number of replicas over which gradients are aggregated."}}, "tf.distribute.ReduceOp": {"description": "Indicates how a set of values should be reduced.", "Class Variables": {"MEAN": "<ReduceOp.MEAN: 'MEAN'>", "SUM": "<ReduceOp.SUM: 'SUM'>"}}, "tf.distribute.ReductionToOneDevice": {"description": "A CrossDeviceOps implementation that copies values to one device to reduce.", "Args": {"reduce_to_device": "the intermediate device to reduce to. If None, reduce\nto the first device in destinations of the reduce method.", "accumulation_fn": "a function that does accumulation.  If None,\ntf.math.add_n is used."}}, "tf.distribute.ReplicaContext": {"description": "A class with a collection of APIs that can be called in a replica context.", "Args": {"strategy": "A tf.distribute.Strategy.", "replica_id_in_sync_group": "An integer, a Tensor or None. Prefer an\ninteger whenever possible to avoid issues with nested tf.function. It\naccepts a Tensor only to be compatible with tpu.replicate."}, "Attributes": {"devices": "Returns the devices this replica is to be executed on, as a tuple of strings. (deprecated)\nDeprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\nInstructions for updating:\nPlease avoid relying on devices property.Note: For tf.distribute.MirroredStrategy and\ntf.distribute.experimental.MultiWorkerMirroredStrategy, this returns a\nnested\nlist of device strings, e.g, [[\"GPU:0\"]].", "num_replicas_in_sync": "Returns number of replicas that are kept in sync.", "replica_id_in_sync_group": "Returns the id of the replica.\nThis identifies the replica among all replicas that are kept in sync. The\nvalue of the replica id can range from 0 to\ntf.distribute.ReplicaContext.num_replicas_in_sync - 1.\nNote: This is not guaranteed to be the same ID as the XLA replica ID use\nfor low-level operations such as collective_permute.", "strategy": "The current tf.distribute.Strategy object."}}, "tf.distribute.RunOptions": {"description": "Run options for strategy.run.", "Attributes": {"experimental_enable_dynamic_batch_size": "Boolean. Only applies to\nTPUStrategy. Default to True. If True, TPUStrategy will enable dynamic\npadder to support dynamic batch size for the inputs. Otherwise only static\nshape inputs are allowed.", "experimental_bucketizing_dynamic_shape": "Boolean. Only applies to\nTPUStrategy. Default to False. If True, TPUStrategy will automatic\nbucketize inputs passed into run if the input shape is\ndynamic. This is a performance optimization to reduce XLA recompilation,\nwhich should not have impact on correctness.", "experimental_xla_options": "A tf.tpu.XLAOptions instance. Only applies to\nTPUStrategy. Controls the XLA compiling options on TPUs. Default to None."}}, "tf.distribute.Server": {"description": "An in-process TensorFlow server, for use in distributed training.", "Args": {"server_or_cluster_def": "A tf.train.ServerDef or tf.train.ClusterDef\nprotocol buffer, or a tf.train.ClusterSpec object, describing the\nserver to be created and/or the cluster of which it is a member.", "job_name": "(Optional.) Specifies the name of the job of which the server is\na member. Defaults to the value in server_or_cluster_def, if\nspecified.", "task_index": "(Optional.) Specifies the task index of the server in its job.\nDefaults to the value in server_or_cluster_def, if specified.\nOtherwise defaults to 0 if the server's job has only one task.", "protocol": "(Optional.) Specifies the protocol to be used by the server.\nAcceptable values include \"grpc\", \"grpc+verbs\". Defaults to the value\nin server_or_cluster_def, if specified. Otherwise defaults to\n\"grpc\".", "config": "(Options.) A tf.compat.v1.ConfigProto that specifies default\nconfiguration options for all sessions that run on this server.", "start": "(Optional.) Boolean, indicating whether to start the server after\ncreating it. Defaults to True."}, "Raises": {"tf.errors.OpError": "Or one of its subclasses if an error occurs while\ncreating the TensorFlow server."}, "Attributes": {"server_def": "Returns the tf.train.ServerDef for this server.", "target": "Returns the target for a tf.compat.v1.Session to connect to this server.\nTo create a\ntf.compat.v1.Session that\nconnects to this server, use the following snippet:\nserver = tf.distribute.Server(...)with tf.compat.v1.Session(server.target):\u00a0 # ..."}}, "tf.distribute.Strategy": {"description": "A state & compute distribution policy on a list of devices.", "Attributes": {"cluster_resolver": "Returns the cluster resolver associated with this strategy.\nIn general, when using a multi-worker tf.distribute strategy such as\ntf.distribute.experimental.MultiWorkerMirroredStrategy or\ntf.distribute.TPUStrategy(), there is a\ntf.distribute.cluster_resolver.ClusterResolver associated with the\nstrategy used, and such an instance is returned by this property.\nStrategies that intend to have an associated\ntf.distribute.cluster_resolver.ClusterResolver must set the\nrelevant attribute, or override this property; otherwise, None is returned\nby default. Those strategies should also provide information regarding what\nis returned by this property.\nSingle-worker strategies usually do not have a\ntf.distribute.cluster_resolver.ClusterResolver, and in those cases this\nproperty will return None.\nThe tf.distribute.cluster_resolver.ClusterResolver may be useful when the\nuser needs to access information such as the cluster spec, task type or task\nid. For example,\nos.environ['TF_CONFIG'] = json.dumps({\u00a0 'cluster': {\u00a0 \u00a0 \u00a0 'worker': [\"localhost:12345\", \"localhost:23456\"],\u00a0 \u00a0 \u00a0 'ps': [\"localhost:34567\"]\u00a0 },\u00a0 'task': {'type': 'worker', 'index': 0}})# This implicitly uses TF_CONFIG for the cluster and current task info.strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()...if strategy.cluster_resolver.task_type == 'worker':\u00a0 # Perform something that's only applicable on workers. Since we set this\u00a0 # as a worker above, this block will run on this particular instance.elif strategy.cluster_resolver.task_type == 'ps':\u00a0 # Perform something that's only applicable on parameter servers. Since we\u00a0 # set this as a worker above, this block will not run on this particular\u00a0 # instance.\nFor more information, please see\ntf.distribute.cluster_resolver.ClusterResolver's API docstring.", "extended": "tf.distribute.StrategyExtended with additional methods.", "num_replicas_in_sync": "Returns number of replicas over which gradients are aggregated."}}, "tf.distribute.StrategyExtended": {"description": "Additional APIs for algorithms that need to be distribution-aware.", "Attributes": {"experimental_require_static_shapes": "Returns True if static shape is required; False otherwise.", "parameter_devices": "Returns the tuple of all devices used to place variables.", "worker_devices": "Returns the tuple of all devices used to for compute replica execution."}}, "tf.distribute.TPUStrategy": {"description": "Synchronous training on TPUs and TPU Pods.", "Args": {"tpu_cluster_resolver": "A\ntf.distribute.cluster_resolver.TPUClusterResolver instance, which\nprovides information about the TPU cluster. If None, it will assume\nrunning on a local TPU worker.", "experimental_device_assignment": "Optional\ntf.tpu.experimental.DeviceAssignment to specify the placement of\nreplicas on the TPU cluster.", "experimental_spmd_xla_partitioning": "If True, enable the SPMD (Single\nProgram Multiple Data) mode in XLA compiler. This flag only affects the\nperformance of XLA compilation and the HBM requirement of the compiled\nTPU program. Ceveat: if this flag is True, calling\ntf.distribute.TPUStrategy.experimental_assign_to_logical_device will\nresult in a ValueError."}, "Attributes": {"cluster_resolver": "Returns the cluster resolver associated with this strategy.\ntf.distribute.TPUStrategy provides the associated\ntf.distribute.cluster_resolver.ClusterResolver. If the user provides one\nin __init__, that instance is returned; if the user does not, a default\ntf.distribute.cluster_resolver.TPUClusterResolver is provided.", "extended": "tf.distribute.StrategyExtended with additional methods.", "num_replicas_in_sync": "Returns number of replicas over which gradients are aggregated."}}, "tf.distribute.experimental_set_strategy": {"description": "Set a tf.distribute.Strategy as current without with strategy.scope().", "Args": {"strategy": "A tf.distribute.Strategy object or None."}, "Raises": {"RuntimeError": "If called inside a with strategy.scope():."}}, "tf.distribute.get_replica_context": {"description": "Returns the current tf.distribute.ReplicaContext or None.", "Returns": "The current tf.distribute.ReplicaContext object when in a replica context\nscope, else None.\nWithin a particular block, exactly one of these two things will be true:\n\nget_replica_context() returns non-None, or\ntf.distribute.is_cross_replica_context() returns True."}, "tf.distribute.get_strategy": {"description": "Returns the current tf.distribute.Strategy object.", "Returns": "A tf.distribute.Strategy object. Inside a with strategy.scope() block,\nit returns strategy, otherwise it returns the default (single-replica)\ntf.distribute.Strategy object."}, "tf.distribute.has_strategy": {"description": "Return if there is a current non-default tf.distribute.Strategy.", "Returns": "True if inside a with strategy.scope():."}, "tf.distribute.in_cross_replica_context": {"description": "Returns True if in a cross-replica context.", "Returns": "True if in a cross-replica context (get_replica_context() returns\nNone), or False if in a replica context (get_replica_context() returns\nnon-None)."}}, "tf.distribute.cluster_resolver": {"tf.distribute.cluster_resolver.ClusterResolver": {"description": "Abstract class for all implementations of ClusterResolvers.", "Attributes": {"environment": "Returns the current environment which TensorFlow is running in.\nThere are two possible return values, \"google\" (when TensorFlow is running\nin a Google-internal environment) or an empty string (when TensorFlow is\nrunning elsewhere).\nIf you are implementing a ClusterResolver that works in both the Google\nenvironment and the open-source world (for instance, a TPU ClusterResolver\nor similar), you will have to return the appropriate string depending on the\nenvironment, which you will have to detect.\nOtherwise, if you are implementing a ClusterResolver that will only work\nin open-source TensorFlow, you do not need to implement this property.", "task_id": "Returns the task id this ClusterResolver indicates.\nIn TensorFlow distributed environment, each job may have an applicable\ntask id, which is the index of the instance within its task type. This is\nuseful when user needs to run specific code according to task index. For\nexample,\ncluster_spec = tf.train.ClusterSpec({\u00a0 \u00a0 \"ps\": [\"localhost:2222\", \"localhost:2223\"],\u00a0 \u00a0 \"worker\": [\"localhost:2224\", \"localhost:2225\", \"localhost:2226\"]})# SimpleClusterResolver is used here for illustration; other cluster# resolvers may be used for other source of task type/id.simple_resolver = SimpleClusterResolver(cluster_spec, task_type=\"worker\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 task_id=0)...if cluster_resolver.task_type == 'worker' and cluster_resolver.task_id == 0:\u00a0 # Perform something that's only applicable on 'worker' type, id 0. This\u00a0 # block will run on this particular instance since we've specified this\u00a0 # task to be a 'worker', id 0 in above cluster resolver.else:\u00a0 # Perform something that's only applicable on other ids. This block will\u00a0 # not run on this particular instance.\nReturns None if such information is not available or is not applicable\nin the current distributed environment, such as training with\ntf.distribute.cluster_resolver.TPUClusterResolver.\nFor more information, please see\ntf.distribute.cluster_resolver.ClusterResolver's class docstring.", "task_type": "Returns the task type this ClusterResolver indicates.\nIn TensorFlow distributed environment, each job may have an applicable\ntask type. Valid task types in TensorFlow include\n'chief': a worker that is designated with more responsibility,\n'worker': a regular worker for training/evaluation,\n'ps': a parameter server, or\n'evaluator': an evaluator that evaluates the checkpoints for metrics.\nSee Multi-worker configuration\nfor more information about 'chief' and 'worker' task type, which are most\ncommonly used.\nHaving access to such information is useful when user needs to run specific\ncode according to task types. For example,\ncluster_spec = tf.train.ClusterSpec({\u00a0 \u00a0 \"ps\": [\"localhost:2222\", \"localhost:2223\"],\u00a0 \u00a0 \"worker\": [\"localhost:2224\", \"localhost:2225\", \"localhost:2226\"]})# SimpleClusterResolver is used here for illustration; other cluster# resolvers may be used for other source of task type/id.simple_resolver = SimpleClusterResolver(cluster_spec, task_type=\"worker\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 task_id=1)...if cluster_resolver.task_type == 'worker':\u00a0 # Perform something that's only applicable on workers. This block\u00a0 # will run on this particular instance since we've specified this task to\u00a0 # be a worker in above cluster resolver.elif cluster_resolver.task_type == 'ps':\u00a0 # Perform something that's only applicable on parameter servers. This\u00a0 # block will not run on this particular instance.\nReturns None if such information is not available or is not applicable\nin the current distributed environment, such as training with\ntf.distribute.experimental.TPUStrategy.\nFor more information, please see\ntf.distribute.cluster_resolver.ClusterResolver's class doc."}}, "tf.distribute.cluster_resolver.GCEClusterResolver": {"description": "ClusterResolver for Google Compute Engine.", "Args": {"project": "Name of the GCE project.", "zone": "Zone of the GCE instance group.", "instance_group": "Name of the GCE instance group.", "port": "Port of the listening TensorFlow server (default: 8470)", "task_type": "Name of the TensorFlow job this GCE instance group of VM\ninstances belong to.", "task_id": "The task index for this particular VM, within the GCE\ninstance group. In particular, every single instance should be assigned\na unique ordinal index within an instance group manually so that they\ncan be distinguished from each other.", "rpc_layer": "The RPC layer TensorFlow should use to communicate across\ninstances.", "credentials": "GCE Credentials. If nothing is specified, this defaults to\nGoogleCredentials.get_application_default().", "service": "The GCE API object returned by the googleapiclient.discovery\nfunction. (Default: discovery.build('compute', 'v1')). If you specify a\ncustom service object, then the credentials parameter will be ignored."}, "Raises": {"ImportError": "If the googleapiclient is not installed."}, "Attributes": {"environment": "Returns the current environment which TensorFlow is running in.\nThere are two possible return values, \"google\" (when TensorFlow is running\nin a Google-internal environment) or an empty string (when TensorFlow is\nrunning elsewhere).\nIf you are implementing a ClusterResolver that works in both the Google\nenvironment and the open-source world (for instance, a TPU ClusterResolver\nor similar), you will have to return the appropriate string depending on the\nenvironment, which you will have to detect.\nOtherwise, if you are implementing a ClusterResolver that will only work\nin open-source TensorFlow, you do not need to implement this property.", "rpc_layer": "", "task_id": "Returns the task id this ClusterResolver indicates.\nIn TensorFlow distributed environment, each job may have an applicable\ntask id, which is the index of the instance within its task type. This is\nuseful when user needs to run specific code according to task index. For\nexample,\ncluster_spec = tf.train.ClusterSpec({\u00a0 \u00a0 \"ps\": [\"localhost:2222\", \"localhost:2223\"],\u00a0 \u00a0 \"worker\": [\"localhost:2224\", \"localhost:2225\", \"localhost:2226\"]})# SimpleClusterResolver is used here for illustration; other cluster# resolvers may be used for other source of task type/id.simple_resolver = SimpleClusterResolver(cluster_spec, task_type=\"worker\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 task_id=0)...if cluster_resolver.task_type == 'worker' and cluster_resolver.task_id == 0:\u00a0 # Perform something that's only applicable on 'worker' type, id 0. This\u00a0 # block will run on this particular instance since we've specified this\u00a0 # task to be a 'worker', id 0 in above cluster resolver.else:\u00a0 # Perform something that's only applicable on other ids. This block will\u00a0 # not run on this particular instance.\nReturns None if such information is not available or is not applicable\nin the current distributed environment, such as training with\ntf.distribute.cluster_resolver.TPUClusterResolver.\nFor more information, please see\ntf.distribute.cluster_resolver.ClusterResolver's class docstring.", "task_type": "Returns the task type this ClusterResolver indicates.\nIn TensorFlow distributed environment, each job may have an applicable\ntask type. Valid task types in TensorFlow include\n'chief': a worker that is designated with more responsibility,\n'worker': a regular worker for training/evaluation,\n'ps': a parameter server, or\n'evaluator': an evaluator that evaluates the checkpoints for metrics.\nSee Multi-worker configuration\nfor more information about 'chief' and 'worker' task type, which are most\ncommonly used.\nHaving access to such information is useful when user needs to run specific\ncode according to task types. For example,\ncluster_spec = tf.train.ClusterSpec({\u00a0 \u00a0 \"ps\": [\"localhost:2222\", \"localhost:2223\"],\u00a0 \u00a0 \"worker\": [\"localhost:2224\", \"localhost:2225\", \"localhost:2226\"]})# SimpleClusterResolver is used here for illustration; other cluster# resolvers may be used for other source of task type/id.simple_resolver = SimpleClusterResolver(cluster_spec, task_type=\"worker\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 task_id=1)...if cluster_resolver.task_type == 'worker':\u00a0 # Perform something that's only applicable on workers. This block\u00a0 # will run on this particular instance since we've specified this task to\u00a0 # be a worker in above cluster resolver.elif cluster_resolver.task_type == 'ps':\u00a0 # Perform something that's only applicable on parameter servers. This\u00a0 # block will not run on this particular instance.\nReturns None if such information is not available or is not applicable\nin the current distributed environment, such as training with\ntf.distribute.experimental.TPUStrategy.\nFor more information, please see\ntf.distribute.cluster_resolver.ClusterResolver's class doc."}}, "tf.distribute.cluster_resolver.KubernetesClusterResolver": {"description": "ClusterResolver for Kubernetes.", "Args": {"job_to_label_mapping": "A mapping of TensorFlow jobs to label selectors.\nThis allows users to specify many TensorFlow jobs in one Cluster\nResolver, and each job can have pods belong with different label\nselectors. For example, a sample mapping might be\n{'worker': ['job-name=worker-cluster-a', 'job-name=worker-cluster-b'],\u00a0'ps': ['job-name=ps-1', 'job-name=ps-2']}", "tf_server_port": "The port the TensorFlow server is listening on.", "rpc_layer": "(Optional) The RPC layer TensorFlow should use to communicate\nbetween tasks in Kubernetes. Defaults to 'grpc'.", "override_client": "The Kubernetes client (usually automatically retrieved\nusing from kubernetes import client as k8sclient). If you pass this\nin, you are responsible for setting Kubernetes credentials manually."}, "Raises": {"ImportError": "If the Kubernetes Python client is not installed and no\noverride_client is passed in.", "RuntimeError": "If autoresolve_task is not a boolean or a callable."}, "Attributes": {"environment": "Returns the current environment which TensorFlow is running in.\nThere are two possible return values, \"google\" (when TensorFlow is running\nin a Google-internal environment) or an empty string (when TensorFlow is\nrunning elsewhere).\nIf you are implementing a ClusterResolver that works in both the Google\nenvironment and the open-source world (for instance, a TPU ClusterResolver\nor similar), you will have to return the appropriate string depending on the\nenvironment, which you will have to detect.\nOtherwise, if you are implementing a ClusterResolver that will only work\nin open-source TensorFlow, you do not need to implement this property.", "task_id": "Returns the task id this ClusterResolver indicates.\nIn TensorFlow distributed environment, each job may have an applicable\ntask id, which is the index of the instance within its task type. This is\nuseful when user needs to run specific code according to task index. For\nexample,\ncluster_spec = tf.train.ClusterSpec({\u00a0 \u00a0 \"ps\": [\"localhost:2222\", \"localhost:2223\"],\u00a0 \u00a0 \"worker\": [\"localhost:2224\", \"localhost:2225\", \"localhost:2226\"]})# SimpleClusterResolver is used here for illustration; other cluster# resolvers may be used for other source of task type/id.simple_resolver = SimpleClusterResolver(cluster_spec, task_type=\"worker\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 task_id=0)...if cluster_resolver.task_type == 'worker' and cluster_resolver.task_id == 0:\u00a0 # Perform something that's only applicable on 'worker' type, id 0. This\u00a0 # block will run on this particular instance since we've specified this\u00a0 # task to be a 'worker', id 0 in above cluster resolver.else:\u00a0 # Perform something that's only applicable on other ids. This block will\u00a0 # not run on this particular instance.\nReturns None if such information is not available or is not applicable\nin the current distributed environment, such as training with\ntf.distribute.cluster_resolver.TPUClusterResolver.\nFor more information, please see\ntf.distribute.cluster_resolver.ClusterResolver's class docstring.", "task_type": "Returns the task type this ClusterResolver indicates.\nIn TensorFlow distributed environment, each job may have an applicable\ntask type. Valid task types in TensorFlow include\n'chief': a worker that is designated with more responsibility,\n'worker': a regular worker for training/evaluation,\n'ps': a parameter server, or\n'evaluator': an evaluator that evaluates the checkpoints for metrics.\nSee Multi-worker configuration\nfor more information about 'chief' and 'worker' task type, which are most\ncommonly used.\nHaving access to such information is useful when user needs to run specific\ncode according to task types. For example,\ncluster_spec = tf.train.ClusterSpec({\u00a0 \u00a0 \"ps\": [\"localhost:2222\", \"localhost:2223\"],\u00a0 \u00a0 \"worker\": [\"localhost:2224\", \"localhost:2225\", \"localhost:2226\"]})# SimpleClusterResolver is used here for illustration; other cluster# resolvers may be used for other source of task type/id.simple_resolver = SimpleClusterResolver(cluster_spec, task_type=\"worker\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 task_id=1)...if cluster_resolver.task_type == 'worker':\u00a0 # Perform something that's only applicable on workers. This block\u00a0 # will run on this particular instance since we've specified this task to\u00a0 # be a worker in above cluster resolver.elif cluster_resolver.task_type == 'ps':\u00a0 # Perform something that's only applicable on parameter servers. This\u00a0 # block will not run on this particular instance.\nReturns None if such information is not available or is not applicable\nin the current distributed environment, such as training with\ntf.distribute.experimental.TPUStrategy.\nFor more information, please see\ntf.distribute.cluster_resolver.ClusterResolver's class doc."}}, "tf.distribute.cluster_resolver.SimpleClusterResolver": {"description": "Simple implementation of ClusterResolver that accepts all attributes.", "Attributes": {"environment": "Returns the current environment which TensorFlow is running in.\nThere are two possible return values, \"google\" (when TensorFlow is running\nin a Google-internal environment) or an empty string (when TensorFlow is\nrunning elsewhere).\nIf you are implementing a ClusterResolver that works in both the Google\nenvironment and the open-source world (for instance, a TPU ClusterResolver\nor similar), you will have to return the appropriate string depending on the\nenvironment, which you will have to detect.\nOtherwise, if you are implementing a ClusterResolver that will only work\nin open-source TensorFlow, you do not need to implement this property.", "rpc_layer": "", "task_id": "Returns the task id this ClusterResolver indicates.\nIn TensorFlow distributed environment, each job may have an applicable\ntask id, which is the index of the instance within its task type. This is\nuseful when user needs to run specific code according to task index. For\nexample,\ncluster_spec = tf.train.ClusterSpec({\u00a0 \u00a0 \"ps\": [\"localhost:2222\", \"localhost:2223\"],\u00a0 \u00a0 \"worker\": [\"localhost:2224\", \"localhost:2225\", \"localhost:2226\"]})# SimpleClusterResolver is used here for illustration; other cluster# resolvers may be used for other source of task type/id.simple_resolver = SimpleClusterResolver(cluster_spec, task_type=\"worker\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 task_id=0)...if cluster_resolver.task_type == 'worker' and cluster_resolver.task_id == 0:\u00a0 # Perform something that's only applicable on 'worker' type, id 0. This\u00a0 # block will run on this particular instance since we've specified this\u00a0 # task to be a 'worker', id 0 in above cluster resolver.else:\u00a0 # Perform something that's only applicable on other ids. This block will\u00a0 # not run on this particular instance.\nReturns None if such information is not available or is not applicable\nin the current distributed environment, such as training with\ntf.distribute.cluster_resolver.TPUClusterResolver.\nFor more information, please see\ntf.distribute.cluster_resolver.ClusterResolver's class docstring.", "task_type": "Returns the task type this ClusterResolver indicates.\nIn TensorFlow distributed environment, each job may have an applicable\ntask type. Valid task types in TensorFlow include\n'chief': a worker that is designated with more responsibility,\n'worker': a regular worker for training/evaluation,\n'ps': a parameter server, or\n'evaluator': an evaluator that evaluates the checkpoints for metrics.\nSee Multi-worker configuration\nfor more information about 'chief' and 'worker' task type, which are most\ncommonly used.\nHaving access to such information is useful when user needs to run specific\ncode according to task types. For example,\ncluster_spec = tf.train.ClusterSpec({\u00a0 \u00a0 \"ps\": [\"localhost:2222\", \"localhost:2223\"],\u00a0 \u00a0 \"worker\": [\"localhost:2224\", \"localhost:2225\", \"localhost:2226\"]})# SimpleClusterResolver is used here for illustration; other cluster# resolvers may be used for other source of task type/id.simple_resolver = SimpleClusterResolver(cluster_spec, task_type=\"worker\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 task_id=1)...if cluster_resolver.task_type == 'worker':\u00a0 # Perform something that's only applicable on workers. This block\u00a0 # will run on this particular instance since we've specified this task to\u00a0 # be a worker in above cluster resolver.elif cluster_resolver.task_type == 'ps':\u00a0 # Perform something that's only applicable on parameter servers. This\u00a0 # block will not run on this particular instance.\nReturns None if such information is not available or is not applicable\nin the current distributed environment, such as training with\ntf.distribute.experimental.TPUStrategy.\nFor more information, please see\ntf.distribute.cluster_resolver.ClusterResolver's class doc."}}, "tf.distribute.cluster_resolver.SlurmClusterResolver": {"description": "ClusterResolver for system with Slurm workload manager.", "Args": {"jobs": "Dictionary with job names as key and number of tasks in the job as\nvalue. Defaults to as many 'worker's as there are (Slurm) tasks.", "port_base": "The first port number to start with for processes on a node.", "gpus_per_node": "Number of GPUs available on each node. Defaults to the\nnumber of GPUs reported by nvidia-smi", "gpus_per_task": "Number of GPUs to be used for each task. Default is to\nevenly distribute the gpus_per_node to tasks_per_node.", "tasks_per_node": "Number of tasks running on each node. Can be an integer if\nthe number of tasks per node is constant or a dictionary mapping\nhostnames to number of tasks on that node. If not set the Slurm\nenvironment is queried for the correct mapping.", "auto_set_gpu": "Set the visible CUDA devices automatically while resolving\nthe cluster by setting CUDA_VISIBLE_DEVICES environment variable.\nDefaults to True.", "rpc_layer": "The protocol TensorFlow used to communicate between nodes.\nDefaults to 'grpc'."}, "Raises": {"RuntimeError": "If requested more GPUs per node then available or\nrequested more tasks then assigned tasks or\nresolving missing values from the environment failed."}, "Attributes": {"environment": "Returns the current environment which TensorFlow is running in.\nThere are two possible return values, \"google\" (when TensorFlow is running\nin a Google-internal environment) or an empty string (when TensorFlow is\nrunning elsewhere).\nIf you are implementing a ClusterResolver that works in both the Google\nenvironment and the open-source world (for instance, a TPU ClusterResolver\nor similar), you will have to return the appropriate string depending on the\nenvironment, which you will have to detect.\nOtherwise, if you are implementing a ClusterResolver that will only work\nin open-source TensorFlow, you do not need to implement this property.", "task_id": "Returns the task id this ClusterResolver indicates.\nIn TensorFlow distributed environment, each job may have an applicable\ntask id, which is the index of the instance within its task type. This is\nuseful when user needs to run specific code according to task index. For\nexample,\ncluster_spec = tf.train.ClusterSpec({\u00a0 \u00a0 \"ps\": [\"localhost:2222\", \"localhost:2223\"],\u00a0 \u00a0 \"worker\": [\"localhost:2224\", \"localhost:2225\", \"localhost:2226\"]})# SimpleClusterResolver is used here for illustration; other cluster# resolvers may be used for other source of task type/id.simple_resolver = SimpleClusterResolver(cluster_spec, task_type=\"worker\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 task_id=0)...if cluster_resolver.task_type == 'worker' and cluster_resolver.task_id == 0:\u00a0 # Perform something that's only applicable on 'worker' type, id 0. This\u00a0 # block will run on this particular instance since we've specified this\u00a0 # task to be a 'worker', id 0 in above cluster resolver.else:\u00a0 # Perform something that's only applicable on other ids. This block will\u00a0 # not run on this particular instance.\nReturns None if such information is not available or is not applicable\nin the current distributed environment, such as training with\ntf.distribute.cluster_resolver.TPUClusterResolver.\nFor more information, please see\ntf.distribute.cluster_resolver.ClusterResolver's class docstring.", "task_type": "Returns the task type this ClusterResolver indicates.\nIn TensorFlow distributed environment, each job may have an applicable\ntask type. Valid task types in TensorFlow include\n'chief': a worker that is designated with more responsibility,\n'worker': a regular worker for training/evaluation,\n'ps': a parameter server, or\n'evaluator': an evaluator that evaluates the checkpoints for metrics.\nSee Multi-worker configuration\nfor more information about 'chief' and 'worker' task type, which are most\ncommonly used.\nHaving access to such information is useful when user needs to run specific\ncode according to task types. For example,\ncluster_spec = tf.train.ClusterSpec({\u00a0 \u00a0 \"ps\": [\"localhost:2222\", \"localhost:2223\"],\u00a0 \u00a0 \"worker\": [\"localhost:2224\", \"localhost:2225\", \"localhost:2226\"]})# SimpleClusterResolver is used here for illustration; other cluster# resolvers may be used for other source of task type/id.simple_resolver = SimpleClusterResolver(cluster_spec, task_type=\"worker\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 task_id=1)...if cluster_resolver.task_type == 'worker':\u00a0 # Perform something that's only applicable on workers. This block\u00a0 # will run on this particular instance since we've specified this task to\u00a0 # be a worker in above cluster resolver.elif cluster_resolver.task_type == 'ps':\u00a0 # Perform something that's only applicable on parameter servers. This\u00a0 # block will not run on this particular instance.\nReturns None if such information is not available or is not applicable\nin the current distributed environment, such as training with\ntf.distribute.experimental.TPUStrategy.\nFor more information, please see\ntf.distribute.cluster_resolver.ClusterResolver's class doc."}}, "tf.distribute.cluster_resolver.TFConfigClusterResolver": {"description": "Implementation of a ClusterResolver which reads the TF_CONFIG EnvVar.", "Args": {"task_type": "(String, optional) Overrides the task type specified in the\nTF_CONFIG environment variable.", "task_id": "(Integer, optional) Overrides the task index specified in the\nTF_CONFIG environment variable.", "rpc_layer": "(String, optional) Overrides the rpc layer TensorFlow uses.", "environment": "(String, optional) Overrides the environment TensorFlow\noperates in."}, "Attributes": {"environment": "Returns the current environment which TensorFlow is running in.\nThere are two possible return values, \"google\" (when TensorFlow is running\nin a Google-internal environment) or an empty string (when TensorFlow is\nrunning elsewhere).\nIf you are implementing a ClusterResolver that works in both the Google\nenvironment and the open-source world (for instance, a TPU ClusterResolver\nor similar), you will have to return the appropriate string depending on the\nenvironment, which you will have to detect.\nOtherwise, if you are implementing a ClusterResolver that will only work\nin open-source TensorFlow, you do not need to implement this property.", "rpc_layer": "", "task_id": "Returns the task id this ClusterResolver indicates.\nIn TensorFlow distributed environment, each job may have an applicable\ntask id, which is the index of the instance within its task type. This is\nuseful when user needs to run specific code according to task index. For\nexample,\ncluster_spec = tf.train.ClusterSpec({\u00a0 \u00a0 \"ps\": [\"localhost:2222\", \"localhost:2223\"],\u00a0 \u00a0 \"worker\": [\"localhost:2224\", \"localhost:2225\", \"localhost:2226\"]})# SimpleClusterResolver is used here for illustration; other cluster# resolvers may be used for other source of task type/id.simple_resolver = SimpleClusterResolver(cluster_spec, task_type=\"worker\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 task_id=0)...if cluster_resolver.task_type == 'worker' and cluster_resolver.task_id == 0:\u00a0 # Perform something that's only applicable on 'worker' type, id 0. This\u00a0 # block will run on this particular instance since we've specified this\u00a0 # task to be a 'worker', id 0 in above cluster resolver.else:\u00a0 # Perform something that's only applicable on other ids. This block will\u00a0 # not run on this particular instance.\nReturns None if such information is not available or is not applicable\nin the current distributed environment, such as training with\ntf.distribute.cluster_resolver.TPUClusterResolver.\nFor more information, please see\ntf.distribute.cluster_resolver.ClusterResolver's class docstring.", "task_type": "Returns the task type this ClusterResolver indicates.\nIn TensorFlow distributed environment, each job may have an applicable\ntask type. Valid task types in TensorFlow include\n'chief': a worker that is designated with more responsibility,\n'worker': a regular worker for training/evaluation,\n'ps': a parameter server, or\n'evaluator': an evaluator that evaluates the checkpoints for metrics.\nSee Multi-worker configuration\nfor more information about 'chief' and 'worker' task type, which are most\ncommonly used.\nHaving access to such information is useful when user needs to run specific\ncode according to task types. For example,\ncluster_spec = tf.train.ClusterSpec({\u00a0 \u00a0 \"ps\": [\"localhost:2222\", \"localhost:2223\"],\u00a0 \u00a0 \"worker\": [\"localhost:2224\", \"localhost:2225\", \"localhost:2226\"]})# SimpleClusterResolver is used here for illustration; other cluster# resolvers may be used for other source of task type/id.simple_resolver = SimpleClusterResolver(cluster_spec, task_type=\"worker\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 task_id=1)...if cluster_resolver.task_type == 'worker':\u00a0 # Perform something that's only applicable on workers. This block\u00a0 # will run on this particular instance since we've specified this task to\u00a0 # be a worker in above cluster resolver.elif cluster_resolver.task_type == 'ps':\u00a0 # Perform something that's only applicable on parameter servers. This\u00a0 # block will not run on this particular instance.\nReturns None if such information is not available or is not applicable\nin the current distributed environment, such as training with\ntf.distribute.experimental.TPUStrategy.\nFor more information, please see\ntf.distribute.cluster_resolver.ClusterResolver's class doc."}}, "tf.distribute.cluster_resolver.TPUClusterResolver": {"description": "Cluster Resolver for Google Cloud TPUs.", "Args": {"tpu": "A string corresponding to the TPU to use. It can be the TPU name or\nTPU worker gRPC address. If not set, it will try automatically resolve\nthe TPU address on Cloud TPUs. If set to \"local\", it will assume that\nthe TPU is directly connected to the VM instead of over the network.", "zone": "Zone where the TPUs are located. If omitted or empty, we will assume\nthat the zone of the TPU is the same as the zone of the GCE VM, which we\nwill try to discover from the GCE metadata service.", "project": "Name of the GCP project containing Cloud TPUs. If omitted or\nempty, we will try to discover the project name of the GCE VM from the\nGCE metadata service.", "job_name": "Name of the TensorFlow job the TPUs belong to.", "coordinator_name": "The name to use for the coordinator. Set to None if the\ncoordinator should not be included in the computed ClusterSpec.", "coordinator_address": "The address of the coordinator (typically an ip:port\npair). If set to None, a TF server will be started. If coordinator_name\nis None, a TF server will not be started even if coordinator_address is\nNone.", "credentials": "GCE Credentials. If None, then we use default credentials\nfrom the oauth2client", "service": "The GCE API object returned by the googleapiclient.discovery\nfunction. If you specify a custom service object, then the credentials\nparameter will be ignored.", "discovery_url": "A URL template that points to the location of the discovery\nservice. It should have two parameters {api} and {apiVersion} that when\nfilled in produce an absolute URL to the discovery document for that\nservice. The environment variable 'TPU_API_DISCOVERY_URL' will override\nthis."}, "Raises": {"ImportError": "If the googleapiclient is not installed.", "ValueError": "If no TPUs are specified.", "RuntimeError": "If an empty TPU name is specified and this is running in a\nGoogle Cloud environment."}, "Attributes": {"environment": "Returns the current environment which TensorFlow is running in.", "task_id": "Returns the task id this ClusterResolver indicates.\nIn TensorFlow distributed environment, each job may have an applicable\ntask id, which is the index of the instance within its task type. This is\nuseful when user needs to run specific code according to task index. For\nexample,\ncluster_spec = tf.train.ClusterSpec({\u00a0 \u00a0 \"ps\": [\"localhost:2222\", \"localhost:2223\"],\u00a0 \u00a0 \"worker\": [\"localhost:2224\", \"localhost:2225\", \"localhost:2226\"]})# SimpleClusterResolver is used here for illustration; other cluster# resolvers may be used for other source of task type/id.simple_resolver = SimpleClusterResolver(cluster_spec, task_type=\"worker\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 task_id=0)...if cluster_resolver.task_type == 'worker' and cluster_resolver.task_id == 0:\u00a0 # Perform something that's only applicable on 'worker' type, id 0. This\u00a0 # block will run on this particular instance since we've specified this\u00a0 # task to be a 'worker', id 0 in above cluster resolver.else:\u00a0 # Perform something that's only applicable on other ids. This block will\u00a0 # not run on this particular instance.\nReturns None if such information is not available or is not applicable\nin the current distributed environment, such as training with\ntf.distribute.cluster_resolver.TPUClusterResolver.\nFor more information, please see\ntf.distribute.cluster_resolver.ClusterResolver's class docstring.", "task_type": "Returns the task type this ClusterResolver indicates.\nIn TensorFlow distributed environment, each job may have an applicable\ntask type. Valid task types in TensorFlow include\n'chief': a worker that is designated with more responsibility,\n'worker': a regular worker for training/evaluation,\n'ps': a parameter server, or\n'evaluator': an evaluator that evaluates the checkpoints for metrics.\nSee Multi-worker configuration\nfor more information about 'chief' and 'worker' task type, which are most\ncommonly used.\nHaving access to such information is useful when user needs to run specific\ncode according to task types. For example,\ncluster_spec = tf.train.ClusterSpec({\u00a0 \u00a0 \"ps\": [\"localhost:2222\", \"localhost:2223\"],\u00a0 \u00a0 \"worker\": [\"localhost:2224\", \"localhost:2225\", \"localhost:2226\"]})# SimpleClusterResolver is used here for illustration; other cluster# resolvers may be used for other source of task type/id.simple_resolver = SimpleClusterResolver(cluster_spec, task_type=\"worker\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 task_id=1)...if cluster_resolver.task_type == 'worker':\u00a0 # Perform something that's only applicable on workers. This block\u00a0 # will run on this particular instance since we've specified this task to\u00a0 # be a worker in above cluster resolver.elif cluster_resolver.task_type == 'ps':\u00a0 # Perform something that's only applicable on parameter servers. This\u00a0 # block will not run on this particular instance.\nReturns None if such information is not available or is not applicable\nin the current distributed environment, such as training with\ntf.distribute.experimental.TPUStrategy.\nFor more information, please see\ntf.distribute.cluster_resolver.ClusterResolver's class doc.", "tpu_hardware_feature": "Returns the tpu topology info stored."}}, "tf.distribute.cluster_resolver.UnionResolver": {"description": "Performs a union on underlying ClusterResolvers.", "Args": {"*args": "ClusterResolver objects to be unionized.", "**kwargs": "rpc_layer - (Optional) Override value for the RPC layer used by\n  TensorFlow.\ntask_type - (Optional) Override value for the current task type.\ntask_id - (Optional) Override value for the current task index."}, "Raises": {"TypeError": "If any argument is not a subclass of ClusterResolvers.", "ValueError": "If there are no arguments passed."}, "Attributes": {"environment": "Returns the current environment which TensorFlow is running in.\nThere are two possible return values, \"google\" (when TensorFlow is running\nin a Google-internal environment) or an empty string (when TensorFlow is\nrunning elsewhere).\nIf you are implementing a ClusterResolver that works in both the Google\nenvironment and the open-source world (for instance, a TPU ClusterResolver\nor similar), you will have to return the appropriate string depending on the\nenvironment, which you will have to detect.\nOtherwise, if you are implementing a ClusterResolver that will only work\nin open-source TensorFlow, you do not need to implement this property.", "rpc_layer": "", "task_id": "Returns the task id this ClusterResolver indicates.\nIn TensorFlow distributed environment, each job may have an applicable\ntask id, which is the index of the instance within its task type. This is\nuseful when user needs to run specific code according to task index. For\nexample,\ncluster_spec = tf.train.ClusterSpec({\u00a0 \u00a0 \"ps\": [\"localhost:2222\", \"localhost:2223\"],\u00a0 \u00a0 \"worker\": [\"localhost:2224\", \"localhost:2225\", \"localhost:2226\"]})# SimpleClusterResolver is used here for illustration; other cluster# resolvers may be used for other source of task type/id.simple_resolver = SimpleClusterResolver(cluster_spec, task_type=\"worker\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 task_id=0)...if cluster_resolver.task_type == 'worker' and cluster_resolver.task_id == 0:\u00a0 # Perform something that's only applicable on 'worker' type, id 0. This\u00a0 # block will run on this particular instance since we've specified this\u00a0 # task to be a 'worker', id 0 in above cluster resolver.else:\u00a0 # Perform something that's only applicable on other ids. This block will\u00a0 # not run on this particular instance.\nReturns None if such information is not available or is not applicable\nin the current distributed environment, such as training with\ntf.distribute.cluster_resolver.TPUClusterResolver.\nFor more information, please see\ntf.distribute.cluster_resolver.ClusterResolver's class docstring.", "task_type": "Returns the task type this ClusterResolver indicates.\nIn TensorFlow distributed environment, each job may have an applicable\ntask type. Valid task types in TensorFlow include\n'chief': a worker that is designated with more responsibility,\n'worker': a regular worker for training/evaluation,\n'ps': a parameter server, or\n'evaluator': an evaluator that evaluates the checkpoints for metrics.\nSee Multi-worker configuration\nfor more information about 'chief' and 'worker' task type, which are most\ncommonly used.\nHaving access to such information is useful when user needs to run specific\ncode according to task types. For example,\ncluster_spec = tf.train.ClusterSpec({\u00a0 \u00a0 \"ps\": [\"localhost:2222\", \"localhost:2223\"],\u00a0 \u00a0 \"worker\": [\"localhost:2224\", \"localhost:2225\", \"localhost:2226\"]})# SimpleClusterResolver is used here for illustration; other cluster# resolvers may be used for other source of task type/id.simple_resolver = SimpleClusterResolver(cluster_spec, task_type=\"worker\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 task_id=1)...if cluster_resolver.task_type == 'worker':\u00a0 # Perform something that's only applicable on workers. This block\u00a0 # will run on this particular instance since we've specified this task to\u00a0 # be a worker in above cluster resolver.elif cluster_resolver.task_type == 'ps':\u00a0 # Perform something that's only applicable on parameter servers. This\u00a0 # block will not run on this particular instance.\nReturns None if such information is not available or is not applicable\nin the current distributed environment, such as training with\ntf.distribute.experimental.TPUStrategy.\nFor more information, please see\ntf.distribute.cluster_resolver.ClusterResolver's class doc."}}}, "tf.distribute.coordinator": {}, "tf.distribute.experimental.coordinator": {"tf.distribute.experimental.coordinator.ClusterCoordinator": {"description": "An object to schedule and coordinate remote function execution.", "Args": {"strategy": "a supported tf.distribute.Strategy object. Currently, only\ntf.distribute.experimental.ParameterServerStrategy is supported."}, "Raises": {"ValueError": "if the strategy being used is not supported."}, "Attributes": {"strategy": "Returns the Strategy associated with the ClusterCoordinator."}}, "tf.distribute.experimental.coordinator.PerWorkerValues": {"description": "A container that holds a list of values, one value per worker."}, "tf.distribute.experimental.coordinator.RemoteValue": {"description": "An asynchronously available value of a scheduled function."}}, "tf.distribute.experimental.partitioners": {"tf.distribute.experimental.partitioners.FixedShardsPartitioner": {"description": "Partitioner that allocates a fixed number of shards.", "Args": {"num_shards": "int, number of shards to partition."}}, "tf.distribute.experimental.partitioners.MaxSizePartitioner": {"description": "Partitioner that keeps shards below max_shard_bytes.", "Args": {"max_shard_bytes": "The maximum size any given shard is allowed to be.", "max_shards": "The maximum number of shards in int created taking\nprecedence over max_shard_bytes.", "bytes_per_string": "If the partition value is of type string, this provides\nan estimate of how large each string is."}}, "tf.distribute.experimental.partitioners.MinSizePartitioner": {"description": "Partitioner that allocates a minimum size per shard.", "Args": {"min_shard_bytes": "Minimum bytes of each shard. Defaults to 256K.", "max_shards": "Upper bound on the number of shards. Defaults to 1.", "bytes_per_string": "If the partition value is of type string, this provides\nan estimate of how large each string is."}}, "tf.distribute.experimental.partitioners.Partitioner": {"description": "Partitioner base class: all partitiners inherit from this class."}}, "tf.distribute.experimental.rpc": {"tf.distribute.experimental.rpc.Client": {"description": "Client class for invoking RPCs to the server."}, "tf.distribute.experimental.rpc.Server": {"description": "A Server base class for accepting RPCs for registered tf.functions."}}, "tf.dtypes": {"tf.dtypes.DType": {"description": "Represents the type of the elements in a Tensor.", "Attributes": {"as_datatype_enum": "Returns a types_pb2.DataType enum value based on this data type.", "as_numpy_dtype": "Returns a Python type object based on this DType.", "base_dtype": "Returns a non-reference DType based on this DType.", "is_bool": "Returns whether this is a boolean data type.", "is_complex": "Returns whether this is a complex floating point type.", "is_floating": "Returns whether this is a (non-quantized, real) floating point type.", "is_integer": "Returns whether this is a (non-quantized) integer type.", "is_numpy_compatible": "Returns whether this data type has a compatible NumPy data type.", "is_quantized": "Returns whether this is a quantized data type.", "is_unsigned": "Returns whether this type is unsigned.\nNon-numeric, unordered, and quantized types are not considered unsigned, and\nthis function returns False.", "limits": "Return intensity limits, i.e.\n(min, max) tuple, of the dtype.\nArgs:\n  clip_negative : bool, optional If True, clip the negative range (i.e.\n    return 0 for min intensity) even if the image dtype allows negative\n    values. Returns\n  min, max : tuple Lower and upper intensity limits.", "max": "Returns the maximum representable value in this data type.", "min": "Returns the minimum representable value in this data type.", "name": "", "real_dtype": "Returns the DType corresponding to this DType's real part.", "size": ""}}, "tf.dtypes.as_dtype": {"description": "Converts the given type_value to a DType.", "Args": {"type_value": "A value that can be converted to a tf.DType object. This may\ncurrently be a tf.DType object, a DataType\nenum,\n  a string type name, or a numpy.dtype."}, "Returns": "A DType corresponding to type_value.", "Raises": {"TypeError": "If type_value cannot be converted to a DType."}}, "tf.dtypes.complex": {"description": "Converts two real numbers to a complex number.", "Args": {"real": "A Tensor. Must be one of the following types: float32, float64.", "imag": "A Tensor. Must have the same type as real.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type complex64 or complex128.", "Raises": {"TypeError": "Real and imag must be correct types"}}, "tf.dtypes.saturate_cast": {"description": "Performs a safe saturating cast of value to dtype.", "Args": {"value": "A Tensor.", "dtype": "The desired output DType.", "name": "A name for the operation (optional)."}, "Returns": "value safely cast to dtype."}}, "tf.errors": {"tf.errors.AbortedError": {"description": "The operation was aborted, typically due to a concurrent action.", "Attributes": {"error_code": "The integer error code that describes the error.", "experimental_payloads": "A dictionary describing the details of the error.", "message": "The error message that describes the error.", "node_def": "The NodeDef proto representing the op that failed.", "op": "The operation that failed, if known.\nNote: If the failed op was synthesized at runtime, e.g. a Send\nor Recv op, there will be no corresponding\ntf.Operation\nobject.  In that case, this will return None, and you should\ninstead use the tf.errors.OpError.node_def to\ndiscover information about the op."}}, "tf.errors.AlreadyExistsError": {"description": "Raised when an entity that we attempted to create already exists.", "Attributes": {"error_code": "The integer error code that describes the error.", "experimental_payloads": "A dictionary describing the details of the error.", "message": "The error message that describes the error.", "node_def": "The NodeDef proto representing the op that failed.", "op": "The operation that failed, if known.\nNote: If the failed op was synthesized at runtime, e.g. a Send\nor Recv op, there will be no corresponding\ntf.Operation\nobject.  In that case, this will return None, and you should\ninstead use the tf.errors.OpError.node_def to\ndiscover information about the op."}}, "tf.errors.CancelledError": {"description": "Raised when an operation or step is cancelled.", "Attributes": {"error_code": "The integer error code that describes the error.", "experimental_payloads": "A dictionary describing the details of the error.", "message": "The error message that describes the error.", "node_def": "The NodeDef proto representing the op that failed.", "op": "The operation that failed, if known.\nNote: If the failed op was synthesized at runtime, e.g. a Send\nor Recv op, there will be no corresponding\ntf.Operation\nobject.  In that case, this will return None, and you should\ninstead use the tf.errors.OpError.node_def to\ndiscover information about the op."}}, "tf.errors.DataLossError": {"description": "Raised when unrecoverable data loss or corruption is encountered.", "Attributes": {"error_code": "The integer error code that describes the error.", "experimental_payloads": "A dictionary describing the details of the error.", "message": "The error message that describes the error.", "node_def": "The NodeDef proto representing the op that failed.", "op": "The operation that failed, if known.\nNote: If the failed op was synthesized at runtime, e.g. a Send\nor Recv op, there will be no corresponding\ntf.Operation\nobject.  In that case, this will return None, and you should\ninstead use the tf.errors.OpError.node_def to\ndiscover information about the op."}}, "tf.errors.DeadlineExceededError": {"description": "Raised when a deadline expires before an operation could complete.", "Attributes": {"error_code": "The integer error code that describes the error.", "experimental_payloads": "A dictionary describing the details of the error.", "message": "The error message that describes the error.", "node_def": "The NodeDef proto representing the op that failed.", "op": "The operation that failed, if known.\nNote: If the failed op was synthesized at runtime, e.g. a Send\nor Recv op, there will be no corresponding\ntf.Operation\nobject.  In that case, this will return None, and you should\ninstead use the tf.errors.OpError.node_def to\ndiscover information about the op."}}, "tf.errors.FailedPreconditionError": {"description": "Operation was rejected because the system is not in a state to execute it.", "Attributes": {"error_code": "The integer error code that describes the error.", "experimental_payloads": "A dictionary describing the details of the error.", "message": "The error message that describes the error.", "node_def": "The NodeDef proto representing the op that failed.", "op": "The operation that failed, if known.\nNote: If the failed op was synthesized at runtime, e.g. a Send\nor Recv op, there will be no corresponding\ntf.Operation\nobject.  In that case, this will return None, and you should\ninstead use the tf.errors.OpError.node_def to\ndiscover information about the op."}}, "tf.errors.InternalError": {"description": "Raised when the system experiences an internal error.", "Attributes": {"error_code": "The integer error code that describes the error.", "experimental_payloads": "A dictionary describing the details of the error.", "message": "The error message that describes the error.", "node_def": "The NodeDef proto representing the op that failed.", "op": "The operation that failed, if known.\nNote: If the failed op was synthesized at runtime, e.g. a Send\nor Recv op, there will be no corresponding\ntf.Operation\nobject.  In that case, this will return None, and you should\ninstead use the tf.errors.OpError.node_def to\ndiscover information about the op."}}, "tf.errors.InvalidArgumentError": {"description": "Raised when an operation receives an invalid argument.", "Attributes": {"error_code": "The integer error code that describes the error.", "experimental_payloads": "A dictionary describing the details of the error.", "message": "The error message that describes the error.", "node_def": "The NodeDef proto representing the op that failed.", "op": "The operation that failed, if known.\nNote: If the failed op was synthesized at runtime, e.g. a Send\nor Recv op, there will be no corresponding\ntf.Operation\nobject.  In that case, this will return None, and you should\ninstead use the tf.errors.OpError.node_def to\ndiscover information about the op."}}, "tf.errors.NotFoundError": {"description": "Raised when a requested entity (e.g., a file or directory) was not found.", "Attributes": {"error_code": "The integer error code that describes the error.", "experimental_payloads": "A dictionary describing the details of the error.", "message": "The error message that describes the error.", "node_def": "The NodeDef proto representing the op that failed.", "op": "The operation that failed, if known.\nNote: If the failed op was synthesized at runtime, e.g. a Send\nor Recv op, there will be no corresponding\ntf.Operation\nobject.  In that case, this will return None, and you should\ninstead use the tf.errors.OpError.node_def to\ndiscover information about the op."}}, "tf.errors.OpError": {"description": "The base class for TensorFlow exceptions.", "Args": {"node_def": "The node_def_pb2.NodeDef proto representing the op that\nfailed, if known; otherwise None.", "op": "The ops.Operation that failed, if known; otherwise None. During\neager execution, this field is always None.", "message": "The message string describing the failure.", "error_code": "The error_codes_pb2.Code describing the error.", "*args": "If not empty, it should contain a dictionary describing details\nabout the error. This argument is inspired by Abseil payloads:\nhttps://github.com/abseil/abseil-cpp/blob/master/absl/status/status.h"}, "Attributes": {"error_code": "The integer error code that describes the error.", "experimental_payloads": "A dictionary describing the details of the error.", "message": "The error message that describes the error.", "node_def": "The NodeDef proto representing the op that failed.", "op": "The operation that failed, if known.\nNote: If the failed op was synthesized at runtime, e.g. a Send\nor Recv op, there will be no corresponding\ntf.Operation\nobject.  In that case, this will return None, and you should\ninstead use the tf.errors.OpError.node_def to\ndiscover information about the op."}}, "tf.errors.OperatorNotAllowedInGraphError": {"description": "An error is raised for unsupported operator in Graph execution."}, "tf.errors.OutOfRangeError": {"description": "Raised when an operation iterates past the valid input range.", "Attributes": {"error_code": "The integer error code that describes the error.", "experimental_payloads": "A dictionary describing the details of the error.", "message": "The error message that describes the error.", "node_def": "The NodeDef proto representing the op that failed.", "op": "The operation that failed, if known.\nNote: If the failed op was synthesized at runtime, e.g. a Send\nor Recv op, there will be no corresponding\ntf.Operation\nobject.  In that case, this will return None, and you should\ninstead use the tf.errors.OpError.node_def to\ndiscover information about the op."}}, "tf.errors.PermissionDeniedError": {"description": "Raised when the caller does not have permission to run an operation.", "Attributes": {"error_code": "The integer error code that describes the error.", "experimental_payloads": "A dictionary describing the details of the error.", "message": "The error message that describes the error.", "node_def": "The NodeDef proto representing the op that failed.", "op": "The operation that failed, if known.\nNote: If the failed op was synthesized at runtime, e.g. a Send\nor Recv op, there will be no corresponding\ntf.Operation\nobject.  In that case, this will return None, and you should\ninstead use the tf.errors.OpError.node_def to\ndiscover information about the op."}}, "tf.errors.ResourceExhaustedError": {"description": "Some resource has been exhausted.", "Attributes": {"error_code": "The integer error code that describes the error.", "experimental_payloads": "A dictionary describing the details of the error.", "message": "The error message that describes the error.", "node_def": "The NodeDef proto representing the op that failed.", "op": "The operation that failed, if known.\nNote: If the failed op was synthesized at runtime, e.g. a Send\nor Recv op, there will be no corresponding\ntf.Operation\nobject.  In that case, this will return None, and you should\ninstead use the tf.errors.OpError.node_def to\ndiscover information about the op."}}, "tf.errors.UnauthenticatedError": {"description": "The request does not have valid authentication credentials.", "Attributes": {"error_code": "The integer error code that describes the error.", "experimental_payloads": "A dictionary describing the details of the error.", "message": "The error message that describes the error.", "node_def": "The NodeDef proto representing the op that failed.", "op": "The operation that failed, if known.\nNote: If the failed op was synthesized at runtime, e.g. a Send\nor Recv op, there will be no corresponding\ntf.Operation\nobject.  In that case, this will return None, and you should\ninstead use the tf.errors.OpError.node_def to\ndiscover information about the op."}}, "tf.errors.UnavailableError": {"description": "Raised when the runtime is currently unavailable.", "Attributes": {"error_code": "The integer error code that describes the error.", "experimental_payloads": "A dictionary describing the details of the error.", "message": "The error message that describes the error.", "node_def": "The NodeDef proto representing the op that failed.", "op": "The operation that failed, if known.\nNote: If the failed op was synthesized at runtime, e.g. a Send\nor Recv op, there will be no corresponding\ntf.Operation\nobject.  In that case, this will return None, and you should\ninstead use the tf.errors.OpError.node_def to\ndiscover information about the op."}}, "tf.errors.UnimplementedError": {"description": "Raised when an operation has not been implemented.", "Attributes": {"error_code": "The integer error code that describes the error.", "experimental_payloads": "A dictionary describing the details of the error.", "message": "The error message that describes the error.", "node_def": "The NodeDef proto representing the op that failed.", "op": "The operation that failed, if known.\nNote: If the failed op was synthesized at runtime, e.g. a Send\nor Recv op, there will be no corresponding\ntf.Operation\nobject.  In that case, this will return None, and you should\ninstead use the tf.errors.OpError.node_def to\ndiscover information about the op."}}, "tf.errors.UnknownError": {"description": "Unknown error.", "Attributes": {"error_code": "The integer error code that describes the error.", "experimental_payloads": "A dictionary describing the details of the error.", "message": "The error message that describes the error.", "node_def": "The NodeDef proto representing the op that failed.", "op": "The operation that failed, if known.\nNote: If the failed op was synthesized at runtime, e.g. a Send\nor Recv op, there will be no corresponding\ntf.Operation\nobject.  In that case, this will return None, and you should\ninstead use the tf.errors.OpError.node_def to\ndiscover information about the op."}}}, "tf.estimator.export": {"tf.estimator.export.ClassificationOutput": {"description": "Represents the output of a classification head.", "Args": {"scores": "A float Tensor giving scores (sometimes but not always\ninterpretable as probabilities) for each class.  May be None, but\nonly if classes is set.  Interpretation varies-- see class doc.", "classes": "A string Tensor giving predicted class labels.  May be None,\nbut only if scores is set.  Interpretation varies-- see class doc."}, "Raises": {"ValueError": "if neither classes nor scores is set, or one of them is not a\nTensor with the correct dtype."}, "Attributes": {"classes": "", "scores": ""}}, "tf.estimator.export.EvalOutput": {"description": "Represents the output of a supervised eval process.", "Args": {"loss": "dict of Tensors or single Tensor representing calculated loss.", "predictions": "dict of Tensors or single Tensor representing model\npredictions.", "metrics": "Dict of metric results keyed by name.\nThe values of the dict can be one of the following:\n(1) instance of Metric class.\n(2) (metric_value, update_op) tuples, or a single tuple.\nmetric_value must be a Tensor, and update_op must be a Tensor or Op."}, "Raises": {"ValueError": "if any of the outputs' dict keys are not strings or tuples of\nstrings or the values are not Tensors (or Operations in the case of\nupdate_op)."}, "Attributes": {"loss": "", "metrics": "", "predictions": ""}, "Class Variables": {"LOSS_NAME": "'loss'", "METRICS_NAME": "'metrics'", "METRIC_UPDATE_SUFFIX": "'update_op'", "METRIC_VALUE_SUFFIX": "'value'", "PREDICTIONS_NAME": "'predictions'"}}, "tf.estimator.export.ExportOutput": {"description": "Represents an output of a model that can be served."}, "tf.estimator.export.PredictOutput": {"description": "Represents the output of a generic prediction head.", "Args": {"outputs": "A Tensor or a dict of string to Tensor representing the\npredictions."}, "Raises": {"ValueError": "if the outputs is not dict, or any of its keys are not\nstrings, or any of its values are not Tensors."}, "Attributes": {"outputs": ""}}, "tf.estimator.export.RegressionOutput": {"description": "Represents the output of a regression head.", "Args": {"value": "a float Tensor giving the predicted values.  Required."}, "Raises": {"ValueError": "if the value is not a Tensor with dtype tf.float32."}, "Attributes": {"value": ""}}, "tf.estimator.export.ServingInputReceiver": {"description": "A return type for a serving_input_receiver_fn.", "Attributes": {"features": "A Tensor, SparseTensor, or dict of string or int to Tensor\nor SparseTensor, specifying the features to be passed to the model.\nNote: if features passed is not a dict, it will be wrapped in a dict\n  with a single entry, using 'feature' as the key.  Consequently, the\n  model\nmust accept a feature dict of the form {'feature': tensor}.  You may use\n  TensorServingInputReceiver if you want the tensor to be passed as is.", "receiver_tensors": "A Tensor, SparseTensor, or dict of string to Tensor\nor SparseTensor, specifying input nodes where this receiver expects to\nbe fed by default.  Typically, this is a single placeholder expecting\nserialized tf.Example protos.", "receiver_tensors_alternatives": "a dict of string to additional groups of\nreceiver tensors, each of which may be a Tensor, SparseTensor, or dict\nof string to Tensor orSparseTensor. These named receiver tensor\nalternatives generate additional serving signatures, which may be used to\nfeed inputs at different points within the input receiver subgraph.  A\ntypical usage is to allow feeding raw feature Tensors downstream of\nthe tf.parse_example() op. Defaults to None."}}, "tf.estimator.export.TensorServingInputReceiver": {"description": "A return type for a serving_input_receiver_fn.", "Attributes": {"features": "A single Tensor or SparseTensor, representing the feature to\nbe passed to the model.", "receiver_tensors": "A Tensor, SparseTensor, or dict of string to Tensor\nor SparseTensor, specifying input nodes where this receiver expects to\nbe fed by default.  Typically, this is a single placeholder expecting\nserialized tf.Example protos.", "receiver_tensors_alternatives": "a dict of string to additional groups of\nreceiver tensors, each of which may be a Tensor, SparseTensor, or dict\nof string to Tensor orSparseTensor. These named receiver tensor\nalternatives generate additional serving signatures, which may be used to\nfeed inputs at different points within the input receiver subgraph.  A\ntypical usage is to allow feeding raw feature Tensors downstream of\nthe tf.parse_example() op. Defaults to None."}}, "tf.estimator.export.build_parsing_serving_input_receiver_fn": {"description": "Build a serving_input_receiver_fn expecting fed tf.Examples.", "Args": {"feature_spec": "a dict of string to VarLenFeature/FixedLenFeature.", "default_batch_size": "the number of query examples expected per batch. Leave\nunset for variable batch size (recommended)."}, "Returns": "A serving_input_receiver_fn suitable for use in serving."}, "tf.estimator.export.build_raw_serving_input_receiver_fn": {"description": "Build a serving_input_receiver_fn expecting feature Tensors.", "Args": {"features": "a dict of string to Tensor.", "default_batch_size": "the number of query examples expected per batch. Leave\nunset for variable batch size (recommended)."}, "Returns": "A serving_input_receiver_fn."}}, "tf.experimental.dlpack": {"tf.experimental.dlpack.from_dlpack": {"description": "Returns the Tensorflow eager tensor.", "Args": {"dlcapsule": "A PyCapsule named as dltensor"}, "Returns": "A Tensorflow eager tensor"}, "tf.experimental.dlpack.to_dlpack": {"description": "Returns the dlpack capsule representing the tensor.", "Args": {"tf_tensor": "Tensorflow eager tensor, to be converted to dlpack capsule."}, "Returns": "A PyCapsule named as dltensor, which shares the underlying memory to other\nframework. This PyCapsule can be consumed only once."}}, "tf.experimental.dtensor": {"tf.experimental.dtensor.DTensorCheckpoint": {"description": "Manages saving/restoring trackable values to disk, for DTensor.", "Args": {"root": "The root object to checkpoint. root may be a trackable object or\nWeakRef of a trackable object.", "**kwargs": "Keyword arguments are set as attributes of this object, and are\nsaved with the checkpoint. All kwargs must be trackable objects, or a\nnested structure of trackable objects (list, dict, or tuple)."}, "Raises": {"ValueError": "If root or the objects in kwargs are not trackable. A\nValueError is also raised if the root object tracks different\nobjects from the ones listed in attributes in kwargs (e.g.\nroot.child = A and tf.train.Checkpoint(root, child=B) are\nincompatible)."}, "Attributes": {"save_counter": "An integer variable which starts at zero and is incremented on save.\nUsed to number checkpoints."}}, "tf.experimental.dtensor.DVariable": {"description": "A replacement for tf.Variable which follows initial value placement.", "Attributes": {"aggregation": "", "constraint": "Returns the constraint function associated with this variable.", "create": "The op responsible for initializing this variable.", "device": "The device this variable is on.", "dtype": "The dtype of this variable.", "graph": "The Graph of this variable.", "handle": "The handle by which this variable can be accessed.", "initial_value": "Returns the Tensor used as the initial value for the variable.", "initializer": "The op responsible for initializing this variable.", "name": "The name of the handle for this variable.", "op": "The op for this variable.", "save_as_bf16": "", "shape": "The shape of this variable.", "synchronization": "", "trainable": ""}}, "tf.experimental.dtensor.Layout": {"description": "Represents the layout information of a DTensor.", "Args": {"sharding_specs": "List of sharding specifications, each corresponding to a\ntensor axis. Each specification (dim_sharding) can either be a mesh\ndimension or the special value UNSHARDED.", "mesh": "A mesh configuration for the Tensor."}}, "tf.experimental.dtensor.Mesh": {"description": "Represents a Mesh configuration over a certain list of Mesh Dimensions.", "Args": {"dim_names": "A list of strings indicating dimension names.", "global_device_ids": "An ndarray of global device IDs is used to compose\nDeviceSpecs describing the mesh. The shape of this array determines the\nsize of each mesh dimension. Values in this array should increment\nsequentially from 0. This argument is the same for every DTensor client.", "local_device_ids": "A list of local device IDs equal to a subset of values\nin global_device_ids. They indicate the position of local devices in the\nglobal mesh. Different DTensor clients must contain distinct\nlocal_device_ids contents. All local_device_ids from all DTensor clients\nmust cover every element in global_device_ids.", "local_devices": "The list of devices hosted locally. The elements correspond\n1:1 to those of local_device_ids.", "mesh_name": "The name of the mesh. Currently, this is rarely used, and is\n  mostly used to indicate whether it is a CPU, GPU, or TPU-based mesh.\nglobal_devices (optional): The list of global devices. Set when multiple\n  device meshes are in use."}, "Attributes": {"dim_names": "", "name": "", "size": ""}}, "tf.experimental.dtensor.barrier": {"description": "Runs a barrier on the mesh.", "Args": {"mesh": "The mesh to run the barrier on.", "barrier_name": "The name of the barrier. mainly used for logging purpose."}}, "tf.experimental.dtensor.call_with_layout": {"description": "Calls a function in the DTensor device scope if layout is not None.", "Args": {"fn": "A supported TF API function such as tf.zeros.", "layout": "Optional, the layout of the output DTensor.", "*args": "Arguments given to fn.", "**kwargs": "Keyword arguments given to fn."}, "Returns": "The return value of fn transformed to a DTensor if requested."}, "tf.experimental.dtensor.check_layout": {"description": "Asserts that the layout of the DTensor is layout.", "Args": {"tensor": "A DTensor whose layout is to be checked.", "layout": "The Layout to compare against."}, "Raises": {"ValueError": "If the layout of tensor does not match the supplied layout."}}, "tf.experimental.dtensor.client_id": {"description": "Returns this client&#39;s ID."}, "tf.experimental.dtensor.copy_to_mesh": {"description": "Copies a tf.Tensor onto the DTensor device with the given layout.", "Args": {"tensor": "A regular tf.Tensor to be copied as a DTensor.", "layout": "Target layout (and mesh) for the result DTensor.", "source_layout": "Source layout of the tensor before copy, used for backward\npasses."}, "Returns": "A DTensor on the DTensor device with the given layout."}, "tf.experimental.dtensor.create_distributed_mesh": {"description": "Creates a single- or multi-client mesh.", "Args": {"mesh_dims": "A list of (dim_name, dim_size) tuples.", "mesh_name": "Name of the created mesh. Defaults to ''.", "num_global_devices": "Number of devices in the DTensor cluster. Defaults to\nthe corresponding environment variable.", "num_clients": "Number of clients in the DTensor cluster. Defaults to the\ncorresponding environment variable, DTENSOR_NUM_CLIENTS.", "client_id": "This client's ID. Defaults to the corresponding environment\nvariable, DTENSOR_CLIENT_ID.", "device_type": "Type of device to build the mesh for. Defaults to 'CPU'."}, "Returns": "A mesh created from specified or default arguments."}, "tf.experimental.dtensor.create_mesh": {"description": "Creates a single-client mesh.", "Args": {"mesh_dims": "A list of (dim_name, dim_size) tuples. Defaults to a single\nbatch-parallel dimension called 'x' using all devices. As a special case,\na single-element mesh_dims whose dim_size is -1 also uses all devices.", "mesh_name": "Name of the created mesh. Defaults to ''.", "devices": "String representations of devices to use. This is the device part\nof tf.DeviceSpec, e.g. 'CPU:0'. Defaults to all available logical devices.", "device_type": "If devices is missing, the type of devices to use. Defaults\nto 'CPU'."}, "Returns": "A single-client mesh created from specified or default arguments."}, "tf.experimental.dtensor.device_name": {"description": "Returns the singleton DTensor device&#39;s name."}, "tf.experimental.dtensor.enable_save_as_bf16": {"description": "Allows float32 DVariables to be checkpointed and restored as bfloat16.", "Args": {"variables": "A list of tf.Variable to be enabled with bfloat16 save/restore.\nOnly has effect on DTensor Variables as they go through d_variables with\nDTensor Specific logis."}}, "tf.experimental.dtensor.fetch_layout": {"description": "Fetches the layout of a DTensor.", "Args": {"tensor": "The DTensor whose layout is to be fetched."}, "Returns": "The Layout of this DTensor.", "Raises": {"RuntimeError": "When not called eagerly."}}, "tf.experimental.dtensor.full_job_name": {"description": "Returns the fully qualified TF job name for this or another task."}, "tf.experimental.dtensor.heartbeat_enabled": {"description": "Returns true if DTensor heartbeat service is enabled."}, "tf.experimental.dtensor.initialize_multi_client": {"description": "Initializes Multi Client DTensor.", "Args": {"enable_coordination_service": "If true, enable distributed coordination\nservice to make sure that workers know the devices on each other, a\nprerequisite for data transfer through cross-worker rendezvous."}}, "tf.experimental.dtensor.initialize_tpu_system": {"description": "Initialize the TPU devices.", "Args": {"enable_coordination_service": "If true, enable distributed coordination\nservice to make sure that workers know the devices on each other, a\nprerequisite for data transfer through cross-worker rendezvous."}, "Raises": {"RuntimeError": "If running inside a tf.function.", "NotFoundError": "If no TPU devices found in eager mode."}}, "tf.experimental.dtensor.job_name": {"description": "Returns the job name used by all clients in this DTensor cluster."}, "tf.experimental.dtensor.jobs": {"description": "Returns a list of job names of all clients in this DTensor cluster."}, "tf.experimental.dtensor.local_devices": {"description": "Returns a list of device specs of device_type attached to this client."}, "tf.experimental.dtensor.name_based_restore": {"description": "Restores from checkpoint_prefix to name based DTensors.", "Args": {"mesh": "The single mesh that all Tensors would be restored to.", "checkpoint_prefix": "The prefix of checkpoint to be restored.", "name_tensor_dict": "A ordered dictionary of tensor_names to a DTensor. The\nDTensor shape/dtype must match the tensors being saved/restored for now."}, "Returns": "A dictionary of name to its restored DTensor value."}, "tf.experimental.dtensor.name_based_save": {"description": "Saves name based Tensor into a Checkpoint.", "Args": {"mesh": "The single mesh that all Tensors would be restored to.", "checkpoint_prefix": "The prefix of checkpoint to be restored.", "name_tensor_dict": "A ordered dictionary of tensor_names to a DTensor. The\nDTensor shape/dtype must match the tensors being saved/restored for now."}}, "tf.experimental.dtensor.num_clients": {"description": "Returns the number of clients in this DTensor cluster."}, "tf.experimental.dtensor.num_global_devices": {"description": "Returns the number of devices of device_type in this DTensor cluster."}, "tf.experimental.dtensor.num_local_devices": {"description": "Returns the number of devices of device_type attached to this client."}, "tf.experimental.dtensor.pack": {"description": "Packs tf.Tensor components into a DTensor.", "Args": {"tensors": "The list of local tensor components to pack into a DTensor.", "layout": "The layout of the DTensor to be created."}, "Returns": "A DTensor created from the individual component tensors.", "Raises": {"RuntimeError": "When pack is not called eagerly."}}, "tf.experimental.dtensor.relayout": {"description": "Changes the layout of tensor.", "Args": {"tensor": "A DTensor to specify a new layout for.", "layout": "A Layout object specifying a new sharding spec."}, "Returns": "A DTensor output from the Relayout op."}, "tf.experimental.dtensor.run_on": {"description": "Runs enclosed functions in the DTensor device scope.", "Args": {"layout_or_mesh": "A Layout or Mesh instance to extract a default mesh from."}}, "tf.experimental.dtensor.sharded_save": {"description": "Saves given named tensor slices in a sharded, multi-client safe fashion.", "Args": {"mesh": "The Mesh that contains the Tensors to save.", "file_prefix": "The prefix of checkpoint.", "tensor_names": "a list of tensor names used in save op.", "shape_and_slices": "a list of shape and slice specification used in save op.\nThe only supported value is \"\" as we don't support distributed saving with\nslices yet.", "tensors": "a list of tensors used in save op. The order should match\ntensor_names."}, "Returns": "A MergeV2Checkpoints op that merged all Metadata."}, "tf.experimental.dtensor.shutdown_tpu_system": {"description": "Shutdown TPU system."}, "tf.experimental.dtensor.unpack": {"description": "Unpacks a DTensor into tf.Tensor components.", "Args": {"tensor": "The DTensor to unpack."}, "Returns": "The individual component tensors of the DTensor. This will include only the\nclient-local components, i.e. the components placed on the local devices.", "Raises": {"RuntimeError": "When unpack is not called eagerly."}}}, "tf.experimental.numpy": {"tf.experimental.numpy.abs": {"description": "TensorFlow variant of NumPy&#39;s abs."}, "tf.experimental.numpy.absolute": {"description": "TensorFlow variant of NumPy&#39;s absolute."}, "tf.experimental.numpy.add": {"description": "TensorFlow variant of NumPy&#39;s add."}, "tf.experimental.numpy.all": {"description": "TensorFlow variant of NumPy&#39;s all."}, "tf.experimental.numpy.allclose": {"description": "TensorFlow variant of NumPy&#39;s allclose."}, "tf.experimental.numpy.amax": {"description": "TensorFlow variant of NumPy&#39;s amax."}, "tf.experimental.numpy.amin": {"description": "TensorFlow variant of NumPy&#39;s amin."}, "tf.experimental.numpy.angle": {"description": "TensorFlow variant of NumPy&#39;s angle."}, "tf.experimental.numpy.any": {"description": "TensorFlow variant of NumPy&#39;s any."}, "tf.experimental.numpy.append": {"description": "TensorFlow variant of NumPy&#39;s append."}, "tf.experimental.numpy.arange": {"description": "TensorFlow variant of NumPy&#39;s arange."}, "tf.experimental.numpy.arccos": {"description": "TensorFlow variant of NumPy&#39;s arccos."}, "tf.experimental.numpy.arccosh": {"description": "TensorFlow variant of NumPy&#39;s arccosh."}, "tf.experimental.numpy.arcsin": {"description": "TensorFlow variant of NumPy&#39;s arcsin."}, "tf.experimental.numpy.arcsinh": {"description": "TensorFlow variant of NumPy&#39;s arcsinh."}, "tf.experimental.numpy.arctan": {"description": "TensorFlow variant of NumPy&#39;s arctan."}, "tf.experimental.numpy.arctan2": {"description": "TensorFlow variant of NumPy&#39;s arctan2."}, "tf.experimental.numpy.arctanh": {"description": "TensorFlow variant of NumPy&#39;s arctanh."}, "tf.experimental.numpy.argmax": {"description": "TensorFlow variant of NumPy&#39;s argmax."}, "tf.experimental.numpy.argmin": {"description": "TensorFlow variant of NumPy&#39;s argmin."}, "tf.experimental.numpy.argsort": {"description": "TensorFlow variant of NumPy&#39;s argsort."}, "tf.experimental.numpy.around": {"description": "TensorFlow variant of NumPy&#39;s around."}, "tf.experimental.numpy.array": {"description": "TensorFlow variant of NumPy&#39;s array."}, "tf.experimental.numpy.array_equal": {"description": "TensorFlow variant of NumPy&#39;s array_equal."}, "tf.experimental.numpy.asanyarray": {"description": "TensorFlow variant of NumPy&#39;s asanyarray."}, "tf.experimental.numpy.asarray": {"description": "TensorFlow variant of NumPy&#39;s asarray."}, "tf.experimental.numpy.ascontiguousarray": {"description": "TensorFlow variant of NumPy&#39;s ascontiguousarray."}, "tf.experimental.numpy.atleast_1d": {"description": "TensorFlow variant of NumPy&#39;s atleast_1d."}, "tf.experimental.numpy.atleast_2d": {"description": "TensorFlow variant of NumPy&#39;s atleast_2d."}, "tf.experimental.numpy.atleast_3d": {"description": "TensorFlow variant of NumPy&#39;s atleast_3d."}, "tf.experimental.numpy.average": {"description": "TensorFlow variant of NumPy&#39;s average."}, "tf.experimental.numpy.bitwise_and": {"description": "TensorFlow variant of NumPy&#39;s bitwise_and."}, "tf.experimental.numpy.bitwise_not": {"description": "TensorFlow variant of NumPy&#39;s bitwise_not."}, "tf.experimental.numpy.bitwise_or": {"description": "TensorFlow variant of NumPy&#39;s bitwise_or."}, "tf.experimental.numpy.bitwise_xor": {"description": "TensorFlow variant of NumPy&#39;s bitwise_xor."}, "tf.experimental.numpy.bool_": {"description": "Boolean type (True or False), stored as a byte.", "Attributes": {"T": "Scalar attribute identical to the corresponding array attribute.\nPlease see ndarray.T.", "base": "Scalar attribute identical to the corresponding array attribute.\nPlease see ndarray.base.", "data": "Pointer to start of data.", "dtype": "Get array data-descriptor.", "flags": "The integer value of flags.", "flat": "A 1-D view of the scalar.", "imag": "The imaginary part of the scalar.", "itemsize": "The length of one element in bytes.", "nbytes": "The length of the scalar in bytes.", "ndim": "The number of array dimensions.", "real": "The real part of the scalar.", "shape": "Tuple of array dimensions.", "size": "The number of elements in the gentype.", "strides": "Tuple of bytes steps in each dimension."}}, "tf.experimental.numpy.broadcast_arrays": {"description": "TensorFlow variant of NumPy&#39;s broadcast_arrays."}, "tf.experimental.numpy.broadcast_to": {"description": "TensorFlow variant of NumPy&#39;s broadcast_to."}, "tf.experimental.numpy.cbrt": {"description": "TensorFlow variant of NumPy&#39;s cbrt."}, "tf.experimental.numpy.ceil": {"description": "TensorFlow variant of NumPy&#39;s ceil."}, "tf.experimental.numpy.clip": {"description": "TensorFlow variant of NumPy&#39;s clip."}, "tf.experimental.numpy.complex128": {"description": "Complex number type composed of two double-precision floating-point", "Attributes": {"T": "Scalar attribute identical to the corresponding array attribute.\nPlease see ndarray.T.", "base": "Scalar attribute identical to the corresponding array attribute.\nPlease see ndarray.base.", "data": "Pointer to start of data.", "dtype": "Get array data-descriptor.", "flags": "The integer value of flags.", "flat": "A 1-D view of the scalar.", "imag": "The imaginary part of the scalar.", "itemsize": "The length of one element in bytes.", "nbytes": "The length of the scalar in bytes.", "ndim": "The number of array dimensions.", "real": "The real part of the scalar.", "shape": "Tuple of array dimensions.", "size": "The number of elements in the gentype.", "strides": "Tuple of bytes steps in each dimension."}}, "tf.experimental.numpy.complex64": {"description": "Complex number type composed of two single-precision floating-point", "Attributes": {"T": "Scalar attribute identical to the corresponding array attribute.\nPlease see ndarray.T.", "base": "Scalar attribute identical to the corresponding array attribute.\nPlease see ndarray.base.", "data": "Pointer to start of data.", "dtype": "Get array data-descriptor.", "flags": "The integer value of flags.", "flat": "A 1-D view of the scalar.", "imag": "The imaginary part of the scalar.", "itemsize": "The length of one element in bytes.", "nbytes": "The length of the scalar in bytes.", "ndim": "The number of array dimensions.", "real": "The real part of the scalar.", "shape": "Tuple of array dimensions.", "size": "The number of elements in the gentype.", "strides": "Tuple of bytes steps in each dimension."}}, "tf.experimental.numpy.compress": {"description": "TensorFlow variant of NumPy&#39;s compress."}, "tf.experimental.numpy.concatenate": {"description": "TensorFlow variant of NumPy&#39;s concatenate."}, "tf.experimental.numpy.conj": {"description": "TensorFlow variant of NumPy&#39;s conj."}, "tf.experimental.numpy.conjugate": {"description": "TensorFlow variant of NumPy&#39;s conjugate."}, "tf.experimental.numpy.copy": {"description": "TensorFlow variant of NumPy&#39;s copy."}, "tf.experimental.numpy.cos": {"description": "TensorFlow variant of NumPy&#39;s cos."}, "tf.experimental.numpy.cosh": {"description": "TensorFlow variant of NumPy&#39;s cosh."}, "tf.experimental.numpy.count_nonzero": {"description": "TensorFlow variant of NumPy&#39;s count_nonzero."}, "tf.experimental.numpy.cross": {"description": "TensorFlow variant of NumPy&#39;s cross."}, "tf.experimental.numpy.cumprod": {"description": "TensorFlow variant of NumPy&#39;s cumprod."}, "tf.experimental.numpy.cumsum": {"description": "TensorFlow variant of NumPy&#39;s cumsum."}, "tf.experimental.numpy.deg2rad": {"description": "TensorFlow variant of NumPy&#39;s deg2rad."}, "tf.experimental.numpy.diag": {"description": "TensorFlow variant of NumPy&#39;s diag."}, "tf.experimental.numpy.diag_indices": {"description": "TensorFlow variant of NumPy&#39;s diag_indices."}, "tf.experimental.numpy.diagflat": {"description": "TensorFlow variant of NumPy&#39;s diagflat."}, "tf.experimental.numpy.diagonal": {"description": "TensorFlow variant of NumPy&#39;s diagonal."}, "tf.experimental.numpy.diff": {"description": "TensorFlow variant of NumPy&#39;s diff."}, "tf.experimental.numpy.divide": {"description": "TensorFlow variant of NumPy&#39;s divide."}, "tf.experimental.numpy.divmod": {"description": "TensorFlow variant of NumPy&#39;s divmod."}, "tf.experimental.numpy.dot": {"description": "TensorFlow variant of NumPy&#39;s dot."}, "tf.experimental.numpy.dsplit": {"description": "TensorFlow variant of NumPy&#39;s dsplit."}, "tf.experimental.numpy.dstack": {"description": "TensorFlow variant of NumPy&#39;s dstack."}, "tf.experimental.numpy.einsum": {"description": "TensorFlow variant of NumPy&#39;s einsum."}, "tf.experimental.numpy.empty": {"description": "TensorFlow variant of NumPy&#39;s empty."}, "tf.experimental.numpy.empty_like": {"description": "TensorFlow variant of NumPy&#39;s empty_like."}, "tf.experimental.numpy.equal": {"description": "TensorFlow variant of NumPy&#39;s equal."}, "tf.experimental.numpy.exp": {"description": "TensorFlow variant of NumPy&#39;s exp."}, "tf.experimental.numpy.exp2": {"description": "TensorFlow variant of NumPy&#39;s exp2."}, "tf.experimental.numpy.expand_dims": {"description": "TensorFlow variant of NumPy&#39;s expand_dims."}, "tf.experimental.numpy.experimental_enable_numpy_behavior": {"description": "Enable NumPy behavior on Tensors.", "Args": {"prefer_float32": "Controls whether dtype inference will use float32\nfor Python floats, or float64 (the default and the\nNumPy-compatible behavior)."}}, "tf.experimental.numpy.expm1": {"description": "TensorFlow variant of NumPy&#39;s expm1."}, "tf.experimental.numpy.eye": {"description": "TensorFlow variant of NumPy&#39;s eye."}, "tf.experimental.numpy.fabs": {"description": "TensorFlow variant of NumPy&#39;s fabs."}, "tf.experimental.numpy.finfo": {"description": "TensorFlow variant of NumPy&#39;s finfo."}, "tf.experimental.numpy.fix": {"description": "TensorFlow variant of NumPy&#39;s fix."}, "tf.experimental.numpy.flip": {"description": "TensorFlow variant of NumPy&#39;s flip."}, "tf.experimental.numpy.fliplr": {"description": "TensorFlow variant of NumPy&#39;s fliplr."}, "tf.experimental.numpy.flipud": {"description": "TensorFlow variant of NumPy&#39;s flipud."}, "tf.experimental.numpy.float16": {"description": "Half-precision floating-point number type.", "Attributes": {"T": "Scalar attribute identical to the corresponding array attribute.\nPlease see ndarray.T.", "base": "Scalar attribute identical to the corresponding array attribute.\nPlease see ndarray.base.", "data": "Pointer to start of data.", "dtype": "Get array data-descriptor.", "flags": "The integer value of flags.", "flat": "A 1-D view of the scalar.", "imag": "The imaginary part of the scalar.", "itemsize": "The length of one element in bytes.", "nbytes": "The length of the scalar in bytes.", "ndim": "The number of array dimensions.", "real": "The real part of the scalar.", "shape": "Tuple of array dimensions.", "size": "The number of elements in the gentype.", "strides": "Tuple of bytes steps in each dimension."}}, "tf.experimental.numpy.float32": {"description": "Single-precision floating-point number type, compatible with C float.", "Attributes": {"T": "Scalar attribute identical to the corresponding array attribute.\nPlease see ndarray.T.", "base": "Scalar attribute identical to the corresponding array attribute.\nPlease see ndarray.base.", "data": "Pointer to start of data.", "dtype": "Get array data-descriptor.", "flags": "The integer value of flags.", "flat": "A 1-D view of the scalar.", "imag": "The imaginary part of the scalar.", "itemsize": "The length of one element in bytes.", "nbytes": "The length of the scalar in bytes.", "ndim": "The number of array dimensions.", "real": "The real part of the scalar.", "shape": "Tuple of array dimensions.", "size": "The number of elements in the gentype.", "strides": "Tuple of bytes steps in each dimension."}}, "tf.experimental.numpy.float64": {"description": "Double-precision floating-point number type, compatible with Python float", "Attributes": {"T": "Scalar attribute identical to the corresponding array attribute.\nPlease see ndarray.T.", "base": "Scalar attribute identical to the corresponding array attribute.\nPlease see ndarray.base.", "data": "Pointer to start of data.", "dtype": "Get array data-descriptor.", "flags": "The integer value of flags.", "flat": "A 1-D view of the scalar.", "imag": "The imaginary part of the scalar.", "itemsize": "The length of one element in bytes.", "nbytes": "The length of the scalar in bytes.", "ndim": "The number of array dimensions.", "real": "The real part of the scalar.", "shape": "Tuple of array dimensions.", "size": "The number of elements in the gentype.", "strides": "Tuple of bytes steps in each dimension."}}, "tf.experimental.numpy.float_power": {"description": "TensorFlow variant of NumPy&#39;s float_power."}, "tf.experimental.numpy.floor": {"description": "TensorFlow variant of NumPy&#39;s floor."}, "tf.experimental.numpy.floor_divide": {"description": "TensorFlow variant of NumPy&#39;s floor_divide."}, "tf.experimental.numpy.full": {"description": "TensorFlow variant of NumPy&#39;s full."}, "tf.experimental.numpy.full_like": {"description": "TensorFlow variant of NumPy&#39;s full_like."}, "tf.experimental.numpy.gcd": {"description": "TensorFlow variant of NumPy&#39;s gcd."}, "tf.experimental.numpy.geomspace": {"description": "TensorFlow variant of NumPy&#39;s geomspace."}, "tf.experimental.numpy.greater": {"description": "TensorFlow variant of NumPy&#39;s greater."}, "tf.experimental.numpy.greater_equal": {"description": "TensorFlow variant of NumPy&#39;s greater_equal."}, "tf.experimental.numpy.heaviside": {"description": "TensorFlow variant of NumPy&#39;s heaviside."}, "tf.experimental.numpy.hsplit": {"description": "TensorFlow variant of NumPy&#39;s hsplit."}, "tf.experimental.numpy.hstack": {"description": "TensorFlow variant of NumPy&#39;s hstack."}, "tf.experimental.numpy.hypot": {"description": "TensorFlow variant of NumPy&#39;s hypot."}, "tf.experimental.numpy.identity": {"description": "TensorFlow variant of NumPy&#39;s identity."}, "tf.experimental.numpy.iinfo": {"description": "iinfo(type)", "Attributes": {"max": "Maximum value of given dtype.", "min": "Minimum value of given dtype."}}, "tf.experimental.numpy.imag": {"description": "TensorFlow variant of NumPy&#39;s imag."}, "tf.experimental.numpy.inexact": {"description": "Abstract base class of all numeric scalar types with a (potentially)", "Attributes": {"T": "Scalar attribute identical to the corresponding array attribute.\nPlease see ndarray.T.", "base": "Scalar attribute identical to the corresponding array attribute.\nPlease see ndarray.base.", "data": "Pointer to start of data.", "dtype": "Get array data-descriptor.", "flags": "The integer value of flags.", "flat": "A 1-D view of the scalar.", "imag": "The imaginary part of the scalar.", "itemsize": "The length of one element in bytes.", "nbytes": "The length of the scalar in bytes.", "ndim": "The number of array dimensions.", "real": "The real part of the scalar.", "shape": "Tuple of array dimensions.", "size": "The number of elements in the gentype.", "strides": "Tuple of bytes steps in each dimension."}}, "tf.experimental.numpy.inner": {"description": "TensorFlow variant of NumPy&#39;s inner."}, "tf.experimental.numpy.int16": {"description": "Signed integer type, compatible with C short.", "Attributes": {"T": "Scalar attribute identical to the corresponding array attribute.\nPlease see ndarray.T.", "base": "Scalar attribute identical to the corresponding array attribute.\nPlease see ndarray.base.", "data": "Pointer to start of data.", "denominator": "denominator of value (1)", "dtype": "Get array data-descriptor.", "flags": "The integer value of flags.", "flat": "A 1-D view of the scalar.", "imag": "The imaginary part of the scalar.", "itemsize": "The length of one element in bytes.", "nbytes": "The length of the scalar in bytes.", "ndim": "The number of array dimensions.", "numerator": "numerator of value (the value itself)", "real": "The real part of the scalar.", "shape": "Tuple of array dimensions.", "size": "The number of elements in the gentype.", "strides": "Tuple of bytes steps in each dimension."}}, "tf.experimental.numpy.int32": {"description": "Signed integer type, compatible with C int.", "Attributes": {"T": "Scalar attribute identical to the corresponding array attribute.\nPlease see ndarray.T.", "base": "Scalar attribute identical to the corresponding array attribute.\nPlease see ndarray.base.", "data": "Pointer to start of data.", "denominator": "denominator of value (1)", "dtype": "Get array data-descriptor.", "flags": "The integer value of flags.", "flat": "A 1-D view of the scalar.", "imag": "The imaginary part of the scalar.", "itemsize": "The length of one element in bytes.", "nbytes": "The length of the scalar in bytes.", "ndim": "The number of array dimensions.", "numerator": "numerator of value (the value itself)", "real": "The real part of the scalar.", "shape": "Tuple of array dimensions.", "size": "The number of elements in the gentype.", "strides": "Tuple of bytes steps in each dimension."}}, "tf.experimental.numpy.int64": {"description": "Signed integer type, compatible with Python int and C long.", "Attributes": {"T": "Scalar attribute identical to the corresponding array attribute.\nPlease see ndarray.T.", "base": "Scalar attribute identical to the corresponding array attribute.\nPlease see ndarray.base.", "data": "Pointer to start of data.", "denominator": "denominator of value (1)", "dtype": "Get array data-descriptor.", "flags": "The integer value of flags.", "flat": "A 1-D view of the scalar.", "imag": "The imaginary part of the scalar.", "itemsize": "The length of one element in bytes.", "nbytes": "The length of the scalar in bytes.", "ndim": "The number of array dimensions.", "numerator": "numerator of value (the value itself)", "real": "The real part of the scalar.", "shape": "Tuple of array dimensions.", "size": "The number of elements in the gentype.", "strides": "Tuple of bytes steps in each dimension."}}, "tf.experimental.numpy.int8": {"description": "Signed integer type, compatible with C char.", "Attributes": {"T": "Scalar attribute identical to the corresponding array attribute.\nPlease see ndarray.T.", "base": "Scalar attribute identical to the corresponding array attribute.\nPlease see ndarray.base.", "data": "Pointer to start of data.", "denominator": "denominator of value (1)", "dtype": "Get array data-descriptor.", "flags": "The integer value of flags.", "flat": "A 1-D view of the scalar.", "imag": "The imaginary part of the scalar.", "itemsize": "The length of one element in bytes.", "nbytes": "The length of the scalar in bytes.", "ndim": "The number of array dimensions.", "numerator": "numerator of value (the value itself)", "real": "The real part of the scalar.", "shape": "Tuple of array dimensions.", "size": "The number of elements in the gentype.", "strides": "Tuple of bytes steps in each dimension."}}, "tf.experimental.numpy.isclose": {"description": "TensorFlow variant of NumPy&#39;s isclose."}, "tf.experimental.numpy.iscomplex": {"description": "TensorFlow variant of NumPy&#39;s iscomplex."}, "tf.experimental.numpy.iscomplexobj": {"description": "TensorFlow variant of NumPy&#39;s iscomplexobj."}, "tf.experimental.numpy.isfinite": {"description": "TensorFlow variant of NumPy&#39;s isfinite."}, "tf.experimental.numpy.isinf": {"description": "TensorFlow variant of NumPy&#39;s isinf."}, "tf.experimental.numpy.isnan": {"description": "TensorFlow variant of NumPy&#39;s isnan."}, "tf.experimental.numpy.isneginf": {"description": "TensorFlow variant of NumPy&#39;s isneginf."}, "tf.experimental.numpy.isposinf": {"description": "TensorFlow variant of NumPy&#39;s isposinf."}, "tf.experimental.numpy.isreal": {"description": "TensorFlow variant of NumPy&#39;s isreal."}, "tf.experimental.numpy.isrealobj": {"description": "TensorFlow variant of NumPy&#39;s isrealobj."}, "tf.experimental.numpy.isscalar": {"description": "TensorFlow variant of NumPy&#39;s isscalar."}, "tf.experimental.numpy.issubdtype": {"description": "Returns True if first argument is a typecode lower/equal in type hierarchy."}, "tf.experimental.numpy.ix_": {"description": "TensorFlow variant of NumPy&#39;s ix_."}, "tf.experimental.numpy.kron": {"description": "TensorFlow variant of NumPy&#39;s kron."}, "tf.experimental.numpy.lcm": {"description": "TensorFlow variant of NumPy&#39;s lcm."}, "tf.experimental.numpy.less": {"description": "TensorFlow variant of NumPy&#39;s less."}, "tf.experimental.numpy.less_equal": {"description": "TensorFlow variant of NumPy&#39;s less_equal."}, "tf.experimental.numpy.linspace": {"description": "TensorFlow variant of NumPy&#39;s linspace."}, "tf.experimental.numpy.log": {"description": "TensorFlow variant of NumPy&#39;s log."}, "tf.experimental.numpy.log10": {"description": "TensorFlow variant of NumPy&#39;s log10."}, "tf.experimental.numpy.log1p": {"description": "TensorFlow variant of NumPy&#39;s log1p."}, "tf.experimental.numpy.log2": {"description": "TensorFlow variant of NumPy&#39;s log2."}, "tf.experimental.numpy.logaddexp": {"description": "TensorFlow variant of NumPy&#39;s logaddexp."}, "tf.experimental.numpy.logaddexp2": {"description": "TensorFlow variant of NumPy&#39;s logaddexp2."}, "tf.experimental.numpy.logical_and": {"description": "TensorFlow variant of NumPy&#39;s logical_and."}, "tf.experimental.numpy.logical_not": {"description": "TensorFlow variant of NumPy&#39;s logical_not."}, "tf.experimental.numpy.logical_or": {"description": "TensorFlow variant of NumPy&#39;s logical_or."}, "tf.experimental.numpy.logical_xor": {"description": "TensorFlow variant of NumPy&#39;s logical_xor."}, "tf.experimental.numpy.logspace": {"description": "TensorFlow variant of NumPy&#39;s logspace."}, "tf.experimental.numpy.matmul": {"description": "TensorFlow variant of NumPy&#39;s matmul."}, "tf.experimental.numpy.max": {"description": "TensorFlow variant of NumPy&#39;s max."}, "tf.experimental.numpy.maximum": {"description": "TensorFlow variant of NumPy&#39;s maximum."}, "tf.experimental.numpy.mean": {"description": "TensorFlow variant of NumPy&#39;s mean."}, "tf.experimental.numpy.meshgrid": {"description": "TensorFlow variant of NumPy&#39;s meshgrid."}, "tf.experimental.numpy.min": {"description": "TensorFlow variant of NumPy&#39;s min."}, "tf.experimental.numpy.minimum": {"description": "TensorFlow variant of NumPy&#39;s minimum."}, "tf.experimental.numpy.mod": {"description": "TensorFlow variant of NumPy&#39;s mod."}, "tf.experimental.numpy.moveaxis": {"description": "TensorFlow variant of NumPy&#39;s moveaxis."}, "tf.experimental.numpy.multiply": {"description": "TensorFlow variant of NumPy&#39;s multiply."}, "tf.experimental.numpy.nanmean": {"description": "TensorFlow variant of NumPy&#39;s nanmean."}, "tf.experimental.numpy.nanprod": {"description": "TensorFlow variant of NumPy&#39;s nanprod."}, "tf.experimental.numpy.nansum": {"description": "TensorFlow variant of NumPy&#39;s nansum."}, "tf.experimental.numpy.ndim": {"description": "TensorFlow variant of NumPy&#39;s ndim."}, "tf.experimental.numpy.negative": {"description": "TensorFlow variant of NumPy&#39;s negative."}, "tf.experimental.numpy.nextafter": {"description": "TensorFlow variant of NumPy&#39;s nextafter."}, "tf.experimental.numpy.nonzero": {"description": "TensorFlow variant of NumPy&#39;s nonzero."}, "tf.experimental.numpy.not_equal": {"description": "TensorFlow variant of NumPy&#39;s not_equal."}, "tf.experimental.numpy.object_": {"description": "Any Python object.", "Attributes": {"T": "Scalar attribute identical to the corresponding array attribute.\nPlease see ndarray.T.", "base": "Scalar attribute identical to the corresponding array attribute.\nPlease see ndarray.base.", "data": "Pointer to start of data.", "dtype": "Get array data-descriptor.", "flags": "The integer value of flags.", "flat": "A 1-D view of the scalar.", "imag": "The imaginary part of the scalar.", "itemsize": "The length of one element in bytes.", "nbytes": "The length of the scalar in bytes.", "ndim": "The number of array dimensions.", "real": "The real part of the scalar.", "shape": "Tuple of array dimensions.", "size": "The number of elements in the gentype.", "strides": "Tuple of bytes steps in each dimension."}}, "tf.experimental.numpy.ones": {"description": "TensorFlow variant of NumPy&#39;s ones."}, "tf.experimental.numpy.ones_like": {"description": "TensorFlow variant of NumPy&#39;s ones_like."}, "tf.experimental.numpy.outer": {"description": "TensorFlow variant of NumPy&#39;s outer."}, "tf.experimental.numpy.pad": {"description": "TensorFlow variant of NumPy&#39;s pad."}, "tf.experimental.numpy.polyval": {"description": "TensorFlow variant of NumPy&#39;s polyval."}, "tf.experimental.numpy.positive": {"description": "TensorFlow variant of NumPy&#39;s positive."}, "tf.experimental.numpy.power": {"description": "TensorFlow variant of NumPy&#39;s power."}, "tf.experimental.numpy.prod": {"description": "TensorFlow variant of NumPy&#39;s prod."}, "tf.experimental.numpy.promote_types": {"description": "TensorFlow variant of NumPy&#39;s promote_types."}, "tf.experimental.numpy.ptp": {"description": "TensorFlow variant of NumPy&#39;s ptp."}, "tf.experimental.numpy.rad2deg": {"description": "TensorFlow variant of NumPy&#39;s rad2deg."}, "tf.experimental.numpy.ravel": {"description": "TensorFlow variant of NumPy&#39;s ravel."}, "tf.experimental.numpy.real": {"description": "TensorFlow variant of NumPy&#39;s real."}, "tf.experimental.numpy.reciprocal": {"description": "TensorFlow variant of NumPy&#39;s reciprocal."}, "tf.experimental.numpy.remainder": {"description": "TensorFlow variant of NumPy&#39;s remainder."}, "tf.experimental.numpy.repeat": {"description": "TensorFlow variant of NumPy&#39;s repeat."}, "tf.experimental.numpy.reshape": {"description": "TensorFlow variant of NumPy&#39;s reshape."}, "tf.experimental.numpy.result_type": {"description": "TensorFlow variant of NumPy&#39;s result_type."}, "tf.experimental.numpy.roll": {"description": "TensorFlow variant of NumPy&#39;s roll."}, "tf.experimental.numpy.rot90": {"description": "TensorFlow variant of NumPy&#39;s rot90."}, "tf.experimental.numpy.round": {"description": "TensorFlow variant of NumPy&#39;s round."}, "tf.experimental.numpy.select": {"description": "TensorFlow variant of NumPy&#39;s select."}, "tf.experimental.numpy.shape": {"description": "TensorFlow variant of NumPy&#39;s shape."}, "tf.experimental.numpy.sign": {"description": "TensorFlow variant of NumPy&#39;s sign."}, "tf.experimental.numpy.signbit": {"description": "TensorFlow variant of NumPy&#39;s signbit."}, "tf.experimental.numpy.sin": {"description": "TensorFlow variant of NumPy&#39;s sin."}, "tf.experimental.numpy.sinc": {"description": "TensorFlow variant of NumPy&#39;s sinc."}, "tf.experimental.numpy.sinh": {"description": "TensorFlow variant of NumPy&#39;s sinh."}, "tf.experimental.numpy.size": {"description": "TensorFlow variant of NumPy&#39;s size."}, "tf.experimental.numpy.sort": {"description": "TensorFlow variant of NumPy&#39;s sort."}, "tf.experimental.numpy.split": {"description": "TensorFlow variant of NumPy&#39;s split."}, "tf.experimental.numpy.sqrt": {"description": "TensorFlow variant of NumPy&#39;s sqrt."}, "tf.experimental.numpy.square": {"description": "TensorFlow variant of NumPy&#39;s square."}, "tf.experimental.numpy.squeeze": {"description": "TensorFlow variant of NumPy&#39;s squeeze."}, "tf.experimental.numpy.stack": {"description": "TensorFlow variant of NumPy&#39;s stack."}, "tf.experimental.numpy.std": {"description": "TensorFlow variant of NumPy&#39;s std."}, "tf.experimental.numpy.string_": {"description": "A byte string.", "Attributes": {"T": "Scalar attribute identical to the corresponding array attribute.\nPlease see ndarray.T.", "base": "Scalar attribute identical to the corresponding array attribute.\nPlease see ndarray.base.", "data": "Pointer to start of data.", "dtype": "Get array data-descriptor.", "flags": "The integer value of flags.", "flat": "A 1-D view of the scalar.", "imag": "The imaginary part of the scalar.", "itemsize": "The length of one element in bytes.", "nbytes": "The length of the scalar in bytes.", "ndim": "The number of array dimensions.", "real": "The real part of the scalar.", "shape": "Tuple of array dimensions.", "size": "The number of elements in the gentype.", "strides": "Tuple of bytes steps in each dimension."}}, "tf.experimental.numpy.subtract": {"description": "TensorFlow variant of NumPy&#39;s subtract."}, "tf.experimental.numpy.sum": {"description": "TensorFlow variant of NumPy&#39;s sum."}, "tf.experimental.numpy.swapaxes": {"description": "TensorFlow variant of NumPy&#39;s swapaxes."}, "tf.experimental.numpy.take": {"description": "TensorFlow variant of NumPy&#39;s take."}, "tf.experimental.numpy.take_along_axis": {"description": "TensorFlow variant of NumPy&#39;s take_along_axis."}, "tf.experimental.numpy.tan": {"description": "TensorFlow variant of NumPy&#39;s tan."}, "tf.experimental.numpy.tanh": {"description": "TensorFlow variant of NumPy&#39;s tanh."}, "tf.experimental.numpy.tensordot": {"description": "TensorFlow variant of NumPy&#39;s tensordot."}, "tf.experimental.numpy.tile": {"description": "TensorFlow variant of NumPy&#39;s tile."}, "tf.experimental.numpy.trace": {"description": "TensorFlow variant of NumPy&#39;s trace."}, "tf.experimental.numpy.transpose": {"description": "TensorFlow variant of NumPy&#39;s transpose."}, "tf.experimental.numpy.tri": {"description": "TensorFlow variant of NumPy&#39;s tri."}, "tf.experimental.numpy.tril": {"description": "TensorFlow variant of NumPy&#39;s tril."}, "tf.experimental.numpy.triu": {"description": "TensorFlow variant of NumPy&#39;s triu."}, "tf.experimental.numpy.true_divide": {"description": "TensorFlow variant of NumPy&#39;s true_divide."}, "tf.experimental.numpy.uint16": {"description": "Unsigned integer type, compatible with C unsigned short.", "Attributes": {"T": "Scalar attribute identical to the corresponding array attribute.\nPlease see ndarray.T.", "base": "Scalar attribute identical to the corresponding array attribute.\nPlease see ndarray.base.", "data": "Pointer to start of data.", "denominator": "denominator of value (1)", "dtype": "Get array data-descriptor.", "flags": "The integer value of flags.", "flat": "A 1-D view of the scalar.", "imag": "The imaginary part of the scalar.", "itemsize": "The length of one element in bytes.", "nbytes": "The length of the scalar in bytes.", "ndim": "The number of array dimensions.", "numerator": "numerator of value (the value itself)", "real": "The real part of the scalar.", "shape": "Tuple of array dimensions.", "size": "The number of elements in the gentype.", "strides": "Tuple of bytes steps in each dimension."}}, "tf.experimental.numpy.uint32": {"description": "Unsigned integer type, compatible with C unsigned int.", "Attributes": {"T": "Scalar attribute identical to the corresponding array attribute.\nPlease see ndarray.T.", "base": "Scalar attribute identical to the corresponding array attribute.\nPlease see ndarray.base.", "data": "Pointer to start of data.", "denominator": "denominator of value (1)", "dtype": "Get array data-descriptor.", "flags": "The integer value of flags.", "flat": "A 1-D view of the scalar.", "imag": "The imaginary part of the scalar.", "itemsize": "The length of one element in bytes.", "nbytes": "The length of the scalar in bytes.", "ndim": "The number of array dimensions.", "numerator": "numerator of value (the value itself)", "real": "The real part of the scalar.", "shape": "Tuple of array dimensions.", "size": "The number of elements in the gentype.", "strides": "Tuple of bytes steps in each dimension."}}, "tf.experimental.numpy.uint64": {"description": "Unsigned integer type, compatible with C unsigned long.", "Attributes": {"T": "Scalar attribute identical to the corresponding array attribute.\nPlease see ndarray.T.", "base": "Scalar attribute identical to the corresponding array attribute.\nPlease see ndarray.base.", "data": "Pointer to start of data.", "denominator": "denominator of value (1)", "dtype": "Get array data-descriptor.", "flags": "The integer value of flags.", "flat": "A 1-D view of the scalar.", "imag": "The imaginary part of the scalar.", "itemsize": "The length of one element in bytes.", "nbytes": "The length of the scalar in bytes.", "ndim": "The number of array dimensions.", "numerator": "numerator of value (the value itself)", "real": "The real part of the scalar.", "shape": "Tuple of array dimensions.", "size": "The number of elements in the gentype.", "strides": "Tuple of bytes steps in each dimension."}}, "tf.experimental.numpy.uint8": {"description": "Unsigned integer type, compatible with C unsigned char.", "Attributes": {"T": "Scalar attribute identical to the corresponding array attribute.\nPlease see ndarray.T.", "base": "Scalar attribute identical to the corresponding array attribute.\nPlease see ndarray.base.", "data": "Pointer to start of data.", "denominator": "denominator of value (1)", "dtype": "Get array data-descriptor.", "flags": "The integer value of flags.", "flat": "A 1-D view of the scalar.", "imag": "The imaginary part of the scalar.", "itemsize": "The length of one element in bytes.", "nbytes": "The length of the scalar in bytes.", "ndim": "The number of array dimensions.", "numerator": "numerator of value (the value itself)", "real": "The real part of the scalar.", "shape": "Tuple of array dimensions.", "size": "The number of elements in the gentype.", "strides": "Tuple of bytes steps in each dimension."}}, "tf.experimental.numpy.unicode_": {"description": "A unicode string.", "Attributes": {"T": "Scalar attribute identical to the corresponding array attribute.\nPlease see ndarray.T.", "base": "Scalar attribute identical to the corresponding array attribute.\nPlease see ndarray.base.", "data": "Pointer to start of data.", "dtype": "Get array data-descriptor.", "flags": "The integer value of flags.", "flat": "A 1-D view of the scalar.", "imag": "The imaginary part of the scalar.", "itemsize": "The length of one element in bytes.", "nbytes": "The length of the scalar in bytes.", "ndim": "The number of array dimensions.", "real": "The real part of the scalar.", "shape": "Tuple of array dimensions.", "size": "The number of elements in the gentype.", "strides": "Tuple of bytes steps in each dimension."}}, "tf.experimental.numpy.vander": {"description": "TensorFlow variant of NumPy&#39;s vander."}, "tf.experimental.numpy.var": {"description": "TensorFlow variant of NumPy&#39;s var."}, "tf.experimental.numpy.vdot": {"description": "TensorFlow variant of NumPy&#39;s vdot."}, "tf.experimental.numpy.vsplit": {"description": "TensorFlow variant of NumPy&#39;s vsplit."}, "tf.experimental.numpy.vstack": {"description": "TensorFlow variant of NumPy&#39;s vstack."}, "tf.experimental.numpy.where": {"description": "TensorFlow variant of NumPy&#39;s where."}, "tf.experimental.numpy.zeros": {"description": "TensorFlow variant of NumPy&#39;s zeros."}, "tf.experimental.numpy.zeros_like": {"description": "TensorFlow variant of NumPy&#39;s zeros_like."}}, "tf.experimental.numpy.random": {"tf.experimental.numpy.random.poisson": {"description": "TensorFlow variant of NumPy&#39;s random.poisson."}, "tf.experimental.numpy.random.rand": {"description": "TensorFlow variant of NumPy&#39;s random.rand."}, "tf.experimental.numpy.random.randint": {"description": "TensorFlow variant of NumPy&#39;s random.randint."}, "tf.experimental.numpy.random.randn": {"description": "TensorFlow variant of NumPy&#39;s random.randn."}, "tf.experimental.numpy.random.random": {"description": "TensorFlow variant of NumPy&#39;s random.random."}, "tf.experimental.numpy.random.seed": {"description": "TensorFlow variant of NumPy&#39;s random.seed."}, "tf.experimental.numpy.random.standard_normal": {"description": "TensorFlow variant of NumPy&#39;s random.standard_normal."}, "tf.experimental.numpy.random.uniform": {"description": "TensorFlow variant of NumPy&#39;s random.uniform."}}, "tf.experimental.tensorrt": {"tf.experimental.tensorrt.ConversionParams": {"description": "Parameters that are used for TF-TRT conversion.", "Attributes": {"max_workspace_size_bytes": "A namedtuple alias for field number 0", "precision_mode": "A namedtuple alias for field number 1", "minimum_segment_size": "A namedtuple alias for field number 2", "maximum_cached_engines": "A namedtuple alias for field number 3", "use_calibration": "A namedtuple alias for field number 4", "allow_build_at_runtime": "A namedtuple alias for field number 5"}}, "tf.experimental.tensorrt.Converter": {"description": "An offline converter for TF-TRT transformation for TF 2.0 SavedModels.", "Args": {"input_saved_model_dir": "the directory to load the SavedModel which contains\nthe input graph to transforms. Required.", "input_saved_model_tags": "list of tags to load the SavedModel.", "input_saved_model_signature_key": "the key of the signature to optimize the\ngraph for.", "use_dynamic_shape": "whether to enable dynamic shape support. None is\nequivalent to False in the current implementation.", "dynamic_shape_profile_strategy": "one of the strings in\nsupported_profile_strategies(). None is equivalent to Range in the\ncurrent implementation.", "max_workspace_size_bytes": "the maximum GPU temporary memory that the TRT\nengine can use at execution time. This corresponds to the\n'workspaceSize' parameter of nvinfer1::IBuilder::setMaxWorkspaceSize().", "precision_mode": "one of the strings in\nTrtPrecisionMode.supported_precision_modes().", "minimum_segment_size": "the minimum number of nodes required for a subgraph\nto be replaced by TRTEngineOp.", "maximum_cached_engines": "max number of cached TRT engines for dynamic TRT\nops. Created TRT engines for a dynamic dimension are cached. If the\nnumber of cached engines is already at max but none of them supports the\ninput shapes, the TRTEngineOp will fall back to run the original TF\nsubgraph that corresponds to the TRTEngineOp.", "use_calibration": "this argument is ignored if precision_mode is not INT8.\nIf set to True, a calibration graph will be created to calibrate the\nmissing ranges. The calibration graph must be converted to an inference\ngraph by running calibration with calibrate(). If set to False,\nquantization nodes will be expected for every tensor in the graph\n(excluding those which will be fused). If a range is missing, an error\nwill occur. Please note that accuracy may be negatively affected if\nthere is a mismatch between which tensors TRT quantizes and which\ntensors were trained with fake quantization.", "allow_build_at_runtime": "whether to allow building TensorRT engines during\nruntime if no prebuilt TensorRT engine can be found that can handle the\ngiven inputs during runtime, then a new TensorRT engine is built at\nruntime if allow_build_at_runtime=True, and otherwise native TF is used.", "conversion_params": "a TrtConversionParams instance (deprecated)."}, "Raises": {"ValueError": "if the combination of the parameters is invalid."}}}, "tf.graph_util": {"tf.graph_util.import_graph_def": {"description": "Imports the graph from graph_def into the current default Graph. (deprecated arguments)", "Args": {"graph_def": "A GraphDef proto containing operations to be imported into\nthe default graph.", "input_map": "A dictionary mapping input names (as strings) in graph_def\nto Tensor objects. The values of the named input tensors in the\nimported graph will be re-mapped to the respective Tensor values.", "return_elements": "A list of strings containing operation names in\ngraph_def that will be returned as Operation objects; and/or\ntensor names in graph_def that will be returned as Tensor objects.", "name": "(Optional.) A prefix that will be prepended to the names in\ngraph_def. Note that this does not apply to imported function names.\nDefaults to \"import\".", "op_dict": "(Optional.) Deprecated, do not use.", "producer_op_list": "(Optional.) An OpList proto with the (possibly stripped)\nlist of OpDefs used by the producer of the graph. If provided,\nunrecognized attrs for ops in graph_def that have their default value\naccording to producer_op_list will be removed. This will allow some more\nGraphDefs produced by later binaries to be accepted by earlier binaries."}, "Returns": "A list of Operation and/or Tensor objects from the imported graph,\ncorresponding to the names in return_elements,\nand None if returns_elements is None.", "Raises": {"TypeError": "If graph_def is not a GraphDef proto,\ninput_map is not a dictionary mapping strings to Tensor objects,\nor return_elements is not a list of strings.", "ValueError": "If input_map, or return_elements contains names that\ndo not appear in graph_def, or graph_def is not well-formed (e.g.\nit refers to an unknown tensor)."}}}, "tf.image": {"tf.image.ResizeMethod": {"description": "See tf.image.resize for details."}, "tf.image.adjust_brightness": {"description": "Adjust the brightness of RGB or Grayscale images.", "Args": {"image": "RGB image or images to adjust.", "delta": "A scalar. Amount to add to the pixel values."}, "Returns": "A brightness-adjusted tensor of the same shape and type as image."}, "tf.image.adjust_contrast": {"description": "Adjust contrast of RGB or grayscale images.", "Args": {"images": "Images to adjust.  At least 3-D.", "contrast_factor": "A float multiplier for adjusting contrast."}, "Returns": "The contrast-adjusted image or images."}, "tf.image.adjust_gamma": {"description": "Performs [Gamma Correction](http://en.wikipedia.org/wiki/Gamma_correction).", "Args": {"image": "RGB image or images to adjust.", "gamma": "A scalar or tensor. Non-negative real number.", "gain": "A scalar or tensor. The constant multiplier."}, "Returns": "A Tensor. A Gamma-adjusted tensor of the same shape and type as image.", "Raises": {"ValueError": "If gamma is negative."}}, "tf.image.adjust_hue": {"description": "Adjust hue of RGB images.", "Args": {"image": "RGB image or images. The size of the last dimension must be 3.", "delta": "float.  How much to add to the hue channel.", "name": "A name for this operation (optional)."}, "Returns": "Adjusted image(s), same shape and DType as image.", "Raises": {"InvalidArgumentError": "The size of the last dimension must be 3.", "ValueError": "if delta is not in the interval of [-1, 1]."}}, "tf.image.adjust_jpeg_quality": {"description": "Adjust jpeg encoding quality of an image.", "Args": {"image": "3D image. The size of the last dimension must be None, 1 or 3.", "jpeg_quality": "Python int or Tensor of type int32. jpeg encoding quality.", "name": "A name for this operation (optional)."}, "Returns": "Adjusted image, same shape and DType as image.", "Raises": {"InvalidArgumentError": "image must have 1 or 3 channels"}}, "tf.image.adjust_saturation": {"description": "Adjust saturation of RGB images.", "Args": {"image": "RGB image or images. The size of the last dimension must be 3.", "saturation_factor": "float. Factor to multiply the saturation by.", "name": "A name for this operation (optional)."}, "Returns": "Adjusted image(s), same shape and DType as image.", "Raises": {"InvalidArgumentError": "input must have 3 channels"}}, "tf.image.central_crop": {"description": "Crop the central region of the image(s).", "Args": {"image": "Either a 3-D float Tensor of shape [height, width, depth], or a 4-D\nTensor of shape [batch_size, height, width, depth].", "central_fraction": "float (0, 1], fraction of size to crop"}, "Raises": {"ValueError": "if central_crop_fraction is not within (0, 1]."}, "Returns": "3-D / 4-D float Tensor, as per the input."}, "tf.image.combined_non_max_suppression": {"description": "Greedily selects a subset of bounding boxes in descending order of score.", "Args": {"boxes": "A 4-D float Tensor of shape [batch_size, num_boxes, q, 4]. If q\nis 1 then same boxes are used for all classes otherwise, if q is equal\nto number of classes, class-specific boxes are used.", "scores": "A 3-D float Tensor of shape [batch_size, num_boxes, num_classes]\nrepresenting a single score corresponding to each box (each row of boxes).", "max_output_size_per_class": "A scalar integer Tensor representing the\nmaximum number of boxes to be selected by non-max suppression per class", "max_total_size": "A int32 scalar representing maximum number of boxes retained\nover all classes. Note that setting this value to a large number may\nresult in OOM error depending on the system workload.", "iou_threshold": "A float representing the threshold for deciding whether boxes\noverlap too much with respect to IOU.", "score_threshold": "A float representing the threshold for deciding when to\nremove boxes based on score.", "pad_per_class": "If false, the output nmsed boxes, scores and classes are\npadded/clipped to max_total_size. If true, the output nmsed boxes,\nscores and classes are padded to be of length\nmax_size_per_class*num_classes, unless it exceeds max_total_size in\nwhich case it is clipped to max_total_size. Defaults to false.", "clip_boxes": "If true, the coordinates of output nmsed boxes will be clipped\nto [0, 1]. If false, output the box coordinates as it is. Defaults to\ntrue.", "name": "A name for the operation (optional)."}, "Returns": "'nmsed_boxes'\n\n\nA [batch_size, max_detections, 4] float32 tensor\ncontaining the non-max suppressed boxes."}, "tf.image.convert_image_dtype": {"description": "Convert image to dtype, scaling its values if needed.", "Args": {"image": "An image.", "dtype": "A DType to convert image to.", "saturate": "If True, clip the input before casting (if necessary).", "name": "A name for this operation (optional)."}, "Returns": "image, converted to dtype.", "Raises": {"AttributeError": "Raises an attribute error when dtype is neither\nfloat nor integer"}}, "tf.image.crop_and_resize": {"description": "Extracts crops from the input image tensor and resizes them.", "Args": {"image": "A 4-D tensor of shape [batch, image_height, image_width, depth].\nBoth image_height and image_width need to be positive.", "boxes": "A 2-D tensor of shape [num_boxes, 4]. The i-th row of the tensor\nspecifies the coordinates of a box in the box_ind[i] image and is\nspecified in normalized coordinates [y1, x1, y2, x2]. A normalized\ncoordinate value of y is mapped to the image coordinate at y *\n(image_height - 1), so as the [0, 1] interval of normalized image\nheight is mapped to [0, image_height - 1] in image height coordinates.\nWe do allow y1 > y2, in which case the sampled crop is an up-down\nflipped version of the original image. The width dimension is treated\nsimilarly. Normalized coordinates outside the [0, 1] range are allowed,\nin which case we use extrapolation_value to extrapolate the input image\nvalues.", "box_indices": "A 1-D tensor of shape [num_boxes] with int32 values in [0,\nbatch). The value of box_ind[i] specifies the image that the i-th box\nrefers to.", "crop_size": "A 1-D tensor of 2 elements, size = [crop_height, crop_width].\nAll cropped image patches are resized to this size. The aspect ratio of\nthe image content is not preserved. Both crop_height and crop_width\nneed to be positive.", "method": "An optional string specifying the sampling method for resizing. It\ncan be either \"bilinear\" or \"nearest\" and default to \"bilinear\".\nCurrently two sampling methods are supported: Bilinear and Nearest\n  Neighbor.", "extrapolation_value": "An optional float. Defaults to 0.0. Value used for\nextrapolation, when applicable.", "name": "A name for the operation (optional)."}, "Returns": "A 4-D tensor of shape [num_boxes, crop_height, crop_width, depth]."}, "tf.image.crop_to_bounding_box": {"description": "Crops an image to a specified bounding box.", "Args": {"image": "4-D Tensor of shape [batch, height, width, channels] or 3-D\nTensor of shape [height, width, channels].", "offset_height": "Vertical coordinate of the top-left corner of the bounding\nbox in image.", "offset_width": "Horizontal coordinate of the top-left corner of the bounding\nbox in image.", "target_height": "Height of the bounding box.", "target_width": "Width of the bounding box."}, "Returns": "If image was 4-D, a 4-D Tensor of shape\n[batch, target_height, target_width, channels].\nIf image was 3-D, a 3-D Tensor of shape\n[target_height, target_width, channels].\nIt has the same dtype with image.", "Raises": {"ValueError": "width < offset_width + target_width or\nheight < offset_height + target_height."}}, "tf.image.draw_bounding_boxes": {"description": "Draw bounding boxes on a batch of images.", "Args": {"images": "A Tensor. Must be one of the following types: float32, half.\n4-D with shape [batch, height, width, depth]. A batch of images.", "boxes": "A Tensor of type float32. 3-D with shape [batch,\nnum_bounding_boxes, 4] containing bounding boxes.", "colors": "A Tensor of type float32. 2-D. A list of RGBA colors to cycle\nthrough for the boxes.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as images."}, "tf.image.extract_glimpse": {"description": "Extracts a glimpse from the input tensor.", "Args": {"input": "A Tensor of type float32. A 4-D float tensor of shape\n[batch_size, height, width, channels].", "size": "A Tensor of type int32. A 1-D tensor of 2 elements containing the\nsize of the glimpses to extract.  The glimpse height must be specified\nfirst, following by the glimpse width.", "offsets": "A Tensor of type float32. A 2-D integer tensor of shape\n[batch_size, 2] containing the y, x locations of the center of each\nwindow.", "centered": "An optional bool. Defaults to True. indicates if the offset\ncoordinates are centered relative to the image, in which case the (0, 0)\noffset is relative to the center of the input images. If false, the (0,0)\noffset corresponds to the upper left corner of the input images.", "normalized": "An optional bool. Defaults to True. indicates if the offset\ncoordinates are normalized.", "noise": "An optional string. Defaults to uniform. indicates if the noise\nshould be uniform (uniform distribution), gaussian (gaussian\ndistribution), or zero (zero padding).", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type float32."}, "tf.image.extract_patches": {"description": "Extract patches from images.", "Args": {"images": "A 4-D Tensor with shape [batch, in_rows, in_cols, depth].", "sizes": "The size of the extracted patches. Must be\n[1, size_rows, size_cols, 1].", "strides": "A 1-D Tensor of length 4. How far the centers of two consecutive\npatches are in the images. Must be: [1, stride_rows, stride_cols, 1].", "rates": "A 1-D Tensor of length 4. Must be: [1, rate_rows, rate_cols, 1].\nThis is the input stride, specifying how far two consecutive patch samples\nare in the input. Equivalent to extracting patches with patch_sizes_eff =\npatch_sizes + (patch_sizes - 1) * (rates - 1), followed by subsampling\nthem spatially by a factor of rates. This is equivalent to rate in\ndilated (a.k.a. Atrous) convolutions.", "padding": "The type of padding algorithm to use.", "name": "A name for the operation (optional)."}, "Returns": "A 4-D Tensor of the same type as the input."}, "tf.image.flip_left_right": {"description": "Flip an image horizontally (left to right).", "Args": {"image": "4-D Tensor of shape [batch, height, width, channels] or 3-D Tensor\nof shape [height, width, channels]."}, "Returns": "A tensor of the same type and shape as image.", "Raises": {"ValueError": "if the shape of image not supported."}}, "tf.image.flip_up_down": {"description": "Flip an image vertically (upside down).", "Args": {"image": "4-D Tensor of shape [batch, height, width, channels] or 3-D Tensor\nof shape [height, width, channels]."}, "Returns": "A Tensor of the same type and shape as image.", "Raises": {"ValueError": "if the shape of image not supported."}}, "tf.image.generate_bounding_box_proposals": {"description": "Generate bounding box proposals from encoded bounding boxes.", "Args": {"scores": "A 4-D float Tensor of shape\n[num_images, height, width, num_achors] containing scores of\n the boxes for given anchors, can be unsorted.", "bbox_deltas": "A 4-D float Tensor of shape\n[num_images, height, width, 4 x num_anchors] encoding boxes\n with respect to each anchor. Coordinates are given\n in the form [dy, dx, dh, dw].", "image_info": "A 2-D float Tensor of shape [num_images, 5]\ncontaining image information Height, Width, Scale.", "anchors": "A 2-D float Tensor of shape [num_anchors, 4]\ndescribing the anchor boxes.\nBoxes are formatted in the form [y1, x1, y2, x2].", "nms_threshold": "A scalar float Tensor for non-maximal-suppression\nthreshold. Defaults to 0.7.", "pre_nms_topn": "A scalar int Tensor for the number of\ntop scoring boxes to be used as input. Defaults to 6000.", "min_size": "A scalar float Tensor. Any box that has a smaller size\nthan min_size will be discarded. Defaults to 16.", "post_nms_topn": "An integer. Maximum number of rois in the output.", "name": "A name for this operation (optional)."}, "Returns": "rois\n\n\nRegion of interest boxes sorted by their scores."}, "tf.image.grayscale_to_rgb": {"description": "Converts one or more images from Grayscale to RGB.", "Args": {"images": "The Grayscale tensor to convert. The last dimension must be size 1.", "name": "A name for the operation (optional)."}, "Returns": "The converted grayscale image(s)."}, "tf.image.hsv_to_rgb": {"description": "Convert one or more images from HSV to RGB.", "Args": {"images": "A Tensor. Must be one of the following types: half, bfloat16, float32, float64.\n1-D or higher rank. HSV data to convert. Last dimension must be size 3.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as images."}, "tf.image.image_gradients": {"description": "Returns image gradients (dy, dx) for each color channel.", "Args": {"image": "Tensor with shape [batch_size, h, w, d]."}, "Returns": "Pair of tensors (dy, dx) holding the vertical and horizontal image\ngradients (1-step finite difference).", "Raises": {"ValueError": "If image is not a 4D tensor."}}, "tf.image.non_max_suppression": {"description": "Greedily selects a subset of bounding boxes in descending order of score.", "Args": {"boxes": "A 2-D float Tensor of shape [num_boxes, 4].", "scores": "A 1-D float Tensor of shape [num_boxes] representing a single\nscore corresponding to each box (each row of boxes).", "max_output_size": "A scalar integer Tensor representing the maximum number\nof boxes to be selected by non-max suppression.", "iou_threshold": "A 0-D float tensor representing the threshold for deciding\nwhether boxes overlap too much with respect to IOU.", "score_threshold": "A 0-D float tensor representing the threshold for deciding\nwhen to remove boxes based on score.", "name": "A name for the operation (optional)."}, "Returns": "selected_indices\n\n\nA 1-D integer Tensor of shape [M] representing the\nselected indices from the boxes tensor, where M <= max_output_size."}, "tf.image.non_max_suppression_overlaps": {"description": "Greedily selects a subset of bounding boxes in descending order of score.", "Args": {"overlaps": "A 2-D float Tensor of shape [num_boxes, num_boxes]\nrepresenting the n-by-n box overlap values.", "scores": "A 1-D float Tensor of shape [num_boxes] representing a single\nscore corresponding to each box (each row of boxes).", "max_output_size": "A scalar integer Tensor representing the maximum number\nof boxes to be selected by non-max suppression.", "overlap_threshold": "A 0-D float tensor representing the threshold for\ndeciding whether boxes overlap too much with respect to the provided\noverlap values.", "score_threshold": "A 0-D float tensor representing the threshold for deciding\nwhen to remove boxes based on score.", "name": "A name for the operation (optional)."}, "Returns": "selected_indices\n\n\nA 1-D integer Tensor of shape [M] representing the\nselected indices from the overlaps tensor, where M <= max_output_size."}, "tf.image.non_max_suppression_padded": {"description": "Greedily selects a subset of bounding boxes in descending order of score.", "Args": {"boxes": "a tensor of rank 2 or higher with a shape of [..., num_boxes, 4].\nDimensions except the last two are batch dimensions.", "scores": "a tensor of rank 1 or higher with a shape of [..., num_boxes].", "max_output_size": "a scalar integer Tensor representing the maximum number\nof boxes to be selected by non max suppression. Note that setting this\nvalue to a large number may result in OOM error depending on the system\nworkload.", "iou_threshold": "a float representing the threshold for deciding whether boxes\noverlap too much with respect to IoU (intersection over union).", "score_threshold": "a float representing the threshold for box scores. Boxes\nwith a score that is not larger than this threshold will be suppressed.", "pad_to_max_output_size": "whether to pad the output idx to max_output_size.\nMust be set to True when the input is a batch of images.", "name": "name of operation.", "sorted_input": "a boolean indicating whether the input boxes and scores\nare sorted in descending order by the score.", "canonicalized_coordinates": "if box coordinates are given as\n[y_min, x_min, y_max, x_max], setting to True eliminate redundant\n computation to canonicalize box coordinates.", "tile_size": "an integer representing the number of boxes in a tile, i.e.,\nthe maximum number of boxes per image that can be used to suppress other\nboxes in parallel; larger tile_size means larger parallelism and\npotentially more redundant work."}, "Returns": "idx: a tensor with a shape of [..., num_boxes] representing the\n  indices selected by non-max suppression. The leading dimensions\n  are the batch dimensions of the input boxes. All numbers are within\n  [0, num_boxes). For each image (i.e., idx[i]), only the first num_valid[i]\n  indices (i.e., idx[i][:num_valid[i]]) are valid.\nnum_valid: a tensor of rank 0 or higher with a shape of [...]\n  representing the number of valid indices in idx. Its dimensions are the\n  batch dimensions of the input boxes."}, "tf.image.non_max_suppression_with_scores": {"description": "Greedily selects a subset of bounding boxes in descending order of score.", "Args": {"boxes": "A 2-D float Tensor of shape [num_boxes, 4].", "scores": "A 1-D float Tensor of shape [num_boxes] representing a single\nscore corresponding to each box (each row of boxes).", "max_output_size": "A scalar integer Tensor representing the maximum number\nof boxes to be selected by non-max suppression.", "iou_threshold": "A 0-D float tensor representing the threshold for deciding\nwhether boxes overlap too much with respect to IOU.", "score_threshold": "A 0-D float tensor representing the threshold for deciding\nwhen to remove boxes based on score.", "soft_nms_sigma": "A 0-D float tensor representing the sigma parameter for Soft\nNMS; see Bodla et al (c.f. https://arxiv.org/abs/1704.04503).  When\nsoft_nms_sigma=0.0 (which is default), we fall back to standard (hard)\nNMS.", "name": "A name for the operation (optional)."}, "Returns": "selected_indices\n\n\nA 1-D integer Tensor of shape [M] representing the\nselected indices from the boxes tensor, where M <= max_output_size."}, "tf.image.pad_to_bounding_box": {"description": "Pad image with zeros to the specified height and width.", "Args": {"image": "4-D Tensor of shape [batch, height, width, channels] or 3-D Tensor\nof shape [height, width, channels].", "offset_height": "Number of rows of zeros to add on top.", "offset_width": "Number of columns of zeros to add on the left.", "target_height": "Height of output image.", "target_width": "Width of output image."}, "Returns": "If image was 4-D, a 4-D float Tensor of shape\n[batch, target_height, target_width, channels]\nIf image was 3-D, a 3-D float Tensor of shape\n[target_height, target_width, channels]", "Raises": {"ValueError": "If the shape of image is incompatible with the offset_* or\ntarget_* arguments, or either offset_height or offset_width is\nnegative."}}, "tf.image.per_image_standardization": {"description": "Linearly scales each image in image to have mean 0 and variance 1.", "Args": {"image": "An n-D Tensor with at least 3 dimensions, the last 3 of which are\nthe dimensions of each image."}, "Returns": "A Tensor with the same shape as image and its dtype is float32.", "Raises": {"ValueError": "The shape of image has fewer than 3 dimensions."}}, "tf.image.psnr": {"description": "Returns the Peak Signal-to-Noise Ratio between a and b.", "Args": {"a": "First set of images.", "b": "Second set of images.", "max_val": "The dynamic range of the images (i.e., the difference between the\nmaximum the and minimum allowed values).", "name": "Namespace to embed the computation in."}, "Returns": "The scalar PSNR between a and b. The returned tensor has type tf.float32\nand shape [batch_size, 1]."}, "tf.image.random_brightness": {"description": "Adjust the brightness of images by a random factor.", "Args": {"image": "An image or images to adjust.", "max_delta": "float, must be non-negative.", "seed": "A Python integer. Used to create a random seed. See\ntf.compat.v1.set_random_seed for behavior."}, "Returns": "The brightness-adjusted image(s).", "Raises": {"ValueError": "if max_delta is negative."}}, "tf.image.random_contrast": {"description": "Adjust the contrast of an image or images by a random factor.", "Args": {"image": "An image tensor with 3 or more dimensions.", "lower": "float.  Lower bound for the random contrast factor.", "upper": "float.  Upper bound for the random contrast factor.", "seed": "A Python integer. Used to create a random seed. See\ntf.compat.v1.set_random_seed for behavior."}, "Returns": "The contrast-adjusted image(s).", "Raises": {"ValueError": "if upper <= lower or if lower < 0."}}, "tf.image.random_crop": {"description": "Randomly crops a tensor to a given size.", "Args": {"value": "Input tensor to crop.", "size": "1-D tensor with size the rank of value.", "seed": "Python integer. Used to create a random seed. See\ntf.random.set_seed\nfor behavior.", "name": "A name for this operation (optional)."}, "Returns": "A cropped tensor of the same rank as value and shape size."}, "tf.image.random_flip_left_right": {"description": "Randomly flip an image horizontally (left to right).", "Args": {"image": "4-D Tensor of shape [batch, height, width, channels] or 3-D Tensor\nof shape [height, width, channels].", "seed": "A Python integer. Used to create a random seed. See\ntf.compat.v1.set_random_seed for behavior."}, "Returns": "A tensor of the same type and shape as image.", "Raises": {"ValueError": "if the shape of image not supported."}}, "tf.image.random_flip_up_down": {"description": "Randomly flips an image vertically (upside down).", "Args": {"image": "4-D Tensor of shape [batch, height, width, channels] or 3-D Tensor\nof shape [height, width, channels].", "seed": "A Python integer. Used to create a random seed. See\ntf.compat.v1.set_random_seed for behavior."}, "Returns": "A tensor of the same type and shape as image.", "Raises": {"ValueError": "if the shape of image not supported."}}, "tf.image.random_hue": {"description": "Adjust the hue of RGB images by a random factor.", "Args": {"image": "RGB image or images. The size of the last dimension must be 3.", "max_delta": "float. The maximum value for the random delta.", "seed": "An operation-specific seed. It will be used in conjunction with the\ngraph-level seed to determine the real seeds that will be used in this\noperation. Please see the documentation of set_random_seed for its\ninteraction with the graph-level random seed."}, "Returns": "Adjusted image(s), same shape and DType as image.", "Raises": {"ValueError": "if max_delta is invalid."}}, "tf.image.random_jpeg_quality": {"description": "Randomly changes jpeg encoding quality for inducing jpeg noise.", "Args": {"image": "3D image. Size of the last dimension must be 1 or 3.", "min_jpeg_quality": "Minimum jpeg encoding quality to use.", "max_jpeg_quality": "Maximum jpeg encoding quality to use.", "seed": "An operation-specific seed. It will be used in conjunction with the\ngraph-level seed to determine the real seeds that will be used in this\noperation. Please see the documentation of set_random_seed for its\ninteraction with the graph-level random seed."}, "Returns": "Adjusted image(s), same shape and DType as image.", "Raises": {"ValueError": "if min_jpeg_quality or max_jpeg_quality is invalid."}}, "tf.image.random_saturation": {"description": "Adjust the saturation of RGB images by a random factor.", "Args": {"image": "RGB image or images. The size of the last dimension must be 3.", "lower": "float.  Lower bound for the random saturation factor.", "upper": "float.  Upper bound for the random saturation factor.", "seed": "An operation-specific seed. It will be used in conjunction with the\ngraph-level seed to determine the real seeds that will be used in this\noperation. Please see the documentation of set_random_seed for its\ninteraction with the graph-level random seed."}, "Returns": "Adjusted image(s), same shape and DType as image.", "Raises": {"ValueError": "if upper <= lower or if lower < 0."}}, "tf.image.resize": {"description": "Resize images to size using the specified method.", "Args": {"images": "4-D Tensor of shape [batch, height, width, channels] or 3-D Tensor\nof shape [height, width, channels].", "size": "A 1-D int32 Tensor of 2 elements: new_height, new_width.  The new\nsize for the images.", "method": "An image.ResizeMethod, or string equivalent.  Defaults to\nbilinear.", "preserve_aspect_ratio": "Whether to preserve the aspect ratio. If this is set,\nthen images will be resized to a size that fits in size while\npreserving the aspect ratio of the original image. Scales up the image if\nsize is bigger than the current size of the image. Defaults to False.", "antialias": "Whether to use an anti-aliasing filter when downsampling an\nimage.", "name": "A name for this operation (optional)."}, "Raises": {"ValueError": "if an unsupported resize method is specified."}, "Returns": "If images was 4-D, a 4-D float Tensor of shape\n[batch, new_height, new_width, channels].\nIf images was 3-D, a 3-D float Tensor of shape\n[new_height, new_width, channels]."}, "tf.image.resize_with_crop_or_pad": {"description": "Crops and/or pads an image to a target width and height.", "Args": {"image": "4-D Tensor of shape [batch, height, width, channels] or 3-D Tensor\nof shape [height, width, channels].", "target_height": "Target height.", "target_width": "Target width."}, "Raises": {"ValueError": "if target_height or target_width are zero or negative."}, "Returns": "Cropped and/or padded image.\nIf images was 4-D, a 4-D float Tensor of shape\n[batch, new_height, new_width, channels].\nIf images was 3-D, a 3-D float Tensor of shape\n[new_height, new_width, channels]."}, "tf.image.resize_with_pad": {"description": "Resizes and pads an image to a target width and height.", "Args": {"image": "4-D Tensor of shape [batch, height, width, channels] or 3-D Tensor\nof shape [height, width, channels].", "target_height": "Target height.", "target_width": "Target width.", "method": "Method to use for resizing image. See image.resize()", "antialias": "Whether to use anti-aliasing when resizing. See 'image.resize()'."}, "Raises": {"ValueError": "if target_height or target_width are zero or negative."}, "Returns": "Resized and padded image.\nIf images was 4-D, a 4-D float Tensor of shape\n[batch, new_height, new_width, channels].\nIf images was 3-D, a 3-D float Tensor of shape\n[new_height, new_width, channels]."}, "tf.image.rgb_to_grayscale": {"description": "Converts one or more images from RGB to Grayscale.", "Args": {"images": "The RGB tensor to convert. The last dimension must have size 3 and\nshould contain RGB values.", "name": "A name for the operation (optional)."}, "Returns": "The converted grayscale image(s)."}, "tf.image.rgb_to_hsv": {"description": "Converts one or more images from RGB to HSV.", "Args": {"images": "A Tensor. Must be one of the following types: half, bfloat16, float32, float64.\n1-D or higher rank. RGB data to convert. Last dimension must be size 3.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as images."}, "tf.image.rgb_to_yiq": {"description": "Converts one or more images from RGB to YIQ.", "Args": {"images": "2-D or higher rank. Image data to convert. Last dimension must be\nsize 3."}, "Returns": "images\n\n\ntensor with the same shape as images."}, "tf.image.rgb_to_yuv": {"description": "Converts one or more images from RGB to YUV.", "Args": {"images": "2-D or higher rank. Image data to convert. Last dimension must be\nsize 3."}, "Returns": "images\n\n\ntensor with the same shape as images."}, "tf.image.rot90": {"description": "Rotate image(s) counter-clockwise by 90 degrees.", "Args": {"image": "4-D Tensor of shape [batch, height, width, channels] or 3-D Tensor\nof shape [height, width, channels].", "k": "A scalar integer tensor. The number of times the image(s) are\nrotated by 90 degrees.", "name": "A name for this operation (optional)."}, "Returns": "A rotated tensor of the same type and shape as image.", "Raises": {"ValueError": "if the shape of image not supported."}}, "tf.image.sample_distorted_bounding_box": {"description": "Generate a single randomly distorted bounding box for an image.", "Args": {"image_size": "A Tensor. Must be one of the following types: uint8, int8,\nint16, int32, int64. 1-D, containing [height, width, channels].", "bounding_boxes": "A Tensor of type float32. 3-D with shape [batch, N, 4]\ndescribing the N bounding boxes associated with the image.", "seed": "An optional int. Defaults to 0. If seed is set to non-zero, the\nrandom number generator is seeded by the given seed.  Otherwise, it is\nseeded by a random seed.", "min_object_covered": "A Tensor of type float32. Defaults to 0.1. The\ncropped area of the image must contain at least this fraction of any\nbounding box supplied. The value of this parameter should be non-negative.\nIn the case of 0, the cropped area does not need to overlap any of the\nbounding boxes supplied.", "aspect_ratio_range": "An optional list of floats. Defaults to [0.75,\n1.33]. The cropped area of the image must have an aspect ratio = width /\nheight within this range.", "area_range": "An optional list of floats. Defaults to [0.05, 1]. The\ncropped area of the image must contain a fraction of the supplied image\nwithin this range.", "max_attempts": "An optional int. Defaults to 100. Number of attempts at\ngenerating a cropped region of the image of the specified constraints.\nAfter max_attempts failures, return the entire image.", "use_image_if_no_bounding_boxes": "An optional bool. Defaults to False.\nControls behavior if no bounding boxes supplied. If true, assume an\nimplicit bounding box covering the whole input. If false, raise an error.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (begin, size, bboxes).", "Raises": {"ValueError": "If no seed is specified and op determinism is enabled."}}, "tf.image.sobel_edges": {"description": "Returns a tensor holding Sobel edge maps.", "Args": {"image": "Image tensor with shape [batch_size, h, w, d] and type float32 or\nfloat64.  The image(s) must be 2x2 or larger."}, "Returns": "Tensor holding edge maps for each channel. Returns a tensor with shape\n[batch_size, h, w, d, 2] where the last two dimensions hold [[dy[0], dx[0]],\n[dy[1], dx[1]], ..., [dy[d-1], dx[d-1]]] calculated using the Sobel filter."}, "tf.image.ssim": {"description": "Computes SSIM index between img1 and img2.", "Args": {"img1": "First image batch. 4-D Tensor of shape [batch, height, width,\nchannels] with only Positive Pixel Values.", "img2": "Second image batch. 4-D Tensor of shape [batch, height, width,\nchannels] with only Positive Pixel Values.", "max_val": "The dynamic range of the images (i.e., the difference between the\nmaximum the and minimum allowed values).", "filter_size": "Default value 11 (size of gaussian filter).", "filter_sigma": "Default value 1.5 (width of gaussian filter).", "k1": "Default value 0.01", "k2": "Default value 0.03 (SSIM is less sensitivity to K2 for lower values, so\nit would be better if we took the values in the range of 0 < K2 < 0.4)."}, "Returns": "A tensor containing an SSIM value for each image in batch.  Returned SSIM\nvalues are in range (-1, 1], when pixel values are non-negative. Returns\na tensor with shape: broadcast(img1.shape[:-3], img2.shape[:-3])."}, "tf.image.ssim_multiscale": {"description": "Computes the MS-SSIM between img1 and img2.", "Args": {"img1": "First image batch with only Positive Pixel Values.", "img2": "Second image batch with only Positive Pixel Values. Must have the\nsame rank as img1.", "max_val": "The dynamic range of the images (i.e., the difference between the\nmaximum the and minimum allowed values).", "power_factors": "Iterable of weights for each of the scales. The number of\nscales used is the length of the list. Index 0 is the unscaled\nresolution's weight and each increasing scale corresponds to the image\nbeing downsampled by 2.  Defaults to (0.0448, 0.2856, 0.3001, 0.2363,\n0.1333), which are the values obtained in the original paper.", "filter_size": "Default value 11 (size of gaussian filter).", "filter_sigma": "Default value 1.5 (width of gaussian filter).", "k1": "Default value 0.01", "k2": "Default value 0.03 (SSIM is less sensitivity to K2 for lower values, so\nit would be better if we took the values in the range of 0 < K2 < 0.4)."}, "Returns": "A tensor containing an MS-SSIM value for each image in batch.  The values\nare in range [0, 1].  Returns a tensor with shape:\nbroadcast(img1.shape[:-3], img2.shape[:-3])."}, "tf.image.stateless_random_brightness": {"description": "Adjust the brightness of images by a random factor deterministically.", "Args": {"image": "An image or images to adjust.", "max_delta": "float, must be non-negative.", "seed": "A shape [2] Tensor, the seed to the random number generator. Must have\ndtype int32 or int64. (When using XLA, only int32 is allowed.)"}, "Returns": "The brightness-adjusted image(s).", "Raises": {"ValueError": "if max_delta is negative."}}, "tf.image.stateless_random_contrast": {"description": "Adjust the contrast of images by a random factor deterministically.", "Args": {"image": "An image tensor with 3 or more dimensions.", "lower": "float.  Lower bound for the random contrast factor.", "upper": "float.  Upper bound for the random contrast factor.", "seed": "A shape [2] Tensor, the seed to the random number generator. Must have\ndtype int32 or int64. (When using XLA, only int32 is allowed.)"}, "Returns": "The contrast-adjusted image(s).", "Raises": {"ValueError": "if upper <= lower or if lower < 0."}}, "tf.image.stateless_random_crop": {"description": "Randomly crops a tensor to a given size in a deterministic manner.", "Args": {"value": "Input tensor to crop.", "size": "1-D tensor with size the rank of value.", "seed": "A shape [2] Tensor, the seed to the random number generator. Must have\ndtype int32 or int64. (When using XLA, only int32 is allowed.)", "name": "A name for this operation (optional)."}, "Returns": "A cropped tensor of the same rank as value and shape size."}, "tf.image.stateless_random_flip_left_right": {"description": "Randomly flip an image horizontally (left to right) deterministically.", "Args": {"image": "4-D Tensor of shape [batch, height, width, channels] or 3-D Tensor\nof shape [height, width, channels].", "seed": "A shape [2] Tensor, the seed to the random number generator. Must have\ndtype int32 or int64. (When using XLA, only int32 is allowed.)"}, "Returns": "A tensor of the same type and shape as image."}, "tf.image.stateless_random_flip_up_down": {"description": "Randomly flip an image vertically (upside down) deterministically.", "Args": {"image": "4-D Tensor of shape [batch, height, width, channels] or 3-D Tensor\nof shape [height, width, channels].", "seed": "A shape [2] Tensor, the seed to the random number generator. Must have\ndtype int32 or int64. (When using XLA, only int32 is allowed.)"}, "Returns": "A tensor of the same type and shape as image."}, "tf.image.stateless_random_hue": {"description": "Adjust the hue of RGB images by a random factor deterministically.", "Args": {"image": "RGB image or images. The size of the last dimension must be 3.", "max_delta": "float. The maximum value for the random delta.", "seed": "A shape [2] Tensor, the seed to the random number generator. Must have\ndtype int32 or int64. (When using XLA, only int32 is allowed.)"}, "Returns": "Adjusted image(s), same shape and DType as image.", "Raises": {"ValueError": "if max_delta is invalid."}}, "tf.image.stateless_random_jpeg_quality": {"description": "Deterministically radomize jpeg encoding quality for inducing jpeg noise.", "Args": {"image": "3D image. Size of the last dimension must be 1 or 3.", "min_jpeg_quality": "Minimum jpeg encoding quality to use.", "max_jpeg_quality": "Maximum jpeg encoding quality to use.", "seed": "A shape [2] Tensor, the seed to the random number generator. Must have\ndtype int32 or int64. (When using XLA, only int32 is allowed.)"}, "Returns": "Adjusted image(s), same shape and DType as image.", "Raises": {"ValueError": "if min_jpeg_quality or max_jpeg_quality is invalid."}}, "tf.image.stateless_random_saturation": {"description": "Adjust the saturation of RGB images by a random factor deterministically.", "Args": {"image": "RGB image or images. The size of the last dimension must be 3.", "lower": "float.  Lower bound for the random saturation factor.", "upper": "float.  Upper bound for the random saturation factor.", "seed": "A shape [2] Tensor, the seed to the random number generator. Must have\ndtype int32 or int64. (When using XLA, only int32 is allowed.)"}, "Returns": "Adjusted image(s), same shape and DType as image.", "Raises": {"ValueError": "if upper <= lower or if lower < 0."}}, "tf.image.stateless_sample_distorted_bounding_box": {"description": "Generate a randomly distorted bounding box for an image deterministically.", "Args": {"image_size": "A Tensor. Must be one of the following types: uint8, int8,\nint16, int32, int64. 1-D, containing [height, width, channels].", "bounding_boxes": "A Tensor of type float32. 3-D with shape [batch, N, 4]\ndescribing the N bounding boxes associated with the image.", "seed": "A shape [2] Tensor, the seed to the random number generator. Must have\ndtype int32 or int64. (When using XLA, only int32 is allowed.)", "min_object_covered": "A Tensor of type float32. Defaults to 0.1. The\ncropped area of the image must contain at least this fraction of any\nbounding box supplied. The value of this parameter should be non-negative.\nIn the case of 0, the cropped area does not need to overlap any of the\nbounding boxes supplied.", "aspect_ratio_range": "An optional list of floats. Defaults to [0.75,\n1.33]. The cropped area of the image must have an aspect ratio = width /\nheight within this range.", "area_range": "An optional list of floats. Defaults to [0.05, 1]. The\ncropped area of the image must contain a fraction of the supplied image\nwithin this range.", "max_attempts": "An optional int. Defaults to 100. Number of attempts at\ngenerating a cropped region of the image of the specified constraints.\nAfter max_attempts failures, return the entire image.", "use_image_if_no_bounding_boxes": "An optional bool. Defaults to False.\nControls behavior if no bounding boxes supplied. If true, assume an\nimplicit bounding box covering the whole input. If false, raise an error.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (begin, size, bboxes)."}, "tf.image.total_variation": {"description": "Calculate and return the total variation for one or more images.", "Args": {"images": "4-D Tensor of shape [batch, height, width, channels] or 3-D Tensor\nof shape [height, width, channels].", "name": "A name for the operation (optional)."}, "Raises": {"ValueError": "if images.shape is not a 3-D or 4-D vector."}, "Returns": "The total variation of images.\nIf images was 4-D, return a 1-D float Tensor of shape [batch] with the\ntotal variation for each image in the batch.\nIf images was 3-D, return a scalar float with the total variation for\nthat image."}, "tf.image.transpose": {"description": "Transpose image(s) by swapping the height and width dimension.", "Args": {"image": "4-D Tensor of shape [batch, height, width, channels] or 3-D Tensor\nof shape [height, width, channels].", "name": "A name for this operation (optional)."}, "Returns": "If image was 4-D, a 4-D float Tensor of shape\n[batch, width, height, channels]\n If image was 3-D, a 3-D float Tensor of shape\n[width, height, channels]", "Raises": {"ValueError": "if the shape of image not supported."}}, "tf.image.yiq_to_rgb": {"description": "Converts one or more images from YIQ to RGB.", "Args": {"images": "2-D or higher rank. Image data to convert. Last dimension must be\nsize 3."}, "Returns": "images\n\n\ntensor with the same shape as images."}, "tf.image.yuv_to_rgb": {"description": "Converts one or more images from YUV to RGB.", "Args": {"images": "2-D or higher rank. Image data to convert. Last dimension must be\nsize 3."}, "Returns": "images\n\n\ntensor with the same shape as images."}}, "tf.io": {"tf.io.FixedLenFeature": {"description": "Configuration for parsing a fixed-length input feature.", "Attributes": {"shape": "A namedtuple alias for field number 0", "dtype": "A namedtuple alias for field number 1", "default_value": "A namedtuple alias for field number 2"}}, "tf.io.FixedLenSequenceFeature": {"description": "Configuration for parsing a variable-length input feature into a Tensor.", "Attributes": {"shape": "A namedtuple alias for field number 0", "dtype": "A namedtuple alias for field number 1", "allow_missing": "A namedtuple alias for field number 2", "default_value": "A namedtuple alias for field number 3"}}, "tf.io.RaggedFeature": {"description": "Configuration for passing a RaggedTensor input feature.", "Attributes": {"dtype": "A namedtuple alias for field number 0", "value_key": "A namedtuple alias for field number 1", "partitions": "A namedtuple alias for field number 2", "row_splits_dtype": "A namedtuple alias for field number 3", "validate": "A namedtuple alias for field number 4"}}, "tf.io.RaggedFeature.RowLengths": {"description": "RowLengths(key,)", "Attributes": {"key": "A namedtuple alias for field number 0"}}, "tf.io.RaggedFeature.RowLimits": {"description": "RowLimits(key,)", "Attributes": {"key": "A namedtuple alias for field number 0"}}, "tf.io.RaggedFeature.RowSplits": {"description": "RowSplits(key,)", "Attributes": {"key": "A namedtuple alias for field number 0"}}, "tf.io.RaggedFeature.RowStarts": {"description": "RowStarts(key,)", "Attributes": {"key": "A namedtuple alias for field number 0"}}, "tf.io.RaggedFeature.UniformRowLength": {"description": "UniformRowLength(length,)", "Attributes": {"length": "A namedtuple alias for field number 0"}}, "tf.io.RaggedFeature.ValueRowIds": {"description": "ValueRowIds(key,)", "Attributes": {"key": "A namedtuple alias for field number 0"}}, "tf.io.SparseFeature": {"description": "Configuration for parsing a sparse input feature from an Example.", "Attributes": {"index_key": "A namedtuple alias for field number 0", "value_key": "A namedtuple alias for field number 1", "dtype": "A namedtuple alias for field number 2", "size": "A namedtuple alias for field number 3", "already_sorted": "A namedtuple alias for field number 4"}}, "tf.io.TFRecordOptions": {"description": "Options used for manipulating TFRecord files.", "Args": {"compression_type": "\"GZIP\", \"ZLIB\", or \"\" (no compression).", "flush_mode": "flush mode or None, Default: Z_NO_FLUSH.", "input_buffer_size": "int or None.", "output_buffer_size": "int or None.", "window_bits": "int or None.", "compression_level": "0 to 9, or None.", "compression_method": "compression method or None.", "mem_level": "1 to 9, or None.", "compression_strategy": "strategy or None. Default: Z_DEFAULT_STRATEGY."}, "Raises": {"ValueError": "If compression_type is invalid."}, "Class Variables": {"compression_type_map": "{\u00a00: '',\u00a01: 'ZLIB',\u00a02: 'GZIP'}"}}, "tf.io.TFRecordWriter": {"description": "A class to write records to a TFRecords file.", "Args": {"path": "The path to the TFRecords file.", "options": "(optional) String specifying compression type,\nTFRecordCompressionType, or TFRecordOptions object."}, "Raises": {"IOError": "If path cannot be opened for writing.", "ValueError": "If valid compression_type can't be determined from options."}}, "tf.io.VarLenFeature": {"description": "Configuration for parsing a variable-length input feature.", "Attributes": {"dtype": "A namedtuple alias for field number 0"}}, "tf.io.decode_and_crop_jpeg": {"description": "Decode and Crop a JPEG-encoded image to a uint8 tensor.", "Args": {"contents": "A Tensor of type string. 0-D.  The JPEG-encoded image.", "crop_window": "A Tensor of type int32.\n1-D.  The crop window: [crop_y, crop_x, crop_height, crop_width].", "channels": "An optional int. Defaults to 0.\nNumber of color channels for the decoded image.", "ratio": "An optional int. Defaults to 1. Downscaling ratio.", "fancy_upscaling": "An optional bool. Defaults to True.\nIf true use a slower but nicer upscaling of the\nchroma planes (yuv420/422 only).", "try_recover_truncated": "An optional bool. Defaults to False.\nIf true try to recover an image from truncated input.", "acceptable_fraction": "An optional float. Defaults to 1.\nThe minimum required fraction of lines before a truncated\ninput is accepted.", "dct_method": "An optional string. Defaults to \"\".\nstring specifying a hint about the algorithm used for\ndecompression.  Defaults to \"\" which maps to a system-specific\ndefault.  Currently valid values are [\"INTEGER_FAST\",\n\"INTEGER_ACCURATE\"].  The hint may be ignored (e.g., the internal\njpeg library changes to a version that does not have that specific\noption.)", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type uint8."}, "tf.io.decode_base64": {"description": "Decode web-safe base64-encoded strings.", "Args": {"input": "A Tensor of type string. Base64 strings to decode.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.io.decode_bmp": {"description": "Decode the first frame of a BMP-encoded image to a uint8 tensor.", "Args": {"contents": "A Tensor of type string. 0-D.  The BMP-encoded image.", "channels": "An optional int. Defaults to 0.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type uint8."}, "tf.io.decode_compressed": {"description": "Decompress strings.", "Args": {"bytes": "A Tensor of type string.\nA Tensor of string which is compressed.", "compression_type": "An optional string. Defaults to \"\".\nA scalar containing either (i) the empty string (no\ncompression), (ii) \"ZLIB\", or (iii) \"GZIP\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.io.decode_csv": {"description": "Convert CSV records to tensors. Each column maps to one tensor.", "Args": {"records": "A Tensor of type string.\nEach string is a record/row in the csv and all records should have\nthe same format.", "record_defaults": "A list of Tensor objects with specific types.\nAcceptable types are float32, float64, int32, int64, string.\nOne tensor per column of the input record, with either a\nscalar default value for that column or an empty vector if the column is\nrequired.", "field_delim": "An optional string. Defaults to \",\".\nchar delimiter to separate fields in a record.", "use_quote_delim": "An optional bool. Defaults to True.\nIf false, treats double quotation marks as regular\ncharacters inside of the string fields (ignoring RFC 4180, Section 2,\nBullet 5).", "na_value": "Additional string to recognize as NA/NaN.", "select_cols": "Optional sorted list of column indices to select. If specified,\nonly this subset of columns will be parsed and returned.", "name": "A name for the operation (optional)."}, "Returns": "A list of Tensor objects. Has the same type as record_defaults.\nEach tensor will have the same shape as records.", "Raises": {"ValueError": "If any of the arguments is malformed."}}, "tf.io.decode_gif": {"description": "Decode the frame(s) of a GIF-encoded image to a uint8 tensor.", "Args": {"contents": "A Tensor of type string. 0-D.  The GIF-encoded image.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type uint8."}, "tf.io.decode_image": {"description": "Function for decode_bmp, decode_gif, decode_jpeg, and decode_png.", "Args": {"contents": "A Tensor of type string. 0-D. The encoded image bytes.", "channels": "An optional int. Defaults to 0. Number of color channels for\nthe decoded image.", "dtype": "The desired DType of the returned Tensor.", "name": "A name for the operation (optional)", "expand_animations": "An optional bool. Defaults to True. Controls the\nshape of the returned op's output. If True, the returned op will produce\na 3-D tensor for PNG, JPEG, and BMP files; and a 4-D tensor for all GIFs,\nwhether animated or not. If, False, the returned op will produce a 3-D\ntensor for all file types and will truncate animated GIFs to the first\nframe."}, "Returns": "Tensor with type dtype and a 3- or 4-dimensional shape, depending on\nthe file type and the value of the expand_animations parameter.", "Raises": {"ValueError": "On incorrect number of channels."}}, "tf.io.decode_jpeg": {"description": "Decode a JPEG-encoded image to a uint8 tensor.", "Args": {"contents": "A Tensor of type string. 0-D.  The JPEG-encoded image.", "channels": "An optional int. Defaults to 0.\nNumber of color channels for the decoded image.", "ratio": "An optional int. Defaults to 1. Downscaling ratio.", "fancy_upscaling": "An optional bool. Defaults to True.\nIf true use a slower but nicer upscaling of the\nchroma planes (yuv420/422 only).", "try_recover_truncated": "An optional bool. Defaults to False.\nIf true try to recover an image from truncated input.", "acceptable_fraction": "An optional float. Defaults to 1.\nThe minimum required fraction of lines before a truncated\ninput is accepted.", "dct_method": "An optional string. Defaults to \"\".\nstring specifying a hint about the algorithm used for\ndecompression.  Defaults to \"\" which maps to a system-specific\ndefault.  Currently valid values are [\"INTEGER_FAST\",\n\"INTEGER_ACCURATE\"].  The hint may be ignored (e.g., the internal\njpeg library changes to a version that does not have that specific\noption.)", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type uint8."}, "tf.io.decode_json_example": {"description": "Convert JSON-encoded Example records to binary protocol buffer strings.", "Args": {"json_examples": "A string tensor containing json-serialized tf.Example\nprotos.", "name": "A name for the op."}, "Returns": "A string Tensor containing the binary-serialized tf.Example protos.", "Raises": {}}, "tf.io.decode_png": {"description": "Decode a PNG-encoded image to a uint8 or uint16 tensor.", "Args": {"contents": "A Tensor of type string. 0-D.  The PNG-encoded image.", "channels": "An optional int. Defaults to 0.\nNumber of color channels for the decoded image.", "dtype": "An optional tf.DType from: tf.uint8, tf.uint16. Defaults to tf.uint8.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dtype."}, "tf.io.decode_proto": {"description": "The op extracts fields from a serialized protocol buffers message into tensors.", "Args": {"bytes": "A Tensor of type string.\nTensor of serialized protos with shape batch_shape.", "message_type": "A string. Name of the proto message type to decode.", "field_names": "A list of strings.\nList of strings containing proto field names. An extension field can be decoded\nby using its full name, e.g. EXT_PACKAGE.EXT_FIELD_NAME.", "output_types": "A list of tf.DTypes.\nList of TF types to use for the respective field in field_names.", "descriptor_source": "An optional string. Defaults to \"local://\".\nEither the special value local:// or a path to a file containing\na serialized FileDescriptorSet.", "message_format": "An optional string. Defaults to \"binary\".\nEither binary or text.", "sanitize": "An optional bool. Defaults to False.\nWhether to sanitize the result or not.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (sizes, values)."}, "tf.io.decode_raw": {"description": "Convert raw bytes from input tensor into numeric tensors.", "Args": {"input_bytes": "Each element of the input Tensor is converted to an array of bytes.\nCurrently, this must be a tensor of strings (bytes), although semantically\nthe operation should support any input.", "out_type": "DType of the output. Acceptable types are half, float, double,\nint32, uint16, uint8, int16, int8, int64.", "little_endian": "Whether the input_bytes data is in little-endian format. Data will be\nconverted into host byte order if necessary.", "fixed_length": "If set, the first fixed_length bytes of each element will be converted.\nData will be zero-padded or truncated to the specified length.\nfixed_length must be a multiple of the size of out_type.\nfixed_length must be specified if the elements of input_bytes are of\nvariable length.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor object storing the decoded bytes."}, "tf.io.deserialize_many_sparse": {"description": "Deserialize and concatenate SparseTensors from a serialized minibatch.", "Args": {"serialized_sparse": "2-D Tensor of type string of shape [N, 3].\nThe serialized and packed SparseTensor objects.", "dtype": "The dtype of the serialized SparseTensor objects.", "rank": "(optional) Python int, the rank of the SparseTensor objects.", "name": "A name prefix for the returned tensors (optional)"}, "Returns": "A SparseTensor representing the deserialized SparseTensors,\nconcatenated along the SparseTensors' first dimension.\nAll of the serialized SparseTensors must have had the same rank and type."}, "tf.io.encode_base64": {"description": "Encode strings into web-safe base64 format.", "Args": {"input": "A Tensor of type string. Strings to be encoded.", "pad": "An optional bool. Defaults to False.\nBool whether padding is applied at the ends.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.io.encode_jpeg": {"description": "JPEG-encode an image.", "Args": {"image": "A Tensor of type uint8.\n3-D with shape [height, width, channels].", "format": "An optional string from: \"\", \"grayscale\", \"rgb\". Defaults to \"\".\nPer pixel image format.", "quality": "An optional int. Defaults to 95.\nQuality of the compression from 0 to 100 (higher is better and slower).", "progressive": "An optional bool. Defaults to False.\nIf True, create a JPEG that loads progressively (coarse to fine).", "optimize_size": "An optional bool. Defaults to False.\nIf True, spend CPU/RAM to reduce size with no quality change.", "chroma_downsampling": "An optional bool. Defaults to True.\nSee http://en.wikipedia.org/wiki/Chroma_subsampling", "density_unit": "An optional string from: \"in\", \"cm\". Defaults to \"in\".\nUnit used to specify x_density and y_density:\npixels per inch ('in') or centimeter ('cm').", "x_density": "An optional int. Defaults to 300.\nHorizontal pixels per density unit.", "y_density": "An optional int. Defaults to 300.\nVertical pixels per density unit.", "xmp_metadata": "An optional string. Defaults to \"\".\nIf not empty, embed this XMP metadata in the image header.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.io.encode_png": {"description": "PNG-encode an image.", "Args": {"image": "A Tensor. Must be one of the following types: uint8, uint16.\n3-D with shape [height, width, channels].", "compression": "An optional int. Defaults to -1. Compression level.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.io.encode_proto": {"description": "The op serializes protobuf messages provided in the input tensors.", "Args": {"sizes": "A Tensor of type int32.\nTensor of int32 with shape [batch_shape, len(field_names)].", "values": "A list of Tensor objects.\nList of tensors containing values for the corresponding field.", "field_names": "A list of strings.\nList of strings containing proto field names.", "message_type": "A string. Name of the proto message type to decode.", "descriptor_source": "An optional string. Defaults to \"local://\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.io.extract_jpeg_shape": {"description": "Extract the shape information of a JPEG-encoded image.", "Args": {"contents": "A Tensor of type string. 0-D. The JPEG-encoded image.", "output_type": "An optional tf.DType from: tf.int32, tf.int64. Defaults to tf.int32.\n(Optional) The output type of the operation (int32 or int64).\nDefaults to int32.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type output_type."}, "tf.io.is_jpeg": {"description": "Convenience function to check if the &#39;contents&#39; encodes a JPEG image.", "Args": {"contents": "0-D string. The encoded image bytes.", "name": "A name for the operation (optional)"}, "Returns": "A scalar boolean tensor indicating if 'contents' may be a JPEG image.\nis_jpeg is susceptible to false positives."}, "tf.io.match_filenames_once": {"description": "Save the list of files matching pattern, so it is only computed once.", "Args": {"pattern": "A file pattern (glob), or 1D tensor of file patterns.", "name": "A name for the operations (optional)."}, "Returns": "A variable that is initialized to the list of files matching the pattern(s)."}, "tf.io.matching_files": {"description": "Returns the set of files matching one or more glob patterns.", "Args": {"pattern": "A Tensor of type string.\nShell wildcard pattern(s). Scalar or vector of type string.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.io.parse_example": {"description": "Parses Example protos into a dict of tensors.", "Args": {"serialized": "A vector (1-D Tensor) of strings, a batch of binary\nserialized Example protos.", "features": "A dict mapping feature keys to FixedLenFeature,\nVarLenFeature, SparseFeature, and RaggedFeature values.", "example_names": "A vector (1-D Tensor) of strings (optional), the names of\nthe serialized protos in the batch.", "name": "A name for this operation (optional)."}, "Returns": "A dict mapping feature keys to Tensor, SparseTensor, and\nRaggedTensor values.", "Raises": {"ValueError": "if any feature is invalid."}}, "tf.io.parse_sequence_example": {"description": "Parses a batch of SequenceExample protos.", "Args": {"serialized": "A vector (1-D Tensor) of type string containing binary\nserialized SequenceExample protos.", "context_features": "A dict mapping feature keys to FixedLenFeature or\nVarLenFeature or RaggedFeature values. These features are associated\nwith a SequenceExample as a whole.", "sequence_features": "A dict mapping feature keys to\nFixedLenSequenceFeature or VarLenFeature or RaggedFeature values.\nThese features are associated with data within the FeatureList section\nof the SequenceExample proto.", "example_names": "A vector (1-D Tensor) of strings (optional), the name of the\nserialized protos.", "name": "A name for this operation (optional)."}, "Returns": "A tuple of three dicts, each mapping keys to Tensors,\nSparseTensors, and RaggedTensor. The first dict contains the context\nkey/values, the second dict contains the feature_list key/values, and the\nfinal dict contains the lengths of any dense feature_list features.", "Raises": {"ValueError": "if any feature is invalid."}}, "tf.io.parse_single_example": {"description": "Parses a single Example proto.", "Args": {"serialized": "A scalar string Tensor, a single serialized Example.", "features": "A dict mapping feature keys to FixedLenFeature or\nVarLenFeature values.", "example_names": "(Optional) A scalar string Tensor, the associated name.", "name": "A name for this operation (optional)."}, "Returns": "A dict mapping feature keys to Tensor and SparseTensor values.", "Raises": {"ValueError": "if any feature is invalid."}}, "tf.io.parse_single_sequence_example": {"description": "Parses a single SequenceExample proto.", "Args": {"serialized": "A scalar (0-D Tensor) of type string, a single binary\nserialized SequenceExample proto.", "context_features": "A dict mapping feature keys to FixedLenFeature or\nVarLenFeature or RaggedFeature values. These features are associated\nwith a SequenceExample as a whole.", "sequence_features": "A dict mapping feature keys to\nFixedLenSequenceFeature or VarLenFeature or RaggedFeature values.\nThese features are associated with data within the FeatureList section\nof the SequenceExample proto.", "example_name": "A scalar (0-D Tensor) of strings (optional), the name of\nthe serialized proto.", "name": "A name for this operation (optional)."}, "Returns": "A tuple of two dicts, each mapping keys to Tensors and SparseTensors\nand RaggedTensors.\n\nThe first dict contains the context key/values.\nThe second dict contains the feature_list key/values.", "Raises": {"ValueError": "if any feature is invalid."}}, "tf.io.parse_tensor": {"description": "Transforms a serialized tensorflow.TensorProto proto into a Tensor.", "Args": {"serialized": "A Tensor of type string.\nA scalar string containing a serialized TensorProto proto.", "out_type": "A tf.DType.\nThe type of the serialized tensor.  The provided type must match the\ntype of the serialized tensor and no implicit conversion will take place.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type out_type."}, "tf.io.read_file": {"description": "Reads the contents of file.", "Args": {"filename": "string. filename to read from.", "name": "string.  Optional name for the op."}, "Returns": "A tensor of dtype \"string\", with the file contents."}, "tf.io.serialize_many_sparse": {"description": "Serialize N-minibatch SparseTensor into an [N, 3] Tensor.", "Args": {"sp_input": "The input rank R SparseTensor.", "out_type": "The dtype to use for serialization.", "name": "A name prefix for the returned tensors (optional)."}, "Returns": "A matrix (2-D Tensor) with N rows and 3 columns. Each column\nrepresents serialized SparseTensor's indices, values, and shape\n(respectively).", "Raises": {"TypeError": "If sp_input is not a SparseTensor."}}, "tf.io.serialize_sparse": {"description": "Serialize a SparseTensor into a 3-vector (1-D Tensor) object.", "Args": {"sp_input": "The input SparseTensor.", "out_type": "The dtype to use for serialization.", "name": "A name prefix for the returned tensors (optional)."}, "Returns": "A 3-vector (1-D Tensor), with each column representing the serialized\nSparseTensor's indices, values, and shape (respectively).", "Raises": {"TypeError": "If sp_input is not a SparseTensor."}}, "tf.io.serialize_tensor": {"description": "Transforms a Tensor into a serialized TensorProto proto.", "Args": {"tensor": "A tf.Tensor.", "name": "string.  Optional name for the op."}, "Returns": "A Tensor of dtype string."}, "tf.io.write_file": {"description": "Writes contents to the file at input filename.", "Args": {"filename": "A Tensor of type string.\nscalar. The name of the file to which we write the contents.", "contents": "A Tensor of type string.\nscalar. The content to be written to the output file.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.io.write_graph": {"description": "Writes a graph proto to a file.", "Args": {"graph_or_graph_def": "A Graph or a GraphDef protocol buffer.", "logdir": "Directory where to write the graph. This can refer to remote\nfilesystems, such as Google Cloud Storage (GCS).", "name": "Filename for the graph.", "as_text": "If True, writes the graph as an ASCII proto."}, "Returns": "The path of the output proto file."}}, "tf.io.gfile": {"tf.io.gfile.GFile": {"description": "File I/O wrappers without thread locking.", "Attributes": {"mode": "Returns the mode in which the file was opened.", "name": "Returns the file name."}}, "tf.io.gfile.copy": {"description": "Copies data from src to dst.", "Args": {"src": "string, name of the file whose contents need to be copied", "dst": "string, name of the file to which to copy to", "overwrite": "boolean, if false it's an error for dst to be occupied by an\nexisting file."}, "Raises": {"errors.OpError": "If the operation fails."}}, "tf.io.gfile.exists": {"description": "Determines whether a path exists or not.", "Args": {"path": "string, a path"}, "Returns": "True if the path exists, whether it's a file or a directory.\nFalse if the path does not exist and there are no filesystem errors.", "Raises": {"errors.OpError": "Propagates any errors reported by the FileSystem API."}}, "tf.io.gfile.get_registered_schemes": {"description": "Returns the currently registered filesystem schemes.", "Returns": "List of string schemes, e.g. ['', 'file', 'ram'], in arbitrary order.", "Raises": {"errors.OpError": "If the operation fails."}}, "tf.io.gfile.glob": {"description": "Returns a list of files that match the given pattern(s).", "Args": {"pattern": "string or iterable of strings. The glob pattern(s)."}, "Returns": "A list of strings containing filenames that match the given pattern(s).", "Raises": {"errors.OpError": "If there are filesystem / directory listing errors.", "errors.NotFoundError": "If pattern to be matched is an invalid directory."}}, "tf.io.gfile.isdir": {"description": "Returns whether the path is a directory or not.", "Args": {"path": "string, path to a potential directory"}, "Returns": "True, if the path is a directory; False otherwise"}, "tf.io.gfile.join": {"description": "Join one or more path components intelligently.", "Args": {"path": "string, path to a directory", "paths": "string, additional paths to concatenate"}, "Returns": "path\n\n\nthe joined path."}, "tf.io.gfile.listdir": {"description": "Returns a list of entries contained within a directory.", "Args": {"path": "string, path to a directory"}, "Returns": "[filename1, filename2, ... filenameN] as strings", "Raises": {}}, "tf.io.gfile.makedirs": {"description": "Creates a directory and all parent/intermediate directories.", "Args": {"path": "string, name of the directory to be created"}, "Raises": {"errors.OpError": "If the operation fails."}}, "tf.io.gfile.mkdir": {"description": "Creates a directory with the name given by path.", "Args": {"path": "string, name of the directory to be created"}, "Raises": {"errors.OpError": "If the operation fails."}}, "tf.io.gfile.remove": {"description": "Deletes the path located at &#39;path&#39;.", "Args": {"path": "string, a path"}, "Raises": {"errors.OpError": "Propagates any errors reported by the FileSystem API.  E.g.,\nNotFoundError if the path does not exist."}}, "tf.io.gfile.rename": {"description": "Rename or move a file / directory.", "Args": {"src": "string, pathname for a file", "dst": "string, pathname to which the file needs to be moved", "overwrite": "boolean, if false it's an error for dst to be occupied by an\nexisting file."}, "Raises": {"errors.OpError": "If the operation fails."}}, "tf.io.gfile.rmtree": {"description": "Deletes everything under path recursively.", "Args": {"path": "string, a path"}, "Raises": {"errors.OpError": "If the operation fails."}}, "tf.io.gfile.stat": {"description": "Returns file statistics for a given path.", "Args": {"path": "string, path to a file"}, "Returns": "FileStatistics struct that contains information about the path", "Raises": {"errors.OpError": "If the operation fails."}}, "tf.io.gfile.walk": {"description": "Recursive directory tree generator for directories.", "Args": {"top": "string, a Directory name", "topdown": "bool, Traverse pre order if True, post order if False.", "onerror": "optional handler for errors. Should be a function, it will be\ncalled with the error as argument. Rethrowing the error aborts the walk.\nErrors that happen while listing directories are ignored."}}}, "tf.keras": {"tf.keras.Input": {"description": "Input() is used to instantiate a Keras tensor.", "Args": {"shape": "A shape tuple (integers), not including the batch size.\nFor instance, shape=(32,) indicates that the expected input\nwill be batches of 32-dimensional vectors. Elements of this tuple\ncan be None; 'None' elements represent dimensions where the shape is\nnot known.", "batch_size": "optional static batch size (integer).", "name": "An optional name string for the layer.\nShould be unique in a model (do not reuse the same name twice).\nIt will be autogenerated if it isn't provided.", "dtype": "The data type expected by the input, as a string\n(float32, float64, int32...)", "sparse": "A boolean specifying whether the placeholder to be created is\nsparse. Only one of 'ragged' and 'sparse' can be True. Note that,\nif sparse is False, sparse tensors can still be passed into the\ninput - they will be densified with a default value of 0.", "tensor": "Optional existing tensor to wrap into the Input layer.\nIf set, the layer will use the tf.TypeSpec of this tensor rather\nthan creating a new placeholder tensor.", "ragged": "A boolean specifying whether the placeholder to be created is\nragged. Only one of 'ragged' and 'sparse' can be True. In this case,\nvalues of 'None' in the 'shape' argument represent ragged dimensions.\nFor more information about RaggedTensors, see\nthis guide.", "type_spec": "A tf.TypeSpec object to create the input placeholder from.\nWhen provided, all other args except name must be None.", "**kwargs": "deprecated arguments support. Supports batch_shape and\nbatch_input_shape."}, "Returns": "A tensor.", "Raises": {"ValueError": "if any unrecognized parameters are provided."}}, "tf.keras.Model": {"description": "Model groups layers into an object with training and inference features.", "Args": {"inputs": "The input(s) of the model: a keras.Input object or list of\nkeras.Input objects.", "outputs": "The output(s) of the model. See Functional API example below.", "name": "String, the name of the model."}, "Attributes": {"distribute_strategy": "The tf.distribute.Strategy this model was created under.", "layers": "", "metrics_names": "Returns the model's display labels for all outputs.\nNote: metrics_names are available only after a keras.Model has been\ntrained/evaluated on actual data.\ninputs = tf.keras.layers.Input(shape=(3,))outputs = tf.keras.layers.Dense(2)(inputs)model = tf.keras.models.Model(inputs=inputs, outputs=outputs)model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])model.metrics_names[]\nx = np.random.random((2, 3))y = np.random.randint(0, 2, (2, 2))model.fit(x, y)model.metrics_names['loss', 'mae']\ninputs = tf.keras.layers.Input(shape=(3,))d = tf.keras.layers.Dense(2, name='out')output_1 = d(inputs)output_2 = d(inputs)model = tf.keras.models.Model(\u00a0 \u00a0inputs=inputs, outputs=[output_1, output_2])model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\", \"acc\"])model.fit(x, (y, y))model.metrics_names['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae','out_1_acc']", "run_eagerly": "Settable attribute indicating whether the model should run eagerly.\nRunning eagerly means that your model will be run step by step,\nlike Python code. Your model might run slower, but it should become easier\nfor you to debug it by stepping into individual layer calls.\nBy default, we will attempt to compile your model to a static graph to\ndeliver the best execution performance."}}, "tf.keras.Sequential": {"description": "Sequential groups a linear stack of layers into a tf.keras.Model.", "Args": {"layers": "Optional list of layers to add to the model.", "name": "Optional name for the model."}, "Attributes": {"distribute_strategy": "The tf.distribute.Strategy this model was created under.", "layers": "", "metrics_names": "Returns the model's display labels for all outputs.\nNote: metrics_names are available only after a keras.Model has been\ntrained/evaluated on actual data.\ninputs = tf.keras.layers.Input(shape=(3,))outputs = tf.keras.layers.Dense(2)(inputs)model = tf.keras.models.Model(inputs=inputs, outputs=outputs)model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])model.metrics_names[]\nx = np.random.random((2, 3))y = np.random.randint(0, 2, (2, 2))model.fit(x, y)model.metrics_names['loss', 'mae']\ninputs = tf.keras.layers.Input(shape=(3,))d = tf.keras.layers.Dense(2, name='out')output_1 = d(inputs)output_2 = d(inputs)model = tf.keras.models.Model(\u00a0 \u00a0inputs=inputs, outputs=[output_1, output_2])model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\", \"acc\"])model.fit(x, (y, y))model.metrics_names['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae','out_1_acc']", "run_eagerly": "Settable attribute indicating whether the model should run eagerly.\nRunning eagerly means that your model will be run step by step,\nlike Python code. Your model might run slower, but it should become easier\nfor you to debug it by stepping into individual layer calls.\nBy default, we will attempt to compile your model to a static graph to\ndeliver the best execution performance."}}}, "tf.keras.activations": {"tf.keras.activations.deserialize": {"description": "Returns activation function given a string identifier.", "Args": {"name": "The name of the activation function.", "custom_objects": "Optional {function_name: function_obj}\ndictionary listing user-provided activation functions."}, "Returns": "Corresponding activation function.", "Raises": {"ValueError": "Unknown activation function if the input string does not\ndenote any defined Tensorflow activation function."}}, "tf.keras.activations.elu": {"description": "Exponential Linear Unit.", "Args": {"x": "Input tensor.", "alpha": "A scalar, slope of negative section. alpha controls the value to\nwhich an ELU saturates for negative net inputs."}, "Returns": "The exponential linear unit (ELU) activation function: x if x > 0 and\nalpha * (exp(x) - 1) if x < 0."}, "tf.keras.activations.exponential": {"description": "Exponential activation function.", "Args": {"x": "Input tensor."}, "Returns": "Tensor with exponential activation: exp(x)."}, "tf.keras.activations.gelu": {"description": "Applies the Gaussian error linear unit (GELU) activation function.", "Args": {"x": "Input tensor.", "approximate": "A bool, whether to enable approximation."}, "Returns": "The gaussian error linear activation:\n0.5 * x * (1 + tanh(sqrt(2 / pi) * (x + 0.044715 * x^3)))\nif approximate is True or\nx * P(X <= x) = 0.5 * x * (1 + erf(x / sqrt(2))),\nwhere P(X) ~ N(0, 1),\nif approximate is False."}, "tf.keras.activations.get": {"description": "Returns function.", "Args": {"identifier": "Function or string"}, "Returns": "Function corresponding to the input string or input function.", "Raises": {"ValueError": "Input is an unknown function or string, i.e., the input does\nnot denote any defined function."}}, "tf.keras.activations.hard_sigmoid": {"description": "Hard sigmoid activation function.", "Args": {"x": "Input tensor."}, "Returns": "The hard sigmoid activation, defined as:\n\nif x < -2.5: return 0\nif x > 2.5: return 1\nif -2.5 <= x <= 2.5: return 0.2 * x + 0.5"}, "tf.keras.activations.linear": {"description": "Linear activation function (pass-through).", "Args": {"x": "Input tensor."}, "Returns": "The input, unmodified."}, "tf.keras.activations.relu": {"description": "Applies the rectified linear unit activation function.", "Args": {"x": "Input tensor or variable.", "alpha": "A float that governs the slope for values lower than the\nthreshold.", "max_value": "A float that sets the saturation threshold (the largest value\nthe function will return).", "threshold": "A float giving the threshold value of the activation function\nbelow which values will be damped or set to zero."}, "Returns": "A Tensor representing the input tensor,\ntransformed by the relu activation function.\nTensor will be of the same shape and dtype of input x."}, "tf.keras.activations.selu": {"description": "Scaled Exponential Linear Unit (SELU).", "Args": {"x": "A tensor or variable to compute the activation function for."}, "Returns": "The scaled exponential unit activation: scale * elu(x, alpha)."}, "tf.keras.activations.serialize": {"description": "Returns the string identifier of an activation function.", "Args": {"activation": "Function object."}, "Returns": "String denoting the name attribute of the input function", "Raises": {"ValueError": "The input function is not a valid one."}}, "tf.keras.activations.sigmoid": {"description": "Sigmoid activation function, sigmoid(x) = 1 / (1 &#43; exp(-x)).", "Args": {"x": "Input tensor."}, "Returns": "Tensor with the sigmoid activation: 1 / (1 + exp(-x))."}, "tf.keras.activations.softmax": {"description": "Softmax converts a vector of values to a probability distribution.", "Args": {"x": "Input tensor.", "axis": "Integer, axis along which the softmax normalization is applied."}, "Returns": "Tensor, output of softmax transformation (all values are non-negative\nand sum to 1)."}, "tf.keras.activations.softplus": {"description": "Softplus activation function, softplus(x) = log(exp(x) &#43; 1).", "Args": {"x": "Input tensor."}, "Returns": "The softplus activation: log(exp(x) + 1)."}, "tf.keras.activations.softsign": {"description": "Softsign activation function, softsign(x) = x / (abs(x) &#43; 1).", "Args": {"x": "Input tensor."}, "Returns": "The softsign activation: x / (abs(x) + 1)."}, "tf.keras.activations.swish": {"description": "Swish activation function, swish(x) = x * sigmoid(x).", "Args": {"x": "Input tensor."}, "Returns": "The swish activation applied to x (see reference paper for details)."}, "tf.keras.activations.tanh": {"description": "Hyperbolic tangent activation function.", "Args": {"x": "Input tensor."}, "Returns": "Tensor of same shape and dtype of input x, with tanh activation:\ntanh(x) = sinh(x)/cosh(x) = ((exp(x) - exp(-x))/(exp(x) + exp(-x)))."}}, "tf.keras.applications": {"tf.keras.applications.MobileNetV3Large": {"description": "Instantiates the MobileNetV3Large architecture.", "Args": {"input_shape": "Optional shape tuple, to be specified if you would\nlike to use a model with an input image resolution that is not\n(224, 224, 3).\nIt should have exactly 3 inputs channels (224, 224, 3).\nYou can also omit this option if you would like\nto infer input_shape from an input_tensor.\nIf you choose to include both input_tensor and input_shape then\ninput_shape will be used if they match, if the shapes\ndo not match then we will throw an error.\nE.g. (160, 160, 3) would be one valid value.", "alpha": "controls the width of the network. This is known as the\ndepth multiplier in the MobileNetV3 paper, but the name is kept for\nconsistency with MobileNetV1 in Keras.\n\nIf alpha < 1.0, proportionally decreases the number\nof filters in each layer.\nIf alpha > 1.0, proportionally increases the number\nof filters in each layer.\nIf alpha = 1, default number of filters from the paper\nare used at each layer.", "minimalistic": "In addition to large and small models this module also\ncontains so-called minimalistic models, these models have the same\nper-layer dimensions characteristic as MobilenetV3 however, they don't\nutilize any of the advanced blocks (squeeze-and-excite units, hard-swish,\nand 5x5 convolutions). While these models are less efficient on CPU, they\nare much more performant on GPU/DSP.", "include_top": "Boolean, whether to include the fully-connected\nlayer at the top of the network. Defaults to True.", "weights": "String, one of None (random initialization),\n'imagenet' (pre-training on ImageNet),\nor the path to the weights file to be loaded.", "input_tensor": "Optional Keras tensor (i.e. output of\nlayers.Input())\nto use as image input for the model.", "pooling": "String, optional pooling mode for feature extraction\nwhen include_top is False.\nNone means that the output of the model\nwill be the 4D tensor output of the\nlast convolutional block.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional block, and thus\nthe output of the model will be a\n2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "Integer, optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified.", "dropout_rate": "fraction of the input units to drop on the last layer.", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\".", "include_preprocessing": "Boolean, whether to include the preprocessing\nlayer (Rescaling) at the bottom of the network. Defaults to True."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.MobileNetV3Small": {"description": "Instantiates the MobileNetV3Small architecture.", "Args": {"input_shape": "Optional shape tuple, to be specified if you would\nlike to use a model with an input image resolution that is not\n(224, 224, 3).\nIt should have exactly 3 inputs channels (224, 224, 3).\nYou can also omit this option if you would like\nto infer input_shape from an input_tensor.\nIf you choose to include both input_tensor and input_shape then\ninput_shape will be used if they match, if the shapes\ndo not match then we will throw an error.\nE.g. (160, 160, 3) would be one valid value.", "alpha": "controls the width of the network. This is known as the\ndepth multiplier in the MobileNetV3 paper, but the name is kept for\nconsistency with MobileNetV1 in Keras.\n\nIf alpha < 1.0, proportionally decreases the number\nof filters in each layer.\nIf alpha > 1.0, proportionally increases the number\nof filters in each layer.\nIf alpha = 1, default number of filters from the paper\nare used at each layer.", "minimalistic": "In addition to large and small models this module also\ncontains so-called minimalistic models, these models have the same\nper-layer dimensions characteristic as MobilenetV3 however, they don't\nutilize any of the advanced blocks (squeeze-and-excite units, hard-swish,\nand 5x5 convolutions). While these models are less efficient on CPU, they\nare much more performant on GPU/DSP.", "include_top": "Boolean, whether to include the fully-connected\nlayer at the top of the network. Defaults to True.", "weights": "String, one of None (random initialization),\n'imagenet' (pre-training on ImageNet),\nor the path to the weights file to be loaded.", "input_tensor": "Optional Keras tensor (i.e. output of\nlayers.Input())\nto use as image input for the model.", "pooling": "String, optional pooling mode for feature extraction\nwhen include_top is False.\nNone means that the output of the model\nwill be the 4D tensor output of the\nlast convolutional block.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional block, and thus\nthe output of the model will be a\n2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "Integer, optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified.", "dropout_rate": "fraction of the input units to drop on the last layer.", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\".", "include_preprocessing": "Boolean, whether to include the preprocessing\nlayer (Rescaling) at the bottom of the network. Defaults to True."}, "Returns": "A keras.Model instance."}}, "tf.keras.applications.densenet": {"tf.keras.applications.densenet.DenseNet121": {"description": "Instantiates the Densenet121 architecture.", "Args": {"include_top": "whether to include the fully-connected\nlayer at the top of the network.", "weights": "one of None (random initialization),\n'imagenet' (pre-training on ImageNet),\nor the path to the weights file to be loaded.", "input_tensor": "optional Keras tensor (i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "optional shape tuple, only to be specified\nif include_top is False (otherwise the input shape\nhas to be (224, 224, 3) (with 'channels_last' data format)\nor (3, 224, 224) (with 'channels_first' data format).\nIt should have exactly 3 inputs channels,\nand width and height should be no smaller than 32.\nE.g. (200, 200, 3) would be one valid value.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional block.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional block, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified.", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A Keras model instance."}, "tf.keras.applications.densenet.DenseNet169": {"description": "Instantiates the Densenet169 architecture.", "Args": {"include_top": "whether to include the fully-connected\nlayer at the top of the network.", "weights": "one of None (random initialization),\n'imagenet' (pre-training on ImageNet),\nor the path to the weights file to be loaded.", "input_tensor": "optional Keras tensor (i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "optional shape tuple, only to be specified\nif include_top is False (otherwise the input shape\nhas to be (224, 224, 3) (with 'channels_last' data format)\nor (3, 224, 224) (with 'channels_first' data format).\nIt should have exactly 3 inputs channels,\nand width and height should be no smaller than 32.\nE.g. (200, 200, 3) would be one valid value.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional block.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional block, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified.", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A Keras model instance."}, "tf.keras.applications.densenet.DenseNet201": {"description": "Instantiates the Densenet201 architecture.", "Args": {"include_top": "whether to include the fully-connected\nlayer at the top of the network.", "weights": "one of None (random initialization),\n'imagenet' (pre-training on ImageNet),\nor the path to the weights file to be loaded.", "input_tensor": "optional Keras tensor (i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "optional shape tuple, only to be specified\nif include_top is False (otherwise the input shape\nhas to be (224, 224, 3) (with 'channels_last' data format)\nor (3, 224, 224) (with 'channels_first' data format).\nIt should have exactly 3 inputs channels,\nand width and height should be no smaller than 32.\nE.g. (200, 200, 3) would be one valid value.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional block.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional block, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified.", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A Keras model instance."}, "tf.keras.applications.densenet.decode_predictions": {"description": "Decodes the prediction of an ImageNet model.", "Args": {"preds": "Numpy array encoding a batch of predictions.", "top": "Integer, how many top-guesses to return. Defaults to 5."}, "Returns": "A list of lists of top class prediction tuples\n(class_name, class_description, score).\nOne list of tuples per sample in batch input.", "Raises": {"ValueError": "In case of invalid shape of the pred array\n(must be 2D)."}}, "tf.keras.applications.densenet.preprocess_input": {"description": "Preprocesses a tensor or Numpy array encoding a batch of images.", "Args": {"x": "A floating point numpy.array or a tf.Tensor, 3D or 4D with 3 color\nchannels, with values in the range [0, 255].\nThe preprocessed data are written over the input data\nif the data types are compatible. To avoid this\nbehaviour, numpy.copy(x) can be used.", "data_format": "Optional data format of the image tensor/array. Defaults to\nNone, in which case the global setting\ntf.keras.backend.image_data_format() is used (unless you changed it,\nit defaults to \"channels_last\")."}, "Returns": "Preprocessed numpy.array or a tf.Tensor with type float32.\nThe input pixels values are scaled between 0 and 1 and each channel is\nnormalized with respect to the ImageNet dataset.", "Raises": {"ValueError": "In case of unknown data_format argument."}}}, "tf.keras.applications.efficientnet": {"tf.keras.applications.efficientnet.EfficientNetB0": {"description": "Instantiates the EfficientNetB0 architecture.", "Args": {"include_top": "Whether to include the fully-connected\nlayer at the top of the network. Defaults to True.", "weights": "One of None (random initialization),\n'imagenet' (pre-training on ImageNet),\nor the path to the weights file to be loaded. Defaults to 'imagenet'.", "input_tensor": "Optional Keras tensor\n(i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "Optional shape tuple, only to be specified\nif include_top is False.\nIt should have exactly 3 inputs channels.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False. Defaults to None.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional layer.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "Optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified. Defaults to 1000 (number of\nImageNet classes).", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nDefaults to 'softmax'.\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.efficientnet.EfficientNetB1": {"description": "Instantiates the EfficientNetB1 architecture.", "Args": {"include_top": "Whether to include the fully-connected\nlayer at the top of the network. Defaults to True.", "weights": "One of None (random initialization),\n'imagenet' (pre-training on ImageNet),\nor the path to the weights file to be loaded. Defaults to 'imagenet'.", "input_tensor": "Optional Keras tensor\n(i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "Optional shape tuple, only to be specified\nif include_top is False.\nIt should have exactly 3 inputs channels.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False. Defaults to None.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional layer.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "Optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified. Defaults to 1000 (number of\nImageNet classes).", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nDefaults to 'softmax'.\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.efficientnet.EfficientNetB2": {"description": "Instantiates the EfficientNetB2 architecture.", "Args": {"include_top": "Whether to include the fully-connected\nlayer at the top of the network. Defaults to True.", "weights": "One of None (random initialization),\n'imagenet' (pre-training on ImageNet),\nor the path to the weights file to be loaded. Defaults to 'imagenet'.", "input_tensor": "Optional Keras tensor\n(i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "Optional shape tuple, only to be specified\nif include_top is False.\nIt should have exactly 3 inputs channels.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False. Defaults to None.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional layer.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "Optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified. Defaults to 1000 (number of\nImageNet classes).", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nDefaults to 'softmax'.\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.efficientnet.EfficientNetB3": {"description": "Instantiates the EfficientNetB3 architecture.", "Args": {"include_top": "Whether to include the fully-connected\nlayer at the top of the network. Defaults to True.", "weights": "One of None (random initialization),\n'imagenet' (pre-training on ImageNet),\nor the path to the weights file to be loaded. Defaults to 'imagenet'.", "input_tensor": "Optional Keras tensor\n(i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "Optional shape tuple, only to be specified\nif include_top is False.\nIt should have exactly 3 inputs channels.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False. Defaults to None.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional layer.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "Optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified. Defaults to 1000 (number of\nImageNet classes).", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nDefaults to 'softmax'.\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.efficientnet.EfficientNetB4": {"description": "Instantiates the EfficientNetB4 architecture.", "Args": {"include_top": "Whether to include the fully-connected\nlayer at the top of the network. Defaults to True.", "weights": "One of None (random initialization),\n'imagenet' (pre-training on ImageNet),\nor the path to the weights file to be loaded. Defaults to 'imagenet'.", "input_tensor": "Optional Keras tensor\n(i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "Optional shape tuple, only to be specified\nif include_top is False.\nIt should have exactly 3 inputs channels.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False. Defaults to None.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional layer.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "Optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified. Defaults to 1000 (number of\nImageNet classes).", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nDefaults to 'softmax'.\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.efficientnet.EfficientNetB5": {"description": "Instantiates the EfficientNetB5 architecture.", "Args": {"include_top": "Whether to include the fully-connected\nlayer at the top of the network. Defaults to True.", "weights": "One of None (random initialization),\n'imagenet' (pre-training on ImageNet),\nor the path to the weights file to be loaded. Defaults to 'imagenet'.", "input_tensor": "Optional Keras tensor\n(i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "Optional shape tuple, only to be specified\nif include_top is False.\nIt should have exactly 3 inputs channels.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False. Defaults to None.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional layer.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "Optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified. Defaults to 1000 (number of\nImageNet classes).", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nDefaults to 'softmax'.\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.efficientnet.EfficientNetB6": {"description": "Instantiates the EfficientNetB6 architecture.", "Args": {"include_top": "Whether to include the fully-connected\nlayer at the top of the network. Defaults to True.", "weights": "One of None (random initialization),\n'imagenet' (pre-training on ImageNet),\nor the path to the weights file to be loaded. Defaults to 'imagenet'.", "input_tensor": "Optional Keras tensor\n(i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "Optional shape tuple, only to be specified\nif include_top is False.\nIt should have exactly 3 inputs channels.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False. Defaults to None.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional layer.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "Optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified. Defaults to 1000 (number of\nImageNet classes).", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nDefaults to 'softmax'.\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.efficientnet.EfficientNetB7": {"description": "Instantiates the EfficientNetB7 architecture.", "Args": {"include_top": "Whether to include the fully-connected\nlayer at the top of the network. Defaults to True.", "weights": "One of None (random initialization),\n'imagenet' (pre-training on ImageNet),\nor the path to the weights file to be loaded. Defaults to 'imagenet'.", "input_tensor": "Optional Keras tensor\n(i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "Optional shape tuple, only to be specified\nif include_top is False.\nIt should have exactly 3 inputs channels.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False. Defaults to None.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional layer.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "Optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified. Defaults to 1000 (number of\nImageNet classes).", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nDefaults to 'softmax'.\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.efficientnet.decode_predictions": {"description": "Decodes the prediction of an ImageNet model.", "Args": {"preds": "Numpy array encoding a batch of predictions.", "top": "Integer, how many top-guesses to return. Defaults to 5."}, "Returns": "A list of lists of top class prediction tuples\n(class_name, class_description, score).\nOne list of tuples per sample in batch input.", "Raises": {"ValueError": "In case of invalid shape of the pred array\n(must be 2D)."}}, "tf.keras.applications.efficientnet.preprocess_input": {"description": "A placeholder method for backward compatibility.", "Args": {"x": "A floating point numpy.array or a tf.Tensor.", "data_format": "Optional data format of the image tensor/array. Defaults to\nNone, in which case the global setting\ntf.keras.backend.image_data_format() is used (unless you changed it,\nit defaults to \"channels_last\").{mode}"}, "Returns": "Unchanged numpy.array or tf.Tensor."}}, "tf.keras.applications.efficientnet_v2": {"tf.keras.applications.efficientnet_v2.EfficientNetV2B0": {"description": "Instantiates the EfficientNetV2B0 architecture.", "Args": {"include_top": "Boolean, whether to include the fully-connected\nlayer at the top of the network. Defaults to True.", "weights": "One of None (random initialization),\n\"imagenet\" (pre-training on ImageNet),\nor the path to the weights file to be loaded. Defaults to \"imagenet\".", "input_tensor": "Optional Keras tensor\n(i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "Optional shape tuple, only to be specified\nif include_top is False.\nIt should have exactly 3 inputs channels.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False. Defaults to None.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional layer.\n\"avg\" means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\n\"max\" means that global max pooling will\nbe applied.", "classes": "Optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified. Defaults to 1000 (number of\nImageNet classes).", "classifier_activation": "A string or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nDefaults to \"softmax\".\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.efficientnet_v2.EfficientNetV2B1": {"description": "Instantiates the EfficientNetV2B1 architecture.", "Args": {"include_top": "Boolean, whether to include the fully-connected\nlayer at the top of the network. Defaults to True.", "weights": "One of None (random initialization),\n\"imagenet\" (pre-training on ImageNet),\nor the path to the weights file to be loaded. Defaults to \"imagenet\".", "input_tensor": "Optional Keras tensor\n(i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "Optional shape tuple, only to be specified\nif include_top is False.\nIt should have exactly 3 inputs channels.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False. Defaults to None.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional layer.\n\"avg\" means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\n\"max\" means that global max pooling will\nbe applied.", "classes": "Optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified. Defaults to 1000 (number of\nImageNet classes).", "classifier_activation": "A string or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nDefaults to \"softmax\".\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.efficientnet_v2.EfficientNetV2B2": {"description": "Instantiates the EfficientNetV2B2 architecture.", "Args": {"include_top": "Boolean, whether to include the fully-connected\nlayer at the top of the network. Defaults to True.", "weights": "One of None (random initialization),\n\"imagenet\" (pre-training on ImageNet),\nor the path to the weights file to be loaded. Defaults to \"imagenet\".", "input_tensor": "Optional Keras tensor\n(i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "Optional shape tuple, only to be specified\nif include_top is False.\nIt should have exactly 3 inputs channels.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False. Defaults to None.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional layer.\n\"avg\" means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\n\"max\" means that global max pooling will\nbe applied.", "classes": "Optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified. Defaults to 1000 (number of\nImageNet classes).", "classifier_activation": "A string or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nDefaults to \"softmax\".\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.efficientnet_v2.EfficientNetV2B3": {"description": "Instantiates the EfficientNetV2B3 architecture.", "Args": {"include_top": "Boolean, whether to include the fully-connected\nlayer at the top of the network. Defaults to True.", "weights": "One of None (random initialization),\n\"imagenet\" (pre-training on ImageNet),\nor the path to the weights file to be loaded. Defaults to \"imagenet\".", "input_tensor": "Optional Keras tensor\n(i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "Optional shape tuple, only to be specified\nif include_top is False.\nIt should have exactly 3 inputs channels.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False. Defaults to None.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional layer.\n\"avg\" means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\n\"max\" means that global max pooling will\nbe applied.", "classes": "Optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified. Defaults to 1000 (number of\nImageNet classes).", "classifier_activation": "A string or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nDefaults to \"softmax\".\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.efficientnet_v2.EfficientNetV2L": {"description": "Instantiates the EfficientNetV2L architecture.", "Args": {"include_top": "Boolean, whether to include the fully-connected\nlayer at the top of the network. Defaults to True.", "weights": "One of None (random initialization),\n\"imagenet\" (pre-training on ImageNet),\nor the path to the weights file to be loaded. Defaults to \"imagenet\".", "input_tensor": "Optional Keras tensor\n(i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "Optional shape tuple, only to be specified\nif include_top is False.\nIt should have exactly 3 inputs channels.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False. Defaults to None.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional layer.\n\"avg\" means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\n\"max\" means that global max pooling will\nbe applied.", "classes": "Optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified. Defaults to 1000 (number of\nImageNet classes).", "classifier_activation": "A string or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nDefaults to \"softmax\".\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.efficientnet_v2.EfficientNetV2M": {"description": "Instantiates the EfficientNetV2M architecture.", "Args": {"include_top": "Boolean, whether to include the fully-connected\nlayer at the top of the network. Defaults to True.", "weights": "One of None (random initialization),\n\"imagenet\" (pre-training on ImageNet),\nor the path to the weights file to be loaded. Defaults to \"imagenet\".", "input_tensor": "Optional Keras tensor\n(i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "Optional shape tuple, only to be specified\nif include_top is False.\nIt should have exactly 3 inputs channels.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False. Defaults to None.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional layer.\n\"avg\" means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\n\"max\" means that global max pooling will\nbe applied.", "classes": "Optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified. Defaults to 1000 (number of\nImageNet classes).", "classifier_activation": "A string or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nDefaults to \"softmax\".\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.efficientnet_v2.EfficientNetV2S": {"description": "Instantiates the EfficientNetV2S architecture.", "Args": {"include_top": "Boolean, whether to include the fully-connected\nlayer at the top of the network. Defaults to True.", "weights": "One of None (random initialization),\n\"imagenet\" (pre-training on ImageNet),\nor the path to the weights file to be loaded. Defaults to \"imagenet\".", "input_tensor": "Optional Keras tensor\n(i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "Optional shape tuple, only to be specified\nif include_top is False.\nIt should have exactly 3 inputs channels.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False. Defaults to None.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional layer.\n\"avg\" means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\n\"max\" means that global max pooling will\nbe applied.", "classes": "Optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified. Defaults to 1000 (number of\nImageNet classes).", "classifier_activation": "A string or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nDefaults to \"softmax\".\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.efficientnet_v2.decode_predictions": {"description": "Decodes the prediction of an ImageNet model.", "Args": {"preds": "Numpy array encoding a batch of predictions.", "top": "Integer, how many top-guesses to return. Defaults to 5."}, "Returns": "A list of lists of top class prediction tuples\n(class_name, class_description, score).\nOne list of tuples per sample in batch input.", "Raises": {"ValueError": "In case of invalid shape of the pred array\n(must be 2D)."}}, "tf.keras.applications.efficientnet_v2.preprocess_input": {"description": "A placeholder method for backward compatibility.", "Args": {"x": "A floating point numpy.array or a tf.Tensor.", "data_format": "Optional data format of the image tensor/array. Defaults to\nNone, in which case the global setting\ntf.keras.backend.image_data_format() is used (unless you changed it, it\ndefaults to \"channels_last\").{mode}"}, "Returns": "Unchanged numpy.array or tf.Tensor."}}, "tf.keras.applications.imagenet_utils": {"tf.keras.applications.imagenet_utils.decode_predictions": {"description": "Decodes the prediction of an ImageNet model.", "Args": {"preds": "Numpy array encoding a batch of predictions.", "top": "Integer, how many top-guesses to return. Defaults to 5."}, "Returns": "A list of lists of top class prediction tuples\n(class_name, class_description, score).\nOne list of tuples per sample in batch input.", "Raises": {"ValueError": "In case of invalid shape of the pred array\n(must be 2D)."}}, "tf.keras.applications.imagenet_utils.preprocess_input": {"description": "Preprocesses a tensor or Numpy array encoding a batch of images.", "Args": {"x": "A floating point numpy.array or a tf.Tensor, 3D or 4D with 3 color\nchannels, with values in the range [0, 255].\nThe preprocessed data are written over the input data\nif the data types are compatible. To avoid this\nbehaviour, numpy.copy(x) can be used.", "data_format": "Optional data format of the image tensor/array. Defaults to\nNone, in which case the global setting\ntf.keras.backend.image_data_format() is used (unless you changed it,\nit defaults to \"channels_last\").", "mode": "One of \"caffe\", \"tf\" or \"torch\". Defaults to \"caffe\".\n\ncaffe: will convert the images from RGB to BGR,\nthen will zero-center each color channel with\nrespect to the ImageNet dataset,\nwithout scaling.\ntf: will scale pixels between -1 and 1,\nsample-wise.\ntorch: will scale pixels between 0 and 1 and then\nwill normalize each channel with respect to the\nImageNet dataset."}, "Returns": "Preprocessed numpy.array or a tf.Tensor with type float32.", "Raises": {"ValueError": "In case of unknown mode or data_format argument."}}}, "tf.keras.applications.inception_resnet_v2": {"tf.keras.applications.inception_resnet_v2.InceptionResNetV2": {"description": "Instantiates the Inception-ResNet v2 architecture.", "Args": {"include_top": "whether to include the fully-connected\nlayer at the top of the network.", "weights": "one of None (random initialization),\n'imagenet' (pre-training on ImageNet),\nor the path to the weights file to be loaded.", "input_tensor": "optional Keras tensor (i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "optional shape tuple, only to be specified\nif include_top is False (otherwise the input shape\nhas to be (299, 299, 3) (with 'channels_last' data format)\nor (3, 299, 299) (with 'channels_first' data format).\nIt should have exactly 3 inputs channels,\nand width and height should be no smaller than 75.\nE.g. (150, 150, 3) would be one valid value.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False.\n\nNone means that the output of the model will be\nthe 4D tensor output of the last convolutional block.\n'avg' means that global average pooling\nwill be applied to the output of the\nlast convolutional block, and thus\nthe output of the model will be a 2D tensor.\n'max' means that global max pooling will be applied.", "classes": "optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified.", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\".", "**kwargs": "For backwards compatibility only."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.inception_resnet_v2.decode_predictions": {"description": "Decodes the prediction of an ImageNet model.", "Args": {"preds": "Numpy array encoding a batch of predictions.", "top": "Integer, how many top-guesses to return. Defaults to 5."}, "Returns": "A list of lists of top class prediction tuples\n(class_name, class_description, score).\nOne list of tuples per sample in batch input.", "Raises": {"ValueError": "In case of invalid shape of the pred array\n(must be 2D)."}}, "tf.keras.applications.inception_resnet_v2.preprocess_input": {"description": "Preprocesses a tensor or Numpy array encoding a batch of images.", "Args": {"x": "A floating point numpy.array or a tf.Tensor, 3D or 4D with 3 color\nchannels, with values in the range [0, 255].\nThe preprocessed data are written over the input data\nif the data types are compatible. To avoid this\nbehaviour, numpy.copy(x) can be used.", "data_format": "Optional data format of the image tensor/array. Defaults to\nNone, in which case the global setting\ntf.keras.backend.image_data_format() is used (unless you changed it,\nit defaults to \"channels_last\")."}, "Returns": "Preprocessed numpy.array or a tf.Tensor with type float32.\nThe inputs pixel values are scaled between -1 and 1, sample-wise.", "Raises": {"ValueError": "In case of unknown data_format argument."}}}, "tf.keras.applications.inception_v3": {"tf.keras.applications.inception_v3.InceptionV3": {"description": "Instantiates the Inception v3 architecture.", "Args": {"include_top": "Boolean, whether to include the fully-connected\nlayer at the top, as the last layer of the network. Default to True.", "weights": "One of None (random initialization),\nimagenet (pre-training on ImageNet),\nor the path to the weights file to be loaded. Default to imagenet.", "input_tensor": "Optional Keras tensor (i.e. output of layers.Input())\nto use as image input for the model. input_tensor is useful for sharing\ninputs between multiple different networks. Default to None.", "input_shape": "Optional shape tuple, only to be specified\nif include_top is False (otherwise the input shape\nhas to be (299, 299, 3) (with channels_last data format)\nor (3, 299, 299) (with channels_first data format).\nIt should have exactly 3 inputs channels,\nand width and height should be no smaller than 75.\nE.g. (150, 150, 3) would be one valid value.\ninput_shape will be ignored if the input_tensor is provided.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False.\n\nNone (default) means that the output of the model will be\nthe 4D tensor output of the last convolutional block.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional block, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will be applied.", "classes": "optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified. Default to 1000.", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.inception_v3.decode_predictions": {"description": "Decodes the prediction of an ImageNet model.", "Args": {"preds": "Numpy array encoding a batch of predictions.", "top": "Integer, how many top-guesses to return. Defaults to 5."}, "Returns": "A list of lists of top class prediction tuples\n(class_name, class_description, score).\nOne list of tuples per sample in batch input.", "Raises": {"ValueError": "In case of invalid shape of the pred array\n(must be 2D)."}}, "tf.keras.applications.inception_v3.preprocess_input": {"description": "Preprocesses a tensor or Numpy array encoding a batch of images.", "Args": {"x": "A floating point numpy.array or a tf.Tensor, 3D or 4D with 3 color\nchannels, with values in the range [0, 255].\nThe preprocessed data are written over the input data\nif the data types are compatible. To avoid this\nbehaviour, numpy.copy(x) can be used.", "data_format": "Optional data format of the image tensor/array. Defaults to\nNone, in which case the global setting\ntf.keras.backend.image_data_format() is used (unless you changed it,\nit defaults to \"channels_last\")."}, "Returns": "Preprocessed numpy.array or a tf.Tensor with type float32.\nThe inputs pixel values are scaled between -1 and 1, sample-wise.", "Raises": {"ValueError": "In case of unknown data_format argument."}}}, "tf.keras.applications.mobilenet": {"tf.keras.applications.mobilenet.MobileNet": {"description": "Instantiates the MobileNet architecture.", "Args": {"input_shape": "Optional shape tuple, only to be specified if include_top\nis False (otherwise the input shape has to be (224, 224, 3) (with\nchannels_last data format) or (3, 224, 224) (with channels_first\ndata format). It should have exactly 3 inputs channels, and width and\nheight should be no smaller than 32. E.g. (200, 200, 3) would be one\nvalid value. Default to None.\ninput_shape will be ignored if the input_tensor is provided.", "alpha": "Controls the width of the network. This is known as the width\nmultiplier in the MobileNet paper. - If alpha < 1.0, proportionally\ndecreases the number of filters in each layer. - If alpha > 1.0,\nproportionally increases the number of filters in each layer. - If\nalpha = 1, default number of filters from the paper are used at each\nlayer. Default to 1.0.", "depth_multiplier": "Depth multiplier for depthwise convolution. This is\ncalled the resolution multiplier in the MobileNet paper. Default to 1.0.", "dropout": "Dropout rate. Default to 0.001.", "include_top": "Boolean, whether to include the fully-connected layer at the\ntop of the network. Default to True.", "weights": "One of None (random initialization), 'imagenet' (pre-training\non ImageNet), or the path to the weights file to be loaded. Default to\nimagenet.", "input_tensor": "Optional Keras tensor (i.e. output of layers.Input()) to\nuse as image input for the model. input_tensor is useful for sharing\ninputs between multiple different networks. Default to None.", "pooling": "Optional pooling mode for feature extraction when include_top\nis False.\n\nNone (default) means that the output of the model will be\nthe 4D tensor output of the last convolutional block.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional block, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will be applied.", "classes": "Optional number of classes to classify images into, only to be\nspecified if include_top is True, and if no weights argument is\nspecified. Defaults to 1000.", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\".", "**kwargs": "For backwards compatibility only."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.mobilenet.decode_predictions": {"description": "Decodes the prediction of an ImageNet model.", "Args": {"preds": "Numpy array encoding a batch of predictions.", "top": "Integer, how many top-guesses to return. Defaults to 5."}, "Returns": "A list of lists of top class prediction tuples\n(class_name, class_description, score).\nOne list of tuples per sample in batch input.", "Raises": {"ValueError": "In case of invalid shape of the pred array\n(must be 2D)."}}, "tf.keras.applications.mobilenet.preprocess_input": {"description": "Preprocesses a tensor or Numpy array encoding a batch of images.", "Args": {"x": "A floating point numpy.array or a tf.Tensor, 3D or 4D with 3 color\nchannels, with values in the range [0, 255].\nThe preprocessed data are written over the input data\nif the data types are compatible. To avoid this\nbehaviour, numpy.copy(x) can be used.", "data_format": "Optional data format of the image tensor/array. Defaults to\nNone, in which case the global setting\ntf.keras.backend.image_data_format() is used (unless you changed it,\nit defaults to \"channels_last\")."}, "Returns": "Preprocessed numpy.array or a tf.Tensor with type float32.\nThe inputs pixel values are scaled between -1 and 1, sample-wise.", "Raises": {"ValueError": "In case of unknown data_format argument."}}}, "tf.keras.applications.mobilenet_v2": {"tf.keras.applications.mobilenet_v2.MobileNetV2": {"description": "Instantiates the MobileNetV2 architecture.", "Args": {"input_shape": "Optional shape tuple, to be specified if you would\nlike to use a model with an input image resolution that is not\n(224, 224, 3).\nIt should have exactly 3 inputs channels (224, 224, 3).\nYou can also omit this option if you would like\nto infer input_shape from an input_tensor.\nIf you choose to include both input_tensor and input_shape then\ninput_shape will be used if they match, if the shapes\ndo not match then we will throw an error.\nE.g. (160, 160, 3) would be one valid value.", "alpha": "Float, larger than zero, controls the width of the network. This is\nknown as the width multiplier in the MobileNetV2 paper, but the name is\nkept for consistency with applications.MobileNetV1 model in Keras.\n\nIf alpha < 1.0, proportionally decreases the number\nof filters in each layer.\nIf alpha > 1.0, proportionally increases the number\nof filters in each layer.\nIf alpha = 1.0, default number of filters from the paper\nare used at each layer.", "include_top": "Boolean, whether to include the fully-connected layer at the\ntop of the network. Defaults to True.", "weights": "String, one of None (random initialization), 'imagenet'\n(pre-training on ImageNet), or the path to the weights file to be loaded.", "input_tensor": "Optional Keras tensor (i.e. output of layers.Input())\nto use as image input for the model.", "pooling": "String, optional pooling mode for feature extraction when\ninclude_top is False.\nNone means that the output of the model\nwill be the 4D tensor output of the\nlast convolutional block.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional block, and thus\nthe output of the model will be a\n2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "Optional integer number of classes to classify images into, only to\nbe specified if include_top is True, and if no weights argument is\nspecified.", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\".", "**kwargs": "For backwards compatibility only."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.mobilenet_v2.decode_predictions": {"description": "Decodes the prediction of an ImageNet model.", "Args": {"preds": "Numpy array encoding a batch of predictions.", "top": "Integer, how many top-guesses to return. Defaults to 5."}, "Returns": "A list of lists of top class prediction tuples\n(class_name, class_description, score).\nOne list of tuples per sample in batch input.", "Raises": {"ValueError": "In case of invalid shape of the pred array\n(must be 2D)."}}, "tf.keras.applications.mobilenet_v2.preprocess_input": {"description": "Preprocesses a tensor or Numpy array encoding a batch of images.", "Args": {"x": "A floating point numpy.array or a tf.Tensor, 3D or 4D with 3 color\nchannels, with values in the range [0, 255].\nThe preprocessed data are written over the input data\nif the data types are compatible. To avoid this\nbehaviour, numpy.copy(x) can be used.", "data_format": "Optional data format of the image tensor/array. Defaults to\nNone, in which case the global setting\ntf.keras.backend.image_data_format() is used (unless you changed it,\nit defaults to \"channels_last\")."}, "Returns": "Preprocessed numpy.array or a tf.Tensor with type float32.\nThe inputs pixel values are scaled between -1 and 1, sample-wise.", "Raises": {"ValueError": "In case of unknown data_format argument."}}}, "tf.keras.applications.mobilenet_v3": {"tf.keras.applications.mobilenet_v3.decode_predictions": {"description": "Decodes the prediction of an ImageNet model.", "Args": {"preds": "Numpy array encoding a batch of predictions.", "top": "Integer, how many top-guesses to return. Defaults to 5."}, "Returns": "A list of lists of top class prediction tuples\n(class_name, class_description, score).\nOne list of tuples per sample in batch input.", "Raises": {"ValueError": "In case of invalid shape of the pred array\n(must be 2D)."}}, "tf.keras.applications.mobilenet_v3.preprocess_input": {"description": "A placeholder method for backward compatibility.", "Args": {"x": "A floating point numpy.array or a tf.Tensor.", "data_format": "Optional data format of the image tensor/array. Defaults to\nNone, in which case the global setting\ntf.keras.backend.image_data_format() is used (unless you changed it,\nit defaults to \"channels_last\").{mode}"}, "Returns": "Unchanged numpy.array or tf.Tensor."}}, "tf.keras.applications.nasnet": {"tf.keras.applications.nasnet.NASNetLarge": {"description": "Instantiates a NASNet model in ImageNet mode.", "Args": {"input_shape": "Optional shape tuple, only to be specified\nif include_top is False (otherwise the input shape\nhas to be (331, 331, 3) for NASNetLarge.\nIt should have exactly 3 inputs channels,\nand width and height should be no smaller than 32.\nE.g. (224, 224, 3) would be one valid value.", "include_top": "Whether to include the fully-connected\nlayer at the top of the network.", "weights": "None (random initialization) or\nimagenet (ImageNet weights)\nFor loading imagenet weights, input_shape should be (331, 331, 3)", "input_tensor": "Optional Keras tensor (i.e. output of\nlayers.Input())\nto use as image input for the model.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False.\n\nNone means that the output of the model\nwill be the 4D tensor output of the\nlast convolutional layer.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a\n2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "Optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified.", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A Keras model instance.", "Raises": {"ValueError": "in case of invalid argument for weights,\nor invalid input shape.", "RuntimeError": "If attempting to run this model with a\nbackend that does not support separable convolutions."}}, "tf.keras.applications.nasnet.NASNetMobile": {"description": "Instantiates a Mobile NASNet model in ImageNet mode.", "Args": {"input_shape": "Optional shape tuple, only to be specified\nif include_top is False (otherwise the input shape\nhas to be (224, 224, 3) for NASNetMobile\nIt should have exactly 3 inputs channels,\nand width and height should be no smaller than 32.\nE.g. (224, 224, 3) would be one valid value.", "include_top": "Whether to include the fully-connected\nlayer at the top of the network.", "weights": "None (random initialization) or\nimagenet (ImageNet weights)\nFor loading imagenet weights, input_shape should be (224, 224, 3)", "input_tensor": "Optional Keras tensor (i.e. output of\nlayers.Input())\nto use as image input for the model.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False.\n\nNone means that the output of the model\nwill be the 4D tensor output of the\nlast convolutional layer.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a\n2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "Optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified.", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A Keras model instance.", "Raises": {"ValueError": "In case of invalid argument for weights,\nor invalid input shape.", "RuntimeError": "If attempting to run this model with a\nbackend that does not support separable convolutions."}}, "tf.keras.applications.nasnet.decode_predictions": {"description": "Decodes the prediction of an ImageNet model.", "Args": {"preds": "Numpy array encoding a batch of predictions.", "top": "Integer, how many top-guesses to return. Defaults to 5."}, "Returns": "A list of lists of top class prediction tuples\n(class_name, class_description, score).\nOne list of tuples per sample in batch input.", "Raises": {"ValueError": "In case of invalid shape of the pred array\n(must be 2D)."}}, "tf.keras.applications.nasnet.preprocess_input": {"description": "Preprocesses a tensor or Numpy array encoding a batch of images.", "Args": {"x": "A floating point numpy.array or a tf.Tensor, 3D or 4D with 3 color\nchannels, with values in the range [0, 255].\nThe preprocessed data are written over the input data\nif the data types are compatible. To avoid this\nbehaviour, numpy.copy(x) can be used.", "data_format": "Optional data format of the image tensor/array. Defaults to\nNone, in which case the global setting\ntf.keras.backend.image_data_format() is used (unless you changed it,\nit defaults to \"channels_last\")."}, "Returns": "Preprocessed numpy.array or a tf.Tensor with type float32.\nThe inputs pixel values are scaled between -1 and 1, sample-wise.", "Raises": {"ValueError": "In case of unknown data_format argument."}}}, "tf.keras.applications.regnet": {"tf.keras.applications.regnet.RegNetX002": {"description": "Instantiates the RegNetX002 architecture.", "Args": {"include_top": "Whether to include the fully-connected\nlayer at the top of the network. Defaults to True.", "weights": "One of None (random initialization),\n\"imagenet\" (pre-training on ImageNet), or the path to the weights\nfile to be loaded. Defaults to \"imagenet\".", "input_tensor": "Optional Keras tensor\n(i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "Optional shape tuple, only to be specified\nif include_top is False.\nIt should have exactly 3 inputs channels.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False. Defaults to None.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional layer.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "Optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified. Defaults to 1000 (number of\nImageNet classes).", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nDefaults to \"softmax\".\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.regnet.RegNetX004": {"description": "Instantiates the RegNetX004 architecture.", "Args": {"include_top": "Whether to include the fully-connected\nlayer at the top of the network. Defaults to True.", "weights": "One of None (random initialization),\n\"imagenet\" (pre-training on ImageNet), or the path to the weights\nfile to be loaded. Defaults to \"imagenet\".", "input_tensor": "Optional Keras tensor\n(i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "Optional shape tuple, only to be specified\nif include_top is False.\nIt should have exactly 3 inputs channels.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False. Defaults to None.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional layer.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "Optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified. Defaults to 1000 (number of\nImageNet classes).", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nDefaults to \"softmax\".\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.regnet.RegNetX006": {"description": "Instantiates the RegNetX006 architecture.", "Args": {"include_top": "Whether to include the fully-connected\nlayer at the top of the network. Defaults to True.", "weights": "One of None (random initialization),\n\"imagenet\" (pre-training on ImageNet), or the path to the weights\nfile to be loaded. Defaults to \"imagenet\".", "input_tensor": "Optional Keras tensor\n(i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "Optional shape tuple, only to be specified\nif include_top is False.\nIt should have exactly 3 inputs channels.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False. Defaults to None.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional layer.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "Optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified. Defaults to 1000 (number of\nImageNet classes).", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nDefaults to \"softmax\".\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.regnet.RegNetX008": {"description": "Instantiates the RegNetX008 architecture.", "Args": {"include_top": "Whether to include the fully-connected\nlayer at the top of the network. Defaults to True.", "weights": "One of None (random initialization),\n\"imagenet\" (pre-training on ImageNet), or the path to the weights\nfile to be loaded. Defaults to \"imagenet\".", "input_tensor": "Optional Keras tensor\n(i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "Optional shape tuple, only to be specified\nif include_top is False.\nIt should have exactly 3 inputs channels.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False. Defaults to None.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional layer.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "Optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified. Defaults to 1000 (number of\nImageNet classes).", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nDefaults to \"softmax\".\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.regnet.RegNetX016": {"description": "Instantiates the RegNetX016 architecture.", "Args": {"include_top": "Whether to include the fully-connected\nlayer at the top of the network. Defaults to True.", "weights": "One of None (random initialization),\n\"imagenet\" (pre-training on ImageNet), or the path to the weights\nfile to be loaded. Defaults to \"imagenet\".", "input_tensor": "Optional Keras tensor\n(i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "Optional shape tuple, only to be specified\nif include_top is False.\nIt should have exactly 3 inputs channels.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False. Defaults to None.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional layer.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "Optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified. Defaults to 1000 (number of\nImageNet classes).", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nDefaults to \"softmax\".\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.regnet.RegNetX032": {"description": "Instantiates the RegNetX032 architecture.", "Args": {"include_top": "Whether to include the fully-connected\nlayer at the top of the network. Defaults to True.", "weights": "One of None (random initialization),\n\"imagenet\" (pre-training on ImageNet), or the path to the weights\nfile to be loaded. Defaults to \"imagenet\".", "input_tensor": "Optional Keras tensor\n(i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "Optional shape tuple, only to be specified\nif include_top is False.\nIt should have exactly 3 inputs channels.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False. Defaults to None.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional layer.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "Optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified. Defaults to 1000 (number of\nImageNet classes).", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nDefaults to \"softmax\".\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.regnet.RegNetX040": {"description": "Instantiates the RegNetX040 architecture.", "Args": {"include_top": "Whether to include the fully-connected\nlayer at the top of the network. Defaults to True.", "weights": "One of None (random initialization),\n\"imagenet\" (pre-training on ImageNet), or the path to the weights\nfile to be loaded. Defaults to \"imagenet\".", "input_tensor": "Optional Keras tensor\n(i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "Optional shape tuple, only to be specified\nif include_top is False.\nIt should have exactly 3 inputs channels.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False. Defaults to None.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional layer.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "Optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified. Defaults to 1000 (number of\nImageNet classes).", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nDefaults to \"softmax\".\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.regnet.RegNetX064": {"description": "Instantiates the RegNetX064 architecture.", "Args": {"include_top": "Whether to include the fully-connected\nlayer at the top of the network. Defaults to True.", "weights": "One of None (random initialization),\n\"imagenet\" (pre-training on ImageNet), or the path to the weights\nfile to be loaded. Defaults to \"imagenet\".", "input_tensor": "Optional Keras tensor\n(i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "Optional shape tuple, only to be specified\nif include_top is False.\nIt should have exactly 3 inputs channels.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False. Defaults to None.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional layer.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "Optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified. Defaults to 1000 (number of\nImageNet classes).", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nDefaults to \"softmax\".\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.regnet.RegNetX080": {"description": "Instantiates the RegNetX080 architecture.", "Args": {"include_top": "Whether to include the fully-connected\nlayer at the top of the network. Defaults to True.", "weights": "One of None (random initialization),\n\"imagenet\" (pre-training on ImageNet), or the path to the weights\nfile to be loaded. Defaults to \"imagenet\".", "input_tensor": "Optional Keras tensor\n(i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "Optional shape tuple, only to be specified\nif include_top is False.\nIt should have exactly 3 inputs channels.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False. Defaults to None.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional layer.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "Optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified. Defaults to 1000 (number of\nImageNet classes).", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nDefaults to \"softmax\".\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.regnet.RegNetX120": {"description": "Instantiates the RegNetX120 architecture.", "Args": {"include_top": "Whether to include the fully-connected\nlayer at the top of the network. Defaults to True.", "weights": "One of None (random initialization),\n\"imagenet\" (pre-training on ImageNet), or the path to the weights\nfile to be loaded. Defaults to \"imagenet\".", "input_tensor": "Optional Keras tensor\n(i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "Optional shape tuple, only to be specified\nif include_top is False.\nIt should have exactly 3 inputs channels.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False. Defaults to None.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional layer.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "Optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified. Defaults to 1000 (number of\nImageNet classes).", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nDefaults to \"softmax\".\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.regnet.RegNetX160": {"description": "Instantiates the RegNetX160 architecture.", "Args": {"include_top": "Whether to include the fully-connected\nlayer at the top of the network. Defaults to True.", "weights": "One of None (random initialization),\n\"imagenet\" (pre-training on ImageNet), or the path to the weights\nfile to be loaded. Defaults to \"imagenet\".", "input_tensor": "Optional Keras tensor\n(i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "Optional shape tuple, only to be specified\nif include_top is False.\nIt should have exactly 3 inputs channels.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False. Defaults to None.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional layer.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "Optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified. Defaults to 1000 (number of\nImageNet classes).", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nDefaults to \"softmax\".\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.regnet.RegNetX320": {"description": "Instantiates the RegNetX320 architecture.", "Args": {"include_top": "Whether to include the fully-connected\nlayer at the top of the network. Defaults to True.", "weights": "One of None (random initialization),\n\"imagenet\" (pre-training on ImageNet), or the path to the weights\nfile to be loaded. Defaults to \"imagenet\".", "input_tensor": "Optional Keras tensor\n(i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "Optional shape tuple, only to be specified\nif include_top is False.\nIt should have exactly 3 inputs channels.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False. Defaults to None.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional layer.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "Optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified. Defaults to 1000 (number of\nImageNet classes).", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nDefaults to \"softmax\".\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.regnet.RegNetY002": {"description": "Instantiates the RegNetY002 architecture.", "Args": {"include_top": "Whether to include the fully-connected\nlayer at the top of the network. Defaults to True.", "weights": "One of None (random initialization),\n\"imagenet\" (pre-training on ImageNet), or the path to the weights\nfile to be loaded. Defaults to \"imagenet\".", "input_tensor": "Optional Keras tensor\n(i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "Optional shape tuple, only to be specified\nif include_top is False.\nIt should have exactly 3 inputs channels.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False. Defaults to None.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional layer.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "Optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified. Defaults to 1000 (number of\nImageNet classes).", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nDefaults to \"softmax\".\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.regnet.RegNetY004": {"description": "Instantiates the RegNetY004 architecture.", "Args": {"include_top": "Whether to include the fully-connected\nlayer at the top of the network. Defaults to True.", "weights": "One of None (random initialization),\n\"imagenet\" (pre-training on ImageNet), or the path to the weights\nfile to be loaded. Defaults to \"imagenet\".", "input_tensor": "Optional Keras tensor\n(i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "Optional shape tuple, only to be specified\nif include_top is False.\nIt should have exactly 3 inputs channels.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False. Defaults to None.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional layer.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "Optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified. Defaults to 1000 (number of\nImageNet classes).", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nDefaults to \"softmax\".\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.regnet.RegNetY006": {"description": "Instantiates the RegNetY006 architecture.", "Args": {"include_top": "Whether to include the fully-connected\nlayer at the top of the network. Defaults to True.", "weights": "One of None (random initialization),\n\"imagenet\" (pre-training on ImageNet), or the path to the weights\nfile to be loaded. Defaults to \"imagenet\".", "input_tensor": "Optional Keras tensor\n(i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "Optional shape tuple, only to be specified\nif include_top is False.\nIt should have exactly 3 inputs channels.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False. Defaults to None.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional layer.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "Optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified. Defaults to 1000 (number of\nImageNet classes).", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nDefaults to \"softmax\".\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.regnet.RegNetY008": {"description": "Instantiates the RegNetY008 architecture.", "Args": {"include_top": "Whether to include the fully-connected\nlayer at the top of the network. Defaults to True.", "weights": "One of None (random initialization),\n\"imagenet\" (pre-training on ImageNet), or the path to the weights\nfile to be loaded. Defaults to \"imagenet\".", "input_tensor": "Optional Keras tensor\n(i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "Optional shape tuple, only to be specified\nif include_top is False.\nIt should have exactly 3 inputs channels.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False. Defaults to None.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional layer.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "Optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified. Defaults to 1000 (number of\nImageNet classes).", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nDefaults to \"softmax\".\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.regnet.RegNetY016": {"description": "Instantiates the RegNetY016 architecture.", "Args": {"include_top": "Whether to include the fully-connected\nlayer at the top of the network. Defaults to True.", "weights": "One of None (random initialization),\n\"imagenet\" (pre-training on ImageNet), or the path to the weights\nfile to be loaded. Defaults to \"imagenet\".", "input_tensor": "Optional Keras tensor\n(i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "Optional shape tuple, only to be specified\nif include_top is False.\nIt should have exactly 3 inputs channels.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False. Defaults to None.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional layer.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "Optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified. Defaults to 1000 (number of\nImageNet classes).", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nDefaults to \"softmax\".\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.regnet.RegNetY032": {"description": "Instantiates the RegNetY032 architecture.", "Args": {"include_top": "Whether to include the fully-connected\nlayer at the top of the network. Defaults to True.", "weights": "One of None (random initialization),\n\"imagenet\" (pre-training on ImageNet), or the path to the weights\nfile to be loaded. Defaults to \"imagenet\".", "input_tensor": "Optional Keras tensor\n(i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "Optional shape tuple, only to be specified\nif include_top is False.\nIt should have exactly 3 inputs channels.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False. Defaults to None.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional layer.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "Optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified. Defaults to 1000 (number of\nImageNet classes).", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nDefaults to \"softmax\".\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.regnet.RegNetY040": {"description": "Instantiates the RegNetY040 architecture.", "Args": {"include_top": "Whether to include the fully-connected\nlayer at the top of the network. Defaults to True.", "weights": "One of None (random initialization),\n\"imagenet\" (pre-training on ImageNet), or the path to the weights\nfile to be loaded. Defaults to \"imagenet\".", "input_tensor": "Optional Keras tensor\n(i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "Optional shape tuple, only to be specified\nif include_top is False.\nIt should have exactly 3 inputs channels.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False. Defaults to None.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional layer.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "Optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified. Defaults to 1000 (number of\nImageNet classes).", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nDefaults to \"softmax\".\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.regnet.RegNetY064": {"description": "Instantiates the RegNetY064 architecture.", "Args": {"include_top": "Whether to include the fully-connected\nlayer at the top of the network. Defaults to True.", "weights": "One of None (random initialization),\n\"imagenet\" (pre-training on ImageNet), or the path to the weights\nfile to be loaded. Defaults to \"imagenet\".", "input_tensor": "Optional Keras tensor\n(i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "Optional shape tuple, only to be specified\nif include_top is False.\nIt should have exactly 3 inputs channels.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False. Defaults to None.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional layer.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "Optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified. Defaults to 1000 (number of\nImageNet classes).", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nDefaults to \"softmax\".\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.regnet.RegNetY080": {"description": "Instantiates the RegNetY080 architecture.", "Args": {"include_top": "Whether to include the fully-connected\nlayer at the top of the network. Defaults to True.", "weights": "One of None (random initialization),\n\"imagenet\" (pre-training on ImageNet), or the path to the weights\nfile to be loaded. Defaults to \"imagenet\".", "input_tensor": "Optional Keras tensor\n(i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "Optional shape tuple, only to be specified\nif include_top is False.\nIt should have exactly 3 inputs channels.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False. Defaults to None.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional layer.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "Optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified. Defaults to 1000 (number of\nImageNet classes).", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nDefaults to \"softmax\".\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.regnet.RegNetY120": {"description": "Instantiates the RegNetY120 architecture.", "Args": {"include_top": "Whether to include the fully-connected\nlayer at the top of the network. Defaults to True.", "weights": "One of None (random initialization),\n\"imagenet\" (pre-training on ImageNet), or the path to the weights\nfile to be loaded. Defaults to \"imagenet\".", "input_tensor": "Optional Keras tensor\n(i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "Optional shape tuple, only to be specified\nif include_top is False.\nIt should have exactly 3 inputs channels.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False. Defaults to None.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional layer.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "Optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified. Defaults to 1000 (number of\nImageNet classes).", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nDefaults to \"softmax\".\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.regnet.RegNetY160": {"description": "Instantiates the RegNetY160 architecture.", "Args": {"include_top": "Whether to include the fully-connected\nlayer at the top of the network. Defaults to True.", "weights": "One of None (random initialization),\n\"imagenet\" (pre-training on ImageNet), or the path to the weights\nfile to be loaded. Defaults to \"imagenet\".", "input_tensor": "Optional Keras tensor\n(i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "Optional shape tuple, only to be specified\nif include_top is False.\nIt should have exactly 3 inputs channels.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False. Defaults to None.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional layer.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "Optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified. Defaults to 1000 (number of\nImageNet classes).", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nDefaults to \"softmax\".\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.regnet.RegNetY320": {"description": "Instantiates the RegNetY320 architecture.", "Args": {"include_top": "Whether to include the fully-connected\nlayer at the top of the network. Defaults to True.", "weights": "One of None (random initialization),\n\"imagenet\" (pre-training on ImageNet), or the path to the weights\nfile to be loaded. Defaults to \"imagenet\".", "input_tensor": "Optional Keras tensor\n(i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "Optional shape tuple, only to be specified\nif include_top is False.\nIt should have exactly 3 inputs channels.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False. Defaults to None.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional layer.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "Optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified. Defaults to 1000 (number of\nImageNet classes).", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nDefaults to \"softmax\".\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.regnet.decode_predictions": {"description": "Decodes the prediction of an ImageNet model.", "Args": {"preds": "Numpy array encoding a batch of predictions.", "top": "Integer, how many top-guesses to return. Defaults to 5."}, "Returns": "A list of lists of top class prediction tuples\n(class_name, class_description, score).\nOne list of tuples per sample in batch input.", "Raises": {"ValueError": "In case of invalid shape of the pred array\n(must be 2D)."}}, "tf.keras.applications.regnet.preprocess_input": {"description": "A placeholder method for backward compatibility.", "Args": {"x": "A floating point numpy.array or a tf.Tensor.", "data_format": "Optional data format of the image tensor/array. Defaults to\nNone, in which case the global setting\ntf.keras.backend.image_data_format() is used (unless you changed it, it\ndefaults to \"channels_last\").{mode}"}, "Returns": "Unchanged numpy.array or tf.Tensor."}}, "tf.keras.applications.resnet": {"tf.keras.applications.resnet.ResNet101": {"description": "Instantiates the ResNet101 architecture.", "Args": {"include_top": "whether to include the fully-connected\nlayer at the top of the network.", "weights": "one of None (random initialization),\n'imagenet' (pre-training on ImageNet),\nor the path to the weights file to be loaded.", "input_tensor": "optional Keras tensor (i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "optional shape tuple, only to be specified\nif include_top is False (otherwise the input shape\nhas to be (224, 224, 3) (with 'channels_last' data format)\nor (3, 224, 224) (with 'channels_first' data format).\nIt should have exactly 3 inputs channels,\nand width and height should be no smaller than 32.\nE.g. (200, 200, 3) would be one valid value.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional block.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional block, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified.", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A Keras model instance."}, "tf.keras.applications.resnet.ResNet152": {"description": "Instantiates the ResNet152 architecture.", "Args": {"include_top": "whether to include the fully-connected\nlayer at the top of the network.", "weights": "one of None (random initialization),\n'imagenet' (pre-training on ImageNet),\nor the path to the weights file to be loaded.", "input_tensor": "optional Keras tensor (i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "optional shape tuple, only to be specified\nif include_top is False (otherwise the input shape\nhas to be (224, 224, 3) (with 'channels_last' data format)\nor (3, 224, 224) (with 'channels_first' data format).\nIt should have exactly 3 inputs channels,\nand width and height should be no smaller than 32.\nE.g. (200, 200, 3) would be one valid value.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional block.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional block, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified.", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A Keras model instance."}}, "tf.keras.applications.resnet50": {"tf.keras.applications.resnet50.ResNet50": {"description": "Instantiates the ResNet50 architecture.", "Args": {"include_top": "whether to include the fully-connected\nlayer at the top of the network.", "weights": "one of None (random initialization),\n'imagenet' (pre-training on ImageNet),\nor the path to the weights file to be loaded.", "input_tensor": "optional Keras tensor (i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "optional shape tuple, only to be specified\nif include_top is False (otherwise the input shape\nhas to be (224, 224, 3) (with 'channels_last' data format)\nor (3, 224, 224) (with 'channels_first' data format).\nIt should have exactly 3 inputs channels,\nand width and height should be no smaller than 32.\nE.g. (200, 200, 3) would be one valid value.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional block.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional block, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified.", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A Keras model instance."}, "tf.keras.applications.resnet50.decode_predictions": {"description": "Decodes the prediction of an ImageNet model.", "Args": {"preds": "Numpy array encoding a batch of predictions.", "top": "Integer, how many top-guesses to return. Defaults to 5."}, "Returns": "A list of lists of top class prediction tuples\n(class_name, class_description, score).\nOne list of tuples per sample in batch input.", "Raises": {"ValueError": "In case of invalid shape of the pred array\n(must be 2D)."}}, "tf.keras.applications.resnet50.preprocess_input": {"description": "Preprocesses a tensor or Numpy array encoding a batch of images.", "Args": {"x": "A floating point numpy.array or a tf.Tensor, 3D or 4D with 3 color\nchannels, with values in the range [0, 255].\nThe preprocessed data are written over the input data\nif the data types are compatible. To avoid this\nbehaviour, numpy.copy(x) can be used.", "data_format": "Optional data format of the image tensor/array. Defaults to\nNone, in which case the global setting\ntf.keras.backend.image_data_format() is used (unless you changed it,\nit defaults to \"channels_last\")."}, "Returns": "Preprocessed numpy.array or a tf.Tensor with type float32.\nThe images are converted from RGB to BGR, then each color channel is\nzero-centered with respect to the ImageNet dataset, without scaling.", "Raises": {"ValueError": "In case of unknown data_format argument."}}}, "tf.keras.applications.resnet_rs": {"tf.keras.applications.resnet_rs.ResNetRS101": {"description": "Build ResNet-RS101 model."}, "tf.keras.applications.resnet_rs.ResNetRS152": {"description": "Instantiates the ResNetRS152 architecture.", "Args": {"depth": "Depth of ResNet network.", "input_shape": "optional shape tuple. It should have exactly 3 inputs\nchannels, and width and height should be no smaller than 32.\nE.g. (200, 200, 3) would be one valid value.", "bn_momentum": "Momentum parameter for Batch Normalization layers.", "bn_epsilon": "Epsilon parameter for Batch Normalization layers.", "activation": "activation function.", "se_ratio": "Squeeze and Excitation layer ratio.", "dropout_rate": "dropout rate before final classifier layer.", "drop_connect_rate": "dropout rate at skip connections.", "include_top": "whether to include the fully-connected layer at the top of\nthe network.", "block_args": "list of dicts, parameters to construct block modules.", "model_name": "name of the model.", "pooling": "optional pooling mode for feature extraction when include_top\nis False.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional layer.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "weights": "one of None (random initialization), 'imagenet'\n(pre-training on ImageNet), or the path to the weights file to be\nloaded.  Note: one model can have multiple imagenet variants\ndepending on input shape it was trained with. For input_shape\n224x224 pass imagenet-i224 as argument. By default, highest input\nshape weights are downloaded.", "input_tensor": "optional Keras tensor (i.e. output of layers.Input()) to\nuse as image input for the model.", "classes": "optional number of classes to classify images into, only to be\nspecified if include_top is True, and if no weights argument is\nspecified.", "classifier_activation": "A str or callable. The activation function to\nuse on the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.", "include_preprocessing": "Boolean, whether to include the preprocessing layer\n(Rescaling) at the bottom of the network. Defaults to True.\nNote: Input image is normalized by ImageNet mean and standard deviation."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.resnet_rs.ResNetRS200": {"description": "Instantiates the ResNetRS200 architecture.", "Args": {"depth": "Depth of ResNet network.", "input_shape": "optional shape tuple. It should have exactly 3 inputs\nchannels, and width and height should be no smaller than 32.\nE.g. (200, 200, 3) would be one valid value.", "bn_momentum": "Momentum parameter for Batch Normalization layers.", "bn_epsilon": "Epsilon parameter for Batch Normalization layers.", "activation": "activation function.", "se_ratio": "Squeeze and Excitation layer ratio.", "dropout_rate": "dropout rate before final classifier layer.", "drop_connect_rate": "dropout rate at skip connections.", "include_top": "whether to include the fully-connected layer at the top of\nthe network.", "block_args": "list of dicts, parameters to construct block modules.", "model_name": "name of the model.", "pooling": "optional pooling mode for feature extraction when include_top\nis False.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional layer.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "weights": "one of None (random initialization), 'imagenet'\n(pre-training on ImageNet), or the path to the weights file to be\nloaded.  Note: one model can have multiple imagenet variants\ndepending on input shape it was trained with. For input_shape\n224x224 pass imagenet-i224 as argument. By default, highest input\nshape weights are downloaded.", "input_tensor": "optional Keras tensor (i.e. output of layers.Input()) to\nuse as image input for the model.", "classes": "optional number of classes to classify images into, only to be\nspecified if include_top is True, and if no weights argument is\nspecified.", "classifier_activation": "A str or callable. The activation function to\nuse on the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.", "include_preprocessing": "Boolean, whether to include the preprocessing layer\n(Rescaling) at the bottom of the network. Defaults to True.\nNote: Input image is normalized by ImageNet mean and standard deviation."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.resnet_rs.ResNetRS270": {"description": "Instantiates the ResNetRS270 architecture.", "Args": {"depth": "Depth of ResNet network.", "input_shape": "optional shape tuple. It should have exactly 3 inputs\nchannels, and width and height should be no smaller than 32.\nE.g. (200, 200, 3) would be one valid value.", "bn_momentum": "Momentum parameter for Batch Normalization layers.", "bn_epsilon": "Epsilon parameter for Batch Normalization layers.", "activation": "activation function.", "se_ratio": "Squeeze and Excitation layer ratio.", "dropout_rate": "dropout rate before final classifier layer.", "drop_connect_rate": "dropout rate at skip connections.", "include_top": "whether to include the fully-connected layer at the top of\nthe network.", "block_args": "list of dicts, parameters to construct block modules.", "model_name": "name of the model.", "pooling": "optional pooling mode for feature extraction when include_top\nis False.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional layer.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "weights": "one of None (random initialization), 'imagenet'\n(pre-training on ImageNet), or the path to the weights file to be\nloaded.  Note: one model can have multiple imagenet variants\ndepending on input shape it was trained with. For input_shape\n224x224 pass imagenet-i224 as argument. By default, highest input\nshape weights are downloaded.", "input_tensor": "optional Keras tensor (i.e. output of layers.Input()) to\nuse as image input for the model.", "classes": "optional number of classes to classify images into, only to be\nspecified if include_top is True, and if no weights argument is\nspecified.", "classifier_activation": "A str or callable. The activation function to\nuse on the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.", "include_preprocessing": "Boolean, whether to include the preprocessing layer\n(Rescaling) at the bottom of the network. Defaults to True.\nNote: Input image is normalized by ImageNet mean and standard deviation."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.resnet_rs.ResNetRS350": {"description": "Instantiates the ResNetRS350 architecture.", "Args": {"depth": "Depth of ResNet network.", "input_shape": "optional shape tuple. It should have exactly 3 inputs\nchannels, and width and height should be no smaller than 32.\nE.g. (200, 200, 3) would be one valid value.", "bn_momentum": "Momentum parameter for Batch Normalization layers.", "bn_epsilon": "Epsilon parameter for Batch Normalization layers.", "activation": "activation function.", "se_ratio": "Squeeze and Excitation layer ratio.", "dropout_rate": "dropout rate before final classifier layer.", "drop_connect_rate": "dropout rate at skip connections.", "include_top": "whether to include the fully-connected layer at the top of\nthe network.", "block_args": "list of dicts, parameters to construct block modules.", "model_name": "name of the model.", "pooling": "optional pooling mode for feature extraction when include_top\nis False.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional layer.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "weights": "one of None (random initialization), 'imagenet'\n(pre-training on ImageNet), or the path to the weights file to be\nloaded.  Note: one model can have multiple imagenet variants\ndepending on input shape it was trained with. For input_shape\n224x224 pass imagenet-i224 as argument. By default, highest input\nshape weights are downloaded.", "input_tensor": "optional Keras tensor (i.e. output of layers.Input()) to\nuse as image input for the model.", "classes": "optional number of classes to classify images into, only to be\nspecified if include_top is True, and if no weights argument is\nspecified.", "classifier_activation": "A str or callable. The activation function to\nuse on the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.", "include_preprocessing": "Boolean, whether to include the preprocessing layer\n(Rescaling) at the bottom of the network. Defaults to True.\nNote: Input image is normalized by ImageNet mean and standard deviation."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.resnet_rs.ResNetRS420": {"description": "Instantiates the ResNetRS420 architecture.", "Args": {"depth": "Depth of ResNet network.", "input_shape": "optional shape tuple. It should have exactly 3 inputs\nchannels, and width and height should be no smaller than 32.\nE.g. (200, 200, 3) would be one valid value.", "bn_momentum": "Momentum parameter for Batch Normalization layers.", "bn_epsilon": "Epsilon parameter for Batch Normalization layers.", "activation": "activation function.", "se_ratio": "Squeeze and Excitation layer ratio.", "dropout_rate": "dropout rate before final classifier layer.", "drop_connect_rate": "dropout rate at skip connections.", "include_top": "whether to include the fully-connected layer at the top of\nthe network.", "block_args": "list of dicts, parameters to construct block modules.", "model_name": "name of the model.", "pooling": "optional pooling mode for feature extraction when include_top\nis False.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional layer.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "weights": "one of None (random initialization), 'imagenet'\n(pre-training on ImageNet), or the path to the weights file to be\nloaded.  Note: one model can have multiple imagenet variants\ndepending on input shape it was trained with. For input_shape\n224x224 pass imagenet-i224 as argument. By default, highest input\nshape weights are downloaded.", "input_tensor": "optional Keras tensor (i.e. output of layers.Input()) to\nuse as image input for the model.", "classes": "optional number of classes to classify images into, only to be\nspecified if include_top is True, and if no weights argument is\nspecified.", "classifier_activation": "A str or callable. The activation function to\nuse on the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.", "include_preprocessing": "Boolean, whether to include the preprocessing layer\n(Rescaling) at the bottom of the network. Defaults to True.\nNote: Input image is normalized by ImageNet mean and standard deviation."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.resnet_rs.ResNetRS50": {"description": "Instantiates the ResNetRS50 architecture.", "Args": {"depth": "Depth of ResNet network.", "input_shape": "optional shape tuple. It should have exactly 3 inputs\nchannels, and width and height should be no smaller than 32.\nE.g. (200, 200, 3) would be one valid value.", "bn_momentum": "Momentum parameter for Batch Normalization layers.", "bn_epsilon": "Epsilon parameter for Batch Normalization layers.", "activation": "activation function.", "se_ratio": "Squeeze and Excitation layer ratio.", "dropout_rate": "dropout rate before final classifier layer.", "drop_connect_rate": "dropout rate at skip connections.", "include_top": "whether to include the fully-connected layer at the top of\nthe network.", "block_args": "list of dicts, parameters to construct block modules.", "model_name": "name of the model.", "pooling": "optional pooling mode for feature extraction when include_top\nis False.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional layer.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "weights": "one of None (random initialization), 'imagenet'\n(pre-training on ImageNet), or the path to the weights file to be\nloaded.  Note: one model can have multiple imagenet variants\ndepending on input shape it was trained with. For input_shape\n224x224 pass imagenet-i224 as argument. By default, highest input\nshape weights are downloaded.", "input_tensor": "optional Keras tensor (i.e. output of layers.Input()) to\nuse as image input for the model.", "classes": "optional number of classes to classify images into, only to be\nspecified if include_top is True, and if no weights argument is\nspecified.", "classifier_activation": "A str or callable. The activation function to\nuse on the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.", "include_preprocessing": "Boolean, whether to include the preprocessing layer\n(Rescaling) at the bottom of the network. Defaults to True.\nNote: Input image is normalized by ImageNet mean and standard deviation."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.resnet_rs.decode_predictions": {"description": "Decodes the prediction of an ImageNet model.", "Args": {"preds": "Numpy array encoding a batch of predictions.", "top": "Integer, how many top-guesses to return. Defaults to 5."}, "Returns": "A list of lists of top class prediction tuples\n(class_name, class_description, score).\nOne list of tuples per sample in batch input.", "Raises": {"ValueError": "In case of invalid shape of the pred array\n(must be 2D)."}}, "tf.keras.applications.resnet_rs.preprocess_input": {"description": "A placeholder method for backward compatibility.", "Args": {"x": "A floating point numpy.array or a tf.Tensor.", "data_format": "Optional data format of the image tensor/array. Defaults to\nNone, in which case the global setting\ntf.keras.backend.image_data_format() is used (unless you changed it,\nit defaults to \"channels_last\").{mode}"}, "Returns": "Unchanged numpy.array or tf.Tensor."}}, "tf.keras.applications.resnet_v2": {"tf.keras.applications.resnet_v2.ResNet101V2": {"description": "Instantiates the ResNet101V2 architecture.", "Args": {"include_top": "whether to include the fully-connected\nlayer at the top of the network.", "weights": "one of None (random initialization),\n'imagenet' (pre-training on ImageNet),\nor the path to the weights file to be loaded.", "input_tensor": "optional Keras tensor (i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "optional shape tuple, only to be specified\nif include_top is False (otherwise the input shape\nhas to be (224, 224, 3) (with 'channels_last' data format)\nor (3, 224, 224) (with 'channels_first' data format).\nIt should have exactly 3 inputs channels,\nand width and height should be no smaller than 32.\nE.g. (200, 200, 3) would be one valid value.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional block.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional block, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified.", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.resnet_v2.ResNet152V2": {"description": "Instantiates the ResNet152V2 architecture.", "Args": {"include_top": "whether to include the fully-connected\nlayer at the top of the network.", "weights": "one of None (random initialization),\n'imagenet' (pre-training on ImageNet),\nor the path to the weights file to be loaded.", "input_tensor": "optional Keras tensor (i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "optional shape tuple, only to be specified\nif include_top is False (otherwise the input shape\nhas to be (224, 224, 3) (with 'channels_last' data format)\nor (3, 224, 224) (with 'channels_first' data format).\nIt should have exactly 3 inputs channels,\nand width and height should be no smaller than 32.\nE.g. (200, 200, 3) would be one valid value.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional block.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional block, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified.", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.resnet_v2.ResNet50V2": {"description": "Instantiates the ResNet50V2 architecture.", "Args": {"include_top": "whether to include the fully-connected\nlayer at the top of the network.", "weights": "one of None (random initialization),\n'imagenet' (pre-training on ImageNet),\nor the path to the weights file to be loaded.", "input_tensor": "optional Keras tensor (i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "optional shape tuple, only to be specified\nif include_top is False (otherwise the input shape\nhas to be (224, 224, 3) (with 'channels_last' data format)\nor (3, 224, 224) (with 'channels_first' data format).\nIt should have exactly 3 inputs channels,\nand width and height should be no smaller than 32.\nE.g. (200, 200, 3) would be one valid value.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional block.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional block, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified.", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.resnet_v2.decode_predictions": {"description": "Decodes the prediction of an ImageNet model.", "Args": {"preds": "Numpy array encoding a batch of predictions.", "top": "Integer, how many top-guesses to return. Defaults to 5."}, "Returns": "A list of lists of top class prediction tuples\n(class_name, class_description, score).\nOne list of tuples per sample in batch input.", "Raises": {"ValueError": "In case of invalid shape of the pred array\n(must be 2D)."}}, "tf.keras.applications.resnet_v2.preprocess_input": {"description": "Preprocesses a tensor or Numpy array encoding a batch of images.", "Args": {"x": "A floating point numpy.array or a tf.Tensor, 3D or 4D with 3 color\nchannels, with values in the range [0, 255].\nThe preprocessed data are written over the input data\nif the data types are compatible. To avoid this\nbehaviour, numpy.copy(x) can be used.", "data_format": "Optional data format of the image tensor/array. Defaults to\nNone, in which case the global setting\ntf.keras.backend.image_data_format() is used (unless you changed it,\nit defaults to \"channels_last\")."}, "Returns": "Preprocessed numpy.array or a tf.Tensor with type float32.\nThe inputs pixel values are scaled between -1 and 1, sample-wise.", "Raises": {"ValueError": "In case of unknown data_format argument."}}}, "tf.keras.applications.vgg16": {"tf.keras.applications.vgg16.VGG16": {"description": "Instantiates the VGG16 model.", "Args": {"include_top": "whether to include the 3 fully-connected\nlayers at the top of the network.", "weights": "one of None (random initialization),\n'imagenet' (pre-training on ImageNet),\nor the path to the weights file to be loaded.", "input_tensor": "optional Keras tensor\n(i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "optional shape tuple, only to be specified\nif include_top is False (otherwise the input shape\nhas to be (224, 224, 3)\n(with channels_last data format)\nor (3, 224, 224) (with channels_first data format).\nIt should have exactly 3 input channels,\nand width and height should be no smaller than 32.\nE.g. (200, 200, 3) would be one valid value.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional block.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional block, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified.", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.vgg16.decode_predictions": {"description": "Decodes the prediction of an ImageNet model.", "Args": {"preds": "Numpy array encoding a batch of predictions.", "top": "Integer, how many top-guesses to return. Defaults to 5."}, "Returns": "A list of lists of top class prediction tuples\n(class_name, class_description, score).\nOne list of tuples per sample in batch input.", "Raises": {"ValueError": "In case of invalid shape of the pred array\n(must be 2D)."}}, "tf.keras.applications.vgg16.preprocess_input": {"description": "Preprocesses a tensor or Numpy array encoding a batch of images.", "Args": {"x": "A floating point numpy.array or a tf.Tensor, 3D or 4D with 3 color\nchannels, with values in the range [0, 255].\nThe preprocessed data are written over the input data\nif the data types are compatible. To avoid this\nbehaviour, numpy.copy(x) can be used.", "data_format": "Optional data format of the image tensor/array. Defaults to\nNone, in which case the global setting\ntf.keras.backend.image_data_format() is used (unless you changed it,\nit defaults to \"channels_last\")."}, "Returns": "Preprocessed numpy.array or a tf.Tensor with type float32.\nThe images are converted from RGB to BGR, then each color channel is\nzero-centered with respect to the ImageNet dataset, without scaling.", "Raises": {"ValueError": "In case of unknown data_format argument."}}}, "tf.keras.applications.vgg19": {"tf.keras.applications.vgg19.VGG19": {"description": "Instantiates the VGG19 architecture.", "Args": {"include_top": "whether to include the 3 fully-connected\nlayers at the top of the network.", "weights": "one of None (random initialization),\n'imagenet' (pre-training on ImageNet),\nor the path to the weights file to be loaded.", "input_tensor": "optional Keras tensor\n(i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "optional shape tuple, only to be specified\nif include_top is False (otherwise the input shape\nhas to be (224, 224, 3)\n(with channels_last data format)\nor (3, 224, 224) (with channels_first data format).\nIt should have exactly 3 inputs channels,\nand width and height should be no smaller than 32.\nE.g. (200, 200, 3) would be one valid value.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional block.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional block, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified.", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.vgg19.decode_predictions": {"description": "Decodes the prediction of an ImageNet model.", "Args": {"preds": "Numpy array encoding a batch of predictions.", "top": "Integer, how many top-guesses to return. Defaults to 5."}, "Returns": "A list of lists of top class prediction tuples\n(class_name, class_description, score).\nOne list of tuples per sample in batch input.", "Raises": {"ValueError": "In case of invalid shape of the pred array\n(must be 2D)."}}, "tf.keras.applications.vgg19.preprocess_input": {"description": "Preprocesses a tensor or Numpy array encoding a batch of images.", "Args": {"x": "A floating point numpy.array or a tf.Tensor, 3D or 4D with 3 color\nchannels, with values in the range [0, 255].\nThe preprocessed data are written over the input data\nif the data types are compatible. To avoid this\nbehaviour, numpy.copy(x) can be used.", "data_format": "Optional data format of the image tensor/array. Defaults to\nNone, in which case the global setting\ntf.keras.backend.image_data_format() is used (unless you changed it,\nit defaults to \"channels_last\")."}, "Returns": "Preprocessed numpy.array or a tf.Tensor with type float32.\nThe images are converted from RGB to BGR, then each color channel is\nzero-centered with respect to the ImageNet dataset, without scaling.", "Raises": {"ValueError": "In case of unknown data_format argument."}}}, "tf.keras.applications.xception": {"tf.keras.applications.xception.Xception": {"description": "Instantiates the Xception architecture.", "Args": {"include_top": "whether to include the fully-connected\nlayer at the top of the network.", "weights": "one of None (random initialization),\n'imagenet' (pre-training on ImageNet),\nor the path to the weights file to be loaded.", "input_tensor": "optional Keras tensor\n(i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "optional shape tuple, only to be specified\nif include_top is False (otherwise the input shape\nhas to be (299, 299, 3).\nIt should have exactly 3 inputs channels,\nand width and height should be no smaller than 71.\nE.g. (150, 150, 3) would be one valid value.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False.\n\nNone means that the output of the model will be\nthe 4D tensor output of the\nlast convolutional block.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional block, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "optional number of classes to classify images\ninto, only to be specified if include_top is True,\nand if no weights argument is specified.", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.xception.decode_predictions": {"description": "Decodes the prediction of an ImageNet model.", "Args": {"preds": "Numpy array encoding a batch of predictions.", "top": "Integer, how many top-guesses to return. Defaults to 5."}, "Returns": "A list of lists of top class prediction tuples\n(class_name, class_description, score).\nOne list of tuples per sample in batch input.", "Raises": {"ValueError": "In case of invalid shape of the pred array\n(must be 2D)."}}, "tf.keras.applications.xception.preprocess_input": {"description": "Preprocesses a tensor or Numpy array encoding a batch of images.", "Args": {"x": "A floating point numpy.array or a tf.Tensor, 3D or 4D with 3 color\nchannels, with values in the range [0, 255].\nThe preprocessed data are written over the input data\nif the data types are compatible. To avoid this\nbehaviour, numpy.copy(x) can be used.", "data_format": "Optional data format of the image tensor/array. Defaults to\nNone, in which case the global setting\ntf.keras.backend.image_data_format() is used (unless you changed it,\nit defaults to \"channels_last\")."}, "Returns": "Preprocessed numpy.array or a tf.Tensor with type float32.\nThe inputs pixel values are scaled between -1 and 1, sample-wise.", "Raises": {"ValueError": "In case of unknown data_format argument."}}}, "tf.keras.backend": {"tf.keras.backend.clear_session": {"description": "Resets all state generated by Keras."}, "tf.keras.backend.epsilon": {"description": "Returns the value of the fuzz factor used in numeric expressions.", "Returns": "A float."}, "tf.keras.backend.floatx": {"description": "Returns the default float type, as a string.", "Returns": "String, the current default float type."}, "tf.keras.backend.get_uid": {"description": "Associates a string prefix with an integer counter in a TensorFlow graph.", "Args": {"prefix": "String prefix to index."}, "Returns": "Unique integer ID."}, "tf.keras.backend.image_data_format": {"description": "Returns the default image data format convention.", "Returns": "A string, either 'channels_first' or 'channels_last'"}, "tf.keras.backend.is_keras_tensor": {"description": "Returns whether x is a Keras tensor.", "Args": {"x": "A candidate tensor."}, "Returns": "A boolean: Whether the argument is a Keras tensor.", "Raises": {"ValueError": "In case x is not a symbolic tensor."}}, "tf.keras.backend.reset_uids": {"description": "Resets graph identifiers."}, "tf.keras.backend.rnn": {"description": "Iterates over the time dimension of a tensor.", "Args": {"step_function": "RNN step function.\nArgs;\n    input; Tensor with shape (samples, ...) (no time dimension),\n        representing input for the batch of samples at a certain\n        time step.\n    states; List of tensors.\nReturns;\n    output; Tensor with shape (samples, output_dim)\n        (no time dimension).\n    new_states; List of tensors, same length and shapes\n        as 'states'. The first state in the list must be the\n        output tensor at the previous timestep.", "inputs": "Tensor of temporal data of shape (samples, time, ...)\n(at least 3D), or nested tensors, and each of which has shape\n(samples, time, ...).", "initial_states": "Tensor with shape (samples, state_size)\n(no time dimension), containing the initial values for the states used\nin the step function. In the case that state_size is in a nested\nshape, the shape of initial_states will also follow the nested\nstructure.", "go_backwards": "Boolean. If True, do the iteration over the time\ndimension in reverse order and return the reversed sequence.", "mask": "Binary tensor with shape (samples, time, 1),\nwith a zero for every element that is masked.", "constants": "List of constant values passed at each step.", "unroll": "Whether to unroll the RNN or to use a symbolic while_loop.", "input_length": "An integer or a 1-D Tensor, depending on whether\nthe time dimension is fixed-length or not. In case of variable length\ninput, it is used for masking in case there's no mask specified.", "time_major": "Boolean. If true, the inputs and outputs will be in shape\n(timesteps, batch, ...), whereas in the False case, it will be\n(batch, timesteps, ...). Using time_major = True is a bit more\nefficient because it avoids transposes at the beginning and end of the\nRNN calculation. However, most TensorFlow data is batch-major, so by\ndefault this function accepts input and emits output in batch-major\nform.", "zero_output_for_mask": "Boolean. If True, the output for masked timestep\nwill be zeros, whereas in the False case, output from previous\ntimestep is returned.", "return_all_outputs": "Boolean. If True, return the recurrent outputs for all\ntimesteps in the sequence. If False, only return the output for the\nlast timestep (which consumes less memory)."}, "Returns": "A tuple, (last_output, outputs, new_states).\nlast_output: the latest output of the rnn, of shape (samples, ...)\noutputs:\n- If `return_all_outputs=True`: a tensor with shape\u00a0 `(samples, time, ...)` where each entry `outputs[s, t]` is the\u00a0 output of the step function at time `t` for sample `s`- Else, a tensor equal to `last_output` with shape\u00a0 `(samples, 1, ...)`\nnew_states: list of tensors, latest states returned by\n    the step function, of shape (samples, ...).", "Raises": {"ValueError": "if mask is provided (not None) but states is not provided\n(len(states) == 0)."}}, "tf.keras.backend.set_epsilon": {"description": "Sets the value of the fuzz factor used in numeric expressions.", "Args": {"value": "float. New value of epsilon."}}, "tf.keras.backend.set_floatx": {"description": "Sets the default float type.", "Args": {"value": "String; 'float16', 'float32', or 'float64'."}, "Raises": {"ValueError": "In case of invalid value."}}, "tf.keras.backend.set_image_data_format": {"description": "Sets the value of the image data format convention.", "Args": {"data_format": "string. 'channels_first' or 'channels_last'."}, "Raises": {"ValueError": "In case of invalid data_format value."}}}, "tf.keras.callbacks": {"tf.keras.callbacks.BackupAndRestore": {"description": "Callback to back up and restore the training state.", "Args": {"backup_dir": "String, path to store the checkpoint.\ne.g. backup_dir = os.path.join(working_dir, 'backup')\nThis is the directory in which the system stores temporary files to\nrecover the model from jobs terminated unexpectedly. The directory\ncannot be reused elsewhere to store other files, e.g. by\nBackupAndRestore callback of another training, or by another callback\n(ModelCheckpoint) of the same training."}}, "tf.keras.callbacks.BaseLogger": {"description": "Callback that accumulates epoch averages of metrics.", "Args": {"stateful_metrics": "Iterable of string names of metrics that\nshould not be averaged over an epoch.\nMetrics in this list will be logged as-is in on_epoch_end.\nAll others will be averaged in on_epoch_end."}}, "tf.keras.callbacks.CSVLogger": {"description": "Callback that streams epoch results to a CSV file.", "Args": {"filename": "Filename of the CSV file, e.g. 'run/log.csv'.", "separator": "String used to separate elements in the CSV file.", "append": "Boolean. True: append if file exists (useful for continuing\ntraining). False: overwrite existing file."}}, "tf.keras.callbacks.Callback": {"description": "Abstract base class used to build new callbacks.", "Attributes": {"params": "Dict. Training parameters\n(eg. verbosity, batch size, number of epochs...).", "model": "Instance of keras.models.Model.\nReference of the model being trained."}}, "tf.keras.callbacks.CallbackList": {"description": "Container abstracting a list of callbacks.", "Args": {"callbacks": "List of Callback instances.", "add_history": "Whether a History callback should be added, if one does not\nalready exist in the callbacks list.", "add_progbar": "Whether a ProgbarLogger callback should be added, if one\ndoes not already exist in the callbacks list.", "model": "The Model these callbacks are used with.", "**params": "If provided, parameters will be passed to each Callback via\nCallback.set_params."}}, "tf.keras.callbacks.EarlyStopping": {"description": "Stop training when a monitored metric has stopped improving.", "Args": {"monitor": "Quantity to be monitored.", "min_delta": "Minimum change in the monitored quantity\nto qualify as an improvement, i.e. an absolute\nchange of less than min_delta, will count as no\nimprovement.", "patience": "Number of epochs with no improvement\nafter which training will be stopped.", "verbose": "Verbosity mode, 0 or 1. Mode 0 is silent, and mode 1\ndisplays messages when the callback takes an action.", "mode": "One of {\"auto\", \"min\", \"max\"}. In min mode,\ntraining will stop when the quantity\nmonitored has stopped decreasing; in \"max\"\nmode it will stop when the quantity\nmonitored has stopped increasing; in \"auto\"\nmode, the direction is automatically inferred\nfrom the name of the monitored quantity.", "baseline": "Baseline value for the monitored quantity.\nTraining will stop if the model doesn't show improvement over the\nbaseline.", "restore_best_weights": "Whether to restore model weights from\nthe epoch with the best value of the monitored quantity.\nIf False, the model weights obtained at the last step of\ntraining are used. An epoch will be restored regardless\nof the performance relative to the baseline. If no epoch\nimproves on baseline, training will run for patience\nepochs and restore weights from the best epoch in that set."}}, "tf.keras.callbacks.History": {"description": "Callback that records events into a History object."}, "tf.keras.callbacks.LambdaCallback": {"description": "Callback for creating simple, custom callbacks on-the-fly.", "Args": {"on_epoch_begin": "called at the beginning of every epoch.", "on_epoch_end": "called at the end of every epoch.", "on_batch_begin": "called at the beginning of every batch.", "on_batch_end": "called at the end of every batch.", "on_train_begin": "called at the beginning of model training.", "on_train_end": "called at the end of model training."}}, "tf.keras.callbacks.LearningRateScheduler": {"description": "Learning rate scheduler.", "Args": {"schedule": "a function that takes an epoch index (integer, indexed from 0)\nand current learning rate (float) as inputs and returns a new\nlearning rate as output (float).", "verbose": "int. 0: quiet, 1: update messages."}}, "tf.keras.callbacks.ModelCheckpoint": {"description": "Callback to save the Keras model or model weights at some frequency.", "Args": {"filepath": "string or PathLike, path to save the model file. e.g.\nfilepath = os.path.join(working_dir, 'ckpt', file_name). filepath\ncan contain named formatting options, which will be filled the value of\nepoch and keys in logs (passed in on_epoch_end). For example: if\nfilepath is weights.{epoch:02d}-{val_loss:.2f}.hdf5, then the model\ncheckpoints will be saved with the epoch number and the validation loss\nin the filename. The directory of the filepath should not be reused by\nany other callbacks to avoid conflicts.", "monitor": "The metric name to monitor. Typically the metrics are set by the\nModel.compile method. Note:\n\nPrefix the name with \"val_\" to monitor validation metrics.\nUse \"loss\" or \"val_loss\" to monitor the model's total loss.\nIf you specify metrics as strings, like \"accuracy\", pass the same\nstring (with or without the \"val_\" prefix).\nIf you pass metrics.Metric objects, monitor should be set to\nmetric.name\nIf you're not sure about the metric names you can check the contents\nof the history.history dictionary returned by\nhistory = model.fit()\nMulti-output models set additional prefixes on the metric names.", "verbose": "Verbosity mode, 0 or 1. Mode 0 is silent, and mode 1\ndisplays messages when the callback takes an action.", "save_best_only": "if save_best_only=True, it only saves when the model\nis considered the \"best\" and the latest best model according to the\nquantity monitored will not be overwritten. If filepath doesn't\ncontain formatting options like {epoch} then filepath will be\noverwritten by each new better model.", "mode": "one of {'auto', 'min', 'max'}. If save_best_only=True, the\ndecision to overwrite the current save file is made based on either\nthe maximization or the minimization of the monitored quantity.\nFor val_acc, this should be max, for val_loss this should be\nmin, etc. In auto mode, the mode is set to max if the quantities\nmonitored are 'acc' or start with 'fmeasure' and are set to min for\nthe rest of the quantities.", "save_weights_only": "if True, then only the model's weights will be saved\n(model.save_weights(filepath)), else the full model is saved\n(model.save(filepath)).", "save_freq": "'epoch' or integer. When using 'epoch', the callback saves\nthe model after each epoch. When using integer, the callback saves the\nmodel at end of this many batches. If the Model is compiled with\nsteps_per_execution=N, then the saving criteria will be\nchecked every Nth batch. Note that if the saving isn't aligned to\nepochs, the monitored metric may potentially be less reliable (it\ncould reflect as little as 1 batch, since the metrics get reset every\nepoch). Defaults to 'epoch'.", "options": "Optional tf.train.CheckpointOptions object if\nsave_weights_only is true or optional tf.saved_model.SaveOptions\nobject if save_weights_only is false.", "initial_value_threshold": "Floating point initial \"best\" value of the metric\nto be monitored. Only applies if save_best_value=True. Only overwrites\nthe model weights already saved if the performance of current\nmodel is better than this value.", "**kwargs": "Additional arguments for backwards compatibility. Possible key\nis period."}}, "tf.keras.callbacks.ProgbarLogger": {"description": "Callback that prints metrics to stdout.", "Args": {"count_mode": "One of \"steps\" or \"samples\".\nWhether the progress bar should\ncount samples seen or steps (batches) seen.", "stateful_metrics": "Iterable of string names of metrics that\nshould not be averaged over an epoch.\nMetrics in this list will be logged as-is.\nAll others will be averaged over time (e.g. loss, etc).\nIf not provided, defaults to the Model's metrics."}, "Raises": {"ValueError": "In case of invalid count_mode."}}, "tf.keras.callbacks.ReduceLROnPlateau": {"description": "Reduce learning rate when a metric has stopped improving.", "Args": {"monitor": "quantity to be monitored.", "factor": "factor by which the learning rate will be reduced.\nnew_lr = lr * factor.", "patience": "number of epochs with no improvement after which learning rate\nwill be reduced.", "verbose": "int. 0: quiet, 1: update messages.", "mode": "one of {'auto', 'min', 'max'}. In 'min' mode,\nthe learning rate will be reduced when the\nquantity monitored has stopped decreasing; in 'max' mode it will be\nreduced when the quantity monitored has stopped increasing; in 'auto'\nmode, the direction is automatically inferred from the name of the\nmonitored quantity.", "min_delta": "threshold for measuring the new optimum, to only focus on\nsignificant changes.", "cooldown": "number of epochs to wait before resuming normal operation after\nlr has been reduced.", "min_lr": "lower bound on the learning rate."}}, "tf.keras.callbacks.RemoteMonitor": {"description": "Callback used to stream events to a server.", "Args": {"root": "String; root url of the target server.", "path": "String; path relative to root to which the events will be sent.", "field": "String; JSON field under which the data will be stored.\nThe field is used only if the payload is sent within a form\n(i.e. send_as_json is set to False).", "headers": "Dictionary; optional custom HTTP headers.", "send_as_json": "Boolean; whether the request should be\nsent as \"application/json\"."}}, "tf.keras.callbacks.TensorBoard": {"description": "Enable visualizations for TensorBoard.", "Args": {"log_dir": "the path of the directory where to save the log files to be\nparsed by TensorBoard. e.g. log_dir = os.path.join(working_dir, 'logs')\nThis directory should not be reused by any other callbacks.", "histogram_freq": "frequency (in epochs) at which to compute\nweight histograms for the layers of the model. If set to 0, histograms\nwon't be computed. Validation data (or split) must be specified for\nhistogram visualizations.", "write_graph": "whether to visualize the graph in TensorBoard. The log file\ncan become quite large when write_graph is set to True.", "write_images": "whether to write model weights to visualize as image in\nTensorBoard.", "write_steps_per_second": "whether to log the training steps per second into\nTensorboard. This supports both epoch and batch frequency logging.", "update_freq": "'batch' or 'epoch' or integer. When using 'batch',\nwrites the losses and metrics to TensorBoard after each batch. The same\napplies for 'epoch'. If using an integer, let's say 1000, the\ncallback will write the metrics and losses to TensorBoard every 1000\nbatches. Note that writing too frequently to TensorBoard can slow down\nyour training.", "profile_batch": "Profile the batch(es) to sample compute characteristics.\nprofile_batch must be a non-negative integer or a tuple of integers.\nA pair of positive integers signify a range of batches to profile.\nBy default, profiling is disabled.", "embeddings_freq": "frequency (in epochs) at which embedding layers will be\nvisualized. If set to 0, embeddings won't be visualized.", "embeddings_metadata": "Dictionary which maps embedding layer names to the\nfilename of a file in which to save metadata for the embedding layer.\nIn case the same metadata file is to be\nused for all embedding layers, a single filename can be passed."}}, "tf.keras.callbacks.TerminateOnNaN": {"description": "Callback that terminates training when a NaN loss is encountered."}}, "tf.keras.constraints": {"tf.keras.constraints.Constraint": {"description": "Base class for weight constraints."}, "tf.keras.constraints.MaxNorm": {"description": "MaxNorm weight constraint.", "Args": {"max_value": "the maximum norm value for the incoming weights.", "axis": "integer, axis along which to calculate weight norms.\nFor instance, in a Dense layer the weight matrix\nhas shape (input_dim, output_dim),\nset axis to 0 to constrain each weight vector\nof length (input_dim,).\nIn a Conv2D layer with data_format=\"channels_last\",\nthe weight tensor has shape\n(rows, cols, input_depth, output_depth),\nset axis to [0, 1, 2]\nto constrain the weights of each filter tensor of size\n(rows, cols, input_depth)."}}, "tf.keras.constraints.MinMaxNorm": {"description": "MinMaxNorm weight constraint.", "Args": {"min_value": "the minimum norm for the incoming weights.", "max_value": "the maximum norm for the incoming weights.", "rate": "rate for enforcing the constraint: weights will be\nrescaled to yield\n(1 - rate) * norm + rate * norm.clip(min_value, max_value).\nEffectively, this means that rate=1.0 stands for strict\nenforcement of the constraint, while rate<1.0 means that\nweights will be rescaled at each step to slowly move\ntowards a value inside the desired interval.", "axis": "integer, axis along which to calculate weight norms.\nFor instance, in a Dense layer the weight matrix\nhas shape (input_dim, output_dim),\nset axis to 0 to constrain each weight vector\nof length (input_dim,).\nIn a Conv2D layer with data_format=\"channels_last\",\nthe weight tensor has shape\n(rows, cols, input_depth, output_depth),\nset axis to [0, 1, 2]\nto constrain the weights of each filter tensor of size\n(rows, cols, input_depth)."}}, "tf.keras.constraints.NonNeg": {"description": "Constrains the weights to be non-negative."}, "tf.keras.constraints.RadialConstraint": {"description": "Constrains Conv2D kernel weights to be the same for each radius."}, "tf.keras.constraints.UnitNorm": {"description": "Constrains the weights incident to each hidden unit to have unit norm.", "Args": {"axis": "integer, axis along which to calculate weight norms.\nFor instance, in a Dense layer the weight matrix\nhas shape (input_dim, output_dim),\nset axis to 0 to constrain each weight vector\nof length (input_dim,).\nIn a Conv2D layer with data_format=\"channels_last\",\nthe weight tensor has shape\n(rows, cols, input_depth, output_depth),\nset axis to [0, 1, 2]\nto constrain the weights of each filter tensor of size\n(rows, cols, input_depth)."}}, "tf.keras.constraints.deserialize": {}, "tf.keras.constraints.get": {"description": "Retrieves a Keras constraint function."}, "tf.keras.constraints.serialize": {}}, "tf.keras.datasets": {}, "tf.keras.datasets.boston_housing": {"tf.keras.datasets.boston_housing.load_data": {"description": "Loads the Boston Housing dataset.", "Args": {"path": "path where to cache the dataset locally\n(relative to ~/.keras/datasets).", "test_split": "fraction of the data to reserve as test set.", "seed": "Random seed for shuffling the data\nbefore computing the test split."}, "Returns": "Tuple of Numpy arrays: (x_train, y_train), (x_test, y_test)."}}, "tf.keras.datasets.cifar10": {"tf.keras.datasets.cifar10.load_data": {"description": "Loads the CIFAR10 dataset.", "Returns": "Tuple of NumPy arrays: (x_train, y_train), (x_test, y_test)."}}, "tf.keras.datasets.cifar100": {"tf.keras.datasets.cifar100.load_data": {"description": "Loads the CIFAR100 dataset.", "Args": {"label_mode": "one of \"fine\", \"coarse\". If it is \"fine\" the category labels\nare the fine-grained labels, if it is \"coarse\" the output labels are the\ncoarse-grained superclasses."}, "Returns": "Tuple of NumPy arrays: (x_train, y_train), (x_test, y_test)."}}, "tf.keras.datasets.fashion_mnist": {"tf.keras.datasets.fashion_mnist.load_data": {"description": "Loads the Fashion-MNIST dataset.", "Returns": "Tuple of NumPy arrays: (x_train, y_train), (x_test, y_test)."}}, "tf.keras.datasets.imdb": {"tf.keras.datasets.imdb.get_word_index": {"description": "Retrieves a dict mapping words to their index in the IMDB dataset.", "Args": {"path": "where to cache the data (relative to ~/.keras/dataset)."}, "Returns": "The word index dictionary. Keys are word strings, values are their index."}, "tf.keras.datasets.imdb.load_data": {"description": "Loads the [IMDB dataset](https://ai.stanford.edu/~amaas/data/sentiment/).", "Args": {"path": "where to cache the data (relative to ~/.keras/dataset).", "num_words": "integer or None. Words are\nranked by how often they occur (in the training set) and only\nthe num_words most frequent words are kept. Any less frequent word\nwill appear as oov_char value in the sequence data. If None,\nall words are kept. Defaults to None, so all words are kept.", "skip_top": "skip the top N most frequently occurring words\n(which may not be informative). These words will appear as\noov_char value in the dataset. Defaults to 0, so no words are\nskipped.", "maxlen": "int or None. Maximum sequence length.\nAny longer sequence will be truncated. Defaults to None, which\nmeans no truncation.", "seed": "int. Seed for reproducible data shuffling.", "start_char": "int. The start of a sequence will be marked with this\ncharacter. Defaults to 1 because 0 is usually the padding character.", "oov_char": "int. The out-of-vocabulary character.\nWords that were cut out because of the num_words or\nskip_top limits will be replaced with this character.", "index_from": "int. Index actual words with this index and higher.", "**kwargs": "Used for backwards compatibility."}, "Returns": "Tuple of Numpy arrays: (x_train, y_train), (x_test, y_test).", "Raises": {"ValueError": "in case maxlen is so low\nthat no input sequence could be kept."}}}, "tf.keras.datasets.mnist": {"tf.keras.datasets.mnist.load_data": {"description": "Loads the MNIST dataset.", "Args": {"path": "path where to cache the dataset locally\n(relative to ~/.keras/datasets)."}, "Returns": "Tuple of NumPy arrays: (x_train, y_train), (x_test, y_test)."}}, "tf.keras.datasets.reuters": {"tf.keras.datasets.reuters.get_word_index": {"description": "Retrieves a dict mapping words to their index in the Reuters dataset.", "Args": {"path": "where to cache the data (relative to ~/.keras/dataset)."}, "Returns": "The word index dictionary. Keys are word strings, values are their index."}, "tf.keras.datasets.reuters.load_data": {"description": "Loads the Reuters newswire classification dataset.", "Args": {"path": "where to cache the data (relative to ~/.keras/dataset).", "num_words": "integer or None. Words are\nranked by how often they occur (in the training set) and only\nthe num_words most frequent words are kept. Any less frequent word\nwill appear as oov_char value in the sequence data. If None,\nall words are kept. Defaults to None, so all words are kept.", "skip_top": "skip the top N most frequently occurring words\n(which may not be informative). These words will appear as\noov_char value in the dataset. Defaults to 0, so no words are\nskipped.", "maxlen": "int or None. Maximum sequence length.\nAny longer sequence will be truncated. Defaults to None, which\nmeans no truncation.", "test_split": "Float between 0 and 1. Fraction of the dataset to be used\nas test data. Defaults to 0.2, meaning 20% of the dataset is used as\ntest data.", "seed": "int. Seed for reproducible data shuffling.", "start_char": "int. The start of a sequence will be marked with this\ncharacter. Defaults to 1 because 0 is usually the padding character.", "oov_char": "int. The out-of-vocabulary character.\nWords that were cut out because of the num_words or\nskip_top limits will be replaced with this character.", "index_from": "int. Index actual words with this index and higher.", "**kwargs": "Used for backwards compatibility."}, "Returns": "Tuple of Numpy arrays: (x_train, y_train), (x_test, y_test)."}}, "tf.keras.dtensor": {}, "tf.keras.dtensor.experimental.optimizers": {"tf.keras.dtensor.experimental.optimizers.Adadelta": {"description": "DTensor specific optimizers.", "Args": {"learning_rate": "Initial value for the learning rate:\neither a floating point value,\nor a tf.keras.optimizers.schedules.LearningRateSchedule instance.\nDefaults to 0.001.\nNote that Adadelta tends to benefit from higher initial learning rate\nvalues compared to other optimizers.\nTo match the exact form in the original paper, use 1.0.", "rho": "A Tensor or a floating point value. The decay rate. Defaults to 0.95.", "epsilon": "Small floating point value used to maintain numerical stability.\nDefaults to 1e-7.", "name": "String. The name to use\nfor momentum accumulator weights created by\nthe optimizer.", "clipnorm": "Float. If set, the gradient of each weight is individually\nclipped so that its norm is no higher than this value.", "clipvalue": "Float. If set, the gradient of each weight is clipped to be no\nhigher than this value.", "global_clipnorm": "Float. If set, the gradient of all weights is clipped so\nthat their global norm is no higher than this value.", "use_ema": "Boolean, defaults to False. If True, exponential moving average\n(EMA) is applied. EMA consists of computing an exponential moving\naverage of the weights of the model (as the weight values change after\neach training batch), and periodically overwriting the weights with\ntheir moving average.", "ema_momentum": "Float, defaults to 0.99. Only used if use_ema=True. This is\nthe momentum to use when computing the EMA of the model's weights:\nnew_average = ema_momentum * old_average + (1 - ema_momentum) *\ncurrent_variable_value.", "ema_overwrite_frequency": "Int or None, defaults to None. Only used if\nuse_ema=True. Every ema_overwrite_frequency steps of iterations, we\noverwrite the model variable by its moving average. If None, the optimizer\n does not overwrite model variables in the middle of training, and you\nneed to explicitly overwrite the variables at the end of training\nby calling optimizer.finalize_variable_values() (which updates the model\nvariables in-place). When using the built-in fit() training loop, this\nhappens automatically after the last epoch, and you don't need to do\nanything.", "jit_compile": "Boolean, defaults to True. If True, the optimizer will use XLA\ncompilation. jit_compile cannot be True when training with\ntf.distribute.experimental.ParameterServerStrategy. Additionally,\nif no GPU device is found, this flag will be ignored.", "**kwargs": "keyword arguments only used for backward compatibility."}, "Attributes": {"iterations": "The number of training steps this optimizer has run.\nBy default, iterations would be incremented by one every time\napply_gradients() is called.", "learning_rate": ""}}, "tf.keras.dtensor.experimental.optimizers.Adagrad": {"description": "DTensor specific optimizers.", "Args": {"learning_rate": "Initial value for the learning rate:\neither a floating point value,\nor a tf.keras.optimizers.schedules.LearningRateSchedule instance.\nDefaults to 0.001.\nNote that Adagrad tends to benefit from higher initial learning rate\nvalues compared to other optimizers.\nTo match the exact form in the original paper, use 1.0.", "initial_accumulator_value": "Floating point value.\nStarting value for the accumulators (per-parameter momentum values).\nMust be non-negative.", "epsilon": "Small floating point value used to maintain numerical stability.", "name": "String. The name to use\nfor momentum accumulator weights created by\nthe optimizer.", "clipnorm": "Float. If set, the gradient of each weight is individually\nclipped so that its norm is no higher than this value.", "clipvalue": "Float. If set, the gradient of each weight is clipped to be no\nhigher than this value.", "global_clipnorm": "Float. If set, the gradient of all weights is clipped so\nthat their global norm is no higher than this value.", "use_ema": "Boolean, defaults to False. If True, exponential moving average\n(EMA) is applied. EMA consists of computing an exponential moving\naverage of the weights of the model (as the weight values change after\neach training batch), and periodically overwriting the weights with\ntheir moving average.", "ema_momentum": "Float, defaults to 0.99. Only used if use_ema=True. This is\nthe momentum to use when computing the EMA of the model's weights:\nnew_average = ema_momentum * old_average + (1 - ema_momentum) *\ncurrent_variable_value.", "ema_overwrite_frequency": "Int or None, defaults to None. Only used if\nuse_ema=True. Every ema_overwrite_frequency steps of iterations, we\noverwrite the model variable by its moving average. If None, the optimizer\n does not overwrite model variables in the middle of training, and you\nneed to explicitly overwrite the variables at the end of training\nby calling optimizer.finalize_variable_values() (which updates the model\nvariables in-place). When using the built-in fit() training loop, this\nhappens automatically after the last epoch, and you don't need to do\nanything.", "jit_compile": "Boolean, defaults to True. If True, the optimizer will use XLA\ncompilation. jit_compile cannot be True when training with\ntf.distribute.experimental.ParameterServerStrategy. Additionally,\nif no GPU device is found, this flag will be ignored.", "**kwargs": "keyword arguments only used for backward compatibility."}, "Attributes": {"iterations": "The number of training steps this optimizer has run.\nBy default, iterations would be incremented by one every time\napply_gradients() is called.", "learning_rate": ""}}, "tf.keras.dtensor.experimental.optimizers.Adam": {"description": "DTensor specific optimizers.", "Args": {"learning_rate": "A tf.Tensor, floating point value, a schedule that is a\ntf.keras.optimizers.schedules.LearningRateSchedule, or a callable\nthat takes no arguments and returns the actual value to use. The\nlearning rate. Defaults to 0.001.", "beta_1": "A float value or a constant float tensor, or a callable\nthat takes no arguments and returns the actual value to use. The\nexponential decay rate for the 1st moment estimates. Defaults to 0.9.", "beta_2": "A float value or a constant float tensor, or a callable\nthat takes no arguments and returns the actual value to use. The\nexponential decay rate for the 2nd moment estimates. Defaults to 0.999.", "epsilon": "A small constant for numerical stability. This epsilon is\n\"epsilon hat\" in the Kingma and Ba paper (in the formula just before\nSection 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to\n1e-7.", "amsgrad": "Boolean. Whether to apply AMSGrad variant of this algorithm from\nthe paper \"On the Convergence of Adam and beyond\". Defaults to False.", "name": "String. The name to use\nfor momentum accumulator weights created by\nthe optimizer.", "clipnorm": "Float. If set, the gradient of each weight is individually\nclipped so that its norm is no higher than this value.", "clipvalue": "Float. If set, the gradient of each weight is clipped to be no\nhigher than this value.", "global_clipnorm": "Float. If set, the gradient of all weights is clipped so\nthat their global norm is no higher than this value.", "use_ema": "Boolean, defaults to False. If True, exponential moving average\n(EMA) is applied. EMA consists of computing an exponential moving\naverage of the weights of the model (as the weight values change after\neach training batch), and periodically overwriting the weights with\ntheir moving average.", "ema_momentum": "Float, defaults to 0.99. Only used if use_ema=True. This is\nthe momentum to use when computing the EMA of the model's weights:\nnew_average = ema_momentum * old_average + (1 - ema_momentum) *\ncurrent_variable_value.", "ema_overwrite_frequency": "Int or None, defaults to None. Only used if\nuse_ema=True. Every ema_overwrite_frequency steps of iterations, we\noverwrite the model variable by its moving average. If None, the optimizer\n does not overwrite model variables in the middle of training, and you\nneed to explicitly overwrite the variables at the end of training\nby calling optimizer.finalize_variable_values() (which updates the model\nvariables in-place). When using the built-in fit() training loop, this\nhappens automatically after the last epoch, and you don't need to do\nanything.", "jit_compile": "Boolean, defaults to True. If True, the optimizer will use XLA\ncompilation. jit_compile cannot be True when training with\ntf.distribute.experimental.ParameterServerStrategy. Additionally,\nif no GPU device is found, this flag will be ignored.", "**kwargs": "keyword arguments only used for backward compatibility."}, "Attributes": {"iterations": "The number of training steps this optimizer has run.\nBy default, iterations would be incremented by one every time\napply_gradients() is called.", "learning_rate": ""}}, "tf.keras.dtensor.experimental.optimizers.RMSprop": {"description": "DTensor specific optimizers.", "Args": {"learning_rate": "Initial value for the learning rate:\neither a floating point value,\nor a tf.keras.optimizers.schedules.LearningRateSchedule instance.\nDefaults to 0.001.", "rho": "float, defaults to 0.9. Discounting factor for the old gradients.", "momentum": "float, defaults to 0.0. If not 0.0., the optimizer tracks the\nmomentum value, with a decay rate equals to 1 - momentum.", "epsilon": "A small constant for numerical stability. This epsilon is\n\"epsilon hat\" in the Kingma and Ba paper (in the formula just before\nSection 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to\n1e-7.", "centered": "Boolean. If True, gradients are normalized by the estimated\nvariance of the gradient; if False, by the uncentered second moment.\nSetting this to True may help with training, but is slightly more\nexpensive in terms of computation and memory. Defaults to False.", "name": "String. The name to use\nfor momentum accumulator weights created by\nthe optimizer.", "clipnorm": "Float. If set, the gradient of each weight is individually\nclipped so that its norm is no higher than this value.", "clipvalue": "Float. If set, the gradient of each weight is clipped to be no\nhigher than this value.", "global_clipnorm": "Float. If set, the gradient of all weights is clipped so\nthat their global norm is no higher than this value.", "use_ema": "Boolean, defaults to False. If True, exponential moving average\n(EMA) is applied. EMA consists of computing an exponential moving\naverage of the weights of the model (as the weight values change after\neach training batch), and periodically overwriting the weights with\ntheir moving average.", "ema_momentum": "Float, defaults to 0.99. Only used if use_ema=True. This is\nthe momentum to use when computing the EMA of the model's weights:\nnew_average = ema_momentum * old_average + (1 - ema_momentum) *\ncurrent_variable_value.", "ema_overwrite_frequency": "Int or None, defaults to None. Only used if\nuse_ema=True. Every ema_overwrite_frequency steps of iterations, we\noverwrite the model variable by its moving average. If None, the optimizer\n does not overwrite model variables in the middle of training, and you\nneed to explicitly overwrite the variables at the end of training\nby calling optimizer.finalize_variable_values() (which updates the model\nvariables in-place). When using the built-in fit() training loop, this\nhappens automatically after the last epoch, and you don't need to do\nanything.", "jit_compile": "Boolean, defaults to True. If True, the optimizer will use XLA\ncompilation. jit_compile cannot be True when training with\ntf.distribute.experimental.ParameterServerStrategy. Additionally,\nif no GPU device is found, this flag will be ignored.", "**kwargs": "keyword arguments only used for backward compatibility."}, "Attributes": {"iterations": "The number of training steps this optimizer has run.\nBy default, iterations would be incremented by one every time\napply_gradients() is called.", "learning_rate": ""}}, "tf.keras.dtensor.experimental.optimizers.SGD": {"description": "DTensor specific optimizers.", "Args": {"learning_rate": "A Tensor, floating point value, or a schedule that is a\ntf.keras.optimizers.schedules.LearningRateSchedule, or a callable\nthat takes no arguments and returns the actual value to use. The\nlearning rate. Defaults to 0.001.", "momentum": "float hyperparameter >= 0 that accelerates gradient descent\nin the relevant\ndirection and dampens oscillations. Defaults to 0, i.e., vanilla gradient\ndescent.", "nesterov": "boolean. Whether to apply Nesterov momentum.\nDefaults to False.", "name": "String. The name to use\nfor momentum accumulator weights created by\nthe optimizer.", "clipnorm": "Float. If set, the gradient of each weight is individually\nclipped so that its norm is no higher than this value.", "clipvalue": "Float. If set, the gradient of each weight is clipped to be no\nhigher than this value.", "global_clipnorm": "Float. If set, the gradient of all weights is clipped so\nthat their global norm is no higher than this value.", "use_ema": "Boolean, defaults to False. If True, exponential moving average\n(EMA) is applied. EMA consists of computing an exponential moving\naverage of the weights of the model (as the weight values change after\neach training batch), and periodically overwriting the weights with\ntheir moving average.", "ema_momentum": "Float, defaults to 0.99. Only used if use_ema=True. This is\nthe momentum to use when computing the EMA of the model's weights:\nnew_average = ema_momentum * old_average + (1 - ema_momentum) *\ncurrent_variable_value.", "ema_overwrite_frequency": "Int or None, defaults to None. Only used if\nuse_ema=True. Every ema_overwrite_frequency steps of iterations, we\noverwrite the model variable by its moving average. If None, the optimizer\n does not overwrite model variables in the middle of training, and you\nneed to explicitly overwrite the variables at the end of training\nby calling optimizer.finalize_variable_values() (which updates the model\nvariables in-place). When using the built-in fit() training loop, this\nhappens automatically after the last epoch, and you don't need to do\nanything.", "jit_compile": "Boolean, defaults to True. If True, the optimizer will use XLA\ncompilation. jit_compile cannot be True when training with\ntf.distribute.experimental.ParameterServerStrategy. Additionally,\nif no GPU device is found, this flag will be ignored.", "**kwargs": "keyword arguments only used for backward compatibility."}, "Attributes": {"iterations": "The number of training steps this optimizer has run.\nBy default, iterations would be incremented by one every time\napply_gradients() is called.", "learning_rate": ""}}}, "tf.keras.estimator": {"tf.keras.estimator.model_to_estimator": {"description": "Constructs an Estimator instance from given keras model.", "Args": {"keras_model": "A compiled Keras model object. This argument is mutually\nexclusive with keras_model_path. Estimator's model_fn uses the\nstructure of the model to clone the model. Defaults to None.", "keras_model_path": "Path to a compiled Keras model saved on disk, in HDF5\nformat, which can be generated with the save() method of a Keras model.\nThis argument is mutually exclusive with keras_model.\nDefaults to None.", "custom_objects": "Dictionary for cloning customized objects. This is\nused with classes that is not part of this pip package. For example, if\nuser maintains a relu6 class that inherits from tf.keras.layers.Layer,\nthen pass custom_objects={'relu6': relu6}. Defaults to None.", "model_dir": "Directory to save Estimator model parameters, graph, summary\nfiles for TensorBoard, etc. If unset a directory will be created with\ntempfile.mkdtemp", "config": "RunConfig to config Estimator. Allows setting up things in\nmodel_fn based on configuration such as num_ps_replicas, or\nmodel_dir. Defaults to None. If both config.model_dir and the\nmodel_dir argument (above) are specified the model_dir argument\ntakes precedence.", "checkpoint_format": "Sets the format of the checkpoint saved by the estimator\nwhen training. May be saver or checkpoint, depending on whether to\nsave checkpoints from tf.compat.v1.train.Saver or tf.train.Checkpoint.\nThe default is checkpoint. Estimators use name-based tf.train.Saver\ncheckpoints, while Keras models use object-based checkpoints from\ntf.train.Checkpoint. Currently, saving object-based checkpoints from\nmodel_to_estimator is only supported by Functional and Sequential\nmodels. Defaults to 'checkpoint'.", "metric_names_map": "Optional dictionary mapping Keras model output metric\nnames to custom names. This can be used to override the default Keras\nmodel output metrics names in a multi IO model use case and provide custom\nnames for the eval_metric_ops in Estimator.\nThe Keras model metric names can be obtained using model.metrics_names\nexcluding any loss metrics such as total loss and output losses.\nFor example, if your Keras model has two outputs out_1 and out_2,\nwith mse loss and acc metric, then model.metrics_names will be\n['loss', 'out_1_loss', 'out_2_loss', 'out_1_acc', 'out_2_acc'].\nThe model metric names excluding the loss metrics will be\n['out_1_acc', 'out_2_acc'].", "export_outputs": "Optional dictionary. This can be used to override the\ndefault Keras model output exports in a multi IO model use case and\nprovide custom names for the export_outputs in\ntf.estimator.EstimatorSpec. Default is None, which is equivalent to\n{'serving_default': tf.estimator.export.PredictOutput}. If not None,\nthe keys must match the keys of model.output_names.\nA dict {name: output} where:\n\nname: An arbitrary name for this output.\noutput: an ExportOutput class such as ClassificationOutput,\nRegressionOutput, or PredictOutput. Single-headed models only need\nto specify one entry in this dictionary. Multi-headed models should\nspecify one entry for each head, one of which must be named using\ntf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY\nIf no entry is provided, a default PredictOutput mapping to\npredictions will be created."}, "Returns": "An Estimator from given keras model.", "Raises": {"ValueError": "If an invalid checkpoint_format was given."}}}, "tf.keras.initializers": {"tf.keras.initializers.Constant": {"description": "Initializer that generates tensors with constant values.", "Args": {"value": "A Python scalar."}}, "tf.keras.initializers.GlorotNormal": {"description": "The Glorot normal initializer, also called Xavier normal initializer.", "Args": {"seed": "A Python integer. Used to make the behavior of the initializer\ndeterministic. Note that a seeded\ninitializer will not produce the same random values across multiple calls,\nbut multiple initializers will produce the same sequence when constructed\nwith the same seed value."}}, "tf.keras.initializers.GlorotUniform": {"description": "The Glorot uniform initializer, also called Xavier uniform initializer.", "Args": {"seed": "A Python integer. Used to make the behavior of the initializer\ndeterministic. Note that a seeded\ninitializer will not produce the same random values across multiple calls,\nbut multiple initializers will produce the same sequence when constructed\nwith the same seed value."}}, "tf.keras.initializers.HeNormal": {"description": "He normal initializer.", "Args": {"seed": "A Python integer. Used to make the behavior of the initializer\ndeterministic. Note that a seeded\ninitializer will not produce the same random values across multiple calls,\nbut multiple initializers will produce the same sequence when constructed\nwith the same seed value."}}, "tf.keras.initializers.HeUniform": {"description": "He uniform variance scaling initializer.", "Args": {"seed": "A Python integer. Used to make the behavior of the initializer\ndeterministic. Note that a seeded\ninitializer will not produce the same random values across multiple calls,\nbut multiple initializers will produce the same sequence when constructed\nwith the same seed value."}}, "tf.keras.initializers.Identity": {"description": "Initializer that generates the identity matrix.", "Args": {"gain": "Multiplicative factor to apply to the identity matrix."}}, "tf.keras.initializers.Initializer": {"description": "Initializer base class: all Keras initializers inherit from this class."}, "tf.keras.initializers.LecunNormal": {"description": "Lecun normal initializer.", "Args": {"seed": "A Python integer. Used to make the behavior of the initializer\ndeterministic. Note that a seeded\ninitializer will not produce the same random values across multiple calls,\nbut multiple initializers will produce the same sequence when constructed\nwith the same seed value."}}, "tf.keras.initializers.LecunUniform": {"description": "Lecun uniform initializer.", "Args": {"seed": "A Python integer. Used to make the behavior of the initializer\ndeterministic. Note that a seeded\ninitializer will not produce the same random values across multiple calls,\nbut multiple initializers will produce the same sequence when constructed\nwith the same seed value."}}, "tf.keras.initializers.Ones": {"description": "Initializer that generates tensors initialized to 1."}, "tf.keras.initializers.Orthogonal": {"description": "Initializer that generates an orthogonal matrix.", "Args": {"gain": "multiplicative factor to apply to the orthogonal matrix", "seed": "A Python integer. Used to make the behavior of the initializer\ndeterministic. Note that a seeded\ninitializer will not produce the same random values across multiple calls,\nbut multiple initializers will produce the same sequence when constructed\nwith the same seed value."}}, "tf.keras.initializers.RandomNormal": {"description": "Initializer that generates tensors with a normal distribution.", "Args": {"mean": "a python scalar or a scalar tensor. Mean of the random values to\ngenerate.", "stddev": "a python scalar or a scalar tensor. Standard deviation of the random\nvalues to generate.", "seed": "A Python integer. Used to make the behavior of the initializer\ndeterministic. Note that a seeded\ninitializer will not produce the same random values across multiple calls,\nbut multiple initializers will produce the same sequence when constructed\nwith the same seed value."}}, "tf.keras.initializers.RandomUniform": {"description": "Initializer that generates tensors with a uniform distribution.", "Args": {"minval": "A python scalar or a scalar tensor. Lower bound of the range of\nrandom values to generate (inclusive).", "maxval": "A python scalar or a scalar tensor. Upper bound of the range of\nrandom values to generate (exclusive).", "seed": "A Python integer. Used to make the behavior of the initializer\ndeterministic. Note that a seeded\ninitializer will not produce the same random values across multiple calls,\nbut multiple initializers will produce the same sequence when constructed\nwith the same seed value."}}, "tf.keras.initializers.TruncatedNormal": {"description": "Initializer that generates a truncated normal distribution.", "Args": {"mean": "a python scalar or a scalar tensor. Mean of the random values\nto generate.", "stddev": "a python scalar or a scalar tensor. Standard deviation of the\nrandom values to generate before truncation.", "seed": "A Python integer. Used to make the behavior of the initializer\ndeterministic. Note that a seeded\ninitializer will not produce the same random values across multiple calls,\nbut multiple initializers will produce the same sequence when constructed\nwith the same seed value."}}, "tf.keras.initializers.VarianceScaling": {"description": "Initializer capable of adapting its scale to the shape of weights tensors.", "Args": {"scale": "Scaling factor (positive float).", "mode": "One of \"fan_in\", \"fan_out\", \"fan_avg\".", "distribution": "Random distribution to use. One of \"truncated_normal\",\n\"untruncated_normal\" and  \"uniform\".", "seed": "A Python integer. Used to make the behavior of the initializer\ndeterministic. Note that a seeded\ninitializer will not produce the same random values across multiple calls,\nbut multiple initializers will produce the same sequence when constructed\nwith the same seed value."}}, "tf.keras.initializers.Zeros": {"description": "Initializer that generates tensors initialized to 0."}, "tf.keras.initializers.deserialize": {"description": "Return an Initializer object from its config."}, "tf.keras.initializers.get": {"description": "Retrieve a Keras initializer by the identifier.", "Args": {"identifier": "String or dict that contains the initializer name or\nconfigurations."}, "Returns": "Initializer instance base on the input identifier.", "Raises": {"ValueError": "If the input identifier is not a supported type or in a bad\nformat."}}, "tf.keras.initializers.serialize": {}}, "tf.keras.layers": {"tf.keras.layers.AbstractRNNCell": {"description": "Abstract object representing an RNN cell.", "Attributes": {"output_size": "Integer or TensorShape: size of outputs produced by this cell.", "state_size": "size(s) of state(s) used by this cell.\nIt can be represented by an Integer, a TensorShape or a tuple of Integers\nor TensorShapes."}}, "tf.keras.layers.Activation": {"description": "Applies an activation function to an output.", "Args": {"activation": "Activation function, such as tf.nn.relu, or string name of\nbuilt-in activation function, such as \"relu\"."}}, "tf.keras.layers.ActivityRegularization": {"description": "Layer that applies an update to the cost function based input activity.", "Args": {"l1": "L1 regularization factor (positive float).", "l2": "L2 regularization factor (positive float)."}}, "tf.keras.layers.Add": {"description": "Layer that adds a list of inputs.", "Args": {"**kwargs": "standard layer keyword arguments."}}, "tf.keras.layers.AdditiveAttention": {"description": "Additive attention layer, a.k.a. Bahdanau-style attention.", "Args": {"use_scale": "If True, will create a variable to scale the attention scores.", "causal": "Boolean. Set to True for decoder self-attention. Adds a mask such\nthat position i cannot attend to positions j > i. This prevents the\nflow of information from the future towards the past.\nDefaults to False.", "dropout": "Float between 0 and 1. Fraction of the units to drop for the\nattention scores. Defaults to 0.0."}}, "tf.keras.layers.AlphaDropout": {"description": "Applies Alpha Dropout to the input.", "Args": {"rate": "float, drop probability (as with Dropout).\nThe multiplicative noise will have\nstandard deviation sqrt(rate / (1 - rate)).", "seed": "Integer, optional random seed to enable deterministic behavior."}}, "tf.keras.layers.Attention": {"description": "Dot-product attention layer, a.k.a. Luong-style attention.", "Args": {"use_scale": "If True, will create a scalar variable to scale the attention\nscores.", "causal": "Boolean. Set to True for decoder self-attention. Adds a mask such\nthat position i cannot attend to positions j > i. This prevents the\nflow of information from the future towards the past.\nDefaults to False.", "dropout": "Float between 0 and 1. Fraction of the units to drop for the\nattention scores. Defaults to 0.0.", "score_mode": "Function to use to compute attention scores, one of\n{\"dot\", \"concat\"}. \"dot\" refers to the dot product between the query\nand key vectors. \"concat\" refers to the hyperbolic tangent of the\nconcatenation of the query and key vectors."}}, "tf.keras.layers.Average": {"description": "Layer that averages a list of inputs element-wise.", "Raises": {"ValueError": "If there is a shape mismatch between the inputs and the shapes\ncannot be broadcasted to match."}, "Args": {"**kwargs": "standard layer keyword arguments."}}, "tf.keras.layers.AveragePooling1D": {"description": "Average pooling for temporal data.", "Args": {"pool_size": "Integer, size of the average pooling windows.", "strides": "Integer, or None. Factor by which to downscale.\nE.g. 2 will halve the input.\nIf None, it will default to pool_size.", "padding": "One of \"valid\" or \"same\" (case-insensitive).\n\"valid\" means no padding. \"same\" results in padding evenly to\nthe left/right or up/down of the input such that output has the same\nheight/width dimension as the input.", "data_format": "A string,\none of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs.\nchannels_last corresponds to inputs with shape\n(batch, steps, features) while channels_first\ncorresponds to inputs with shape\n(batch, features, steps)."}}, "tf.keras.layers.AveragePooling2D": {"description": "Average pooling operation for spatial data.", "Args": {"pool_size": "integer or tuple of 2 integers,\nfactors by which to downscale (vertical, horizontal).\n(2, 2) will halve the input in both spatial dimension.\nIf only one integer is specified, the same window length\nwill be used for both dimensions.", "strides": "Integer, tuple of 2 integers, or None.\nStrides values.\nIf None, it will default to pool_size.", "padding": "One of \"valid\" or \"same\" (case-insensitive).\n\"valid\" means no padding. \"same\" results in padding evenly to\nthe left/right or up/down of the input such that output has the same\nheight/width dimension as the input.", "data_format": "A string,\none of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs.\nchannels_last corresponds to inputs with shape\n(batch, height, width, channels) while channels_first\ncorresponds to inputs with shape\n(batch, channels, height, width).\nIt defaults to the image_data_format value found in your\nKeras config file at ~/.keras/keras.json.\nIf you never set it, then it will be \"channels_last\"."}}, "tf.keras.layers.AveragePooling3D": {"description": "Average pooling operation for 3D data (spatial or spatio-temporal).", "Args": {"pool_size": "tuple of 3 integers,\nfactors by which to downscale (dim1, dim2, dim3).\n(2, 2, 2) will halve the size of the 3D input in each dimension.", "strides": "tuple of 3 integers, or None. Strides values.", "padding": "One of \"valid\" or \"same\" (case-insensitive).\n\"valid\" means no padding. \"same\" results in padding evenly to\nthe left/right or up/down of the input such that output has the same\nheight/width dimension as the input.", "data_format": "A string,\none of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs.\nchannels_last corresponds to inputs with shape\n(batch, spatial_dim1, spatial_dim2, spatial_dim3, channels)\nwhile channels_first corresponds to inputs with shape\n(batch, channels, spatial_dim1, spatial_dim2, spatial_dim3).\nIt defaults to the image_data_format value found in your\nKeras config file at ~/.keras/keras.json.\nIf you never set it, then it will be \"channels_last\"."}}, "tf.keras.layers.BatchNormalization": {"description": "Layer that normalizes its inputs.", "Args": {"axis": "Integer, the axis that should be normalized (typically the features\naxis). For instance, after a Conv2D layer with\ndata_format=\"channels_first\", set axis=1 in BatchNormalization.", "momentum": "Momentum for the moving average.", "epsilon": "Small float added to variance to avoid dividing by zero.", "center": "If True, add offset of beta to normalized tensor. If False, beta\nis ignored.", "scale": "If True, multiply by gamma. If False, gamma is not used. When the\nnext layer is linear (also e.g. nn.relu), this can be disabled since the\nscaling will be done by the next layer.", "beta_initializer": "Initializer for the beta weight.", "gamma_initializer": "Initializer for the gamma weight.", "moving_mean_initializer": "Initializer for the moving mean.", "moving_variance_initializer": "Initializer for the moving variance.", "beta_regularizer": "Optional regularizer for the beta weight.", "gamma_regularizer": "Optional regularizer for the gamma weight.", "beta_constraint": "Optional constraint for the beta weight.", "gamma_constraint": "Optional constraint for the gamma weight."}}, "tf.keras.layers.Bidirectional": {"description": "Bidirectional wrapper for RNNs.", "Args": {"layer": "keras.layers.RNN instance, such as keras.layers.LSTM or\nkeras.layers.GRU. It could also be a keras.layers.Layer instance\nthat meets the following criteria:\n\nBe a sequence-processing layer (accepts 3D+ inputs).\nHave a go_backwards, return_sequences and return_state\nattribute (with the same semantics as for the RNN class).\nHave an input_spec attribute.\nImplement serialization via get_config() and from_config().\nNote that the recommended way to create new RNN layers is to write a\ncustom RNN cell and use it with keras.layers.RNN, instead of\nsubclassing keras.layers.Layer directly.\nWhen the returns_sequences is true, the output of the masked timestep\nwill be zero regardless of the layer's original zero_output_for_mask\nvalue.", "merge_mode": "Mode by which outputs of the forward and backward RNNs will be\ncombined. One of {'sum', 'mul', 'concat', 'ave', None}. If None, the\noutputs will not be combined, they will be returned as a list. Default\nvalue is 'concat'.", "backward_layer": "Optional keras.layers.RNN, or keras.layers.Layer\ninstance to be used to handle backwards input processing.\nIf backward_layer is not provided, the layer instance passed as the\nlayer argument will be used to generate the backward layer\nautomatically.\nNote that the provided backward_layer layer should have properties\nmatching those of the layer argument, in particular it should have the\nsame values for stateful, return_states, return_sequences, etc.\nIn addition, backward_layer and layer should have different\ngo_backwards argument values.\nA ValueError will be raised if these requirements are not met."}, "Raises": {"ValueError": "If layer or backward_layer is not a Layer instance.\nIn case of invalid merge_mode argument.\nIf backward_layer has mismatched properties compared to layer."}, "Attributes": {"constraints": ""}}, "tf.keras.layers.CategoryEncoding": {"description": "A preprocessing layer which encodes integer features.", "Args": {"num_tokens": "The total number of tokens the layer should support. All inputs\nto the layer must integers in the range 0 <= value < num_tokens, or an\nerror will be thrown.", "output_mode": "Specification for the output of the layer.\nDefaults to \"multi_hot\". Values can be \"one_hot\", \"multi_hot\" or\n\"count\", configuring the layer as follows:\n\n\"one_hot\": Encodes each individual element in the input into an\narray of num_tokens size, containing a 1 at the element index. If\nthe last dimension is size 1, will encode on that dimension. If the\nlast dimension is not size 1, will append a new dimension for the\nencoded output.\n\"multi_hot\": Encodes each sample in the input into a single array\nof num_tokens size, containing a 1 for each vocabulary term present\nin the sample. Treats the last dimension as the sample dimension, if\ninput shape is (..., sample_length), output shape will be\n(..., num_tokens).\n\"count\": Like \"multi_hot\", but the int array contains a count of\nthe number of times the token at that index appeared in the sample.\nFor all output modes, currently only output up to rank 2 is supported.", "sparse": "Boolean. If true, returns a SparseTensor instead of a dense\nTensor. Defaults to False."}}, "tf.keras.layers.CenterCrop": {"description": "A preprocessing layer which crops images.", "Args": {"height": "Integer, the height of the output shape.", "width": "Integer, the width of the output shape."}}, "tf.keras.layers.Concatenate": {"description": "Layer that concatenates a list of inputs.", "Args": {"axis": "Axis along which to concatenate.", "**kwargs": "standard layer keyword arguments."}}, "tf.keras.layers.Conv1D": {"description": "1D convolution layer (e.g. temporal convolution).", "Args": {"filters": "Integer, the dimensionality of the output space\n(i.e. the number of output filters in the convolution).", "kernel_size": "An integer or tuple/list of a single integer,\nspecifying the length of the 1D convolution window.", "strides": "An integer or tuple/list of a single integer,\nspecifying the stride length of the convolution.\nSpecifying any stride value != 1 is incompatible with specifying\nany dilation_rate value != 1.", "padding": "One of \"valid\", \"same\" or \"causal\" (case-insensitive).\n\"valid\" means no padding. \"same\" results in padding with zeros evenly\nto the left/right or up/down of the input such that output has the same\nheight/width dimension as the input.\n\"causal\" results in causal (dilated) convolutions, e.g. output[t]\ndoes not depend on input[t+1:]. Useful when modeling temporal data\nwhere the model should not violate the temporal order.\nSee WaveNet: A Generative Model for Raw Audio, section\n  2.1.", "data_format": "A string,\none of channels_last (default) or channels_first.", "dilation_rate": "an integer or tuple/list of a single integer, specifying\nthe dilation rate to use for dilated convolution.\nCurrently, specifying any dilation_rate value != 1 is\nincompatible with specifying any strides value != 1.", "groups": "A positive integer specifying the number of groups in which the\ninput is split along the channel axis. Each group is convolved\nseparately with filters / groups filters. The output is the\nconcatenation of all the groups results along the channel axis.\nInput channels and filters must both be divisible by groups.", "activation": "Activation function to use.\nIf you don't specify anything, no activation is applied\n(see keras.activations).", "use_bias": "Boolean, whether the layer uses a bias vector.", "kernel_initializer": "Initializer for the kernel weights matrix\n(see keras.initializers). Defaults to 'glorot_uniform'.", "bias_initializer": "Initializer for the bias vector\n(see keras.initializers). Defaults to 'zeros'.", "kernel_regularizer": "Regularizer function applied to\nthe kernel weights matrix (see keras.regularizers).", "bias_regularizer": "Regularizer function applied to the bias vector\n(see keras.regularizers).", "activity_regularizer": "Regularizer function applied to\nthe output of the layer (its \"activation\")\n(see keras.regularizers).", "kernel_constraint": "Constraint function applied to the kernel matrix\n(see keras.constraints).", "bias_constraint": "Constraint function applied to the bias vector\n(see keras.constraints)."}, "Returns": "A tensor of rank 3 representing\nactivation(conv1d(inputs, kernel) + bias).", "Raises": {"ValueError": "when both strides > 1 and dilation_rate > 1."}}, "tf.keras.layers.Conv1DTranspose": {"description": "Transposed convolution layer (sometimes called Deconvolution).", "Args": {"filters": "Integer, the dimensionality of the output space\n(i.e. the number of output filters in the convolution).", "kernel_size": "An integer length of the 1D convolution window.", "strides": "An integer specifying the stride of the convolution along the\ntime dimension. Specifying a stride value != 1 is incompatible with\nspecifying a dilation_rate value != 1. Defaults to 1.", "padding": "one of \"valid\" or \"same\" (case-insensitive).\n\"valid\" means no padding. \"same\" results in padding with zeros evenly\nto the left/right or up/down of the input such that output has the same\nheight/width dimension as the input.", "output_padding": "An integer specifying the amount of padding along\nthe time dimension of the output tensor.\nThe amount of output padding must be lower than the stride.\nIf set to None (default), the output shape is inferred.", "data_format": "A string, one of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs.\nchannels_last corresponds to inputs with shape\n(batch_size, length, channels) while channels_first corresponds to\ninputs with shape (batch_size, channels, length).", "dilation_rate": "an integer, specifying\nthe dilation rate to use for dilated convolution.\nCurrently, specifying a dilation_rate value != 1 is\nincompatible with specifying a stride value != 1.\nAlso dilation rate larger than 1 is not currently supported.", "activation": "Activation function to use.\nIf you don't specify anything, no activation is applied\n(see keras.activations).", "use_bias": "Boolean, whether the layer uses a bias vector.", "kernel_initializer": "Initializer for the kernel weights matrix\n(see keras.initializers). Defaults to 'glorot_uniform'.", "bias_initializer": "Initializer for the bias vector\n(see keras.initializers). Defaults to 'zeros'.", "kernel_regularizer": "Regularizer function applied to\nthe kernel weights matrix (see keras.regularizers).", "bias_regularizer": "Regularizer function applied to the bias vector\n(see keras.regularizers).", "activity_regularizer": "Regularizer function applied to\nthe output of the layer (its \"activation\") (see keras.regularizers).", "kernel_constraint": "Constraint function applied to the kernel matrix\n(see keras.constraints).", "bias_constraint": "Constraint function applied to the bias vector\n(see keras.constraints)."}, "Returns": "A tensor of rank 3 representing\nactivation(conv1dtranspose(inputs, kernel) + bias).", "Raises": {"ValueError": "when both strides > 1 and dilation_rate > 1."}}, "tf.keras.layers.Conv2D": {"description": "2D convolution layer (e.g. spatial convolution over images).", "Args": {"filters": "Integer, the dimensionality of the output space (i.e. the number of\noutput filters in the convolution).", "kernel_size": "An integer or tuple/list of 2 integers, specifying the height\nand width of the 2D convolution window. Can be a single integer to specify\nthe same value for all spatial dimensions.", "strides": "An integer or tuple/list of 2 integers, specifying the strides of\nthe convolution along the height and width. Can be a single integer to\nspecify the same value for all spatial dimensions. Specifying any stride\nvalue != 1 is incompatible with specifying any dilation_rate value != 1.", "padding": "one of \"valid\" or \"same\" (case-insensitive).\n\"valid\" means no padding. \"same\" results in padding with zeros evenly\nto the left/right or up/down of the input. When padding=\"same\" and\nstrides=1, the output has the same size as the input.", "data_format": "A string, one of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs. channels_last corresponds\nto inputs with shape (batch_size, height, width, channels) while\nchannels_first corresponds to inputs with shape (batch_size, channels,\nheight, width). It defaults to the image_data_format value found in\nyour Keras config file at ~/.keras/keras.json. If you never set it, then\nit will be channels_last.", "dilation_rate": "an integer or tuple/list of 2 integers, specifying the\ndilation rate to use for dilated convolution. Can be a single integer to\nspecify the same value for all spatial dimensions. Currently, specifying\nany dilation_rate value != 1 is incompatible with specifying any stride\nvalue != 1.", "groups": "A positive integer specifying the number of groups in which the\ninput is split along the channel axis. Each group is convolved separately\nwith filters / groups filters. The output is the concatenation of all\nthe groups results along the channel axis. Input channels and filters\nmust both be divisible by groups.", "activation": "Activation function to use. If you don't specify anything, no\nactivation is applied (see keras.activations).", "use_bias": "Boolean, whether the layer uses a bias vector.", "kernel_initializer": "Initializer for the kernel weights matrix (see\nkeras.initializers). Defaults to 'glorot_uniform'.", "bias_initializer": "Initializer for the bias vector (see\nkeras.initializers). Defaults to 'zeros'.", "kernel_regularizer": "Regularizer function applied to the kernel weights\nmatrix (see keras.regularizers).", "bias_regularizer": "Regularizer function applied to the bias vector (see\nkeras.regularizers).", "activity_regularizer": "Regularizer function applied to the output of the\nlayer (its \"activation\") (see keras.regularizers).", "kernel_constraint": "Constraint function applied to the kernel matrix (see\nkeras.constraints).", "bias_constraint": "Constraint function applied to the bias vector (see\nkeras.constraints)."}, "Returns": "A tensor of rank 4+ representing\nactivation(conv2d(inputs, kernel) + bias).", "Raises": {"ValueError": "when both strides > 1 and dilation_rate > 1."}}, "tf.keras.layers.Conv2DTranspose": {"description": "Transposed convolution layer (sometimes called Deconvolution).", "Args": {"filters": "Integer, the dimensionality of the output space\n(i.e. the number of output filters in the convolution).", "kernel_size": "An integer or tuple/list of 2 integers, specifying the\nheight and width of the 2D convolution window.\nCan be a single integer to specify the same value for\nall spatial dimensions.", "strides": "An integer or tuple/list of 2 integers,\nspecifying the strides of the convolution along the height and width.\nCan be a single integer to specify the same value for\nall spatial dimensions.\nSpecifying any stride value != 1 is incompatible with specifying\nany dilation_rate value != 1.", "padding": "one of \"valid\" or \"same\" (case-insensitive).\n\"valid\" means no padding. \"same\" results in padding with zeros evenly\nto the left/right or up/down of the input such that output has the same\nheight/width dimension as the input.", "output_padding": "An integer or tuple/list of 2 integers,\nspecifying the amount of padding along the height and width\nof the output tensor.\nCan be a single integer to specify the same value for all\nspatial dimensions.\nThe amount of output padding along a given dimension must be\nlower than the stride along that same dimension.\nIf set to None (default), the output shape is inferred.", "data_format": "A string,\none of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs.\nchannels_last corresponds to inputs with shape\n(batch_size, height, width, channels) while channels_first\ncorresponds to inputs with shape\n(batch_size, channels, height, width).\nIt defaults to the image_data_format value found in your\nKeras config file at ~/.keras/keras.json.\nIf you never set it, then it will be \"channels_last\".", "dilation_rate": "an integer, specifying the dilation rate for all spatial\ndimensions for dilated convolution. Specifying different dilation rates\nfor different dimensions is not supported.\nCurrently, specifying any dilation_rate value != 1 is\nincompatible with specifying any stride value != 1.", "activation": "Activation function to use.\nIf you don't specify anything, no activation is applied\n(see keras.activations).", "use_bias": "Boolean, whether the layer uses a bias vector.", "kernel_initializer": "Initializer for the kernel weights matrix\n(see keras.initializers). Defaults to 'glorot_uniform'.", "bias_initializer": "Initializer for the bias vector\n(see keras.initializers). Defaults to 'zeros'.", "kernel_regularizer": "Regularizer function applied to\nthe kernel weights matrix (see keras.regularizers).", "bias_regularizer": "Regularizer function applied to the bias vector\n(see keras.regularizers).", "activity_regularizer": "Regularizer function applied to\nthe output of the layer (its \"activation\") (see keras.regularizers).", "kernel_constraint": "Constraint function applied to the kernel matrix\n(see keras.constraints).", "bias_constraint": "Constraint function applied to the bias vector\n(see keras.constraints)."}, "Returns": "A tensor of rank 4 representing\nactivation(conv2dtranspose(inputs, kernel) + bias).", "Raises": {"ValueError": "when both strides > 1 and dilation_rate > 1."}}, "tf.keras.layers.Conv3D": {"description": "3D convolution layer (e.g. spatial convolution over volumes).", "Args": {"filters": "Integer, the dimensionality of the output space (i.e. the number of\noutput filters in the convolution).", "kernel_size": "An integer or tuple/list of 3 integers, specifying the depth,\nheight and width of the 3D convolution window. Can be a single integer to\nspecify the same value for all spatial dimensions.", "strides": "An integer or tuple/list of 3 integers, specifying the strides of\nthe convolution along each spatial dimension. Can be a single integer to\nspecify the same value for all spatial dimensions. Specifying any stride\nvalue != 1 is incompatible with specifying any dilation_rate value != 1.", "padding": "one of \"valid\" or \"same\" (case-insensitive).\n\"valid\" means no padding. \"same\" results in padding with zeros evenly\nto the left/right or up/down of the input such that output has the same\nheight/width dimension as the input.", "data_format": "A string, one of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs. channels_last corresponds\nto inputs with shape batch_shape + (spatial_dim1, spatial_dim2,\nspatial_dim3, channels) while channels_first corresponds to inputs with\nshape batch_shape + (channels, spatial_dim1, spatial_dim2,\nspatial_dim3). It defaults to the image_data_format value found in your\nKeras config file at ~/.keras/keras.json. If you never set it, then it\nwill be \"channels_last\".", "dilation_rate": "an integer or tuple/list of 3 integers, specifying the\ndilation rate to use for dilated convolution. Can be a single integer to\nspecify the same value for all spatial dimensions. Currently, specifying\nany dilation_rate value != 1 is incompatible with specifying any stride\nvalue != 1.", "groups": "A positive integer specifying the number of groups in which the\ninput is split along the channel axis. Each group is convolved separately\nwith filters / groups filters. The output is the concatenation of all\nthe groups results along the channel axis. Input channels and filters\nmust both be divisible by groups.", "activation": "Activation function to use. If you don't specify anything, no\nactivation is applied (see keras.activations).", "use_bias": "Boolean, whether the layer uses a bias vector.", "kernel_initializer": "Initializer for the kernel weights matrix (see\nkeras.initializers). Defaults to 'glorot_uniform'.", "bias_initializer": "Initializer for the bias vector (see\nkeras.initializers). Defaults to 'zeros'.", "kernel_regularizer": "Regularizer function applied to the kernel weights\nmatrix (see keras.regularizers).", "bias_regularizer": "Regularizer function applied to the bias vector (see\nkeras.regularizers).", "activity_regularizer": "Regularizer function applied to the output of the\nlayer (its \"activation\") (see keras.regularizers).", "kernel_constraint": "Constraint function applied to the kernel matrix (see\nkeras.constraints).", "bias_constraint": "Constraint function applied to the bias vector (see\nkeras.constraints)."}, "Returns": "A tensor of rank 5+ representing\nactivation(conv3d(inputs, kernel) + bias).", "Raises": {"ValueError": "when both strides > 1 and dilation_rate > 1."}}, "tf.keras.layers.Conv3DTranspose": {"description": "Transposed convolution layer (sometimes called Deconvolution).", "Args": {"filters": "Integer, the dimensionality of the output space\n(i.e. the number of output filters in the convolution).", "kernel_size": "An integer or tuple/list of 3 integers, specifying the\ndepth, height and width of the 3D convolution window.\nCan be a single integer to specify the same value for\nall spatial dimensions.", "strides": "An integer or tuple/list of 3 integers,\nspecifying the strides of the convolution along the depth, height\n  and width.\nCan be a single integer to specify the same value for\nall spatial dimensions.\nSpecifying any stride value != 1 is incompatible with specifying\nany dilation_rate value != 1.", "padding": "one of \"valid\" or \"same\" (case-insensitive).\n\"valid\" means no padding. \"same\" results in padding with zeros evenly\nto the left/right or up/down of the input such that output has the same\nheight/width dimension as the input.", "output_padding": "An integer or tuple/list of 3 integers,\nspecifying the amount of padding along the depth, height, and\nwidth.\nCan be a single integer to specify the same value for all\nspatial dimensions.\nThe amount of output padding along a given dimension must be\nlower than the stride along that same dimension.\nIf set to None (default), the output shape is inferred.", "data_format": "A string,\none of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs.\nchannels_last corresponds to inputs with shape\n(batch_size, depth, height, width, channels) while channels_first\ncorresponds to inputs with shape\n(batch_size, channels, depth, height, width).\nIt defaults to the image_data_format value found in your\nKeras config file at ~/.keras/keras.json.\nIf you never set it, then it will be \"channels_last\".", "dilation_rate": "an integer or tuple/list of 3 integers, specifying\nthe dilation rate to use for dilated convolution.\nCan be a single integer to specify the same value for\nall spatial dimensions.\nCurrently, specifying any dilation_rate value != 1 is\nincompatible with specifying any stride value != 1.", "activation": "Activation function to use.\nIf you don't specify anything, no activation is applied\n(see keras.activations).", "use_bias": "Boolean, whether the layer uses a bias vector.", "kernel_initializer": "Initializer for the kernel weights matrix\n(see keras.initializers). Defaults to 'glorot_uniform'.", "bias_initializer": "Initializer for the bias vector\n(see keras.initializers). Defaults to 'zeros'.", "kernel_regularizer": "Regularizer function applied to\nthe kernel weights matrix\n(see keras.regularizers).", "bias_regularizer": "Regularizer function applied to the bias vector\n(see keras.regularizers).", "activity_regularizer": "Regularizer function applied to\nthe output of the layer (its \"activation\")\n(see keras.regularizers).", "kernel_constraint": "Constraint function applied to the kernel matrix\n(see keras.constraints).", "bias_constraint": "Constraint function applied to the bias vector\n(see keras.constraints)."}, "Returns": "A tensor of rank 5 representing\nactivation(conv3dtranspose(inputs, kernel) + bias).", "Raises": {"ValueError": "when both strides > 1 and dilation_rate > 1."}}, "tf.keras.layers.ConvLSTM1D": {"description": "1D Convolutional LSTM.", "Args": {"filters": "Integer, the dimensionality of the output space (i.e. the number of\noutput filters in the convolution).", "kernel_size": "An integer or tuple/list of n integers, specifying the\ndimensions of the convolution window.", "strides": "An integer or tuple/list of n integers, specifying the strides of\nthe convolution. Specifying any stride value != 1 is incompatible with\nspecifying any dilation_rate value != 1.", "padding": "One of \"valid\" or \"same\" (case-insensitive). \"valid\" means no\npadding. \"same\" results in padding evenly to the left/right or up/down\nof the input such that output has the same height/width dimension as the\ninput.", "data_format": "A string, one of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs. channels_last corresponds\nto inputs with shape (batch, time, ..., channels) while channels_first\ncorresponds to inputs with shape (batch, time, channels, ...). It\ndefaults to the image_data_format value found in your Keras config file\nat ~/.keras/keras.json. If you never set it, then it will be\n\"channels_last\".", "dilation_rate": "An integer or tuple/list of n integers, specifying the\ndilation rate to use for dilated convolution. Currently, specifying any\ndilation_rate value != 1 is incompatible with specifying any strides\nvalue != 1.", "activation": "Activation function to use. By default hyperbolic tangent\nactivation function is applied (tanh(x)).", "recurrent_activation": "Activation function to use for the recurrent step.", "use_bias": "Boolean, whether the layer uses a bias vector.", "kernel_initializer": "Initializer for the kernel weights matrix, used for\nthe linear transformation of the inputs.", "recurrent_initializer": "Initializer for the recurrent_kernel weights\nmatrix, used for the linear transformation of the recurrent state.", "bias_initializer": "Initializer for the bias vector.", "unit_forget_bias": "Boolean. If True, add 1 to the bias of the forget gate at\ninitialization. Use in combination with bias_initializer=\"zeros\". This\nis recommended in Jozefowicz et al., 2015", "kernel_regularizer": "Regularizer function applied to the kernel weights\nmatrix.", "recurrent_regularizer": "Regularizer function applied to the\nrecurrent_kernel weights matrix.", "bias_regularizer": "Regularizer function applied to the bias vector.", "activity_regularizer": "Regularizer function applied to.", "kernel_constraint": "Constraint function applied to the kernel weights\nmatrix.", "recurrent_constraint": "Constraint function applied to the recurrent_kernel\nweights matrix.", "bias_constraint": "Constraint function applied to the bias vector.", "return_sequences": "Boolean. Whether to return the last output in the output\nsequence, or the full sequence. (default False)", "return_state": "Boolean Whether to return the last state in addition to the\noutput. (default False)", "go_backwards": "Boolean (default False). If True, process the input sequence\nbackwards.", "stateful": "Boolean (default False). If True, the last state for each sample\nat index i in a batch will be used as initial state for the sample of\nindex i in the following batch.", "dropout": "Float between 0 and 1. Fraction of the units to drop for the linear\ntransformation of the inputs.", "recurrent_dropout": "Float between 0 and 1. Fraction of the units to drop for\nthe linear transformation of the recurrent state."}, "Raises": {"ValueError": "in case of invalid constructor arguments."}, "Attributes": {"activation": "", "bias_constraint": "", "bias_initializer": "", "bias_regularizer": "", "data_format": "", "dilation_rate": "", "dropout": "", "filters": "", "kernel_constraint": "", "kernel_initializer": "", "kernel_regularizer": "", "kernel_size": "", "padding": "", "recurrent_activation": "", "recurrent_constraint": "", "recurrent_dropout": "", "recurrent_initializer": "", "recurrent_regularizer": "", "states": "", "strides": "", "unit_forget_bias": "", "use_bias": ""}}, "tf.keras.layers.ConvLSTM2D": {"description": "2D Convolutional LSTM.", "Args": {"filters": "Integer, the dimensionality of the output space (i.e. the number of\noutput filters in the convolution).", "kernel_size": "An integer or tuple/list of n integers, specifying the\ndimensions of the convolution window.", "strides": "An integer or tuple/list of n integers, specifying the strides of\nthe convolution. Specifying any stride value != 1 is incompatible with\nspecifying any dilation_rate value != 1.", "padding": "One of \"valid\" or \"same\" (case-insensitive). \"valid\" means no\npadding. \"same\" results in padding evenly to the left/right or up/down\nof the input such that output has the same height/width dimension as the\ninput.", "data_format": "A string, one of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs. channels_last corresponds\nto inputs with shape (batch, time, ..., channels) while channels_first\ncorresponds to inputs with shape (batch, time, channels, ...). It\ndefaults to the image_data_format value found in your Keras config file\nat ~/.keras/keras.json. If you never set it, then it will be\n\"channels_last\".", "dilation_rate": "An integer or tuple/list of n integers, specifying the\ndilation rate to use for dilated convolution. Currently, specifying any\ndilation_rate value != 1 is incompatible with specifying any strides\nvalue != 1.", "activation": "Activation function to use. By default hyperbolic tangent\nactivation function is applied (tanh(x)).", "recurrent_activation": "Activation function to use for the recurrent step.", "use_bias": "Boolean, whether the layer uses a bias vector.", "kernel_initializer": "Initializer for the kernel weights matrix, used for\nthe linear transformation of the inputs.", "recurrent_initializer": "Initializer for the recurrent_kernel weights\nmatrix, used for the linear transformation of the recurrent state.", "bias_initializer": "Initializer for the bias vector.", "unit_forget_bias": "Boolean. If True, add 1 to the bias of the forget gate at\ninitialization. Use in combination with bias_initializer=\"zeros\". This\nis recommended in Jozefowicz et al., 2015", "kernel_regularizer": "Regularizer function applied to the kernel weights\nmatrix.", "recurrent_regularizer": "Regularizer function applied to the\nrecurrent_kernel weights matrix.", "bias_regularizer": "Regularizer function applied to the bias vector.", "activity_regularizer": "Regularizer function applied to.", "kernel_constraint": "Constraint function applied to the kernel weights\nmatrix.", "recurrent_constraint": "Constraint function applied to the recurrent_kernel\nweights matrix.", "bias_constraint": "Constraint function applied to the bias vector.", "return_sequences": "Boolean. Whether to return the last output in the output\nsequence, or the full sequence. (default False)", "return_state": "Boolean Whether to return the last state in addition to the\noutput. (default False)", "go_backwards": "Boolean (default False). If True, process the input sequence\nbackwards.", "stateful": "Boolean (default False). If True, the last state for each sample\nat index i in a batch will be used as initial state for the sample of\nindex i in the following batch.", "dropout": "Float between 0 and 1. Fraction of the units to drop for the linear\ntransformation of the inputs.", "recurrent_dropout": "Float between 0 and 1. Fraction of the units to drop for\nthe linear transformation of the recurrent state."}, "Raises": {"ValueError": "in case of invalid constructor arguments."}, "Attributes": {"activation": "", "bias_constraint": "", "bias_initializer": "", "bias_regularizer": "", "data_format": "", "dilation_rate": "", "dropout": "", "filters": "", "kernel_constraint": "", "kernel_initializer": "", "kernel_regularizer": "", "kernel_size": "", "padding": "", "recurrent_activation": "", "recurrent_constraint": "", "recurrent_dropout": "", "recurrent_initializer": "", "recurrent_regularizer": "", "states": "", "strides": "", "unit_forget_bias": "", "use_bias": ""}}, "tf.keras.layers.ConvLSTM3D": {"description": "3D Convolutional LSTM.", "Args": {"filters": "Integer, the dimensionality of the output space (i.e. the number of\noutput filters in the convolution).", "kernel_size": "An integer or tuple/list of n integers, specifying the\ndimensions of the convolution window.", "strides": "An integer or tuple/list of n integers, specifying the strides of\nthe convolution. Specifying any stride value != 1 is incompatible with\nspecifying any dilation_rate value != 1.", "padding": "One of \"valid\" or \"same\" (case-insensitive). \"valid\" means no\npadding. \"same\" results in padding evenly to the left/right or up/down\nof the input such that output has the same height/width dimension as the\ninput.", "data_format": "A string, one of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs. channels_last corresponds\nto inputs with shape (batch, time, ..., channels) while channels_first\ncorresponds to inputs with shape (batch, time, channels, ...). It\ndefaults to the image_data_format value found in your Keras config file\nat ~/.keras/keras.json. If you never set it, then it will be\n\"channels_last\".", "dilation_rate": "An integer or tuple/list of n integers, specifying the\ndilation rate to use for dilated convolution. Currently, specifying any\ndilation_rate value != 1 is incompatible with specifying any strides\nvalue != 1.", "activation": "Activation function to use. By default hyperbolic tangent\nactivation function is applied (tanh(x)).", "recurrent_activation": "Activation function to use for the recurrent step.", "use_bias": "Boolean, whether the layer uses a bias vector.", "kernel_initializer": "Initializer for the kernel weights matrix, used for\nthe linear transformation of the inputs.", "recurrent_initializer": "Initializer for the recurrent_kernel weights\nmatrix, used for the linear transformation of the recurrent state.", "bias_initializer": "Initializer for the bias vector.", "unit_forget_bias": "Boolean. If True, add 1 to the bias of the forget gate at\ninitialization. Use in combination with bias_initializer=\"zeros\". This\nis recommended in Jozefowicz et al., 2015", "kernel_regularizer": "Regularizer function applied to the kernel weights\nmatrix.", "recurrent_regularizer": "Regularizer function applied to the\nrecurrent_kernel weights matrix.", "bias_regularizer": "Regularizer function applied to the bias vector.", "activity_regularizer": "Regularizer function applied to.", "kernel_constraint": "Constraint function applied to the kernel weights\nmatrix.", "recurrent_constraint": "Constraint function applied to the recurrent_kernel\nweights matrix.", "bias_constraint": "Constraint function applied to the bias vector.", "return_sequences": "Boolean. Whether to return the last output in the output\nsequence, or the full sequence. (default False)", "return_state": "Boolean Whether to return the last state in addition to the\noutput. (default False)", "go_backwards": "Boolean (default False). If True, process the input sequence\nbackwards.", "stateful": "Boolean (default False). If True, the last state for each sample\nat index i in a batch will be used as initial state for the sample of\nindex i in the following batch.", "dropout": "Float between 0 and 1. Fraction of the units to drop for the linear\ntransformation of the inputs.", "recurrent_dropout": "Float between 0 and 1. Fraction of the units to drop for\nthe linear transformation of the recurrent state."}, "Raises": {"ValueError": "in case of invalid constructor arguments."}, "Attributes": {"activation": "", "bias_constraint": "", "bias_initializer": "", "bias_regularizer": "", "data_format": "", "dilation_rate": "", "dropout": "", "filters": "", "kernel_constraint": "", "kernel_initializer": "", "kernel_regularizer": "", "kernel_size": "", "padding": "", "recurrent_activation": "", "recurrent_constraint": "", "recurrent_dropout": "", "recurrent_initializer": "", "recurrent_regularizer": "", "states": "", "strides": "", "unit_forget_bias": "", "use_bias": ""}}, "tf.keras.layers.Cropping1D": {"description": "Cropping layer for 1D input (e.g. temporal sequence).", "Args": {"cropping": "Int or tuple of int (length 2)\nHow many units should be trimmed off at the beginning and end of\nthe cropping dimension (axis 1).\nIf a single int is provided, the same value will be used for both."}}, "tf.keras.layers.Cropping2D": {"description": "Cropping layer for 2D input (e.g. picture).", "Args": {"cropping": "Int, or tuple of 2 ints, or tuple of 2 tuples of 2 ints.\n\nIf int: the same symmetric cropping\nis applied to height and width.\nIf tuple of 2 ints:\ninterpreted as two different\nsymmetric cropping values for height and width:\n(symmetric_height_crop, symmetric_width_crop).\nIf tuple of 2 tuples of 2 ints:\ninterpreted as\n((top_crop, bottom_crop), (left_crop, right_crop))", "data_format": "A string,\none of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs.\nchannels_last corresponds to inputs with shape\n(batch_size, height, width, channels) while channels_first\ncorresponds to inputs with shape\n(batch_size, channels, height, width).\nIt defaults to the image_data_format value found in your\nKeras config file at ~/.keras/keras.json.\nIf you never set it, then it will be \"channels_last\"."}}, "tf.keras.layers.Cropping3D": {"description": "Cropping layer for 3D data (e.g. spatial or spatio-temporal).", "Args": {"cropping": "Int, or tuple of 3 ints, or tuple of 3 tuples of 2 ints.\n\nIf int: the same symmetric cropping\nis applied to depth, height, and width.\nIf tuple of 3 ints: interpreted as two different\nsymmetric cropping values for depth, height, and width:\n(symmetric_dim1_crop, symmetric_dim2_crop, symmetric_dim3_crop).\nIf tuple of 3 tuples of 2 ints: interpreted as\n((left_dim1_crop, right_dim1_crop), (left_dim2_crop,\nright_dim2_crop), (left_dim3_crop, right_dim3_crop))", "data_format": "A string,\none of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs.\nchannels_last corresponds to inputs with shape\n(batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)\nwhile channels_first corresponds to inputs with shape\n(batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3).\nIt defaults to the image_data_format value found in your\nKeras config file at ~/.keras/keras.json.\nIf you never set it, then it will be \"channels_last\"."}}, "tf.keras.layers.Dense": {"description": "Just your regular densely-connected NN layer.", "Args": {"units": "Positive integer, dimensionality of the output space.", "activation": "Activation function to use.\nIf you don't specify anything, no activation is applied\n(ie. \"linear\" activation: a(x) = x).", "use_bias": "Boolean, whether the layer uses a bias vector.", "kernel_initializer": "Initializer for the kernel weights matrix.", "bias_initializer": "Initializer for the bias vector.", "kernel_regularizer": "Regularizer function applied to\nthe kernel weights matrix.", "bias_regularizer": "Regularizer function applied to the bias vector.", "activity_regularizer": "Regularizer function applied to\nthe output of the layer (its \"activation\").", "kernel_constraint": "Constraint function applied to\nthe kernel weights matrix.", "bias_constraint": "Constraint function applied to the bias vector."}}, "tf.keras.layers.DenseFeatures": {"description": "A layer that produces a dense Tensor based on given feature_columns.", "Args": {"feature_columns": "An iterable containing the FeatureColumns to use as\ninputs to your model. All items should be instances of classes derived\nfrom DenseColumn such as numeric_column, embedding_column,\nbucketized_column, indicator_column. If you have categorical\nfeatures, you can wrap them with an embedding_column or\nindicator_column.", "trainable": "Boolean, whether the layer's variables will be updated via\ngradient descent during training.", "name": "Name to give to the DenseFeatures.", "**kwargs": "Keyword arguments to construct a layer."}, "Raises": {"ValueError": "if an item in feature_columns is not a DenseColumn."}}, "tf.keras.layers.DepthwiseConv1D": {"description": "Depthwise 1D convolution.", "Args": {"kernel_size": "An integer, specifying the height and width of the 1D\nconvolution window. Can be a single integer to specify the same value for\nall spatial dimensions.", "strides": "An integer, specifying the strides of the convolution along the\nheight and width. Can be a single integer to specify the same value for\nall spatial dimensions. Specifying any stride value != 1 is incompatible\nwith specifying any dilation_rate value != 1.", "padding": "one of 'valid' or 'same' (case-insensitive). \"valid\" means no\npadding. \"same\" results in padding with zeros evenly to the left/right\nor up/down of the input such that output has the same height/width\ndimension as the input.", "depth_multiplier": "The number of depthwise convolution output channels for\neach input channel. The total number of depthwise convolution output\nchannels will be equal to filters_in * depth_multiplier.", "data_format": "A string, one of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs. channels_last corresponds\nto inputs with shape (batch_size, height, width, channels) while\nchannels_first corresponds to inputs with shape (batch_size, channels,\nheight, width). It defaults to the image_data_format value found in\nyour Keras config file at ~/.keras/keras.json. If you never set it, then\nit will be 'channels_last'.", "dilation_rate": "A single integer, specifying the dilation rate to use for\ndilated convolution. Currently, specifying any dilation_rate value != 1\nis incompatible with specifying any stride value != 1.", "activation": "Activation function to use. If you don't specify anything, no\nactivation is applied (see keras.activations).", "use_bias": "Boolean, whether the layer uses a bias vector.", "depthwise_initializer": "Initializer for the depthwise kernel matrix (see\nkeras.initializers). If None, the default initializer\n('glorot_uniform') will be used.", "bias_initializer": "Initializer for the bias vector (see\nkeras.initializers). If None, the default initializer ('zeros') will be\nused.", "depthwise_regularizer": "Regularizer function applied to the depthwise kernel\nmatrix (see keras.regularizers).", "bias_regularizer": "Regularizer function applied to the bias vector (see\nkeras.regularizers).", "activity_regularizer": "Regularizer function applied to the output of the\nlayer (its 'activation') (see keras.regularizers).", "depthwise_constraint": "Constraint function applied to the depthwise kernel\nmatrix (see keras.constraints).", "bias_constraint": "Constraint function applied to the bias vector (see\nkeras.constraints)."}, "Returns": "A tensor of rank 4 representing\nactivation(depthwiseconv2d(inputs, kernel) + bias).", "Raises": {"ValueError": "when both strides > 1 and dilation_rate > 1."}}, "tf.keras.layers.DepthwiseConv2D": {"description": "Depthwise 2D convolution.", "Args": {"kernel_size": "An integer or tuple/list of 2 integers, specifying the height\nand width of the 2D convolution window. Can be a single integer to specify\nthe same value for all spatial dimensions.", "strides": "An integer or tuple/list of 2 integers, specifying the strides of\nthe convolution along the height and width. Can be a single integer to\nspecify the same value for all spatial dimensions. Specifying any stride\nvalue != 1 is incompatible with specifying any dilation_rate value != 1.", "padding": "one of 'valid' or 'same' (case-insensitive). \"valid\" means no\npadding. \"same\" results in padding with zeros evenly to the left/right\nor up/down of the input such that output has the same height/width\ndimension as the input.", "depth_multiplier": "The number of depthwise convolution output channels for\neach input channel. The total number of depthwise convolution output\nchannels will be equal to filters_in * depth_multiplier.", "data_format": "A string, one of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs. channels_last corresponds\nto inputs with shape (batch_size, height, width, channels) while\nchannels_first corresponds to inputs with shape (batch_size, channels,\nheight, width). It defaults to the image_data_format value found in\nyour Keras config file at ~/.keras/keras.json. If you never set it, then\nit will be 'channels_last'.", "dilation_rate": "An integer or tuple/list of 2 integers, specifying the\ndilation rate to use for dilated convolution. Currently, specifying any\ndilation_rate value != 1 is incompatible with specifying any strides\nvalue != 1.", "activation": "Activation function to use. If you don't specify anything, no\nactivation is applied (see keras.activations).", "use_bias": "Boolean, whether the layer uses a bias vector.", "depthwise_initializer": "Initializer for the depthwise kernel matrix (see\nkeras.initializers). If None, the default initializer\n('glorot_uniform') will be used.", "bias_initializer": "Initializer for the bias vector (see\nkeras.initializers). If None, the default initializer ('zeros') will be\nused.", "depthwise_regularizer": "Regularizer function applied to the depthwise kernel\nmatrix (see keras.regularizers).", "bias_regularizer": "Regularizer function applied to the bias vector (see\nkeras.regularizers).", "activity_regularizer": "Regularizer function applied to the output of the\nlayer (its 'activation') (see keras.regularizers).", "depthwise_constraint": "Constraint function applied to the depthwise kernel\nmatrix (see keras.constraints).", "bias_constraint": "Constraint function applied to the bias vector (see\nkeras.constraints)."}, "Returns": "A tensor of rank 4 representing\nactivation(depthwiseconv2d(inputs, kernel) + bias).", "Raises": {"ValueError": "when both strides > 1 and dilation_rate > 1."}}, "tf.keras.layers.Discretization": {"description": "A preprocessing layer which buckets continuous features by ranges.", "Attributes": {"is_adapted": "Whether the layer has been fit to data already."}}, "tf.keras.layers.Dot": {"description": "Layer that computes a dot product between samples in two tensors.", "Args": {"axes": "Integer or tuple of integers,\naxis or axes along which to take the dot product. If a tuple, should\nbe two integers corresponding to the desired axis from the first input\nand the desired axis from the second input, respectively. Note that the\nsize of the two selected axes must match.", "normalize": "Whether to L2-normalize samples along the\ndot product axis before taking the dot product.\nIf set to True, then the output of the dot product\nis the cosine proximity between the two samples.", "**kwargs": "Standard layer keyword arguments."}}, "tf.keras.layers.Dropout": {"description": "Applies Dropout to the input.", "Args": {"rate": "Float between 0 and 1. Fraction of the input units to drop.", "noise_shape": "1D integer tensor representing the shape of the\nbinary dropout mask that will be multiplied with the input.\nFor instance, if your inputs have shape\n(batch_size, timesteps, features) and\nyou want the dropout mask to be the same for all timesteps,\nyou can use noise_shape=(batch_size, 1, features).", "seed": "A Python integer to use as random seed."}}, "tf.keras.layers.ELU": {"description": "Exponential Linear Unit.", "Args": {"alpha": "Scale for the negative factor."}}, "tf.keras.layers.EinsumDense": {"description": "A layer that uses tf.einsum as the backing computation.", "Args": {"equation": "An equation describing the einsum to perform. This equation must\nbe a valid einsum string of the form ab,bc->ac, ...ab,bc->...ac, or\nab...,bc->ac... where 'ab', 'bc', and 'ac' can be any valid einsum\naxis expression sequence.", "output_shape": "The expected shape of the output tensor (excluding the batch\ndimension and any dimensions represented by ellipses). You can specify\nNone for any dimension that is unknown or can be inferred from the input\nshape.", "activation": "Activation function to use. If you don't specify anything, no\nactivation is applied (that is, a \"linear\" activation: a(x) = x).", "bias_axes": "A string containing the output dimension(s) to apply a bias to.\nEach character in the bias_axes string should correspond to a\ncharacter in the output portion of the equation string.", "kernel_initializer": "Initializer for the kernel weights matrix.", "bias_initializer": "Initializer for the bias vector.", "kernel_regularizer": "Regularizer function applied to the kernel weights\nmatrix.", "bias_regularizer": "Regularizer function applied to the bias vector.", "activity_regularizer": "Regularizer function applied to the output of the\nlayer (its \"activation\").", "kernel_constraint": "Constraint function applied to the kernel weights\nmatrix.", "bias_constraint": "Constraint function applied to the bias vector."}}, "tf.keras.layers.Embedding": {"description": "Turns positive integers (indexes) into dense vectors of fixed size.", "Args": {"input_dim": "Integer. Size of the vocabulary,\ni.e. maximum integer index + 1.", "output_dim": "Integer. Dimension of the dense embedding.", "embeddings_initializer": "Initializer for the embeddings\nmatrix (see keras.initializers).", "embeddings_regularizer": "Regularizer function applied to\nthe embeddings matrix (see keras.regularizers).", "embeddings_constraint": "Constraint function applied to\nthe embeddings matrix (see keras.constraints).", "mask_zero": "Boolean, whether or not the input value 0 is a special \"padding\"\nvalue that should be masked out.\nThis is useful when using recurrent layers\nwhich may take variable length input.\nIf this is True, then all subsequent layers\nin the model need to support masking or an exception will be raised.\nIf mask_zero is set to True, as a consequence, index 0 cannot be\nused in the vocabulary (input_dim should equal size of\nvocabulary + 1).", "input_length": "Length of input sequences, when it is constant.\nThis argument is required if you are going to connect\nFlatten then Dense layers upstream\n(without it, the shape of the dense outputs cannot be computed)."}}, "tf.keras.layers.Flatten": {"description": "Flattens the input. Does not affect the batch size.", "Args": {"data_format": "A string,\none of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs.\nchannels_last corresponds to inputs with shape\n(batch, ..., channels) while channels_first corresponds to\ninputs with shape (batch, channels, ...).\nIt defaults to the image_data_format value found in your\nKeras config file at ~/.keras/keras.json.\nIf you never set it, then it will be \"channels_last\"."}}, "tf.keras.layers.GRU": {"description": "Gated Recurrent Unit - Cho et al. 2014.", "Args": {"units": "Positive integer, dimensionality of the output space.", "activation": "Activation function to use.\nDefault: hyperbolic tangent (tanh).\nIf you pass None, no activation is applied\n(ie. \"linear\" activation: a(x) = x).", "recurrent_activation": "Activation function to use\nfor the recurrent step.\nDefault: sigmoid (sigmoid).\nIf you pass None, no activation is applied\n(ie. \"linear\" activation: a(x) = x).", "use_bias": "Boolean, (default True), whether the layer uses a bias vector.", "kernel_initializer": "Initializer for the kernel weights matrix,\nused for the linear transformation of the inputs. Default:\nglorot_uniform.", "recurrent_initializer": "Initializer for the recurrent_kernel\nweights matrix, used for the linear transformation of the recurrent\nstate. Default: orthogonal.", "bias_initializer": "Initializer for the bias vector. Default: zeros.", "kernel_regularizer": "Regularizer function applied to the kernel weights\nmatrix. Default: None.", "recurrent_regularizer": "Regularizer function applied to the\nrecurrent_kernel weights matrix. Default: None.", "bias_regularizer": "Regularizer function applied to the bias vector. Default:\nNone.", "activity_regularizer": "Regularizer function applied to the output of the\nlayer (its \"activation\"). Default: None.", "kernel_constraint": "Constraint function applied to the kernel weights\nmatrix. Default: None.", "recurrent_constraint": "Constraint function applied to the recurrent_kernel\nweights matrix. Default: None.", "bias_constraint": "Constraint function applied to the bias vector. Default:\nNone.", "dropout": "Float between 0 and 1. Fraction of the units to drop for the linear\ntransformation of the inputs. Default: 0.", "recurrent_dropout": "Float between 0 and 1. Fraction of the units to drop for\nthe linear transformation of the recurrent state. Default: 0.", "return_sequences": "Boolean. Whether to return the last output\nin the output sequence, or the full sequence. Default: False.", "return_state": "Boolean. Whether to return the last state in addition to the\noutput. Default: False.", "go_backwards": "Boolean (default False).\nIf True, process the input sequence backwards and return the\nreversed sequence.", "stateful": "Boolean (default False). If True, the last state\nfor each sample at index i in a batch will be used as initial\nstate for the sample of index i in the following batch.", "unroll": "Boolean (default False).\nIf True, the network will be unrolled,\nelse a symbolic loop will be used.\nUnrolling can speed-up a RNN,\nalthough it tends to be more memory-intensive.\nUnrolling is only suitable for short sequences.", "time_major": "The shape format of the inputs and outputs tensors.\nIf True, the inputs and outputs will be in shape\n[timesteps, batch, feature], whereas in the False case, it will be\n[batch, timesteps, feature]. Using time_major = True is a bit more\nefficient because it avoids transposes at the beginning and end of the\nRNN calculation. However, most TensorFlow data is batch-major, so by\ndefault this function accepts input and emits output in batch-major\nform.", "reset_after": "GRU convention (whether to apply reset gate after or\nbefore matrix multiplication). False = \"before\",\nTrue = \"after\" (default and cuDNN compatible)."}, "Attributes": {"activation": "", "bias_constraint": "", "bias_initializer": "", "bias_regularizer": "", "dropout": "", "implementation": "", "kernel_constraint": "", "kernel_initializer": "", "kernel_regularizer": "", "recurrent_activation": "", "recurrent_constraint": "", "recurrent_dropout": "", "recurrent_initializer": "", "recurrent_regularizer": "", "reset_after": "", "states": "", "units": "", "use_bias": ""}}, "tf.keras.layers.GRUCell": {"description": "Cell class for the GRU layer.", "Args": {"units": "Positive integer, dimensionality of the output space.", "activation": "Activation function to use. Default: hyperbolic tangent\n(tanh). If you pass None, no activation is applied\n(ie. \"linear\" activation: a(x) = x).", "recurrent_activation": "Activation function to use for the recurrent step.\nDefault: sigmoid (sigmoid). If you pass None, no activation is\napplied (ie. \"linear\" activation: a(x) = x).", "use_bias": "Boolean, (default True), whether the layer uses a bias vector.", "kernel_initializer": "Initializer for the kernel weights matrix,\nused for the linear transformation of the inputs. Default:\nglorot_uniform.", "recurrent_initializer": "Initializer for the recurrent_kernel\nweights matrix, used for the linear transformation of the recurrent state.\nDefault: orthogonal.", "bias_initializer": "Initializer for the bias vector. Default: zeros.", "kernel_regularizer": "Regularizer function applied to the kernel weights\nmatrix. Default: None.", "recurrent_regularizer": "Regularizer function applied to the\nrecurrent_kernel weights matrix. Default: None.", "bias_regularizer": "Regularizer function applied to the bias vector. Default:\nNone.", "kernel_constraint": "Constraint function applied to the kernel weights\nmatrix. Default: None.", "recurrent_constraint": "Constraint function applied to the recurrent_kernel\nweights matrix. Default: None.", "bias_constraint": "Constraint function applied to the bias vector. Default:\nNone.", "dropout": "Float between 0 and 1. Fraction of the units to drop for the\nlinear transformation of the inputs. Default: 0.", "recurrent_dropout": "Float between 0 and 1. Fraction of the units to drop for\nthe linear transformation of the recurrent state. Default: 0.", "reset_after": "GRU convention (whether to apply reset gate after or\nbefore matrix multiplication). False = \"before\",\nTrue = \"after\" (default and cuDNN compatible)."}}, "tf.keras.layers.GaussianDropout": {"description": "Apply multiplicative 1-centered Gaussian noise.", "Args": {"rate": "Float, drop probability (as with Dropout).\nThe multiplicative noise will have\nstandard deviation sqrt(rate / (1 - rate)).", "seed": "Integer, optional random seed to enable deterministic behavior."}}, "tf.keras.layers.GaussianNoise": {"description": "Apply additive zero-centered Gaussian noise.", "Args": {"stddev": "Float, standard deviation of the noise distribution.", "seed": "Integer, optional random seed to enable deterministic behavior."}}, "tf.keras.layers.GlobalAveragePooling1D": {"description": "Global average pooling operation for temporal data.", "Args": {"data_format": "A string,\none of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs.\nchannels_last corresponds to inputs with shape\n(batch, steps, features) while channels_first\ncorresponds to inputs with shape\n(batch, features, steps).", "keepdims": "A boolean, whether to keep the temporal dimension or not.\nIf keepdims is False (default), the rank of the tensor is reduced\nfor spatial dimensions.\nIf keepdims is True, the temporal dimension are retained with\nlength 1.\nThe behavior is the same as for tf.reduce_mean or np.mean."}}, "tf.keras.layers.GlobalAveragePooling2D": {"description": "Global average pooling operation for spatial data.", "Args": {"data_format": "A string,\none of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs.\nchannels_last corresponds to inputs with shape\n(batch, height, width, channels) while channels_first\ncorresponds to inputs with shape\n(batch, channels, height, width).\nIt defaults to the image_data_format value found in your\nKeras config file at ~/.keras/keras.json.\nIf you never set it, then it will be \"channels_last\".", "keepdims": "A boolean, whether to keep the spatial dimensions or not.\nIf keepdims is False (default), the rank of the tensor is reduced\nfor spatial dimensions.\nIf keepdims is True, the spatial dimensions are retained with\nlength 1.\nThe behavior is the same as for tf.reduce_mean or np.mean."}}, "tf.keras.layers.GlobalAveragePooling3D": {"description": "Global Average pooling operation for 3D data.", "Args": {"data_format": "A string,\none of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs.\nchannels_last corresponds to inputs with shape\n(batch, spatial_dim1, spatial_dim2, spatial_dim3, channels)\nwhile channels_first corresponds to inputs with shape\n(batch, channels, spatial_dim1, spatial_dim2, spatial_dim3).\nIt defaults to the image_data_format value found in your\nKeras config file at ~/.keras/keras.json.\nIf you never set it, then it will be \"channels_last\".", "keepdims": "A boolean, whether to keep the spatial dimensions or not.\nIf keepdims is False (default), the rank of the tensor is reduced\nfor spatial dimensions.\nIf keepdims is True, the spatial dimensions are retained with\nlength 1.\nThe behavior is the same as for tf.reduce_mean or np.mean."}}, "tf.keras.layers.GlobalMaxPool1D": {"description": "Global max pooling operation for 1D temporal data.", "Args": {"data_format": "A string,\none of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs.\nchannels_last corresponds to inputs with shape\n(batch, steps, features) while channels_first\ncorresponds to inputs with shape\n(batch, features, steps).", "keepdims": "A boolean, whether to keep the temporal dimension or not.\nIf keepdims is False (default), the rank of the tensor is reduced\nfor spatial dimensions.\nIf keepdims is True, the temporal dimension are retained with\nlength 1.\nThe behavior is the same as for tf.reduce_max or np.max."}}, "tf.keras.layers.GlobalMaxPool2D": {"description": "Global max pooling operation for spatial data.", "Args": {"data_format": "A string,\none of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs.\nchannels_last corresponds to inputs with shape\n(batch, height, width, channels) while channels_first\ncorresponds to inputs with shape\n(batch, channels, height, width).\nIt defaults to the image_data_format value found in your\nKeras config file at ~/.keras/keras.json.\nIf you never set it, then it will be \"channels_last\".", "keepdims": "A boolean, whether to keep the spatial dimensions or not.\nIf keepdims is False (default), the rank of the tensor is reduced\nfor spatial dimensions.\nIf keepdims is True, the spatial dimensions are retained with\nlength 1.\nThe behavior is the same as for tf.reduce_max or np.max."}}, "tf.keras.layers.GlobalMaxPool3D": {}, "tf.keras.layers.Hashing": {"description": "A preprocessing layer which hashes and bins categorical features.", "Args": {"num_bins": "Number of hash bins. Note that this includes the mask_value bin,\nso the effective number of bins is (num_bins - 1) if mask_value is\nset.", "mask_value": "A value that represents masked inputs, which are mapped to\nindex 0. Defaults to None, meaning no mask term will be added and the\nhashing will start at index 0.", "salt": "A single unsigned integer or None.\nIf passed, the hash function used will be SipHash64, with these values\nused as an additional input (known as a \"salt\" in cryptography).\nThese should be non-zero. Defaults to None (in that\ncase, the FarmHash64 hash function is used). It also supports\ntuple/list of 2 unsigned integer numbers, see reference paper for details.", "output_mode": "Specification for the output of the layer. Defaults to \"int\".\nValues can be \"int\", \"one_hot\", \"multi_hot\", or \"count\"\nconfiguring the layer as follows:\n\n\"int\": Return the integer bin indices directly.\n\"one_hot\": Encodes each individual element in the input into an\narray the same size as num_bins, containing a 1 at the input's bin\nindex. If the last dimension is size 1, will encode on that dimension.\nIf the last dimension is not size 1, will append a new dimension for\nthe encoded output.\n\"multi_hot\": Encodes each sample in the input into a single array\nthe same size as num_bins, containing a 1 for each bin index\nindex present in the sample. Treats the last dimension as the sample\ndimension, if input shape is (..., sample_length), output shape will\nbe (..., num_tokens).\n\"count\": As \"multi_hot\", but the int array contains a count of the\nnumber of times the bin index appeared in the sample.", "sparse": "Boolean. Only applicable to \"one_hot\", \"multi_hot\",\nand \"count\" output modes. If True, returns a SparseTensor instead of\na dense Tensor. Defaults to False.", "**kwargs": "Keyword arguments to construct a layer."}}, "tf.keras.layers.InputLayer": {"description": "Layer to be used as an entry point into a Network (a graph of layers).", "Args": {"input_shape": "Shape tuple (not including the batch axis), or TensorShape\ninstance (not including the batch axis).", "batch_size": "Optional input batch size (integer or None).", "dtype": "Optional datatype of the input. When not provided, the Keras\ndefault float type will be used.", "input_tensor": "Optional tensor to use as layer input. If set, the layer\nwill use the tf.TypeSpec of this tensor rather\nthan creating a new placeholder tensor.", "sparse": "Boolean, whether the placeholder created is meant to be sparse.\nDefault to False.", "ragged": "Boolean, whether the placeholder created is meant to be ragged.\nIn this case, values of None in the shape argument represent\nragged dimensions. For more information about tf.RaggedTensor, see\nthis guide.\nDefault to False.", "type_spec": "A tf.TypeSpec object to create Input from. This tf.TypeSpec\nrepresents the entire batch. When provided, all other args except\nname must be None.", "name": "Optional name of the layer (string)."}}, "tf.keras.layers.InputSpec": {"description": "Specifies the rank, dtype and shape of every input to a layer.", "Args": {"dtype": "Expected DataType of the input.", "shape": "Shape tuple, expected shape of the input\n(may include None for unchecked axes). Includes the batch size.", "ndim": "Integer, expected rank of the input.", "max_ndim": "Integer, maximum rank of the input.", "min_ndim": "Integer, minimum rank of the input.", "axes": "Dictionary mapping integer axes to\na specific dimension value.", "allow_last_axis_squeeze": "If True, then allow inputs of rank N+1 as long\nas the last axis of the input is 1, as well as inputs of rank N-1\nas long as the last axis of the spec is 1.", "name": "Expected key corresponding to this input when passing data as\na dictionary."}}, "tf.keras.layers.IntegerLookup": {"description": "A preprocessing layer which maps integer features to contiguous ranges.", "Args": {"max_tokens": "Maximum size of the vocabulary for this layer. This should only\nbe specified when adapting the vocabulary or when setting\npad_to_max_tokens=True. If None, there is no cap on the size of the\nvocabulary. Note that this size includes the OOV and mask tokens. Defaults\nto None.", "num_oov_indices": "The number of out-of-vocabulary tokens to use. If this\nvalue is more than 1, OOV inputs are modulated to determine their OOV\nvalue. If this value is 0, OOV inputs will cause an error when calling the\nlayer. Defaults to 1.", "mask_token": "An integer token that represents masked inputs. When\noutput_mode is \"int\", the token is included in vocabulary and mapped\nto index 0. In other output modes, the token will not appear in the\nvocabulary and instances of the mask token in the input will be dropped.\nIf set to None, no mask term will be added. Defaults to None.", "oov_token": "Only used when invert is True. The token to return for OOV\nindices. Defaults to -1.", "vocabulary": "Optional. Either an array of integers or a string path to a text\nfile. If passing an array, can pass a tuple, list, 1D numpy array, or 1D\ntensor containing the integer vocbulary terms. If passing a file path, the\nfile should contain one line per term in the vocabulary. If this argument\nis set, there is no need to adapt() the layer.", "vocabulary_dtype": "The dtype of the vocabulary terms, for example\n\"int64\" or \"int32\". Defaults to \"int64\".", "idf_weights": "Only valid when output_mode is \"tf_idf\". A tuple, list, 1D\nnumpy array, or 1D tensor or the same length as the vocabulary, containing\nthe floating point inverse document frequency weights, which will be\nmultiplied by per sample term counts for the final tf_idf weight. If the\nvocabulary argument is set, and output_mode is \"tf_idf\", this\nargument must be supplied.", "invert": "Only valid when output_mode is \"int\". If True, this layer will\nmap indices to vocabulary items instead of mapping vocabulary items to\nindices. Default to False.", "output_mode": "Specification for the output of the layer. Defaults to \"int\".\nValues can be \"int\", \"one_hot\", \"multi_hot\", \"count\", or\n\"tf_idf\" configuring the layer as follows:\n\n\"int\": Return the vocabulary indices of the input tokens.\n\"one_hot\": Encodes each individual element in the input into an\narray the same size as the vocabulary, containing a 1 at the element\nindex. If the last dimension is size 1, will encode on that dimension.\nIf the last dimension is not size 1, will append a new dimension for\nthe encoded output.\n\"multi_hot\": Encodes each sample in the input into a single array\nthe same size as the vocabulary, containing a 1 for each vocabulary\nterm present in the sample. Treats the last dimension as the sample\ndimension, if input shape is (..., sample_length), output shape will\nbe (..., num_tokens).\n\"count\": As \"multi_hot\", but the int array contains a count of the\nnumber of times the token at that index appeared in the sample.\n\"tf_idf\": As \"multi_hot\", but the TF-IDF algorithm is applied to\nfind the value in each token slot.\nFor \"int\" output, any shape of input and output is supported. For all\nother output modes, currently only output up to rank 2 is supported.", "pad_to_max_tokens": "Only applicable when output_mode is \"multi_hot\",\n\"count\", or \"tf_idf\". If True, the output will have its feature axis\npadded to max_tokens even if the number of unique tokens in the\nvocabulary is less than max_tokens, resulting in a tensor of shape\n[batch_size, max_tokens] regardless of vocabulary size. Defaults to False.", "sparse": "Boolean. Only applicable when output_mode is \"multi_hot\",\n\"count\", or \"tf_idf\". If True, returns a SparseTensor instead of a\ndense Tensor. Defaults to False."}, "Attributes": {"is_adapted": "Whether the layer has been fit to data already."}}, "tf.keras.layers.LSTM": {"description": "Long Short-Term Memory layer - Hochreiter 1997.", "Args": {"units": "Positive integer, dimensionality of the output space.", "activation": "Activation function to use.\nDefault: hyperbolic tangent (tanh). If you pass None, no activation\nis applied (ie. \"linear\" activation: a(x) = x).", "recurrent_activation": "Activation function to use for the recurrent step.\nDefault: sigmoid (sigmoid). If you pass None, no activation is\napplied (ie. \"linear\" activation: a(x) = x).", "use_bias": "Boolean (default True), whether the layer uses a bias vector.", "kernel_initializer": "Initializer for the kernel weights matrix, used for\nthe linear transformation of the inputs. Default: glorot_uniform.", "recurrent_initializer": "Initializer for the recurrent_kernel weights\nmatrix, used for the linear transformation of the recurrent state.\nDefault: orthogonal.", "bias_initializer": "Initializer for the bias vector. Default: zeros.", "unit_forget_bias": "Boolean (default True). If True, add 1 to the bias of\nthe forget gate at initialization. Setting it to true will also force\nbias_initializer=\"zeros\". This is recommended in Jozefowicz et\n    al..", "kernel_regularizer": "Regularizer function applied to the kernel weights\nmatrix. Default: None.", "recurrent_regularizer": "Regularizer function applied to the\nrecurrent_kernel weights matrix. Default: None.", "bias_regularizer": "Regularizer function applied to the bias vector. Default:\nNone.", "activity_regularizer": "Regularizer function applied to the output of the\nlayer (its \"activation\"). Default: None.", "kernel_constraint": "Constraint function applied to the kernel weights\nmatrix. Default: None.", "recurrent_constraint": "Constraint function applied to the recurrent_kernel\nweights matrix. Default: None.", "bias_constraint": "Constraint function applied to the bias vector. Default:\nNone.", "dropout": "Float between 0 and 1. Fraction of the units to drop for the linear\ntransformation of the inputs. Default: 0.", "recurrent_dropout": "Float between 0 and 1. Fraction of the units to drop for\nthe linear transformation of the recurrent state. Default: 0.", "return_sequences": "Boolean. Whether to return the last output. in the output\nsequence, or the full sequence. Default: False.", "return_state": "Boolean. Whether to return the last state in addition to the\noutput. Default: False.", "go_backwards": "Boolean (default False). If True, process the input sequence\nbackwards and return the reversed sequence.", "stateful": "Boolean (default False). If True, the last state for each sample\nat index i in a batch will be used as initial state for the sample of\nindex i in the following batch.", "time_major": "The shape format of the inputs and outputs tensors.\nIf True, the inputs and outputs will be in shape\n[timesteps, batch, feature], whereas in the False case, it will be\n[batch, timesteps, feature]. Using time_major = True is a bit more\nefficient because it avoids transposes at the beginning and end of the\nRNN calculation. However, most TensorFlow data is batch-major, so by\ndefault this function accepts input and emits output in batch-major\nform.", "unroll": "Boolean (default False). If True, the network will be unrolled,\nelse a symbolic loop will be used. Unrolling can speed-up a RNN, although\nit tends to be more memory-intensive. Unrolling is only suitable for short\nsequences."}, "Attributes": {"activation": "", "bias_constraint": "", "bias_initializer": "", "bias_regularizer": "", "dropout": "", "implementation": "", "kernel_constraint": "", "kernel_initializer": "", "kernel_regularizer": "", "recurrent_activation": "", "recurrent_constraint": "", "recurrent_dropout": "", "recurrent_initializer": "", "recurrent_regularizer": "", "states": "", "unit_forget_bias": "", "units": "", "use_bias": ""}}, "tf.keras.layers.LSTMCell": {"description": "Cell class for the LSTM layer.", "Args": {"units": "Positive integer, dimensionality of the output space.", "activation": "Activation function to use. Default: hyperbolic tangent\n(tanh). If you pass None, no activation is applied (ie. \"linear\"\nactivation: a(x) = x).", "recurrent_activation": "Activation function to use for the recurrent step.\nDefault: sigmoid (sigmoid). If you pass None, no activation is applied\n(ie. \"linear\" activation: a(x) = x).", "use_bias": "Boolean, (default True), whether the layer uses a bias vector.", "kernel_initializer": "Initializer for the kernel weights matrix, used for\nthe linear transformation of the inputs. Default: glorot_uniform.", "recurrent_initializer": "Initializer for the recurrent_kernel weights\nmatrix, used for the linear transformation of the recurrent state.\nDefault: orthogonal.", "bias_initializer": "Initializer for the bias vector. Default: zeros.", "unit_forget_bias": "Boolean (default True). If True, add 1 to the bias of\nthe forget gate at initialization. Setting it to true will also force\nbias_initializer=\"zeros\". This is recommended in Jozefowicz et\n  al.", "kernel_regularizer": "Regularizer function applied to the kernel weights\nmatrix. Default: None.", "recurrent_regularizer": "Regularizer function applied to\nthe recurrent_kernel weights matrix. Default: None.", "bias_regularizer": "Regularizer function applied to the bias vector. Default:\nNone.", "kernel_constraint": "Constraint function applied to the kernel weights\nmatrix. Default: None.", "recurrent_constraint": "Constraint function applied to the recurrent_kernel\nweights matrix. Default: None.", "bias_constraint": "Constraint function applied to the bias vector. Default:\nNone.", "dropout": "Float between 0 and 1. Fraction of the units to drop for the linear\ntransformation of the inputs. Default: 0.", "recurrent_dropout": "Float between 0 and 1. Fraction of the units to drop for\nthe linear transformation of the recurrent state. Default: 0."}}, "tf.keras.layers.Lambda": {"description": "Wraps arbitrary expressions as a Layer object.", "Args": {"function": "The function to be evaluated. Takes input tensor as first\nargument.", "output_shape": "Expected output shape from function. This argument can be\ninferred if not explicitly provided. Can be a tuple or function. If a\ntuple, it only specifies the first dimension onward;\nsample dimension is assumed either the same as the input: output_shape =\n  (input_shape[0], ) + output_shape or, the input is None and\nthe sample dimension is also None: output_shape = (None, ) +\n  output_shape If a function, it specifies the entire shape as a function\n  of the\ninput shape: output_shape = f(input_shape)", "mask": "Either None (indicating no masking) or a callable with the same\nsignature as the compute_mask layer method, or a tensor that will be\nreturned as output mask regardless of what the input is.", "arguments": "Optional dictionary of keyword arguments to be passed to the\nfunction."}}, "tf.keras.layers.Layer": {"description": "This is the class from which all layers inherit.", "Args": {"trainable": "Boolean, whether the layer's variables should be trainable.", "name": "String name of the layer.", "dtype": "The dtype of the layer's computations and weights. Can also be a\ntf.keras.mixed_precision.Policy, which allows the computation and weight\ndtype to differ. Default of None means to use\ntf.keras.mixed_precision.global_policy(), which is a float32 policy\nunless set to different value.", "dynamic": "Set this to True if your layer should only be run eagerly, and\nshould not be used to generate a static computation graph.\nThis would be the case for a Tree-RNN or a recursive network,\nfor example, or generally for any layer that manipulates tensors\nusing Python control flow. If False, we assume that the layer can\nsafely be used to generate a static computation graph."}, "Attributes": {"name": "The name of the layer (string).", "dtype": "The dtype of the layer's weights.", "variable_dtype": "Alias of dtype.", "compute_dtype": "The dtype of the layer's computations. Layers automatically\ncast inputs to this dtype which causes the computations and output to also\nbe in this dtype. When mixed precision is used with a\ntf.keras.mixed_precision.Policy, this will be different than\nvariable_dtype.", "dtype_policy": "The layer's dtype policy. See the\ntf.keras.mixed_precision.Policy documentation for details.", "trainable_weights": "List of variables to be included in backprop.", "non_trainable_weights": "List of variables that should not be\nincluded in backprop.", "weights": "The concatenation of the lists trainable_weights and\nnon_trainable_weights (in this order).", "trainable": "Whether the layer should be trained (boolean), i.e. whether\nits potentially-trainable weights should be returned as part of\nlayer.trainable_weights.", "input_spec": "Optional (list of) InputSpec object(s) specifying the\nconstraints on inputs that can be accepted by the layer.", "activity_regularizer": "Optional regularizer function for the output of this layer.", "dynamic": "Whether the layer is dynamic (eager-only); set in the constructor.", "input": "Retrieves the input tensor(s) of a layer.\nOnly applicable if the layer has exactly one input,\ni.e. if it is connected to one incoming layer.", "losses": "List of losses added using the add_loss() API.\nVariable regularization tensors are created when this property is accessed,\nso it is eager safe: accessing losses under a tf.GradientTape will\npropagate gradients back to the corresponding variables.\nclass MyLayer(tf.keras.layers.Layer):\u00a0 def call(self, inputs):\u00a0 \u00a0 self.add_loss(tf.abs(tf.reduce_mean(inputs)))\u00a0 \u00a0 return inputsl = MyLayer()l(np.ones((10, 1)))l.losses[1.0]\ninputs = tf.keras.Input(shape=(10,))x = tf.keras.layers.Dense(10)(inputs)outputs = tf.keras.layers.Dense(1)(x)model = tf.keras.Model(inputs, outputs)# Activity regularization.len(model.losses)0model.add_loss(tf.abs(tf.reduce_mean(x)))len(model.losses)1\ninputs = tf.keras.Input(shape=(10,))d = tf.keras.layers.Dense(10, kernel_initializer='ones')x = d(inputs)outputs = tf.keras.layers.Dense(1)(x)model = tf.keras.Model(inputs, outputs)# Weight regularization.model.add_loss(lambda: tf.reduce_mean(d.kernel))model.losses[<tf.Tensor: shape=(), dtype=float32, numpy=1.0>]", "metrics": "List of metrics added using the add_metric() API.\ninput = tf.keras.layers.Input(shape=(3,))d = tf.keras.layers.Dense(2)output = d(input)d.add_metric(tf.reduce_max(output), name='max')d.add_metric(tf.reduce_min(output), name='min')[m.name for m in d.metrics]['max', 'min']", "output": "Retrieves the output tensor(s) of a layer.\nOnly applicable if the layer has exactly one output,\ni.e. if it is connected to one incoming layer.", "supports_masking": "Whether this layer supports computing a mask using compute_mask."}}, "tf.keras.layers.LayerNormalization": {"description": "Layer normalization layer (Ba et al., 2016).", "Args": {"axis": "Integer or List/Tuple. The axis or axes to normalize across. Typically\nthis is the features axis/axes. The left-out axes are typically the batch\naxis/axes. This argument defaults to -1, the last dimension in the\ninput.", "epsilon": "Small float added to variance to avoid dividing by zero. Defaults\nto 1e-3", "center": "If True, add offset of beta to normalized tensor. If False, beta\nis ignored. Defaults to True.", "scale": "If True, multiply by gamma. If False, gamma is not used. Defaults\nto True. When the next layer is linear (also e.g. nn.relu), this can be\ndisabled since the scaling will be done by the next layer.", "beta_initializer": "Initializer for the beta weight. Defaults to zeros.", "gamma_initializer": "Initializer for the gamma weight. Defaults to ones.", "beta_regularizer": "Optional regularizer for the beta weight. None by default.", "gamma_regularizer": "Optional regularizer for the gamma weight. None by\ndefault.", "beta_constraint": "Optional constraint for the beta weight. None by default.", "gamma_constraint": "Optional constraint for the gamma weight. None by default."}}, "tf.keras.layers.LeakyReLU": {"description": "Leaky version of a Rectified Linear Unit.", "Args": {"alpha": "Float >= 0. Negative slope coefficient. Default to 0.3."}}, "tf.keras.layers.LocallyConnected1D": {"description": "Locally-connected layer for 1D inputs.", "Args": {"filters": "Integer, the dimensionality of the output space (i.e. the number\nof output filters in the convolution).", "kernel_size": "An integer or tuple/list of a single integer, specifying the\nlength of the 1D convolution window.", "strides": "An integer or tuple/list of a single integer, specifying the\nstride length of the convolution.", "padding": "Currently only supports \"valid\" (case-insensitive). \"same\"\nmay be supported in the future. \"valid\" means no padding.", "data_format": "A string, one of channels_last (default) or\nchannels_first. The ordering of the dimensions in the inputs.\nchannels_last corresponds to inputs with shape (batch, length,\nchannels) while channels_first corresponds to inputs with shape\n(batch, channels, length). It defaults to the image_data_format\nvalue found in your Keras config file at ~/.keras/keras.json. If you\nnever set it, then it will be \"channels_last\".", "activation": "Activation function to use. If you don't specify anything, no\nactivation is applied\n  (ie. \"linear\" activation: a(x) = x).", "use_bias": "Boolean, whether the layer uses a bias vector.", "kernel_initializer": "Initializer for the kernel weights matrix.", "bias_initializer": "Initializer for the bias vector.", "kernel_regularizer": "Regularizer function applied to the kernel weights\nmatrix.", "bias_regularizer": "Regularizer function applied to the bias vector.", "activity_regularizer": "Regularizer function applied to the output of the\nlayer (its \"activation\")..", "kernel_constraint": "Constraint function applied to the kernel matrix.", "bias_constraint": "Constraint function applied to the bias vector.", "implementation": "implementation mode, either 1, 2, or 3. 1 loops\nover input spatial locations to perform the forward pass. It is\nmemory-efficient but performs a lot of (small) ops.  2 stores layer\nweights in a dense but sparsely-populated 2D matrix and implements the\nforward pass as a single matrix-multiply. It uses a lot of RAM but\nperforms few (large) ops.  3 stores layer weights in a sparse tensor\nand implements the forward pass as a single sparse matrix-multiply.\n  How to choose:\n  1: large, dense models,\n  2: small models,\n  3: large, sparse models,  where \"large\" stands for large\n    input/output activations (i.e. many filters, input_filters,\n    large input_size, output_size), and \"sparse\" stands for few\n    connections between inputs and outputs, i.e. small ratio filters *\n    input_filters * kernel_size / (input_size * strides), where inputs\n    to and outputs of the layer are assumed to have shapes (input_size,\n    input_filters), (output_size, filters) respectively.  It is\n    recommended to benchmark each in the setting of interest to pick the\n    most efficient one (in terms of speed and memory usage). Correct\n    choice of implementation can lead to dramatic speed improvements\n    (e.g. 50X), potentially at the expense of RAM.  Also, only\n    padding=\"valid\" is supported by implementation=1."}}, "tf.keras.layers.LocallyConnected2D": {"description": "Locally-connected layer for 2D inputs.", "Args": {"filters": "Integer, the dimensionality of the output space (i.e. the number\nof output filters in the convolution).", "kernel_size": "An integer or tuple/list of 2 integers, specifying the width\nand height of the 2D convolution window. Can be a single integer to\nspecify the same value for all spatial dimensions.", "strides": "An integer or tuple/list of 2 integers, specifying the strides of\nthe convolution along the width and height. Can be a single integer to\nspecify the same value for all spatial dimensions.", "padding": "Currently only support \"valid\" (case-insensitive). \"same\"\nwill be supported in future. \"valid\" means no padding.", "data_format": "A string, one of channels_last (default) or\nchannels_first. The ordering of the dimensions in the inputs.\nchannels_last corresponds to inputs with shape (batch, height, width,\nchannels) while channels_first corresponds to inputs with shape\n(batch, channels, height, width). It defaults to the\nimage_data_format value found in your Keras config file at\n~/.keras/keras.json. If you never set it, then it will be\n\"channels_last\".", "activation": "Activation function to use. If you don't specify anything, no\nactivation is applied\n  (ie. \"linear\" activation: a(x) = x).", "use_bias": "Boolean, whether the layer uses a bias vector.", "kernel_initializer": "Initializer for the kernel weights matrix.", "bias_initializer": "Initializer for the bias vector.", "kernel_regularizer": "Regularizer function applied to the kernel weights\nmatrix.", "bias_regularizer": "Regularizer function applied to the bias vector.", "activity_regularizer": "Regularizer function applied to the output of the\nlayer (its \"activation\").", "kernel_constraint": "Constraint function applied to the kernel matrix.", "bias_constraint": "Constraint function applied to the bias vector.", "implementation": "implementation mode, either 1, 2, or 3. 1 loops\nover input spatial locations to perform the forward pass. It is\nmemory-efficient but performs a lot of (small) ops.  2 stores layer\nweights in a dense but sparsely-populated 2D matrix and implements the\nforward pass as a single matrix-multiply. It uses a lot of RAM but\nperforms few (large) ops.  3 stores layer weights in a sparse tensor\nand implements the forward pass as a single sparse matrix-multiply.\n  How to choose:\n  1: large, dense models,\n  2: small models,\n  3: large, sparse models,  where \"large\" stands for large\n    input/output activations (i.e. many filters, input_filters,\n    large np.prod(input_size), np.prod(output_size)), and \"sparse\"\n    stands for few connections between inputs and outputs, i.e. small\n    ratio filters * input_filters * np.prod(kernel_size) /\n    (np.prod(input_size) * np.prod(strides)), where inputs to and\n    outputs of the layer are assumed to have shapes input_size +\n    (input_filters,), output_size + (filters,) respectively.  It is\n    recommended to benchmark each in the setting of interest to pick the\n    most efficient one (in terms of speed and memory usage). Correct\n    choice of implementation can lead to dramatic speed improvements\n    (e.g. 50X), potentially at the expense of RAM.  Also, only\n    padding=\"valid\" is supported by implementation=1."}}, "tf.keras.layers.Masking": {"description": "Masks a sequence by using a mask value to skip timesteps."}, "tf.keras.layers.MaxPool1D": {"description": "Max pooling operation for 1D temporal data.", "Args": {"pool_size": "Integer, size of the max pooling window.", "strides": "Integer, or None. Specifies how much the pooling window moves\nfor each pooling step.\nIf None, it will default to pool_size.", "padding": "One of \"valid\" or \"same\" (case-insensitive).\n\"valid\" means no padding. \"same\" results in padding evenly to\nthe left/right or up/down of the input such that output has the same\nheight/width dimension as the input.", "data_format": "A string,\none of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs.\nchannels_last corresponds to inputs with shape\n(batch, steps, features) while channels_first\ncorresponds to inputs with shape\n(batch, features, steps)."}}, "tf.keras.layers.MaxPool2D": {"description": "Max pooling operation for 2D spatial data.", "Args": {"pool_size": "integer or tuple of 2 integers,\nwindow size over which to take the maximum.\n(2, 2) will take the max value over a 2x2 pooling window.\nIf only one integer is specified, the same window length\nwill be used for both dimensions.", "strides": "Integer, tuple of 2 integers, or None.\nStrides values.  Specifies how far the pooling window moves\nfor each pooling step. If None, it will default to pool_size.", "padding": "One of \"valid\" or \"same\" (case-insensitive).\n\"valid\" means no padding. \"same\" results in padding evenly to\nthe left/right or up/down of the input such that output has the same\nheight/width dimension as the input.", "data_format": "A string,\none of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs.\nchannels_last corresponds to inputs with shape\n(batch, height, width, channels) while channels_first\ncorresponds to inputs with shape\n(batch, channels, height, width).\nIt defaults to the image_data_format value found in your\nKeras config file at ~/.keras/keras.json.\nIf you never set it, then it will be \"channels_last\"."}, "Returns": "A tensor of rank 4 representing the maximum pooled values.  See above for\noutput shape."}, "tf.keras.layers.MaxPool3D": {"description": "Max pooling operation for 3D data (spatial or spatio-temporal).", "Args": {"pool_size": "Tuple of 3 integers,\nfactors by which to downscale (dim1, dim2, dim3).\n(2, 2, 2) will halve the size of the 3D input in each dimension.", "strides": "tuple of 3 integers, or None. Strides values.", "padding": "One of \"valid\" or \"same\" (case-insensitive).\n\"valid\" means no padding. \"same\" results in padding evenly to\nthe left/right or up/down of the input such that output has the same\nheight/width dimension as the input.", "data_format": "A string,\none of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs.\nchannels_last corresponds to inputs with shape\n(batch, spatial_dim1, spatial_dim2, spatial_dim3, channels)\nwhile channels_first corresponds to inputs with shape\n(batch, channels, spatial_dim1, spatial_dim2, spatial_dim3).\nIt defaults to the image_data_format value found in your\nKeras config file at ~/.keras/keras.json.\nIf you never set it, then it will be \"channels_last\"."}}, "tf.keras.layers.Maximum": {"description": "Layer that computes the maximum (element-wise) a list of inputs.", "Args": {"**kwargs": "standard layer keyword arguments."}}, "tf.keras.layers.Minimum": {"description": "Layer that computes the minimum (element-wise) a list of inputs.", "Args": {"**kwargs": "standard layer keyword arguments."}}, "tf.keras.layers.MultiHeadAttention": {"description": "MultiHeadAttention layer.", "Args": {"num_heads": "Number of attention heads.", "key_dim": "Size of each attention head for query and key.", "value_dim": "Size of each attention head for value.", "dropout": "Dropout probability.", "use_bias": "Boolean, whether the dense layers use bias vectors/matrices.", "output_shape": "The expected shape of an output tensor, besides the batch and\nsequence dims. If not specified, projects back to the key feature dim.", "attention_axes": "axes over which the attention is applied. None means\nattention over all axes, but batch, heads, and features.", "kernel_initializer": "Initializer for dense layer kernels.", "bias_initializer": "Initializer for dense layer biases.", "kernel_regularizer": "Regularizer for dense layer kernels.", "bias_regularizer": "Regularizer for dense layer biases.", "activity_regularizer": "Regularizer for dense layer activity.", "kernel_constraint": "Constraint for dense layer kernels.", "bias_constraint": "Constraint for dense layer kernels."}, "Returns": "attention_output\n\n\nThe result of the computation, of shape (B, T, E),\nwhere T is for target sequence shapes and E is the query input last\ndimension if output_shape is None. Otherwise, the multi-head outputs\nare project to the shape specified by output_shape."}, "tf.keras.layers.Multiply": {"description": "Layer that multiplies (element-wise) a list of inputs.", "Args": {"**kwargs": "standard layer keyword arguments."}}, "tf.keras.layers.Normalization": {"description": "A preprocessing layer which normalizes continuous features.", "Args": {"axis": "Integer, tuple of integers, or None. The axis or axes that should\nhave a separate mean and variance for each index in the shape. For\nexample, if shape is (None, 5) and axis=1, the layer will track 5\nseparate mean and variance values for the last axis. If axis is set to\nNone, the layer will normalize all elements in the input by a scalar\nmean and variance. Defaults to -1, where the last axis of the input is\nassumed to be a feature dimension and is normalized per index. Note that\nin the specific case of batched scalar inputs where the only axis is the\nbatch axis, the default will normalize each index in the batch\nseparately. In this case, consider passing axis=None.", "mean": "The mean value(s) to use during normalization. The passed value(s)\nwill be broadcast to the shape of the kept axes above; if the value(s)\ncannot be broadcast, an error will be raised when this layer's build()\nmethod is called.", "variance": "The variance value(s) to use during normalization. The passed\nvalue(s) will be broadcast to the shape of the kept axes above; if the\nvalue(s) cannot be broadcast, an error will be raised when this layer's\nbuild() method is called."}, "Attributes": {"is_adapted": "Whether the layer has been fit to data already."}}, "tf.keras.layers.PReLU": {"description": "Parametric Rectified Linear Unit.", "Args": {"alpha_initializer": "Initializer function for the weights.", "alpha_regularizer": "Regularizer for the weights.", "alpha_constraint": "Constraint for the weights.", "shared_axes": "The axes along which to share learnable\nparameters for the activation function.\nFor example, if the incoming feature maps\nare from a 2D convolution\nwith output shape (batch, height, width, channels),\nand you wish to share parameters across space\nso that each filter only has one set of parameters,\nset shared_axes=[1, 2]."}}, "tf.keras.layers.Permute": {"description": "Permutes the dimensions of the input according to a given pattern.", "Args": {"dims": "Tuple of integers. Permutation pattern does not include the\nsamples dimension. Indexing starts at 1.\nFor instance, (2, 1) permutes the first and second dimensions\nof the input."}}, "tf.keras.layers.RNN": {"description": "Base class for recurrent layers.", "Args": {"cell": "A RNN cell instance or a list of RNN cell instances.\nA RNN cell is a class that has:\n\nA call(input_at_t, states_at_t) method, returning\n(output_at_t, states_at_t_plus_1). The call method of the\ncell can also take the optional argument constants, see\nsection \"Note on passing external constants\" below.\nA state_size attribute. This can be a single integer\n(single state) in which case it is the size of the recurrent\nstate. This can also be a list/tuple of integers (one size per state).\nThe state_size can also be TensorShape or tuple/list of\nTensorShape, to represent high dimension state.\nA output_size attribute. This can be a single integer or a\nTensorShape, which represent the shape of the output. For backward\ncompatible reason, if this attribute is not available for the\ncell, the value will be inferred by the first element of the\nstate_size.\nA get_initial_state(inputs=None, batch_size=None, dtype=None)\nmethod that creates a tensor meant to be fed to call() as the\ninitial state, if the user didn't specify any initial state via other\nmeans. The returned initial state should have a shape of\n[batch_size, cell.state_size]. The cell might choose to create a\ntensor full of zeros, or full of other values based on the cell's\nimplementation.\ninputs is the input tensor to the RNN layer, which should\ncontain the batch size as its shape[0], and also dtype. Note that\nthe shape[0] might be None during the graph construction. Either\nthe inputs or the pair of batch_size and dtype are provided.\nbatch_size is a scalar tensor that represents the batch size\nof the inputs. dtype is tf.DType that represents the dtype of\nthe inputs.\nFor backward compatibility, if this method is not implemented\nby the cell, the RNN layer will create a zero filled tensor with the\nsize of [batch_size, cell.state_size].\nIn the case that cell is a list of RNN cell instances, the cells\nwill be stacked on top of each other in the RNN, resulting in an\nefficient stacked RNN.", "return_sequences": "Boolean (default False). Whether to return the last\noutput in the output sequence, or the full sequence.", "return_state": "Boolean (default False). Whether to return the last state\nin addition to the output.", "go_backwards": "Boolean (default False).\nIf True, process the input sequence backwards and return the\nreversed sequence.", "stateful": "Boolean (default False). If True, the last state\nfor each sample at index i in a batch will be used as initial\nstate for the sample of index i in the following batch.", "unroll": "Boolean (default False).\nIf True, the network will be unrolled, else a symbolic loop will be used.\nUnrolling can speed-up a RNN, although it tends to be more\nmemory-intensive. Unrolling is only suitable for short sequences.", "time_major": "The shape format of the inputs and outputs tensors.\nIf True, the inputs and outputs will be in shape\n(timesteps, batch, ...), whereas in the False case, it will be\n(batch, timesteps, ...). Using time_major = True is a bit more\nefficient because it avoids transposes at the beginning and end of the\nRNN calculation. However, most TensorFlow data is batch-major, so by\ndefault this function accepts input and emits output in batch-major\nform.", "zero_output_for_mask": "Boolean (default False).\nWhether the output should use zeros for the masked timesteps. Note that\nthis field is only used when return_sequences is True and mask is\nprovided. It can useful if you want to reuse the raw output sequence of\nthe RNN without interference from the masked timesteps, eg, merging\nbidirectional RNNs."}, "Attributes": {"states": ""}}, "tf.keras.layers.RandomBrightness": {"description": "A preprocessing layer which randomly adjusts brightness during training.", "Args": {"factor": "Float or a list/tuple of 2 floats between -1.0 and 1.0. The\nfactor is used to determine the lower bound and upper bound of the\nbrightness adjustment. A float value will be chosen randomly between\nthe limits. When -1.0 is chosen, the output image will be black, and\nwhen 1.0 is chosen, the image will be fully white. When only one float\nis provided, eg, 0.2, then -0.2 will be used for lower bound and 0.2\nwill be used for upper bound.", "value_range": "Optional list/tuple of 2 floats for the lower and upper limit\nof the values of the input data. Defaults to [0.0, 255.0]. Can be changed\nto e.g. [0.0, 1.0] if the image input has been scaled before this layer.\nThe brightness adjustment will be scaled to this range, and the\noutput values will be clipped to this range.", "seed": "optional integer, for fixed RNG behavior."}, "Attributes": {"auto_vectorize": "Control whether automatic vectorization occurs.\nBy default the call() method leverages the tf.vectorized_map() function.\nAuto-vectorization can be disabled by setting self.auto_vectorize = False\nin your __init__() method.  When disabled, call() instead relies\non tf.map_fn(). For example:\nclass SubclassLayer(BaseImageAugmentationLayer):\u00a0 def __init__(self):\u00a0 \u00a0 super().__init__()\u00a0 \u00a0 self.auto_vectorize = False"}}, "tf.keras.layers.RandomContrast": {"description": "A preprocessing layer which randomly adjusts contrast during training.", "Attributes": {"auto_vectorize": "Control whether automatic vectorization occurs.\nBy default the call() method leverages the tf.vectorized_map() function.\nAuto-vectorization can be disabled by setting self.auto_vectorize = False\nin your __init__() method.  When disabled, call() instead relies\non tf.map_fn(). For example:\nclass SubclassLayer(BaseImageAugmentationLayer):\u00a0 def __init__(self):\u00a0 \u00a0 super().__init__()\u00a0 \u00a0 self.auto_vectorize = False"}}, "tf.keras.layers.RandomCrop": {"description": "A preprocessing layer which randomly crops images during training.", "Args": {"height": "Integer, the height of the output shape.", "width": "Integer, the width of the output shape.", "seed": "Integer. Used to create a random seed."}}, "tf.keras.layers.RandomFlip": {"description": "A preprocessing layer which randomly flips images during training.", "Attributes": {"auto_vectorize": "Control whether automatic vectorization occurs.\nBy default the call() method leverages the tf.vectorized_map() function.\nAuto-vectorization can be disabled by setting self.auto_vectorize = False\nin your __init__() method.  When disabled, call() instead relies\non tf.map_fn(). For example:\nclass SubclassLayer(BaseImageAugmentationLayer):\u00a0 def __init__(self):\u00a0 \u00a0 super().__init__()\u00a0 \u00a0 self.auto_vectorize = False"}}, "tf.keras.layers.RandomHeight": {"description": "A preprocessing layer which randomly varies image height during training.", "Args": {"factor": "A positive float (fraction of original height), or a tuple of size 2\nrepresenting lower and upper bound for resizing vertically. When\nrepresented as a single float, this value is used for both the upper and\nlower bound. For instance, factor=(0.2, 0.3) results in an output with\nheight changed by a random amount in the range [20%, 30%].\nfactor=(-0.2, 0.3) results in an output with height changed by a random\namount in the range [-20%, +30%]. factor=0.2 results in an output with\nheight changed by a random amount in the range [-20%, +20%].", "interpolation": "String, the interpolation method. Defaults to \"bilinear\".\nSupports \"bilinear\", \"nearest\", \"bicubic\", \"area\",\n\"lanczos3\", \"lanczos5\", \"gaussian\", \"mitchellcubic\".", "seed": "Integer. Used to create a random seed."}}, "tf.keras.layers.RandomRotation": {"description": "A preprocessing layer which randomly rotates images during training."}, "tf.keras.layers.RandomTranslation": {"description": "A preprocessing layer which randomly translates images during training.", "Args": {"height_factor": "a float represented as fraction of value, or a tuple of size\n2 representing lower and upper bound for shifting vertically. A negative\nvalue means shifting image up, while a positive value means shifting image\ndown. When represented as a single positive float, this value is used for\nboth the upper and lower bound. For instance, height_factor=(-0.2, 0.3)\nresults in an output shifted by a random amount in the range\n[-20%, +30%].\nheight_factor=0.2 results in an output height shifted by a random amount\nin the range [-20%, +20%].", "width_factor": "a float represented as fraction of value, or a tuple of size 2\nrepresenting lower and upper bound for shifting horizontally. A negative\nvalue means shifting image left, while a positive value means shifting\nimage right. When represented as a single positive float, this value is\nused for both the upper and lower bound. For instance,\nwidth_factor=(-0.2, 0.3) results in an output shifted left by 20%, and\nshifted right by 30%. width_factor=0.2 results in an output height\nshifted left or right by 20%.", "fill_mode": "Points outside the boundaries of the input are filled according\nto the given mode (one of {\"constant\", \"reflect\", \"wrap\", \"nearest\"}).\n\nreflect: (d c b a | a b c d | d c b a) The input is extended by\nreflecting about the edge of the last pixel.\nconstant: (k k k k | a b c d | k k k k) The input is extended by\nfilling all values beyond the edge with the same constant value k = 0.\nwrap: (a b c d | a b c d | a b c d) The input is extended by\nwrapping around to the opposite edge.\nnearest: (a a a a | a b c d | d d d d) The input is extended by the\nnearest pixel.", "interpolation": "Interpolation mode. Supported values: \"nearest\",\n\"bilinear\".", "seed": "Integer. Used to create a random seed.", "fill_value": "a float represents the value to be filled outside the boundaries\nwhen fill_mode=\"constant\"."}, "Attributes": {"auto_vectorize": "Control whether automatic vectorization occurs.\nBy default the call() method leverages the tf.vectorized_map() function.\nAuto-vectorization can be disabled by setting self.auto_vectorize = False\nin your __init__() method.  When disabled, call() instead relies\non tf.map_fn(). For example:\nclass SubclassLayer(BaseImageAugmentationLayer):\u00a0 def __init__(self):\u00a0 \u00a0 super().__init__()\u00a0 \u00a0 self.auto_vectorize = False"}}, "tf.keras.layers.RandomWidth": {"description": "A preprocessing layer which randomly varies image width during training.", "Args": {"factor": "A positive float (fraction of original width), or a tuple of size 2\nrepresenting lower and upper bound for resizing vertically. When\nrepresented as a single float, this value is used for both the upper and\nlower bound. For instance, factor=(0.2, 0.3) results in an output with\nwidth changed by a random amount in the range [20%, 30%]. factor=(-0.2,\n0.3) results in an output with width changed by a random amount in the\nrange [-20%, +30%]. factor=0.2 results in an output with width changed\nby a random amount in the range [-20%, +20%].", "interpolation": "String, the interpolation method. Defaults to bilinear.\nSupports \"bilinear\", \"nearest\", \"bicubic\", \"area\", \"lanczos3\",\n\"lanczos5\", \"gaussian\", \"mitchellcubic\".", "seed": "Integer. Used to create a random seed."}}, "tf.keras.layers.RandomZoom": {"description": "A preprocessing layer which randomly zooms images during training.", "Args": {"height_factor": "a float represented as fraction of value, or a tuple of size\n2 representing lower and upper bound for zooming vertically. When\nrepresented as a single float, this value is used for both the upper and\nlower bound. A positive value means zooming out, while a negative value\nmeans zooming in. For instance, height_factor=(0.2, 0.3) result in an\noutput zoomed out by a random amount in the range [+20%, +30%].\nheight_factor=(-0.3, -0.2) result in an output zoomed in by a random\namount in the range [+20%, +30%].", "width_factor": "a float represented as fraction of value, or a tuple of size 2\nrepresenting lower and upper bound for zooming horizontally. When\nrepresented as a single float, this value is used for both the upper and\nlower bound. For instance, width_factor=(0.2, 0.3) result in an output\nzooming out between 20% to 30%. width_factor=(-0.3, -0.2) result in an\noutput zooming in between 20% to 30%. Defaults to None, i.e., zooming\nvertical and horizontal directions by preserving the aspect ratio.", "fill_mode": "Points outside the boundaries of the input are filled according\nto the given mode (one of {\"constant\", \"reflect\", \"wrap\", \"nearest\"}).\n\nreflect: (d c b a | a b c d | d c b a) The input is extended by\nreflecting about the edge of the last pixel.\nconstant: (k k k k | a b c d | k k k k) The input is extended by\nfilling all values beyond the edge with the same constant value k = 0.\nwrap: (a b c d | a b c d | a b c d) The input is extended by\nwrapping around to the opposite edge.\nnearest: (a a a a | a b c d | d d d d) The input is extended by the\nnearest pixel.", "interpolation": "Interpolation mode. Supported values: \"nearest\",\n\"bilinear\".", "seed": "Integer. Used to create a random seed.", "fill_value": "a float represents the value to be filled outside the boundaries\nwhen fill_mode=\"constant\"."}}, "tf.keras.layers.ReLU": {"description": "Rectified Linear Unit activation function.", "Args": {"max_value": "Float >= 0. Maximum activation value. Default to None, which\nmeans unlimited.", "negative_slope": "Float >= 0. Negative slope coefficient. Default to 0.", "threshold": "Float >= 0. Threshold value for thresholded activation. Default\nto 0."}}, "tf.keras.layers.RepeatVector": {"description": "Repeats the input n times.", "Args": {"n": "Integer, repetition factor."}}, "tf.keras.layers.Rescaling": {"description": "A preprocessing layer which rescales input values to a new range.", "Args": {"scale": "Float, the scale to apply to the inputs.", "offset": "Float, the offset to apply to the inputs."}}, "tf.keras.layers.Reshape": {"description": "Layer that reshapes inputs into the given shape.", "Args": {"target_shape": "Target shape. Tuple of integers, does not include the\nsamples dimension (batch size).", "**kwargs": "Any additional layer keyword arguments."}}, "tf.keras.layers.Resizing": {"description": "A preprocessing layer which resizes images.", "Args": {"height": "Integer, the height of the output shape.", "width": "Integer, the width of the output shape.", "interpolation": "String, the interpolation method. Defaults to \"bilinear\".\nSupports \"bilinear\", \"nearest\", \"bicubic\", \"area\", \"lanczos3\",\n\"lanczos5\", \"gaussian\", \"mitchellcubic\".", "crop_to_aspect_ratio": "If True, resize the images without aspect\nratio distortion. When the original aspect ratio differs from the target\naspect ratio, the output image will be cropped so as to return the largest\npossible window in the image (of size (height, width)) that matches\nthe target aspect ratio. By default (crop_to_aspect_ratio=False),\naspect ratio may not be preserved."}}, "tf.keras.layers.SeparableConv1D": {"description": "Depthwise separable 1D convolution.", "Args": {"filters": "Integer, the dimensionality of the output space (i.e. the number\nof filters in the convolution).", "kernel_size": "A single integer specifying the spatial\ndimensions of the filters.", "strides": "A single integer specifying the strides\nof the convolution.\nSpecifying any stride value != 1 is incompatible with specifying\nany dilation_rate value != 1.", "padding": "One of \"valid\", \"same\", or \"causal\" (case-insensitive).\n\"valid\" means no padding. \"same\" results in padding with zeros evenly\nto the left/right or up/down of the input such that output has the same\nheight/width dimension as the input. \"causal\" results in causal\n(dilated) convolutions, e.g. output[t] does not depend on input[t+1:].", "data_format": "A string, one of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs.\nchannels_last corresponds to inputs with shape\n(batch_size, length, channels) while channels_first corresponds to\ninputs with shape (batch_size, channels, length).", "dilation_rate": "A single integer, specifying\nthe dilation rate to use for dilated convolution.", "depth_multiplier": "The number of depthwise convolution output channels for\neach input channel. The total number of depthwise convolution output\nchannels will be equal to num_filters_in * depth_multiplier.", "activation": "Activation function to use.\nIf you don't specify anything, no activation is applied\n(see keras.activations).", "use_bias": "Boolean, whether the layer uses a bias.", "depthwise_initializer": "An initializer for the depthwise convolution kernel\n(see keras.initializers). If None, then the default initializer\n('glorot_uniform') will be used.", "pointwise_initializer": "An initializer for the pointwise convolution kernel\n(see keras.initializers). If None, then the default initializer\n('glorot_uniform') will be used.", "bias_initializer": "An initializer for the bias vector. If None, the default\ninitializer ('zeros') will be used (see keras.initializers).", "depthwise_regularizer": "Optional regularizer for the depthwise\nconvolution kernel (see keras.regularizers).", "pointwise_regularizer": "Optional regularizer for the pointwise\nconvolution kernel (see keras.regularizers).", "bias_regularizer": "Optional regularizer for the bias vector\n(see keras.regularizers).", "activity_regularizer": "Optional regularizer function for the output\n(see keras.regularizers).", "depthwise_constraint": "Optional projection function to be applied to the\ndepthwise kernel after being updated by an Optimizer (e.g. used for\nnorm constraints or value constraints for layer weights). The function\nmust take as input the unprojected variable and must return the\nprojected variable (which must have the same shape). Constraints are\nnot safe to use when doing asynchronous distributed training\n(see keras.constraints).", "pointwise_constraint": "Optional projection function to be applied to the\npointwise kernel after being updated by an Optimizer\n(see keras.constraints).", "bias_constraint": "Optional projection function to be applied to the\nbias after being updated by an Optimizer\n(see keras.constraints).", "trainable": "Boolean, if True the weights of this layer will be marked as\ntrainable (and listed in layer.trainable_weights)."}, "Returns": "A tensor of rank 3 representing\nactivation(separableconv1d(inputs, kernel) + bias)."}, "tf.keras.layers.SeparableConv2D": {"description": "Depthwise separable 2D convolution.", "Args": {"filters": "Integer, the dimensionality of the output space\n(i.e. the number of output filters in the convolution).", "kernel_size": "An integer or tuple/list of 2 integers, specifying the\nheight and width of the 2D convolution window.\nCan be a single integer to specify the same value for\nall spatial dimensions.", "strides": "An integer or tuple/list of 2 integers,\nspecifying the strides of the convolution along the height and width.\nCan be a single integer to specify the same value for\nall spatial dimensions. Current implementation only supports equal\nlength strides in the row and column dimensions.\nSpecifying any stride value != 1 is incompatible with specifying\nany dilation_rate value != 1.", "padding": "one of \"valid\" or \"same\" (case-insensitive).\n\"valid\" means no padding. \"same\" results in padding with zeros evenly\nto the left/right or up/down of the input such that output has the same\nheight/width dimension as the input.", "data_format": "A string,\none of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs.\nchannels_last corresponds to inputs with shape\n(batch_size, height, width, channels) while channels_first\ncorresponds to inputs with shape\n(batch_size, channels, height, width).\nIt defaults to the image_data_format value found in your\nKeras config file at ~/.keras/keras.json.\nIf you never set it, then it will be \"channels_last\".", "dilation_rate": "An integer or tuple/list of 2 integers, specifying\nthe dilation rate to use for dilated convolution.", "depth_multiplier": "The number of depthwise convolution output channels\nfor each input channel.\nThe total number of depthwise convolution output\nchannels will be equal to filters_in * depth_multiplier.", "activation": "Activation function to use.\nIf you don't specify anything, no activation is applied\n(see keras.activations).", "use_bias": "Boolean, whether the layer uses a bias vector.", "depthwise_initializer": "An initializer for the depthwise convolution kernel\n(see keras.initializers). If None, then the default initializer\n('glorot_uniform') will be used.", "pointwise_initializer": "An initializer for the pointwise convolution kernel\n(see keras.initializers). If None, then the default initializer\n('glorot_uniform') will be used.", "bias_initializer": "An initializer for the bias vector. If None, the default\ninitializer ('zeros') will be used (see keras.initializers).", "depthwise_regularizer": "Regularizer function applied to\nthe depthwise kernel matrix (see keras.regularizers).", "pointwise_regularizer": "Regularizer function applied to\nthe pointwise kernel matrix (see keras.regularizers).", "bias_regularizer": "Regularizer function applied to the bias vector\n(see keras.regularizers).", "activity_regularizer": "Regularizer function applied to\nthe output of the layer (its \"activation\")\n(see keras.regularizers).", "depthwise_constraint": "Constraint function applied to\nthe depthwise kernel matrix\n(see keras.constraints).", "pointwise_constraint": "Constraint function applied to\nthe pointwise kernel matrix\n(see keras.constraints).", "bias_constraint": "Constraint function applied to the bias vector\n(see keras.constraints)."}, "Returns": "A tensor of rank 4 representing\nactivation(separableconv2d(inputs, kernel) + bias).", "Raises": {"ValueError": "if padding is \"causal\"."}}, "tf.keras.layers.SimpleRNN": {"description": "Fully-connected RNN where the output is to be fed back to input.", "Args": {"units": "Positive integer, dimensionality of the output space.", "activation": "Activation function to use.\nDefault: hyperbolic tangent (tanh).\nIf you pass None, no activation is applied\n(ie. \"linear\" activation: a(x) = x).", "use_bias": "Boolean, (default True), whether the layer uses a bias vector.", "kernel_initializer": "Initializer for the kernel weights matrix,\nused for the linear transformation of the inputs. Default:\nglorot_uniform.", "recurrent_initializer": "Initializer for the recurrent_kernel\nweights matrix, used for the linear transformation of the recurrent state.\nDefault: orthogonal.", "bias_initializer": "Initializer for the bias vector. Default: zeros.", "kernel_regularizer": "Regularizer function applied to the kernel weights\nmatrix. Default: None.", "recurrent_regularizer": "Regularizer function applied to the\nrecurrent_kernel weights matrix. Default: None.", "bias_regularizer": "Regularizer function applied to the bias vector. Default:\nNone.", "activity_regularizer": "Regularizer function applied to the output of the\nlayer (its \"activation\"). Default: None.", "kernel_constraint": "Constraint function applied to the kernel weights\nmatrix. Default: None.", "recurrent_constraint": "Constraint function applied to the recurrent_kernel\nweights matrix.  Default: None.", "bias_constraint": "Constraint function applied to the bias vector. Default:\nNone.", "dropout": "Float between 0 and 1.\nFraction of the units to drop for the linear transformation of the inputs.\nDefault: 0.", "recurrent_dropout": "Float between 0 and 1.\nFraction of the units to drop for the linear transformation of the\nrecurrent state. Default: 0.", "return_sequences": "Boolean. Whether to return the last output\nin the output sequence, or the full sequence. Default: False.", "return_state": "Boolean. Whether to return the last state\nin addition to the output. Default: False", "go_backwards": "Boolean (default False).\nIf True, process the input sequence backwards and return the\nreversed sequence.", "stateful": "Boolean (default False). If True, the last state\nfor each sample at index i in a batch will be used as initial\nstate for the sample of index i in the following batch.", "unroll": "Boolean (default False).\nIf True, the network will be unrolled,\nelse a symbolic loop will be used.\nUnrolling can speed-up a RNN,\nalthough it tends to be more memory-intensive.\nUnrolling is only suitable for short sequences."}, "Attributes": {"activation": "", "bias_constraint": "", "bias_initializer": "", "bias_regularizer": "", "dropout": "", "kernel_constraint": "", "kernel_initializer": "", "kernel_regularizer": "", "recurrent_constraint": "", "recurrent_dropout": "", "recurrent_initializer": "", "recurrent_regularizer": "", "states": "", "units": "", "use_bias": ""}}, "tf.keras.layers.SimpleRNNCell": {"description": "Cell class for SimpleRNN.", "Args": {"units": "Positive integer, dimensionality of the output space.", "activation": "Activation function to use.\nDefault: hyperbolic tangent (tanh).\nIf you pass None, no activation is applied\n(ie. \"linear\" activation: a(x) = x).", "use_bias": "Boolean, (default True), whether the layer uses a bias vector.", "kernel_initializer": "Initializer for the kernel weights matrix,\nused for the linear transformation of the inputs. Default:\nglorot_uniform.", "recurrent_initializer": "Initializer for the recurrent_kernel\nweights matrix, used for the linear transformation of the recurrent state.\nDefault: orthogonal.", "bias_initializer": "Initializer for the bias vector. Default: zeros.", "kernel_regularizer": "Regularizer function applied to the kernel weights\nmatrix. Default: None.", "recurrent_regularizer": "Regularizer function applied to the\nrecurrent_kernel weights matrix. Default: None.", "bias_regularizer": "Regularizer function applied to the bias vector. Default:\nNone.", "kernel_constraint": "Constraint function applied to the kernel weights\nmatrix. Default: None.", "recurrent_constraint": "Constraint function applied to the recurrent_kernel\nweights matrix. Default: None.", "bias_constraint": "Constraint function applied to the bias vector. Default:\nNone.", "dropout": "Float between 0 and 1. Fraction of the units to drop for the linear\ntransformation of the inputs. Default: 0.", "recurrent_dropout": "Float between 0 and 1. Fraction of the units to drop for\nthe linear transformation of the recurrent state. Default: 0."}}, "tf.keras.layers.Softmax": {"description": "Softmax activation function.", "Args": {"axis": "Integer, or list of Integers, axis along which the softmax\nnormalization is applied."}, "Returns": "softmaxed output with the same shape as inputs."}, "tf.keras.layers.SpatialDropout1D": {"description": "Spatial 1D version of Dropout.", "Args": {"rate": "Float between 0 and 1. Fraction of the input units to drop."}}, "tf.keras.layers.SpatialDropout2D": {"description": "Spatial 2D version of Dropout.", "Args": {"rate": "Float between 0 and 1. Fraction of the input units to drop.", "data_format": "'channels_first' or 'channels_last'. In 'channels_first' mode,\nthe channels dimension (the depth) is at index 1, in 'channels_last' mode\nis it at index 3. It defaults to the image_data_format value found in\nyour Keras config file at ~/.keras/keras.json. If you never set it, then\nit will be \"channels_last\"."}}, "tf.keras.layers.SpatialDropout3D": {"description": "Spatial 3D version of Dropout.", "Args": {"rate": "Float between 0 and 1. Fraction of the input units to drop.", "data_format": "'channels_first' or 'channels_last'. In 'channels_first' mode,\nthe channels dimension (the depth) is at index 1, in 'channels_last' mode\nis it at index 4. It defaults to the image_data_format value found in\nyour Keras config file at ~/.keras/keras.json. If you never set it, then\nit will be \"channels_last\"."}}, "tf.keras.layers.StackedRNNCells": {"description": "Wrapper allowing a stack of RNN cells to behave as a single cell.", "Args": {"cells": "List of RNN cell instances."}, "Attributes": {"output_size": "", "state_size": ""}}, "tf.keras.layers.StringLookup": {"description": "A preprocessing layer which maps string features to integer indices.", "Args": {"max_tokens": "Maximum size of the vocabulary for this layer. This should only\nbe specified when adapting the vocabulary or when setting\npad_to_max_tokens=True. If None, there is no cap on the size of the\nvocabulary. Note that this size includes the OOV and mask tokens. Defaults\nto None.", "num_oov_indices": "The number of out-of-vocabulary tokens to use. If this\nvalue is more than 1, OOV inputs are hashed to determine their OOV value.\nIf this value is 0, OOV inputs will cause an error when calling the layer.\nDefaults to 1.", "mask_token": "A token that represents masked inputs. When output_mode is\n\"int\", the token is included in vocabulary and mapped to index 0. In\nother output modes, the token will not appear in the vocabulary and\ninstances of the mask token in the input will be dropped. If set to None,\nno mask term will be added. Defaults to None.", "oov_token": "Only used when invert is True. The token to return for OOV\nindices. Defaults to \"[UNK]\".", "vocabulary": "Optional. Either an array of strings or a string path to a text\nfile. If passing an array, can pass a tuple, list, 1D numpy array, or 1D\ntensor containing the string vocbulary terms. If passing a file path, the\nfile should contain one line per term in the vocabulary. If this argument\nis set, there is no need to adapt() the layer.", "idf_weights": "Only valid when output_mode is \"tf_idf\". A tuple, list, 1D\nnumpy array, or 1D tensor or the same length as the vocabulary, containing\nthe floating point inverse document frequency weights, which will be\nmultiplied by per sample term counts for the final tf_idf weight. If the\nvocabulary argument is set, and output_mode is \"tf_idf\", this\nargument must be supplied.", "invert": "Only valid when output_mode is \"int\". If True, this layer will\nmap indices to vocabulary items instead of mapping vocabulary items to\nindices. Default to False.", "output_mode": "Specification for the output of the layer. Defaults to \"int\".\nValues can be \"int\", \"one_hot\", \"multi_hot\", \"count\", or\n\"tf_idf\" configuring the layer as follows:\n\n\"int\": Return the raw integer indices of the input tokens.\n\"one_hot\": Encodes each individual element in the input into an\narray the same size as the vocabulary, containing a 1 at the element\nindex. If the last dimension is size 1, will encode on that dimension.\nIf the last dimension is not size 1, will append a new dimension for\nthe encoded output.\n\"multi_hot\": Encodes each sample in the input into a single array\nthe same size as the vocabulary, containing a 1 for each vocabulary\nterm present in the sample. Treats the last dimension as the sample\ndimension, if input shape is (..., sample_length), output shape will\nbe (..., num_tokens).\n\"count\": As \"multi_hot\", but the int array contains a count of the\nnumber of times the token at that index appeared in the sample.\n\"tf_idf\": As \"multi_hot\", but the TF-IDF algorithm is applied to\nfind the value in each token slot.\nFor \"int\" output, any shape of input and output is supported. For all\nother output modes, currently only output up to rank 2 is supported.", "pad_to_max_tokens": "Only applicable when output_mode is \"multi_hot\",\n\"count\", or \"tf_idf\". If True, the output will have its feature axis\npadded to max_tokens even if the number of unique tokens in the\nvocabulary is less than max_tokens, resulting in a tensor of shape\n[batch_size, max_tokens] regardless of vocabulary size. Defaults to False.", "sparse": "Boolean. Only applicable when output_mode is \"multi_hot\",\n\"count\", or \"tf_idf\". If True, returns a SparseTensor instead of a\ndense Tensor. Defaults to False."}, "Attributes": {"is_adapted": "Whether the layer has been fit to data already."}}, "tf.keras.layers.Subtract": {"description": "Layer that subtracts two inputs.", "Args": {"**kwargs": "standard layer keyword arguments."}}, "tf.keras.layers.TextVectorization": {"description": "A preprocessing layer which maps text features to integer sequences.", "Args": {"max_tokens": "Maximum size of the vocabulary for this layer. This should only\nbe specified when adapting a vocabulary or when setting\npad_to_max_tokens=True. Note that this vocabulary\ncontains 1 OOV token, so the effective number of tokens is (max_tokens -\n1 - (1 if output_mode == \"int\" else 0)).", "standardize": "Optional specification for standardization to apply to the\ninput text. Values can be:\n\nNone: No standardization.\n\"lower_and_strip_punctuation\": Text will be lowercased and all\npunctuation removed.\n\"lower\": Text will be lowercased.\n\"strip_punctuation\": All punctuation will be removed.\nCallable: Inputs will passed to the callable function, which should\nstandardized and returned.", "split": "Optional specification for splitting the input text. Values can be:\nNone: No splitting.\n\"whitespace\": Split on whitespace.\n\"character\": Split on each unicode character.\nCallable: Standardized inputs will passed to the callable function,\nwhich should split and returned.", "ngrams": "Optional specification for ngrams to create from the possibly-split\ninput text. Values can be None, an integer or tuple of integers; passing\nan integer will create ngrams up to that integer, and passing a tuple of\nintegers will create ngrams for the specified values in the tuple. Passing\nNone means that no ngrams will be created.", "output_mode": "Optional specification for the output of the layer. Values can\nbe \"int\", \"multi_hot\", \"count\" or \"tf_idf\", configuring the layer\nas follows:\n\n\"int\": Outputs integer indices, one integer index per split string\ntoken. When output_mode == \"int\", 0 is reserved for masked\nlocations; this reduces the vocab size to\nmax_tokens - 2 instead of max_tokens - 1.\n\"multi_hot\": Outputs a single int array per batch, of either\nvocab_size or max_tokens size, containing 1s in all elements where the\ntoken mapped to that index exists at least once in the batch item.\n\"count\": Like \"multi_hot\", but the int array contains a count of\nthe number of times the token at that index appeared in the\nbatch item.\n\"tf_idf\": Like \"multi_hot\", but the TF-IDF algorithm is applied to\nfind the value in each token slot.\nFor \"int\" output, any shape of input and output is supported. For all\nother output modes, currently only rank 1 inputs (and rank 2 outputs after\nsplitting) are supported.", "output_sequence_length": "Only valid in INT mode. If set, the output will have\nits time dimension padded or truncated to exactly output_sequence_length\nvalues, resulting in a tensor of shape\n(batch_size, output_sequence_length) regardless of how many tokens\nresulted from the splitting step. Defaults to None.", "pad_to_max_tokens": "Only valid in  \"multi_hot\", \"count\", and \"tf_idf\"\nmodes. If True, the output will have its feature axis padded to\nmax_tokens even if the number of unique tokens in the vocabulary is less\nthan max_tokens, resulting in a tensor of shape (batch_size, max_tokens)\nregardless of vocabulary size. Defaults to False.", "vocabulary": "Optional. Either an array of strings or a string path to a text\nfile. If passing an array, can pass a tuple, list, 1D numpy array, or 1D\ntensor containing the string vocbulary terms. If passing a file path, the\nfile should contain one line per term in the vocabulary. If this argument\nis set, there is no need to adapt() the layer.", "idf_weights": "Only valid when output_mode is \"tf_idf\". A tuple, list, 1D\nnumpy array, or 1D tensor or the same length as the vocabulary, containing\nthe floating point inverse document frequency weights, which will be\nmultiplied by per sample term counts for the final tf_idf weight. If the\nvocabulary argument is set, and output_mode is \"tf_idf\", this\nargument must be supplied.", "ragged": "Boolean. Only applicable to \"int\" output mode. If True, returns a\nRaggedTensor instead of a dense Tensor, where each sequence may have a\ndifferent length after string splitting. Defaults to False.", "sparse": "Boolean. Only applicable to \"multi_hot\", \"count\", and\n\"tf_idf\" output modes. If True, returns a SparseTensor instead of a\ndense Tensor. Defaults to False."}, "Attributes": {"is_adapted": "Whether the layer has been fit to data already."}}, "tf.keras.layers.ThresholdedReLU": {"description": "Thresholded Rectified Linear Unit.", "Args": {"theta": "Float >= 0. Threshold location of activation."}}, "tf.keras.layers.TimeDistributed": {"description": "This wrapper allows to apply a layer to every temporal slice of an input.", "Args": {"layer": "a tf.keras.layers.Layer instance."}, "Raises": {"ValueError": "If not initialized with a tf.keras.layers.Layer instance."}}, "tf.keras.layers.UnitNormalization": {"description": "Unit normalization layer.", "Args": {"axis": "Integer or list/tuple. The axis or axes to normalize across. Typically\nthis is the features axis or axes. The left-out axes are typically the\nbatch axis or axes. Defaults to -1, the last dimension in\nthe input."}}, "tf.keras.layers.UpSampling1D": {"description": "Upsampling layer for 1D inputs.", "Args": {"size": "Integer. Upsampling factor."}}, "tf.keras.layers.UpSampling2D": {"description": "Upsampling layer for 2D inputs.", "Args": {"size": "Int, or tuple of 2 integers.\nThe upsampling factors for rows and columns.", "data_format": "A string,\none of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs.\nchannels_last corresponds to inputs with shape\n(batch_size, height, width, channels) while channels_first\ncorresponds to inputs with shape\n(batch_size, channels, height, width).\nIt defaults to the image_data_format value found in your\nKeras config file at ~/.keras/keras.json.\nIf you never set it, then it will be \"channels_last\".", "interpolation": "A string, one of \"area\", \"bicubic\", \"bilinear\",\n\"gaussian\", \"lanczos3\", \"lanczos5\", \"mitchellcubic\", \"nearest\"."}}, "tf.keras.layers.UpSampling3D": {"description": "Upsampling layer for 3D inputs.", "Args": {"size": "Int, or tuple of 3 integers.\nThe upsampling factors for dim1, dim2 and dim3.", "data_format": "A string,\none of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs.\nchannels_last corresponds to inputs with shape\n(batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)\nwhile channels_first corresponds to inputs with shape\n(batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3).\nIt defaults to the image_data_format value found in your\nKeras config file at ~/.keras/keras.json.\nIf you never set it, then it will be \"channels_last\"."}}, "tf.keras.layers.Wrapper": {"description": "Abstract wrapper base class.", "Args": {"layer": "The layer to be wrapped."}}, "tf.keras.layers.ZeroPadding1D": {"description": "Zero-padding layer for 1D input (e.g. temporal sequence).", "Args": {"padding": "Int, or tuple of int (length 2), or dictionary.\n\nIf int:\nHow many zeros to add at the beginning and end of\nthe padding dimension (axis 1).\nIf tuple of int (length 2):\nHow many zeros to add at the beginning and the end of\nthe padding dimension ((left_pad, right_pad))."}}, "tf.keras.layers.ZeroPadding2D": {"description": "Zero-padding layer for 2D input (e.g. picture).", "Args": {"padding": "Int, or tuple of 2 ints, or tuple of 2 tuples of 2 ints.\n\nIf int: the same symmetric padding\nis applied to height and width.\nIf tuple of 2 ints:\ninterpreted as two different\nsymmetric padding values for height and width:\n(symmetric_height_pad, symmetric_width_pad).\nIf tuple of 2 tuples of 2 ints:\ninterpreted as\n((top_pad, bottom_pad), (left_pad, right_pad))", "data_format": "A string,\none of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs.\nchannels_last corresponds to inputs with shape\n(batch_size, height, width, channels) while channels_first\ncorresponds to inputs with shape\n(batch_size, channels, height, width).\nIt defaults to the image_data_format value found in your\nKeras config file at ~/.keras/keras.json.\nIf you never set it, then it will be \"channels_last\"."}}, "tf.keras.layers.ZeroPadding3D": {"description": "Zero-padding layer for 3D data (spatial or spatio-temporal).", "Args": {"padding": "Int, or tuple of 3 ints, or tuple of 3 tuples of 2 ints.\n\nIf int: the same symmetric padding\nis applied to height and width.\nIf tuple of 3 ints:\ninterpreted as two different\nsymmetric padding values for height and width:\n(symmetric_dim1_pad, symmetric_dim2_pad, symmetric_dim3_pad).\nIf tuple of 3 tuples of 2 ints:\ninterpreted as\n((left_dim1_pad, right_dim1_pad), (left_dim2_pad,\nright_dim2_pad), (left_dim3_pad, right_dim3_pad))", "data_format": "A string,\none of channels_last (default) or channels_first.\nThe ordering of the dimensions in the inputs.\nchannels_last corresponds to inputs with shape\n(batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)\nwhile channels_first corresponds to inputs with shape\n(batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3).\nIt defaults to the image_data_format value found in your\nKeras config file at ~/.keras/keras.json.\nIf you never set it, then it will be \"channels_last\"."}}, "tf.keras.layers.add": {"description": "Functional interface to the tf.keras.layers.Add layer.", "Args": {"inputs": "A list of input tensors with the same shape.", "**kwargs": "Standard layer keyword arguments."}, "Returns": "A tensor as the sum of the inputs. It has the same shape as the inputs."}, "tf.keras.layers.average": {"description": "Functional interface to the tf.keras.layers.Average layer.", "Args": {"inputs": "A list of input tensors.", "**kwargs": "Standard layer keyword arguments."}, "Returns": "A tensor, the average of the inputs.", "Raises": {"ValueError": "If there is a shape mismatch between the inputs and the shapes\ncannot be broadcasted to match."}}, "tf.keras.layers.concatenate": {"description": "Functional interface to the Concatenate layer.", "Args": {"inputs": "A list of input tensors.", "axis": "Concatenation axis.", "**kwargs": "Standard layer keyword arguments."}, "Returns": "A tensor, the concatenation of the inputs alongside axis axis."}, "tf.keras.layers.deserialize": {"description": "Instantiates a layer from a config dictionary.", "Args": {"config": "dict of the form {'class_name': str, 'config': dict}", "custom_objects": "dict mapping class names (or function names) of custom\n(non-Keras) objects to class/functions"}, "Returns": "Layer instance (may be Model, Sequential, Network, Layer...)"}, "tf.keras.layers.dot": {"description": "Functional interface to the Dot layer.", "Args": {"inputs": "A list of input tensors (at least 2).", "axes": "Integer or tuple of integers,\naxis or axes along which to take the dot product.", "normalize": "Whether to L2-normalize samples along the\ndot product axis before taking the dot product.\nIf set to True, then the output of the dot product\nis the cosine proximity between the two samples.", "**kwargs": "Standard layer keyword arguments."}, "Returns": "A tensor, the dot product of the samples from the inputs."}, "tf.keras.layers.maximum": {"description": "Functional interface to compute maximum (element-wise) list of inputs.", "Args": {"inputs": "A list of input tensors of same shape.", "**kwargs": "Standard layer keyword arguments."}, "Returns": "A tensor (of same shape as input tensor) with the element-wise\nmaximum of the inputs.", "Raises": {"ValueError": "If input tensors are of different shape."}}, "tf.keras.layers.minimum": {"description": "Functional interface to the Minimum layer.", "Args": {"inputs": "A list of input tensors.", "**kwargs": "Standard layer keyword arguments."}, "Returns": "A tensor, the element-wise minimum of the inputs."}, "tf.keras.layers.multiply": {"description": "Functional interface to the Multiply layer.", "Args": {"inputs": "A list of input tensors.", "**kwargs": "Standard layer keyword arguments."}, "Returns": "A tensor, the element-wise product of the inputs."}, "tf.keras.layers.serialize": {"description": "Serializes a Layer object into a JSON-compatible representation.", "Args": {"layer": "The Layer object to serialize."}, "Returns": "A JSON-serializable dict representing the object's config."}, "tf.keras.layers.subtract": {"description": "Functional interface to the Subtract layer.", "Args": {"inputs": "A list of input tensors (exactly 2).", "**kwargs": "Standard layer keyword arguments."}, "Returns": "A tensor, the difference of the inputs."}}, "tf.keras.layers.experimental.preprocessing": {"tf.keras.layers.experimental.preprocessing.HashedCrossing": {"description": "A preprocessing layer which crosses features using the &#34;hashing trick&#34;.", "Args": {"num_bins": "Number of hash bins.", "output_mode": "Specification for the output of the layer. Defaults to \"int\".\nValues can be \"int\", or \"one_hot\" configuring the layer as follows:\n\n\"int\": Return the integer bin indices directly.\n\"one_hot\": Encodes each individual element in the input into an\narray the same size as num_bins, containing a 1 at the input's bin\nindex.", "sparse": "Boolean. Only applicable to \"one_hot\" mode. If True, returns a\nSparseTensor instead of a dense Tensor. Defaults to False.", "**kwargs": "Keyword arguments to construct a layer."}}, "tf.keras.layers.experimental.preprocessing.PreprocessingLayer": {"description": "Base class for Preprocessing Layers.", "Attributes": {"is_adapted": "Whether the layer has been fit to data already."}}}, "tf.keras.losses": {"tf.keras.losses.BinaryCrossentropy": {"description": "Computes the cross-entropy loss between true labels and predicted labels.", "Args": {"from_logits": "Whether to interpret y_pred as a tensor of\nlogit values. By default, we\n  assume that y_pred contains probabilities (i.e., values in [0, 1]).", "label_smoothing": "Float in [0, 1]. When 0, no smoothing occurs. When > 0,\nwe compute the loss between the predicted labels and a smoothed version\nof the true labels, where the smoothing squeezes the labels towards 0.5.\nLarger values of label_smoothing correspond to heavier smoothing.", "axis": "The axis along which to compute crossentropy (the features axis).\nDefaults to -1.", "reduction": "Type of tf.keras.losses.Reduction to apply to\nloss. Default value is AUTO. AUTO indicates that the reduction\noption will be determined by the usage context. For almost all cases\nthis defaults to SUM_OVER_BATCH_SIZE. When used with\ntf.distribute.Strategy, outside of built-in training loops such as\ntf.keras compile and fit, using AUTO or SUM_OVER_BATCH_SIZE\nwill raise an error. Please see this custom training tutorial for\n    more details.", "name": "Name for the op. Defaults to 'binary_crossentropy'."}}, "tf.keras.losses.BinaryFocalCrossentropy": {"description": "Computes the focal cross-entropy loss between true labels and predictions.", "Args": {"gamma": "A focusing parameter used to compute the focal factor, default is\n2.0 as mentioned in the reference\nLin et al., 2018.", "from_logits": "Whether to interpret y_pred as a tensor of\nlogit values. By default, we\nassume that y_pred are probabilities (i.e., values in [0, 1]).", "label_smoothing": "Float in [0, 1]. When 0, no smoothing occurs. When >\n0, we compute the loss between the predicted labels and a smoothed\nversion of the true labels, where the smoothing squeezes the labels\ntowards 0.5. Larger values of label_smoothing correspond to heavier\nsmoothing.", "axis": "The axis along which to compute crossentropy (the features axis).\nDefaults to -1.", "reduction": "Type of tf.keras.losses.Reduction to apply to\nloss. Default value is AUTO. AUTO indicates that the reduction\noption will be determined by the usage context. For almost all cases\nthis defaults to SUM_OVER_BATCH_SIZE. When used with\ntf.distribute.Strategy, outside of built-in training loops such as\ntf.keras, compile() and fit(), using SUM_OVER_BATCH_SIZE or\nAUTO will raise an error. Please see this custom training tutorial for\nmore details.", "name": "Name for the op. Defaults to 'binary_focal_crossentropy'."}}, "tf.keras.losses.CategoricalCrossentropy": {"description": "Computes the crossentropy loss between the labels and predictions.", "Args": {"from_logits": "Whether y_pred is expected to be a logits tensor. By\ndefault, we assume that y_pred encodes a probability distribution.", "label_smoothing": "Float in [0, 1]. When > 0, label values are smoothed,\nmeaning the confidence on label values are relaxed. For example, if\n0.1, use 0.1 / num_classes for non-target labels and\n0.9 + 0.1 / num_classes for target labels.", "axis": "The axis along which to compute crossentropy (the features axis).\nDefaults to -1.", "reduction": "Type of tf.keras.losses.Reduction to apply to\nloss. Default value is AUTO. AUTO indicates that the reduction\noption will be determined by the usage context. For almost all cases\nthis defaults to SUM_OVER_BATCH_SIZE. When used with\ntf.distribute.Strategy, outside of built-in training loops such as\ntf.keras compile and fit, using AUTO or SUM_OVER_BATCH_SIZE\nwill raise an error. Please see this custom training tutorial for\n    more details.", "name": "Optional name for the instance.\nDefaults to 'categorical_crossentropy'."}}, "tf.keras.losses.CategoricalHinge": {"description": "Computes the categorical hinge loss between y_true and y_pred.", "Args": {"reduction": "Type of tf.keras.losses.Reduction to apply to\nloss. Default value is AUTO. AUTO indicates that the reduction\noption will be determined by the usage context. For almost all cases\nthis defaults to SUM_OVER_BATCH_SIZE. When used with\ntf.distribute.Strategy, outside of built-in training loops such as\ntf.keras compile and fit, using AUTO or SUM_OVER_BATCH_SIZE\nwill raise an error. Please see this custom training tutorial for\n    more details.", "name": "Optional name for the instance. Defaults to 'categorical_hinge'."}}, "tf.keras.losses.CosineSimilarity": {"description": "Computes the cosine similarity between labels and predictions.", "Args": {"axis": "The axis along which the cosine similarity is computed\n(the features axis). Defaults to -1.", "reduction": "Type of tf.keras.losses.Reduction to apply to loss.\nDefault value is AUTO. AUTO indicates that the reduction option will\nbe determined by the usage context. For almost all cases this defaults to\nSUM_OVER_BATCH_SIZE. When used with tf.distribute.Strategy, outside of\nbuilt-in training loops such as tf.keras compile and fit, using\nAUTO or SUM_OVER_BATCH_SIZE will raise an error. Please see this\ncustom training tutorial for more\n  details.", "name": "Optional name for the instance."}}, "tf.keras.losses.Hinge": {"description": "Computes the hinge loss between y_true and y_pred.", "Args": {"reduction": "Type of tf.keras.losses.Reduction to apply to\nloss. Default value is AUTO. AUTO indicates that the reduction\noption will be determined by the usage context. For almost all cases\nthis defaults to SUM_OVER_BATCH_SIZE. When used with\ntf.distribute.Strategy, outside of built-in training loops such as\ntf.keras compile and fit, using AUTO or SUM_OVER_BATCH_SIZE\nwill raise an error. Please see this custom training tutorial for\n    more details.", "name": "Optional name for the instance. Defaults to 'hinge'."}}, "tf.keras.losses.Huber": {"description": "Computes the Huber loss between y_true and y_pred.", "Args": {"delta": "A float, the point where the Huber loss function changes from a\nquadratic to linear.", "reduction": "Type of tf.keras.losses.Reduction to apply to\nloss. Default value is AUTO. AUTO indicates that the reduction\noption will be determined by the usage context. For almost all cases\nthis defaults to SUM_OVER_BATCH_SIZE. When used with\ntf.distribute.Strategy, outside of built-in training loops such as\ntf.keras compile and fit, using AUTO or SUM_OVER_BATCH_SIZE\nwill raise an error. Please see this custom training tutorial for\n    more details.", "name": "Optional name for the instance. Defaults to 'huber_loss'."}}, "tf.keras.losses.KLDivergence": {"description": "Computes Kullback-Leibler divergence loss between y_true and y_pred.", "Args": {"reduction": "Type of tf.keras.losses.Reduction to apply to\nloss. Default value is AUTO. AUTO indicates that the reduction\noption will be determined by the usage context. For almost all cases\nthis defaults to SUM_OVER_BATCH_SIZE. When used with\ntf.distribute.Strategy, outside of built-in training loops such as\ntf.keras compile and fit, using AUTO or SUM_OVER_BATCH_SIZE\nwill raise an error. Please see this custom training tutorial for\n    more details.", "name": "Optional name for the instance. Defaults to 'kl_divergence'."}}, "tf.keras.losses.LogCosh": {"description": "Computes the logarithm of the hyperbolic cosine of the prediction error.", "Args": {"reduction": "Type of tf.keras.losses.Reduction to apply to\nloss. Default value is AUTO. AUTO indicates that the reduction\noption will be determined by the usage context. For almost all cases\nthis defaults to SUM_OVER_BATCH_SIZE. When used with\ntf.distribute.Strategy, outside of built-in training loops such as\ntf.keras compile and fit, using AUTO or SUM_OVER_BATCH_SIZE\nwill raise an error. Please see this custom training tutorial for\n    more details.", "name": "Optional name for the instance. Defaults to 'log_cosh'."}}, "tf.keras.losses.Loss": {"description": "Loss base class.", "Args": {"reduction": "Type of tf.keras.losses.Reduction to apply to\nloss. Default value is AUTO. AUTO indicates that the reduction\noption will be determined by the usage context. For almost all cases\nthis defaults to SUM_OVER_BATCH_SIZE. When used with\ntf.distribute.Strategy, outside of built-in training loops such as\ntf.keras compile and fit, using AUTO or SUM_OVER_BATCH_SIZE\nwill raise an error. Please see this custom training tutorial for\n    more details.", "name": "Optional name for the instance."}}, "tf.keras.losses.MeanAbsoluteError": {"description": "Computes the mean of absolute difference between labels and predictions.", "Args": {"reduction": "Type of tf.keras.losses.Reduction to apply to\nloss. Default value is AUTO. AUTO indicates that the reduction\noption will be determined by the usage context. For almost all cases\nthis defaults to SUM_OVER_BATCH_SIZE. When used with\ntf.distribute.Strategy, outside of built-in training loops such as\ntf.keras compile and fit, using AUTO or SUM_OVER_BATCH_SIZE\nwill raise an error. Please see this custom training tutorial for\n    more details.", "name": "Optional name for the instance. Defaults to 'mean_absolute_error'."}}, "tf.keras.losses.MeanAbsolutePercentageError": {"description": "Computes the mean absolute percentage error between y_true and y_pred.", "Args": {"reduction": "Type of tf.keras.losses.Reduction to apply to\nloss. Default value is AUTO. AUTO indicates that the reduction\noption will be determined by the usage context. For almost all cases\nthis defaults to SUM_OVER_BATCH_SIZE. When used with\ntf.distribute.Strategy, outside of built-in training loops such as\ntf.keras compile and fit, using AUTO or SUM_OVER_BATCH_SIZE\nwill raise an error. Please see this custom training tutorial for\n    more details.", "name": "Optional name for the instance. Defaults to\n'mean_absolute_percentage_error'."}}, "tf.keras.losses.MeanSquaredError": {"description": "Computes the mean of squares of errors between labels and predictions.", "Args": {"reduction": "Type of tf.keras.losses.Reduction to apply to\nloss. Default value is AUTO. AUTO indicates that the reduction\noption will be determined by the usage context. For almost all cases\nthis defaults to SUM_OVER_BATCH_SIZE. When used with\ntf.distribute.Strategy, outside of built-in training loops such as\ntf.keras compile and fit, using AUTO or SUM_OVER_BATCH_SIZE\nwill raise an error. Please see this custom training tutorial for\n    more details.", "name": "Optional name for the instance. Defaults to 'mean_squared_error'."}}, "tf.keras.losses.MeanSquaredLogarithmicError": {"description": "Computes the mean squared logarithmic error between y_true and y_pred.", "Args": {"reduction": "Type of tf.keras.losses.Reduction to apply to\nloss. Default value is AUTO. AUTO indicates that the reduction\noption will be determined by the usage context. For almost all cases\nthis defaults to SUM_OVER_BATCH_SIZE. When used with\ntf.distribute.Strategy, outside of built-in training loops such as\ntf.keras compile and fit, using AUTO or SUM_OVER_BATCH_SIZE\nwill raise an error. Please see this custom training tutorial for\n    more details.", "name": "Optional name for the instance. Defaults to\n'mean_squared_logarithmic_error'."}}, "tf.keras.losses.Poisson": {"description": "Computes the Poisson loss between y_true and y_pred.", "Args": {"reduction": "Type of tf.keras.losses.Reduction to apply to\nloss. Default value is AUTO. AUTO indicates that the reduction\noption will be determined by the usage context. For almost all cases\nthis defaults to SUM_OVER_BATCH_SIZE. When used with\ntf.distribute.Strategy, outside of built-in training loops such as\ntf.keras compile and fit, using AUTO or SUM_OVER_BATCH_SIZE\nwill raise an error. Please see this custom training tutorial for\n    more details.", "name": "Optional name for the instance. Defaults to 'poisson'."}}, "tf.keras.losses.Reduction": {"description": "Types of loss reduction.", "Class Variables": {"AUTO": "'auto'", "NONE": "'none'", "SUM": "'sum'", "SUM_OVER_BATCH_SIZE": "'sum_over_batch_size'"}}, "tf.keras.losses.SparseCategoricalCrossentropy": {"description": "Computes the crossentropy loss between the labels and predictions.", "Args": {"from_logits": "Whether y_pred is expected to be a logits tensor. By\ndefault, we assume that y_pred encodes a probability distribution.", "reduction": "Type of tf.keras.losses.Reduction to apply to\nloss. Default value is AUTO. AUTO indicates that the reduction\noption will be determined by the usage context. For almost all cases\nthis defaults to SUM_OVER_BATCH_SIZE. When used with\ntf.distribute.Strategy, outside of built-in training loops such as\ntf.keras compile and fit, using AUTO or SUM_OVER_BATCH_SIZE\nwill raise an error. Please see this custom training tutorial for\n    more details.", "name": "Optional name for the instance. Defaults to\n'sparse_categorical_crossentropy'."}}, "tf.keras.losses.SquaredHinge": {"description": "Computes the squared hinge loss between y_true and y_pred.", "Args": {"reduction": "Type of tf.keras.losses.Reduction to apply to\nloss. Default value is AUTO. AUTO indicates that the reduction\noption will be determined by the usage context. For almost all cases\nthis defaults to SUM_OVER_BATCH_SIZE. When used with\ntf.distribute.Strategy, outside of built-in training loops such as\ntf.keras compile and fit, using AUTO or SUM_OVER_BATCH_SIZE\nwill raise an error. Please see this custom training tutorial for\n    more details.", "name": "Optional name for the instance. Defaults to 'squared_hinge'."}}, "tf.keras.losses.categorical_hinge": {"description": "Computes the categorical hinge loss between y_true and y_pred.", "Args": {"y_true": "The ground truth values. y_true values are expected to be\neither {-1, +1} or {0, 1} (i.e. a one-hot-encoded tensor).", "y_pred": "The predicted values."}, "Returns": "Categorical hinge loss values."}, "tf.keras.losses.cosine_similarity": {"description": "Computes the cosine similarity between labels and predictions.", "Args": {"y_true": "Tensor of true targets.", "y_pred": "Tensor of predicted targets.", "axis": "Axis along which to determine similarity."}, "Returns": "Cosine similarity tensor."}, "tf.keras.losses.deserialize": {"description": "Deserializes a serialized loss class/function instance.", "Args": {"name": "Loss configuration.", "custom_objects": "Optional dictionary mapping names (strings) to custom\nobjects (classes and functions) to be considered during deserialization."}, "Returns": "A Keras Loss instance or a loss function."}, "tf.keras.losses.get": {"description": "Retrieves a Keras loss as a function/Loss class instance.", "Args": {"identifier": "A loss identifier. One of None or string name of a loss\nfunction/class or loss configuration dictionary or a loss function or a\nloss class instance."}, "Returns": "A Keras loss as a function/ Loss class instance.", "Raises": {"ValueError": "If identifier cannot be interpreted."}}, "tf.keras.losses.huber": {"description": "Computes Huber loss value.", "Args": {"y_true": "tensor of true targets.", "y_pred": "tensor of predicted targets.", "delta": "A float, the point where the Huber loss function changes from a\nquadratic to linear."}, "Returns": "Tensor with one scalar loss entry per sample."}, "tf.keras.losses.log_cosh": {"description": "Logarithm of the hyperbolic cosine of the prediction error.", "Args": {"y_true": "Ground truth values. shape = [batch_size, d0, .. dN].", "y_pred": "The predicted values. shape = [batch_size, d0, .. dN]."}, "Returns": "Logcosh error values. shape = [batch_size, d0, .. dN-1]."}, "tf.keras.losses.serialize": {"description": "Serializes loss function or Loss instance.", "Args": {"loss": "A Keras Loss instance or a loss function."}, "Returns": "Loss configuration dictionary."}}, "tf.keras.metrics": {"tf.keras.metrics.AUC": {"description": "Approximates the AUC (Area under the curve) of the ROC or PR curves.", "Args": {"num_thresholds": "(Optional) Defaults to 200. The number of thresholds to\nuse when discretizing the roc curve. Values must be > 1.", "curve": "(Optional) Specifies the name of the curve to be computed, 'ROC'\n[default] or 'PR' for the Precision-Recall-curve.", "summation_method": "(Optional) Specifies the Riemann summation method used.\n'interpolation' (default) applies mid-point summation scheme for ROC.\nFor PR-AUC, interpolates (true/false) positives but not the ratio that\nis precision (see Davis & Goadrich 2006 for details);\n'minoring' applies left summation\nfor increasing intervals and right summation for decreasing intervals;\n'majoring' does the opposite.", "name": "(Optional) string name of the metric instance.", "dtype": "(Optional) data type of the metric result.", "thresholds": "(Optional) A list of floating point values to use as the\nthresholds for discretizing the curve. If set, the num_thresholds\nparameter is ignored. Values should be in [0, 1]. Endpoint thresholds\nequal to {-epsilon, 1+epsilon} for a small positive epsilon value will\nbe automatically included with these to correctly handle predictions\nequal to exactly 0 or 1.", "multi_label": "boolean indicating whether multilabel data should be\ntreated as such, wherein AUC is computed separately for each label and\nthen averaged across labels, or (when False) if the data should be\nflattened into a single label before AUC computation. In the latter\ncase, when multilabel data is passed to AUC, each label-prediction pair\nis treated as an individual data point. Should be set to False for\nmulti-class data.", "num_labels": "(Optional) The number of labels, used when multi_label is\nTrue. If num_labels is not specified, then state variables get created\non the first call to update_state.", "label_weights": "(Optional) list, array, or tensor of non-negative weights\nused to compute AUCs for multilabel data. When multi_label is True,\nthe weights are applied to the individual label AUCs when they are\naveraged to produce the multi-label AUC. When it's False, they are used\nto weight the individual label predictions in computing the confusion\nmatrix on the flattened data. Note that this is unlike class_weights in\nthat class_weights weights the example depending on the value of its\nlabel, whereas label_weights depends only on the index of that label\nbefore flattening; therefore label_weights should not be used for\nmulti-class data.", "from_logits": "boolean indicating whether the predictions (y_pred in\nupdate_state) are probabilities or sigmoid logits. As a rule of thumb,\nwhen using a keras loss, the from_logits constructor argument of the\nloss should match the AUC from_logits constructor argument."}, "Attributes": {"thresholds": "The thresholds used for evaluating AUC."}}, "tf.keras.metrics.Accuracy": {"description": "Calculates how often predictions equal labels.", "Args": {"name": "(Optional) string name of the metric instance.", "dtype": "(Optional) data type of the metric result."}}, "tf.keras.metrics.BinaryAccuracy": {"description": "Calculates how often predictions match binary labels.", "Args": {"name": "(Optional) string name of the metric instance.", "dtype": "(Optional) data type of the metric result.", "threshold": "(Optional) Float representing the threshold for deciding\nwhether prediction values are 1 or 0."}}, "tf.keras.metrics.BinaryCrossentropy": {"description": "Computes the crossentropy metric between the labels and predictions.", "Args": {"name": "(Optional) string name of the metric instance.", "dtype": "(Optional) data type of the metric result.", "from_logits": "(Optional )Whether output is expected to be a logits tensor.\nBy default, we consider that output encodes a probability distribution.", "label_smoothing": "(Optional) Float in [0, 1]. When > 0, label values are\nsmoothed, meaning the confidence on label values are relaxed.\ne.g. label_smoothing=0.2 means that we will use a value of 0.1 for\nlabel 0 and 0.9 for label 1\"."}}, "tf.keras.metrics.BinaryIoU": {"description": "Computes the Intersection-Over-Union metric for class 0 and/or 1.", "Args": {"target_class_ids": "A tuple or list of target class ids for which the metric\nis returned. Options are [0], [1], or [0, 1]. With [0] (or [1]),\nthe IoU metric for class 0 (or class 1, respectively) is returned. With\n[0, 1], the mean of IoUs for the two classes is returned.", "threshold": "A threshold that applies to the prediction logits to convert them\nto either predicted class 0 if the logit is below threshold or predicted\nclass 1 if the logit is above threshold.", "name": "(Optional) string name of the metric instance.", "dtype": "(Optional) data type of the metric result."}}, "tf.keras.metrics.CategoricalAccuracy": {"description": "Calculates how often predictions match one-hot labels.", "Args": {"name": "(Optional) string name of the metric instance.", "dtype": "(Optional) data type of the metric result."}}, "tf.keras.metrics.CategoricalCrossentropy": {"description": "Computes the crossentropy metric between the labels and predictions.", "Args": {"name": "(Optional) string name of the metric instance.", "dtype": "(Optional) data type of the metric result.", "from_logits": "(Optional) Whether output is expected to be a logits tensor.\nBy default, we consider that output encodes a probability distribution.", "label_smoothing": "(Optional) Float in [0, 1]. When > 0, label values are\nsmoothed, meaning the confidence on label values are relaxed. e.g.\nlabel_smoothing=0.2 means that we will use a value of 0.1 for label\n0 and 0.9 for label 1\""}}, "tf.keras.metrics.CategoricalHinge": {"description": "Computes the categorical hinge metric between y_true and y_pred.", "Args": {"name": "(Optional) string name of the metric instance.", "dtype": "(Optional) data type of the metric result."}}, "tf.keras.metrics.CosineSimilarity": {"description": "Computes the cosine similarity between the labels and predictions.", "Args": {"name": "(Optional) string name of the metric instance.", "dtype": "(Optional) data type of the metric result.", "axis": "(Optional) Defaults to -1. The dimension along which the cosine\nsimilarity is computed."}}, "tf.keras.metrics.FalseNegatives": {"description": "Calculates the number of false negatives.", "Args": {"thresholds": "(Optional) Defaults to 0.5. A float value or a python\nlist/tuple of float threshold values in [0, 1]. A threshold is compared\nwith prediction values to determine the truth value of predictions\n(i.e., above the threshold is true, below is false). One metric\nvalue is generated for each threshold value.", "name": "(Optional) string name of the metric instance.", "dtype": "(Optional) data type of the metric result."}}, "tf.keras.metrics.FalsePositives": {"description": "Calculates the number of false positives.", "Args": {"thresholds": "(Optional) Defaults to 0.5. A float value or a python\nlist/tuple of float threshold values in [0, 1]. A threshold is compared\nwith prediction values to determine the truth value of predictions\n(i.e., above the threshold is true, below is false). One metric\nvalue is generated for each threshold value.", "name": "(Optional) string name of the metric instance.", "dtype": "(Optional) data type of the metric result."}}, "tf.keras.metrics.Hinge": {"description": "Computes the hinge metric between y_true and y_pred.", "Args": {"name": "(Optional) string name of the metric instance.", "dtype": "(Optional) data type of the metric result."}}, "tf.keras.metrics.IoU": {"description": "Computes the Intersection-Over-Union metric for specific target classes.", "Args": {"num_classes": "The possible number of labels the prediction task can have.\nA confusion matrix of dimension = [num_classes, num_classes] will be\nallocated to accumulate predictions from which the metric is calculated.", "target_class_ids": "A tuple or list of target class ids for which the metric\nis returned. To compute IoU for a specific class, a list (or tuple) of a\nsingle id value should be provided.", "name": "(Optional) string name of the metric instance.", "dtype": "(Optional) data type of the metric result."}}, "tf.keras.metrics.KLDivergence": {"description": "Computes Kullback-Leibler divergence metric between y_true and y_pred.", "Args": {"name": "(Optional) string name of the metric instance.", "dtype": "(Optional) data type of the metric result."}}, "tf.keras.metrics.LogCoshError": {"description": "Computes the logarithm of the hyperbolic cosine of the prediction error.", "Args": {"name": "(Optional) string name of the metric instance.", "dtype": "(Optional) data type of the metric result."}}, "tf.keras.metrics.Mean": {"description": "Computes the (weighted) mean of the given values.", "Args": {"name": "(Optional) string name of the metric instance.", "dtype": "(Optional) data type of the metric result."}}, "tf.keras.metrics.MeanAbsoluteError": {"description": "Computes the mean absolute error between the labels and predictions.", "Args": {"name": "(Optional) string name of the metric instance.", "dtype": "(Optional) data type of the metric result."}}, "tf.keras.metrics.MeanAbsolutePercentageError": {"description": "Computes the mean absolute percentage error between y_true and y_pred.", "Args": {"name": "(Optional) string name of the metric instance.", "dtype": "(Optional) data type of the metric result."}}, "tf.keras.metrics.MeanIoU": {"description": "Computes the mean Intersection-Over-Union metric.", "Args": {"num_classes": "The possible number of labels the prediction task can have.\nThis value must be provided, since a confusion matrix of dimension =\n[num_classes, num_classes] will be allocated.", "name": "(Optional) string name of the metric instance.", "dtype": "(Optional) data type of the metric result."}}, "tf.keras.metrics.MeanMetricWrapper": {"description": "Wraps a stateless metric function with the Mean metric.", "Args": {"fn": "The metric function to wrap, with signature fn(y_true, y_pred,\n**kwargs).", "name": "(Optional) string name of the metric instance.", "dtype": "(Optional) data type of the metric result.", "**kwargs": "Keyword arguments to pass on to fn."}}, "tf.keras.metrics.MeanRelativeError": {"description": "Computes the mean relative error by normalizing with the given values.", "Args": {"normalizer": "The normalizer values with same shape as predictions.", "name": "(Optional) string name of the metric instance.", "dtype": "(Optional) data type of the metric result."}}, "tf.keras.metrics.MeanSquaredError": {"description": "Computes the mean squared error between y_true and y_pred.", "Args": {"name": "(Optional) string name of the metric instance.", "dtype": "(Optional) data type of the metric result."}}, "tf.keras.metrics.MeanSquaredLogarithmicError": {"description": "Computes the mean squared logarithmic error between y_true and y_pred.", "Args": {"name": "(Optional) string name of the metric instance.", "dtype": "(Optional) data type of the metric result."}}, "tf.keras.metrics.MeanTensor": {"description": "Computes the element-wise (weighted) mean of the given tensors.", "Args": {"name": "(Optional) string name of the metric instance.", "dtype": "(Optional) data type of the metric result.", "shape": "(Optional) A list of integers, a tuple of integers, or a 1-D Tensor\nof type int32. If not specified, the shape is inferred from the values at\nthe first call of update_state."}, "Attributes": {"count": "", "total": ""}}, "tf.keras.metrics.Metric": {"description": "Encapsulates metric logic and state.", "Args": {"name": "(Optional) string name of the metric instance.", "dtype": "(Optional) data type of the metric result.", "**kwargs": "Additional layer keywords arguments."}}, "tf.keras.metrics.OneHotIoU": {"description": "Computes the Intersection-Over-Union metric for one-hot encoded labels.", "Args": {"num_classes": "The possible number of labels the prediction task can have.\nA confusion matrix of shape (num_classes, num_classes) will be\nallocated to accumulate predictions from which the metric is calculated.", "target_class_ids": "A tuple or list of target class ids for which the metric\nis returned. To compute IoU for a specific class, a list (or tuple) of a\nsingle id value should be provided.", "name": "(Optional) string name of the metric instance.", "dtype": "(Optional) data type of the metric result."}}, "tf.keras.metrics.OneHotMeanIoU": {"description": "Computes mean Intersection-Over-Union metric for one-hot encoded labels.", "Args": {"num_classes": "The possible number of labels the prediction task can have.\nA confusion matrix of shape (num_classes, num_classes) will be\nallocated to accumulate predictions from which the metric is calculated.", "name": "(Optional) string name of the metric instance.", "dtype": "(Optional) data type of the metric result."}}, "tf.keras.metrics.Poisson": {"description": "Computes the Poisson metric between y_true and y_pred.", "Args": {"name": "(Optional) string name of the metric instance.", "dtype": "(Optional) data type of the metric result."}}, "tf.keras.metrics.Precision": {"description": "Computes the precision of the predictions with respect to the labels.", "Args": {"thresholds": "(Optional) A float value or a python list/tuple of float\nthreshold values in [0, 1]. A threshold is compared with prediction\nvalues to determine the truth value of predictions (i.e., above the\nthreshold is true, below is false). One metric value is generated\nfor each threshold value. If neither thresholds nor top_k are set, the\ndefault is to calculate precision with thresholds=0.5.", "top_k": "(Optional) Unset by default. An int value specifying the top-k\npredictions to consider when calculating precision.", "class_id": "(Optional) Integer class ID for which we want binary metrics.\nThis must be in the half-open interval [0, num_classes), where\nnum_classes is the last dimension of predictions.", "name": "(Optional) string name of the metric instance.", "dtype": "(Optional) data type of the metric result."}}, "tf.keras.metrics.PrecisionAtRecall": {"description": "Computes best precision where recall is &gt;= specified value.", "Args": {"recall": "A scalar value in range [0, 1].", "num_thresholds": "(Optional) Defaults to 200. The number of thresholds to\nuse for matching the given recall.", "class_id": "(Optional) Integer class ID for which we want binary metrics.\nThis must be in the half-open interval [0, num_classes), where\nnum_classes is the last dimension of predictions.", "name": "(Optional) string name of the metric instance.", "dtype": "(Optional) data type of the metric result."}}, "tf.keras.metrics.Recall": {"description": "Computes the recall of the predictions with respect to the labels.", "Args": {"thresholds": "(Optional) A float value or a python list/tuple of float\nthreshold values in [0, 1]. A threshold is compared with prediction\nvalues to determine the truth value of predictions (i.e., above the\nthreshold is true, below is false). One metric value is generated\nfor each threshold value. If neither thresholds nor top_k are set, the\ndefault is to calculate recall with thresholds=0.5.", "top_k": "(Optional) Unset by default. An int value specifying the top-k\npredictions to consider when calculating recall.", "class_id": "(Optional) Integer class ID for which we want binary metrics.\nThis must be in the half-open interval [0, num_classes), where\nnum_classes is the last dimension of predictions.", "name": "(Optional) string name of the metric instance.", "dtype": "(Optional) data type of the metric result."}}, "tf.keras.metrics.RecallAtPrecision": {"description": "Computes best recall where precision is &gt;= specified value.", "Args": {"precision": "A scalar value in range [0, 1].", "num_thresholds": "(Optional) Defaults to 200. The number of thresholds to\nuse for matching the given precision.", "class_id": "(Optional) Integer class ID for which we want binary metrics.\nThis must be in the half-open interval [0, num_classes), where\nnum_classes is the last dimension of predictions.", "name": "(Optional) string name of the metric instance.", "dtype": "(Optional) data type of the metric result."}}, "tf.keras.metrics.RootMeanSquaredError": {"description": "Computes root mean squared error metric between y_true and y_pred."}, "tf.keras.metrics.SensitivityAtSpecificity": {"description": "Computes best sensitivity where specificity is &gt;= specified value.", "Args": {"specificity": "A scalar value in range [0, 1].", "num_thresholds": "(Optional) Defaults to 200. The number of thresholds to\nuse for matching the given specificity.", "class_id": "(Optional) Integer class ID for which we want binary metrics.\nThis must be in the half-open interval [0, num_classes), where\nnum_classes is the last dimension of predictions.", "name": "(Optional) string name of the metric instance.", "dtype": "(Optional) data type of the metric result."}}, "tf.keras.metrics.SparseCategoricalAccuracy": {"description": "Calculates how often predictions match integer labels.", "Args": {"name": "(Optional) string name of the metric instance.", "dtype": "(Optional) data type of the metric result."}}, "tf.keras.metrics.SparseCategoricalCrossentropy": {"description": "Computes the crossentropy metric between the labels and predictions.", "Args": {"name": "(Optional) string name of the metric instance.", "dtype": "(Optional) data type of the metric result.", "from_logits": "(Optional) Whether output is expected to be a logits tensor.\nBy default, we consider that output encodes a probability distribution.", "axis": "(Optional) Defaults to -1. The dimension along which the metric is\ncomputed."}}, "tf.keras.metrics.SparseTopKCategoricalAccuracy": {"description": "Computes how often integer targets are in the top K predictions.", "Args": {"k": "(Optional) Number of top elements to look at for computing accuracy.\nDefaults to 5.", "name": "(Optional) string name of the metric instance.", "dtype": "(Optional) data type of the metric result."}}, "tf.keras.metrics.SpecificityAtSensitivity": {"description": "Computes best specificity where sensitivity is &gt;= specified value.", "Args": {"sensitivity": "A scalar value in range [0, 1].", "num_thresholds": "(Optional) Defaults to 200. The number of thresholds to\nuse for matching the given sensitivity.", "class_id": "(Optional) Integer class ID for which we want binary metrics.\nThis must be in the half-open interval [0, num_classes), where\nnum_classes is the last dimension of predictions.", "name": "(Optional) string name of the metric instance.", "dtype": "(Optional) data type of the metric result."}}, "tf.keras.metrics.SquaredHinge": {"description": "Computes the squared hinge metric between y_true and y_pred.", "Args": {"name": "(Optional) string name of the metric instance.", "dtype": "(Optional) data type of the metric result."}}, "tf.keras.metrics.Sum": {"description": "Computes the (weighted) sum of the given values.", "Args": {"name": "(Optional) string name of the metric instance.", "dtype": "(Optional) data type of the metric result."}}, "tf.keras.metrics.TopKCategoricalAccuracy": {"description": "Computes how often targets are in the top K predictions.", "Args": {"k": "(Optional) Number of top elements to look at for computing accuracy.\nDefaults to 5.", "name": "(Optional) string name of the metric instance.", "dtype": "(Optional) data type of the metric result."}}, "tf.keras.metrics.TrueNegatives": {"description": "Calculates the number of true negatives.", "Args": {"thresholds": "(Optional) Defaults to 0.5. A float value or a python\nlist/tuple of float threshold values in [0, 1]. A threshold is compared\nwith prediction values to determine the truth value of predictions\n(i.e., above the threshold is true, below is false). One metric\nvalue is generated for each threshold value.", "name": "(Optional) string name of the metric instance.", "dtype": "(Optional) data type of the metric result."}}, "tf.keras.metrics.TruePositives": {"description": "Calculates the number of true positives.", "Args": {"thresholds": "(Optional) Defaults to 0.5. A float value or a python\nlist/tuple of float threshold values in [0, 1]. A threshold is compared\nwith prediction values to determine the truth value of predictions\n(i.e., above the threshold is true, below is false). One metric\nvalue is generated for each threshold value.", "name": "(Optional) string name of the metric instance.", "dtype": "(Optional) data type of the metric result."}}, "tf.keras.metrics.binary_accuracy": {"description": "Calculates how often predictions match binary labels.", "Args": {"y_true": "Ground truth values. shape = [batch_size, d0, .. dN].", "y_pred": "The predicted values. shape = [batch_size, d0, .. dN].", "threshold": "(Optional) Float representing the threshold for deciding whether\nprediction values are 1 or 0."}, "Returns": "Binary accuracy values. shape = [batch_size, d0, .. dN-1]"}, "tf.keras.metrics.binary_crossentropy": {"description": "Computes the binary crossentropy loss.", "Args": {"y_true": "Ground truth values. shape = [batch_size, d0, .. dN].", "y_pred": "The predicted values. shape = [batch_size, d0, .. dN].", "from_logits": "Whether y_pred is expected to be a logits tensor. By default,\nwe assume that y_pred encodes a probability distribution.", "label_smoothing": "Float in [0, 1]. If > 0 then smooth the labels by\nsqueezing them towards 0.5 That is, using 1. - 0.5 * label_smoothing\nfor the target class and 0.5 * label_smoothing for the non-target class.", "axis": "The axis along which the mean is computed. Defaults to -1."}, "Returns": "Binary crossentropy loss value. shape = [batch_size, d0, .. dN-1]."}, "tf.keras.metrics.binary_focal_crossentropy": {"description": "Computes the binary focal crossentropy loss.", "Args": {"y_true": "Ground truth values, of shape (batch_size, d0, .. dN).", "y_pred": "The predicted values, of shape (batch_size, d0, .. dN).", "gamma": "A focusing parameter, default is 2.0 as mentioned in the reference.", "from_logits": "Whether y_pred is expected to be a logits tensor. By default,\nwe assume that y_pred encodes a probability distribution.", "label_smoothing": "Float in [0, 1]. If higher than 0 then smooth the labels\nby squeezing them towards 0.5, i.e., using 1. - 0.5 * label_smoothing\nfor the target class and 0.5 * label_smoothing for the non-target class.", "axis": "The axis along which the mean is computed. Defaults to -1."}, "Returns": "Binary focal crossentropy loss value. shape = [batch_size, d0, .. dN-1]."}, "tf.keras.metrics.categorical_accuracy": {"description": "Calculates how often predictions match one-hot labels.", "Args": {"y_true": "One-hot ground truth values.", "y_pred": "The prediction values."}, "Returns": "Categorical accuracy values."}, "tf.keras.metrics.categorical_crossentropy": {"description": "Computes the categorical crossentropy loss.", "Args": {"y_true": "Tensor of one-hot true targets.", "y_pred": "Tensor of predicted targets.", "from_logits": "Whether y_pred is expected to be a logits tensor. By default,\nwe assume that y_pred encodes a probability distribution.", "label_smoothing": "Float in [0, 1]. If > 0 then smooth the labels. For\nexample, if 0.1, use 0.1 / num_classes for non-target labels\nand 0.9 + 0.1 / num_classes for target labels.", "axis": "Defaults to -1. The dimension along which the entropy is\ncomputed."}, "Returns": "Categorical crossentropy loss value."}, "tf.keras.metrics.deserialize": {"description": "Deserializes a serialized metric class/function instance.", "Args": {"config": "Metric configuration.", "custom_objects": "Optional dictionary mapping names (strings) to custom\nobjects (classes and functions) to be considered during deserialization."}, "Returns": "A Keras Metric instance or a metric function."}, "tf.keras.metrics.get": {"description": "Retrieves a Keras metric as a function/Metric class instance.", "Args": {"identifier": "A metric identifier. One of None or string name of a metric\nfunction/class or metric configuration dictionary or a metric function or\na metric class instance"}, "Returns": "A Keras metric as a function/ Metric class instance.", "Raises": {"ValueError": "If identifier cannot be interpreted."}}, "tf.keras.metrics.hinge": {"description": "Computes the hinge loss between y_true and y_pred.", "Args": {"y_true": "The ground truth values. y_true values are expected to be -1 or 1.\nIf binary (0 or 1) labels are provided they will be converted to -1 or 1.\nshape = [batch_size, d0, .. dN].", "y_pred": "The predicted values. shape = [batch_size, d0, .. dN]."}, "Returns": "Hinge loss values. shape = [batch_size, d0, .. dN-1]."}, "tf.keras.metrics.kl_divergence": {"description": "Computes Kullback-Leibler divergence loss between y_true and y_pred.", "Args": {"y_true": "Tensor of true targets.", "y_pred": "Tensor of predicted targets."}, "Returns": "A Tensor with loss.", "Raises": {"TypeError": "If y_true cannot be cast to the y_pred.dtype."}}, "tf.keras.metrics.mean_absolute_error": {"description": "Computes the mean absolute error between labels and predictions.", "Args": {"y_true": "Ground truth values. shape = [batch_size, d0, .. dN].", "y_pred": "The predicted values. shape = [batch_size, d0, .. dN]."}, "Returns": "Mean absolute error values. shape = [batch_size, d0, .. dN-1]."}, "tf.keras.metrics.mean_absolute_percentage_error": {"description": "Computes the mean absolute percentage error between y_true and y_pred.", "Args": {"y_true": "Ground truth values. shape = [batch_size, d0, .. dN].", "y_pred": "The predicted values. shape = [batch_size, d0, .. dN]."}, "Returns": "Mean absolute percentage error values. shape = [batch_size, d0, .. dN-1]."}, "tf.keras.metrics.mean_squared_error": {"description": "Computes the mean squared error between labels and predictions.", "Args": {"y_true": "Ground truth values. shape = [batch_size, d0, .. dN].", "y_pred": "The predicted values. shape = [batch_size, d0, .. dN]."}, "Returns": "Mean squared error values. shape = [batch_size, d0, .. dN-1]."}, "tf.keras.metrics.mean_squared_logarithmic_error": {"description": "Computes the mean squared logarithmic error between y_true and y_pred.", "Args": {"y_true": "Ground truth values. shape = [batch_size, d0, .. dN].", "y_pred": "The predicted values. shape = [batch_size, d0, .. dN]."}, "Returns": "Mean squared logarithmic error values. shape = [batch_size, d0, .. dN-1]."}, "tf.keras.metrics.poisson": {"description": "Computes the Poisson loss between y_true and y_pred.", "Args": {"y_true": "Ground truth values. shape = [batch_size, d0, .. dN].", "y_pred": "The predicted values. shape = [batch_size, d0, .. dN]."}, "Returns": "Poisson loss value. shape = [batch_size, d0, .. dN-1].", "Raises": {"InvalidArgumentError": "If y_true and y_pred have incompatible shapes."}}, "tf.keras.metrics.serialize": {"description": "Serializes metric function or Metric instance.", "Args": {"metric": "A Keras Metric instance or a metric function."}, "Returns": "Metric configuration dictionary."}, "tf.keras.metrics.sparse_categorical_accuracy": {"description": "Calculates how often predictions match integer labels.", "Args": {"y_true": "Integer ground truth values.", "y_pred": "The prediction values."}, "Returns": "Sparse categorical accuracy values."}, "tf.keras.metrics.sparse_categorical_crossentropy": {"description": "Computes the sparse categorical crossentropy loss.", "Args": {"y_true": "Ground truth values.", "y_pred": "The predicted values.", "from_logits": "Whether y_pred is expected to be a logits tensor. By default,\nwe assume that y_pred encodes a probability distribution.", "axis": "Defaults to -1. The dimension along which the entropy is\ncomputed."}, "Returns": "Sparse categorical crossentropy loss value."}, "tf.keras.metrics.sparse_top_k_categorical_accuracy": {"description": "Computes how often integer targets are in the top K predictions.", "Args": {"y_true": "tensor of true targets.", "y_pred": "tensor of predicted targets.", "k": "(Optional) Number of top elements to look at for computing accuracy.\nDefaults to 5."}, "Returns": "Sparse top K categorical accuracy value."}, "tf.keras.metrics.squared_hinge": {"description": "Computes the squared hinge loss between y_true and y_pred.", "Args": {"y_true": "The ground truth values. y_true values are expected to be -1 or 1.\nIf binary (0 or 1) labels are provided we will convert them to -1 or 1.\nshape = [batch_size, d0, .. dN].", "y_pred": "The predicted values. shape = [batch_size, d0, .. dN]."}, "Returns": "Squared hinge loss values. shape = [batch_size, d0, .. dN-1]."}, "tf.keras.metrics.top_k_categorical_accuracy": {"description": "Computes how often targets are in the top K predictions.", "Args": {"y_true": "The ground truth values.", "y_pred": "The prediction values.", "k": "(Optional) Number of top elements to look at for computing accuracy.\nDefaults to 5."}, "Returns": "Top K categorical accuracy value."}}, "tf.keras.mixed_precision": {"tf.keras.mixed_precision.LossScaleOptimizer": {"description": "An optimizer that applies loss scaling to prevent numeric underflow.", "Args": {"inner_optimizer": "The tf.keras.optimizers.Optimizer or\ntf.keras.optimizers.experimental.Optimizer instance to wrap.", "dynamic": "Bool indicating whether dynamic loss scaling is used. Defaults to\nTrue. If True, the loss scale will be dynamically updated over time using\nan algorithm that keeps the loss scale at approximately its optimal value.\nIf False, a single fixed loss scale is used and initial_scale must be\nspecified, which is used as the loss scale. Recommended to keep as True,\nas choosing a fixed loss scale can be tricky. Currently, there is a small\nperformance overhead to dynamic loss scaling compared to fixed loss\nscaling.", "initial_scale": "The initial loss scale. If dynamic is True, this defaults\nto 2 ** 15. If dynamic is False, this must be specified and acts as\nthe sole loss scale, as the loss scale does not change over time. When\ndynamic loss scaling is used, is better for this to be a very high number,\nbecause a loss scale that is too high gets lowered far more quickly than a\nloss scale that is too low gets raised.", "dynamic_growth_steps": "With dynamic loss scaling, every\ndynamic_growth_steps steps with finite gradients, the loss scale is\ndoubled. Defaults to 2000. If a nonfinite gradient is encountered, the\ncount is reset back to zero, gradients are skipped that step, and the loss\nscale is halved. The count can be queried with\nLossScaleOptimizer.dynamic_counter. This argument can only be specified\nif dynamic is True."}, "Attributes": {"dynamic": "Bool indicating whether dynamic loss scaling is used.", "dynamic_counter": "The number of steps since the loss scale was last increased or decreased.\nThis is None if LossScaleOptimizer.dynamic is False.\nThe counter is incremented every step. Once it reaches\nLossScaleOptimizer.dynamic_growth_steps, the loss scale will be doubled\nand the counter will be reset back to zero. If nonfinite gradients are\nencountered, the loss scale will be halved and the counter will be reset\nback to zero.", "dynamic_growth_steps": "The number of steps it takes to increase the loss scale.\nThis is None if LossScaleOptimizer.dynamic is False.\nEvery dynamic_growth_steps consecutive steps with finite gradients, the\nloss scale is increased.", "initial_scale": "The initial loss scale.\nIf LossScaleOptimizer.dynamic is False, this is the same number as\nLossScaleOptimizer.loss_scale, as the loss scale never changes.", "inner_optimizer": "The optimizer that this LossScaleOptimizer is wrapping.", "loss_scale": "The current loss scale as a float32 scalar tensor."}}, "tf.keras.mixed_precision.Policy": {"description": "A dtype policy for a Keras layer.", "Args": {"name": "The policy name, which determines the compute and variable dtypes. Can\nbe any dtype name, such as 'float32' or 'float64', which causes both\nthe compute and variable dtypes will be that dtype. Can also be the string\n'mixed_float16' or 'mixed_bfloat16', which causes the compute dtype to\nbe float16 or bfloat16 and the variable dtype to be float32."}, "Attributes": {"compute_dtype": "The compute dtype of this policy.\nThis is the dtype layers will do their computations in. Typically layers\noutput tensors with the compute dtype as well.\nNote that even if the compute dtype is float16 or bfloat16, hardware devices\nmay not do individual adds, multiplies, and other fundamental operations in\nfloat16 or bfloat16, but instead may do some of them in float32 for numeric\nstability. The compute dtype is the dtype of the inputs and outputs of the\nTensorFlow ops that the layer executes. Internally, many TensorFlow ops will\ndo certain internal calculations in float32 or some other device-internal\nintermediate format with higher precision than float16/bfloat16, to increase\nnumeric stability.\nFor example, a tf.keras.layers.Dense layer, when run on a GPU with a\nfloat16 compute dtype, will pass float16 inputs to tf.linalg.matmul. But,\ntf.linalg.matmul will do use float32 intermediate math. The performance\nbenefit of float16 is still apparent, due to increased memory bandwidth and\nthe fact modern GPUs have specialized hardware for computing matmuls on\nfloat16 inputs while still keeping intermediate computations in float32.", "name": "Returns the name of this policy.", "variable_dtype": "The variable dtype of this policy.\nThis is the dtype layers will create their variables in, unless a layer\nexplicitly chooses a different dtype. If this is different than\nPolicy.compute_dtype, Layers will cast variables to the compute dtype to\navoid type errors.\nVariable regularizers are run in the variable dtype, not the compute dtype."}}, "tf.keras.mixed_precision.global_policy": {"description": "Returns the global dtype policy.", "Returns": "The global Policy."}, "tf.keras.mixed_precision.set_global_policy": {"description": "Sets the global dtype policy.", "Args": {"policy": "A Policy, or a string that will be converted to a Policy. Can also\nbe None, in which case the global policy will be constructed from\ntf.keras.backend.floatx()"}}}, "tf.keras.models": {"tf.keras.models.clone_model": {"description": "Clone a Functional or Sequential Model instance.", "Args": {"model": "Instance of Model\n(could be a Functional model or a Sequential model).", "input_tensors": "optional list of input tensors or InputLayer objects\nto build the model upon. If not provided,\nnew Input objects will be created.", "clone_function": "Callable to be used to clone each layer in the target\nmodel (except InputLayer instances). It takes as argument the layer\ninstance to be cloned, and returns the corresponding layer instance to\nbe used in the model copy. If unspecified, this callable defaults to\nthe following serialization/deserialization function:\nlambda layer: layer.__class__.from_config(layer.get_config()).\nBy passing a custom callable, you can customize your copy of the\nmodel, e.g. by wrapping certain layers of interest (you might want to\nreplace all LSTM instances with equivalent\nBidirectional(LSTM(...)) instances, for example)."}, "Returns": "An instance of Model reproducing the behavior\nof the original model, on top of new inputs tensors,\nusing newly instantiated weights. The cloned model may behave\ndifferently from the original model if a custom clone_function\nmodifies the layer."}, "tf.keras.models.load_model": {"description": "Loads a model saved via model.save().", "Args": {"filepath": "One of the following:\n\nString or pathlib.Path object, path to the saved model\nh5py.File object from which to load the model", "custom_objects": "Optional dictionary mapping names\n(strings) to custom classes or functions to be\nconsidered during deserialization.", "compile": "Boolean, whether to compile the model\nafter loading.", "options": "Optional tf.saved_model.LoadOptions object that specifies\noptions for loading from SavedModel."}, "Returns": "A Keras model instance. If the original model was compiled, and saved with\nthe optimizer, then the returned model will be compiled. Otherwise, the\nmodel will be left uncompiled. In the case that an uncompiled model is\nreturned, a warning is displayed if the compile argument is set to\nTrue.", "Raises": {"ImportError": "if loading from an hdf5 file and h5py is not available.", "IOError": "In case of an invalid savefile."}}, "tf.keras.models.model_from_config": {"description": "Instantiates a Keras model from its config.", "Args": {"config": "Configuration dictionary.", "custom_objects": "Optional dictionary mapping names\n(strings) to custom classes or functions to be\nconsidered during deserialization."}, "Returns": "A Keras model instance (uncompiled).", "Raises": {"TypeError": "if config is not a dictionary."}}, "tf.keras.models.model_from_json": {"description": "Parses a JSON model configuration string and returns a model instance.", "Args": {"json_string": "JSON string encoding a model configuration.", "custom_objects": "Optional dictionary mapping names\n(strings) to custom classes or functions to be\nconsidered during deserialization."}, "Returns": "A Keras model instance (uncompiled)."}, "tf.keras.models.model_from_yaml": {"description": "Parses a yaml model configuration file and returns a model instance.", "Args": {"yaml_string": "YAML string or open file encoding a model configuration.", "custom_objects": "Optional dictionary mapping names\n(strings) to custom classes or functions to be\nconsidered during deserialization."}, "Returns": "A Keras model instance (uncompiled).", "Raises": {"RuntimeError": "announces that the method poses a security risk"}}, "tf.keras.models.save_model": {"description": "Saves a model as a TensorFlow SavedModel or HDF5 file.", "Args": {"model": "Keras model instance to be saved.", "filepath": "One of the following:\n\nString or pathlib.Path object, path where to save the model\nh5py.File object where to save the model", "overwrite": "Whether we should overwrite any existing model at the target\nlocation, or instead ask the user with a manual prompt.", "include_optimizer": "If True, save optimizer's state together.", "save_format": "Either 'tf' or 'h5', indicating whether to save the model\nto Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X, and 'h5'\nin TF 1.X.", "signatures": "Signatures to save with the SavedModel. Applicable to the 'tf'\nformat only. Please see the signatures argument in\ntf.saved_model.save for details.", "options": "(only applies to SavedModel format) tf.saved_model.SaveOptions\nobject that specifies options for saving to SavedModel.", "save_traces": "(only applies to SavedModel format) When enabled, the\nSavedModel will store the function traces for each layer. This\ncan be disabled, so that only the configs of each layer are stored.\nDefaults to True. Disabling this will decrease serialization time and\nreduce file size, but it requires that all custom layers/models\nimplement a get_config() method."}, "Raises": {"ImportError": "If save format is hdf5, and h5py is not available."}}}, "tf.keras.optimizers": {"tf.keras.optimizers.Adadelta": {"description": "Optimizer that implements the Adadelta algorithm.", "Args": {"learning_rate": "Initial value for the learning rate:\neither a floating point value,\nor a tf.keras.optimizers.schedules.LearningRateSchedule instance.\nDefaults to 0.001.\nNote that Adadelta tends to benefit from higher initial learning rate\nvalues compared to other optimizers.\nTo match the exact form in the original paper, use 1.0.", "rho": "A Tensor or a floating point value. The decay rate.", "epsilon": "Small floating point value used to maintain numerical stability.", "name": "Optional name prefix for the operations created when applying\ngradients.  Defaults to \"Adadelta\".", "**kwargs": "keyword arguments. Allowed arguments are clipvalue,\nclipnorm, global_clipnorm.\nIf clipvalue (float) is set, the gradient of each weight\nis clipped to be no higher than this value.\nIf clipnorm (float) is set, the gradient of each weight\nis individually clipped so that its norm is no higher than this value.\nIf global_clipnorm (float) is set the gradient of all weights is\nclipped so that their global norm is no higher than this value."}, "Raises": {"ValueError": "in case of any invalid argument."}}, "tf.keras.optimizers.Adagrad": {"description": "Optimizer that implements the Adagrad algorithm.", "Args": {"learning_rate": "Initial value for the learning rate:\neither a floating point value,\nor a tf.keras.optimizers.schedules.LearningRateSchedule instance.\nDefaults to 0.001.\nNote that Adagrad tends to benefit from higher initial learning rate\nvalues compared to other optimizers.\nTo match the exact form in the original paper, use 1.0.", "initial_accumulator_value": "Floating point value.\nStarting value for the accumulators (per-parameter momentum values).\nMust be non-negative.", "epsilon": "Small floating point value used to maintain numerical stability.", "name": "Optional name prefix for the operations created when applying\ngradients.  Defaults to \"Adagrad\".", "**kwargs": "keyword arguments. Allowed arguments are clipvalue,\nclipnorm, global_clipnorm.\nIf clipvalue (float) is set, the gradient of each weight\nis clipped to be no higher than this value.\nIf clipnorm (float) is set, the gradient of each weight\nis individually clipped so that its norm is no higher than this value.\nIf global_clipnorm (float) is set the gradient of all weights is\nclipped so that their global norm is no higher than this value.."}, "Raises": {"ValueError": "in case of any invalid argument."}}, "tf.keras.optimizers.Adam": {"description": "Optimizer that implements the Adam algorithm.", "Args": {"learning_rate": "A Tensor, floating point value, or a schedule that is a\ntf.keras.optimizers.schedules.LearningRateSchedule, or a callable\nthat takes no arguments and returns the actual value to use, The\nlearning rate. Defaults to 0.001.", "beta_1": "A float value or a constant float tensor, or a callable\nthat takes no arguments and returns the actual value to use. The\nexponential decay rate for the 1st moment estimates. Defaults to 0.9.", "beta_2": "A float value or a constant float tensor, or a callable\nthat takes no arguments and returns the actual value to use, The\nexponential decay rate for the 2nd moment estimates. Defaults to 0.999.", "epsilon": "A small constant for numerical stability. This epsilon is\n\"epsilon hat\" in the Kingma and Ba paper (in the formula just before\nSection 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to\n1e-7.", "amsgrad": "Boolean. Whether to apply AMSGrad variant of this algorithm from\nthe paper \"On the Convergence of Adam and beyond\". Defaults to False.", "name": "Optional name for the operations created when applying gradients.\nDefaults to \"Adam\".", "**kwargs": "keyword arguments. Allowed arguments are clipvalue,\nclipnorm, global_clipnorm.\nIf clipvalue (float) is set, the gradient of each weight\nis clipped to be no higher than this value.\nIf clipnorm (float) is set, the gradient of each weight\nis individually clipped so that its norm is no higher than this value.\nIf global_clipnorm (float) is set the gradient of all weights is\nclipped so that their global norm is no higher than this value."}, "Raises": {"ValueError": "in case of any invalid argument."}}, "tf.keras.optimizers.Adamax": {"description": "Optimizer that implements the Adamax algorithm.", "Args": {"learning_rate": "A Tensor, floating point value, or a schedule that is a\ntf.keras.optimizers.schedules.LearningRateSchedule. The learning rate.", "beta_1": "A float value or a constant float tensor. The exponential decay\nrate for the 1st moment estimates.", "beta_2": "A float value or a constant float tensor. The exponential decay\nrate for the exponentially weighted infinity norm.", "epsilon": "A small constant for numerical stability.", "name": "Optional name for the operations created when applying gradients.\nDefaults to \"Adamax\".", "**kwargs": "keyword arguments. Allowed arguments are clipvalue,\nclipnorm, global_clipnorm.\nIf clipvalue (float) is set, the gradient of each weight\nis clipped to be no higher than this value.\nIf clipnorm (float) is set, the gradient of each weight\nis individually clipped so that its norm is no higher than this value.\nIf global_clipnorm (float) is set the gradient of all weights is\nclipped so that their global norm is no higher than this value."}, "Raises": {"ValueError": "in case of any invalid argument."}}, "tf.keras.optimizers.Ftrl": {"description": "Optimizer that implements the FTRL algorithm.", "Args": {"learning_rate": "A Tensor, floating point value, or a schedule that is a\ntf.keras.optimizers.schedules.LearningRateSchedule. The learning rate.", "learning_rate_power": "A float value, must be less or equal to zero.\nControls how the learning rate decreases during training. Use zero for\na fixed learning rate.", "initial_accumulator_value": "The starting value for accumulators.\nOnly zero or positive values are allowed.", "l1_regularization_strength": "A float value, must be greater than or\nequal to zero. Defaults to 0.0.", "l2_regularization_strength": "A float value, must be greater than or\nequal to zero. Defaults to 0.0.", "name": "Optional name prefix for the operations created when applying\ngradients.  Defaults to \"Ftrl\".", "l2_shrinkage_regularization_strength": "A float value, must be greater than\nor equal to zero. This differs from L2 above in that the L2 above is a\nstabilization penalty, whereas this L2 shrinkage is a magnitude penalty.\nWhen input is sparse shrinkage will only happen on the active weights.", "beta": "A float value, representing the beta value from the paper.\nDefaults to 0.0.", "**kwargs": "keyword arguments. Allowed arguments are clipvalue,\nclipnorm, global_clipnorm.\nIf clipvalue (float) is set, the gradient of each weight\nis clipped to be no higher than this value.\nIf clipnorm (float) is set, the gradient of each weight\nis individually clipped so that its norm is no higher than this value.\nIf global_clipnorm (float) is set the gradient of all weights is\nclipped so that their global norm is no higher than this value."}, "Raises": {"ValueError": "in case of any invalid argument."}}, "tf.keras.optimizers.Nadam": {"description": "Optimizer that implements the NAdam algorithm.", "Args": {"learning_rate": "A Tensor or a floating point value.  The learning rate.", "beta_1": "A float value or a constant float tensor. The exponential decay\nrate for the 1st moment estimates.", "beta_2": "A float value or a constant float tensor. The exponential decay\nrate for the exponentially weighted infinity norm.", "epsilon": "A small constant for numerical stability.", "name": "Optional name for the operations created when applying gradients.\nDefaults to \"Nadam\".", "**kwargs": "keyword arguments. Allowed arguments are clipvalue,\nclipnorm, global_clipnorm.\nIf clipvalue (float) is set, the gradient of each weight\nis clipped to be no higher than this value.\nIf clipnorm (float) is set, the gradient of each weight\nis individually clipped so that its norm is no higher than this value.\nIf global_clipnorm (float) is set the gradient of all weights is\nclipped so that their global norm is no higher than this value."}, "Raises": {"ValueError": "in case of any invalid argument."}}, "tf.keras.optimizers.Optimizer": {"description": "Base class for Keras optimizers.", "Args": {"name": "String. The name to use for momentum accumulator weights created\nby the optimizer.", "gradient_aggregator": "The function to use to aggregate gradients across\ndevices (when using tf.distribute.Strategy). If None, defaults to\nsumming the gradients across devices. The function should accept and\nreturn a list of (gradient, variable) tuples.", "gradient_transformers": "Optional. List of functions to use to transform\ngradients before applying updates to Variables. The functions are\napplied after gradient_aggregator. The functions should accept and\nreturn a list of (gradient, variable) tuples.", "**kwargs": "keyword arguments. Allowed arguments are clipvalue,\nclipnorm, global_clipnorm.\nIf clipvalue (float) is set, the gradient of each weight\nis clipped to be no higher than this value.\nIf clipnorm (float) is set, the gradient of each weight\nis individually clipped so that its norm is no higher than this value.\nIf global_clipnorm (float) is set the gradient of all weights is\nclipped so that their global norm is no higher than this value."}, "Raises": {"ValueError": "in case of any invalid argument."}, "Attributes": {"clipnorm": "float or None. If set, clips gradients to a maximum norm.", "clipvalue": "float or None. If set, clips gradients to a maximum value.", "global_clipnorm": "float or None.\nIf set, clips gradients to a maximum norm.\nCheck tf.clip_by_global_norm for more details.", "iterations": "Variable. The number of training steps this Optimizer has run.", "weights": "Returns variables of this Optimizer based on the order created."}}, "tf.keras.optimizers.RMSprop": {"description": "Optimizer that implements the RMSprop algorithm.", "Args": {"learning_rate": "A Tensor, floating point value, or a schedule that is a\ntf.keras.optimizers.schedules.LearningRateSchedule, or a callable\nthat takes no arguments and returns the actual value to use. The\nlearning rate. Defaults to 0.001.", "rho": "Discounting factor for the history/coming gradient. Defaults to 0.9.", "momentum": "A scalar or a scalar Tensor. Defaults to 0.0.", "epsilon": "A small constant for numerical stability. This epsilon is\n\"epsilon hat\" in the Kingma and Ba paper (in the formula just before\nSection 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to\n1e-7.", "centered": "Boolean. If True, gradients are normalized by the estimated\nvariance of the gradient; if False, by the uncentered second moment.\nSetting this to True may help with training, but is slightly more\nexpensive in terms of computation and memory. Defaults to False.", "name": "Optional name prefix for the operations created when applying\ngradients. Defaults to \"RMSprop\".", "**kwargs": "keyword arguments. Allowed arguments are clipvalue,\nclipnorm, global_clipnorm.\nIf clipvalue (float) is set, the gradient of each weight\nis clipped to be no higher than this value.\nIf clipnorm (float) is set, the gradient of each weight\nis individually clipped so that its norm is no higher than this value.\nIf global_clipnorm (float) is set the gradient of all weights is\nclipped so that their global norm is no higher than this value."}}, "tf.keras.optimizers.SGD": {"description": "Gradient descent (with momentum) optimizer.", "Args": {"learning_rate": "A Tensor, floating point value, or a schedule that is a\ntf.keras.optimizers.schedules.LearningRateSchedule, or a callable\nthat takes no arguments and returns the actual value to use. The\nlearning rate. Defaults to 0.01.", "momentum": "float hyperparameter >= 0 that accelerates gradient descent\nin the relevant\ndirection and dampens oscillations. Defaults to 0, i.e., vanilla gradient\ndescent.", "nesterov": "boolean. Whether to apply Nesterov momentum.\nDefaults to False.", "name": "Optional name prefix for the operations created when applying\ngradients.  Defaults to \"SGD\".", "**kwargs": "keyword arguments. Allowed arguments are clipvalue,\nclipnorm, global_clipnorm.\nIf clipvalue (float) is set, the gradient of each weight\nis clipped to be no higher than this value.\nIf clipnorm (float) is set, the gradient of each weight\nis individually clipped so that its norm is no higher than this value.\nIf global_clipnorm (float) is set the gradient of all weights is\nclipped so that their global norm is no higher than this value."}, "Raises": {"ValueError": "in case of any invalid argument."}}, "tf.keras.optimizers.deserialize": {"description": "Inverse of the serialize function.", "Args": {"config": "Optimizer configuration dictionary.", "custom_objects": "Optional dictionary mapping names (strings) to custom\nobjects (classes and functions) to be considered during deserialization."}, "Returns": "A Keras Optimizer instance."}, "tf.keras.optimizers.get": {"description": "Retrieves a Keras Optimizer instance.", "Args": {"identifier": "Optimizer identifier, one of\n\nString: name of an optimizer\nDictionary: configuration dictionary. - Keras Optimizer instance (it\nwill be returned unchanged). - TensorFlow Optimizer instance (it\nwill be wrapped as a Keras Optimizer)."}, "Returns": "A Keras Optimizer instance.", "Raises": {"ValueError": "If identifier cannot be interpreted."}}, "tf.keras.optimizers.serialize": {"description": "Serialize the optimizer configuration to JSON compatible python dict.", "Args": {"optimizer": "An Optimizer instance to serialize."}, "Returns": "Python dict which contains the configuration of the input optimizer."}}, "tf.keras.optimizers.legacy": {"tf.keras.optimizers.legacy.Adadelta": {"description": "Optimizer that implements the Adadelta algorithm.", "Args": {"learning_rate": "Initial value for the learning rate:\neither a floating point value,\nor a tf.keras.optimizers.schedules.LearningRateSchedule instance.\nDefaults to 0.001.\nNote that Adadelta tends to benefit from higher initial learning rate\nvalues compared to other optimizers.\nTo match the exact form in the original paper, use 1.0.", "rho": "A Tensor or a floating point value. The decay rate.", "epsilon": "Small floating point value used to maintain numerical stability.", "name": "Optional name prefix for the operations created when applying\ngradients.  Defaults to \"Adadelta\".", "**kwargs": "keyword arguments. Allowed arguments are clipvalue,\nclipnorm, global_clipnorm.\nIf clipvalue (float) is set, the gradient of each weight\nis clipped to be no higher than this value.\nIf clipnorm (float) is set, the gradient of each weight\nis individually clipped so that its norm is no higher than this value.\nIf global_clipnorm (float) is set the gradient of all weights is\nclipped so that their global norm is no higher than this value."}, "Raises": {"ValueError": "in case of any invalid argument."}}, "tf.keras.optimizers.legacy.Adagrad": {"description": "Optimizer that implements the Adagrad algorithm.", "Args": {"learning_rate": "Initial value for the learning rate:\neither a floating point value,\nor a tf.keras.optimizers.schedules.LearningRateSchedule instance.\nDefaults to 0.001.\nNote that Adagrad tends to benefit from higher initial learning rate\nvalues compared to other optimizers.\nTo match the exact form in the original paper, use 1.0.", "initial_accumulator_value": "Floating point value.\nStarting value for the accumulators (per-parameter momentum values).\nMust be non-negative.", "epsilon": "Small floating point value used to maintain numerical stability.", "name": "Optional name prefix for the operations created when applying\ngradients.  Defaults to \"Adagrad\".", "**kwargs": "keyword arguments. Allowed arguments are clipvalue,\nclipnorm, global_clipnorm.\nIf clipvalue (float) is set, the gradient of each weight\nis clipped to be no higher than this value.\nIf clipnorm (float) is set, the gradient of each weight\nis individually clipped so that its norm is no higher than this value.\nIf global_clipnorm (float) is set the gradient of all weights is\nclipped so that their global norm is no higher than this value.."}, "Raises": {"ValueError": "in case of any invalid argument."}}, "tf.keras.optimizers.legacy.Adam": {"description": "Optimizer that implements the Adam algorithm.", "Args": {"learning_rate": "A Tensor, floating point value, or a schedule that is a\ntf.keras.optimizers.schedules.LearningRateSchedule, or a callable\nthat takes no arguments and returns the actual value to use, The\nlearning rate. Defaults to 0.001.", "beta_1": "A float value or a constant float tensor, or a callable\nthat takes no arguments and returns the actual value to use. The\nexponential decay rate for the 1st moment estimates. Defaults to 0.9.", "beta_2": "A float value or a constant float tensor, or a callable\nthat takes no arguments and returns the actual value to use, The\nexponential decay rate for the 2nd moment estimates. Defaults to 0.999.", "epsilon": "A small constant for numerical stability. This epsilon is\n\"epsilon hat\" in the Kingma and Ba paper (in the formula just before\nSection 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to\n1e-7.", "amsgrad": "Boolean. Whether to apply AMSGrad variant of this algorithm from\nthe paper \"On the Convergence of Adam and beyond\". Defaults to False.", "name": "Optional name for the operations created when applying gradients.\nDefaults to \"Adam\".", "**kwargs": "keyword arguments. Allowed arguments are clipvalue,\nclipnorm, global_clipnorm.\nIf clipvalue (float) is set, the gradient of each weight\nis clipped to be no higher than this value.\nIf clipnorm (float) is set, the gradient of each weight\nis individually clipped so that its norm is no higher than this value.\nIf global_clipnorm (float) is set the gradient of all weights is\nclipped so that their global norm is no higher than this value."}, "Raises": {"ValueError": "in case of any invalid argument."}}, "tf.keras.optimizers.legacy.Adamax": {"description": "Optimizer that implements the Adamax algorithm.", "Args": {"learning_rate": "A Tensor, floating point value, or a schedule that is a\ntf.keras.optimizers.schedules.LearningRateSchedule. The learning rate.", "beta_1": "A float value or a constant float tensor. The exponential decay\nrate for the 1st moment estimates.", "beta_2": "A float value or a constant float tensor. The exponential decay\nrate for the exponentially weighted infinity norm.", "epsilon": "A small constant for numerical stability.", "name": "Optional name for the operations created when applying gradients.\nDefaults to \"Adamax\".", "**kwargs": "keyword arguments. Allowed arguments are clipvalue,\nclipnorm, global_clipnorm.\nIf clipvalue (float) is set, the gradient of each weight\nis clipped to be no higher than this value.\nIf clipnorm (float) is set, the gradient of each weight\nis individually clipped so that its norm is no higher than this value.\nIf global_clipnorm (float) is set the gradient of all weights is\nclipped so that their global norm is no higher than this value."}, "Raises": {"ValueError": "in case of any invalid argument."}}, "tf.keras.optimizers.legacy.Ftrl": {"description": "Optimizer that implements the FTRL algorithm.", "Args": {"learning_rate": "A Tensor, floating point value, or a schedule that is a\ntf.keras.optimizers.schedules.LearningRateSchedule. The learning rate.", "learning_rate_power": "A float value, must be less or equal to zero.\nControls how the learning rate decreases during training. Use zero for\na fixed learning rate.", "initial_accumulator_value": "The starting value for accumulators.\nOnly zero or positive values are allowed.", "l1_regularization_strength": "A float value, must be greater than or\nequal to zero. Defaults to 0.0.", "l2_regularization_strength": "A float value, must be greater than or\nequal to zero. Defaults to 0.0.", "name": "Optional name prefix for the operations created when applying\ngradients.  Defaults to \"Ftrl\".", "l2_shrinkage_regularization_strength": "A float value, must be greater than\nor equal to zero. This differs from L2 above in that the L2 above is a\nstabilization penalty, whereas this L2 shrinkage is a magnitude penalty.\nWhen input is sparse shrinkage will only happen on the active weights.", "beta": "A float value, representing the beta value from the paper.\nDefaults to 0.0.", "**kwargs": "keyword arguments. Allowed arguments are clipvalue,\nclipnorm, global_clipnorm.\nIf clipvalue (float) is set, the gradient of each weight\nis clipped to be no higher than this value.\nIf clipnorm (float) is set, the gradient of each weight\nis individually clipped so that its norm is no higher than this value.\nIf global_clipnorm (float) is set the gradient of all weights is\nclipped so that their global norm is no higher than this value."}, "Raises": {"ValueError": "in case of any invalid argument."}}, "tf.keras.optimizers.legacy.Nadam": {"description": "Optimizer that implements the NAdam algorithm.", "Args": {"learning_rate": "A Tensor or a floating point value.  The learning rate.", "beta_1": "A float value or a constant float tensor. The exponential decay\nrate for the 1st moment estimates.", "beta_2": "A float value or a constant float tensor. The exponential decay\nrate for the exponentially weighted infinity norm.", "epsilon": "A small constant for numerical stability.", "name": "Optional name for the operations created when applying gradients.\nDefaults to \"Nadam\".", "**kwargs": "keyword arguments. Allowed arguments are clipvalue,\nclipnorm, global_clipnorm.\nIf clipvalue (float) is set, the gradient of each weight\nis clipped to be no higher than this value.\nIf clipnorm (float) is set, the gradient of each weight\nis individually clipped so that its norm is no higher than this value.\nIf global_clipnorm (float) is set the gradient of all weights is\nclipped so that their global norm is no higher than this value."}, "Raises": {"ValueError": "in case of any invalid argument."}}, "tf.keras.optimizers.legacy.Optimizer": {"description": "Base class for Keras optimizers.", "Args": {"name": "String. The name to use for momentum accumulator weights created\nby the optimizer.", "gradient_aggregator": "The function to use to aggregate gradients across\ndevices (when using tf.distribute.Strategy). If None, defaults to\nsumming the gradients across devices. The function should accept and\nreturn a list of (gradient, variable) tuples.", "gradient_transformers": "Optional. List of functions to use to transform\ngradients before applying updates to Variables. The functions are\napplied after gradient_aggregator. The functions should accept and\nreturn a list of (gradient, variable) tuples.", "**kwargs": "keyword arguments. Allowed arguments are clipvalue,\nclipnorm, global_clipnorm.\nIf clipvalue (float) is set, the gradient of each weight\nis clipped to be no higher than this value.\nIf clipnorm (float) is set, the gradient of each weight\nis individually clipped so that its norm is no higher than this value.\nIf global_clipnorm (float) is set the gradient of all weights is\nclipped so that their global norm is no higher than this value."}, "Raises": {"ValueError": "in case of any invalid argument."}}, "tf.keras.optimizers.legacy.RMSprop": {"description": "Optimizer that implements the RMSprop algorithm.", "Args": {"learning_rate": "A Tensor, floating point value, or a schedule that is a\ntf.keras.optimizers.schedules.LearningRateSchedule, or a callable\nthat takes no arguments and returns the actual value to use. The\nlearning rate. Defaults to 0.001.", "rho": "Discounting factor for the history/coming gradient. Defaults to 0.9.", "momentum": "A scalar or a scalar Tensor. Defaults to 0.0.", "epsilon": "A small constant for numerical stability. This epsilon is\n\"epsilon hat\" in the Kingma and Ba paper (in the formula just before\nSection 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to\n1e-7.", "centered": "Boolean. If True, gradients are normalized by the estimated\nvariance of the gradient; if False, by the uncentered second moment.\nSetting this to True may help with training, but is slightly more\nexpensive in terms of computation and memory. Defaults to False.", "name": "Optional name prefix for the operations created when applying\ngradients. Defaults to \"RMSprop\".", "**kwargs": "keyword arguments. Allowed arguments are clipvalue,\nclipnorm, global_clipnorm.\nIf clipvalue (float) is set, the gradient of each weight\nis clipped to be no higher than this value.\nIf clipnorm (float) is set, the gradient of each weight\nis individually clipped so that its norm is no higher than this value.\nIf global_clipnorm (float) is set the gradient of all weights is\nclipped so that their global norm is no higher than this value."}}, "tf.keras.optimizers.legacy.SGD": {"description": "Gradient descent (with momentum) optimizer.", "Args": {"learning_rate": "A Tensor, floating point value, or a schedule that is a\ntf.keras.optimizers.schedules.LearningRateSchedule, or a callable\nthat takes no arguments and returns the actual value to use. The\nlearning rate. Defaults to 0.01.", "momentum": "float hyperparameter >= 0 that accelerates gradient descent\nin the relevant\ndirection and dampens oscillations. Defaults to 0, i.e., vanilla gradient\ndescent.", "nesterov": "boolean. Whether to apply Nesterov momentum.\nDefaults to False.", "name": "Optional name prefix for the operations created when applying\ngradients.  Defaults to \"SGD\".", "**kwargs": "keyword arguments. Allowed arguments are clipvalue,\nclipnorm, global_clipnorm.\nIf clipvalue (float) is set, the gradient of each weight\nis clipped to be no higher than this value.\nIf clipnorm (float) is set, the gradient of each weight\nis individually clipped so that its norm is no higher than this value.\nIf global_clipnorm (float) is set the gradient of all weights is\nclipped so that their global norm is no higher than this value."}, "Raises": {"ValueError": "in case of any invalid argument."}}}, "tf.keras.optimizers.schedules": {"tf.keras.optimizers.schedules.CosineDecay": {"description": "A LearningRateSchedule that uses a cosine decay schedule.", "Returns": "A 1-arg callable learning rate schedule that takes the current optimizer\nstep and outputs the decayed learning rate, a scalar Tensor of the same\ntype as initial_learning_rate.", "Args": {"initial_learning_rate": "A scalar float32 or float64 Tensor or a\nPython number. The initial learning rate.", "decay_steps": "A scalar int32 or int64 Tensor or a Python number.\nNumber of steps to decay over.", "alpha": "A scalar float32 or float64 Tensor or a Python number.\nMinimum learning rate value as a fraction of initial_learning_rate.", "name": "String. Optional name of the operation.  Defaults to 'CosineDecay'."}}, "tf.keras.optimizers.schedules.CosineDecayRestarts": {"description": "A LearningRateSchedule that uses a cosine decay schedule with restarts.", "Returns": "A 1-arg callable learning rate schedule that takes the current optimizer\nstep and outputs the decayed learning rate, a scalar Tensor of the same\ntype as initial_learning_rate.", "Args": {"initial_learning_rate": "A scalar float32 or float64 Tensor or a Python\nnumber. The initial learning rate.", "first_decay_steps": "A scalar int32 or int64 Tensor or a Python\nnumber. Number of steps to decay over.", "t_mul": "A scalar float32 or float64 Tensor or a Python number.\nUsed to derive the number of iterations in the i-th period.", "m_mul": "A scalar float32 or float64 Tensor or a Python number.\nUsed to derive the initial learning rate of the i-th period.", "alpha": "A scalar float32 or float64 Tensor or a Python number.\nMinimum learning rate value as a fraction of the initial_learning_rate.", "name": "String. Optional name of the operation.  Defaults to 'SGDRDecay'."}}, "tf.keras.optimizers.schedules.ExponentialDecay": {"description": "A LearningRateSchedule that uses an exponential decay schedule.", "Returns": "A 1-arg callable learning rate schedule that takes the current optimizer\nstep and outputs the decayed learning rate, a scalar Tensor of the same\ntype as initial_learning_rate.", "Args": {"initial_learning_rate": "A scalar float32 or float64 Tensor or a\nPython number.  The initial learning rate.", "decay_steps": "A scalar int32 or int64 Tensor or a Python number.\nMust be positive.  See the decay computation above.", "decay_rate": "A scalar float32 or float64 Tensor or a\nPython number.  The decay rate.", "staircase": "Boolean.  If True decay the learning rate at discrete\nintervals", "name": "String.  Optional name of the operation.  Defaults to\n'ExponentialDecay'."}}, "tf.keras.optimizers.schedules.InverseTimeDecay": {"description": "A LearningRateSchedule that uses an inverse time decay schedule.", "Returns": "A 1-arg callable learning rate schedule that takes the current optimizer\nstep and outputs the decayed learning rate, a scalar Tensor of the same\ntype as initial_learning_rate.", "Args": {"initial_learning_rate": "A scalar float32 or float64 Tensor or a\nPython number.  The initial learning rate.", "decay_steps": "How often to apply decay.", "decay_rate": "A Python number.  The decay rate.", "staircase": "Whether to apply decay in a discrete staircase, as opposed to\ncontinuous, fashion.", "name": "String.  Optional name of the operation.  Defaults to\n'InverseTimeDecay'."}}, "tf.keras.optimizers.schedules.LearningRateSchedule": {"description": "The learning rate schedule base class."}, "tf.keras.optimizers.schedules.PiecewiseConstantDecay": {"description": "A LearningRateSchedule that uses a piecewise constant decay schedule.", "Returns": "A 1-arg callable learning rate schedule that takes the current optimizer\nstep and outputs the decayed learning rate, a scalar Tensor of the same\ntype as the boundary tensors.\nThe output of the 1-arg function that takes the step\nis values[0] when step <= boundaries[0],\nvalues[1] when step > boundaries[0] and step <= boundaries[1], ...,\nand values[-1] when step > boundaries[-1].", "Args": {"boundaries": "A list of Tensors or ints or floats with strictly\nincreasing entries, and with all elements having the same type as the\noptimizer step.", "values": "A list of Tensors or floats or ints that specifies the\nvalues for the intervals defined by boundaries. It should have one\nmore element than boundaries, and all elements should have the same\ntype.", "name": "A string. Optional name of the operation. Defaults to\n'PiecewiseConstant'."}, "Raises": {"ValueError": "if the number of elements in the lists do not match."}}, "tf.keras.optimizers.schedules.PolynomialDecay": {"description": "A LearningRateSchedule that uses a polynomial decay schedule.", "Returns": "A 1-arg callable learning rate schedule that takes the current optimizer\nstep and outputs the decayed learning rate, a scalar Tensor of the same\ntype as initial_learning_rate.", "Args": {"initial_learning_rate": "A scalar float32 or float64 Tensor or a\nPython number.  The initial learning rate.", "decay_steps": "A scalar int32 or int64 Tensor or a Python number.\nMust be positive.  See the decay computation above.", "end_learning_rate": "A scalar float32 or float64 Tensor or a\nPython number.  The minimal end learning rate.", "power": "A scalar float32 or float64 Tensor or a\nPython number.  The power of the polynomial. Defaults to linear, 1.0.", "cycle": "A boolean, whether or not it should cycle beyond decay_steps.", "name": "String.  Optional name of the operation. Defaults to\n'PolynomialDecay'."}}, "tf.keras.optimizers.schedules.deserialize": {"description": "Instantiates a LearningRateSchedule object from a serialized form.", "Args": {"config": "The serialized form of the LearningRateSchedule.\nDictionary of the form {'class_name': str, 'config': dict}.", "custom_objects": "A dictionary mapping class names (or function names) of\ncustom (non-Keras) objects to class/functions."}, "Returns": "A LearningRateSchedule object."}, "tf.keras.optimizers.schedules.serialize": {"description": "Serializes a LearningRateSchedule into a JSON-compatible representation.", "Args": {"learning_rate_schedule": "The LearningRateSchedule object to serialize."}, "Returns": "A JSON-serializable dict representing the object's config."}}, "tf.keras.preprocessing.image": {"tf.keras.preprocessing.image.DirectoryIterator": {"description": "Iterator capable of reading images from a directory on disk.", "Args": {"directory": "Path to the directory to read images from. Each subdirectory in\nthis directory will be considered to contain images from one class, or\nalternatively you could specify class subdirectories via the classes\nargument.", "image_data_generator": "Instance of ImageDataGenerator to use for random\ntransformations and normalization.", "target_size": "tuple of integers, dimensions to resize input images to.", "color_mode": "One of \"rgb\", \"rgba\", \"grayscale\". Color mode to read\nimages.", "classes": "Optional list of strings, names of subdirectories containing\nimages from each class (e.g. [\"dogs\", \"cats\"]). It will be computed\nautomatically if not set.", "class_mode": "Mode for yielding the targets:\n\n\"binary\": binary targets (if there are only two classes),\n\"categorical\": categorical targets,\n\"sparse\": integer targets,\n\"input\": targets are images identical to input images (mainly used\nto work with autoencoders),\nNone: no targets get yielded (only input images are yielded).", "batch_size": "Integer, size of a batch.", "shuffle": "Boolean, whether to shuffle the data between epochs.", "seed": "Random seed for data shuffling.", "data_format": "String, one of channels_first, channels_last.", "save_to_dir": "Optional directory where to save the pictures being yielded,\nin a viewable format. This is useful for visualizing the random\ntransformations being applied, for debugging purposes.", "save_prefix": "String prefix to use for saving sample images (if\nsave_to_dir is set).", "save_format": "Format to use for saving sample images (if save_to_dir is\nset).", "subset": "Subset of data (\"training\" or \"validation\") if\nvalidation_split is set in ImageDataGenerator.", "interpolation": "Interpolation method used to resample the image if the\ntarget size is different from that of the loaded image. Supported\nmethods are \"nearest\", \"bilinear\", and \"bicubic\". If PIL version 1.1.3\nor newer is installed, \"lanczos\" is also supported. If PIL version 3.4.0\nor newer is installed, \"box\" and \"hamming\" are also supported. By\ndefault, \"nearest\" is used.", "keep_aspect_ratio": "Boolean, whether to resize images to a target size\nwithout aspect ratio distortion. The image is cropped in the center\nwith target aspect ratio before resizing.", "dtype": "Dtype to use for generated arrays."}, "Attributes": {"filepaths": "List of absolute paths to image files.", "labels": "Class labels of every observation.", "sample_weight": ""}, "Class Variables": {"allowed_class_modes": "{\u00a0'binary',\u00a0'categorical',\u00a0'input',\u00a0'sparse',\u00a0None}", "white_list_formats": "('png', 'jpg', 'jpeg', 'bmp', 'ppm', 'tif', 'tiff')"}}, "tf.keras.preprocessing.image.ImageDataGenerator": {"description": "Generate batches of tensor image data with real-time data augmentation.", "Args": {"featurewise_center": "Boolean. Set input mean to 0 over the dataset,\nfeature-wise.", "samplewise_center": "Boolean. Set each sample mean to 0.", "featurewise_std_normalization": "Boolean. Divide inputs by std of the\ndataset, feature-wise.", "samplewise_std_normalization": "Boolean. Divide each input by its std.", "zca_epsilon": "epsilon for ZCA whitening. Default is 1e-6.", "zca_whitening": "Boolean. Apply ZCA whitening.", "rotation_range": "Int. Degree range for random rotations.", "width_shift_range": "Float, 1-D array-like or int\n\nfloat: fraction of total width, if < 1, or pixels if >= 1.\n1-D array-like: random elements from the array.\nint: integer number of pixels from interval (-width_shift_range,\n+width_shift_range) - With width_shift_range=2 possible values\nare integers [-1, 0, +1], same as with width_shift_range=[-1, 0,\n+1], while with width_shift_range=1.0 possible values are floats\nin the interval [-1.0, +1.0).", "height_shift_range": "Float, 1-D array-like or int\nfloat: fraction of total height, if < 1, or pixels if >= 1.\n1-D array-like: random elements from the array.\nint: integer number of pixels from interval (-height_shift_range,\n+height_shift_range) - With height_shift_range=2 possible values\nare integers [-1, 0, +1], same as with height_shift_range=[-1, 0,\n+1], while with height_shift_range=1.0 possible values are floats\nin the interval [-1.0, +1.0).", "brightness_range": "Tuple or list of two floats. Range for picking a\nbrightness shift value from.", "shear_range": "Float. Shear Intensity (Shear angle in counter-clockwise\ndirection in degrees)", "zoom_range": "Float or [lower, upper]. Range for random zoom. If a float,\n[lower, upper] = [1-zoom_range, 1+zoom_range].", "channel_shift_range": "Float. Range for random channel shifts.", "fill_mode": "One of {\"constant\", \"nearest\", \"reflect\" or \"wrap\"}. Default is\n'nearest'. Points outside the boundaries of the input are filled\naccording to the given mode:\n\n'constant': kkkkkkkk|abcd|kkkkkkkk (cval=k)\n'nearest':  aaaaaaaa|abcd|dddddddd\n'reflect':  abcddcba|abcd|dcbaabcd\n'wrap':  abcdabcd|abcd|abcdabcd", "cval": "Float or Int. Value used for points outside the boundaries when\nfill_mode = \"constant\".", "horizontal_flip": "Boolean. Randomly flip inputs horizontally.", "vertical_flip": "Boolean. Randomly flip inputs vertically.", "rescale": "rescaling factor. Defaults to None. If None or 0, no rescaling is\napplied, otherwise we multiply the data by the value provided (after\napplying all other transformations).", "preprocessing_function": "function that will be applied on each input. The\nfunction will run after the image is resized and augmented.\nThe function should take one argument: one image (Numpy tensor with\nrank 3), and should output a Numpy tensor with the same shape.", "data_format": "Image data format, either \"channels_first\" or\n\"channels_last\". \"channels_last\" mode means that the images should have\nshape (samples, height, width, channels), \"channels_first\" mode means\nthat the images should have shape (samples, channels, height, width).\nIt defaults to the image_data_format value found in your Keras config\nfile at ~/.keras/keras.json. If you never set it, then it will be\n\"channels_last\".", "validation_split": "Float. Fraction of images reserved for validation\n(strictly between 0 and 1).", "dtype": "Dtype to use for the generated arrays."}, "Raises": {"ValueError": "If the value of the argument, validation_split > 1\nor validation_split < 0."}}, "tf.keras.preprocessing.image.Iterator": {"description": "Base class for image data iterators.", "Args": {"n": "Integer, total number of samples in the dataset to loop over.", "batch_size": "Integer, size of a batch.", "shuffle": "Boolean, whether to shuffle the data between epochs.", "seed": "Random seeding for data shuffling."}, "Class Variables": {"white_list_formats": "('png', 'jpg', 'jpeg', 'bmp', 'ppm', 'tif', 'tiff')"}}, "tf.keras.preprocessing.image.NumpyArrayIterator": {"description": "Iterator yielding data from a Numpy array.", "Args": {"x": "Numpy array of input data or tuple. If tuple, the second elements is\neither another numpy array or a list of numpy arrays, each of which gets\npassed through as an output without any modifications.", "y": "Numpy array of targets data.", "image_data_generator": "Instance of ImageDataGenerator to use for random\ntransformations and normalization.", "batch_size": "Integer, size of a batch.", "shuffle": "Boolean, whether to shuffle the data between epochs.", "sample_weight": "Numpy array of sample weights.", "seed": "Random seed for data shuffling.", "data_format": "String, one of channels_first, channels_last.", "save_to_dir": "Optional directory where to save the pictures being yielded,\nin a viewable format. This is useful for visualizing the random\ntransformations being applied, for debugging purposes.", "save_prefix": "String prefix to use for saving sample images (if\nsave_to_dir is set).", "save_format": "Format to use for saving sample images (if save_to_dir is\nset).", "subset": "Subset of data (\"training\" or \"validation\") if\nvalidation_split is set in ImageDataGenerator.", "ignore_class_split": "Boolean (default: False), ignore difference\nin number of classes in labels across train and validation\nsplit (useful for non-classification tasks)", "dtype": "Dtype to use for the generated arrays."}, "Class Variables": {"white_list_formats": "('png', 'jpg', 'jpeg', 'bmp', 'ppm', 'tif', 'tiff')"}}, "tf.keras.preprocessing.image.apply_affine_transform": {"description": "Applies an affine transformation specified by the parameters given.", "Args": {"x": "3D numpy array - a 2D image with one or more channels.", "theta": "Rotation angle in degrees.", "tx": "Width shift.", "ty": "Heigh shift.", "shear": "Shear angle in degrees.", "zx": "Zoom in x direction.", "zy": "Zoom in y direction", "row_axis": "Index of axis for rows (aka Y axis) in the input\nimage. Direction: left to right.", "col_axis": "Index of axis for columns (aka X axis) in the input\nimage. Direction: top to bottom.", "channel_axis": "Index of axis for channels in the input image.", "fill_mode": "Points outside the boundaries of the input\nare filled according to the given mode\n(one of {'constant', 'nearest', 'reflect', 'wrap'}).", "cval": "Value used for points outside the boundaries\nof the input if mode='constant'.", "order": "int, order of interpolation"}, "Returns": "The transformed version of the input.", "Raises": {"ImportError": "if SciPy is not available."}}, "tf.keras.preprocessing.image.apply_brightness_shift": {"description": "Performs a brightness shift.", "Args": {"x": "Input tensor. Must be 3D.", "brightness": "Float. The new brightness value.", "scale": "Whether to rescale the image such that minimum and maximum values\nare 0 and 255 respectively. Default: True."}, "Returns": "Numpy image tensor.", "Raises": {"ImportError": "if PIL is not available."}}, "tf.keras.preprocessing.image.apply_channel_shift": {"description": "Performs a channel shift.", "Args": {"x": "Input tensor. Must be 3D.", "intensity": "Transformation intensity.", "channel_axis": "Index of axis for channels in the input tensor."}, "Returns": "Numpy image tensor."}, "tf.keras.preprocessing.image.random_brightness": {"description": "Performs a random brightness shift.", "Args": {"x": "Input tensor. Must be 3D.", "brightness_range": "Tuple of floats; brightness range.", "scale": "Whether to rescale the image such that minimum and maximum values\nare 0 and 255 respectively. Default: True."}, "Returns": "Numpy image tensor.", "Raises": {}}, "tf.keras.preprocessing.image.random_channel_shift": {"description": "Performs a random channel shift.", "Args": {"x": "Input tensor. Must be 3D.", "intensity_range": "Transformation intensity.", "channel_axis": "Index of axis for channels in the input tensor."}, "Returns": "Numpy image tensor."}, "tf.keras.preprocessing.image.random_rotation": {"description": "Performs a random rotation of a Numpy image tensor.", "Args": {"x": "Input tensor. Must be 3D.", "rg": "Rotation range, in degrees.", "row_axis": "Index of axis for rows in the input tensor.", "col_axis": "Index of axis for columns in the input tensor.", "channel_axis": "Index of axis for channels in the input tensor.", "fill_mode": "Points outside the boundaries of the input\nare filled according to the given mode\n(one of {'constant', 'nearest', 'reflect', 'wrap'}).", "cval": "Value used for points outside the boundaries\nof the input if mode='constant'.", "interpolation_order": "int, order of spline interpolation.\nsee ndimage.interpolation.affine_transform"}, "Returns": "Rotated Numpy image tensor."}, "tf.keras.preprocessing.image.random_shear": {"description": "Performs a random spatial shear of a Numpy image tensor.", "Args": {"x": "Input tensor. Must be 3D.", "intensity": "Transformation intensity in degrees.", "row_axis": "Index of axis for rows in the input tensor.", "col_axis": "Index of axis for columns in the input tensor.", "channel_axis": "Index of axis for channels in the input tensor.", "fill_mode": "Points outside the boundaries of the input\nare filled according to the given mode\n(one of {'constant', 'nearest', 'reflect', 'wrap'}).", "cval": "Value used for points outside the boundaries\nof the input if mode='constant'.", "interpolation_order": "int, order of spline interpolation.\nsee ndimage.interpolation.affine_transform"}, "Returns": "Sheared Numpy image tensor."}, "tf.keras.preprocessing.image.random_shift": {"description": "Performs a random spatial shift of a Numpy image tensor.", "Args": {"x": "Input tensor. Must be 3D.", "wrg": "Width shift range, as a float fraction of the width.", "hrg": "Height shift range, as a float fraction of the height.", "row_axis": "Index of axis for rows in the input tensor.", "col_axis": "Index of axis for columns in the input tensor.", "channel_axis": "Index of axis for channels in the input tensor.", "fill_mode": "Points outside the boundaries of the input\nare filled according to the given mode\n(one of {'constant', 'nearest', 'reflect', 'wrap'}).", "cval": "Value used for points outside the boundaries\nof the input if mode='constant'.", "interpolation_order": "int, order of spline interpolation.\nsee ndimage.interpolation.affine_transform"}, "Returns": "Shifted Numpy image tensor."}, "tf.keras.preprocessing.image.random_zoom": {"description": "Performs a random spatial zoom of a Numpy image tensor.", "Args": {"x": "Input tensor. Must be 3D.", "zoom_range": "Tuple of floats; zoom range for width and height.", "row_axis": "Index of axis for rows in the input tensor.", "col_axis": "Index of axis for columns in the input tensor.", "channel_axis": "Index of axis for channels in the input tensor.", "fill_mode": "Points outside the boundaries of the input\nare filled according to the given mode\n(one of {'constant', 'nearest', 'reflect', 'wrap'}).", "cval": "Value used for points outside the boundaries\nof the input if mode='constant'.", "interpolation_order": "int, order of spline interpolation.\nsee ndimage.interpolation.affine_transform"}, "Returns": "Zoomed Numpy image tensor.", "Raises": {"ValueError": "if zoom_range isn't a tuple."}}, "tf.keras.preprocessing.image.smart_resize": {"description": "Resize images to a target size without aspect ratio distortion.", "Args": {"x": "Input image or batch of images (as a tensor or NumPy array). Must be in\nformat (height, width, channels) or (batch_size, height, width,\nchannels).", "size": "Tuple of (height, width) integer. Target size.", "interpolation": "String, interpolation to use for resizing. Defaults to\n'bilinear'. Supports bilinear, nearest, bicubic, area,\nlanczos3, lanczos5, gaussian, mitchellcubic."}, "Returns": "Array with shape (size[0], size[1], channels). If the input image was a\nNumPy array, the output is a NumPy array, and if it was a TF tensor,\nthe output is a TF tensor."}}, "tf.keras.preprocessing.sequence": {"tf.keras.preprocessing.sequence.TimeseriesGenerator": {"description": "Utility class for generating batches of temporal data.", "Returns": "A Sequence\ninstance."}, "tf.keras.preprocessing.sequence.make_sampling_table": {"description": "Generates a word rank-based probabilistic sampling table.", "Args": {"size": "Int, number of possible words to sample.", "sampling_factor": "The sampling factor in the word2vec formula."}, "Returns": "A 1D Numpy array of length size where the ith entry\nis the probability that a word of rank i should be sampled."}, "tf.keras.preprocessing.sequence.skipgrams": {"description": "Generates skipgram word pairs.", "Args": {"sequence": "A word sequence (sentence), encoded as a list\nof word indices (integers). If using a sampling_table,\nword indices are expected to match the rank\nof the words in a reference dataset (e.g. 10 would encode\nthe 10-th most frequently occurring token).\nNote that index 0 is expected to be a non-word and will be skipped.", "vocabulary_size": "Int, maximum possible word index + 1", "window_size": "Int, size of sampling windows (technically half-window).\nThe window of a word w_i will be\n[i - window_size, i + window_size+1].", "negative_samples": "Float >= 0. 0 for no negative (i.e. random) samples.\n1 for same number as positive samples.", "shuffle": "Whether to shuffle the word couples before returning them.", "categorical": "bool. if False, labels will be\nintegers (eg. [0, 1, 1 .. ]),\nif True, labels will be categorical, e.g.\n[[1,0],[0,1],[0,1] .. ].", "sampling_table": "1D array of size vocabulary_size where the entry i\nencodes the probability to sample a word of rank i.", "seed": "Random seed."}, "Returns": "couples, labels: where couples are int pairs and\nlabels are either 0 or 1."}}, "tf.keras.preprocessing.text": {"tf.keras.preprocessing.text.Tokenizer": {"description": "Text tokenization utility class.", "Args": {"num_words": "the maximum number of words to keep, based\non word frequency. Only the most common num_words-1 words will\nbe kept.", "filters": "a string where each element is a character that will be\nfiltered from the texts. The default is all punctuation, plus\ntabs and line breaks, minus the ' character.", "lower": "boolean. Whether to convert the texts to lowercase.", "split": "str. Separator for word splitting.", "char_level": "if True, every character will be treated as a token.", "oov_token": "if given, it will be added to word_index and used to\nreplace out-of-vocabulary words during text_to_sequence calls", "analyzer": "function. Custom analyzer to split the text.\nThe default analyzer is text_to_word_sequence"}}, "tf.keras.preprocessing.text.hashing_trick": {"description": "Converts a text to a sequence of indexes in a fixed-size hashing space.", "Args": {"text": "Input text (string).", "n": "Dimension of the hashing space.", "hash_function": "defaults to python hash function, can be 'md5' or\nany function that takes in input a string and returns a int.\nNote that 'hash' is not a stable hashing function, so\nit is not consistent across different runs, while 'md5'\nis a stable hashing function.", "filters": "list (or concatenation) of characters to filter out, such as\npunctuation. Default: !\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\\\t\\\\n,\nincludes basic punctuation, tabs, and newlines.", "lower": "boolean. Whether to set the text to lowercase.", "split": "str. Separator for word splitting.", "analyzer": "function. Custom analyzer to split the text"}, "Returns": "A list of integer word indices (unicity non-guaranteed).\n0 is a reserved index that won't be assigned to any word.\nTwo or more words may be assigned to the same index, due to possible\ncollisions by the hashing function.\nThe probability\nof a collision is in relation to the dimension of the hashing space and\nthe number of distinct objects."}, "tf.keras.preprocessing.text.one_hot": {"description": "One-hot encodes a text into a list of word indexes of size n.", "Args": {"input_text": "Input text (string).", "n": "int. Size of vocabulary.", "filters": "list (or concatenation) of characters to filter out, such as\npunctuation. Default:\n'!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n,\nincludes basic punctuation, tabs, and newlines.", "lower": "boolean. Whether to set the text to lowercase.", "split": "str. Separator for word splitting.", "analyzer": "function. Custom analyzer to split the text"}, "Returns": "List of integers in [1, n]. Each integer encodes a word\n(unicity non-guaranteed)."}, "tf.keras.preprocessing.text.text_to_word_sequence": {"description": "Converts a text to a sequence of words (or tokens).", "Args": {"input_text": "Input text (string).", "filters": "list (or concatenation) of characters to filter out, such as\npunctuation. Default: '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\\\t\\\\n',\n  includes basic punctuation, tabs, and newlines.", "lower": "boolean. Whether to convert the input to lowercase.", "split": "str. Separator for word splitting."}, "Returns": "A list of words (or tokens)."}, "tf.keras.preprocessing.text.tokenizer_from_json": {"description": "Parses a JSON tokenizer configuration and returns a tokenizer instance.", "Args": {"json_string": "JSON string encoding a tokenizer configuration."}, "Returns": "A Keras Tokenizer instance"}}, "tf.keras.regularizers": {"tf.keras.regularizers.L1": {"description": "A regularizer that applies a L1 regularization penalty."}, "tf.keras.regularizers.L1L2": {"description": "A regularizer that applies both L1 and L2 regularization penalties."}, "tf.keras.regularizers.L2": {"description": "A regularizer that applies a L2 regularization penalty."}, "tf.keras.regularizers.OrthogonalRegularizer": {"description": "A regularizer that encourages input vectors to be orthogonal to each other."}, "tf.keras.regularizers.Regularizer": {"description": "Regularizer base class."}, "tf.keras.regularizers.deserialize": {}, "tf.keras.regularizers.get": {"description": "Retrieve a regularizer instance from a config or identifier."}, "tf.keras.regularizers.l1_l2": {"description": "Create a regularizer that applies both L1 and L2 penalties.", "Args": {"l1": "Float; L1 regularization factor.", "l2": "Float; L2 regularization factor."}, "Returns": "An L1L2 Regularizer with the given regularization factors."}, "tf.keras.regularizers.serialize": {}}, "tf.keras.utils": {"tf.keras.utils.GeneratorEnqueuer": {"description": "Builds a queue out of a data generator.", "Args": {"generator": "a generator function which yields data", "use_multiprocessing": "use multiprocessing if True, otherwise threading", "random_seed": "Initial seed for workers,\nwill be incremented by one for each worker."}}, "tf.keras.utils.OrderedEnqueuer": {"description": "Builds a Enqueuer from a Sequence.", "Args": {"sequence": "A tf.keras.utils.data_utils.Sequence object.", "use_multiprocessing": "use multiprocessing if True, otherwise threading", "shuffle": "whether to shuffle the data at the beginning of each epoch"}}, "tf.keras.utils.Progbar": {"description": "Displays a progress bar.", "Args": {"target": "Total number of steps expected, None if unknown.", "width": "Progress bar width on screen.", "verbose": "Verbosity mode, 0 (silent), 1 (verbose), 2 (semi-verbose)", "stateful_metrics": "Iterable of string names of metrics that should not be\naveraged over time. Metrics in this list will be displayed as-is. All\nothers will be averaged by the progbar before display.", "interval": "Minimum visual progress update interval (in seconds).", "unit_name": "Display name for step counts (usually \"step\" or \"sample\")."}}, "tf.keras.utils.Sequence": {"description": "Base object for fitting to a sequence of data, such as a dataset."}, "tf.keras.utils.SequenceEnqueuer": {"description": "Base class to enqueue inputs."}, "tf.keras.utils.SidecarEvaluator": {"description": "A class designed for a dedicated evaluator task.", "Args": {"model": "Model to use for evaluation. The model object used here should be a\ntf.keras.Model, and should be the same as the one that is used in\ntraining, where tf.keras.Models are checkpointed. The model should\nhave one or more metrics compiled before using SidecarEvaluator.", "data": "The input data for evaluation. SidecarEvaluator supports all data\ntypes that Keras model.evaluate supports as the input data x, such\nas a tf.data.Dataset.", "checkpoint_dir": "Directory where checkpoint files are saved.", "steps": "Number of steps to perform evaluation for, when evaluating a single\ncheckpoint file. If None, evaluation continues until the dataset is\nexhausted. For repeated evaluation dataset, user must specify steps to\navoid infinite evaluation loop.", "max_evaluations": "Maximum number of the checkpoint file to be evaluated,\nfor SidecarEvaluator to know when to stop. The evaluator will stop\nafter it evaluates a checkpoint filepath ending with\n'-'. If using\ntf.train.CheckpointManager.save for saving checkpoints, the kth saved\ncheckpoint has the filepath suffix '-' (k=1 for the first\nsaved), and if checkpoints are saved every epoch after training, the\nfilepath saved at the kth epoch would end with '-. Thus,\nif training runs for n epochs, and the evaluator should end after the\ntraining finishes, use n for this parameter. Note that this is not\nnecessarily equal to the number of total evaluations, since some\ncheckpoints may be skipped if evaluation is slower than checkpoint\ncreation. If None, SidecarEvaluator will evaluate indefinitely, and\nthe user must terminate evaluator program themselves.", "callbacks": "List of keras.callbacks.Callback instances to apply during\nevaluation. See callbacks."}}, "tf.keras.utils.array_to_img": {"description": "Converts a 3D Numpy array to a PIL Image instance.", "Args": {"x": "Input data, in any form that can be converted to a Numpy array.", "data_format": "Image data format, can be either \"channels_first\" or\n\"channels_last\". Defaults to None, in which case the global setting\ntf.keras.backend.image_data_format() is used (unless you changed it,\nit defaults to \"channels_last\").", "scale": "Whether to rescale the image such that minimum and maximum values\nare 0 and 255 respectively. Defaults to True.", "dtype": "Dtype to use. Default to None, in which case the global setting\ntf.keras.backend.floatx() is used (unless you changed it, it defaults\nto \"float32\")"}, "Returns": "A PIL Image instance.", "Raises": {"ImportError": "if PIL is not available.", "ValueError": "if invalid x or data_format is passed."}}, "tf.keras.utils.audio_dataset_from_directory": {"description": "Generates a tf.data.Dataset from audio files in a directory.", "Args": {"directory": "Directory where the data is located. If labels is \"inferred\",\nit should contain subdirectories, each containing audio files for a\nclass. Otherwise, the directory structure is ignored.", "labels": "Either \"inferred\" (labels are generated from the directory\nstructure), None (no labels), or a list/tuple of integer labels of the\nsame size as the number of audio files found in the directory. Labels\nshould be sorted according to the alphanumeric order of the audio file\npaths (obtained via os.walk(directory) in Python).", "label_mode": "String describing the encoding of labels. Options are:\n\n'int': means that the labels are encoded as integers (e.g. for\nsparse_categorical_crossentropy loss). - 'categorical' means that\nthe labels are encoded as a categorical vector (e.g. for\ncategorical_crossentropy loss). - 'binary' means that the labels\n(there can be only 2) are encoded as float32 scalars with values 0\nor 1 (e.g. for binary_crossentropy). - None (no labels).", "class_names": "Only valid if \"labels\" is \"inferred\". This is the explicit\nlist of class names (must match names of subdirectories). Used to\ncontrol the order of the classes (otherwise alphanumerical order is\nused).", "batch_size": "Size of the batches of data. Default: 32. If None, the data\nwill not be batched (the dataset will yield individual samples).", "sampling_rate": "Audio sampling rate (in samples per second).", "output_sequence_length": "Maximum length of an audio sequence. Audio files\nlonger than this will be truncated to output_sequence_length. If set\nto None, then all sequences in the same batch will be padded to the\nlength of the longest sequence in the batch.", "ragged": "Whether to return a Ragged dataset (where each sequence has its\nown length). Default: False.", "shuffle": "Whether to shuffle the data. Default: True. If set to False,\nsorts the data in alphanumeric order.", "seed": "Optional random seed for shuffling and transformations.", "validation_split": "Optional float between 0 and 1, fraction of data to\nreserve for validation.", "subset": "Subset of the data to return. One of \"training\", \"validation\" or\n\"both\". Only used if validation_split is set.", "follow_links": "Whether to visits subdirectories pointed to by symlinks.\nDefaults to False."}, "Returns": "A tf.data.Dataset object.\n\nIf label_mode is None, it yields string tensors of shape\n(batch_size,), containing the contents of a batch of audio files.\nOtherwise, it yields a tuple (audio, labels), where audio\nhas shape (batch_size, sequence_length, num_channels) and labels\nfollows the format described\nbelow."}, "tf.keras.utils.custom_object_scope": {"description": "Exposes custom classes/functions to Keras deserialization internals.", "Args": {"*args": "Dictionary or dictionaries of {name: object} pairs."}}, "tf.keras.utils.deserialize_keras_object": {"description": "Turns the serialized form of a Keras object back into an actual object.", "Args": {"identifier": "the serialized form of the object.", "module_objects": "A dictionary of built-in objects to look the name up in.\nGenerally, module_objects is provided by midlevel library implementers.", "custom_objects": "A dictionary of custom objects to look the name up in.\nGenerally, custom_objects is provided by the end user.", "printable_module_name": "A human-readable string representing the type of the\nobject. Printed in case of exception."}, "Returns": "The deserialized object."}, "tf.keras.utils.disable_interactive_logging": {"description": "Turn off interactive logging."}, "tf.keras.utils.enable_interactive_logging": {"description": "Turn on interactive logging."}, "tf.keras.utils.get_custom_objects": {"description": "Retrieves a live reference to the global dictionary of custom objects.", "Returns": "Global dictionary of names to classes (_GLOBAL_CUSTOM_OBJECTS)."}, "tf.keras.utils.get_file": {"description": "Downloads a file from a URL if it not already in the cache.", "Args": {"fname": "Name of the file. If an absolute path /path/to/file.txt is\nspecified the file will be saved at that location. If None, the\nname of the file at origin will be used.", "origin": "Original URL of the file.", "untar": "Deprecated in favor of extract argument.\nboolean, whether the file should be decompressed", "md5_hash": "Deprecated in favor of file_hash argument.\nmd5 hash of the file for verification", "file_hash": "The expected hash string of the file after download.\nThe sha256 and md5 hash algorithms are both supported.", "cache_subdir": "Subdirectory under the Keras cache dir where the file is\nsaved. If an absolute path /path/to/folder is\nspecified the file will be saved at that location.", "hash_algorithm": "Select the hash algorithm to verify the file.\noptions are 'md5', 'sha256', and 'auto'.\nThe default 'auto' detects the hash algorithm in use.", "extract": "True tries extracting the file as an Archive, like tar or zip.", "archive_format": "Archive format to try for extracting the file.\nOptions are 'auto', 'tar', 'zip', and None.\n'tar' includes tar, tar.gz, and tar.bz files.\nThe default 'auto' corresponds to ['tar', 'zip'].\nNone or an empty list will return no matches found.", "cache_dir": "Location to store cached files, when None it\ndefaults to the default directory ~/.keras/."}, "Returns": "Path to the downloaded file"}, "tf.keras.utils.get_registered_name": {"description": "Returns the name registered to an object within the Keras framework.", "Args": {"obj": "The object to look up."}, "Returns": "The name associated with the object, or the default Python name if the\nobject is not registered."}, "tf.keras.utils.get_registered_object": {"description": "Returns the class associated with name if it is registered with Keras.", "Args": {"name": "The name to look up.", "custom_objects": "A dictionary of custom objects to look the name up in.\nGenerally, custom_objects is provided by the user.", "module_objects": "A dictionary of custom objects to look the name up in.\nGenerally, module_objects is provided by midlevel library implementers."}, "Returns": "An instantiable class associated with 'name', or None if no such class\nexists."}, "tf.keras.utils.get_source_inputs": {"description": "Returns the list of input tensors necessary to compute tensor.", "Args": {"tensor": "The tensor to start from.", "layer": "Origin layer of the tensor. Will be\ndetermined via tensor._keras_history if not provided.", "node_index": "Origin node index of the tensor."}, "Returns": "List of input tensors."}, "tf.keras.utils.image_dataset_from_directory": {"description": "Generates a tf.data.Dataset from image files in a directory.", "Args": {"directory": "Directory where the data is located.\nIf labels is \"inferred\", it should contain\nsubdirectories, each containing images for a class.\nOtherwise, the directory structure is ignored.", "labels": "Either \"inferred\"\n(labels are generated from the directory structure),\nNone (no labels),\nor a list/tuple of integer labels of the same size as the number of\nimage files found in the directory. Labels should be sorted according\nto the alphanumeric order of the image file paths\n(obtained via os.walk(directory) in Python).", "label_mode": "String describing the encoding of labels. Options are:\n\n'int': means that the labels are encoded as integers\n(e.g. for sparse_categorical_crossentropy loss).\n'categorical' means that the labels are\nencoded as a categorical vector\n(e.g. for categorical_crossentropy loss).\n'binary' means that the labels (there can be only 2)\nare encoded as float32 scalars with values 0 or 1\n(e.g. for binary_crossentropy).\nNone (no labels).", "class_names": "Only valid if \"labels\" is \"inferred\". This is the explicit\nlist of class names (must match names of subdirectories). Used\nto control the order of the classes\n(otherwise alphanumerical order is used).", "color_mode": "One of \"grayscale\", \"rgb\", \"rgba\". Default: \"rgb\".\nWhether the images will be converted to\nhave 1, 3, or 4 channels.", "batch_size": "Size of the batches of data. Default: 32.\nIf None, the data will not be batched\n(the dataset will yield individual samples).", "image_size": "Size to resize images to after they are read from disk,\nspecified as (height, width). Defaults to (256, 256).\nSince the pipeline processes batches of images that must all have\nthe same size, this must be provided.", "shuffle": "Whether to shuffle the data. Default: True.\nIf set to False, sorts the data in alphanumeric order.", "seed": "Optional random seed for shuffling and transformations.", "validation_split": "Optional float between 0 and 1,\nfraction of data to reserve for validation.", "subset": "Subset of the data to return.\nOne of \"training\" or \"validation\".\nOnly used if validation_split is set.", "interpolation": "String, the interpolation method used when resizing images.\nDefaults to bilinear. Supports bilinear, nearest, bicubic,\narea, lanczos3, lanczos5, gaussian, mitchellcubic.", "follow_links": "Whether to visits subdirectories pointed to by symlinks.\nDefaults to False.", "crop_to_aspect_ratio": "If True, resize the images without aspect\nratio distortion. When the original aspect ratio differs from the target\naspect ratio, the output image will be cropped so as to return the largest\npossible window in the image (of size image_size) that matches\nthe target aspect ratio. By default (crop_to_aspect_ratio=False),\naspect ratio may not be preserved.", "**kwargs": "Legacy keyword arguments."}, "Returns": "A tf.data.Dataset object.\n\nIf label_mode is None, it yields float32 tensors of shape\n(batch_size, image_size[0], image_size[1], num_channels),\nencoding images (see below for rules regarding num_channels).\nOtherwise, it yields a tuple (images, labels), where images\nhas shape (batch_size, image_size[0], image_size[1], num_channels),\nand labels follows the format described below."}, "tf.keras.utils.img_to_array": {"description": "Converts a PIL Image instance to a Numpy array.", "Args": {"img": "Input PIL Image instance.", "data_format": "Image data format, can be either \"channels_first\" or\n\"channels_last\". Defaults to None, in which case the global setting\ntf.keras.backend.image_data_format() is used (unless you changed it,\nit defaults to \"channels_last\").", "dtype": "Dtype to use. Default to None, in which case the global setting\ntf.keras.backend.floatx() is used (unless you changed it, it defaults\nto \"float32\")."}, "Returns": "A 3D Numpy array.", "Raises": {"ValueError": "if invalid img or data_format is passed."}}, "tf.keras.utils.is_interactive_logging_enabled": {"description": "Check if interactive logging is enabled.", "Returns": "Boolean (True if interactive logging is enabled and False otherwise)."}, "tf.keras.utils.load_img": {"description": "Loads an image into PIL format.", "Args": {"path": "Path to image file.", "grayscale": "DEPRECATED use color_mode=\"grayscale\".", "color_mode": "One of \"grayscale\", \"rgb\", \"rgba\". Default: \"rgb\".\nThe desired image format.", "target_size": "Either None (default to original size) or tuple of ints\n(img_height, img_width).", "interpolation": "Interpolation method used to resample the image if the\ntarget size is different from that of the loaded image. Supported\nmethods are \"nearest\", \"bilinear\", and \"bicubic\". If PIL version\n1.1.3 or newer is installed, \"lanczos\" is also supported. If PIL\nversion 3.4.0 or newer is installed, \"box\" and \"hamming\" are also\nsupported. By default, \"nearest\" is used.", "keep_aspect_ratio": "Boolean, whether to resize images to a target\nsize without aspect ratio distortion. The image is cropped in\nthe center with target aspect ratio before resizing."}, "Returns": "A PIL Image instance.", "Raises": {"ImportError": "if PIL is not available.", "ValueError": "if interpolation method is not supported."}}, "tf.keras.utils.model_to_dot": {"description": "Convert a Keras model to dot format.", "Args": {"model": "A Keras model instance.", "show_shapes": "whether to display shape information.", "show_dtype": "whether to display layer dtypes.", "show_layer_names": "whether to display layer names.", "rankdir": "rankdir argument passed to PyDot,\na string specifying the format of the plot:\n'TB' creates a vertical plot;\n'LR' creates a horizontal plot.", "expand_nested": "whether to expand nested models into clusters.", "dpi": "Dots per inch.", "subgraph": "whether to return a pydot.Cluster instance.", "layer_range": "input of list containing two str items, which is the\nstarting layer name and ending layer name (both inclusive) indicating\nthe range of layers for which the pydot.Dot will be generated. It\nalso accepts regex patterns instead of exact name. In such case, start\npredicate will be the first element it matches to layer_range[0]\nand the end predicate will be the last element it matches to\nlayer_range[1]. By default None which considers all layers of\nmodel. Note that you must pass range such that the resultant subgraph\nmust be complete.", "show_layer_activations": "Display layer activations (only for layers that\nhave an activation property)."}, "Returns": "A pydot.Dot instance representing the Keras model or\na pydot.Cluster instance representing nested model if\nsubgraph=True.", "Raises": {"ValueError": "if model_to_dot is called before the model is built.", "ImportError": "if graphviz or pydot are not available."}}, "tf.keras.utils.normalize": {"description": "Normalizes a Numpy array.", "Args": {"x": "Numpy array to normalize.", "axis": "axis along which to normalize.", "order": "Normalization order (e.g. order=2 for L2 norm)."}, "Returns": "A normalized copy of the array."}, "tf.keras.utils.pack_x_y_sample_weight": {"description": "Packs user-provided data into a tuple.", "Args": {"x": "Features to pass to Model.", "y": "Ground-truth targets to pass to Model.", "sample_weight": "Sample weight for each element."}, "Returns": "Tuple in the format used in Model.fit."}, "tf.keras.utils.pad_sequences": {"description": "Pads sequences to the same length.", "Args": {"sequences": "List of sequences (each sequence is a list of integers).", "maxlen": "Optional Int, maximum length of all sequences. If not provided,\nsequences will be padded to the length of the longest individual\nsequence.", "dtype": "(Optional, defaults to \"int32\"). Type of the output sequences.\nTo pad sequences with variable length strings, you can use object.", "padding": "String, \"pre\" or \"post\" (optional, defaults to \"pre\"):\npad either before or after each sequence.", "truncating": "String, \"pre\" or \"post\" (optional, defaults to \"pre\"):\nremove values from sequences larger than\nmaxlen, either at the beginning or at the end of the sequences.", "value": "Float or String, padding value. (Optional, defaults to 0.)"}, "Returns": "Numpy array with shape (len(sequences), maxlen)", "Raises": {"ValueError": "In case of invalid values for truncating or padding,\nor in case of invalid shape for a sequences entry."}}, "tf.keras.utils.plot_model": {"description": "Converts a Keras model to dot format and save to a file.", "Args": {"model": "A Keras model instance", "to_file": "File name of the plot image.", "show_shapes": "whether to display shape information.", "show_dtype": "whether to display layer dtypes.", "show_layer_names": "whether to display layer names.", "rankdir": "rankdir argument passed to PyDot,\na string specifying the format of the plot: 'TB' creates a vertical\n  plot; 'LR' creates a horizontal plot.", "expand_nested": "Whether to expand nested models into clusters.", "dpi": "Dots per inch.", "layer_range": "input of list containing two str items, which is the\nstarting layer name and ending layer name (both inclusive) indicating the\nrange of layers for which the plot will be generated. It also accepts\nregex patterns instead of exact name. In such case, start predicate will\nbe the first element it matches to layer_range[0] and the end predicate\nwill be the last element it matches to layer_range[1]. By default None\nwhich considers all layers of model. Note that you must pass range such\nthat the resultant subgraph must be complete.", "show_layer_activations": "Display layer activations (only for layers that\nhave an activation property)."}, "Raises": {"ValueError": "if plot_model is called before the model is built."}, "Returns": "A Jupyter notebook Image object if Jupyter is installed.\nThis enables in-line display of the model plots in notebooks."}, "tf.keras.utils.register_keras_serializable": {"description": "Registers an object with the Keras serialization framework.", "Args": {"package": "The package that this class belongs to.", "name": "The name to serialize this class under in this package. If None, the\nclass' name will be used."}, "Returns": "A decorator that registers the decorated class with the passed names."}, "tf.keras.utils.save_img": {"description": "Saves an image stored as a Numpy array to a path or file object.", "Args": {"path": "Path or file object.", "x": "Numpy array.", "data_format": "Image data format, either \"channels_first\" or\n\"channels_last\".", "file_format": "Optional file format override. If omitted, the format to use\nis determined from the filename extension. If a file object was used\ninstead of a filename, this parameter should always be used.", "scale": "Whether to rescale image values to be within [0, 255].", "**kwargs": "Additional keyword arguments passed to PIL.Image.save()."}}, "tf.keras.utils.serialize_keras_object": {"description": "Serialize a Keras object into a JSON-compatible representation.", "Args": {"instance": "The object to serialize."}, "Returns": "A dict-like, JSON-compatible representation of the object's config."}, "tf.keras.utils.set_random_seed": {"description": "Sets all random seeds for the program (Python, NumPy, and TensorFlow)."}, "tf.keras.utils.split_dataset": {"description": "Split a dataset into a left half and a right half (e.g. train / test).", "Args": {"dataset": "A tf.data.Dataset object or a list/tuple of arrays with the\nsame length.", "left_size": "If float, it should be in range [0, 1] range and signifies\nthe fraction of the data to pack in the left dataset. If integer, it\nsignifies the number of samples to pack in the left dataset. If\nNone, it defaults to the complement to right_size.", "right_size": "If float, it should be in range [0, 1] range and signifies\nthe fraction of the data to pack in the right dataset. If integer, it\nsignifies the number of samples to pack in the right dataset. If\nNone, it defaults to the complement to left_size.", "shuffle": "Boolean, whether to shuffle the data before splitting it.", "seed": "A random seed for shuffling."}, "Returns": "A tuple of two tf.data.Dataset objects: the left and right splits."}, "tf.keras.utils.text_dataset_from_directory": {"description": "Generates a tf.data.Dataset from text files in a directory.", "Args": {"directory": "Directory where the data is located.\nIf labels is \"inferred\", it should contain\nsubdirectories, each containing text files for a class.\nOtherwise, the directory structure is ignored.", "labels": "Either \"inferred\"\n(labels are generated from the directory structure),\nNone (no labels),\nor a list/tuple of integer labels of the same size as the number of\ntext files found in the directory. Labels should be sorted according\nto the alphanumeric order of the text file paths\n(obtained via os.walk(directory) in Python).", "label_mode": "String describing the encoding of labels. Options are:\n\n'int': means that the labels are encoded as integers\n(e.g. for sparse_categorical_crossentropy loss).\n'categorical' means that the labels are\nencoded as a categorical vector\n(e.g. for categorical_crossentropy loss).\n'binary' means that the labels (there can be only 2)\nare encoded as float32 scalars with values 0 or 1\n(e.g. for binary_crossentropy).\nNone (no labels).", "class_names": "Only valid if \"labels\" is \"inferred\". This is the explicit\nlist of class names (must match names of subdirectories). Used\nto control the order of the classes\n(otherwise alphanumerical order is used).", "batch_size": "Size of the batches of data. Default: 32.\nIf None, the data will not be batched\n(the dataset will yield individual samples).", "max_length": "Maximum size of a text string. Texts longer than this will\nbe truncated to max_length.", "shuffle": "Whether to shuffle the data. Default: True.\nIf set to False, sorts the data in alphanumeric order.", "seed": "Optional random seed for shuffling and transformations.", "validation_split": "Optional float between 0 and 1,\nfraction of data to reserve for validation.", "subset": "Subset of the data to return.\nOne of \"training\" or \"validation\".\nOnly used if validation_split is set.", "follow_links": "Whether to visits subdirectories pointed to by symlinks.\nDefaults to False."}, "Returns": "A tf.data.Dataset object.\n\nIf label_mode is None, it yields string tensors of shape\n(batch_size,), containing the contents of a batch of text files.\nOtherwise, it yields a tuple (texts, labels), where texts\nhas shape (batch_size,) and labels follows the format described\nbelow."}, "tf.keras.utils.timeseries_dataset_from_array": {"description": "Creates a dataset of sliding windows over a timeseries provided as array.", "Args": {"data": "Numpy array or eager tensor\ncontaining consecutive data points (timesteps).\nAxis 0 is expected to be the time dimension.", "targets": "Targets corresponding to timesteps in data.\ntargets[i] should be the target\ncorresponding to the window that starts at index i\n(see example 2 below).\nPass None if you don't have target data (in this case the dataset will\nonly yield the input data).", "sequence_length": "Length of the output sequences (in number of timesteps).", "sequence_stride": "Period between successive output sequences.\nFor stride s, output samples would\nstart at index data[i], data[i + s], data[i + 2 * s], etc.", "sampling_rate": "Period between successive individual timesteps\nwithin sequences. For rate r, timesteps\ndata[i], data[i + r], ... data[i + sequence_length]\nare used for creating a sample sequence.", "batch_size": "Number of timeseries samples in each batch\n(except maybe the last one). If None, the data will not be batched\n(the dataset will yield individual samples).", "shuffle": "Whether to shuffle output samples,\nor instead draw them in chronological order.", "seed": "Optional int; random seed for shuffling.", "start_index": "Optional int; data points earlier (exclusive)\nthan start_index will not be used\nin the output sequences. This is useful to reserve part of the\ndata for test or validation.", "end_index": "Optional int; data points later (exclusive) than end_index\nwill not be used in the output sequences.\nThis is useful to reserve part of the data for test or validation."}, "Returns": "A tf.data.Dataset instance. If targets was passed, the dataset yields\ntuple (batch_of_sequences, batch_of_targets). If not, the dataset yields\nonly batch_of_sequences."}, "tf.keras.utils.to_categorical": {"description": "Converts a class vector (integers) to binary class matrix.", "Args": {"y": "Array-like with class values to be converted into a matrix\n(integers from 0 to num_classes - 1).", "num_classes": "Total number of classes. If None, this would be inferred\nas max(y) + 1.", "dtype": "The data type expected by the input. Default: 'float32'."}, "Returns": "A binary matrix representation of the input. The class axis is placed\nlast."}, "tf.keras.utils.unpack_x_y_sample_weight": {"description": "Unpacks user-provided data tuple.", "Args": {"data": "A tuple of the form (x,), (x, y), or (x, y, sample_weight)."}, "Returns": "The unpacked tuple, with Nones for y and sample_weight if they are not\nprovided."}}, "tf.keras.wrappers": {}, "tf.keras.wrappers.scikit_learn": {}, "tf.linalg": {"tf.linalg.LinearOperator": {"description": "Base class defining a [batch of] linear operator[s].", "Args": {"dtype": "The type of the this LinearOperator.  Arguments to matmul and\nsolve will have to be this type.", "graph_parents": "(Deprecated) Python list of graph prerequisites of this\nLinearOperator Typically tensors that are passed during initialization", "is_non_singular": "Expect that this operator is non-singular.", "is_self_adjoint": "Expect that this operator is equal to its hermitian\ntranspose.  If dtype is real, this is equivalent to being symmetric.", "is_positive_definite": "Expect that this operator is positive definite,\nmeaning the quadratic form x^H A x has positive real part for all\nnonzero x.  Note that we do not require the operator to be\nself-adjoint to be positive-definite.  See:\nhttps://en.wikipedia.org/wiki/Positive-definite_matrix#Extension_for_non-symmetric_matrices", "is_square": "Expect that this operator acts like square [batch] matrices.", "name": "A name for this LinearOperator.", "parameters": "Python dict of parameters used to instantiate this\nLinearOperator."}, "Raises": {"ValueError": "If hints are set incorrectly."}, "Attributes": {"H": "Returns the adjoint of the current LinearOperator.\nGiven A representing this LinearOperator, return A*.\nNote that calling self.adjoint() and self.H are equivalent.", "batch_shape": "TensorShape of batch dimensions of this LinearOperator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns\nTensorShape([B1,...,Bb]), equivalent to A.shape[:-2]", "domain_dimension": "Dimension (in the sense of vector spaces) of the domain of this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns N.", "dtype": "The DType of Tensors handled by this LinearOperator.", "graph_parents": "List of graph dependencies of this LinearOperator. (deprecated)\nDeprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\nInstructions for updating:\nDo not call graph_parents.", "is_non_singular": "", "is_positive_definite": "", "is_self_adjoint": "", "is_square": "Return True/False depending on if this operator is square.", "parameters": "Dictionary of parameters used to instantiate this LinearOperator.", "range_dimension": "Dimension (in the sense of vector spaces) of the range of this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns M.", "shape": "TensorShape of this LinearOperator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns\nTensorShape([B1,...,Bb, M, N]), equivalent to A.shape.", "tensor_rank": "Rank (in the sense of tensors) of matrix corresponding to this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns b + 2."}}, "tf.linalg.LinearOperatorAdjoint": {"description": "LinearOperator representing the adjoint of another operator.", "Args": {"operator": "LinearOperator object.", "is_non_singular": "Expect that this operator is non-singular.", "is_self_adjoint": "Expect that this operator is equal to its hermitian\ntranspose.", "is_positive_definite": "Expect that this operator is positive definite,\nmeaning the quadratic form x^H A x has positive real part for all\nnonzero x.  Note that we do not require the operator to be\nself-adjoint to be positive-definite.  See:\nhttps://en.wikipedia.org/wiki/Positive-definite_matrix#Extension_for_non-symmetric_matrices", "is_square": "Expect that this operator acts like square [batch] matrices.", "name": "A name for this LinearOperator. Default is operator.name +\n\"_adjoint\"."}, "Raises": {"ValueError": "If operator.is_non_singular is False."}, "Attributes": {"H": "Returns the adjoint of the current LinearOperator.\nGiven A representing this LinearOperator, return A*.\nNote that calling self.adjoint() and self.H are equivalent.", "batch_shape": "TensorShape of batch dimensions of this LinearOperator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns\nTensorShape([B1,...,Bb]), equivalent to A.shape[:-2]", "domain_dimension": "Dimension (in the sense of vector spaces) of the domain of this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns N.", "dtype": "The DType of Tensors handled by this LinearOperator.", "graph_parents": "List of graph dependencies of this LinearOperator. (deprecated)\nDeprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\nInstructions for updating:\nDo not call graph_parents.", "is_non_singular": "", "is_positive_definite": "", "is_self_adjoint": "", "is_square": "Return True/False depending on if this operator is square.", "operator": "The operator before taking the adjoint.", "parameters": "Dictionary of parameters used to instantiate this LinearOperator.", "range_dimension": "Dimension (in the sense of vector spaces) of the range of this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns M.", "shape": "TensorShape of this LinearOperator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns\nTensorShape([B1,...,Bb, M, N]), equivalent to A.shape.", "tensor_rank": "Rank (in the sense of tensors) of matrix corresponding to this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns b + 2."}}, "tf.linalg.LinearOperatorBlockDiag": {"description": "Combines one or more LinearOperators in to a Block Diagonal matrix.", "Args": {"operators": "Iterable of LinearOperator objects, each with\nthe same dtype and composable shape.", "is_non_singular": "Expect that this operator is non-singular.", "is_self_adjoint": "Expect that this operator is equal to its hermitian\ntranspose.", "is_positive_definite": "Expect that this operator is positive definite,\nmeaning the quadratic form x^H A x has positive real part for all\nnonzero x.  Note that we do not require the operator to be\nself-adjoint to be positive-definite.  See:\nhttps://en.wikipedia.org/wiki/Positive-definite_matrix#Extension_for_non-symmetric_matrices", "is_square": "Expect that this operator acts like square [batch] matrices.\nThis is true by default, and will raise a ValueError otherwise.", "name": "A name for this LinearOperator.  Default is the individual\noperators names joined with _o_."}, "Raises": {"TypeError": "If all operators do not have the same dtype.", "ValueError": "If operators is empty or are non-square."}, "Attributes": {"H": "Returns the adjoint of the current LinearOperator.\nGiven A representing this LinearOperator, return A*.\nNote that calling self.adjoint() and self.H are equivalent.", "batch_shape": "TensorShape of batch dimensions of this LinearOperator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns\nTensorShape([B1,...,Bb]), equivalent to A.shape[:-2]", "domain_dimension": "Dimension (in the sense of vector spaces) of the domain of this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns N.", "dtype": "The DType of Tensors handled by this LinearOperator.", "graph_parents": "List of graph dependencies of this LinearOperator. (deprecated)\nDeprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\nInstructions for updating:\nDo not call graph_parents.", "is_non_singular": "", "is_positive_definite": "", "is_self_adjoint": "", "is_square": "Return True/False depending on if this operator is square.", "operators": "", "parameters": "Dictionary of parameters used to instantiate this LinearOperator.", "range_dimension": "Dimension (in the sense of vector spaces) of the range of this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns M.", "shape": "TensorShape of this LinearOperator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns\nTensorShape([B1,...,Bb, M, N]), equivalent to A.shape.", "tensor_rank": "Rank (in the sense of tensors) of matrix corresponding to this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns b + 2."}}, "tf.linalg.LinearOperatorBlockLowerTriangular": {"description": "Combines LinearOperators into a blockwise lower-triangular matrix.", "Args": {"operators": "Iterable of iterables of LinearOperator objects, each with\nthe same dtype. Each element of operators corresponds to a row-\npartition, in top-to-bottom order. The operators in each row-partition\nare filled in left-to-right. For example,\noperators = [[op_0], [op_1, op_2], [op_3, op_4, op_5]] creates a\nLinearOperatorBlockLowerTriangular with full block structure\n[[op_0, 0, 0], [op_1, op_2, 0], [op_3, op_4, op_5]]. The number of\noperators in the ith row must be equal to i, such that each operator\nfalls on or below the diagonal of the blockwise structure.\nLinearOperators that fall on the diagonal (the last elements of each\nrow) must be square. The other LinearOperators must have domain\ndimension equal to the domain dimension of the LinearOperators in the\nsame column-partition, and range dimension equal to the range dimension\nof the LinearOperators in the same row-partition.", "is_non_singular": "Expect that this operator is non-singular.", "is_self_adjoint": "Expect that this operator is equal to its hermitian\ntranspose.", "is_positive_definite": "Expect that this operator is positive definite,\nmeaning the quadratic form x^H A x has positive real part for all\nnonzero x.  Note that we do not require the operator to be\nself-adjoint to be positive-definite.  See:\nhttps://en.wikipedia.org/wiki/Positive-definite_matrix#Extension_for_non-symmetric_matrices", "is_square": "Expect that this operator acts like square [batch] matrices.\nThis will raise a ValueError if set to False.", "name": "A name for this LinearOperator."}, "Raises": {"TypeError": "If all operators do not have the same dtype.", "ValueError": "If operators is empty, contains an erroneous number of\nelements, or contains operators with incompatible shapes."}, "Attributes": {"H": "Returns the adjoint of the current LinearOperator.\nGiven A representing this LinearOperator, return A*.\nNote that calling self.adjoint() and self.H are equivalent.", "batch_shape": "TensorShape of batch dimensions of this LinearOperator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns\nTensorShape([B1,...,Bb]), equivalent to A.shape[:-2]", "domain_dimension": "Dimension (in the sense of vector spaces) of the domain of this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns N.", "dtype": "The DType of Tensors handled by this LinearOperator.", "graph_parents": "List of graph dependencies of this LinearOperator. (deprecated)\nDeprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\nInstructions for updating:\nDo not call graph_parents.", "is_non_singular": "", "is_positive_definite": "", "is_self_adjoint": "", "is_square": "Return True/False depending on if this operator is square.", "operators": "", "parameters": "Dictionary of parameters used to instantiate this LinearOperator.", "range_dimension": "Dimension (in the sense of vector spaces) of the range of this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns M.", "shape": "TensorShape of this LinearOperator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns\nTensorShape([B1,...,Bb, M, N]), equivalent to A.shape.", "tensor_rank": "Rank (in the sense of tensors) of matrix corresponding to this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns b + 2."}}, "tf.linalg.LinearOperatorCirculant": {"description": "LinearOperator acting like a circulant matrix.", "Args": {"spectrum": "Shape [B1,...,Bb, N] Tensor.  Allowed dtypes: float16,\nfloat32, float64, complex64, complex128.  Type can be different\nthan input_output_dtype", "input_output_dtype": "dtype for input/output.", "is_non_singular": "Expect that this operator is non-singular.", "is_self_adjoint": "Expect that this operator is equal to its hermitian\ntranspose.  If spectrum is real, this will always be true.", "is_positive_definite": "Expect that this operator is positive definite,\nmeaning the quadratic form x^H A x has positive real part for all\nnonzero x.  Note that we do not require the operator to be\nself-adjoint to be positive-definite.  See:\nhttps://en.wikipedia.org/wiki/Positive-definite_matrix\n    #Extension_for_non_symmetric_matrices", "is_square": "Expect that this operator acts like square [batch] matrices.", "name": "A name to prepend to all ops created by this class."}, "Attributes": {"H": "Returns the adjoint of the current LinearOperator.\nGiven A representing this LinearOperator, return A*.\nNote that calling self.adjoint() and self.H are equivalent.", "batch_shape": "TensorShape of batch dimensions of this LinearOperator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns\nTensorShape([B1,...,Bb]), equivalent to A.shape[:-2]", "block_depth": "Depth of recursively defined circulant blocks defining this Operator.\nWith A the dense representation of this Operator,\nblock_depth = 1 means A is symmetric circulant.  For example,\nA = |w z y x|\u00a0 \u00a0 |x w z y|\u00a0 \u00a0 |y x w z|\u00a0 \u00a0 |z y x w|\nblock_depth = 2 means A is block symmetric circulant with symmetric\ncirculant blocks.  For example, with W, X, Y, Z symmetric circulant,\nA = |W Z Y X|\u00a0 \u00a0 |X W Z Y|\u00a0 \u00a0 |Y X W Z|\u00a0 \u00a0 |Z Y X W|\nblock_depth = 3 means A is block symmetric circulant with block\nsymmetric circulant blocks.", "block_shape": "", "domain_dimension": "Dimension (in the sense of vector spaces) of the domain of this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns N.", "dtype": "The DType of Tensors handled by this LinearOperator.", "graph_parents": "List of graph dependencies of this LinearOperator. (deprecated)\nDeprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\nInstructions for updating:\nDo not call graph_parents.", "is_non_singular": "", "is_positive_definite": "", "is_self_adjoint": "", "is_square": "Return True/False depending on if this operator is square.", "parameters": "Dictionary of parameters used to instantiate this LinearOperator.", "range_dimension": "Dimension (in the sense of vector spaces) of the range of this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns M.", "shape": "TensorShape of this LinearOperator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns\nTensorShape([B1,...,Bb, M, N]), equivalent to A.shape.", "spectrum": "", "tensor_rank": "Rank (in the sense of tensors) of matrix corresponding to this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns b + 2."}}, "tf.linalg.LinearOperatorCirculant2D": {"description": "LinearOperator acting like a block circulant matrix.", "Args": {"spectrum": "Shape [B1,...,Bb, N] Tensor.  Allowed dtypes: float16,\nfloat32, float64, complex64, complex128.  Type can be different\nthan input_output_dtype", "input_output_dtype": "dtype for input/output.", "is_non_singular": "Expect that this operator is non-singular.", "is_self_adjoint": "Expect that this operator is equal to its hermitian\ntranspose.  If spectrum is real, this will always be true.", "is_positive_definite": "Expect that this operator is positive definite,\nmeaning the quadratic form x^H A x has positive real part for all\nnonzero x.  Note that we do not require the operator to be\nself-adjoint to be positive-definite.  See:\nhttps://en.wikipedia.org/wiki/Positive-definite_matrix\n    #Extension_for_non_symmetric_matrices", "is_square": "Expect that this operator acts like square [batch] matrices.", "name": "A name to prepend to all ops created by this class."}, "Attributes": {"H": "Returns the adjoint of the current LinearOperator.\nGiven A representing this LinearOperator, return A*.\nNote that calling self.adjoint() and self.H are equivalent.", "batch_shape": "TensorShape of batch dimensions of this LinearOperator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns\nTensorShape([B1,...,Bb]), equivalent to A.shape[:-2]", "block_depth": "Depth of recursively defined circulant blocks defining this Operator.\nWith A the dense representation of this Operator,\nblock_depth = 1 means A is symmetric circulant.  For example,\nA = |w z y x|\u00a0 \u00a0 |x w z y|\u00a0 \u00a0 |y x w z|\u00a0 \u00a0 |z y x w|\nblock_depth = 2 means A is block symmetric circulant with symmetric\ncirculant blocks.  For example, with W, X, Y, Z symmetric circulant,\nA = |W Z Y X|\u00a0 \u00a0 |X W Z Y|\u00a0 \u00a0 |Y X W Z|\u00a0 \u00a0 |Z Y X W|\nblock_depth = 3 means A is block symmetric circulant with block\nsymmetric circulant blocks.", "block_shape": "", "domain_dimension": "Dimension (in the sense of vector spaces) of the domain of this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns N.", "dtype": "The DType of Tensors handled by this LinearOperator.", "graph_parents": "List of graph dependencies of this LinearOperator. (deprecated)\nDeprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\nInstructions for updating:\nDo not call graph_parents.", "is_non_singular": "", "is_positive_definite": "", "is_self_adjoint": "", "is_square": "Return True/False depending on if this operator is square.", "parameters": "Dictionary of parameters used to instantiate this LinearOperator.", "range_dimension": "Dimension (in the sense of vector spaces) of the range of this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns M.", "shape": "TensorShape of this LinearOperator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns\nTensorShape([B1,...,Bb, M, N]), equivalent to A.shape.", "spectrum": "", "tensor_rank": "Rank (in the sense of tensors) of matrix corresponding to this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns b + 2."}}, "tf.linalg.LinearOperatorCirculant3D": {"description": "LinearOperator acting like a nested block circulant matrix.", "Args": {"spectrum": "Shape [B1,...,Bb, N] Tensor.  Allowed dtypes: float16,\nfloat32, float64, complex64, complex128.  Type can be different\nthan input_output_dtype", "input_output_dtype": "dtype for input/output.", "is_non_singular": "Expect that this operator is non-singular.", "is_self_adjoint": "Expect that this operator is equal to its hermitian\ntranspose.  If spectrum is real, this will always be true.", "is_positive_definite": "Expect that this operator is positive definite,\nmeaning the real part of all eigenvalues is positive.  We do not require\nthe operator to be self-adjoint to be positive-definite.  See:\nhttps://en.wikipedia.org/wiki/Positive-definite_matrix\n    #Extension_for_non_symmetric_matrices", "is_square": "Expect that this operator acts like square [batch] matrices.", "name": "A name to prepend to all ops created by this class."}, "Attributes": {"H": "Returns the adjoint of the current LinearOperator.\nGiven A representing this LinearOperator, return A*.\nNote that calling self.adjoint() and self.H are equivalent.", "batch_shape": "TensorShape of batch dimensions of this LinearOperator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns\nTensorShape([B1,...,Bb]), equivalent to A.shape[:-2]", "block_depth": "Depth of recursively defined circulant blocks defining this Operator.\nWith A the dense representation of this Operator,\nblock_depth = 1 means A is symmetric circulant.  For example,\nA = |w z y x|\u00a0 \u00a0 |x w z y|\u00a0 \u00a0 |y x w z|\u00a0 \u00a0 |z y x w|\nblock_depth = 2 means A is block symmetric circulant with symmetric\ncirculant blocks.  For example, with W, X, Y, Z symmetric circulant,\nA = |W Z Y X|\u00a0 \u00a0 |X W Z Y|\u00a0 \u00a0 |Y X W Z|\u00a0 \u00a0 |Z Y X W|\nblock_depth = 3 means A is block symmetric circulant with block\nsymmetric circulant blocks.", "block_shape": "", "domain_dimension": "Dimension (in the sense of vector spaces) of the domain of this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns N.", "dtype": "The DType of Tensors handled by this LinearOperator.", "graph_parents": "List of graph dependencies of this LinearOperator. (deprecated)\nDeprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\nInstructions for updating:\nDo not call graph_parents.", "is_non_singular": "", "is_positive_definite": "", "is_self_adjoint": "", "is_square": "Return True/False depending on if this operator is square.", "parameters": "Dictionary of parameters used to instantiate this LinearOperator.", "range_dimension": "Dimension (in the sense of vector spaces) of the range of this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns M.", "shape": "TensorShape of this LinearOperator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns\nTensorShape([B1,...,Bb, M, N]), equivalent to A.shape.", "spectrum": "", "tensor_rank": "Rank (in the sense of tensors) of matrix corresponding to this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns b + 2."}}, "tf.linalg.LinearOperatorComposition": {"description": "Composes one or more LinearOperators.", "Args": {"operators": "Iterable of LinearOperator objects, each with\nthe same dtype and composable shape.", "is_non_singular": "Expect that this operator is non-singular.", "is_self_adjoint": "Expect that this operator is equal to its hermitian\ntranspose.", "is_positive_definite": "Expect that this operator is positive definite,\nmeaning the quadratic form x^H A x has positive real part for all\nnonzero x.  Note that we do not require the operator to be\nself-adjoint to be positive-definite.  See:\nhttps://en.wikipedia.org/wiki/Positive-definite_matrix#Extension_for_non-symmetric_matrices", "is_square": "Expect that this operator acts like square [batch] matrices.", "name": "A name for this LinearOperator.  Default is the individual\noperators names joined with _o_."}, "Raises": {"TypeError": "If all operators do not have the same dtype.", "ValueError": "If operators is empty."}, "Attributes": {"H": "Returns the adjoint of the current LinearOperator.\nGiven A representing this LinearOperator, return A*.\nNote that calling self.adjoint() and self.H are equivalent.", "batch_shape": "TensorShape of batch dimensions of this LinearOperator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns\nTensorShape([B1,...,Bb]), equivalent to A.shape[:-2]", "domain_dimension": "Dimension (in the sense of vector spaces) of the domain of this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns N.", "dtype": "The DType of Tensors handled by this LinearOperator.", "graph_parents": "List of graph dependencies of this LinearOperator. (deprecated)\nDeprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\nInstructions for updating:\nDo not call graph_parents.", "is_non_singular": "", "is_positive_definite": "", "is_self_adjoint": "", "is_square": "Return True/False depending on if this operator is square.", "operators": "", "parameters": "Dictionary of parameters used to instantiate this LinearOperator.", "range_dimension": "Dimension (in the sense of vector spaces) of the range of this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns M.", "shape": "TensorShape of this LinearOperator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns\nTensorShape([B1,...,Bb, M, N]), equivalent to A.shape.", "tensor_rank": "Rank (in the sense of tensors) of matrix corresponding to this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns b + 2."}}, "tf.linalg.LinearOperatorDiag": {"description": "LinearOperator acting like a [batch] square diagonal matrix.", "Args": {"diag": "Shape [B1,...,Bb, N] Tensor with b >= 0 N >= 0.\nThe diagonal of the operator.  Allowed dtypes: float16, float32,\n  float64, complex64, complex128.", "is_non_singular": "Expect that this operator is non-singular.", "is_self_adjoint": "Expect that this operator is equal to its hermitian\ntranspose.  If diag.dtype is real, this is auto-set to True.", "is_positive_definite": "Expect that this operator is positive definite,\nmeaning the quadratic form x^H A x has positive real part for all\nnonzero x.  Note that we do not require the operator to be\nself-adjoint to be positive-definite.  See:\nhttps://en.wikipedia.org/wiki/Positive-definite_matrix#Extension_for_non-symmetric_matrices", "is_square": "Expect that this operator acts like square [batch] matrices.", "name": "A name for this LinearOperator."}, "Raises": {"TypeError": "If diag.dtype is not an allowed type.", "ValueError": "If diag.dtype is real, and is_self_adjoint is not True."}, "Attributes": {"H": "Returns the adjoint of the current LinearOperator.\nGiven A representing this LinearOperator, return A*.\nNote that calling self.adjoint() and self.H are equivalent.", "batch_shape": "TensorShape of batch dimensions of this LinearOperator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns\nTensorShape([B1,...,Bb]), equivalent to A.shape[:-2]", "diag": "", "domain_dimension": "Dimension (in the sense of vector spaces) of the domain of this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns N.", "dtype": "The DType of Tensors handled by this LinearOperator.", "graph_parents": "List of graph dependencies of this LinearOperator. (deprecated)\nDeprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\nInstructions for updating:\nDo not call graph_parents.", "is_non_singular": "", "is_positive_definite": "", "is_self_adjoint": "", "is_square": "Return True/False depending on if this operator is square.", "parameters": "Dictionary of parameters used to instantiate this LinearOperator.", "range_dimension": "Dimension (in the sense of vector spaces) of the range of this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns M.", "shape": "TensorShape of this LinearOperator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns\nTensorShape([B1,...,Bb, M, N]), equivalent to A.shape.", "tensor_rank": "Rank (in the sense of tensors) of matrix corresponding to this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns b + 2."}}, "tf.linalg.LinearOperatorFullMatrix": {"description": "LinearOperator that wraps a [batch] matrix.", "Args": {"matrix": "Shape [B1,...,Bb, M, N] with b >= 0, M, N >= 0.\nAllowed dtypes: float16, float32, float64, complex64,\ncomplex128.", "is_non_singular": "Expect that this operator is non-singular.", "is_self_adjoint": "Expect that this operator is equal to its hermitian\ntranspose.", "is_positive_definite": "Expect that this operator is positive definite,\nmeaning the quadratic form x^H A x has positive real part for all\nnonzero x.  Note that we do not require the operator to be\nself-adjoint to be positive-definite.  See:\nhttps://en.wikipedia.org/wiki/Positive-definite_matrix#Extension_for_non-symmetric_matrices", "is_square": "Expect that this operator acts like square [batch] matrices.", "name": "A name for this LinearOperator."}, "Raises": {"TypeError": "If diag.dtype is not an allowed type."}, "Attributes": {"H": "Returns the adjoint of the current LinearOperator.\nGiven A representing this LinearOperator, return A*.\nNote that calling self.adjoint() and self.H are equivalent.", "batch_shape": "TensorShape of batch dimensions of this LinearOperator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns\nTensorShape([B1,...,Bb]), equivalent to A.shape[:-2]", "domain_dimension": "Dimension (in the sense of vector spaces) of the domain of this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns N.", "dtype": "The DType of Tensors handled by this LinearOperator.", "graph_parents": "List of graph dependencies of this LinearOperator. (deprecated)\nDeprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\nInstructions for updating:\nDo not call graph_parents.", "is_non_singular": "", "is_positive_definite": "", "is_self_adjoint": "", "is_square": "Return True/False depending on if this operator is square.", "parameters": "Dictionary of parameters used to instantiate this LinearOperator.", "range_dimension": "Dimension (in the sense of vector spaces) of the range of this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns M.", "shape": "TensorShape of this LinearOperator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns\nTensorShape([B1,...,Bb, M, N]), equivalent to A.shape.", "tensor_rank": "Rank (in the sense of tensors) of matrix corresponding to this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns b + 2."}}, "tf.linalg.LinearOperatorHouseholder": {"description": "LinearOperator acting like a [batch] of Householder transformations.", "Args": {"reflection_axis": "Shape [B1,...,Bb, N] Tensor with b >= 0 N >= 0.\nThe vector defining the hyperplane to reflect about.\nAllowed dtypes: float16, float32, float64, complex64,\ncomplex128.", "is_non_singular": "Expect that this operator is non-singular.", "is_self_adjoint": "Expect that this operator is equal to its hermitian\ntranspose.  This is autoset to true", "is_positive_definite": "Expect that this operator is positive definite,\nmeaning the quadratic form x^H A x has positive real part for all\nnonzero x.  Note that we do not require the operator to be\nself-adjoint to be positive-definite.  See:\nhttps://en.wikipedia.org/wiki/Positive-definite_matrix#Extension_for_non-symmetric_matrices\nThis is autoset to false.", "is_square": "Expect that this operator acts like square [batch] matrices.\nThis is autoset to true.", "name": "A name for this LinearOperator."}, "Raises": {"ValueError": "is_self_adjoint is not True, is_positive_definite is\nnot False or is_square is not True."}, "Attributes": {"H": "Returns the adjoint of the current LinearOperator.\nGiven A representing this LinearOperator, return A*.\nNote that calling self.adjoint() and self.H are equivalent.", "batch_shape": "TensorShape of batch dimensions of this LinearOperator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns\nTensorShape([B1,...,Bb]), equivalent to A.shape[:-2]", "domain_dimension": "Dimension (in the sense of vector spaces) of the domain of this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns N.", "dtype": "The DType of Tensors handled by this LinearOperator.", "graph_parents": "List of graph dependencies of this LinearOperator. (deprecated)\nDeprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\nInstructions for updating:\nDo not call graph_parents.", "is_non_singular": "", "is_positive_definite": "", "is_self_adjoint": "", "is_square": "Return True/False depending on if this operator is square.", "parameters": "Dictionary of parameters used to instantiate this LinearOperator.", "range_dimension": "Dimension (in the sense of vector spaces) of the range of this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns M.", "reflection_axis": "", "shape": "TensorShape of this LinearOperator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns\nTensorShape([B1,...,Bb, M, N]), equivalent to A.shape.", "tensor_rank": "Rank (in the sense of tensors) of matrix corresponding to this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns b + 2."}}, "tf.linalg.LinearOperatorIdentity": {"description": "LinearOperator acting like a [batch] square identity matrix.", "Args": {"num_rows": "Scalar non-negative integer Tensor.  Number of rows in the\ncorresponding identity matrix.", "batch_shape": "Optional 1-D integer Tensor.  The shape of the leading\ndimensions.  If None, this operator has no leading dimensions.", "dtype": "Data type of the matrix that this operator represents.", "is_non_singular": "Expect that this operator is non-singular.", "is_self_adjoint": "Expect that this operator is equal to its hermitian\ntranspose.", "is_positive_definite": "Expect that this operator is positive definite,\nmeaning the quadratic form x^H A x has positive real part for all\nnonzero x.  Note that we do not require the operator to be\nself-adjoint to be positive-definite.  See:\nhttps://en.wikipedia.org/wiki/Positive-definite_matrix#Extension_for_non-symmetric_matrices", "is_square": "Expect that this operator acts like square [batch] matrices.", "assert_proper_shapes": "Python bool.  If False, only perform static\nchecks that initialization and method arguments have proper shape.\nIf True, and static checks are inconclusive, add asserts to the graph.", "name": "A name for this LinearOperator"}, "Raises": {"ValueError": "If any of the following is not True:\n{is_self_adjoint, is_non_singular, is_positive_definite}.", "TypeError": "If num_rows or batch_shape is ref-type (e.g. Variable)."}, "Attributes": {"H": "Returns the adjoint of the current LinearOperator.\nGiven A representing this LinearOperator, return A*.\nNote that calling self.adjoint() and self.H are equivalent.", "batch_shape": "TensorShape of batch dimensions of this LinearOperator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns\nTensorShape([B1,...,Bb]), equivalent to A.shape[:-2]", "domain_dimension": "Dimension (in the sense of vector spaces) of the domain of this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns N.", "dtype": "The DType of Tensors handled by this LinearOperator.", "graph_parents": "List of graph dependencies of this LinearOperator. (deprecated)\nDeprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\nInstructions for updating:\nDo not call graph_parents.", "is_non_singular": "", "is_positive_definite": "", "is_self_adjoint": "", "is_square": "Return True/False depending on if this operator is square.", "parameters": "Dictionary of parameters used to instantiate this LinearOperator.", "range_dimension": "Dimension (in the sense of vector spaces) of the range of this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns M.", "shape": "TensorShape of this LinearOperator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns\nTensorShape([B1,...,Bb, M, N]), equivalent to A.shape.", "tensor_rank": "Rank (in the sense of tensors) of matrix corresponding to this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns b + 2."}}, "tf.linalg.LinearOperatorInversion": {"description": "LinearOperator representing the inverse of another operator.", "Args": {"operator": "LinearOperator object. If operator.is_non_singular == False,\nan exception is raised.  We do allow operator.is_non_singular == None,\nin which case this operator will have is_non_singular == None.\nSimilarly for is_self_adjoint and is_positive_definite.", "is_non_singular": "Expect that this operator is non-singular.", "is_self_adjoint": "Expect that this operator is equal to its hermitian\ntranspose.", "is_positive_definite": "Expect that this operator is positive definite,\nmeaning the quadratic form x^H A x has positive real part for all\nnonzero x.  Note that we do not require the operator to be\nself-adjoint to be positive-definite.  See:\nhttps://en.wikipedia.org/wiki/Positive-definite_matrix#Extension_for_non-symmetric_matrices", "is_square": "Expect that this operator acts like square [batch] matrices.", "name": "A name for this LinearOperator. Default is operator.name +\n\"_inv\"."}, "Raises": {"ValueError": "If operator.is_non_singular is False."}, "Attributes": {"H": "Returns the adjoint of the current LinearOperator.\nGiven A representing this LinearOperator, return A*.\nNote that calling self.adjoint() and self.H are equivalent.", "batch_shape": "TensorShape of batch dimensions of this LinearOperator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns\nTensorShape([B1,...,Bb]), equivalent to A.shape[:-2]", "domain_dimension": "Dimension (in the sense of vector spaces) of the domain of this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns N.", "dtype": "The DType of Tensors handled by this LinearOperator.", "graph_parents": "List of graph dependencies of this LinearOperator. (deprecated)\nDeprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\nInstructions for updating:\nDo not call graph_parents.", "is_non_singular": "", "is_positive_definite": "", "is_self_adjoint": "", "is_square": "Return True/False depending on if this operator is square.", "operator": "The operator before inversion.", "parameters": "Dictionary of parameters used to instantiate this LinearOperator.", "range_dimension": "Dimension (in the sense of vector spaces) of the range of this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns M.", "shape": "TensorShape of this LinearOperator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns\nTensorShape([B1,...,Bb, M, N]), equivalent to A.shape.", "tensor_rank": "Rank (in the sense of tensors) of matrix corresponding to this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns b + 2."}}, "tf.linalg.LinearOperatorKronecker": {"description": "Kronecker product between two LinearOperators.", "Args": {"operators": "Iterable of LinearOperator objects, each with\nthe same dtype and composable shape, representing the Kronecker\nfactors.", "is_non_singular": "Expect that this operator is non-singular.", "is_self_adjoint": "Expect that this operator is equal to its hermitian\ntranspose.", "is_positive_definite": "Expect that this operator is positive definite,\nmeaning the quadratic form x^H A x has positive real part for all\nnonzero x.  Note that we do not require the operator to be\nself-adjoint to be positive-definite.  See:\nhttps://en.wikipedia.org/wiki/Positive-definite_matrix\n    #Extension_for_non_symmetric_matrices", "is_square": "Expect that this operator acts like square [batch] matrices.", "name": "A name for this LinearOperator.  Default is the individual\noperators names joined with _x_."}, "Raises": {"TypeError": "If all operators do not have the same dtype.", "ValueError": "If operators is empty."}, "Attributes": {"H": "Returns the adjoint of the current LinearOperator.\nGiven A representing this LinearOperator, return A*.\nNote that calling self.adjoint() and self.H are equivalent.", "batch_shape": "TensorShape of batch dimensions of this LinearOperator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns\nTensorShape([B1,...,Bb]), equivalent to A.shape[:-2]", "domain_dimension": "Dimension (in the sense of vector spaces) of the domain of this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns N.", "dtype": "The DType of Tensors handled by this LinearOperator.", "graph_parents": "List of graph dependencies of this LinearOperator. (deprecated)\nDeprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\nInstructions for updating:\nDo not call graph_parents.", "is_non_singular": "", "is_positive_definite": "", "is_self_adjoint": "", "is_square": "Return True/False depending on if this operator is square.", "operators": "", "parameters": "Dictionary of parameters used to instantiate this LinearOperator.", "range_dimension": "Dimension (in the sense of vector spaces) of the range of this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns M.", "shape": "TensorShape of this LinearOperator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns\nTensorShape([B1,...,Bb, M, N]), equivalent to A.shape.", "tensor_rank": "Rank (in the sense of tensors) of matrix corresponding to this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns b + 2."}}, "tf.linalg.LinearOperatorLowRankUpdate": {"description": "Perturb a LinearOperator with a rank K update.", "Args": {"base_operator": "Shape [B1,...,Bb, M, N].", "u": "Shape [B1,...,Bb, M, K] Tensor of same dtype as base_operator.\nThis is U above.", "diag_update": "Optional shape [B1,...,Bb, K] Tensor with same dtype\nas base_operator.  This is the diagonal of D above.\n Defaults to D being the identity operator.", "v": "Optional Tensor of same dtype as u and shape [B1,...,Bb, N, K]\nDefaults to v = u, in which case the perturbation is symmetric.\nIf M != N, then v must be set since the perturbation is not square.", "is_diag_update_positive": "Python bool.\nIf True, expect diag_update > 0.", "is_non_singular": "Expect that this operator is non-singular.\nDefault is None, unless is_positive_definite is auto-set to be\nTrue (see below).", "is_self_adjoint": "Expect that this operator is equal to its hermitian\ntranspose.  Default is None, unless base_operator is self-adjoint\nand v = None (meaning u=v), in which case this defaults to True.", "is_positive_definite": "Expect that this operator is positive definite.\nDefault is None, unless base_operator is positive-definite\nv = None (meaning u=v), and is_diag_update_positive, in which case\nthis defaults to True.\nNote that we say an operator is positive definite when the quadratic\nform x^H A x has positive real part for all nonzero x.", "is_square": "Expect that this operator acts like square [batch] matrices.", "name": "A name for this LinearOperator."}, "Raises": {"ValueError": "If is_X flags are set in an inconsistent way."}, "Attributes": {"H": "Returns the adjoint of the current LinearOperator.\nGiven A representing this LinearOperator, return A*.\nNote that calling self.adjoint() and self.H are equivalent.", "base_operator": "If this operator is A = L + U D V^H, this is the L.", "batch_shape": "TensorShape of batch dimensions of this LinearOperator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns\nTensorShape([B1,...,Bb]), equivalent to A.shape[:-2]", "diag_operator": "If this operator is A = L + U D V^H, this is D.", "diag_update": "If this operator is A = L + U D V^H, this is the diagonal of D.", "domain_dimension": "Dimension (in the sense of vector spaces) of the domain of this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns N.", "dtype": "The DType of Tensors handled by this LinearOperator.", "graph_parents": "List of graph dependencies of this LinearOperator. (deprecated)\nDeprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\nInstructions for updating:\nDo not call graph_parents.", "is_diag_update_positive": "If this operator is A = L + U D V^H, this hints D > 0 elementwise.", "is_non_singular": "", "is_positive_definite": "", "is_self_adjoint": "", "is_square": "Return True/False depending on if this operator is square.", "parameters": "Dictionary of parameters used to instantiate this LinearOperator.", "range_dimension": "Dimension (in the sense of vector spaces) of the range of this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns M.", "shape": "TensorShape of this LinearOperator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns\nTensorShape([B1,...,Bb, M, N]), equivalent to A.shape.", "tensor_rank": "Rank (in the sense of tensors) of matrix corresponding to this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns b + 2.", "u": "If this operator is A = L + U D V^H, this is the U.", "v": "If this operator is A = L + U D V^H, this is the V."}}, "tf.linalg.LinearOperatorLowerTriangular": {"description": "LinearOperator acting like a [batch] square lower triangular matrix.", "Args": {"tril": "Shape [B1,...,Bb, N, N] with b >= 0, N >= 0.\nThe lower triangular part of tril defines this operator.  The strictly\nupper triangle is ignored.", "is_non_singular": "Expect that this operator is non-singular.\nThis operator is non-singular if and only if its diagonal elements are\nall non-zero.", "is_self_adjoint": "Expect that this operator is equal to its hermitian\ntranspose.  This operator is self-adjoint only if it is diagonal with\nreal-valued diagonal entries.  In this case it is advised to use\nLinearOperatorDiag.", "is_positive_definite": "Expect that this operator is positive definite,\nmeaning the quadratic form x^H A x has positive real part for all\nnonzero x.  Note that we do not require the operator to be\nself-adjoint to be positive-definite.  See:\nhttps://en.wikipedia.org/wiki/Positive-definite_matrix#Extension_for_non-symmetric_matrices", "is_square": "Expect that this operator acts like square [batch] matrices.", "name": "A name for this LinearOperator."}, "Raises": {"ValueError": "If is_square is False."}, "Attributes": {"H": "Returns the adjoint of the current LinearOperator.\nGiven A representing this LinearOperator, return A*.\nNote that calling self.adjoint() and self.H are equivalent.", "batch_shape": "TensorShape of batch dimensions of this LinearOperator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns\nTensorShape([B1,...,Bb]), equivalent to A.shape[:-2]", "domain_dimension": "Dimension (in the sense of vector spaces) of the domain of this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns N.", "dtype": "The DType of Tensors handled by this LinearOperator.", "graph_parents": "List of graph dependencies of this LinearOperator. (deprecated)\nDeprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\nInstructions for updating:\nDo not call graph_parents.", "is_non_singular": "", "is_positive_definite": "", "is_self_adjoint": "", "is_square": "Return True/False depending on if this operator is square.", "parameters": "Dictionary of parameters used to instantiate this LinearOperator.", "range_dimension": "Dimension (in the sense of vector spaces) of the range of this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns M.", "shape": "TensorShape of this LinearOperator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns\nTensorShape([B1,...,Bb, M, N]), equivalent to A.shape.", "tensor_rank": "Rank (in the sense of tensors) of matrix corresponding to this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns b + 2."}}, "tf.linalg.LinearOperatorPermutation": {"description": "LinearOperator acting like a [batch] of permutation matrices.", "Args": {"perm": "Shape [B1,...,Bb, N] Integer Tensor with b >= 0\nN >= 0. An integer vector that represents the permutation to apply.\nNote that this argument is same as tf.transpose. However, this\npermutation is applied on the rows, while the permutation in\ntf.transpose is applied on the dimensions of the Tensor. perm\nis required to have unique entries from {0, 1, ... N-1}.", "dtype": "The dtype of arguments to this operator. Default: float32.\nAllowed dtypes: float16, float32, float64, complex64,\ncomplex128.", "is_non_singular": "Expect that this operator is non-singular.", "is_self_adjoint": "Expect that this operator is equal to its hermitian\ntranspose.  This is autoset to true", "is_positive_definite": "Expect that this operator is positive definite,\nmeaning the quadratic form x^H A x has positive real part for all\nnonzero x.  Note that we do not require the operator to be\nself-adjoint to be positive-definite.  See:\nhttps://en.wikipedia.org/wiki/Positive-definite_matrix#Extension_for_non-symmetric_matrices\nThis is autoset to false.", "is_square": "Expect that this operator acts like square [batch] matrices.\nThis is autoset to true.", "name": "A name for this LinearOperator."}, "Raises": {"ValueError": "is_self_adjoint is not True, is_positive_definite is\nnot False or is_square is not True."}, "Attributes": {"H": "Returns the adjoint of the current LinearOperator.\nGiven A representing this LinearOperator, return A*.\nNote that calling self.adjoint() and self.H are equivalent.", "batch_shape": "TensorShape of batch dimensions of this LinearOperator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns\nTensorShape([B1,...,Bb]), equivalent to A.shape[:-2]", "domain_dimension": "Dimension (in the sense of vector spaces) of the domain of this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns N.", "dtype": "The DType of Tensors handled by this LinearOperator.", "graph_parents": "List of graph dependencies of this LinearOperator. (deprecated)\nDeprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\nInstructions for updating:\nDo not call graph_parents.", "is_non_singular": "", "is_positive_definite": "", "is_self_adjoint": "", "is_square": "Return True/False depending on if this operator is square.", "parameters": "Dictionary of parameters used to instantiate this LinearOperator.", "perm": "", "range_dimension": "Dimension (in the sense of vector spaces) of the range of this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns M.", "shape": "TensorShape of this LinearOperator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns\nTensorShape([B1,...,Bb, M, N]), equivalent to A.shape.", "tensor_rank": "Rank (in the sense of tensors) of matrix corresponding to this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns b + 2."}}, "tf.linalg.LinearOperatorScaledIdentity": {"description": "LinearOperator acting like a scaled [batch] identity matrix A = c I.", "Args": {"num_rows": "Scalar non-negative integer Tensor.  Number of rows in the\ncorresponding identity matrix.", "multiplier": "Tensor of shape [B1,...,Bb], or [] (a scalar).", "is_non_singular": "Expect that this operator is non-singular.", "is_self_adjoint": "Expect that this operator is equal to its hermitian\ntranspose.", "is_positive_definite": "Expect that this operator is positive definite,\nmeaning the quadratic form x^H A x has positive real part for all\nnonzero x.  Note that we do not require the operator to be\nself-adjoint to be positive-definite.  See:\nhttps://en.wikipedia.org/wiki/Positive-definite_matrix#Extension_for_non-symmetric_matrices", "is_square": "Expect that this operator acts like square [batch] matrices.", "assert_proper_shapes": "Python bool.  If False, only perform static\nchecks that initialization and method arguments have proper shape.\nIf True, and static checks are inconclusive, add asserts to the graph.", "name": "A name for this LinearOperator"}, "Raises": {"ValueError": "If num_rows is determined statically to be non-scalar, or\nnegative."}, "Attributes": {"H": "Returns the adjoint of the current LinearOperator.\nGiven A representing this LinearOperator, return A*.\nNote that calling self.adjoint() and self.H are equivalent.", "batch_shape": "TensorShape of batch dimensions of this LinearOperator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns\nTensorShape([B1,...,Bb]), equivalent to A.shape[:-2]", "domain_dimension": "Dimension (in the sense of vector spaces) of the domain of this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns N.", "dtype": "The DType of Tensors handled by this LinearOperator.", "graph_parents": "List of graph dependencies of this LinearOperator. (deprecated)\nDeprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\nInstructions for updating:\nDo not call graph_parents.", "is_non_singular": "", "is_positive_definite": "", "is_self_adjoint": "", "is_square": "Return True/False depending on if this operator is square.", "multiplier": "The [batch] scalar Tensor, c in cI.", "parameters": "Dictionary of parameters used to instantiate this LinearOperator.", "range_dimension": "Dimension (in the sense of vector spaces) of the range of this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns M.", "shape": "TensorShape of this LinearOperator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns\nTensorShape([B1,...,Bb, M, N]), equivalent to A.shape.", "tensor_rank": "Rank (in the sense of tensors) of matrix corresponding to this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns b + 2."}}, "tf.linalg.LinearOperatorToeplitz": {"description": "LinearOperator acting like a [batch] of toeplitz matrices.", "Args": {"col": "Shape [B1,...,Bb, N] Tensor with b >= 0 N >= 0.\nThe first column of the operator. Allowed dtypes: float16, float32,\n  float64, complex64, complex128. Note that the first entry of\n  col is assumed to be the same as the first entry of row.", "row": "Shape [B1,...,Bb, N] Tensor with b >= 0 N >= 0.\nThe first row of the operator. Allowed dtypes: float16, float32,\n  float64, complex64, complex128. Note that the first entry of\n  row is assumed to be the same as the first entry of col.", "is_non_singular": "Expect that this operator is non-singular.", "is_self_adjoint": "Expect that this operator is equal to its hermitian\ntranspose.  If diag.dtype is real, this is auto-set to True.", "is_positive_definite": "Expect that this operator is positive definite,\nmeaning the quadratic form x^H A x has positive real part for all\nnonzero x.  Note that we do not require the operator to be\nself-adjoint to be positive-definite.  See:\nhttps://en.wikipedia.org/wiki/Positive-definite_matrix#Extension_for_non-symmetric_matrices", "is_square": "Expect that this operator acts like square [batch] matrices.", "name": "A name for this LinearOperator."}, "Attributes": {"H": "Returns the adjoint of the current LinearOperator.\nGiven A representing this LinearOperator, return A*.\nNote that calling self.adjoint() and self.H are equivalent.", "batch_shape": "TensorShape of batch dimensions of this LinearOperator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns\nTensorShape([B1,...,Bb]), equivalent to A.shape[:-2]", "col": "", "domain_dimension": "Dimension (in the sense of vector spaces) of the domain of this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns N.", "dtype": "The DType of Tensors handled by this LinearOperator.", "graph_parents": "List of graph dependencies of this LinearOperator. (deprecated)\nDeprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\nInstructions for updating:\nDo not call graph_parents.", "is_non_singular": "", "is_positive_definite": "", "is_self_adjoint": "", "is_square": "Return True/False depending on if this operator is square.", "parameters": "Dictionary of parameters used to instantiate this LinearOperator.", "range_dimension": "Dimension (in the sense of vector spaces) of the range of this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns M.", "row": "", "shape": "TensorShape of this LinearOperator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns\nTensorShape([B1,...,Bb, M, N]), equivalent to A.shape.", "tensor_rank": "Rank (in the sense of tensors) of matrix corresponding to this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns b + 2."}}, "tf.linalg.LinearOperatorTridiag": {"description": "LinearOperator acting like a [batch] square tridiagonal matrix.", "Args": {"diagonals": "Tensor or list of Tensors depending on diagonals_format.\nIf diagonals_format=sequence, this is a list of three Tensor's each\nwith shape [B1, ..., Bb, N], b >= 0, N >= 0, representing the\nsuperdiagonal, diagonal and subdiagonal in that order. Note the\nsuperdiagonal is padded with an element in the last position, and the\nsubdiagonal is padded with an element in the front.\nIf diagonals_format=matrix this is a [B1, ... Bb, N, N] shaped\nTensor representing the full tridiagonal matrix.\nIf diagonals_format=compact this is a [B1, ... Bb, 3, N] shaped\nTensor with the second to last dimension indexing the\nsuperdiagonal, diagonal and subdiagonal in that order. Note the\nsuperdiagonal is padded with an element in the last position, and the\nsubdiagonal is padded with an element in the front.\nIn every case, these Tensors are all floating dtype.", "diagonals_format": "one of matrix, sequence, or compact. Default is\ncompact.", "is_non_singular": "Expect that this operator is non-singular.", "is_self_adjoint": "Expect that this operator is equal to its hermitian\ntranspose.  If diag.dtype is real, this is auto-set to True.", "is_positive_definite": "Expect that this operator is positive definite,\nmeaning the quadratic form x^H A x has positive real part for all\nnonzero x.  Note that we do not require the operator to be\nself-adjoint to be positive-definite.  See:\nhttps://en.wikipedia.org/wiki/Positive-definite_matrix#Extension_for_non-symmetric_matrices", "is_square": "Expect that this operator acts like square [batch] matrices.", "name": "A name for this LinearOperator."}, "Raises": {"TypeError": "If diag.dtype is not an allowed type.", "ValueError": "If diag.dtype is real, and is_self_adjoint is not True."}, "Attributes": {"H": "Returns the adjoint of the current LinearOperator.\nGiven A representing this LinearOperator, return A*.\nNote that calling self.adjoint() and self.H are equivalent.", "batch_shape": "TensorShape of batch dimensions of this LinearOperator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns\nTensorShape([B1,...,Bb]), equivalent to A.shape[:-2]", "diagonals": "", "diagonals_format": "", "domain_dimension": "Dimension (in the sense of vector spaces) of the domain of this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns N.", "dtype": "The DType of Tensors handled by this LinearOperator.", "graph_parents": "List of graph dependencies of this LinearOperator. (deprecated)\nDeprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\nInstructions for updating:\nDo not call graph_parents.", "is_non_singular": "", "is_positive_definite": "", "is_self_adjoint": "", "is_square": "Return True/False depending on if this operator is square.", "parameters": "Dictionary of parameters used to instantiate this LinearOperator.", "range_dimension": "Dimension (in the sense of vector spaces) of the range of this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns M.", "shape": "TensorShape of this LinearOperator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns\nTensorShape([B1,...,Bb, M, N]), equivalent to A.shape.", "tensor_rank": "Rank (in the sense of tensors) of matrix corresponding to this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns b + 2."}}, "tf.linalg.LinearOperatorZeros": {"description": "LinearOperator acting like a [batch] zero matrix.", "Args": {"num_rows": "Scalar non-negative integer Tensor.  Number of rows in the\ncorresponding zero matrix.", "num_columns": "Scalar non-negative integer Tensor.  Number of columns in\nthe corresponding zero matrix. If None, defaults to the value of\nnum_rows.", "batch_shape": "Optional 1-D integer Tensor.  The shape of the leading\ndimensions.  If None, this operator has no leading dimensions.", "dtype": "Data type of the matrix that this operator represents.", "is_non_singular": "Expect that this operator is non-singular.", "is_self_adjoint": "Expect that this operator is equal to its hermitian\ntranspose.", "is_positive_definite": "Expect that this operator is positive definite,\nmeaning the quadratic form x^H A x has positive real part for all\nnonzero x.  Note that we do not require the operator to be\nself-adjoint to be positive-definite.  See:\nhttps://en.wikipedia.org/wiki/Positive-definite_matrix#Extension_for_non-symmetric_matrices", "is_square": "Expect that this operator acts like square [batch] matrices.", "assert_proper_shapes": "Python bool.  If False, only perform static\nchecks that initialization and method arguments have proper shape.\nIf True, and static checks are inconclusive, add asserts to the graph.", "name": "A name for this LinearOperator"}, "Raises": {"ValueError": "If any of the following is not True:\n{is_self_adjoint, is_non_singular, is_positive_definite}."}, "Attributes": {"H": "Returns the adjoint of the current LinearOperator.\nGiven A representing this LinearOperator, return A*.\nNote that calling self.adjoint() and self.H are equivalent.", "batch_shape": "TensorShape of batch dimensions of this LinearOperator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns\nTensorShape([B1,...,Bb]), equivalent to A.shape[:-2]", "domain_dimension": "Dimension (in the sense of vector spaces) of the domain of this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns N.", "dtype": "The DType of Tensors handled by this LinearOperator.", "graph_parents": "List of graph dependencies of this LinearOperator. (deprecated)\nDeprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\nInstructions for updating:\nDo not call graph_parents.", "is_non_singular": "", "is_positive_definite": "", "is_self_adjoint": "", "is_square": "Return True/False depending on if this operator is square.", "parameters": "Dictionary of parameters used to instantiate this LinearOperator.", "range_dimension": "Dimension (in the sense of vector spaces) of the range of this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns M.", "shape": "TensorShape of this LinearOperator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns\nTensorShape([B1,...,Bb, M, N]), equivalent to A.shape.", "tensor_rank": "Rank (in the sense of tensors) of matrix corresponding to this operator.\nIf this operator acts like the batch matrix A with\nA.shape = [B1,...,Bb, M, N], then this returns b + 2."}}, "tf.linalg.adjoint": {"description": "Transposes the last two dimensions of and conjugates tensor matrix.", "Args": {"matrix": "A Tensor. Must be float16, float32, float64, complex64,\nor complex128 with shape [..., M, M].", "name": "A name to give this Op (optional)."}, "Returns": "The adjoint (a.k.a. Hermitian transpose a.k.a. conjugate transpose) of\nmatrix."}, "tf.linalg.band_part": {"description": "Copy a tensor setting everything outside a central band in each innermost matrix to zero.", "Args": {"input": "A Tensor. Rank k tensor.", "num_lower": "A Tensor. Must be one of the following types: int32, int64.\n0-D tensor. Number of subdiagonals to keep. If negative, keep entire\nlower triangle.", "num_upper": "A Tensor. Must have the same type as num_lower.\n0-D tensor. Number of superdiagonals to keep. If negative, keep\nentire upper triangle.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.linalg.banded_triangular_solve": {"description": "Solve triangular systems of equations with a banded solver.", "Args": {"bands": "A Tensor describing the bands of the left hand side, with shape\n[..., K, M]. The K rows correspond to the diagonal to the K - 1-th\ndiagonal (the diagonal is the top row) when lower is True and\notherwise the K - 1-th superdiagonal to the diagonal (the diagonal is\nthe bottom row) when lower is False. The bands are stored with\n'LEFT_RIGHT' alignment, where the superdiagonals are padded on the right\nand subdiagonals are padded on the left. This is the alignment cuSPARSE\nuses.  See  tf.linalg.set_diag for more details.", "rhs": "A Tensor of shape [..., M] or [..., M, N] and with the same dtype as\ndiagonals. Note that if the shape of rhs and/or diags isn't known\nstatically, rhs will be treated as a matrix rather than a vector.", "lower": "An optional bool. Defaults to True. Boolean indicating whether\nbands represents a lower or upper triangular matrix.", "adjoint": "An optional bool. Defaults to False. Boolean indicating whether\nto solve with the matrix's block-wise adjoint.", "name": "A name to give this Op (optional)."}, "Returns": "A Tensor of shape [..., M] or [..., M, N] containing the solutions."}, "tf.linalg.cholesky": {"description": "Computes the Cholesky decomposition of one or more square matrices.", "Args": {"input": "A Tensor. Must be one of the following types: float64, float32, half, complex64, complex128.\nShape is [..., M, M].", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.linalg.cholesky_solve": {"description": "Solves systems of linear eqns A X = RHS, given Cholesky factorizations.", "Args": {"chol": "A Tensor.  Must be float32 or float64, shape is [..., M, M].\nCholesky factorization of A, e.g. chol = tf.linalg.cholesky(A).\nFor that reason, only the lower triangular parts (including the diagonal)\nof the last two dimensions of chol are used.  The strictly upper part is\nassumed to be zero and not accessed.", "rhs": "A Tensor, same type as chol, shape is [..., M, K].", "name": "A name to give this Op.  Defaults to cholesky_solve."}, "Returns": "Solution to A x = rhs, shape [..., M, K]."}, "tf.linalg.cross": {"description": "Compute the pairwise cross product.", "Args": {"a": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.\nA tensor containing 3-element vectors.", "b": "A Tensor. Must have the same type as a.\nAnother tensor, of same type and shape as a.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as a."}, "tf.linalg.det": {"description": "Computes the determinant of one or more square matrices.", "Args": {"input": "A Tensor. Must be one of the following types: half, float32, float64, complex64, complex128.\nShape is [..., M, M].", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.linalg.diag": {"description": "Returns a batched diagonal tensor with given batched diagonal values.", "Args": {"diagonal": "A Tensor with rank k >= 1.", "name": "A name for the operation (optional).", "k": "Diagonal offset(s). Positive value means superdiagonal, 0 refers to the\nmain diagonal, and negative value means subdiagonals. k can be a single\ninteger (for a single diagonal) or a pair of integers specifying the low\nand high ends of a matrix band. k[0] must not be larger than k[1].", "num_rows": "The number of rows of the output matrix. If it is not provided,\nthe op assumes the output matrix is a square matrix and infers the matrix\nsize from d_lower, d_upper, and the innermost dimension of diagonal.", "num_cols": "The number of columns of the output matrix. If it is not provided,\nthe op assumes the output matrix is a square matrix and infers the matrix\nsize from d_lower, d_upper, and the innermost dimension of diagonal.", "padding_value": "The value to fill the area outside the specified diagonal\nband with. Default is 0.", "align": "Some diagonals are shorter than max_diag_len and need to be padded.\nalign is a string specifying how superdiagonals and subdiagonals should\nbe aligned, respectively. There are four possible alignments: \"RIGHT_LEFT\"\n(default), \"LEFT_RIGHT\", \"LEFT_LEFT\", and \"RIGHT_RIGHT\". \"RIGHT_LEFT\"\naligns superdiagonals to the right (left-pads the row) and subdiagonals to\nthe left (right-pads the row). It is the packing format LAPACK uses.\ncuSPARSE uses \"LEFT_RIGHT\", which is the opposite alignment."}, "Returns": "A Tensor. Has the same type as diagonal."}, "tf.linalg.diag_part": {"description": "Returns the batched diagonal part of a batched tensor.", "Args": {"input": "A Tensor with rank k >= 2.", "name": "A name for the operation (optional).", "k": "Diagonal offset(s). Positive value means superdiagonal, 0 refers to the\nmain diagonal, and negative value means subdiagonals. k can be a single\ninteger (for a single diagonal) or a pair of integers specifying the low\nand high ends of a matrix band. k[0] must not be larger than k[1].", "padding_value": "The value to fill the area outside the specified diagonal\nband with. Default is 0.", "align": "Some diagonals are shorter than max_diag_len and need to be padded.\nalign is a string specifying how superdiagonals and subdiagonals should\nbe aligned, respectively. There are four possible alignments: \"RIGHT_LEFT\"\n(default), \"LEFT_RIGHT\", \"LEFT_LEFT\", and \"RIGHT_RIGHT\". \"RIGHT_LEFT\"\naligns superdiagonals to the right (left-pads the row) and subdiagonals to\nthe left (right-pads the row). It is the packing format LAPACK uses.\ncuSPARSE uses \"LEFT_RIGHT\", which is the opposite alignment."}, "Returns": "A Tensor containing diagonals of input. Has the same type as input.", "Raises": {"InvalidArgumentError": "When k is out of bound or when k[0]>k[1:]."}}, "tf.linalg.eig": {"description": "Computes the eigen decomposition of a batch of matrices.", "Args": {"tensor": "Tensor of shape [..., N, N]. Only the lower triangular part of\neach inner inner matrix is referenced.", "name": "string, optional name of the operation."}, "Returns": "e\n\n\nEigenvalues. Shape is [..., N]. Sorted in non-decreasing order."}, "tf.linalg.eigh": {"description": "Computes the eigen decomposition of a batch of self-adjoint matrices.", "Args": {"tensor": "Tensor of shape [..., N, N]. Only the lower triangular part of\neach inner inner matrix is referenced.", "name": "string, optional name of the operation."}, "Returns": "e\n\n\nEigenvalues. Shape is [..., N]. Sorted in non-decreasing order."}, "tf.linalg.eigh_tridiagonal": {"description": "Computes the eigenvalues of a Hermitian tridiagonal matrix.", "Args": {"alpha": "A real or complex tensor of shape (n), the diagonal elements of the\nmatrix. NOTE: If alpha is complex, the imaginary part is ignored (assumed\n  zero) to satisfy the requirement that the matrix be Hermitian.", "beta": "A real or complex tensor of shape (n-1), containing the elements of\nthe first super-diagonal of the matrix. If beta is complex, the first\nsub-diagonal of the matrix is assumed to be the conjugate of beta to\nsatisfy the requirement that the matrix be Hermitian", "eigvals_only": "If False, both eigenvalues and corresponding eigenvectors are\ncomputed. If True, only eigenvalues are computed. Default is True.", "select": "Optional string with values in {\u2018a\u2019, \u2018v\u2019, \u2018i\u2019} (default is 'a') that\ndetermines which eigenvalues to calculate:\n  'a': all eigenvalues.\n  \u2018v\u2019: eigenvalues in the interval (min, max] given by select_range.\n  'i\u2019: eigenvalues with indices min <= i <= max.", "select_range": "Size 2 tuple or list or tensor specifying the range of\neigenvalues to compute together with select. If select is 'a',\nselect_range is ignored.", "tol": "Optional scalar. The absolute tolerance to which each eigenvalue is\nrequired. An eigenvalue (or cluster) is considered to have converged if it\nlies in an interval of this width. If tol is None (default), the value\neps*|T|_2 is used where eps is the machine precision, and |T|_2 is the\n2-norm of the matrix T.", "name": "Optional name of the op."}, "Returns": "eig_vals\n\n\nThe eigenvalues of the matrix in non-decreasing order.", "Raises": {"ValueError": "If input values are invalid.", "NotImplemented": "Computing eigenvectors for eigvals_only = False is\nnot implemented yet."}}, "tf.linalg.eigvals": {"description": "Computes the eigenvalues of one or more matrices.", "Args": {"tensor": "Tensor of shape [..., N, N].", "name": "string, optional name of the operation."}, "Returns": "e\n\n\nEigenvalues. Shape is [..., N]. The vector e[..., :] contains the N\neigenvalues of tensor[..., :, :]."}, "tf.linalg.eigvalsh": {"description": "Computes the eigenvalues of one or more self-adjoint matrices.", "Args": {"tensor": "Tensor of shape [..., N, N].", "name": "string, optional name of the operation."}, "Returns": "e\n\n\nEigenvalues. Shape is [..., N]. The vector e[..., :] contains the N\neigenvalues of tensor[..., :, :]."}, "tf.linalg.expm": {"description": "Computes the matrix exponential of one or more square matrices.", "Args": {"input": "A Tensor. Must be float16, float32, float64, complex64, or\ncomplex128 with shape [..., M, M].", "name": "A name to give this Op (optional)."}, "Returns": "the matrix exponential of the input.", "Raises": {"ValueError": "An unsupported type is provided as input."}}, "tf.linalg.global_norm": {"description": "Computes the global norm of multiple tensors.", "Args": {"t_list": "A tuple or list of mixed Tensors, IndexedSlices, or None.", "name": "A name for the operation (optional)."}, "Returns": "A 0-D (scalar) Tensor of type float.", "Raises": {"TypeError": "If t_list is not a sequence."}}, "tf.linalg.inv": {"description": "Computes the inverse of one or more square invertible matrices or their adjoints (conjugate transposes).", "Args": {"input": "A Tensor. Must be one of the following types: float64, float32, half, complex64, complex128.\nShape is [..., M, M].", "adjoint": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.linalg.logdet": {"description": "Computes log of the determinant of a hermitian positive definite matrix.", "Args": {"matrix": "A Tensor. Must be float16, float32, float64, complex64,\nor complex128 with shape [..., M, M].", "name": "A name to give this Op.  Defaults to logdet."}, "Returns": "The natural log of the determinant of matrix."}, "tf.linalg.logm": {"description": "Computes the matrix logarithm of one or more square matrices:", "Args": {"input": "A Tensor. Must be one of the following types: complex64, complex128.\nShape is [..., M, M].", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.linalg.lstsq": {"description": "Solves one or more linear least-squares problems.", "Args": {"matrix": "Tensor of shape [..., M, N].", "rhs": "Tensor of shape [..., M, K].", "l2_regularizer": "0-D double Tensor. Ignored if fast=False.", "fast": "bool. Defaults to True.", "name": "string, optional name of the operation."}, "Returns": "output\n\n\nTensor of shape [..., N, K] whose inner-most 2 dimensions form\nM-by-K matrices that solve the equations\nmatrix[..., :, :] * output[..., :, :] = rhs[..., :, :] in the least\nsquares sense.", "Raises": {"NotImplementedError": "linalg.lstsq is currently disabled for complex128\nand l2_regularizer != 0 due to poor accuracy."}}, "tf.linalg.lu": {"description": "Computes the LU decomposition of one or more square matrices.", "Args": {"input": "A Tensor. Must be one of the following types: float64, float32, half, complex64, complex128.\nA tensor of shape [..., M, M] whose inner-most 2 dimensions form matrices of\nsize [M, M].", "output_idx_type": "An optional tf.DType from: tf.int32, tf.int64. Defaults to tf.int32.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (lu, p)."}, "tf.linalg.lu_matrix_inverse": {"description": "Computes the inverse given the LU decomposition(s) of one or more matrices.", "Args": {"lower_upper": "lu as returned by tf.linalg.lu, i.e., if matmul(P,\nmatmul(L, U)) = X then lower_upper = L + U - eye.", "perm": "p as returned by tf.linag.lu, i.e., if matmul(P, matmul(L, U)) =\nX then perm = argmax(P).", "validate_args": "Python bool indicating whether arguments should be checked\nfor correctness. Note: this function does not verify the implied matrix is\n  actually invertible, even when validate_args=True.\nDefault value: False (i.e., don't validate arguments).", "name": "Python str name given to ops managed by this object.\nDefault value: None (i.e., 'lu_matrix_inverse')."}, "Returns": "inv_x\n\n\nThe matrix_inv, i.e.,\ntf.matrix_inverse(tf.linalg.lu_reconstruct(lu, perm))."}, "tf.linalg.lu_reconstruct": {"description": "The reconstruct one or more matrices from their LU decomposition(s).", "Args": {"lower_upper": "lu as returned by tf.linalg.lu, i.e., if matmul(P,\nmatmul(L, U)) = X then lower_upper = L + U - eye.", "perm": "p as returned by tf.linag.lu, i.e., if matmul(P, matmul(L, U)) =\nX then perm = argmax(P).", "validate_args": "Python bool indicating whether arguments should be checked\nfor correctness.\nDefault value: False (i.e., don't validate arguments).", "name": "Python str name given to ops managed by this object.\nDefault value: None (i.e., 'lu_reconstruct')."}, "Returns": "x\n\n\nThe original input to tf.linalg.lu, i.e., x as in,\nlu_reconstruct(*tf.linalg.lu(x))."}, "tf.linalg.lu_solve": {"description": "Solves systems of linear eqns A X = RHS, given LU factorizations.", "Args": {"lower_upper": "lu as returned by tf.linalg.lu, i.e., if matmul(P,\nmatmul(L, U)) = X then lower_upper = L + U - eye.", "perm": "p as returned by tf.linag.lu, i.e., if matmul(P, matmul(L, U)) =\nX then perm = argmax(P).", "rhs": "Matrix-shaped float Tensor representing targets for which to solve;\nA X = RHS. To handle vector cases, use: lu_solve(..., rhs[...,\n  tf.newaxis])[..., 0].", "validate_args": "Python bool indicating whether arguments should be checked\nfor correctness. Note: this function does not verify the implied matrix is\n  actually invertible, even when validate_args=True.\nDefault value: False (i.e., don't validate arguments).", "name": "Python str name given to ops managed by this object.\nDefault value: None (i.e., 'lu_solve')."}, "Returns": "x\n\n\nThe X in A @ X = RHS."}, "tf.linalg.matmul": {"description": "Multiplies matrix a by matrix b, producing a * b.", "Args": {"a": "tf.Tensor of type float16, float32, float64, int32,\ncomplex64, complex128 and rank > 1.", "b": "tf.Tensor with same type and rank as a.", "transpose_a": "If True, a is transposed before multiplication.", "transpose_b": "If True, b is transposed before multiplication.", "adjoint_a": "If True, a is conjugated and transposed before\nmultiplication.", "adjoint_b": "If True, b is conjugated and transposed before\nmultiplication.", "a_is_sparse": "If True, a is treated as a sparse matrix. Notice, this\ndoes not support tf.sparse.SparseTensor, it just makes optimizations\nthat assume most values in a are zero.\nSee tf.sparse.sparse_dense_matmul\nfor some support for tf.sparse.SparseTensor multiplication.", "b_is_sparse": "If True, b is treated as a sparse matrix. Notice, this\ndoes not support tf.sparse.SparseTensor, it just makes optimizations\nthat assume most values in a are zero.\nSee tf.sparse.sparse_dense_matmul\nfor some support for tf.sparse.SparseTensor multiplication.", "output_type": "The output datatype if needed. Defaults to None in which case\nthe output_type is the same as input type. Currently only works when input\ntensors are type (u)int8 and output_type can be int32.", "name": "Name for the operation (optional)."}, "Returns": "A tf.Tensor of the same type as a and b where each inner-most matrix\nis the product of the corresponding matrices in a and b, e.g. if all\ntranspose or adjoint attributes are False:\noutput[..., i, j] = sum_k (a[..., i, k] * b[..., k, j]),\nfor all indices i, j.", "Raises": {"ValueError": "If transpose_a and adjoint_a, or transpose_b and\nadjoint_b are both set to True.", "TypeError": "If output_type is specified but the types of a, b and\noutput_type is not (u)int8, (u)int8 and int32."}}, "tf.linalg.matrix_rank": {"description": "Compute the matrix rank of one or more matrices.", "Args": {"a": "(Batch of) float-like matrix-shaped Tensor(s) which are to be\npseudo-inverted.", "tol": "Threshold below which the singular value is counted as 'zero'.\nDefault value: None (i.e., eps * max(rows, cols) * max(singular_val)).", "validate_args": "When True, additional assertions might be embedded in the\ngraph.\nDefault value: False (i.e., no graph assertions are added).", "name": "Python str prefixed to ops created by this function.\nDefault value: 'matrix_rank'."}, "Returns": "matrix_rank\n\n\n(Batch of) int32 scalars representing the number of non-zero\nsingular values."}, "tf.linalg.matrix_transpose": {"description": "Transposes last two dimensions of tensor a.", "Args": {"a": "A Tensor with rank >= 2.", "name": "A name for the operation (optional).", "conjugate": "Optional bool. Setting it to True is mathematically equivalent\nto tf.math.conj(tf.linalg.matrix_transpose(input))."}, "Returns": "A transposed batch matrix Tensor.", "Raises": {"ValueError": "If a is determined statically to have rank < 2."}}, "tf.linalg.matvec": {"description": "Multiplies matrix a by vector b, producing a * b.", "Args": {"a": "Tensor of type float16, float32, float64, int32, complex64,\ncomplex128 and rank > 1.", "b": "Tensor with same type as a and compatible dimensions.", "transpose_a": "If True, a is transposed before multiplication.", "adjoint_a": "If True, a is conjugated and transposed before\nmultiplication.", "a_is_sparse": "If True, a is treated as a sparse matrix.", "b_is_sparse": "If True, b is treated as a sparse matrix.", "name": "Name for the operation (optional)."}, "Returns": "A Tensor of the same type as a and b where each inner-most vector is\nthe product of the corresponding matrices in a and vectors in b, e.g. if\nall transpose or adjoint attributes are False:\noutput[..., i] = sum_k (a[..., i, k] * b[..., k]), for all indices i.", "Raises": {"ValueError": "If transpose_a and adjoint_a are both set to True."}}, "tf.linalg.normalize": {"description": "Normalizes tensor along dimension axis using specified norm.", "Args": {"tensor": "Tensor of types float32, float64, complex64, complex128", "ord": "Order of the norm. Supported values are 'fro', 'euclidean', 1,\n2, np.inf and any positive real number yielding the corresponding\np-norm. Default is 'euclidean' which is equivalent to Frobenius norm if\ntensor is a matrix and equivalent to 2-norm for vectors.\nSome restrictions apply: a) The Frobenius norm 'fro' is not defined for\n  vectors, b) If axis is a 2-tuple (matrix norm), only 'euclidean',\n  'fro', 1, 2, np.inf are supported. See the description of axis\n  on how to compute norms for a batch of vectors or matrices stored in a\n  tensor.", "axis": "If axis is None (the default), the input is considered a vector\nand a single vector norm is computed over the entire set of values in the\ntensor, i.e. norm(tensor, ord=ord) is equivalent to\nnorm(reshape(tensor, [-1]), ord=ord). If axis is a Python integer, the\ninput is considered a batch of vectors, and axis determines the axis in\ntensor over which to compute vector norms. If axis is a 2-tuple of\nPython integers it is considered a batch of matrices and axis determines\nthe axes in tensor over which to compute a matrix norm.\nNegative indices are supported. Example: If you are passing a tensor that\n  can be either a matrix or a batch of matrices at runtime, pass\n  axis=[-2,-1] instead of axis=None to make sure that matrix norms are\n  computed.", "name": "The name of the op."}, "Returns": "normalized\n\n\nA normalized Tensor with the same shape as tensor.", "Raises": {"ValueError": "If ord or axis is invalid."}}, "tf.linalg.pinv": {"description": "Compute the Moore-Penrose pseudo-inverse of one or more matrices.", "Args": {"a": "(Batch of) float-like matrix-shaped Tensor(s) which are to be\npseudo-inverted.", "rcond": "Tensor of small singular value cutoffs.  Singular values smaller\n(in modulus) than rcond * largest_singular_value (again, in modulus) are\nset to zero. Must broadcast against tf.shape(a)[:-2].\nDefault value: 10. * max(num_rows, num_cols) * np.finfo(a.dtype).eps.", "validate_args": "When True, additional assertions might be embedded in the\ngraph.\nDefault value: False (i.e., no graph assertions are added).", "name": "Python str prefixed to ops created by this function.\nDefault value: 'pinv'."}, "Returns": "a_pinv\n\n\n(Batch of) pseudo-inverse of input a. Has same shape as a except\nrightmost two dimensions are transposed.", "Raises": {"TypeError": "if input a does not have float-like dtype.", "ValueError": "if input a has fewer than 2 dimensions."}}, "tf.linalg.qr": {"description": "Computes the QR decompositions of one or more matrices.", "Args": {"input": "A Tensor. Must be one of the following types: float64, float32, half, complex64, complex128.\nA tensor of shape [..., M, N] whose inner-most 2 dimensions\nform matrices of size [M, N]. Let P be the minimum of M and N.", "full_matrices": "An optional bool. Defaults to False.\nIf true, compute full-sized q and r. If false\n(the default), compute only the leading P columns of q.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (q, r)."}, "tf.linalg.set_diag": {"description": "Returns a batched matrix tensor with new batched diagonal values.", "Args": {"input": "A Tensor with rank k + 1, where k >= 1.", "diagonal": "A Tensor with rank k, when d_lower == d_upper, or k + 1,\notherwise. k >= 1.", "name": "A name for the operation (optional).", "k": "Diagonal offset(s). Positive value means superdiagonal, 0 refers to the\nmain diagonal, and negative value means subdiagonals. k can be a single\ninteger (for a single diagonal) or a pair of integers specifying the low\nand high ends of a matrix band. k[0] must not be larger than k[1].", "align": "Some diagonals are shorter than max_diag_len and need to be padded.\nalign is a string specifying how superdiagonals and subdiagonals should\nbe aligned, respectively. There are four possible alignments: \"RIGHT_LEFT\"\n(default), \"LEFT_RIGHT\", \"LEFT_LEFT\", and \"RIGHT_RIGHT\". \"RIGHT_LEFT\"\naligns superdiagonals to the right (left-pads the row) and subdiagonals to\nthe left (right-pads the row). It is the packing format LAPACK uses.\ncuSPARSE uses \"LEFT_RIGHT\", which is the opposite alignment."}}, "tf.linalg.slogdet": {"description": "Computes the sign and the log of the absolute value of the determinant of", "Args": {"input": "A Tensor. Must be one of the following types: half, float32, float64, complex64, complex128.\nShape is [N, M, M].", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (sign, log_abs_determinant)."}, "tf.linalg.solve": {"description": "Solves systems of linear equations.", "Args": {"matrix": "A Tensor. Must be one of the following types: float64, float32, half, complex64, complex128.\nShape is [..., M, M].", "rhs": "A Tensor. Must have the same type as matrix.\nShape is [..., M, K].", "adjoint": "An optional bool. Defaults to False.\nBoolean indicating whether to solve with matrix or its (block-wise)\nadjoint.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as matrix."}, "tf.linalg.sqrtm": {"description": "Computes the matrix square root of one or more square matrices:", "Args": {"input": "A Tensor. Must be one of the following types: float64, float32, half, complex64, complex128.\nShape is [..., M, M].", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.linalg.svd": {"description": "Computes the singular value decompositions of one or more matrices.", "Args": {"tensor": "Tensor of shape [..., M, N]. Let P be the minimum of M and\nN.", "full_matrices": "If true, compute full-sized u and v. If false\n(the default), compute only the leading P singular vectors.\nIgnored if compute_uv is False.", "compute_uv": "If True then left and right singular vectors will be\ncomputed and returned in u and v, respectively. Otherwise, only the\nsingular values will be computed, which can be significantly faster.", "name": "string, optional name of the operation."}, "Returns": "s\n\n\nSingular values. Shape is [..., P]. The values are sorted in reverse\norder of magnitude, so s[..., 0] is the largest value, s[..., 1] is the\nsecond largest, etc."}, "tf.linalg.tensor_diag": {"description": "Returns a diagonal tensor with a given diagonal values.", "Args": {"diagonal": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, int32, int64, complex64, complex128.\nRank k tensor where k is at most 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as diagonal."}, "tf.linalg.tensor_diag_part": {"description": "Returns the diagonal part of the tensor.", "Args": {"input": "A Tensor with rank 2k.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor containing diagonals of input. Has the same type as input, and\nrank k."}, "tf.linalg.trace": {"description": "Compute the trace of a tensor x.", "Args": {"x": "tensor.", "name": "A name for the operation (optional)."}, "Returns": "The trace of input tensor."}, "tf.linalg.triangular_solve": {"description": "Solve systems of linear equations with upper or lower triangular matrices.", "Args": {"matrix": "A Tensor. Must be one of the following types: float64,\nfloat32, half, complex64, complex128. Shape is [..., M, M].", "rhs": "A Tensor. Must have the same type as matrix. Shape is [..., M,\nN].", "lower": "An optional bool. Defaults to True. Boolean indicating whether\nthe innermost matrices in matrix are lower or upper triangular.", "adjoint": "An optional bool. Defaults to False. Boolean indicating whether\nto solve with matrix or its (block-wise) adjoint.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as matrix, and shape is [..., M, N]."}, "tf.linalg.tridiagonal_matmul": {"description": "Multiplies tridiagonal matrix by matrix.", "Args": {"diagonals": "A Tensor or tuple of Tensors describing left-hand sides. The\nshape depends of diagonals_format, see description above. Must be\nfloat32, float64, complex64, or complex128.", "rhs": "A Tensor of shape [..., M, N] and with the same dtype as diagonals.", "diagonals_format": "one of sequence, or compact. Default is compact.", "name": "A name to give this Op (optional)."}, "Returns": "A Tensor of shape [..., M, N] containing the result of multiplication.", "Raises": {"ValueError": "An unsupported type is provided as input, or when the input\ntensors have incorrect shapes."}}, "tf.linalg.tridiagonal_solve": {"description": "Solves tridiagonal systems of equations.", "Args": {"diagonals": "A Tensor or tuple of Tensors describing left-hand sides. The\nshape depends of diagonals_format, see description above. Must be\nfloat32, float64, complex64, or complex128.", "rhs": "A Tensor of shape [..., M] or [..., M, K] and with the same dtype as\ndiagonals. Note that if the shape of rhs and/or diags isn't known\nstatically, rhs will be treated as a matrix rather than a vector.", "diagonals_format": "one of matrix, sequence, or compact. Default is\ncompact.", "transpose_rhs": "If True, rhs is transposed before solving (has no effect\nif the shape of rhs is [..., M]).", "conjugate_rhs": "If True, rhs is conjugated before solving.", "name": "A name to give this Op (optional).", "partial_pivoting": "whether to perform partial pivoting. True by default.\nPartial pivoting makes the procedure more stable, but slower. Partial\npivoting is unnecessary in some cases, including diagonally dominant and\nsymmetric positive definite matrices (see e.g. theorem 9.12 in [1]).", "perturb_singular": "whether to perturb singular matrices to return a finite\nresult. False by default. If true, solutions to systems involving\na singular matrix will be computed by perturbing near-zero pivots in\nthe partially pivoted LU decomposition. Specifically, tiny pivots are\nperturbed by an amount of order eps * max_{ij} |U(i,j)| to avoid\noverflow. Here U is the upper triangular part of the LU decomposition,\nand eps is the machine precision. This is useful for solving\nnumerically singular systems when computing eigenvectors by inverse\niteration.\nIf partial_pivoting is False, perturb_singular must be False as\nwell."}, "Returns": "A Tensor of shape [..., M] or [..., M, K] containing the solutions.\nIf the input matrix is singular, the result is undefined.", "Raises": {"ValueError": "Is raised if any of the following conditions hold:\n\nAn unsupported type is provided as input,\nthe input tensors have incorrect shapes,\nperturb_singular is True but partial_pivoting is not.", "UnimplementedError": "Whenever partial_pivoting is true and the backend is\nXLA, or whenever perturb_singular is true and the backend is\nXLA or GPU."}}}, "tf.lite": {"tf.lite.Interpreter": {"description": "Interpreter interface for running TensorFlow Lite models.", "Args": {"model_path": "Path to TF-Lite Flatbuffer file.", "model_content": "Content of model.", "experimental_delegates": "Experimental. Subject to change. List of\nTfLiteDelegate\n  objects returned by lite.load_delegate().", "num_threads": "Sets the number of threads used by the interpreter and\navailable to CPU kernels. If not set, the interpreter will use an\nimplementation-dependent default number of threads. Currently, only a\nsubset of kernels, such as conv, support multi-threading. num_threads\nshould be >= -1. Setting num_threads to 0 has the effect to disable\nmultithreading, which is equivalent to setting num_threads to 1. If set\nto the value -1, the number of threads used will be\nimplementation-defined and platform-dependent.", "experimental_op_resolver_type": "The op resolver used by the interpreter. It\nmust be an instance of OpResolverType. By default, we use the built-in\nop resolver which corresponds to tflite::ops::builtin::BuiltinOpResolver\nin C++.", "experimental_preserve_all_tensors": "If true, then intermediate tensors used\nduring computation are preserved for inspection, and if the passed op\nresolver type is AUTO or BUILTIN, the type will be changed to\nBUILTIN_WITHOUT_DEFAULT_DELEGATES so that no Tensorflow Lite default\ndelegates are applied. If false, getting intermediate tensors could\nresult in undefined values or None, especially when the graph is\nsuccessfully modified by the Tensorflow Lite default delegate."}, "Raises": {"ValueError": "If the interpreter was unable to create."}}, "tf.lite.OpsSet": {"description": "Enum class defining the sets of ops available to generate TFLite models.", "Class Variables": {"EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8": "<OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8: 'EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8'>", "SELECT_TF_OPS": "<OpsSet.SELECT_TF_OPS: 'SELECT_TF_OPS'>", "TFLITE_BUILTINS": "<OpsSet.TFLITE_BUILTINS: 'TFLITE_BUILTINS'>", "TFLITE_BUILTINS_INT8": "<OpsSet.TFLITE_BUILTINS_INT8: 'TFLITE_BUILTINS_INT8'>"}}, "tf.lite.Optimize": {"description": "Enum defining the optimizations to apply when generating a tflite model.", "Class Variables": {"DEFAULT": "<Optimize.DEFAULT: 'DEFAULT'>", "EXPERIMENTAL_SPARSITY": "<Optimize.EXPERIMENTAL_SPARSITY: 'EXPERIMENTAL_SPARSITY'>", "OPTIMIZE_FOR_LATENCY": "<Optimize.OPTIMIZE_FOR_LATENCY: 'OPTIMIZE_FOR_LATENCY'>", "OPTIMIZE_FOR_SIZE": "<Optimize.OPTIMIZE_FOR_SIZE: 'OPTIMIZE_FOR_SIZE'>"}}, "tf.lite.RepresentativeDataset": {"description": "Representative dataset used to optimize the model.", "Args": {"input_gen": "A generator function that generates input samples for the\nmodel and has the same order, type and shape as the inputs to the model.\nUsually, this is a small subset of a few hundred samples randomly\nchosen, in no particular order, from the training or evaluation dataset."}}, "tf.lite.TFLiteConverter": {"description": "Converts a TensorFlow model into TensorFlow Lite model.", "Args": {"funcs": "List of TensorFlow ConcreteFunctions. The list should not contain\nduplicate elements.", "trackable_obj": "tf.AutoTrackable object associated with funcs. A\nreference to this object needs to be maintained so that Variables do not\nget garbage collected since functions have a weak reference to\nVariables. This is only required when the tf.AutoTrackable object is not\nmaintained by the user (e.g. from_saved_model)."}, "Attributes": {"optimizations": "Experimental flag, subject to change. Set of optimizations to\napply. e.g {tf.lite.Optimize.DEFAULT}. (default None, must be None or a\nset of values of type tf.lite.Optimize)", "representative_dataset": "A generator function used for integer quantization\nwhere each generated sample has the same order, type and shape as the\ninputs to the model. Usually, this is a small subset of a few hundred\nsamples randomly chosen, in no particular order, from the training or\nevaluation dataset. This is an optional attribute, but required for full\ninteger quantization, i.e, if tf.int8 is the only supported type in\ntarget_spec.supported_types. Refer to tf.lite.RepresentativeDataset.\n(default None)", "target_spec": "Experimental flag, subject to change. Specifications of target\ndevice, including supported ops set, supported types and a set of user's\ndefined TensorFlow operators required in the TensorFlow Lite runtime.\nRefer to tf.lite.TargetSpec.", "inference_input_type": "Data type of the input layer. Note that integer types\n(tf.int8 and tf.uint8) are currently only supported for post training\ninteger quantization and quantization aware training. (default tf.float32,\nmust be in {tf.float32, tf.int8, tf.uint8})", "inference_output_type": "Data type of the output layer. Note that integer\ntypes (tf.int8 and tf.uint8) are currently only supported for post\ntraining integer quantization and quantization aware training. (default\ntf.float32, must be in {tf.float32, tf.int8, tf.uint8})", "allow_custom_ops": "Boolean indicating whether to allow custom operations.\nWhen False, any unknown operation is an error. When True, custom ops are\ncreated for any op that is unknown. The developer needs to provide these\nto the TensorFlow Lite runtime with a custom resolver. (default False)", "exclude_conversion_metadata": "Whether not to embed the conversion metadata\ninto the converted model. (default False)", "experimental_new_converter": "Experimental flag, subject to change. Enables\nMLIR-based conversion. (default True)", "experimental_new_quantizer": "Experimental flag, subject to change. Enables\nMLIR-based quantization conversion instead of Flatbuffer-based conversion.\n(default True)", "experimental_enable_resource_variables": "Experimental flag, subject to\nchange. Enables resource variables to be converted by this converter. This\nis only allowed if from_saved_model interface is used. (default True)"}}, "tf.lite.TargetSpec": {"description": "Specification of target device used to optimize the model.", "Attributes": {"supported_ops": "Experimental flag, subject to change. Set of tf.lite.OpsSet\noptions, where each option represents a set of operators supported by the\ntarget device. (default {tf.lite.OpsSet.TFLITE_BUILTINS}))", "supported_types": "Set of tf.dtypes.DType data types supported on the target\ndevice. If initialized, optimization might be driven by the smallest type\nin this set. (default set())", "experimental_select_user_tf_ops": "Experimental flag, subject to change. Set\nof user's TensorFlow operators' names that are required in the TensorFlow\nLite runtime. These ops will be exported as select TensorFlow ops in the\nmodel (in conjunction with the tf.lite.OpsSet.SELECT_TF_OPS flag). This is\nan advanced feature that should only be used if the client is using TF ops\nthat may not be linked in by default with the TF ops that are provided\nwhen using the SELECT_TF_OPS path. The client is responsible for linking\nthese ops into the target runtime.", "experimental_supported_backends": "Experimental flag, subject to change.\nSet containing names of supported backends. Currently only \"GPU\" is\nsupported, more options will be available later."}}}, "tf.lite.experimental.authoring": {"tf.lite.experimental.authoring.compatible": {"description": "Wraps tf.function into a callable function with TFLite compatibility checking.", "Args": {"target": "A tf.function to decorate.", "converter_target_spec": "target_spec of TFLite converter parameter.", "**kwargs": "The keyword arguments of the decorator class _Compatible."}, "Returns": "A callable object of tf.lite.experimental.authoring._Compatible."}}, "tf.lookup": {"tf.lookup.KeyValueTensorInitializer": {"description": "Table initializers given keys and values tensors.", "Args": {"keys": "The tensor for the keys.", "values": "The tensor for the values.", "key_dtype": "The keys data type. Used when keys is a python array.", "value_dtype": "The values data type. Used when values is a python array.", "name": "A name for the operation (optional)."}, "Attributes": {"key_dtype": "The expected table key dtype.", "value_dtype": "The expected table value dtype."}}, "tf.lookup.StaticHashTable": {"description": "A generic hash table that is immutable once initialized.", "Args": {"initializer": "The table initializer to use. See HashTable kernel for\nsupported key and value types.", "default_value": "The value to use if a key is missing in the table.", "name": "A name for the operation (optional).", "experimental_is_anonymous": "Whether to use anonymous mode for the\ntable (default is False). In anonymous mode, the table\nresource can only be accessed via a resource handle. It can't\nbe looked up by a name. When all resource handles pointing to\nthat resource are gone, the resource will be deleted\nautomatically."}, "Attributes": {"default_value": "The default value of the table.", "key_dtype": "The table key dtype.", "name": "The name of the table.", "resource_handle": "Returns the resource handle associated with this Resource.", "value_dtype": "The table value dtype."}}, "tf.lookup.StaticVocabularyTable": {"description": "String to Id table that assigns out-of-vocabulary keys to hash buckets.", "Args": {"initializer": "A TableInitializerBase object that contains the data used\nto initialize the table. If None, then we only use out-of-vocab buckets.", "num_oov_buckets": "Number of buckets to use for out-of-vocabulary keys. Must\nbe greater than zero. If out-of-vocab buckets are not required, use\nStaticHashTable instead.", "lookup_key_dtype": "Data type of keys passed to lookup. Defaults to\ninitializer.key_dtype if initializer is specified, otherwise\ntf.string. Must be string or integer, and must be castable to\ninitializer.key_dtype.", "name": "A name for the operation (optional).", "experimental_is_anonymous": "Whether to use anonymous mode for the\ntable (default is False). In anonymous mode, the table\nresource can only be accessed via a resource handle. It can't\nbe looked up by a name. When all resource handles pointing to\nthat resource are gone, the resource will be deleted\nautomatically."}, "Raises": {"ValueError": "when num_oov_buckets is not positive.", "TypeError": "when lookup_key_dtype or initializer.key_dtype are not\ninteger or string. Also when initializer.value_dtype != int64."}, "Attributes": {"key_dtype": "The table key dtype.", "name": "The name of the table.", "resource_handle": "Returns the resource handle associated with this Resource.", "value_dtype": "The table value dtype."}}, "tf.lookup.TextFileIndex": {"description": "The key and value content to get from each line.", "Class Variables": {"LINE_NUMBER": "-1", "WHOLE_LINE": "-2"}}, "tf.lookup.TextFileInitializer": {"description": "Table initializers from a text file.", "Args": {"filename": "The filename of the text file to be used for initialization. The\npath must be accessible from wherever the graph is initialized (eg.\ntrainer or eval workers). The filename may be a scalar Tensor.", "key_dtype": "The key data type.", "key_index": "the index that represents information of a line to get the\ntable 'key' values from.", "value_dtype": "The value data type.", "value_index": "the index that represents information of a line to get the\ntable 'value' values from.'", "vocab_size": "The number of elements in the file, if known.", "delimiter": "The delimiter to separate fields in a line.", "name": "A name for the operation (optional).", "value_index_offset": "A number to add to all indices extracted from the file\nThis is useful for cases where a user would like to reserve one or more\nlow index values for control characters. For instance, if you would\nlike to ensure that no vocabulary item is mapped to index 0 (so you can\nreserve 0 for a masking value), you can set value_index_offset to 1;\nthis will mean that the first vocabulary element is mapped to 1\ninstead of 0."}, "Raises": {"ValueError": "when the filename is empty, or when the table key and value\ndata types do not match the expected data types."}, "Attributes": {"key_dtype": "The expected table key dtype.", "value_dtype": "The expected table value dtype."}}}, "tf.math": {"tf.math.abs": {"description": "Computes the absolute value of a tensor.", "Args": {"x": "A Tensor or SparseTensor of type float16, float32, float64,\nint32, int64, complex64 or complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor or SparseTensor of the same size, type and sparsity as x,\n  with absolute values. Note, for complex64 or complex128 input, the\n  returned Tensor will be of type float32 or float64, respectively.\nIf x is a SparseTensor, returns\nSparseTensor(x.indices, tf.math.abs(x.values, ...), x.dense_shape)"}, "tf.math.accumulate_n": {"description": "Returns the element-wise sum of a list of tensors.", "Args": {"inputs": "A list of Tensor objects, each with same shape and type.", "shape": "Expected shape of elements of inputs (optional). Also controls the\noutput shape of this op, which may affect type inference in other ops. A\nvalue of None means \"infer the input shape from the shapes in inputs\".", "tensor_dtype": "Expected data type of inputs (optional). A value of None\nmeans \"infer the input dtype from inputs[0]\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of same shape and type as the elements of inputs.", "Raises": {"ValueError": "If inputs don't all have same shape and dtype or the shape\ncannot be inferred."}}, "tf.math.acos": {"description": "Computes acos of x element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half,\nfloat32, float64, uint8, int8, int16, int32, int64,\ncomplex64, complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.math.acosh": {"description": "Computes inverse hyperbolic cosine of x element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, complex64, complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.math.add": {"description": "Returns x &#43; y element-wise.", "Args": {"x": "A tf.Tensor. Must be one of the following types: bfloat16, half,\nfloat32, float64, uint8, int8, int16, int32, int64, complex64, complex128,\nstring.", "y": "A tf.Tensor. Must have the same type as x.", "name": "A name for the operation (optional)"}}, "tf.math.add_n": {"description": "Adds all input tensors element-wise.", "Args": {"inputs": "A list of tf.Tensor or tf.IndexedSlices objects, each with the\nsame shape and type. tf.IndexedSlices objects will be converted into\ndense tensors prior to adding.", "name": "A name for the operation (optional)."}, "Returns": "A tf.Tensor of the same shape and type as the elements of inputs.", "Raises": {"ValueError": "If inputs don't all have same shape and dtype or the shape\ncannot be inferred."}}, "tf.math.angle": {"description": "Returns the element-wise argument of a complex (or real) tensor.", "Args": {"input": "A Tensor. Must be one of the following types: float, double,\ncomplex64, complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type float32 or float64."}, "tf.math.approx_max_k": {"description": "Returns max k values and their indices of the input operand in an approximate manner.", "Args": {"operand": "Array to search for max-k. Must be a floating number type.", "k": "Specifies the number of max-k.", "reduction_dimension": "Integer dimension along which to search. Default: -1.", "recall_target": "Recall target for the approximation.", "reduction_input_size_override": "When set to a positive value, it overrides\nthe size determined by operand[reduction_dim] for evaluating the recall.\nThis option is useful when the given operand is only a subset of the\noverall computation in SPMD or distributed pipelines, where the true input\nsize cannot be deferred by the operand shape.", "aggregate_to_topk": "When true, aggregates approximate results to top-k. When\nfalse, returns the approximate results. The number of the approximate\nresults is implementation defined and is greater equals to the specified\nk.", "name": "Optional name for the operation."}, "Returns": "Tuple of two arrays. The arrays are the max k values and the\ncorresponding indices along the reduction_dimension of the input\noperand. The arrays' dimensions are the same as the input operand\nexcept for the reduction_dimension: when aggregate_to_topk is true,\nthe reduction dimension is k; otherwise, it is greater equals to k\nwhere the size is implementation-defined."}, "tf.math.approx_min_k": {"description": "Returns min k values and their indices of the input operand in an approximate manner.", "Args": {"operand": "Array to search for min-k. Must be a floating number type.", "k": "Specifies the number of min-k.", "reduction_dimension": "Integer dimension along which to search. Default: -1.", "recall_target": "Recall target for the approximation.", "reduction_input_size_override": "When set to a positive value, it overrides\nthe size determined by operand[reduction_dim] for evaluating the recall.\nThis option is useful when the given operand is only a subset of the\noverall computation in SPMD or distributed pipelines, where the true input\nsize cannot be deferred by the operand shape.", "aggregate_to_topk": "When true, aggregates approximate results to top-k. When\nfalse, returns the approximate results. The number of the approximate\nresults is implementation defined and is greater equals to the specified\nk.", "name": "Optional name for the operation."}, "Returns": "Tuple of two arrays. The arrays are the least k values and the\ncorresponding indices along the reduction_dimension of the input\noperand.  The arrays' dimensions are the same as the input operand\nexcept for the reduction_dimension: when aggregate_to_topk is true,\nthe reduction dimension is k; otherwise, it is greater equals to k\nwhere the size is implementation-defined."}, "tf.math.argmax": {"description": "Returns the index with the largest value across axes of a tensor.", "Args": {"input": "A Tensor.", "axis": "An integer, the axis to reduce across. Default to 0.", "output_type": "An optional output dtype (tf.int32 or tf.int64). Defaults\nto tf.int64.", "name": "An optional name for the operation."}, "Returns": "A Tensor of type output_type."}, "tf.math.argmin": {"description": "Returns the index with the smallest value across axes of a tensor.", "Args": {"input": "A Tensor. Must be one of the following types: float32, float64,\nint32, uint8, int16, int8, complex64, int64, qint8,\nquint8, qint32, bfloat16, uint16, complex128, half, uint32,\nuint64.", "axis": "A Tensor. Must be one of the following types: int32, int64.\nint32 or int64, must be in the range -rank(input), rank(input)).\nDescribes which axis of the input Tensor to reduce across. For vectors,\nuse axis = 0.", "output_type": "An optional tf.DType from: tf.int32, tf.int64. Defaults to\ntf.int64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type output_type."}, "tf.math.asin": {"description": "Computes the trignometric inverse sine of x element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, int8, int16, int32, int64, complex64, complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.math.asinh": {"description": "Computes inverse hyperbolic sine of x element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, complex64, complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.math.atan": {"description": "Computes the trignometric inverse tangent of x element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, int8, int16, int32, int64, complex64, complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.math.atan2": {"description": "Computes arctangent of y/x element-wise, respecting signs of the arguments.", "Args": {"y": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "x": "A Tensor. Must have the same type as y.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as y."}, "tf.math.atanh": {"description": "Computes inverse hyperbolic tangent of x element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, complex64, complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.math.bessel_i0": {"description": "Computes the Bessel i0 function of x element-wise.", "Args": {"x": "A Tensor or SparseTensor. Must be one of the following types: half,\nfloat32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor or SparseTensor, respectively. Has the same type as x."}, "tf.math.bessel_i0e": {"description": "Computes the Bessel i0e function of x element-wise.", "Args": {"x": "A Tensor or SparseTensor. Must be one of the following types: half,\nfloat32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor or SparseTensor, respectively. Has the same type as x.\nIf x is a SparseTensor, returns\nSparseTensor(x.indices, tf.math.bessel_i0e(x.values, ...), x.dense_shape)"}, "tf.math.bessel_i1": {"description": "Computes the Bessel i1 function of x element-wise.", "Args": {"x": "A Tensor or SparseTensor. Must be one of the following types: half,\nfloat32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor or SparseTensor, respectively. Has the same type as x."}, "tf.math.bessel_i1e": {"description": "Computes the Bessel i1e function of x element-wise.", "Args": {"x": "A Tensor or SparseTensor. Must be one of the following types: half,\nfloat32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor or SparseTensor, respectively. Has the same type as x.\nIf x is a SparseTensor, returns\nSparseTensor(x.indices, tf.math.bessel_i1e(x.values, ...), x.dense_shape)"}, "tf.math.betainc": {"description": "Compute the regularized incomplete beta integral \\\\(I_x(a, b)\\\\).", "Args": {"a": "A Tensor. Must be one of the following types: float32, float64.", "b": "A Tensor. Must have the same type as a.", "x": "A Tensor. Must have the same type as a.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as a."}, "tf.math.bincount": {"description": "Counts the number of occurrences of each value in an integer array.", "Args": {"arr": "A Tensor, RaggedTensor, or SparseTensor whose values should be counted.\nThese tensors must have a rank of 2 if axis=-1.", "weights": "If non-None, must be the same shape as arr. For each value in\narr, the bin will be incremented by the corresponding weight instead of\n1.", "minlength": "If given, ensures the output has length at least minlength,\npadding with zeros at the end if necessary.", "maxlength": "If given, skips values in arr that are equal or greater than\nmaxlength, ensuring that the output has length at most maxlength.", "dtype": "If weights is None, determines the type of the output bins.", "name": "A name scope for the associated operations (optional).", "axis": "The axis to slice over. Axes at and below axis will be flattened\nbefore bin counting. Currently, only 0, and -1 are supported. If None,\nall axes will be flattened (identical to passing 0).", "binary_output": "If True, this op will output 1 instead of the number of times\na token appears (equivalent to one_hot + reduce_any instead of one_hot +\nreduce_add). Defaults to False."}, "Returns": "A vector with the same dtype as weights or the given dtype. The bin\nvalues.", "Raises": {}}, "tf.math.ceil": {"description": "Return the ceiling of the input, element-wise.", "Args": {"x": "A tf.Tensor. Must be one of the following types: bfloat16, half,\nfloat32, float64. int32", "name": "A name for the operation (optional)."}, "Returns": "A tf.Tensor. Has the same type as x."}, "tf.math.confusion_matrix": {"description": "Computes the confusion matrix from predictions and labels.", "Args": {"labels": "1-D Tensor of real labels for the classification task.", "predictions": "1-D Tensor of predictions for a given classification.", "num_classes": "The possible number of labels the classification task can\nhave. If this value is not provided, it will be calculated\nusing both predictions and labels array.", "weights": "An optional Tensor whose shape matches predictions.", "dtype": "Data type of the confusion matrix.", "name": "Scope name."}, "Returns": "A Tensor of type dtype with shape [n, n] representing the confusion\nmatrix, where n is the number of possible labels in the classification\ntask.", "Raises": {"ValueError": "If both predictions and labels are not 1-D vectors and have\nmismatched shapes, or if weights is not None and its shape doesn't\nmatch predictions."}}, "tf.math.conj": {"description": "Returns the complex conjugate of a complex number.", "Args": {"x": "Tensor to conjugate.  Must have numeric or variant type.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor that is the conjugate of x (with the same type).", "Raises": {"TypeError": "If x is not a numeric tensor."}}, "tf.math.cos": {"description": "Computes cos of x element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, complex64, complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.math.cosh": {"description": "Computes hyperbolic cosine of x element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, complex64, complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.math.count_nonzero": {"description": "Computes number of nonzero elements across dimensions of a tensor.", "Args": {"input": "The tensor to reduce. Should be of numeric type, bool, or string.", "axis": "The dimensions to reduce. If None (the default), reduces all\ndimensions. Must be in the range [-rank(input), rank(input)).", "keepdims": "If true, retains reduced dimensions with length 1.", "dtype": "The output dtype; defaults to tf.int64.", "name": "A name for the operation (optional)."}, "Returns": "The reduced tensor (number of nonzero values)."}, "tf.math.cumprod": {"description": "Compute the cumulative product of the tensor x along axis.", "Args": {"x": "A Tensor. Must be one of the following types: float32, float64,\nint64, int32, uint8, uint16, int16, int8, complex64,\ncomplex128, qint8, quint8, qint32, half.", "axis": "A Tensor of type int32 (default: 0). Must be in the range\n[-rank(x), rank(x)).", "exclusive": "If True, perform exclusive cumprod.", "reverse": "A bool (default: False).", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.math.cumsum": {"description": "Compute the cumulative sum of the tensor x along axis.", "Args": {"x": "A Tensor. Must be one of the following types: float32, float64,\nint64, int32, uint8, uint16, int16, int8, complex64,\ncomplex128, qint8, quint8, qint32, half.", "axis": "A Tensor of type int32 (default: 0). Must be in the range\n[-rank(x), rank(x)).", "exclusive": "If True, perform exclusive cumsum.", "reverse": "A bool (default: False).", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.math.cumulative_logsumexp": {"description": "Compute the cumulative log-sum-exp of the tensor x along axis.", "Args": {"x": "A Tensor. Must be one of the following types: float16, float32,\nfloat64.", "axis": "A Tensor of type int32 or int64 (default: 0). Must be in the\nrange [-rank(x), rank(x)).", "exclusive": "If True, perform exclusive cumulative log-sum-exp.", "reverse": "If True, performs the cumulative log-sum-exp in the reverse\ndirection.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same shape and type as x."}, "tf.math.digamma": {"description": "Computes Psi, the derivative of Lgamma (the log of the absolute value of", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.math.divide": {"description": "Computes Python style division of x by y.", "Args": {"x": "A Tensor", "y": "A Tensor", "name": "A name for the operation (optional)."}, "Returns": "A Tensor with same shape as input"}, "tf.math.divide_no_nan": {"description": "Computes a safe divide which returns 0 if y (denominator) is zero.", "Args": {"x": "A Tensor. Must be one of the following types: float32, float64.", "y": "A Tensor whose dtype is compatible with x.", "name": "A name for the operation (optional)."}, "Returns": "The element-wise value of the x divided by y."}, "tf.math.equal": {"description": "Returns the truth value of (x == y) element-wise.", "Args": {"x": "A tf.Tensor or tf.sparse.SparseTensor or tf.IndexedSlices.", "y": "A tf.Tensor or tf.sparse.SparseTensor or tf.IndexedSlices.", "name": "A name for the operation (optional)."}, "Returns": "A tf.Tensor of type bool with the same size as that of x or y.", "Raises": {}}, "tf.math.erf": {"description": "Computes the [Gauss error function](https://en.wikipedia.org/wiki/Error_function) of x element-wise. In statistics, for non-negative values of \\\\(x\\\\), the error function has the following interpretation: for a random variable \\\\(Y\\\\) that is normally distributed with mean 0 and variance \\\\(1/\\sqrt{2}\\\\), \\\\(erf(x)\\\\) is the probability that \\\\(Y\\\\) falls in the range \\\\([\u2212x, x]\\\\).", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x.\nIf x is a SparseTensor, returns\nSparseTensor(x.indices, tf.math.erf(x.values, ...), x.dense_shape)"}, "tf.math.erfc": {"description": "Computes the complementary error function of x element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.math.erfcinv": {"description": "Computes the inverse of complementary error function.", "Args": {"x": "Tensor with type float or double.", "name": "A name for the operation (optional)."}, "Returns": "Inverse complementary error function of x."}, "tf.math.erfinv": {"description": "Compute inverse error function.", "Args": {"x": "Tensor with type float or double.", "name": "A name for the operation (optional)."}, "Returns": "Inverse error function of x."}, "tf.math.exp": {"description": "Computes exponential of x element-wise. \\\\(y = e^x\\\\).", "Args": {"x": "A tf.Tensor. Must be one of the following types: bfloat16, half,\nfloat32, float64, complex64, complex128.", "name": "A name for the operation (optional)."}, "Returns": "A tf.Tensor. Has the same type as x."}, "tf.math.expm1": {"description": "Computes exp(x) - 1 element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, complex64, complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.math.floor": {"description": "Returns element-wise largest integer not greater than x.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half,\nfloat32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.math.floordiv": {"description": "Divides x / y elementwise, rounding toward the most negative integer.", "Args": {"x": "Tensor numerator of real numeric type.", "y": "Tensor denominator of real numeric type.", "name": "A name for the operation (optional)."}, "Returns": "x / y rounded toward -infinity.", "Raises": {"TypeError": "If the inputs are complex."}}, "tf.math.floormod": {"description": "Returns element-wise remainder of division. When x &lt; 0 xor y &lt; 0 is", "Args": {"x": "A Tensor. Must be one of the following types: int8, int16, int32, int64, uint8, uint16, uint32, uint64, bfloat16, half, float32, float64.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.math.greater": {"description": "Returns the truth value of (x &gt; y) element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type bool."}, "tf.math.greater_equal": {"description": "Returns the truth value of (x &gt;= y) element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type bool."}, "tf.math.igamma": {"description": "Compute the lower regularized incomplete Gamma function P(a, x).", "Args": {"a": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "x": "A Tensor. Must have the same type as a.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as a."}, "tf.math.igammac": {"description": "Compute the upper regularized incomplete Gamma function Q(a, x).", "Args": {"a": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "x": "A Tensor. Must have the same type as a.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as a."}, "tf.math.imag": {"description": "Returns the imaginary part of a complex (or real) tensor.", "Args": {"input": "A Tensor. Must be one of the following types: float, double,\ncomplex64, complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type float32 or float64."}, "tf.math.in_top_k": {"description": "Says whether the targets are in the top K predictions.", "Args": {"predictions": "A Tensor of type float32.\nA batch_size x classes tensor.", "targets": "A Tensor. Must be one of the following types: int32, int64.\nA batch_size vector of class ids.", "k": "An int. Number of top elements to look at for computing precision.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type bool. Computed Precision at k as a bool Tensor."}, "tf.math.invert_permutation": {"description": "Computes the inverse permutation of a tensor.", "Args": {"x": "A Tensor. Must be one of the following types: int32, int64. 1-D.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.math.is_finite": {"description": "Returns which elements of x are finite.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type bool."}, "tf.math.is_inf": {"description": "Returns which elements of x are Inf.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type bool."}, "tf.math.is_nan": {"description": "Returns which elements of x are NaN.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type bool."}, "tf.math.is_non_decreasing": {"description": "Returns True if x is non-decreasing.", "Args": {"x": "Numeric Tensor.", "name": "A name for this operation (optional).  Defaults to \"is_non_decreasing\""}, "Returns": "Boolean Tensor, equal to True iff x is non-decreasing.", "Raises": {"TypeError": "if x is not a numeric tensor."}}, "tf.math.is_strictly_increasing": {"description": "Returns True if x is strictly increasing.", "Args": {"x": "Numeric Tensor.", "name": "A name for this operation (optional).\nDefaults to \"is_strictly_increasing\""}, "Returns": "Boolean Tensor, equal to True iff x is strictly increasing.", "Raises": {"TypeError": "if x is not a numeric tensor."}}, "tf.math.l2_normalize": {"description": "Normalizes along dimension axis using an L2 norm. (deprecated arguments)", "Args": {"x": "A Tensor.", "axis": "Dimension along which to normalize.  A scalar or a vector of\nintegers.", "epsilon": "A lower bound value for the norm. Will use sqrt(epsilon) as the\ndivisor if norm < sqrt(epsilon).", "name": "A name for this operation (optional).", "dim": "Deprecated, do not use."}, "Returns": "A Tensor with the same shape as x."}, "tf.math.lbeta": {"description": "Computes \\\\(ln(|Beta(x)|)\\\\), reducing along the last dimension.", "Args": {"x": "A rank n + 1 Tensor, n >= 0 with type float, or double.", "name": "A name for the operation (optional)."}, "Returns": "The logarithm of |Beta(x)| reducing along the last dimension."}, "tf.math.less": {"description": "Returns the truth value of (x &lt; y) element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type bool."}, "tf.math.less_equal": {"description": "Returns the truth value of (x &lt;= y) element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type bool."}, "tf.math.lgamma": {"description": "Computes the log of the absolute value of Gamma(x) element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.math.log": {"description": "Computes natural logarithm of x element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, complex64, complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.math.log1p": {"description": "Computes natural logarithm of (1 &#43; x) element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, complex64, complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.math.log_sigmoid": {"description": "Computes log sigmoid of x element-wise.", "Args": {"x": "A Tensor with type float32 or float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor with the same type as x."}, "tf.math.logical_and": {"description": "Returns the truth value of x AND y element-wise.", "Args": {"x": "A Tensor of type bool.", "y": "A Tensor of type bool.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type bool."}, "tf.math.logical_not": {"description": "Returns the truth value of NOT x element-wise.", "Args": {"x": "A Tensor of type bool. A Tensor of type bool.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type bool."}, "tf.math.logical_or": {"description": "Returns the truth value of x OR y element-wise.", "Args": {"x": "A Tensor of type bool.", "y": "A Tensor of type bool.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type bool."}, "tf.math.logical_xor": {"description": "Logical XOR function.", "Args": {"x": "A tf.Tensor type bool.", "y": "A tf.Tensor of type bool.", "name": "A name for the operation (optional)."}, "Returns": "A tf.Tensor of type bool with the same size as that of x or y."}, "tf.math.maximum": {"description": "Returns the max of x and y (i.e. x &gt; y ? x : y) element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, int8, uint8, int16, uint16, int32, uint32, int64, uint64.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.math.minimum": {"description": "Returns the min of x and y (i.e. x &lt; y ? x : y) element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, int8, uint8, int16, uint16, int32, uint32, int64, uint64.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.math.multiply": {"description": "Returns an element-wise x * y.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16,\nhalf, float32, float64, uint8, int8, uint16,\nint16, int32, int64, complex64, complex128.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Raises": {}}, "tf.math.multiply_no_nan": {"description": "Computes the product of x and y and returns 0 if the y is zero, even if x is NaN or infinite.", "Args": {"x": "A Tensor. Must be one of the following types: float32, float64.", "y": "A Tensor whose dtype is compatible with x.", "name": "A name for the operation (optional)."}, "Returns": "The element-wise value of the x times y."}, "tf.math.ndtri": {"description": "Compute quantile of Standard Normal.", "Args": {"x": "Tensor with type float or double.", "name": "A name for the operation (optional)."}, "Returns": "Inverse error function of x."}, "tf.math.negative": {"description": "Computes numerical negative value element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, int8, int16, int32, int64, complex64, complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x.\nIf x is a SparseTensor, returns\nSparseTensor(x.indices, tf.math.negative(x.values, ...), x.dense_shape)"}, "tf.math.nextafter": {"description": "Returns the next representable value of x1 in the direction of x2, element-wise.", "Args": {"x1": "A Tensor. Must be one of the following types: float64, float32.", "x2": "A Tensor. Must have the same type as x1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x1."}, "tf.math.not_equal": {"description": "Returns the truth value of (x != y) element-wise.", "Args": {"x": "A tf.Tensor or tf.sparse.SparseTensor or tf.IndexedSlices.", "y": "A tf.Tensor or tf.sparse.SparseTensor or tf.IndexedSlices.", "name": "A name for the operation (optional)."}, "Returns": "A tf.Tensor of type bool with the same size as that of x or y.", "Raises": {}}, "tf.math.polygamma": {"description": "Compute the polygamma function \\\\(\\psi^{(n)}(x)\\\\).", "Args": {"a": "A Tensor. Must be one of the following types: float32, float64.", "x": "A Tensor. Must have the same type as a.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as a."}, "tf.math.polyval": {"description": "Computes the elementwise value of a polynomial.", "Args": {"coeffs": "A list of Tensor representing the coefficients of the polynomial.", "x": "A Tensor representing the variable of the polynomial.", "name": "A name for the operation (optional)."}, "Returns": "A tensor of the shape as the expression p(x) with usual broadcasting\nrules for element-wise addition and multiplication applied."}, "tf.math.pow": {"description": "Computes the power of one value to another.", "Args": {"x": "A Tensor of type float16, float32, float64, int32, int64,\ncomplex64, or complex128.", "y": "A Tensor of type float16, float32, float64, int32, int64,\ncomplex64, or complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor."}, "tf.math.real": {"description": "Returns the real part of a complex (or real) tensor.", "Args": {"input": "A Tensor. Must have numeric type.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type float32 or float64."}, "tf.math.reciprocal": {"description": "Computes the reciprocal of x element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, int8, int16, int32, int64, complex64, complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.math.reciprocal_no_nan": {"description": "Performs a safe reciprocal operation, element wise.", "Args": {"x": "A Tensor of type float16, float32, float64 complex64 or\ncomplex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of same shape and type as x.", "Raises": {"TypeError": "x must be of a valid dtype."}}, "tf.math.reduce_all": {"description": "Computes tf.math.logical_and of elements across dimensions of a tensor.", "Args": {"input_tensor": "The boolean tensor to reduce.", "axis": "The dimensions to reduce. If None (the default), reduces all\ndimensions. Must be in the range [-rank(input_tensor),\nrank(input_tensor)).", "keepdims": "If true, retains reduced dimensions with length 1.", "name": "A name for the operation (optional)."}, "Returns": "The reduced tensor."}, "tf.math.reduce_any": {"description": "Computes tf.math.logical_or of elements across dimensions of a tensor.", "Args": {"input_tensor": "The boolean tensor to reduce.", "axis": "The dimensions to reduce. If None (the default), reduces all\ndimensions. Must be in the range [-rank(input_tensor),\nrank(input_tensor)).", "keepdims": "If true, retains reduced dimensions with length 1.", "name": "A name for the operation (optional)."}, "Returns": "The reduced tensor."}, "tf.math.reduce_euclidean_norm": {"description": "Computes the Euclidean norm of elements across dimensions of a tensor.", "Args": {"input_tensor": "The tensor to reduce. Should have numeric type.", "axis": "The dimensions to reduce. If None (the default), reduces all\ndimensions. Must be in the range [-rank(input_tensor),\nrank(input_tensor)).", "keepdims": "If true, retains reduced dimensions with length 1.", "name": "A name for the operation (optional)."}, "Returns": "The reduced tensor, of the same dtype as the input_tensor."}, "tf.math.reduce_logsumexp": {"description": "Computes log(sum(exp(elements across dimensions of a tensor))).", "Args": {"input_tensor": "The tensor to reduce. Should have numeric type.", "axis": "The dimensions to reduce. If None (the default), reduces all\ndimensions. Must be in the range [-rank(input_tensor),\nrank(input_tensor)).", "keepdims": "If true, retains reduced dimensions with length 1.", "name": "A name for the operation (optional)."}, "Returns": "The reduced tensor."}, "tf.math.reduce_max": {"description": "Computes tf.math.maximum of elements across dimensions of a tensor.", "Args": {"input_tensor": "The tensor to reduce. Should have real numeric type.", "axis": "The dimensions to reduce. If None (the default), reduces all\ndimensions. Must be in the range [-rank(input_tensor),\nrank(input_tensor)).", "keepdims": "If true, retains reduced dimensions with length 1.", "name": "A name for the operation (optional)."}, "Returns": "The reduced tensor."}, "tf.math.reduce_mean": {"description": "Computes the mean of elements across dimensions of a tensor.", "Args": {"input_tensor": "The tensor to reduce. Should have numeric type.", "axis": "The dimensions to reduce. If None (the default), reduces all\ndimensions. Must be in the range [-rank(input_tensor),\nrank(input_tensor)).", "keepdims": "If true, retains reduced dimensions with length 1.", "name": "A name for the operation (optional)."}, "Returns": "The reduced tensor."}, "tf.math.reduce_min": {"description": "Computes the tf.math.minimum of elements across dimensions of a tensor.", "Args": {"input_tensor": "The tensor to reduce. Should have real numeric type.", "axis": "The dimensions to reduce. If None (the default), reduces all\ndimensions. Must be in the range [-rank(input_tensor),\nrank(input_tensor)).", "keepdims": "If true, retains reduced dimensions with length 1.", "name": "A name for the operation (optional)."}, "Returns": "The reduced tensor."}, "tf.math.reduce_prod": {"description": "Computes tf.math.multiply of elements across dimensions of a tensor.", "Args": {"input_tensor": "The tensor to reduce. Should have numeric type.", "axis": "The dimensions to reduce. If None (the default), reduces all\ndimensions. Must be in the range [-rank(input_tensor),\nrank(input_tensor)).", "keepdims": "If true, retains reduced dimensions with length 1.", "name": "A name for the operation (optional)."}, "Returns": "The reduced tensor."}, "tf.math.reduce_std": {"description": "Computes the standard deviation of elements across dimensions of a tensor.", "Args": {"input_tensor": "The tensor to reduce. Should have real or complex type.", "axis": "The dimensions to reduce. If None (the default), reduces all\ndimensions. Must be in the range [-rank(input_tensor),\nrank(input_tensor)).", "keepdims": "If true, retains reduced dimensions with length 1.", "name": "A name scope for the associated operations (optional)."}, "Returns": "The reduced tensor, of the same dtype as the input_tensor. Note,  for\ncomplex64 or complex128 input, the returned Tensor will be of type\nfloat32 or float64, respectively."}, "tf.math.reduce_sum": {"description": "Computes the sum of elements across dimensions of a tensor.", "Args": {"input_tensor": "The tensor to reduce. Should have numeric type.", "axis": "The dimensions to reduce. If None (the default), reduces all\ndimensions. Must be in the range [-rank(input_tensor),\nrank(input_tensor)].", "keepdims": "If true, retains reduced dimensions with length 1.", "name": "A name for the operation (optional)."}, "Returns": "The reduced tensor, of the same dtype as the input_tensor."}, "tf.math.reduce_variance": {"description": "Computes the variance of elements across dimensions of a tensor.", "Args": {"input_tensor": "The tensor to reduce. Should have real or complex type.", "axis": "The dimensions to reduce. If None (the default), reduces all\ndimensions. Must be in the range [-rank(input_tensor),\nrank(input_tensor)).", "keepdims": "If true, retains reduced dimensions with length 1.", "name": "A name scope for the associated operations (optional)."}, "Returns": "The reduced tensor, of the same dtype as the input_tensor. Note,  for\ncomplex64 or complex128 input, the returned Tensor will be of type\nfloat32 or float64, respectively."}, "tf.math.rint": {"description": "Returns element-wise integer closest to x.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.math.round": {"description": "Rounds the values of a tensor to the nearest integer, element-wise.", "Args": {"x": "A Tensor of type float16, float32, float64, int32, or int64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of same shape and type as x."}, "tf.math.rsqrt": {"description": "Computes reciprocal of square root of x element-wise.", "Args": {"x": "A tf.Tensor. Must be one of the following types: bfloat16, half,\nfloat32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A tf.Tensor. Has the same type as x."}, "tf.math.scalar_mul": {"description": "Multiplies a scalar times a Tensor or IndexedSlices object.", "Args": {"scalar": "A 0-D scalar Tensor. Must have known shape.", "x": "A Tensor or IndexedSlices to be scaled.", "name": "A name for the operation (optional)."}, "Returns": "scalar * x of the same type (Tensor or IndexedSlices) as x.", "Raises": {"ValueError": "if scalar is not a 0-D scalar."}}, "tf.math.segment_max": {"description": "Computes the maximum along segments of a tensor.", "Args": {"data": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.", "segment_ids": "A Tensor. Must be one of the following types: int32, int64.\nA 1-D tensor whose size is equal to the size of data's\nfirst dimension.  Values should be sorted and can be repeated.\nCaution: The values are always validated to be sorted on CPU, never validated\non GPU.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as data."}, "tf.math.segment_mean": {"description": "Computes the mean along segments of a tensor.", "Args": {"data": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.", "segment_ids": "A Tensor. Must be one of the following types: int32, int64.\nA 1-D tensor whose size is equal to the size of data's\nfirst dimension.  Values should be sorted and can be repeated.\nCaution: The values are always validated to be sorted on CPU, never validated\non GPU.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as data."}, "tf.math.segment_min": {"description": "Computes the minimum along segments of a tensor.", "Args": {"data": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.", "segment_ids": "A Tensor. Must be one of the following types: int32, int64.\nA 1-D tensor whose size is equal to the size of data's\nfirst dimension.  Values should be sorted and can be repeated.\nCaution: The values are always validated to be sorted on CPU, never validated\non GPU.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as data."}, "tf.math.segment_prod": {"description": "Computes the product along segments of a tensor.", "Args": {"data": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.", "segment_ids": "A Tensor. Must be one of the following types: int32, int64.\nA 1-D tensor whose size is equal to the size of data's\nfirst dimension.  Values should be sorted and can be repeated.\nCaution: The values are always validated to be sorted on CPU, never validated\non GPU.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as data."}, "tf.math.segment_sum": {"description": "Computes the sum along segments of a tensor.", "Args": {"data": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.", "segment_ids": "A Tensor. Must be one of the following types: int32, int64.\nA 1-D tensor whose size is equal to the size of data's\nfirst dimension.  Values should be sorted and can be repeated.\nCaution: The values are always validated to be sorted on CPU, never validated\non GPU.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as data."}, "tf.math.sigmoid": {"description": "Computes sigmoid of x element-wise.", "Args": {"x": "A Tensor with type float16, float32, float64, complex64, or\ncomplex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor with the same type as x."}, "tf.math.sign": {"description": "Returns an element-wise indication of the sign of a number.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32,\nfloat64, int32, int64, complex64, complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x.\nIf x is a SparseTensor, returns SparseTensor(x.indices,\n  tf.math.sign(x.values, ...), x.dense_shape).\nIf x is a SparseTensor, returns\n SparseTensor(x.indices, tf.math.sign(x.values, ...), x.dense_shape)"}, "tf.math.sin": {"description": "Computes sine of x element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, complex64, complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.math.sinh": {"description": "Computes hyperbolic sine of x element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, complex64, complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.math.sobol_sample": {"description": "Generates points from the Sobol sequence.", "Args": {"dim": "Positive scalar Tensor representing each sample's dimension.", "num_results": "Positive scalar Tensor of dtype int32. The number of Sobol\npoints to return in the output.", "skip": "(Optional) Positive scalar Tensor of dtype int32. The number of\ninitial points of the Sobol sequence to skip. Default value is 0.", "dtype": "(Optional) The tf.Dtype of the sample. One of: tf.float32 or\ntf.float64. Defaults to tf.float32.", "name": "(Optional) Python str name prefixed to ops created by this function."}, "Returns": "Tensor of samples from Sobol sequence with shape [num_results, dim]."}, "tf.math.softplus": {"description": "Computes elementwise softplus: softplus(x) = log(exp(x) &#43; 1).", "Args": {"features": "Tensor", "name": "Optional: name to associate with this operation."}, "Returns": "Tensor"}, "tf.math.sqrt": {"description": "Computes element-wise square root of the input tensor.", "Args": {"x": "A tf.Tensor of type bfloat16, half, float32, float64,\ncomplex64, complex128", "name": "A name for the operation (optional)."}, "Returns": "A tf.Tensor of same size, type and sparsity as x.\nIf x is a SparseTensor, returns\nSparseTensor(x.indices, tf.math.sqrt(x.values, ...), x.dense_shape)"}, "tf.math.square": {"description": "Computes square of x element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, int8, int16, int32, int64, uint8, uint16, uint32, uint64, complex64, complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x.\nIf x is a SparseTensor, returns\nSparseTensor(x.indices, tf.math.square(x.values, ...), x.dense_shape)"}, "tf.math.squared_difference": {"description": "Returns conj(x - y)(x - y) element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, int32, int64, complex64, complex128.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.math.subtract": {"description": "Returns x - y element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, uint8, int8, uint16, int16, int32, int64, complex64, complex128, uint32, uint64.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.math.tan": {"description": "Computes tan of x element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, int8, int16, int32, int64, complex64, complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.math.tanh": {"description": "Computes hyperbolic tangent of x element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, complex64, complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x.\nIf x is a SparseTensor, returns\nSparseTensor(x.indices, tf.math.tanh(x.values, ...), x.dense_shape)"}, "tf.math.top_k": {"description": "Finds values and indices of the k largest entries for the last dimension.", "Args": {"input": "1-D or higher Tensor with last dimension at least k.", "k": "0-D int32 Tensor.  Number of top elements to look for along the last\ndimension (along each row for matrices).", "sorted": "If true the resulting k elements will be sorted by the values in\ndescending order.", "name": "Optional name for the operation."}, "Returns": "A tuple with two named fields:"}, "tf.math.truediv": {"description": "Divides x / y elementwise (using Python 3 division operator semantics).", "Args": {"x": "Tensor numerator of numeric type.", "y": "Tensor denominator of numeric type.", "name": "A name for the operation (optional)."}, "Returns": "x / y evaluated in floating point.", "Raises": {"TypeError": "If x and y have different dtypes."}}, "tf.math.unsorted_segment_max": {"description": "Computes the maximum along segments of a tensor.", "Args": {"data": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.", "segment_ids": "A Tensor. Must be one of the following types: int32, int64.\nA tensor whose shape is a prefix of data.shape.\nThe values must be less than num_segments.\nCaution: The values are always validated to be in range on CPU, never validated\non GPU.", "num_segments": "A Tensor. Must be one of the following types: int32, int64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as data."}, "tf.math.unsorted_segment_mean": {"description": "Computes the mean along segments of a tensor.", "Args": {"data": "A Tensor with floating point or complex dtype.", "segment_ids": "An integer tensor whose shape is a prefix of data.shape.\nThe values must be less than num_segments.\nThe values are always validated to be in range on CPU,\nnever validated on GPU.", "num_segments": "An integer scalar Tensor.  The number of distinct segment\nIDs.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor.  Has same shape as data, except for the first segment_ids.rank\n dimensions, which are replaced with a single dimension which has size\nnum_segments."}, "tf.math.unsorted_segment_min": {"description": "Computes the minimum along segments of a tensor.", "Args": {"data": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.", "segment_ids": "A Tensor. Must be one of the following types: int32, int64.\nA tensor whose shape is a prefix of data.shape.\nThe values must be less than num_segments.\nCaution: The values are always validated to be in range on CPU, never validated\non GPU.", "num_segments": "A Tensor. Must be one of the following types: int32, int64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as data."}, "tf.math.unsorted_segment_prod": {"description": "Computes the product along segments of a tensor.", "Args": {"data": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.", "segment_ids": "A Tensor. Must be one of the following types: int32, int64.\nA tensor whose shape is a prefix of data.shape.\nThe values must be less than num_segments.\nCaution: The values are always validated to be in range on CPU, never validated\non GPU.", "num_segments": "A Tensor. Must be one of the following types: int32, int64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as data."}, "tf.math.unsorted_segment_sqrt_n": {"description": "Computes the sum along segments of a tensor divided by the sqrt(N).", "Args": {"data": "A Tensor with floating point or complex dtype.", "segment_ids": "An integer tensor whose shape is a prefix of data.shape.\nThe values must be in the range [0, num_segments).\nThe values are always validated to be in range on CPU,\nnever validated on GPU.", "num_segments": "An integer scalar Tensor.  The number of distinct segment\nIDs.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor.  Has same shape as data, except for the first segment_ids.rank\n dimensions, which are replaced with a single dimension which has size\nnum_segments."}, "tf.math.unsorted_segment_sum": {"description": "Computes the sum along segments of a tensor.", "Args": {"data": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.", "segment_ids": "A Tensor. Must be one of the following types: int32, int64.\nA tensor whose shape is a prefix of data.shape.\nThe values must be less than num_segments.\nCaution: The values are always validated to be in range on CPU, never validated\non GPU.", "num_segments": "A Tensor. Must be one of the following types: int32, int64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as data."}, "tf.math.xdivy": {"description": "Returns 0 if x == 0, and x / y otherwise, elementwise.", "Args": {"x": "A Tensor. Must be one of the following types: half, float32, float64, complex64, complex128.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.math.xlog1py": {"description": "Compute x * log1p(y).", "Args": {"x": "A tf.Tensor of type bfloat16, half, float32, float64,\ncomplex64, complex128", "y": "A tf.Tensor of type bfloat16, half, float32, float64,\ncomplex64, complex128", "name": "A name for the operation (optional)."}, "Returns": "x * log1p(y)."}, "tf.math.xlogy": {"description": "Returns 0 if x == 0, and x * log(y) otherwise, elementwise.", "Args": {"x": "A Tensor. Must be one of the following types: half, float32, float64, complex64, complex128.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.math.zero_fraction": {"description": "Returns the fraction of zeros in value.", "Args": {"value": "A tensor of numeric type.", "name": "A name for the operation (optional)."}, "Returns": "The fraction of zeros in value, with type float32."}, "tf.math.zeta": {"description": "Compute the Hurwitz zeta function \\\\(\\zeta(x, q)\\\\).", "Args": {"x": "A Tensor. Must be one of the following types: float32, float64.", "q": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}}, "tf.math.special": {"tf.math.special.bessel_j0": {"description": "Computes the Bessel j0 function of x element-wise.", "Args": {"x": "A Tensor or SparseTensor. Must be one of the following types: half,\nfloat32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor or SparseTensor, respectively. Has the same type as x."}, "tf.math.special.bessel_j1": {"description": "Computes the Bessel j1 function of x element-wise.", "Args": {"x": "A Tensor or SparseTensor. Must be one of the following types: half,\nfloat32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor or SparseTensor, respectively. Has the same type as x."}, "tf.math.special.bessel_k0": {"description": "Computes the Bessel k0 function of x element-wise.", "Args": {"x": "A Tensor or SparseTensor. Must be one of the following types: half,\nfloat32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor or SparseTensor, respectively. Has the same type as x."}, "tf.math.special.bessel_k0e": {"description": "Computes the Bessel k0e function of x element-wise.", "Args": {"x": "A Tensor or SparseTensor. Must be one of the following types: half,\nfloat32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor or SparseTensor, respectively. Has the same type as x."}, "tf.math.special.bessel_k1": {"description": "Computes the Bessel k1 function of x element-wise.", "Args": {"x": "A Tensor or SparseTensor. Must be one of the following types: half,\nfloat32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor or SparseTensor, respectively. Has the same type as x."}, "tf.math.special.bessel_k1e": {"description": "Computes the Bessel k1e function of x element-wise.", "Args": {"x": "A Tensor or SparseTensor. Must be one of the following types: half,\nfloat32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor or SparseTensor, respectively. Has the same type as x."}, "tf.math.special.bessel_y0": {"description": "Computes the Bessel y0 function of x element-wise.", "Args": {"x": "A Tensor or SparseTensor. Must be one of the following types: half,\nfloat32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor or SparseTensor, respectively. Has the same type as x."}, "tf.math.special.bessel_y1": {"description": "Computes the Bessel y1 function of x element-wise.", "Args": {"x": "A Tensor or SparseTensor. Must be one of the following types: half,\nfloat32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor or SparseTensor, respectively. Has the same type as x."}, "tf.math.special.dawsn": {"description": "Computes Dawson&#39;s integral of x element-wise.", "Args": {"x": "A Tensor or SparseTensor. Must be one of the following types:\nfloat32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor or SparseTensor, respectively. Has the same type as x."}, "tf.math.special.expint": {"description": "Computes the Exponential integral of x element-wise.", "Args": {"x": "A Tensor or SparseTensor. Must be one of the following types:\nfloat32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor or SparseTensor, respectively. Has the same type as x."}, "tf.math.special.fresnel_cos": {"description": "Computes Fresnel&#39;s cosine integral of x element-wise.", "Args": {"x": "A Tensor or SparseTensor. Must be one of the following types:\nfloat32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor or SparseTensor, respectively. Has the same type as x."}, "tf.math.special.fresnel_sin": {"description": "Computes Fresnel&#39;s sine integral of x element-wise.", "Args": {"x": "A Tensor or SparseTensor. Must be one of the following types:\nfloat32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor or SparseTensor, respectively. Has the same type as x."}, "tf.math.special.spence": {"description": "Computes Spence&#39;s integral of x element-wise.", "Args": {"x": "A Tensor or SparseTensor. Must be one of the following types:\nfloat32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor or SparseTensor, respectively. Has the same type as x."}}, "tf.mlir": {}, "tf.nest": {"tf.nest.assert_same_structure": {"description": "Asserts that two structures are nested in the same way.", "Args": {"nest1": "an atom or a nested structure.", "nest2": "an atom or a nested structure.", "check_types": "if True (default) types of structures are checked as well,\nincluding the keys of dictionaries. If set to False, for example a list\nand a tuple of objects will look the same if they have the same size. Note\nthat namedtuples with identical name and fields are always considered to\nhave the same shallow structure. Two types will also be considered the\nsame if they are both list subtypes (which allows \"list\" and\n\"_ListWrapper\" from trackable dependency tracking to compare equal).\ncheck_types=True only checks type of sub-structures. The types of atoms\nare not checked.", "expand_composites": "If true, then composite tensors such as\ntf.sparse.SparseTensor and tf.RaggedTensor are expanded into their\ncomponent tensors."}, "Raises": {"ValueError": "If the two structures do not have the same number of atoms or\nif the two structures are not nested in the same way.", "TypeError": "If the two structures differ in the type of sequence in any of\ntheir substructures. Only possible if check_types is True."}}, "tf.nest.flatten": {"description": "Returns a flat list from a given structure.", "Args": {"structure": "an atom or a nested structure. Note, numpy arrays are considered\natoms and are not flattened.", "expand_composites": "If true, then composite tensors such as\ntf.sparse.SparseTensor and tf.RaggedTensor are expanded into their\ncomponent tensors."}, "Returns": "A Python list, the flattened version of the input.", "Raises": {"TypeError": "The nest is or contains a dict with non-sortable keys."}}, "tf.nest.is_nested": {"description": "Returns true if its input is a nested structure.", "Args": {"seq": "the value to test."}, "Returns": "True if the input is a nested structure."}, "tf.nest.map_structure": {"description": "Creates a new structure by applying func to each atom in structure.", "Args": {"func": "A callable that accepts as many arguments as there are structures.", "*structure": "atom or nested structure.", "**kwargs": "Valid keyword args are:\n\ncheck_types: If set to True (default) the types of iterables within\nthe structures have to be same (e.g. map_structure(func, [1], (1,))\nraises a TypeError exception). To allow this set this argument to\nFalse. Note that namedtuples with identical name and fields are always\nconsidered to have the same shallow structure.\nexpand_composites: If set to True, then composite tensors such as\ntf.sparse.SparseTensor and tf.RaggedTensor are expanded into their\ncomponent tensors.  If False (the default), then composite tensors are\nnot expanded."}, "Returns": "A new structure with the same arity as structure[0], whose atoms\ncorrespond to func(x[0], x[1], ...) where x[i] is the atom in the\ncorresponding location in structure[i]. If there are different structure\ntypes and check_types is False the structure types of the first\nstructure will be used.", "Raises": {"TypeError": "If func is not callable or if the structures do not match\neach other by depth tree.", "ValueError": "If wrong keyword arguments are provided."}}, "tf.nest.pack_sequence_as": {"description": "Returns a given flattened sequence packed into a given structure.", "Args": {"structure": "Nested structure, whose structure is given by nested lists,\ntuples, and dicts. Note: numpy arrays and strings are considered\nscalars.", "flat_sequence": "flat sequence to pack.", "expand_composites": "If true, then composite tensors such as\ntf.sparse.SparseTensor and tf.RaggedTensor are expanded into their\ncomponent tensors."}, "Returns": "packed\n\n\nflat_sequence converted to have the same recursive structure as\nstructure.", "Raises": {"ValueError": "If flat_sequence and structure have different\natom counts.", "TypeError": "structure is or contains a dict with non-sortable keys."}}}, "tf.nn": {"tf.nn.RNNCellDeviceWrapper": {"description": "Operator that ensures an RNNCell runs on a particular device.", "Args": {"cell": "An instance of RNNCell.", "device": "A device string or function, for passing to tf.device.", "**kwargs": "dict of keyword arguments for base layer."}, "Attributes": {"activity_regularizer": "Optional regularizer function for the output of this layer.", "compute_dtype": "The dtype of the layer's computations.\nThis is equivalent to Layer.dtype_policy.compute_dtype. Unless\nmixed precision is used, this is the same as Layer.dtype, the dtype of\nthe weights.\nLayers automatically cast their inputs to the compute dtype, which causes\ncomputations and the output to be in the compute dtype as well. This is done\nby the base Layer class in Layer.call, so you do not have to insert\nthese casts if implementing your own layer.\nLayers often perform certain internal computations in higher precision when\ncompute_dtype is float16 or bfloat16 for numeric stability. The output\nwill still typically be float16 or bfloat16 in such cases.", "dtype": "The dtype of the layer weights.\nThis is equivalent to Layer.dtype_policy.variable_dtype. Unless\nmixed precision is used, this is the same as Layer.compute_dtype, the\ndtype of the layer's computations.", "dtype_policy": "The dtype policy associated with this layer.\nThis is an instance of a tf.keras.mixed_precision.Policy.", "dynamic": "Whether the layer is dynamic (eager-only); set in the constructor.", "input": "Retrieves the input tensor(s) of a layer.\nOnly applicable if the layer has exactly one input,\ni.e. if it is connected to one incoming layer.", "input_spec": "InputSpec instance(s) describing the input format for this layer.\nWhen you create a layer subclass, you can set self.input_spec to enable\nthe layer to run input compatibility checks when it is called.\nConsider a Conv2D layer: it can only be called on a single input tensor\nof rank 4. As such, you can set, in __init__():\nself.input_spec = tf.keras.layers.InputSpec(ndim=4)\nNow, if you try to call the layer on an input that isn't rank 4\n(for instance, an input of shape (2,), it will raise a nicely-formatted\nerror:\nValueError: Input 0 of layer conv2d is incompatible with the layer:expected ndim=4, found ndim=1. Full shape received: [2]\nInput checks that can be specified via input_spec include:\n\nStructure (e.g. a single input, a list of 2 inputs, etc)\nShape\nRank (ndim)\nDtype\n\nFor more information, see tf.keras.layers.InputSpec.", "losses": "List of losses added using the add_loss() API.\nVariable regularization tensors are created when this property is accessed,\nso it is eager safe: accessing losses under a tf.GradientTape will\npropagate gradients back to the corresponding variables.\nclass MyLayer(tf.keras.layers.Layer):\u00a0 def call(self, inputs):\u00a0 \u00a0 self.add_loss(tf.abs(tf.reduce_mean(inputs)))\u00a0 \u00a0 return inputsl = MyLayer()l(np.ones((10, 1)))l.losses[1.0]\ninputs = tf.keras.Input(shape=(10,))x = tf.keras.layers.Dense(10)(inputs)outputs = tf.keras.layers.Dense(1)(x)model = tf.keras.Model(inputs, outputs)# Activity regularization.len(model.losses)0model.add_loss(tf.abs(tf.reduce_mean(x)))len(model.losses)1\ninputs = tf.keras.Input(shape=(10,))d = tf.keras.layers.Dense(10, kernel_initializer='ones')x = d(inputs)outputs = tf.keras.layers.Dense(1)(x)model = tf.keras.Model(inputs, outputs)# Weight regularization.model.add_loss(lambda: tf.reduce_mean(d.kernel))model.losses[<tf.Tensor: shape=(), dtype=float32, numpy=1.0>]", "metrics": "List of metrics added using the add_metric() API.\ninput = tf.keras.layers.Input(shape=(3,))d = tf.keras.layers.Dense(2)output = d(input)d.add_metric(tf.reduce_max(output), name='max')d.add_metric(tf.reduce_min(output), name='min')[m.name for m in d.metrics]['max', 'min']", "non_trainable_weights": "List of all non-trainable weights tracked by this layer.\nNon-trainable weights are not updated during training. They are expected\nto be updated manually in call().", "output": "Retrieves the output tensor(s) of a layer.\nOnly applicable if the layer has exactly one output,\ni.e. if it is connected to one incoming layer.", "output_size": "", "state_size": "", "supports_masking": "Whether this layer supports computing a mask using compute_mask.", "trainable": "", "trainable_weights": "List of all trainable weights tracked by this layer.\nTrainable weights are updated via gradient descent during training.", "variable_dtype": "Alias of Layer.dtype, the dtype of the weights.", "weights": "Returns the list of all layer variables/weights."}}, "tf.nn.RNNCellDropoutWrapper": {"description": "Operator adding dropout to inputs and outputs of the given cell.", "Args": {"cell": "an RNNCell, a projection to output_size is added to it.", "input_keep_prob": "unit Tensor or float between 0 and 1, input keep\nprobability; if it is constant and 1, no input dropout will be added.", "output_keep_prob": "unit Tensor or float between 0 and 1, output keep\nprobability; if it is constant and 1, no output dropout will be added.", "state_keep_prob": "unit Tensor or float between 0 and 1, output keep\nprobability; if it is constant and 1, no output dropout will be added.\nState dropout is performed on the outgoing states of the cell. Note\nthe state components to which dropout is applied when state_keep_prob\nis in (0, 1) are also determined by the argument\ndropout_state_filter_visitor (e.g. by default dropout is never applied\nto the c component of an LSTMStateTuple).", "variational_recurrent": "Python bool.  If True, then the same dropout\npattern is applied across all time steps per run call. If this parameter\nis set, input_size must be provided.", "input_size": "(optional) (possibly nested tuple of) TensorShape objects\ncontaining the depth(s) of the input tensors expected to be passed in to\nthe DropoutWrapper.  Required and used iff variational_recurrent\n= True and input_keep_prob < 1.", "dtype": "(optional) The dtype of the input, state, and output tensors.\nRequired and used iff variational_recurrent = True.", "seed": "(optional) integer, the randomness seed.", "dropout_state_filter_visitor": "(optional), default: (see below).  Function\nthat takes any hierarchical level of the state and returns a scalar or\ndepth=1 structure of Python booleans describing which terms in the state\nshould be dropped out.  In addition, if the function returns True,\ndropout is applied across this sublevel.  If the function returns\nFalse, dropout is not applied across this entire sublevel.\nDefault behavior: perform dropout on all terms except the memory (c)\n  state of LSTMCellState objects, and don't try to apply dropout to\nTensorArray objects: def dropout_state_filter_visitor(s):\n  if isinstance(s, LSTMCellState): # Never perform dropout on the c\n    state. return LSTMCellState(c=False, h=True)\n  elif isinstance(s, TensorArray): return False return True", "**kwargs": "dict of keyword arguments for base layer."}, "Raises": {"TypeError": "if cell is not an RNNCell, or keep_state_fn is provided\nbut not callable.", "ValueError": "if any of the keep_probs are not between 0 and 1."}, "Attributes": {"activity_regularizer": "Optional regularizer function for the output of this layer.", "compute_dtype": "The dtype of the layer's computations.\nThis is equivalent to Layer.dtype_policy.compute_dtype. Unless\nmixed precision is used, this is the same as Layer.dtype, the dtype of\nthe weights.\nLayers automatically cast their inputs to the compute dtype, which causes\ncomputations and the output to be in the compute dtype as well. This is done\nby the base Layer class in Layer.call, so you do not have to insert\nthese casts if implementing your own layer.\nLayers often perform certain internal computations in higher precision when\ncompute_dtype is float16 or bfloat16 for numeric stability. The output\nwill still typically be float16 or bfloat16 in such cases.", "dtype": "The dtype of the layer weights.\nThis is equivalent to Layer.dtype_policy.variable_dtype. Unless\nmixed precision is used, this is the same as Layer.compute_dtype, the\ndtype of the layer's computations.", "dtype_policy": "The dtype policy associated with this layer.\nThis is an instance of a tf.keras.mixed_precision.Policy.", "dynamic": "Whether the layer is dynamic (eager-only); set in the constructor.", "input": "Retrieves the input tensor(s) of a layer.\nOnly applicable if the layer has exactly one input,\ni.e. if it is connected to one incoming layer.", "input_spec": "InputSpec instance(s) describing the input format for this layer.\nWhen you create a layer subclass, you can set self.input_spec to enable\nthe layer to run input compatibility checks when it is called.\nConsider a Conv2D layer: it can only be called on a single input tensor\nof rank 4. As such, you can set, in __init__():\nself.input_spec = tf.keras.layers.InputSpec(ndim=4)\nNow, if you try to call the layer on an input that isn't rank 4\n(for instance, an input of shape (2,), it will raise a nicely-formatted\nerror:\nValueError: Input 0 of layer conv2d is incompatible with the layer:expected ndim=4, found ndim=1. Full shape received: [2]\nInput checks that can be specified via input_spec include:\n\nStructure (e.g. a single input, a list of 2 inputs, etc)\nShape\nRank (ndim)\nDtype\n\nFor more information, see tf.keras.layers.InputSpec.", "losses": "List of losses added using the add_loss() API.\nVariable regularization tensors are created when this property is accessed,\nso it is eager safe: accessing losses under a tf.GradientTape will\npropagate gradients back to the corresponding variables.\nclass MyLayer(tf.keras.layers.Layer):\u00a0 def call(self, inputs):\u00a0 \u00a0 self.add_loss(tf.abs(tf.reduce_mean(inputs)))\u00a0 \u00a0 return inputsl = MyLayer()l(np.ones((10, 1)))l.losses[1.0]\ninputs = tf.keras.Input(shape=(10,))x = tf.keras.layers.Dense(10)(inputs)outputs = tf.keras.layers.Dense(1)(x)model = tf.keras.Model(inputs, outputs)# Activity regularization.len(model.losses)0model.add_loss(tf.abs(tf.reduce_mean(x)))len(model.losses)1\ninputs = tf.keras.Input(shape=(10,))d = tf.keras.layers.Dense(10, kernel_initializer='ones')x = d(inputs)outputs = tf.keras.layers.Dense(1)(x)model = tf.keras.Model(inputs, outputs)# Weight regularization.model.add_loss(lambda: tf.reduce_mean(d.kernel))model.losses[<tf.Tensor: shape=(), dtype=float32, numpy=1.0>]", "metrics": "List of metrics added using the add_metric() API.\ninput = tf.keras.layers.Input(shape=(3,))d = tf.keras.layers.Dense(2)output = d(input)d.add_metric(tf.reduce_max(output), name='max')d.add_metric(tf.reduce_min(output), name='min')[m.name for m in d.metrics]['max', 'min']", "non_trainable_weights": "List of all non-trainable weights tracked by this layer.\nNon-trainable weights are not updated during training. They are expected\nto be updated manually in call().", "output": "Retrieves the output tensor(s) of a layer.\nOnly applicable if the layer has exactly one output,\ni.e. if it is connected to one incoming layer.", "output_size": "", "state_size": "", "supports_masking": "Whether this layer supports computing a mask using compute_mask.", "trainable": "", "trainable_weights": "List of all trainable weights tracked by this layer.\nTrainable weights are updated via gradient descent during training.", "variable_dtype": "Alias of Layer.dtype, the dtype of the weights.", "weights": "Returns the list of all layer variables/weights.", "wrapped_cell": ""}}, "tf.nn.RNNCellResidualWrapper": {"description": "RNNCell wrapper that ensures cell inputs are added to the outputs.", "Args": {"cell": "An instance of RNNCell.", "residual_fn": "(Optional) The function to map raw cell inputs and raw cell\noutputs to the actual cell outputs of the residual network.\nDefaults to calling nest.map_structure on (lambda i, o: i + o), inputs\n  and outputs.", "**kwargs": "dict of keyword arguments for base layer."}, "Attributes": {"activity_regularizer": "Optional regularizer function for the output of this layer.", "compute_dtype": "The dtype of the layer's computations.\nThis is equivalent to Layer.dtype_policy.compute_dtype. Unless\nmixed precision is used, this is the same as Layer.dtype, the dtype of\nthe weights.\nLayers automatically cast their inputs to the compute dtype, which causes\ncomputations and the output to be in the compute dtype as well. This is done\nby the base Layer class in Layer.call, so you do not have to insert\nthese casts if implementing your own layer.\nLayers often perform certain internal computations in higher precision when\ncompute_dtype is float16 or bfloat16 for numeric stability. The output\nwill still typically be float16 or bfloat16 in such cases.", "dtype": "The dtype of the layer weights.\nThis is equivalent to Layer.dtype_policy.variable_dtype. Unless\nmixed precision is used, this is the same as Layer.compute_dtype, the\ndtype of the layer's computations.", "dtype_policy": "The dtype policy associated with this layer.\nThis is an instance of a tf.keras.mixed_precision.Policy.", "dynamic": "Whether the layer is dynamic (eager-only); set in the constructor.", "input": "Retrieves the input tensor(s) of a layer.\nOnly applicable if the layer has exactly one input,\ni.e. if it is connected to one incoming layer.", "input_spec": "InputSpec instance(s) describing the input format for this layer.\nWhen you create a layer subclass, you can set self.input_spec to enable\nthe layer to run input compatibility checks when it is called.\nConsider a Conv2D layer: it can only be called on a single input tensor\nof rank 4. As such, you can set, in __init__():\nself.input_spec = tf.keras.layers.InputSpec(ndim=4)\nNow, if you try to call the layer on an input that isn't rank 4\n(for instance, an input of shape (2,), it will raise a nicely-formatted\nerror:\nValueError: Input 0 of layer conv2d is incompatible with the layer:expected ndim=4, found ndim=1. Full shape received: [2]\nInput checks that can be specified via input_spec include:\n\nStructure (e.g. a single input, a list of 2 inputs, etc)\nShape\nRank (ndim)\nDtype\n\nFor more information, see tf.keras.layers.InputSpec.", "losses": "List of losses added using the add_loss() API.\nVariable regularization tensors are created when this property is accessed,\nso it is eager safe: accessing losses under a tf.GradientTape will\npropagate gradients back to the corresponding variables.\nclass MyLayer(tf.keras.layers.Layer):\u00a0 def call(self, inputs):\u00a0 \u00a0 self.add_loss(tf.abs(tf.reduce_mean(inputs)))\u00a0 \u00a0 return inputsl = MyLayer()l(np.ones((10, 1)))l.losses[1.0]\ninputs = tf.keras.Input(shape=(10,))x = tf.keras.layers.Dense(10)(inputs)outputs = tf.keras.layers.Dense(1)(x)model = tf.keras.Model(inputs, outputs)# Activity regularization.len(model.losses)0model.add_loss(tf.abs(tf.reduce_mean(x)))len(model.losses)1\ninputs = tf.keras.Input(shape=(10,))d = tf.keras.layers.Dense(10, kernel_initializer='ones')x = d(inputs)outputs = tf.keras.layers.Dense(1)(x)model = tf.keras.Model(inputs, outputs)# Weight regularization.model.add_loss(lambda: tf.reduce_mean(d.kernel))model.losses[<tf.Tensor: shape=(), dtype=float32, numpy=1.0>]", "metrics": "List of metrics added using the add_metric() API.\ninput = tf.keras.layers.Input(shape=(3,))d = tf.keras.layers.Dense(2)output = d(input)d.add_metric(tf.reduce_max(output), name='max')d.add_metric(tf.reduce_min(output), name='min')[m.name for m in d.metrics]['max', 'min']", "non_trainable_weights": "List of all non-trainable weights tracked by this layer.\nNon-trainable weights are not updated during training. They are expected\nto be updated manually in call().", "output": "Retrieves the output tensor(s) of a layer.\nOnly applicable if the layer has exactly one output,\ni.e. if it is connected to one incoming layer.", "output_size": "", "state_size": "", "supports_masking": "Whether this layer supports computing a mask using compute_mask.", "trainable": "", "trainable_weights": "List of all trainable weights tracked by this layer.\nTrainable weights are updated via gradient descent during training.", "variable_dtype": "Alias of Layer.dtype, the dtype of the weights.", "weights": "Returns the list of all layer variables/weights."}}, "tf.nn.atrous_conv2d": {"description": "Atrous convolution (a.k.a. convolution with holes or dilated convolution).", "Args": {"value": "A 4-D Tensor of type float. It needs to be in the default \"NHWC\"\nformat. Its shape is [batch, in_height, in_width, in_channels].", "filters": "A 4-D Tensor with the same type as value and shape\n[filter_height, filter_width, in_channels, out_channels]. filters'\nin_channels dimension must match that of value. Atrous convolution is\nequivalent to standard convolution with upsampled filters with effective\nheight filter_height + (filter_height - 1) * (rate - 1) and effective\nwidth filter_width + (filter_width - 1) * (rate - 1), produced by\ninserting rate - 1 zeros along consecutive elements across the\nfilters' spatial dimensions.", "rate": "A positive int32. The stride with which we sample input values across\nthe height and width dimensions. Equivalently, the rate by which we\nupsample the filter values by inserting zeros across the height and\nwidth dimensions. In the literature, the same parameter is sometimes\ncalled input stride or dilation.", "padding": "A string, either 'VALID' or 'SAME'. The padding algorithm. See\nhere\nfor more information.", "name": "Optional name for the returned tensor."}, "Returns": "A Tensor with the same type as value.\nOutput shape with 'VALID' padding is:\n[batch, height - 2 * (filter_width - 1),\u00a0width - 2 * (filter_height - 1), out_channels].\nOutput shape with 'SAME' padding is:\n[batch, height, width, out_channels].", "Raises": {"ValueError": "If input/output depth does not match filters' shape, or if\npadding is other than 'VALID' or 'SAME'."}}, "tf.nn.atrous_conv2d_transpose": {"description": "The transpose of atrous_conv2d.", "Args": {"value": "A 4-D Tensor of type float. It needs to be in the default NHWC\nformat. Its shape is [batch, in_height, in_width, in_channels].", "filters": "A 4-D Tensor with the same type as value and shape\n[filter_height, filter_width, out_channels, in_channels]. filters'\nin_channels dimension must match that of value. Atrous convolution is\nequivalent to standard convolution with upsampled filters with effective\nheight filter_height + (filter_height - 1) * (rate - 1) and effective\nwidth filter_width + (filter_width - 1) * (rate - 1), produced by\ninserting rate - 1 zeros along consecutive elements across the\nfilters' spatial dimensions.", "output_shape": "A 1-D Tensor of shape representing the output shape of the\ndeconvolution op, of form [batch, out_height, out_width, out_channels].", "rate": "A positive int32. The stride with which we sample input values across\nthe height and width dimensions. Equivalently, the rate by which we\nupsample the filter values by inserting zeros across the height and\nwidth dimensions. In the literature, the same parameter is sometimes\ncalled input stride or dilation.", "padding": "A string, either 'VALID' or 'SAME'. The padding algorithm. See\nhere\nfor more information.", "name": "Optional name for the returned tensor."}, "Returns": "A Tensor with the same type as value.", "Raises": {"ValueError": "If input/output depth does not match filters' shape, or if\npadding is other than 'VALID' or 'SAME', or if the rate is less\nthan one, or if the output_shape is not a tensor with 4 elements."}}, "tf.nn.avg_pool": {"description": "Performs the avg pooling on the input.", "Args": {"input": "Tensor of rank N+2, of shape [batch_size] + input_spatial_shape +\n[num_channels] if data_format does not start with \"NC\" (default), or\n[batch_size, num_channels] + input_spatial_shape if data_format starts\nwith \"NC\". Pooling happens over the spatial dimensions only.", "ksize": "An int or list of ints that has length 1, N or N+2. The size\nof the window for each dimension of the input tensor.", "strides": "An int or list of ints that has length 1, N or N+2. The\nstride of the sliding window for each dimension of the input tensor.", "padding": "A string, either 'VALID' or 'SAME'. The padding algorithm. See\nhere\nfor more information.", "data_format": "A string. Specifies the channel dimension. For N=1 it can be\neither \"NWC\" (default) or \"NCW\", for N=2 it can be either \"NHWC\" (default)\nor \"NCHW\" and for N=3 either \"NDHWC\" (default) or \"NCDHW\".", "name": "Optional name for the operation."}, "Returns": "A Tensor of format specified by data_format.\nThe average pooled output tensor."}, "tf.nn.avg_pool1d": {"description": "Performs the average pooling on the input.", "Args": {"input": "A 3-D Tensor of the format specified by data_format.", "ksize": "An int or list of ints that has length 1 or 3. The size of the\nwindow for each dimension of the input tensor.", "strides": "An int or list of ints that has length 1 or 3. The stride of\nthe sliding window for each dimension of the input tensor.", "padding": "A string, either 'VALID' or 'SAME'. The padding algorithm. See\nhere\nfor more information.", "data_format": "An optional string from: \"NWC\", \"NCW\". Defaults to \"NWC\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of format specified by data_format.\nThe max pooled output tensor."}, "tf.nn.avg_pool2d": {"description": "Performs the average pooling on the input.", "Args": {"input": "A 4-D Tensor of shape [batch, height, width, channels] and type\nfloat32, float64, qint8, quint8, or qint32.", "ksize": "An int or list of ints that has length 1, 2 or 4. The size of\nthe window for each dimension of the input tensor.", "strides": "An int or list of ints that has length 1, 2 or 4. The\nstride of the sliding window for each dimension of the input tensor.", "padding": "A string, either 'VALID' or 'SAME'. The padding algorithm. See\nhere\nfor more information.", "data_format": "A string. 'NHWC' and 'NCHW' are supported.", "name": "Optional name for the operation."}, "Returns": "A Tensor with the same type as value.  The average pooled output tensor."}, "tf.nn.avg_pool3d": {"description": "Performs the average pooling on the input.", "Args": {"input": "A 5-D Tensor of shape [batch, depth, height, width, channels]\nand type float32, float64, qint8, quint8, or qint32.", "ksize": "An int or list of ints that has length 1, 3 or 5. The size of\nthe window for each dimension of the input tensor.", "strides": "An int or list of ints that has length 1, 3 or 5. The\nstride of the sliding window for each dimension of the input tensor.", "padding": "A string, either 'VALID' or 'SAME'. The padding algorithm. See\nhere\nfor more information.", "data_format": "A string. 'NDHWC' and 'NCDHW' are supported.", "name": "Optional name for the operation."}, "Returns": "A Tensor with the same type as value.  The average pooled output tensor."}, "tf.nn.batch_norm_with_global_normalization": {"description": "Batch normalization.", "Args": {"input": "A 4D input Tensor.", "mean": "A 1D mean Tensor with size matching the last dimension of t.\nThis is the first output from tf.nn.moments,\nor a saved moving average thereof.", "variance": "A 1D variance Tensor with size matching the last dimension of t.\nThis is the second output from tf.nn.moments,\nor a saved moving average thereof.", "beta": "A 1D beta Tensor with size matching the last dimension of t.\nAn offset to be added to the normalized tensor.", "gamma": "A 1D gamma Tensor with size matching the last dimension of t.\nIf \"scale_after_normalization\" is true, this tensor will be multiplied\nwith the normalized tensor.", "variance_epsilon": "A small float number to avoid dividing by 0.", "scale_after_normalization": "A bool indicating whether the resulted tensor\nneeds to be multiplied with gamma.", "name": "A name for this operation (optional)."}, "Returns": "A batch-normalized t."}, "tf.nn.batch_normalization": {"description": "Batch normalization.", "Args": {"x": "Input Tensor of arbitrary dimensionality.", "mean": "A mean Tensor.", "variance": "A variance Tensor.", "offset": "An offset Tensor, often denoted \u03b2 in equations, or\nNone. If present, will be added to the normalized tensor.", "scale": "A scale Tensor, often denoted \u03b3 in equations, or\nNone. If present, the scale is applied to the normalized tensor.", "variance_epsilon": "A small float number to avoid dividing by 0.", "name": "A name for this operation (optional)."}, "Returns": "the normalized, scaled, offset tensor."}, "tf.nn.bias_add": {"description": "Adds bias to value.", "Args": {"value": "A Tensor with type float, double, int64, int32, uint8,\nint16, int8, complex64, or complex128.", "bias": "A 1-D Tensor with size matching the channel dimension of value.\nMust be the same type as value unless value is a quantized type,\nin which case a different quantized type may be used.", "data_format": "A string. 'N...C' and 'NC...' are supported. If None (the\ndefault) is specified then 'N..C' is assumed.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor with the same type as value.", "Raises": {}}, "tf.nn.collapse_repeated": {"description": "Merge repeated labels into single labels.", "Args": {"labels": "Tensor of shape [batch, max value in seq_length]", "seq_length": "Tensor of shape [batch], sequence length of each batch element.", "name": "A name for this Op. Defaults to \"collapse_repeated_labels\"."}, "Returns": "A tuple (collapsed_labels, new_seq_length) where"}, "tf.nn.compute_accidental_hits": {"description": "Compute the position ids in sampled_candidates matching true_classes.", "Args": {"true_classes": "A Tensor of type int64 and shape [batch_size,\nnum_true]. The target classes.", "sampled_candidates": "A tensor of type int64 and shape [num_sampled].\nThe sampled_candidates output of CandidateSampler.", "num_true": "An int.  The number of target classes per training example.", "seed": "An int. An operation-specific seed. Default is 0.", "name": "A name for the operation (optional)."}, "Returns": "indices\n\n\nA Tensor of type int32 and shape [num_accidental_hits].\nValues indicate rows in true_classes."}, "tf.nn.compute_average_loss": {"description": "Scales per-example losses with sample_weights and computes their average.", "Args": {"per_example_loss": "Per-example loss.", "sample_weight": "Optional weighting for each example.", "global_batch_size": "Optional global batch size value. Defaults to (size of\nfirst dimension of losses) * (number of replicas)."}, "Returns": "Scalar loss value."}, "tf.nn.conv1d": {"description": "Computes a 1-D convolution given 3-D input and filter tensors.", "Args": {"input": "A Tensor of rank at least 3. Must be of type float16, float32, or\nfloat64.", "filters": "A Tensor of rank at least 3.  Must have the same type as input.", "stride": "An int or list of ints that has length 1 or 3.  The number of\nentries by which the filter is moved right at each step.", "padding": "'SAME' or 'VALID'. See\nhere\nfor more information.", "data_format": "An optional string from \"NWC\", \"NCW\".  Defaults to \"NWC\",\nthe data is stored in the order of\nbatch_shape + [in_width, in_channels].  The \"NCW\" format stores data\nas batch_shape + [in_channels, in_width].", "dilations": "An int or list of ints that has length 1 or 3 which\ndefaults to 1. The dilation factor for each dimension of input. If set to\nk > 1, there will be k-1 skipped cells between each filter element on that\ndimension. Dilations in the batch and depth dimensions must be 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor.  Has the same type as input.", "Raises": {"ValueError": "if data_format is invalid."}}, "tf.nn.conv1d_transpose": {"description": "The transpose of conv1d.", "Args": {"input": "A 3-D Tensor of type float and shape\n[batch, in_width, in_channels] for NWC data format or\n[batch, in_channels, in_width] for NCW data format.", "filters": "A 3-D Tensor with the same type as input and shape\n[filter_width, output_channels, in_channels].  filter's\nin_channels dimension must match that of input.", "output_shape": "A 1-D Tensor, containing three elements, representing the\noutput shape of the deconvolution op.", "strides": "An int or list of ints that has length 1 or 3.  The number of\nentries by which the filter is moved right at each step.", "padding": "A string, either 'VALID' or 'SAME'. The padding algorithm. See\nhere\nfor more information.", "data_format": "A string. 'NWC' and 'NCW' are supported.", "dilations": "An int or list of ints that has length 1 or 3 which\ndefaults to 1. The dilation factor for each dimension of input. If set to\nk > 1, there will be k-1 skipped cells between each filter element on that\ndimension. Dilations in the batch and depth dimensions must be 1.", "name": "Optional name for the returned tensor."}, "Returns": "A Tensor with the same type as input.", "Raises": {"ValueError": "If input/output depth does not match filter's shape, if\noutput_shape is not at 3-element vector, if padding is other than\n'VALID' or 'SAME', or if data_format is invalid."}}, "tf.nn.conv2d": {"description": "Computes a 2-D convolution given input and 4-D filters tensors.", "Args": {"input": "A Tensor. Must be one of the following types:\nhalf, bfloat16, float32, float64.\nA Tensor of rank at least 4. The dimension order is interpreted according\nto the value of data_format; with the all-but-inner-3 dimensions acting\nas batch dimensions. See below for details.", "filters": "A Tensor. Must have the same type as input.\nA 4-D tensor of shape\n[filter_height, filter_width, in_channels, out_channels]", "strides": "An int or list of ints that has length 1, 2 or 4.  The\nstride of the sliding window for each dimension of input. If a single\nvalue is given it is replicated in the H and W dimension. By default\nthe N and C dimensions are set to 1. The dimension order is determined\nby the value of data_format, see below for details.", "padding": "Either the string \"SAME\" or \"VALID\" indicating the type of\npadding algorithm to use, or a list indicating the explicit paddings at\nthe start and end of each dimension. See\nhere\nfor more information. When explicit padding is used and data_format is\n\"NHWC\", this should be in the form [[0, 0], [pad_top, pad_bottom],\n[pad_left, pad_right], [0, 0]]. When explicit padding used and\ndata_format is \"NCHW\", this should be in the form [[0, 0], [0, 0],\n[pad_top, pad_bottom], [pad_left, pad_right]].", "data_format": "An optional string from: \"NHWC\", \"NCHW\".\nDefaults to \"NHWC\".\nSpecify the data format of the input and output data. With the\ndefault format \"NHWC\", the data is stored in the order of:\n    batch_shape + [height, width, channels].\nAlternatively, the format could be \"NCHW\", the data storage order of:\n    batch_shape + [channels, height, width].", "dilations": "An int or list of ints that has length 1, 2 or 4,\ndefaults to 1. The dilation factor for each dimension ofinput. If a\nsingle value is given it is replicated in the H and W dimension. By\ndefault the N and C dimensions are set to 1. If set to k > 1, there\nwill be k-1 skipped cells between each filter element on that dimension.\nThe dimension order is determined by the value of data_format, see above\nfor details. Dilations in the batch and depth dimensions if a 4-d tensor\nmust be 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input and the same outer batch shape."}, "tf.nn.conv2d_transpose": {"description": "The transpose of conv2d.", "Args": {"input": "A 4-D Tensor of type float and shape [batch, height, width,\nin_channels] for NHWC data format or [batch, in_channels, height,\nwidth] for NCHW data format.", "filters": "A 4-D Tensor with the same type as input and shape [height,\nwidth, output_channels, in_channels].  filter's in_channels dimension\nmust match that of input.", "output_shape": "A 1-D Tensor representing the output shape of the\ndeconvolution op.", "strides": "An int or list of ints that has length 1, 2 or 4.  The\nstride of the sliding window for each dimension of input. If a single\nvalue is given it is replicated in the H and W dimension. By default\nthe N and C dimensions are set to 0. The dimension order is determined\nby the value of data_format, see below for details.", "padding": "Either the string \"SAME\" or \"VALID\" indicating the type of\npadding algorithm to use, or a list indicating the explicit paddings at\nthe start and end of each dimension. See\nhere\nfor more information.  When explicit padding is used and data_format is\n\"NHWC\", this should be in the form [[0, 0], [pad_top, pad_bottom],\n[pad_left, pad_right], [0, 0]]. When explicit padding used and\ndata_format is \"NCHW\", this should be in the form [[0, 0], [0, 0],\n[pad_top, pad_bottom], [pad_left, pad_right]].", "data_format": "A string. 'NHWC' and 'NCHW' are supported.", "dilations": "An int or list of ints that has length 1, 2 or 4,\ndefaults to 1. The dilation factor for each dimension ofinput. If a\nsingle value is given it is replicated in the H and W dimension. By\ndefault the N and C dimensions are set to 1. If set to k > 1, there\nwill be k-1 skipped cells between each filter element on that dimension.\nThe dimension order is determined by the value of data_format, see above\nfor details. Dilations in the batch and depth dimensions if a 4-d tensor\nmust be 1.", "name": "Optional name for the returned tensor."}, "Returns": "A Tensor with the same type as input.", "Raises": {"ValueError": "If input/output depth does not match filter's shape, or if\npadding is other than 'VALID' or 'SAME'."}}, "tf.nn.conv3d": {"description": "Computes a 3-D convolution given 5-D input and filters tensors.", "Args": {"input": "A Tensor. Must be one of the following types: half, bfloat16, float32, float64.\nShape [batch, in_depth, in_height, in_width, in_channels].", "filters": "A Tensor. Must have the same type as input.\nShape [filter_depth, filter_height, filter_width, in_channels,\nout_channels]. in_channels must match between input and filters.", "strides": "A list of ints that has length >= 5.\n1-D tensor of length 5. The stride of the sliding window for each\ndimension of input. Must have strides[0] = strides[4] = 1.", "padding": "A string from: \"SAME\", \"VALID\".\nThe type of padding algorithm to use.", "data_format": "An optional string from: \"NDHWC\", \"NCDHW\". Defaults to \"NDHWC\".\nThe data format of the input and output data. With the\ndefault format \"NDHWC\", the data is stored in the order of:\n    [batch, in_depth, in_height, in_width, in_channels].\nAlternatively, the format could be \"NCDHW\", the data storage order is:\n    [batch, in_channels, in_depth, in_height, in_width].", "dilations": "An optional list of ints. Defaults to [1, 1, 1, 1, 1].\n1-D tensor of length 5.  The dilation factor for each dimension of\ninput. If set to k > 1, there will be k-1 skipped cells between each\nfilter element on that dimension. The dimension order is determined by the\nvalue of data_format, see above for details. Dilations in the batch and\ndepth dimensions must be 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.nn.conv3d_transpose": {"description": "The transpose of conv3d.", "Args": {"input": "A 5-D Tensor of type float and shape [batch, depth, height,\nwidth, in_channels] for NDHWC data format or [batch, in_channels,\ndepth, height, width] for NCDHW data format.", "filters": "A 5-D Tensor with the same type as input and shape [depth,\nheight, width, output_channels, in_channels].  filter's in_channels\ndimension must match that of input.", "output_shape": "A 1-D Tensor representing the output shape of the\ndeconvolution op.", "strides": "An int or list of ints that has length 1, 3 or 5.  The\nstride of the sliding window for each dimension of input. If a single\nvalue is given it is replicated in the D, H and W dimension. By\ndefault the N and C dimensions are set to 0. The dimension order is\ndetermined by the value of data_format, see below for details.", "padding": "A string, either 'VALID' or 'SAME'. The padding algorithm. See\nhere\nfor more information.", "data_format": "A string. 'NDHWC' and 'NCDHW' are supported.", "dilations": "An int or list of ints that has length 1, 3 or 5,\ndefaults to 1. The dilation factor for each dimension ofinput. If a\nsingle value is given it is replicated in the D, H and W dimension.\nBy default the N and C dimensions are set to 1. If set to k > 1, there\nwill be k-1 skipped cells between each filter element on that dimension.\nThe dimension order is determined by the value of data_format, see above\nfor details. Dilations in the batch and depth dimensions if a 5-d tensor\nmust be 1.", "name": "Optional name for the returned tensor."}, "Returns": "A Tensor with the same type as input."}, "tf.nn.conv_transpose": {"description": "The transpose of convolution.", "Args": {"input": "An N+2 dimensional Tensor of shape\n[batch_size] + input_spatial_shape + [in_channels] if data_format does\nnot start with \"NC\" (default), or\n[batch_size, in_channels] + input_spatial_shape if data_format starts\nwith \"NC\". It must be one of the following types:\nhalf, bfloat16, float32, float64.", "filters": "An N+2 dimensional Tensor with the same type as input and\nshape spatial_filter_shape + [in_channels, out_channels].", "output_shape": "A 1-D Tensor representing the output shape of the\ndeconvolution op.", "strides": "An int or list of ints that has length 1, N or N+2.  The\nstride of the sliding window for each dimension of input. If a single\nvalue is given it is replicated in the spatial dimensions. By default\nthe N and C dimensions are set to 0. The dimension order is determined\nby the value of data_format, see below for details.", "padding": "A string, either 'VALID' or 'SAME'. The padding algorithm. See\nhere\nfor more information.", "data_format": "A string or None.  Specifies whether the channel dimension of\nthe input and output is the last dimension (default, or if data_format\ndoes not start with \"NC\"), or the second dimension (if data_format\nstarts with \"NC\").  For N=1, the valid values are \"NWC\" (default) and\n\"NCW\".  For N=2, the valid values are \"NHWC\" (default) and \"NCHW\".\nFor N=3, the valid values are \"NDHWC\" (default) and \"NCDHW\".", "dilations": "An int or list of ints that has length 1, N or N+2,\ndefaults to 1. The dilation factor for each dimension ofinput. If a\nsingle value is given it is replicated in the spatial dimensions. By\ndefault the N and C dimensions are set to 1. If set to k > 1, there\nwill be k-1 skipped cells between each filter element on that dimension.\nThe dimension order is determined by the value of data_format, see above\nfor details.", "name": "A name for the operation (optional). If not specified \"conv_transpose\"\nis used."}, "Returns": "A Tensor with the same type as value."}, "tf.nn.convolution": {"description": "Computes sums of N-D convolutions (actually cross-correlation).", "Args": {"input": "An (N+2)-D Tensor of type T, of shape\n[batch_size] + input_spatial_shape + [in_channels] if data_format does\nnot start with \"NC\" (default), or\n[batch_size, in_channels] + input_spatial_shape if data_format starts\nwith \"NC\".", "filters": "An (N+2)-D Tensor with the same type as input and shape\nspatial_filter_shape + [in_channels, out_channels].", "padding": "A string, either \"VALID\" or \"SAME\". The padding algorithm.\n\"valid\" means no padding. \"same\" results in padding evenly to\nthe left/right or up/down of the input such that output has the same\nheight/width dimension as the input when the strides are 1. See\nhere\nfor more information.", "strides": "Optional.  Sequence of N ints >= 1.  Specifies the output stride.\nDefaults to [1]*N.  If any value of strides is > 1, then all values of\ndilation_rate must be 1.", "dilations": "Optional.  Sequence of N ints >= 1.  Specifies the filter\nupsampling/input downsampling rate.  In the literature, the same parameter\nis sometimes called input stride or dilation.  The effective filter\nsize used for the convolution will be spatial_filter_shape +\n(spatial_filter_shape - 1) * (rate - 1), obtained by inserting\n(dilation_rate[i]-1) zeros between consecutive elements of the original\nfilter in each spatial dimension i.  If any value of dilation_rate is > 1,\nthen all values of strides must be 1.", "name": "Optional name for the returned tensor.", "data_format": "A string or None.  Specifies whether the channel dimension of\nthe input and output is the last dimension (default, or if data_format\ndoes not start with \"NC\"), or the second dimension (if data_format\nstarts with \"NC\").  For N=1, the valid values are \"NWC\" (default) and\n\"NCW\".  For N=2, the valid values are \"NHWC\" (default) and \"NCHW\".\nFor N=3, the valid values are \"NDHWC\" (default) and \"NCDHW\"."}, "Returns": "A Tensor with the same type as input of shape\n`[batch_size] + output_spatial_shape + [out_channels]`\nif data_format is None or does not start with \"NC\", or\n`[batch_size, out_channels] + output_spatial_shape`\nif data_format starts with \"NC\",\nwhere output_spatial_shape depends on the value of padding.\nIf padding == \"SAME\":\n  output_spatial_shape[i] = ceil(input_spatial_shape[i] / strides[i])\nIf padding == \"VALID\":\n  output_spatial_shape[i] =\n    ceil((input_spatial_shape[i] -\n          (spatial_filter_shape[i]-1) * dilation_rate[i])\n         / strides[i]).", "Raises": {"ValueError": "If input/output depth does not match filters shape, if padding\nis other than \"VALID\" or \"SAME\", or if data_format is invalid."}}, "tf.nn.crelu": {"description": "Computes Concatenated ReLU.", "Args": {"features": "A Tensor with type float, double, int32, int64, uint8,\nint16, or int8.", "name": "A name for the operation (optional).", "axis": "The axis that the output values are concatenated along. Default is -1."}, "Returns": "A Tensor with the same type as features."}, "tf.nn.ctc_beam_search_decoder": {"description": "Performs beam search decoding on the logits given in input.", "Args": {"inputs": "3-D float Tensor, size [max_time, batch_size, num_classes].\nThe logits.", "sequence_length": "1-D int32 vector containing sequence lengths, having size\n[batch_size].", "beam_width": "An int scalar >= 0 (beam search beam width).", "top_paths": "An int scalar >= 0, <= beam_width (controls output size)."}, "Returns": "A tuple (decoded, log_probabilities) where"}, "tf.nn.ctc_greedy_decoder": {"description": "Performs greedy decoding on the logits given in input (best path).", "Args": {"inputs": "3-D float Tensor sized [max_time, batch_size, num_classes].\nThe logits.", "sequence_length": "1-D int32 vector containing sequence lengths, having size\n[batch_size].", "merge_repeated": "Boolean.  Default: True.", "blank_index": "(Optional). Default: num_classes - 1. Define the class index\nto use for the blank label. Negative values will start from num_classes,\nie, -1 will reproduce the ctc_greedy_decoder behavior of using\nnum_classes - 1 for the blank symbol, which corresponds to the default."}, "Returns": "A tuple (decoded, neg_sum_logits) where"}, "tf.nn.ctc_loss": {"description": "Computes CTC (Connectionist Temporal Classification) loss.", "Args": {"labels": "tensor of shape [batch_size, max_label_seq_length] or SparseTensor", "logits": "tensor of shape [frames, batch_size, num_labels], if\nlogits_time_major == False, shape is [batch_size, frames, num_labels].", "label_length": "tensor of shape [batch_size], None if labels is SparseTensor\nLength of reference label sequence in labels.", "logit_length": "tensor of shape [batch_size] Length of input sequence in\nlogits.", "logits_time_major": "(optional) If True (default), logits is shaped [time,\nbatch, logits]. If False, shape is [batch, time, logits]", "unique": "(optional) Unique label indices as computed by\nctc_unique_labels(labels).  If supplied, enable a faster, memory efficient\nimplementation on TPU.", "blank_index": "(optional) Set the class index to use for the blank label.\nNegative values will start from num_classes, ie, -1 will reproduce the\nctc_loss behavior of using num_classes - 1 for the blank symbol. There is\nsome memory/performance overhead to switching from the default of 0 as an\nadditional shifted copy of the logits may be created.", "name": "A name for this Op. Defaults to \"ctc_loss_dense\"."}, "Returns": "loss\n\n\ntensor of shape [batch_size], negative log probabilities."}, "tf.nn.ctc_unique_labels": {"description": "Get unique labels and indices for batched labels for tf.nn.ctc_loss.", "Args": {"labels": "tensor of shape [batch_size, max_label_length] padded with 0.", "name": "A name for this Op. Defaults to \"ctc_unique_labels\"."}, "Returns": "tuple of\n\nunique labels, tensor of shape [batch_size, max_label_length]\nindices into unique labels, shape [batch_size, max_label_length]"}, "tf.nn.depth_to_space": {"description": "DepthToSpace for tensors of type T.", "Args": {"input": "A Tensor.", "block_size": "An int that is >= 2.\nThe size of the spatial block, same as in Space2Depth.", "data_format": "An optional string from: \"NHWC\", \"NCHW\", \"NCHW_VECT_C\". Defaults to \"NHWC\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.nn.depthwise_conv2d": {"description": "Depthwise 2-D convolution.", "Args": {"input": "4-D with shape according to data_format.", "filter": "4-D with shape\n[filter_height, filter_width, in_channels, channel_multiplier].", "strides": "1-D of size 4.  The stride of the sliding window for each\ndimension of input.", "padding": "Controls how to pad the image before applying the convolution. Can\nbe the string \"SAME\" or \"VALID\" indicating the type of padding\nalgorithm to use, or a list indicating the explicit paddings at the start\nand end of each dimension. When explicit padding is used and data_format\nis \"NHWC\", this should be in the form [[0, 0], [pad_top, pad_bottom],\n[pad_left, pad_right], [0, 0]]. When explicit padding used and\ndata_format is \"NCHW\", this should be in the form [[0, 0], [0, 0],\n[pad_top, pad_bottom], [pad_left, pad_right]].", "data_format": "The data format for input. Either \"NHWC\" (default) or \"NCHW\".", "dilations": "1-D of size 2. The dilation rate in which we sample input values\nacross the height and width dimensions in atrous convolution. If it is\ngreater than 1, then all values of strides must be 1.", "name": "A name for this operation (optional)."}, "Returns": "A 4-D Tensor with shape according to data_format.  E.g., for\n\"NHWC\" format, shape is\n[batch, out_height, out_width, in_channels * channel_multiplier]."}, "tf.nn.depthwise_conv2d_backprop_filter": {"description": "Computes the gradients of depthwise convolution with respect to the filter.", "Args": {"input": "A Tensor. Must be one of the following types: half, bfloat16,\nfloat32, float64. 4-D with shape based on data_format.  For example,\nif data_format is 'NHWC' then input is a 4-D [batch, in_height,\nin_width, in_channels] tensor.", "filter_sizes": "A Tensor of type int32. An integer vector representing the\ntensor shape of filter, where filter is a 4-D [filter_height,\nfilter_width, in_channels, depthwise_multiplier] tensor.", "out_backprop": "A Tensor. Must have the same type as input. 4-D with shape\nbased on data_format. For example, if data_format is 'NHWC' then\nout_backprop shape is [batch, out_height, out_width, out_channels].\nGradients w.r.t. the output of the convolution.", "strides": "A list of ints. The stride of the sliding window for each\ndimension of the input of the convolution.", "padding": "Controls how to pad the image before applying the convolution. Can\nbe the string \"SAME\" or \"VALID\" indicating the type of padding\nalgorithm to use, or a list indicating the explicit paddings at the start\nand end of each dimension. See\nhere\nfor more information. When explicit padding is used and data_format is\n\"NHWC\", this should be in the form [[0, 0], [pad_top, pad_bottom],\n[pad_left, pad_right], [0, 0]]. When explicit padding used and\ndata_format is \"NCHW\", this should be in the form [[0, 0], [0, 0],\n[pad_top, pad_bottom], [pad_left, pad_right]].", "data_format": "An optional string from: \"NHWC\", \"NCHW\". Defaults to\n\"NHWC\". Specify the data format of the input and output data. With the\ndefault format \"NHWC\", the data is stored in the order of: [batch, height,\n  width, channels].\nAlternatively, the format could be \"NCHW\", the data storage order of:\n  [batch, channels, height, width].", "dilations": "An optional list of ints. Defaults to [1, 1, 1, 1]. 1-D\ntensor of length 4.  The dilation factor for each dimension of input. If\nset to k > 1, there will be k-1 skipped cells between each filter element\non that dimension. The dimension order is determined by the value of\ndata_format, see above for details. Dilations in the batch and depth\ndimensions must be 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.nn.depthwise_conv2d_backprop_input": {"description": "Computes the gradients of depthwise convolution with respect to the input.", "Args": {"input_sizes": "A Tensor of type int32. An integer vector representing the\nshape of input, based on data_format.  For example, if data_format\nis 'NHWC' then input is a 4-D [batch, height, width, channels] tensor.", "filter": "A Tensor. Must be one of the following types: half, bfloat16,\nfloat32, float64. 4-D with shape [filter_height, filter_width,\nin_channels, depthwise_multiplier].", "out_backprop": "A Tensor. Must have the same type as filter. 4-D with\nshape  based on data_format. For example, if data_format is 'NHWC'\nthen out_backprop shape is [batch, out_height, out_width, out_channels].\nGradients w.r.t. the output of the convolution.", "strides": "A list of ints. The stride of the sliding window for each\ndimension of the input of the convolution.", "padding": "Controls how to pad the image before applying the convolution. Can\nbe the string \"SAME\" or \"VALID\" indicating the type of padding\nalgorithm to use, or a list indicating the explicit paddings at the start\nand end of each dimension. See\nhere\nfor more information. When explicit padding is used and data_format is\n\"NHWC\", this should be in the form [[0, 0], [pad_top, pad_bottom],\n[pad_left, pad_right], [0, 0]]. When explicit padding used and\ndata_format is \"NCHW\", this should be in the form [[0, 0], [0, 0],\n[pad_top, pad_bottom], [pad_left, pad_right]].", "data_format": "An optional string from: \"NHWC\", \"NCHW\". Defaults to\n\"NHWC\". Specify the data format of the input and output data. With the\ndefault format \"NHWC\", the data is stored in the order of: [batch, height,\n  width, channels].\nAlternatively, the format could be \"NCHW\", the data storage order of:\n  [batch, channels, height, width].", "dilations": "An optional list of ints. Defaults to [1, 1, 1, 1]. 1-D\ntensor of length 4.  The dilation factor for each dimension of input. If\nset to k > 1, there will be k-1 skipped cells between each filter element\non that dimension. The dimension order is determined by the value of\ndata_format, see above for details. Dilations in the batch and depth\ndimensions must be 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as filter."}, "tf.nn.dilation2d": {"description": "Computes the grayscale dilation of 4-D input and 3-D filters tensors.", "Args": {"input": "A Tensor. Must be one of the following types: float32, float64,\nint32, uint8, int16, int8, int64, bfloat16, uint16, half,\nuint32, uint64.\n4-D with shape [batch, in_height, in_width, depth].", "filters": "A Tensor. Must have the same type as input.\n3-D with shape [filter_height, filter_width, depth].", "strides": "A list of ints that has length >= 4.\nThe stride of the sliding window for each dimension of the input\ntensor. Must be: [1, stride_height, stride_width, 1].", "padding": "A string from: \"SAME\", \"VALID\".\nThe type of padding algorithm to use. See\nhere\nfor more information.", "data_format": "A string, only \"NHWC\" is currently supported.", "dilations": "A list of ints that has length >= 4.\nThe input stride for atrous morphological dilation. Must be:\n[1, rate_height, rate_width, 1].", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.nn.dropout": {"description": "Computes dropout: randomly sets elements to zero to prevent overfitting.", "Args": {"x": "A floating point tensor.", "rate": "A scalar Tensor with the same type as x. The probability\nthat each element is dropped. For example, setting rate=0.1 would drop\n10% of input elements.", "noise_shape": "A 1-D integer Tensor, representing the\nshape for randomly generated keep/drop flags.", "seed": "A Python integer. Used to create random seeds. See\ntf.random.set_seed for behavior.", "name": "A name for this operation (optional)."}, "Returns": "A Tensor of the same shape of x.", "Raises": {"ValueError": "If rate is not in [0, 1) or if x is not a floating point\ntensor. rate=1 is disallowed, because the output would be all zeros,\nwhich is likely not what was intended."}}, "tf.nn.elu": {"description": "Computes the exponential linear function.", "Args": {"features": "A Tensor. Must be one of the following types: half, bfloat16, float32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as features."}, "tf.nn.embedding_lookup": {"description": "Looks up embeddings for the given ids from a list of tensors.", "Args": {"params": "A single tensor representing the complete embedding tensor, or a\nlist of tensors all of same shape except for the first dimension,\nrepresenting sharded embedding tensors following \"div\" partition strategy.", "ids": "A Tensor with type int32 or int64 containing the ids to be looked\nup in params.", "max_norm": "If not None, each embedding is clipped if its l2-norm is larger\nthan this value.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor with the same type as the tensors in params.\nFor instance, if params is a 5x2 matrix:\n[[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]\nor a list of matrices:\nparams[0]: [[1, 2], [3, 4]]params[1]: [[5, 6], [7, 8]]params[2]: [[9, 10]]\nand ids is:\n[0, 3, 4]\nThe output will be a 3x2 matrix:\n[[1, 2], [7, 8], [9, 10]]", "Raises": {"ValueError": "If params is empty."}}, "tf.nn.embedding_lookup_sparse": {"description": "Looks up embeddings for the given ids and weights from a list of tensors.", "Args": {"params": "A single tensor representing the complete embedding tensor, or a\nlist of tensors all of same shape except for the first dimension,\nrepresenting sharded embedding tensors following \"div\" partition strategy.", "sp_ids": "N x M SparseTensor of int64 ids where N is typically batch size\nand M is arbitrary.", "sp_weights": "either a SparseTensor of float / double weights, or None to\nindicate all weights should be taken to be 1. If specified, sp_weights\nmust have exactly the same shape and indices as sp_ids.", "combiner": "A string specifying the reduction op. Currently \"mean\", \"sqrtn\"\nand \"sum\" are supported. \"sum\" computes the weighted sum of the embedding\nresults for each row. \"mean\" is the weighted sum divided by the total\nweight. \"sqrtn\" is the weighted sum divided by the square root of the sum\nof the squares of the weights. Defaults to mean.", "max_norm": "If not None, each embedding is clipped if its l2-norm is larger\nthan this value, before combining.", "name": "Optional name for the op."}, "Returns": "A dense tensor representing the combined embeddings for the\nsparse ids. For each row in the dense tensor represented by sp_ids, the op\nlooks up the embeddings for all ids in that row, multiplies them by the\ncorresponding weight, and combines these embeddings as specified.\nIn other words, if\nshape(combined params) = [p0, p1, ..., pm]\nand\nshape(sp_ids) = shape(sp_weights) = [d0, d1]\nthen\nshape(output) = [d0, p1, ..., pm].\nFor instance, if params is a 10x20 matrix, and sp_ids / sp_weights are\n\u00a0 [0, 0]: id 1, weight 2.0\u00a0 [0, 1]: id 3, weight 0.5\u00a0 [1, 0]: id 0, weight 1.0\u00a0 [2, 3]: id 1, weight 3.0\nwith combiner=\"mean\", then the output will be a 3x20 matrix where\n\u00a0 output[0, :] = (params[1, :] * 2.0 + params[3, :] * 0.5) / (2.0 + 0.5)\u00a0 output[1, :] = (params[0, :] * 1.0) / 1.0\u00a0 output[2, :] = (params[1, :] * 3.0) / 3.0", "Raises": {"TypeError": "If sp_ids is not a SparseTensor, or if sp_weights is\nneither None nor SparseTensor.", "ValueError": "If combiner is not one of {\"mean\", \"sqrtn\", \"sum\"}."}}, "tf.nn.erosion2d": {"description": "Computes the grayscale erosion of 4-D value and 3-D filters tensors.", "Args": {"value": "A Tensor. 4-D with shape [batch, in_height, in_width, depth].", "filters": "A Tensor. Must have the same type as value.\n3-D with shape [filters_height, filters_width, depth].", "strides": "A list of ints that has length >= 4.\n1-D of length 4. The stride of the sliding window for each dimension of\nthe input tensor. Must be: [1, stride_height, stride_width, 1].", "padding": "A string from: \"SAME\", \"VALID\".\nThe type of padding algorithm to use. See\nhere\nfor more information.", "data_format": "A string, only \"NHWC\" is currently supported.", "dilations": "A list of ints that has length >= 4.\n1-D of length 4. The input stride for atrous morphological dilation.\nMust be: [1, rate_height, rate_width, 1].", "name": "A name for the operation (optional). If not specified \"erosion2d\"\nis used."}, "Returns": "A Tensor. Has the same type as value.\n4-D with shape [batch, out_height, out_width, depth].", "Raises": {"ValueError": "If the value depth does not match filters' shape, or if\npadding is other than 'VALID' or 'SAME'."}}, "tf.nn.fractional_avg_pool": {"description": "Performs fractional average pooling on the input.", "Args": {"value": "A Tensor. 4-D with shape [batch, height, width, channels].", "pooling_ratio": "A list of floats that has length >= 4.  Pooling ratio for\neach dimension of value, currently only supports row and col dimension\nand should be >= 1.0. For example, a valid pooling ratio looks like [1.0,\n1.44, 1.73, 1.0]. The first and last elements must be 1.0 because we don't\nallow pooling on batch and channels dimensions.  1.44 and 1.73 are pooling\nratio on height and width dimensions respectively.", "pseudo_random": "An optional bool.  Defaults to False. When set to True,\ngenerates the pooling sequence in a pseudorandom fashion, otherwise, in a\nrandom fashion. Check paper (Graham, 2015) for difference between\npseudorandom and random.", "overlapping": "An optional bool.  Defaults to False.  When set to True,\nit means when pooling, the values at the boundary of adjacent pooling\ncells are used by both cells. For example:\nindex  0  1  2  3  4\nvalue  20 5  16 3  7\nIf the pooling sequence is [0, 2, 4], then 16, at index 2 will be used\ntwice.  The result would be [20, 16] for fractional avg pooling.", "seed": "An optional int.  Defaults to 0.  If set to be non-zero, the\nrandom number generator is seeded by the given seed.  Otherwise it is\nseeded by a random seed.", "name": "A name for the operation (optional)."}}, "tf.nn.fractional_max_pool": {"description": "Performs fractional max pooling on the input.", "Args": {"value": "A Tensor. 4-D with shape [batch, height, width, channels].", "pooling_ratio": "An int or list of ints that has length 1, 2 or 4.\nPooling ratio for each dimension of value, currently only supports row\nand col dimension and should be >= 1.0. For example, a valid pooling ratio\nlooks like [1.0, 1.44, 1.73, 1.0]. The first and last elements must be 1.0\nbecause we don't allow pooling on batch and channels dimensions.  1.44 and\n1.73 are pooling ratio on height and width dimensions respectively.", "pseudo_random": "An optional bool.  Defaults to False. When set to True,\ngenerates the pooling sequence in a pseudorandom fashion, otherwise, in a\nrandom fashion. Check paper (Graham, 2015) for difference between\npseudorandom and random.", "overlapping": "An optional bool.  Defaults to False.  When set to True,\nit means when pooling, the values at the boundary of adjacent pooling\ncells are used by both cells. For example:\nindex  0  1  2  3  4\nvalue  20 5  16 3  7\nIf the pooling sequence is [0, 2, 4], then 16, at index 2 will be used\ntwice.  The result would be [20, 16] for fractional max pooling.", "seed": "An optional int.  Defaults to 0.  If set to be non-zero, the\nrandom number generator is seeded by the given seed.  Otherwise it is\nseeded by a random seed.", "name": "A name for the operation (optional)."}, "Raises": {"ValueError": "If no seed is specified and op determinism is enabled."}}, "tf.nn.gelu": {"description": "Compute the Gaussian Error Linear Unit (GELU) activation function.", "Args": {"features": "A Tensor representing preactivation values.", "approximate": "An optional bool. Defaults to False. Whether to enable\napproximation.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor with the same type as features."}, "tf.nn.isotonic_regression": {"description": "Solves isotonic regression problems along the given axis.", "Args": {"inputs": "A tensor holding the inputs.", "decreasing": "If set to False, the inequalities in the optimizing constrained\nare flipped.", "axis": "The axis along which the problems should be solved."}, "Returns": "output\n\n\nThe solutions, same shape as type as the input."}, "tf.nn.l2_loss": {"description": "L2 Loss.", "Args": {"t": "A Tensor. Must be one of the following types: half, bfloat16, float32, float64.\nTypically 2-D, but may have any dimensions.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as t."}, "tf.nn.leaky_relu": {"description": "Compute the Leaky ReLU activation function.", "Args": {"features": "A Tensor representing preactivation values. Must be one of\nthe following types: float16, float32, float64, int32, int64.", "alpha": "Slope of the activation function at x < 0.", "name": "A name for the operation (optional)."}, "Returns": "The activation value."}, "tf.nn.local_response_normalization": {"description": "Local Response Normalization.", "Args": {"input": "A Tensor. Must be one of the following types: half, bfloat16, float32.\n4-D.", "depth_radius": "An optional int. Defaults to 5.\n0-D.  Half-width of the 1-D normalization window.", "bias": "An optional float. Defaults to 1.\nAn offset (usually positive to avoid dividing by 0).", "alpha": "An optional float. Defaults to 1.\nA scale factor, usually positive.", "beta": "An optional float. Defaults to 0.5. An exponent.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.nn.log_poisson_loss": {"description": "Computes log Poisson loss given log_input.", "Args": {"targets": "A Tensor of the same type and shape as log_input.", "log_input": "A Tensor of type float32 or float64.", "compute_full_loss": "whether to compute the full loss. If false, a constant\nterm is dropped in favor of more efficient optimization.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of the same shape as log_input with the componentwise\nlogistic losses.", "Raises": {"ValueError": "If log_input and targets do not have the same shape."}}, "tf.nn.log_softmax": {"description": "Computes log softmax activations.", "Args": {"logits": "A non-empty Tensor. Must be one of the following types: half,\nfloat32, float64.", "axis": "The dimension softmax would be performed on. The default is -1 which\nindicates the last dimension.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as logits. Same shape as logits.", "Raises": {"InvalidArgumentError": "if logits is empty or axis is beyond the last\ndimension of logits."}}, "tf.nn.max_pool": {"description": "Performs max pooling on the input.", "Args": {"input": "Tensor of rank N+2, of shape [batch_size] + input_spatial_shape +\n[num_channels] if data_format does not start with \"NC\" (default), or\n[batch_size, num_channels] + input_spatial_shape if data_format starts\nwith \"NC\". Pooling happens over the spatial dimensions only.", "ksize": "An int or list of ints that has length 1, N or N+2. The size\nof the window for each dimension of the input tensor.", "strides": "An int or list of ints that has length 1, N or N+2. The\nstride of the sliding window for each dimension of the input tensor.", "padding": "Either the string \"SAME\" or \"VALID\" indicating the type of\npadding algorithm to use, or a list indicating the explicit paddings at\nthe start and end of each dimension. See\nhere\nfor more information. When explicit padding is used and data_format is\n\"NHWC\", this should be in the form [[0, 0], [pad_top, pad_bottom],\n[pad_left, pad_right], [0, 0]]. When explicit padding used and\ndata_format is \"NCHW\", this should be in the form [[0, 0], [0, 0],\n[pad_top, pad_bottom], [pad_left, pad_right]]. When using explicit\npadding, the size of the paddings cannot be greater than the sliding\nwindow size.", "data_format": "A string. Specifies the channel dimension. For N=1 it can be\neither \"NWC\" (default) or \"NCW\", for N=2 it can be either \"NHWC\" (default)\nor \"NCHW\" and for N=3 either \"NDHWC\" (default) or \"NCDHW\".", "name": "Optional name for the operation."}, "Returns": "A Tensor of format specified by data_format.\nThe max pooled output tensor."}, "tf.nn.max_pool1d": {"description": "Performs the max pooling on the input.", "Args": {"input": "A 3-D Tensor of the format specified by data_format.", "ksize": "An int or list of ints that has length 1 or 3. The size of the\nwindow for each dimension of the input tensor.", "strides": "An int or list of ints that has length 1 or 3. The stride of\nthe sliding window for each dimension of the input tensor.", "padding": "Either the string \"SAME\" or \"VALID\" indicating the type of\npadding algorithm to use, or a list indicating the explicit paddings at\nthe start and end of each dimension. See\nhere\nfor more information. When explicit padding is used and data_format is\n\"NWC\", this should be in the form [[0, 0], [pad_left, pad_right], [0,\n0]]. When explicit padding used and data_format is \"NCW\", this should\nbe in the form [[0, 0], [0, 0], [pad_left, pad_right]]. When using\nexplicit padding, the size of the paddings cannot be greater than the\nsliding window size.", "data_format": "An optional string from: \"NWC\", \"NCW\". Defaults to \"NWC\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of format specified by data_format.\nThe max pooled output tensor."}, "tf.nn.max_pool2d": {"description": "Performs max pooling on 2D spatial data such as images.", "Args": {"input": "A 4-D Tensor of the format specified by data_format.", "ksize": "An int or list of ints that has length 1, 2 or 4. The size of\nthe window for each dimension of the input tensor. If only one integer is\nspecified, then we apply the same window for all 4 dims. If two are\nprovided then we use those for H, W dimensions and keep N, C dimension\nwindow size = 1.", "strides": "An int or list of ints that has length 1, 2 or 4. The\nstride of the sliding window for each dimension of the input tensor. If\nonly one integer is specified, we apply the same stride to all 4 dims. If\ntwo are provided we use those for the H, W dimensions and keep N, C of\nstride = 1.", "padding": "Either the string \"SAME\" or \"VALID\" indicating the type of\npadding algorithm to use, or a list indicating the explicit paddings at\nthe start and end of each dimension. See\nhere\n  for more information. When explicit padding is used and data_format is\n  \"NHWC\", this should be in the form [[0, 0], [pad_top, pad_bottom],\n  [pad_left, pad_right], [0, 0]]. When explicit padding used and\n  data_format is \"NCHW\", this should be in the form [[0, 0], [0, 0],\n  [pad_top, pad_bottom], [pad_left, pad_right]]. When using explicit\n  padding, the size of the paddings cannot be greater than the sliding\n  window size.", "data_format": "A string. 'NHWC', 'NCHW' and 'NCHW_VECT_C' are supported.", "name": "Optional name for the operation."}, "Returns": "A Tensor of format specified by data_format.\nThe max pooled output tensor."}, "tf.nn.max_pool3d": {"description": "Performs the max pooling on the input.", "Args": {"input": "A 5-D Tensor of the format specified by data_format.", "ksize": "An int or list of ints that has length 1, 3 or 5. The size of\nthe window for each dimension of the input tensor.", "strides": "An int or list of ints that has length 1, 3 or 5. The\nstride of the sliding window for each dimension of the input tensor.", "padding": "A string, either 'VALID' or 'SAME'. The padding algorithm. See\nhere\nfor more information.", "data_format": "An optional string from: \"NDHWC\", \"NCDHW\". Defaults to \"NDHWC\".\nThe data format of the input and output data. With the default format\n\"NDHWC\", the data is stored in the order of: [batch, in_depth, in_height,\n  in_width, in_channels]. Alternatively, the format could be \"NCDHW\", the\ndata storage order is: [batch, in_channels, in_depth, in_height,\n  in_width].", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of format specified by data_format.\nThe max pooled output tensor."}, "tf.nn.max_pool_with_argmax": {"description": "Performs max pooling on the input and outputs both max values and indices.", "Args": {"input": "A Tensor. Must be one of the following types: float32, float64,\nint32, uint8, int16, int8, int64, bfloat16, uint16, half,\nuint32, uint64.\n4-D with shape [batch, height, width, channels].  Input to pool over.", "ksize": "An int or list of ints that has length 1, 2 or 4.\nThe size of the window for each dimension of the input tensor.", "strides": "An int or list of ints that has length 1, 2 or 4.\nThe stride of the sliding window for each dimension of the\ninput tensor.", "padding": "A string from: \"SAME\", \"VALID\".\nThe type of padding algorithm to use. See\nhere\nfor more information.", "data_format": "An optional string, must be set to \"NHWC\". Defaults to\n\"NHWC\".\nSpecify the data format of the input and output data.", "output_dtype": "An optional tf.DType from: tf.int32, tf.int64.\nDefaults to tf.int64.\nThe dtype of the returned argmax tensor.", "include_batch_in_index": "An optional boolean. Defaults to False.\nWhether to include batch dimension in flattened index of argmax.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output, argmax)."}, "tf.nn.moments": {"description": "Calculates the mean and variance of x.", "Args": {"x": "A Tensor.", "axes": "Array of ints.  Axes along which to compute mean and\nvariance.", "shift": "Not used in the current implementation.", "keepdims": "produce moments with the same dimensionality as the input.", "name": "Name used to scope the operations that compute the moments."}, "Returns": "Two Tensor objects: mean and variance."}, "tf.nn.nce_loss": {"description": "Computes and returns the noise-contrastive estimation training loss.", "Args": {"weights": "A Tensor of shape [num_classes, dim], or a list of Tensor\nobjects whose concatenation along dimension 0 has shape [num_classes,\ndim].  The (possibly-partitioned) class embeddings.", "biases": "A Tensor of shape [num_classes].  The class biases.", "labels": "A Tensor of type int64 and shape [batch_size, num_true]. The\ntarget classes.", "inputs": "A Tensor of shape [batch_size, dim].  The forward activations of\nthe input network.", "num_sampled": "An int.  The number of negative classes to randomly sample\nper batch. This single sample of negative classes is evaluated for each\nelement in the batch.", "num_classes": "An int. The number of possible classes.", "num_true": "An int.  The number of target classes per training example.", "sampled_values": "a tuple of (sampled_candidates, true_expected_count,\nsampled_expected_count) returned by a *_candidate_sampler function.\n(if None, we default to log_uniform_candidate_sampler)", "remove_accidental_hits": "A bool.  Whether to remove \"accidental hits\"\nwhere a sampled class equals one of the target classes.  If set to True,\nthis is a \"Sampled Logistic\" loss instead of NCE, and we are learning to\ngenerate log-odds instead of log probabilities.  See our Candidate\nSampling Algorithms Reference. Default is\n    False.", "name": "A name for the operation (optional)."}, "Returns": "A batch_size 1-D tensor of per-example NCE losses."}, "tf.nn.normalize_moments": {"description": "Calculate the mean and variance of based on the sufficient statistics.", "Args": {"counts": "A Tensor containing the total count of the data (one value).", "mean_ss": "A Tensor containing the mean sufficient statistics: the (possibly\nshifted) sum of the elements to average over.", "variance_ss": "A Tensor containing the variance sufficient statistics: the\n(possibly shifted) squared sum of the data to compute the variance over.", "shift": "A Tensor containing the value by which the data is shifted for\nnumerical stability, or None if no shift was performed.", "name": "Name used to scope the operations that compute the moments."}, "Returns": "Two Tensor objects: mean and variance."}, "tf.nn.pool": {"description": "Performs an N-D pooling operation.", "Args": {"input": "Tensor of rank N+2, of shape [batch_size] + input_spatial_shape +\n[num_channels] if data_format does not start with \"NC\" (default), or\n[batch_size, num_channels] + input_spatial_shape if data_format starts\nwith \"NC\".  Pooling happens over the spatial dimensions only.", "window_shape": "Sequence of N ints >= 1.", "pooling_type": "Specifies pooling operation, must be \"AVG\" or \"MAX\".", "strides": "Optional. Sequence of N ints >= 1.  Defaults to [1]*N. If any value of\nstrides is > 1, then all values of dilation_rate must be 1.", "padding": "The padding algorithm, must be \"SAME\" or \"VALID\". Defaults to \"SAME\".\nSee\nhere\nfor more information.", "data_format": "A string or None.  Specifies whether the channel dimension of\nthe input and output is the last dimension (default, or if data_format\ndoes not start with \"NC\"), or the second dimension (if data_format\nstarts with \"NC\").  For N=1, the valid values are \"NWC\" (default) and\n\"NCW\".  For N=2, the valid values are \"NHWC\" (default) and \"NCHW\". For\nN=3, the valid values are \"NDHWC\" (default) and \"NCDHW\".", "dilations": "Optional.  Dilation rate.  List of N ints >= 1. Defaults to\n[1]*N.  If any value of dilation_rate is > 1, then all values of strides\nmust be 1.", "name": "Optional. Name of the op."}, "Returns": "Tensor of rank N+2, of shape\n  [batch_size] + output_spatial_shape + [num_channels]\nif data_format is None or does not start with \"NC\", or\n[batch_size, num_channels] + output_spatial_shape\nif data_format starts with \"NC\",\nwhere output_spatial_shape depends on the value of padding:\nIf padding = \"SAME\":\n  output_spatial_shape[i] = ceil(input_spatial_shape[i] / strides[i])\nIf padding = \"VALID\":\n  output_spatial_shape[i] =\n    ceil((input_spatial_shape[i] - (window_shape[i] - 1) * dilation_rate[i])\n         / strides[i]).", "Raises": {"ValueError": "if arguments are invalid."}}, "tf.nn.relu": {"description": "Computes rectified linear: max(features, 0).", "Args": {"features": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64, qint8.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as features."}, "tf.nn.relu6": {"description": "Computes Rectified Linear 6: min(max(features, 0), 6).", "Args": {"features": "A Tensor with type float, double, int32, int64, uint8,\nint16, or int8.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor with the same type as features."}, "tf.nn.safe_embedding_lookup_sparse": {"description": "Lookup embedding results, accounting for invalid IDs and empty features.", "Args": {"embedding_weights": "A single tensor representing the complete embedding\ntensor, or a list of tensors all of same shape except for the first\ndimension, representing sharded embedding tensors following \"div\"\npartition strategy.", "sparse_ids": "SparseTensor of shape [d_0, d_1, ..., d_n] containing the\nids. d_0 is typically batch size.", "sparse_weights": "SparseTensor of same shape as sparse_ids, containing\nfloat weights corresponding to sparse_ids, or None if all weights are\nbe assumed to be 1.0.", "combiner": "A string specifying how to combine embedding results for each\nentry. Currently \"mean\", \"sqrtn\" and \"sum\" are supported, with \"mean\" the\ndefault.", "default_id": "The id to use for an entry with no features. Defaults to\n0-vector.", "max_norm": "If not None, all embeddings are l2-normalized to max_norm before\ncombining.", "name": "A name for this operation (optional)."}, "Returns": "A dense tensor representing the combined embeddings for the\nsparse ids. For each row in the dense tensor represented by sparse_ids,\nthe op looks up the embeddings for all ids in that row, multiplies them by\nthe corresponding weight, and combines these embeddings as specified.\nIn other words, if\nshape(combined embedding_weights) = [p0, p1, ..., pm]\nand\nshape(sparse_ids) = shape(sparse_weights) = [d0, d1, ..., dn]\nthen\nshape(output) = [d0, d1, ... dn-1, p1, ..., pm].\nFor instance, if params is a 10x20 matrix, and sp_ids / sp_weights are\n\u00a0 [0, 0]: id 1, weight 2.0\u00a0 [0, 1]: id 3, weight 0.5\u00a0 [1, 0]: id -1, weight 1.0\u00a0 [2, 3]: id 1, weight 3.0\ndefault_id is 0.\nwith combiner=\"mean\", then the output will be a 3x20 matrix where\n\u00a0 output[0, :] = (params[1, :] * 2.0 + params[3, :] * 0.5) / (2.0 + 0.5)\u00a0 output[1, :] = (params[0, :] * 1.0) / 1.0\u00a0 output[2, :] = (params[1, :] * 3.0) / 3.0", "Raises": {"ValueError": "if embedding_weights is empty."}}, "tf.nn.sampled_softmax_loss": {"description": "Computes and returns the sampled softmax training loss.", "Args": {"weights": "A Tensor of shape [num_classes, dim], or a list of Tensor\nobjects whose concatenation along dimension 0 has shape [num_classes,\ndim].  The (possibly-sharded) class embeddings.", "biases": "A Tensor of shape [num_classes].  The class biases.", "labels": "A Tensor of type int64 and shape [batch_size, num_true]. The\ntarget classes.  Note that this format differs from the labels argument\nof nn.softmax_cross_entropy_with_logits.", "inputs": "A Tensor of shape [batch_size, dim].  The forward activations of\nthe input network.", "num_sampled": "An int.  The number of classes to randomly sample per batch.", "num_classes": "An int. The number of possible classes.", "num_true": "An int.  The number of target classes per training example.", "sampled_values": "a tuple of (sampled_candidates, true_expected_count,\nsampled_expected_count) returned by a *_candidate_sampler function.\n(if None, we default to log_uniform_candidate_sampler)", "remove_accidental_hits": "A bool.  whether to remove \"accidental hits\"\nwhere a sampled class equals one of the target classes.  Default is True.", "seed": "random seed for candidate sampling. Default to None, which doesn't set\nthe op-level random seed for candidate sampling.", "name": "A name for the operation (optional)."}, "Returns": "A batch_size 1-D tensor of per-example sampled softmax losses."}, "tf.nn.scale_regularization_loss": {"description": "Scales the sum of the given regularization losses by number of replicas.", "Args": {"regularization_loss": "Regularization loss."}, "Returns": "Scalar loss value."}, "tf.nn.selu": {"description": "Computes scaled exponential linear: scale * alpha * (exp(features) - 1)", "Args": {"features": "A Tensor. Must be one of the following types: half, bfloat16, float32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as features."}, "tf.nn.separable_conv2d": {"description": "2-D convolution with separable filters.", "Args": {"input": "4-D Tensor with shape according to data_format.", "depthwise_filter": "4-D Tensor with shape [filter_height, filter_width,\nin_channels, channel_multiplier]. Contains in_channels convolutional\nfilters of depth 1.", "pointwise_filter": "4-D Tensor with shape [1, 1, channel_multiplier *\nin_channels, out_channels].  Pointwise filter to mix channels after\ndepthwise_filter has convolved spatially.", "strides": "1-D of size 4.  The strides for the depthwise convolution for each\ndimension of input.", "padding": "Controls how to pad the image before applying the depthwise\nconvolution. Can be the string \"SAME\" or \"VALID\" indicating the type\nof padding algorithm to use, or a Python list indicating the explicit\npaddings at the start and end of each dimension. When explicit padding is\nused and data_format is \"NHWC\", this should be in the form [[0, 0],\n[pad_top, pad_bottom], [pad_left, pad_right], [0, 0]]. When explicit\npadding used and data_format is \"NCHW\", this should be in the form\n[[0, 0], [0, 0], [pad_top, pad_bottom], [pad_left, pad_right]].", "data_format": "The data format for input. Either \"NHWC\" (default) or \"NCHW\".", "dilations": "1-D of size 2. The dilation rate in which we sample input values\nacross the height and width dimensions in atrous convolution. If it is\ngreater than 1, then all values of strides must be 1.", "name": "A name for this operation (optional)."}, "Returns": "A 4-D Tensor with shape according to 'data_format'. For\nexample, with data_format=\"NHWC\", shape is [batch, out_height,\nout_width, out_channels]."}, "tf.nn.sigmoid_cross_entropy_with_logits": {"description": "Computes sigmoid cross entropy given logits.", "Args": {"labels": "A Tensor of the same type and shape as logits. Between 0 and 1,\ninclusive.", "logits": "A Tensor of type float32 or float64. Any real number.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of the same shape as logits with the componentwise\nlogistic losses.", "Raises": {"ValueError": "If logits and labels do not have the same shape."}}, "tf.nn.silu": {"description": "Computes the SiLU or Swish activation function: x * sigmoid(beta * x).", "Args": {"features": "A Tensor representing preactivation values.", "beta": "A 'Tensor' representing value of beta hyperparameter."}, "Returns": "The activation value."}, "tf.nn.softmax": {"description": "Computes softmax activations.", "Args": {"logits": "A non-empty Tensor. Must be one of the following types: half,\nfloat32, float64.", "axis": "The dimension softmax would be performed on. The default is -1 which\nindicates the last dimension.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type and shape as logits.", "Raises": {"InvalidArgumentError": "if logits is empty or axis is beyond the last\ndimension of logits."}}, "tf.nn.softmax_cross_entropy_with_logits": {"description": "Computes softmax cross entropy between logits and labels.", "Args": {"labels": "Each vector along the class dimension should hold a valid\nprobability distribution e.g. for the case in which labels are of shape\n[batch_size, num_classes], each row of labels[i] must be a valid\nprobability distribution.", "logits": "Per-label activations, typically a linear output. These activation\nenergies are interpreted as unnormalized log probabilities.", "axis": "The class dimension. Defaulted to -1 which is the last dimension.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor that contains the softmax cross entropy loss. Its type is the\nsame as logits and its shape is the same as labels except that it does\nnot have the last dimension of labels."}, "tf.nn.softsign": {"description": "Computes softsign: features / (abs(features) &#43; 1).", "Args": {"features": "A Tensor. Must be one of the following types: half, bfloat16, float32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as features."}, "tf.nn.space_to_depth": {"description": "SpaceToDepth for tensors of type T.", "Args": {"input": "A Tensor.", "block_size": "An int that is >= 2. The size of the spatial block.", "data_format": "An optional string from: \"NHWC\", \"NCHW\", \"NCHW_VECT_C\". Defaults to \"NHWC\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.nn.sparse_softmax_cross_entropy_with_logits": {"description": "Computes sparse softmax cross entropy between logits and labels.", "Args": {"labels": "Tensor of shape [d_0, d_1, ..., d_{r-1}] (where r is rank of\nlabels and result) and dtype int32 or int64. Each entry in labels\nmust be an index in [0, num_classes). Other values will raise an\nexception when this op is run on CPU, and return NaN for corresponding\nloss and gradient rows on GPU.", "logits": "Unscaled log probabilities of shape [d_0, d_1, ..., d_{r-1},\nnum_classes] and dtype float16, float32, or float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of the same shape as labels and of the same type as logits\nwith the softmax cross entropy loss.", "Raises": {"ValueError": "If logits are scalars (need to have rank >= 1) or if the rank\nof the labels is not equal to the rank of the logits minus one."}}, "tf.nn.sufficient_statistics": {"description": "Calculate the sufficient statistics for the mean and variance of x.", "Args": {"x": "A Tensor.", "axes": "Array of ints. Axes along which to compute mean and variance.", "shift": "A Tensor containing the value by which to shift the data for\nnumerical stability, or None if no shift is to be performed. A shift\nclose to the true mean provides the most numerically stable results.", "keepdims": "produce statistics with the same dimensionality as the input.", "name": "Name used to scope the operations that compute the sufficient stats."}, "Returns": "Four Tensor objects of the same type as x:\n\nthe count (number of elements to average over).\nthe (possibly shifted) sum of the elements in the array.\nthe (possibly shifted) sum of squares of the elements in the array.\nthe shift by which the mean must be corrected or None if shift is None."}, "tf.nn.weighted_cross_entropy_with_logits": {"description": "Computes a weighted cross entropy.", "Args": {"labels": "A Tensor of the same type and shape as logits, with values\nbetween 0 and 1 inclusive.", "logits": "A Tensor of type float32 or float64, any real numbers.", "pos_weight": "A coefficient to use on the positive examples, typically a\nscalar but otherwise broadcastable to the shape of logits. Its value\nshould be non-negative.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of the same shape as logits with the componentwise\nweighted logistic losses.", "Raises": {"ValueError": "If logits and labels do not have the same shape."}}, "tf.nn.weighted_moments": {"description": "Returns the frequency-weighted mean and variance of x.", "Args": {"x": "A tensor.", "axes": "1-d tensor of int32 values; these are the axes along which\nto compute mean and variance.", "frequency_weights": "A tensor of positive weights which can be\nbroadcast with x.", "keepdims": "Produce moments with the same dimensionality as the input.", "name": "Name used to scope the operation."}, "Returns": "Two tensors: weighted_mean and weighted_variance."}, "tf.nn.with_space_to_batch": {"description": "Performs op on the space-to-batch representation of input.", "Args": {"input": "Tensor of rank > max(spatial_dims).", "dilation_rate": "int32 Tensor of known shape [num_spatial_dims].", "padding": "str constant equal to \"VALID\" or \"SAME\"", "op": "Function that maps (input, num_spatial_dims, padding) -> output", "filter_shape": "If padding = \"SAME\", specifies the shape of the convolution\nkernel/pooling window as an integer Tensor of shape [>=num_spatial_dims].\nIf padding = \"VALID\", filter_shape is ignored and need not be specified.", "spatial_dims": "Monotonically increasing sequence of num_spatial_dims\nintegers (which are >= 1) specifying the spatial dimensions of input\nand output.  Defaults to: range(1, num_spatial_dims+1).", "data_format": "A string or None.  Specifies whether the channel dimension of\nthe input and output is the last dimension (default, or if data_format\ndoes not start with \"NC\"), or the second dimension (if data_format\nstarts with \"NC\").  For N=1, the valid values are \"NWC\" (default) and\n\"NCW\".  For N=2, the valid values are \"NHWC\" (default) and \"NCHW\".\nFor N=3, the valid values are \"NDHWC\" (default) and \"NCDHW\"."}, "Returns": "The output Tensor as described above, dimensions will vary based on the op\nprovided.", "Raises": {"ValueError": "if spatial_dims are invalid."}}}, "tf.profiler": {}, "tf.profiler.experimental.client": {"tf.profiler.experimental.client.monitor": {"description": "Sends grpc requests to profiler server to perform on-demand monitoring.", "Args": {"service_addr": "gRPC address of profiler service e.g. grpc://10.0.0.2:8466.", "duration_ms": "Duration of monitoring in ms.", "level": "Choose a monitoring level between 1 and 2 to monitor your job. Level\n2 is more verbose than level 1 and shows more metrics."}, "Returns": "A string of monitoring output."}, "tf.profiler.experimental.client.trace": {"description": "Sends gRPC requests to one or more profiler servers to perform on-demand profiling.", "Args": {"service_addr": "A comma delimited string of gRPC addresses of the workers to\nprofile.\ne.g. service_addr='grpc://localhost:6009'\n     service_addr='grpc://10.0.0.2:8466,grpc://10.0.0.3:8466'\n     service_addr='grpc://localhost:12345,grpc://localhost:23456'", "logdir": "Path to save profile data to, typically a TensorBoard log directory.\nThis path must be accessible to both the client and server.\ne.g. logdir='gs://your_tb_dir'", "duration_ms": "Duration of tracing or monitoring in milliseconds. Must be\ngreater than zero.", "worker_list": "An optional TPU only configuration. The list of workers to\nprofile in the current session.", "num_tracing_attempts": "Optional. Automatically retry N times when no trace\nevent is collected (default 3).", "options": "profiler.experimental.ProfilerOptions namedtuple for miscellaneous\nprofiler options."}, "Raises": {"InvalidArgumentError": "For when arguments fail validation checks.", "UnavailableError": "If no trace event was collected."}}}, "tf.profiler.experimental.server": {"tf.profiler.experimental.server.start": {"description": "Start a profiler grpc server that listens to given port.", "Args": {"port": "port profiler server listens to."}}}, "tf.quantization": {"tf.quantization.dequantize": {"description": "Dequantize the &#39;input&#39; tensor into a float or bfloat16 Tensor.", "Args": {"input": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.", "min_range": "A Tensor of type float32.\nThe minimum scalar value possibly produced for the input.", "max_range": "A Tensor of type float32.\nThe maximum scalar value possibly produced for the input.", "mode": "An optional string from: \"MIN_COMBINED\", \"MIN_FIRST\", \"SCALED\". Defaults to \"MIN_COMBINED\".", "narrow_range": "An optional bool. Defaults to False.", "axis": "An optional int. Defaults to -1.", "dtype": "An optional tf.DType from: tf.bfloat16, tf.float32. Defaults to tf.float32.\nType of the output tensor. Currently Dequantize supports float and bfloat16.\nIf 'dtype' is 'bfloat16', it only supports 'MIN_COMBINED' mode.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dtype."}, "tf.quantization.fake_quant_with_min_max_args": {"description": "Fake-quantize the &#39;inputs&#39; tensor, type float to &#39;outputs&#39; tensor of same type.", "Args": {"inputs": "A Tensor of type float32.", "min": "An optional float. Defaults to -6.", "max": "An optional float. Defaults to 6.", "num_bits": "An optional int. Defaults to 8.", "narrow_range": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type float32."}, "tf.quantization.fake_quant_with_min_max_args_gradient": {"description": "Compute gradients for a FakeQuantWithMinMaxArgs operation.", "Args": {"gradients": "A Tensor of type float32.\nBackpropagated gradients above the FakeQuantWithMinMaxArgs operation.", "inputs": "A Tensor of type float32.\nValues passed as inputs to the FakeQuantWithMinMaxArgs operation.", "min": "An optional float. Defaults to -6.", "max": "An optional float. Defaults to 6.", "num_bits": "An optional int. Defaults to 8.", "narrow_range": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type float32."}, "tf.quantization.fake_quant_with_min_max_vars": {"description": "Fake-quantize the &#39;inputs&#39; tensor of type float via global float scalars", "Args": {"inputs": "A Tensor of type float32.", "min": "A Tensor of type float32.", "max": "A Tensor of type float32.", "num_bits": "An optional int. Defaults to 8.", "narrow_range": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type float32."}, "tf.quantization.fake_quant_with_min_max_vars_gradient": {"description": "Compute gradients for a FakeQuantWithMinMaxVars operation.", "Args": {"gradients": "A Tensor of type float32.\nBackpropagated gradients above the FakeQuantWithMinMaxVars operation.", "inputs": "A Tensor of type float32.\nValues passed as inputs to the FakeQuantWithMinMaxVars operation.\nmin, max: Quantization interval, scalar floats.", "min": "A Tensor of type float32.", "max": "A Tensor of type float32.", "num_bits": "An optional int. Defaults to 8.\nThe bitwidth of the quantization; between 2 and 8, inclusive.", "narrow_range": "An optional bool. Defaults to False.\nWhether to quantize into 2^num_bits - 1 distinct values.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (backprops_wrt_input, backprop_wrt_min, backprop_wrt_max)."}, "tf.quantization.fake_quant_with_min_max_vars_per_channel": {"description": "Fake-quantize the &#39;inputs&#39; tensor of type float via per-channel floats", "Args": {"inputs": "A Tensor of type float32.", "min": "A Tensor of type float32.", "max": "A Tensor of type float32.", "num_bits": "An optional int. Defaults to 8.", "narrow_range": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type float32."}, "tf.quantization.fake_quant_with_min_max_vars_per_channel_gradient": {"description": "Compute gradients for a FakeQuantWithMinMaxVarsPerChannel operation.", "Args": {"gradients": "A Tensor of type float32.\nBackpropagated gradients above the FakeQuantWithMinMaxVars operation,\nshape one of: [d], [b, d],  [b, h, w, d].", "inputs": "A Tensor of type float32.\nValues passed as inputs to the FakeQuantWithMinMaxVars operation, shape\n  same as gradients.\nmin, max: Quantization interval, floats of shape [d].", "min": "A Tensor of type float32.", "max": "A Tensor of type float32.", "num_bits": "An optional int. Defaults to 8.\nThe bitwidth of the quantization; between 2 and 16, inclusive.", "narrow_range": "An optional bool. Defaults to False.\nWhether to quantize into 2^num_bits - 1 distinct values.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (backprops_wrt_input, backprop_wrt_min, backprop_wrt_max)."}, "tf.quantization.quantize": {"description": "Quantize the &#39;input&#39; tensor of type float to &#39;output&#39; tensor of type &#39;T&#39;.", "Args": {"input": "A Tensor of type float32.", "min_range": "A Tensor of type float32.\nThe minimum value of the quantization range. This value may be adjusted by the\nop depending on other parameters. The adjusted value is written to output_min.\nIf the axis attribute is specified, this must be a 1-D tensor whose size\nmatches the axis dimension of the input and output tensors.", "max_range": "A Tensor of type float32.\nThe maximum value of the quantization range. This value may be adjusted by the\nop depending on other parameters. The adjusted value is written to output_max.\nIf the axis attribute is specified, this must be a 1-D tensor whose size\nmatches the axis dimension of the input and output tensors.", "T": "A tf.DType from: tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16.", "mode": "An optional string from: \"MIN_COMBINED\", \"MIN_FIRST\", \"SCALED\". Defaults to \"MIN_COMBINED\".", "round_mode": "An optional string from: \"HALF_AWAY_FROM_ZERO\", \"HALF_TO_EVEN\". Defaults to \"HALF_AWAY_FROM_ZERO\".", "narrow_range": "An optional bool. Defaults to False.", "axis": "An optional int. Defaults to -1.", "ensure_minimum_range": "An optional float. Defaults to 0.01.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output, output_min, output_max)."}, "tf.quantization.quantize_and_dequantize": {"description": "Quantizes then dequantizes a tensor. (deprecated)", "Args": {"input": "A Tensor to quantize and dequantize.", "input_min": "If range_given=True, the minimum input value, that needs to be\nrepresented in the quantized representation. If axis is specified, this\nshould be a vector of minimum values for each slice along axis.", "input_max": "If range_given=True, the maximum input value that needs to be\nrepresented in the quantized representation. If axis is specified, this\nshould be a vector of maximum values for each slice along axis.", "signed_input": "True if the quantization is signed or unsigned.", "num_bits": "The bitwidth of the quantization.", "range_given": "If true use input_min and input_max for the range of the\ninput, otherwise determine min and max from the input Tensor.", "round_mode": "Rounding mode when rounding from float values to quantized ones.\none of ['HALF_TO_EVEN', 'HALF_UP']", "name": "Optional name for the operation.", "narrow_range": "If true, then the absolute value of the quantized minimum\nvalue is the same as the quantized maximum value, instead of 1 greater.\ni.e. for 8 bit quantization, the minimum value is -127 instead of -128.", "axis": "Integer. If specified, refers to a dimension of the input tensor, such\nthat quantization will be per slice along that dimension."}, "Returns": "A Tensor. Each element is the result of quantizing and dequantizing the\ncorresponding element of input."}, "tf.quantization.quantize_and_dequantize_v2": {"description": "Quantizes then dequantizes a tensor.", "Args": {"input": "A Tensor to quantize and dequantize.", "input_min": "If range_given=True, the minimum input value, that needs to be\nrepresented in the quantized representation. If axis is specified, this\nshould be a vector of minimum values for each slice along axis.", "input_max": "If range_given=True, the maximum input value that needs to be\nrepresented in the quantized representation. If axis is specified, this\nshould be a vector of maximum values for each slice along axis.", "signed_input": "True if the quantization is signed or unsigned.", "num_bits": "The bitwidth of the quantization.", "range_given": "If true use input_min and input_max for the range of the\ninput, otherwise determine min and max from the input Tensor.", "round_mode": "Rounding mode when rounding from float values to quantized ones.\none of ['HALF_TO_EVEN', 'HALF_UP']", "name": "Optional name for the operation.", "narrow_range": "If true, then the absolute value of the quantized minimum\nvalue is the same as the quantized maximum value, instead of 1 greater.\ni.e. for 8 bit quantization, the minimum value is -127 instead of -128.", "axis": "Integer. If specified, refers to a dimension of the input tensor, such\nthat quantization will be per slice along that dimension."}, "Returns": "A Tensor. Each element is the result of quantizing and dequantizing the\ncorresponding element of input."}, "tf.quantization.quantized_concat": {"description": "Concatenates quantized tensors along one dimension.", "Args": {"concat_dim": "A Tensor of type int32.\n0-D.  The dimension along which to concatenate.  Must be in the\nrange [0, rank(values)).", "values": "A list of at least 2 Tensor objects with the same type.\nThe N Tensors to concatenate. Their ranks and types must match,\nand their sizes must match in all dimensions except concat_dim.", "input_mins": "A list with the same length as values of Tensor objects with type float32.\nThe minimum scalar values for each of the input tensors.", "input_maxes": "A list with the same length as values of Tensor objects with type float32.\nThe maximum scalar values for each of the input tensors.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output, output_min, output_max)."}}, "tf.queue": {"tf.queue.FIFOQueue": {"description": "A queue implementation that dequeues elements in first-in first-out order.", "Args": {"capacity": "An integer. The upper bound on the number of elements\nthat may be stored in this queue.", "dtypes": "A list of DType objects. The length of dtypes must equal\nthe number of tensors in each queue element.", "shapes": "(Optional.) A list of fully-defined TensorShape objects\nwith the same length as dtypes, or None.", "names": "(Optional.) A list of string naming the components in the queue\nwith the same length as dtypes, or None.  If specified the dequeue\nmethods return a dictionary with the names as keys.", "shared_name": "(Optional.) If non-empty, this queue will be shared under\nthe given name across multiple sessions.", "name": "Optional name for the queue operation."}, "Attributes": {"dtypes": "The list of dtypes for each component of a queue element.", "name": "The name of the underlying queue.", "names": "The list of names for each component of a queue element.", "queue_ref": "The underlying queue reference.", "shapes": "The list of shapes for each component of a queue element."}}, "tf.queue.PaddingFIFOQueue": {"description": "A FIFOQueue that supports batching variable-sized tensors by padding.", "Args": {"capacity": "An integer. The upper bound on the number of elements\nthat may be stored in this queue.", "dtypes": "A list of DType objects. The length of dtypes must equal\nthe number of tensors in each queue element.", "shapes": "A list of TensorShape objects, with the same length as\ndtypes.  Any dimension in the TensorShape containing value\nNone is dynamic and allows values to be enqueued with\n variable size in that dimension.", "names": "(Optional.) A list of string naming the components in the queue\nwith the same length as dtypes, or None.  If specified the dequeue\nmethods return a dictionary with the names as keys.", "shared_name": "(Optional.) If non-empty, this queue will be shared under\nthe given name across multiple sessions.", "name": "Optional name for the queue operation."}, "Raises": {"ValueError": "If shapes is not a list of shapes, or the lengths of dtypes\nand shapes do not match, or if names is specified and the lengths of\ndtypes and names do not match."}, "Attributes": {"dtypes": "The list of dtypes for each component of a queue element.", "name": "The name of the underlying queue.", "names": "The list of names for each component of a queue element.", "queue_ref": "The underlying queue reference.", "shapes": "The list of shapes for each component of a queue element."}}, "tf.queue.PriorityQueue": {"description": "A queue implementation that dequeues elements in prioritized order.", "Args": {"capacity": "An integer. The upper bound on the number of elements\nthat may be stored in this queue.", "types": "A list of DType objects. The length of types must equal\nthe number of tensors in each queue element, except the first priority\nelement.  The first tensor in each element is the priority,\nwhich must be type int64.", "shapes": "(Optional.) A list of fully-defined TensorShape objects,\nwith the same length as types, or None.", "names": "(Optional.) A list of strings naming the components in the queue\nwith the same length as dtypes, or None.  If specified, the dequeue\nmethods return a dictionary with the names as keys.", "shared_name": "(Optional.) If non-empty, this queue will be shared under\nthe given name across multiple sessions.", "name": "Optional name for the queue operation."}, "Attributes": {"dtypes": "The list of dtypes for each component of a queue element.", "name": "The name of the underlying queue.", "names": "The list of names for each component of a queue element.", "queue_ref": "The underlying queue reference.", "shapes": "The list of shapes for each component of a queue element."}}, "tf.queue.QueueBase": {"description": "Base class for queue implementations.", "Args": {"dtypes": "A list of types.  The length of dtypes must equal the number\nof tensors in each element.", "shapes": "Constraints on the shapes of tensors in an element:\nA list of shape tuples or None. This list is the same length\nas dtypes.  If the shape of any tensors in the element are constrained,\nall must be; shapes can be None if the shapes should not be constrained.", "names": "Optional list of names.  If provided, the enqueue() and\ndequeue() methods will use dictionaries with these names as keys.\nMust be None or a list or tuple of the same length as dtypes.", "queue_ref": "The queue reference, i.e. the output of the queue op."}, "Raises": {"ValueError": "If one of the arguments is invalid."}, "Attributes": {"dtypes": "The list of dtypes for each component of a queue element.", "name": "The name of the underlying queue.", "names": "The list of names for each component of a queue element.", "queue_ref": "The underlying queue reference.", "shapes": "The list of shapes for each component of a queue element."}}, "tf.queue.RandomShuffleQueue": {"description": "A queue implementation that dequeues elements in a random order.", "Args": {"capacity": "An integer. The upper bound on the number of elements\nthat may be stored in this queue.", "min_after_dequeue": "An integer (described above).", "dtypes": "A list of DType objects. The length of dtypes must equal\nthe number of tensors in each queue element.", "shapes": "(Optional.) A list of fully-defined TensorShape objects\nwith the same length as dtypes, or None.", "names": "(Optional.) A list of string naming the components in the queue\nwith the same length as dtypes, or None.  If specified the dequeue\nmethods return a dictionary with the names as keys.", "seed": "A Python integer. Used to create a random seed. See\ntf.compat.v1.set_random_seed\nfor behavior.", "shared_name": "(Optional.) If non-empty, this queue will be shared under\nthe given name across multiple sessions.", "name": "Optional name for the queue operation."}, "Attributes": {"dtypes": "The list of dtypes for each component of a queue element.", "name": "The name of the underlying queue.", "names": "The list of names for each component of a queue element.", "queue_ref": "The underlying queue reference.", "shapes": "The list of shapes for each component of a queue element."}}}, "tf.ragged": {"tf.ragged.boolean_mask": {"description": "Applies a boolean mask to data without flattening the mask dimensions.", "Args": {"data": "A potentially ragged tensor.", "mask": "A potentially ragged boolean tensor.  mask's shape must be a prefix\nof data's shape.  rank(mask) must be known statically.", "name": "A name prefix for the returned tensor (optional)."}, "Returns": "A potentially ragged tensor that is formed by retaining the elements in\ndata where the corresponding value in mask is True.\n\nrank(output) = rank(data).\noutput.ragged_rank = max(data.ragged_rank, rank(mask) - 1).", "Raises": {"ValueError": "if rank(mask) is not known statically; or if mask.shape is\nnot a prefix of data.shape."}}, "tf.ragged.constant": {"description": "Constructs a constant RaggedTensor from a nested Python list.", "Args": {"pylist": "A nested list, tuple or np.ndarray.  Any nested element that\nis not a list, tuple or np.ndarray must be a scalar value\ncompatible with dtype.", "dtype": "The type of elements for the returned RaggedTensor.  If not\nspecified, then a default is chosen based on the scalar values in\npylist.", "ragged_rank": "An integer specifying the ragged rank of the returned\nRaggedTensor.  Must be nonnegative and less than K. Defaults to\nmax(0, K - 1) if inner_shape is not specified.  Defaults to\nmax(0, K - 1 - len(inner_shape)) if inner_shape is specified.", "inner_shape": "A tuple of integers specifying the shape for individual inner\nvalues in the returned RaggedTensor.  Defaults to () if ragged_rank\nis not specified.  If ragged_rank is specified, then a default is chosen\nbased on the contents of pylist.", "name": "A name prefix for the returned tensor (optional).", "row_splits_dtype": "data type for the constructed RaggedTensor's row_splits.\nOne of tf.int32 or tf.int64."}, "Returns": "A potentially ragged tensor with rank K and the specified ragged_rank,\ncontaining the values from pylist.", "Raises": {"ValueError": "If the scalar values in pylist have inconsistent nesting\ndepth; or if ragged_rank or inner_shape are incompatible with pylist."}}, "tf.ragged.cross": {"description": "Generates feature cross from a list of tensors.", "Args": {"inputs": "A list of RaggedTensor or Tensor or SparseTensor.", "name": "Optional name for the op."}, "Returns": "A 2D RaggedTensor of type string."}, "tf.ragged.cross_hashed": {"description": "Generates hashed feature cross from a list of tensors.", "Args": {"inputs": "A list of RaggedTensor or Tensor or SparseTensor.", "num_buckets": "A non-negative int that used to bucket the hashed values. If\nnum_buckets != 0, then output = hashed_value % num_buckets.", "hash_key": "Integer hash_key that will be used by the FingerprintCat64\nfunction. If not given, a default key is used.", "name": "Optional name for the op."}, "Returns": "A 2D RaggedTensor of type int64."}, "tf.ragged.map_flat_values": {"description": "Applies op to the flat_values of one or more RaggedTensors.", "Args": {"op": "The operation that should be applied to the RaggedTensor flat_values.\nop is typically an element-wise operation (such as math_ops.add), but\nany operation that preserves the size of the outermost dimension can be\nused.  I.e., shape[0] of the value returned by op must match\nshape[0] of the RaggedTensors' flat_values tensors.", "*args": "Arguments for op.", "**kwargs": "Keyword arguments for op."}, "Returns": "A RaggedTensor whose ragged_rank matches the ragged_rank of all\ninput RaggedTensors.", "Raises": {"ValueError": "If args contains no RaggedTensors, or if the nested_splits\nof the input RaggedTensors are not identical."}}, "tf.ragged.range": {"description": "Returns a RaggedTensor containing the specified sequences of numbers.", "Args": {"starts": "Vector or scalar Tensor.  Specifies the first entry for each range\nif limits is not None; otherwise, specifies the range limits, and the\nfirst entries default to 0.", "limits": "Vector or scalar Tensor.  Specifies the exclusive upper limits for\neach range.", "deltas": "Vector or scalar Tensor.  Specifies the increment for each range.\nDefaults to 1.", "dtype": "The type of the elements of the resulting tensor.  If not specified,\nthen a value is chosen based on the other args.", "name": "A name for the operation.", "row_splits_dtype": "dtype for the returned RaggedTensor's row_splits\ntensor.  One of tf.int32 or tf.int64."}, "Returns": "A RaggedTensor of type dtype with ragged_rank=1."}, "tf.ragged.row_splits_to_segment_ids": {"description": "Generates the segmentation corresponding to a RaggedTensor row_splits.", "Args": {"splits": "A sorted 1-D integer Tensor.  splits[0] must be zero.", "name": "A name prefix for the returned tensor (optional).", "out_type": "The dtype for the return value.  Defaults to splits.dtype,\nor tf.int64 if splits does not have a dtype."}, "Returns": "A sorted 1-D integer Tensor, with shape=[splits[-1]]", "Raises": {"ValueError": "If splits is invalid."}}, "tf.ragged.segment_ids_to_row_splits": {"description": "Generates the RaggedTensor row_splits corresponding to a segmentation.", "Args": {"segment_ids": "A 1-D integer Tensor.", "num_segments": "A scalar integer indicating the number of segments.  Defaults\nto max(segment_ids) + 1 (or zero if segment_ids is empty).", "out_type": "The dtype for the return value.  Defaults to segment_ids.dtype,\nor tf.int64 if segment_ids does not have a dtype.", "name": "A name prefix for the returned tensor (optional)."}, "Returns": "A sorted 1-D integer Tensor, with shape=[num_segments + 1]."}, "tf.ragged.stack": {"description": "Stacks a list of rank-R tensors into one rank-(R&#43;1) RaggedTensor.", "Args": {"values": "A list of tf.Tensor or tf.RaggedTensor.  May not be empty. All\nvalues must have the same rank and the same dtype; but unlike\ntf.stack, they can have arbitrary dimension sizes.", "axis": "A python integer, indicating the dimension along which to stack.\n(Note: Unlike tf.stack, the axis parameter must be statically known.)\nNegative values are supported only if the rank of at least one\nvalues value is statically known.", "name": "A name prefix for the returned tensor (optional)."}, "Returns": "A RaggedTensor with rank R+1 (if R>0).\nIf R==0, then the result will be returned as a 1D Tensor, since\nRaggedTensor can only be used when rank>1.\nresult.ragged_rank=1+max(axis, max(rt.ragged_rank for rt in values])).", "Raises": {"ValueError": "If values is empty, if axis is out of bounds or if\nthe input tensors have different ranks."}}, "tf.ragged.stack_dynamic_partitions": {"description": "Stacks dynamic partitions of a Tensor or RaggedTensor.", "Args": {"data": "A Tensor or RaggedTensor containing the values to stack.", "partitions": "An int32 or int64 Tensor or RaggedTensor specifying the\npartition that each slice of data should be added to. partitions.shape\nmust be a prefix of data.shape.  Values must be greater than or equal to\nzero, and less than num_partitions. partitions is not required to be\nsorted.", "num_partitions": "An int32 or int64 scalar specifying the number of\npartitions to output.  This determines the number of rows in output.", "name": "A name prefix for the returned tensor (optional)."}, "Returns": "A RaggedTensor containing the stacked partitions.  The returned tensor\nhas the same dtype as data, and its shape is\n[num_partitions, (D)] + data.shape[partitions.rank:], where (D) is a\nragged dimension whose length is the number of data slices stacked for\neach partition."}}, "tf.random": {"tf.random.Algorithm": {"description": "An enumeration."}, "tf.random.Generator": {"description": "Random-number generator.", "Args": {"copy_from": "a generator to be copied from.", "state": "a vector of dtype STATE_TYPE representing the initial state of the\nRNG, whose length and semantics are algorithm-specific. If it's a\nvariable, the generator will reuse it instead of creating a new\nvariable.", "alg": "the RNG algorithm. Possible values are\ntf.random.Algorithm.PHILOX for the Philox algorithm and\ntf.random.Algorithm.THREEFRY for the ThreeFry algorithm\n(see paper 'Parallel Random Numbers: As Easy as 1, 2, 3'\n[https://www.thesalmons.org/john/random123/papers/random123sc11.pdf]).\nThe string names \"philox\" and \"threefry\" can also be used.\nNote PHILOX guarantees the same numbers are produced (given\nthe same random state) across all architectures (CPU, GPU, XLA etc)."}, "Attributes": {"algorithm": "The RNG algorithm id (a Python integer or scalar integer Tensor).", "key": "The 'key' part of the state of a counter-based RNG.\nFor a counter-base RNG algorithm such as Philox and ThreeFry (as\ndescribed in paper 'Parallel Random Numbers: As Easy as 1, 2, 3'\n[https://www.thesalmons.org/john/random123/papers/random123sc11.pdf]),\nthe RNG state consists of two parts: counter and key. The output is\ngenerated via the formula: output=hash(key, counter), i.e. a hashing of\nthe counter parametrized by the key. Two RNGs with two different keys can\nbe thought as generating two independent random-number streams (a stream\nis formed by increasing the counter).", "state": "The internal state of the RNG."}}, "tf.random.all_candidate_sampler": {"description": "Generate the set of all classes.", "Args": {"true_classes": "A Tensor of type int64 and shape [batch_size,\nnum_true]. The target classes.", "num_true": "An int.  The number of target classes per training example.", "num_sampled": "An int.  The number of possible classes.", "unique": "A bool. Ignored.\nunique.", "seed": "An int. An operation-specific seed. Default is 0.", "name": "A name for the operation (optional)."}, "Returns": "sampled_candidates\n\n\nA tensor of type int64 and shape [num_sampled].\nThis operation deterministically returns the entire range\n[0, num_sampled]."}, "tf.random.categorical": {"description": "Draws samples from a categorical distribution.", "Args": {"logits": "2-D Tensor with shape [batch_size, num_classes].  Each slice\n[i, :] represents the unnormalized log-probabilities for all classes.", "num_samples": "0-D.  Number of independent samples to draw for each row slice.", "dtype": "The integer type of the output: int32 or int64. Defaults to\nint64.", "seed": "A Python integer. Used to create a random seed for the distribution.\nSee tf.random.set_seed for behavior.", "name": "Optional name for the operation."}, "Returns": "The drawn samples of shape [batch_size, num_samples]."}, "tf.random.create_rng_state": {"description": "Creates a RNG state from an integer or a vector.", "Args": {"seed": "an integer or 1-D numpy array.", "alg": "the RNG algorithm. Can be a string, an Algorithm or an integer."}, "Returns": "a 1-D numpy array whose size depends on the algorithm."}, "tf.random.fixed_unigram_candidate_sampler": {"description": "Samples a set of classes using the provided (fixed) base distribution.", "Args": {"true_classes": "A Tensor of type int64 and shape [batch_size,\nnum_true]. The target classes.", "num_true": "An int.  The number of target classes per training example.", "num_sampled": "An int.  The number of classes to randomly sample.", "unique": "A bool. Determines whether all sampled classes in a batch are\nunique.", "range_max": "An int. The number of possible classes.", "vocab_file": "Each valid line in this file (which should have a CSV-like\nformat) corresponds to a valid word ID. IDs are in sequential order,\nstarting from num_reserved_ids. The last entry in each line is expected\nto be a value corresponding to the count or relative probability. Exactly\none of vocab_file and unigrams needs to be passed to this operation.", "distortion": "The distortion is used to skew the unigram probability\ndistribution.  Each weight is first raised to the distortion's power\nbefore adding to the internal unigram distribution. As a result,\ndistortion = 1.0 gives regular unigram sampling (as defined by the vocab\nfile), and distortion = 0.0 gives a uniform distribution.", "num_reserved_ids": "Optionally some reserved IDs can be added in the range\n[0, num_reserved_ids) by the users. One use case is that a special\nunknown word token is used as ID 0. These IDs will have a sampling\nprobability of 0.", "num_shards": "A sampler can be used to sample from a subset of the original\nrange in order to speed up the whole computation through parallelism. This\nparameter (together with shard) indicates the number of partitions that\nare being used in the overall computation.", "shard": "A sampler can be used to sample from a subset of the original range\nin order to speed up the whole computation through parallelism. This\nparameter (together with num_shards) indicates the particular partition\nnumber of the operation, when partitioning is being used.", "unigrams": "A list of unigram counts or probabilities, one per ID in\nsequential order. Exactly one of vocab_file and unigrams should be\npassed to this operation.", "seed": "An int. An operation-specific seed. Default is 0.", "name": "A name for the operation (optional)."}, "Returns": "sampled_candidates\n\n\nA tensor of type int64 and shape [num_sampled].\nThe sampled classes."}, "tf.random.gamma": {"description": "Draws shape samples from each of the given Gamma distribution(s).", "Args": {"shape": "A 1-D integer Tensor or Python array. The shape of the output samples\nto be drawn per alpha/beta-parameterized distribution.", "alpha": "A Tensor or Python value or N-D array of type dtype. alpha\nprovides the shape parameter(s) describing the gamma distribution(s) to\nsample. Must be broadcastable with beta.", "beta": "A Tensor or Python value or N-D array of type dtype. Defaults to 1.\nbeta provides the inverse scale parameter(s) of the gamma\ndistribution(s) to sample. Must be broadcastable with alpha.", "dtype": "The type of alpha, beta, and the output: float16, float32, or\nfloat64.", "seed": "A Python integer. Used to create a random seed for the distributions.\nSee\ntf.random.set_seed\nfor behavior.", "name": "Optional name for the operation."}, "Returns": "samples\n\n\na Tensor of shape\ntf.concat([shape, tf.shape(alpha + beta)], axis=0) with values of type\ndtype."}, "tf.random.get_global_generator": {"description": "Retrieves the global generator.", "Returns": "The global tf.random.Generator object."}, "tf.random.learned_unigram_candidate_sampler": {"description": "Samples a set of classes from a distribution learned during training.", "Args": {"true_classes": "A Tensor of type int64 and shape [batch_size,\nnum_true]. The target classes.", "num_true": "An int.  The number of target classes per training example.", "num_sampled": "An int.  The number of classes to randomly sample.", "unique": "A bool. Determines whether all sampled classes in a batch are\nunique.", "range_max": "An int. The number of possible classes.", "seed": "An int. An operation-specific seed. Default is 0.", "name": "A name for the operation (optional)."}, "Returns": "sampled_candidates\n\n\nA tensor of type int64 and shape [num_sampled].\nThe sampled classes."}, "tf.random.log_uniform_candidate_sampler": {"description": "Samples a set of classes using a log-uniform (Zipfian) base distribution.", "Args": {"true_classes": "A Tensor of type int64 and shape [batch_size,\nnum_true]. The target classes.", "num_true": "An int.  The number of target classes per training example.", "num_sampled": "An int.  The number of classes to randomly sample.", "unique": "A bool. Determines whether all sampled classes in a batch are\nunique.", "range_max": "An int. The number of possible classes.", "seed": "An int. An operation-specific seed. Default is 0.", "name": "A name for the operation (optional)."}, "Returns": "sampled_candidates\n\n\nA tensor of type int64 and shape [num_sampled].\nThe sampled classes."}, "tf.random.normal": {"description": "Outputs random values from a normal distribution.", "Args": {"shape": "A 1-D integer Tensor or Python array. The shape of the output tensor.", "mean": "A Tensor or Python value of type dtype, broadcastable with stddev.\nThe mean of the normal distribution.", "stddev": "A Tensor or Python value of type dtype, broadcastable with mean.\nThe standard deviation of the normal distribution.", "dtype": "The float type of the output: float16, bfloat16, float32,\nfloat64. Defaults to float32.", "seed": "A Python integer. Used to create a random seed for the distribution.\nSee\ntf.random.set_seed\nfor behavior.", "name": "A name for the operation (optional)."}, "Returns": "A tensor of the specified shape filled with random normal values."}, "tf.random.poisson": {"description": "Draws shape samples from each of the given Poisson distribution(s).", "Args": {"shape": "A 1-D integer Tensor or Python array. The shape of the output samples\nto be drawn per \"rate\"-parameterized distribution.", "lam": "A Tensor or Python value or N-D array of type dtype.\nlam provides the rate parameter(s) describing the poisson\ndistribution(s) to sample.", "dtype": "The type of the output: float16, float32, float64, int32 or\nint64.", "seed": "A Python integer. Used to create a random seed for the distributions.\nSee\ntf.random.set_seed\nfor behavior.", "name": "Optional name for the operation."}, "Returns": "samples\n\n\na Tensor of shape tf.concat([shape, tf.shape(lam)], axis=0)\nwith values of type dtype."}, "tf.random.set_global_generator": {"description": "Replaces the global generator with another Generator object.", "Args": {"generator": "the new Generator object."}}, "tf.random.set_seed": {"description": "Sets the global random seed.", "Args": {"seed": "integer."}}, "tf.random.shuffle": {"description": "Randomly shuffles a tensor along its first dimension.", "Args": {"value": "A Tensor to be shuffled.", "seed": "A Python integer. Used to create a random seed for the distribution.\nSee\ntf.random.set_seed\nfor behavior.", "name": "A name for the operation (optional)."}, "Returns": "A tensor of same shape and type as value, shuffled along its first\ndimension."}, "tf.random.stateless_binomial": {"description": "Outputs deterministic pseudorandom values from a binomial distribution.", "Args": {"shape": "A 1-D integer Tensor or Python array. The shape of the output tensor.", "seed": "A shape [2] Tensor, the seed to the random number generator. Must have\ndtype int32 or int64. (When using XLA, only int32 is allowed.)", "counts": "Tensor. The counts of the binomial distribution. Must be\nbroadcastable with probs, and broadcastable with the rightmost\ndimensions of shape.", "probs": "Tensor. The probability of success for the binomial distribution.\nMust be broadcastable with counts and broadcastable with the rightmost\ndimensions of shape.", "output_dtype": "The type of the output. Default: tf.int32", "name": "A name for the operation (optional)."}, "Returns": "samples\n\n\nA Tensor of the specified shape filled with random binomial\nvalues.  For each i, each samples[..., i] is an independent draw from\nthe binomial distribution on counts[i] trials with probability of\nsuccess probs[i]."}, "tf.random.stateless_categorical": {"description": "Draws deterministic pseudorandom samples from a categorical distribution.", "Args": {"logits": "2-D Tensor with shape [batch_size, num_classes].  Each slice\n[i, :] represents the unnormalized log-probabilities for all classes.", "num_samples": "0-D.  Number of independent samples to draw for each row slice.", "seed": "A shape [2] Tensor, the seed to the random number generator. Must have\ndtype int32 or int64. (When using XLA, only int32 is allowed.)", "dtype": "The integer type of the output: int32 or int64. Defaults to\nint64.", "name": "Optional name for the operation."}, "Returns": "The drawn samples of shape [batch_size, num_samples]."}, "tf.random.stateless_gamma": {"description": "Outputs deterministic pseudorandom values from a gamma distribution.", "Args": {"shape": "A 1-D integer Tensor or Python array. The shape of the output tensor.", "seed": "A shape [2] Tensor, the seed to the random number generator. Must have\ndtype int32 or int64. (When using XLA, only int32 is allowed.)", "alpha": "Tensor. The concentration parameter of the gamma distribution. Must\nbe broadcastable with beta, and broadcastable with the rightmost\ndimensions of shape.", "beta": "Tensor. The inverse scale parameter of the gamma distribution. Must be\nbroadcastable with alpha and broadcastable with the rightmost dimensions\nof shape.", "dtype": "Floating point dtype of alpha, beta, and the output.", "name": "A name for the operation (optional)."}, "Returns": "samples\n\n\nA Tensor of the specified shape filled with random gamma values.\nFor each i, each `samples[..., i] is an independent draw from the gamma\ndistribution with concentration alpha[i] and scale beta[i]."}, "tf.random.stateless_normal": {"description": "Outputs deterministic pseudorandom values from a normal distribution.", "Args": {"shape": "A 1-D integer Tensor or Python array. The shape of the output tensor.", "seed": "A shape [2] Tensor, the seed to the random number generator. Must have\ndtype int32 or int64. (When using XLA, only int32 is allowed.)", "mean": "A 0-D Tensor or Python value of type dtype. The mean of the normal\ndistribution.", "stddev": "A 0-D Tensor or Python value of type dtype. The standard deviation\nof the normal distribution.", "dtype": "The float type of the output: float16, bfloat16, float32,\nfloat64. Defaults to float32.", "name": "A name for the operation (optional).", "alg": "The RNG algorithm used to generate the random numbers. See\ntf.random.stateless_uniform for a detailed explanation."}, "Returns": "A tensor of the specified shape filled with random normal values."}, "tf.random.stateless_parameterized_truncated_normal": {"description": "Outputs random values from a truncated normal distribution.", "Args": {"shape": "A 1-D integer Tensor or Python array. The shape of the output\ntensor.", "seed": "A shape [2] Tensor, the seed to the random number generator. Must have\ndtype int32 or int64. (When using XLA, only int32 is allowed.)", "means": "A Tensor or Python value of type dtype. The mean of the truncated\nnormal distribution. This must broadcast with stddevs, minvals and\nmaxvals, and the broadcasted shape must be dominated by shape.", "stddevs": "A Tensor or Python value of type dtype. The standard deviation\nof the truncated normal distribution. This must broadcast with means,\nminvals and maxvals, and the broadcasted shape must be dominated by\nshape.", "minvals": "A Tensor or Python value of type dtype. The minimum value of\nthe truncated normal distribution. This must broadcast with means,\nstddevs and maxvals, and the broadcasted shape must be dominated by\nshape.", "maxvals": "A Tensor or Python value of type dtype. The maximum value of\nthe truncated normal distribution. This must broadcast with means,\nstddevs and minvals, and the broadcasted shape must be dominated by\nshape.", "name": "A name for the operation (optional)."}, "Returns": "A tensor of the specified shape filled with random truncated normal values."}, "tf.random.stateless_poisson": {"description": "Outputs deterministic pseudorandom values from a Poisson distribution.", "Args": {"shape": "A 1-D integer Tensor or Python array. The shape of the output tensor.", "seed": "A shape [2] Tensor, the seed to the random number generator. Must have\ndtype int32 or int64. (When using XLA, only int32 is allowed.)", "lam": "Tensor. The rate parameter \"lambda\" of the Poisson distribution. Shape\nmust match the rightmost dimensions of shape.", "dtype": "Dtype of the samples (int or float dtypes are permissible, as samples\nare discrete). Default: int32.", "name": "A name for the operation (optional)."}, "Returns": "samples\n\n\nA Tensor of the specified shape filled with random Poisson values.\nFor each i, each samples[..., i] is an independent draw from the Poisson\ndistribution with rate lam[i]."}, "tf.random.stateless_truncated_normal": {"description": "Outputs deterministic pseudorandom values, truncated normally distributed.", "Args": {"shape": "A 1-D integer Tensor or Python array. The shape of the output tensor.", "seed": "A shape [2] Tensor, the seed to the random number generator. Must have\ndtype int32 or int64. (When using XLA, only int32 is allowed.)", "mean": "A 0-D Tensor or Python value of type dtype. The mean of the\ntruncated normal distribution.", "stddev": "A 0-D Tensor or Python value of type dtype. The standard deviation\nof the normal distribution, before truncation.", "dtype": "The type of the output.", "name": "A name for the operation (optional).", "alg": "The RNG algorithm used to generate the random numbers. See\ntf.random.stateless_uniform for a detailed explanation."}, "Returns": "A tensor of the specified shape filled with random truncated normal values."}, "tf.random.stateless_uniform": {"description": "Outputs deterministic pseudorandom values from a uniform distribution.", "Args": {"shape": "A 1-D integer Tensor or Python array. The shape of the output tensor.", "seed": "A shape [2] Tensor, the seed to the random number generator. Must have\ndtype int32 or int64. (When using XLA, only int32 is allowed.)", "minval": "A Tensor or Python value of type dtype, broadcastable with\nshape (for integer types, broadcasting is not supported, so it needs to\nbe a scalar). The lower bound on the range of random values to\ngenerate. Pass None for full-range integers.  Defaults to 0.", "maxval": "A Tensor or Python value of type dtype, broadcastable with\nshape (for integer types, broadcasting is not supported, so it needs to\nbe a scalar). The upper bound on the range of random values to generate.\nDefaults to 1 if dtype is floating point. Pass None for full-range\nintegers.", "dtype": "The type of the output: float16, bfloat16, float32, float64,\nint32, or int64. For unbounded uniform ints (minval, maxval both\nNone), uint32 and uint64 may be used. Defaults to float32.", "name": "A name for the operation (optional).", "alg": "The RNG algorithm used to generate the random numbers. Valid\nchoices are \"philox\" for the Philox\nalgorithm,\n\"threefry\" for the ThreeFry\nalgorithm,\nand \"auto_select\" (default) for the system to automatically\nselect an algorithm based the device type. Values of\ntf.random.Algorithm can also be used. Note that with\n\"auto_select\", the outputs of this function may change when\nit is running on a different device."}, "Returns": "A tensor of the specified shape filled with random uniform values.", "Raises": {"ValueError": "If dtype is integral and only one of minval or maxval is\nspecified."}}, "tf.random.truncated_normal": {"description": "Outputs random values from a truncated normal distribution.", "Args": {"shape": "A 1-D integer Tensor or Python array. The shape of the output tensor.", "mean": "A 0-D Tensor or Python value of type dtype. The mean of the\ntruncated normal distribution.", "stddev": "A 0-D Tensor or Python value of type dtype. The standard deviation\nof the normal distribution, before truncation.", "dtype": "The type of the output. Restricted to floating-point types:\ntf.half, tf.float, tf.double, etc.", "seed": "A Python integer. Used to create a random seed for the distribution.\nSee tf.random.set_seed for more information.", "name": "A name for the operation (optional)."}, "Returns": "A tensor of the specified shape filled with random truncated normal values."}, "tf.random.uniform": {"description": "Outputs random values from a uniform distribution.", "Args": {"shape": "A 1-D integer Tensor or Python array. The shape of the output tensor.", "minval": "A Tensor or Python value of type dtype, broadcastable with\nshape (for integer types, broadcasting is not supported, so it needs to\nbe a scalar). The lower bound on the range of random values to generate\n(inclusive).  Defaults to 0.", "maxval": "A Tensor or Python value of type dtype, broadcastable with\nshape (for integer types, broadcasting is not supported, so it needs to\nbe a scalar). The upper bound on the range of random values to generate\n(exclusive). Defaults to 1 if dtype is floating point.", "dtype": "The type of the output: float16, bfloat16, float32, float64,\nint32, or int64. Defaults to float32.", "seed": "A Python integer. Used in combination with tf.random.set_seed to\ncreate a reproducible sequence of tensors across multiple calls.", "name": "A name for the operation (optional)."}, "Returns": "A tensor of the specified shape filled with random uniform values.", "Raises": {"ValueError": "If dtype is integral and maxval is not specified."}}, "tf.random.uniform_candidate_sampler": {"description": "Samples a set of classes using a uniform base distribution.", "Args": {"true_classes": "A Tensor of type int64 and shape [batch_size,\nnum_true]. The target classes.", "num_true": "An int.  The number of target classes per training example.", "num_sampled": "An int.  The number of classes to randomly sample. The\nsampled_candidates return value will have shape [num_sampled]. If\nunique=True, num_sampled must be less than or equal to range_max.", "unique": "A bool. Determines whether all sampled classes in a batch are\nunique.", "range_max": "An int. The number of possible classes.", "seed": "An int. An operation-specific seed. Default is 0.", "name": "A name for the operation (optional)."}, "Returns": "sampled_candidates\n\n\nA tensor of type int64 and shape [num_sampled].  The\nsampled classes, either with possible duplicates (unique=False) or all\nunique (unique=True). In either case, sampled_candidates is\nindependent of the true classes."}}, "tf.raw_ops": {"tf.raw_ops.Abort": {"description": "Raise a exception to abort the process when called.", "Args": {"error_msg": "An optional string. Defaults to \"\".\nA string which is the message associated with the exception.", "exit_without_error": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.Abs": {"description": "Computes the absolute value of a tensor.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, int8, int16, int32, int64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.AccumulateNV2": {"description": "Returns the element-wise sum of a list of tensors.", "Args": {"inputs": "A list of at least 1 Tensor objects with the same type in: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nA list of Tensor objects, each with same shape and type.", "shape": "A tf.TensorShape or list of ints.\nShape of elements of inputs.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as inputs."}, "tf.raw_ops.AccumulatorApplyGradient": {"description": "Applies a gradient to a given accumulator.", "Args": {"handle": "A Tensor of type mutable string. The handle to a accumulator.", "local_step": "A Tensor of type int64.\nThe local_step value at which the gradient was computed.", "gradient": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nA tensor of the gradient to be accumulated.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.AccumulatorNumAccumulated": {"description": "Returns the number of gradients aggregated in the given accumulators.", "Args": {"handle": "A Tensor of type mutable string. The handle to an accumulator.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int32."}, "tf.raw_ops.AccumulatorSetGlobalStep": {"description": "Updates the accumulator with a new value for global_step.", "Args": {"handle": "A Tensor of type mutable string. The handle to an accumulator.", "new_global_step": "A Tensor of type int64.\nThe new global_step value to set.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.AccumulatorTakeGradient": {"description": "Extracts the average gradient in the given ConditionalAccumulator.", "Args": {"handle": "A Tensor of type mutable string. The handle to an accumulator.", "num_required": "A Tensor of type int32.\nNumber of gradients required before we return an aggregate.", "dtype": "A tf.DType from: tf.float32, tf.float64, tf.int32, tf.uint8, tf.int16, tf.int8, tf.complex64, tf.int64, tf.qint8, tf.quint8, tf.qint32, tf.bfloat16, tf.uint16, tf.complex128, tf.half, tf.uint32, tf.uint64.\nThe data type of accumulated gradients. Needs to correspond to the type\nof the accumulator.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dtype."}, "tf.raw_ops.Acos": {"description": "Computes acos of x element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, int8, int16, int32, int64, complex64, complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.Acosh": {"description": "Computes inverse hyperbolic cosine of x element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, complex64, complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.Add": {"description": "Returns x &#43; y element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, uint8, int8, int16, int32, int64, complex64, complex128, string.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.AddManySparseToTensorsMap": {"description": "Add an N-minibatch SparseTensor to a SparseTensorsMap, return N handles.", "Args": {"sparse_indices": "A Tensor of type int64.\n2-D.  The indices of the minibatch SparseTensor.\nsparse_indices[:, 0] must be ordered values in [0, N).", "sparse_values": "A Tensor.\n1-D.  The values of the minibatch SparseTensor.", "sparse_shape": "A Tensor of type int64.\n1-D.  The shape of the minibatch SparseTensor.\nThe minibatch size N == sparse_shape[0].", "container": "An optional string. Defaults to \"\".\nThe container name for the SparseTensorsMap created by this op.", "shared_name": "An optional string. Defaults to \"\".\nThe shared name for the SparseTensorsMap created by this op.\nIf blank, the new Operation's unique name is used.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int64."}, "tf.raw_ops.AddN": {"description": "Add all input tensors element wise.", "Args": {"inputs": "A list of at least 1 Tensor objects with the same type in: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64, variant.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as inputs."}, "tf.raw_ops.AddSparseToTensorsMap": {"description": "Add a SparseTensor to a SparseTensorsMap return its handle.", "Args": {"sparse_indices": "A Tensor of type int64.\n2-D.  The indices of the SparseTensor.", "sparse_values": "A Tensor. 1-D.  The values of the SparseTensor.", "sparse_shape": "A Tensor of type int64.\n1-D.  The shape of the SparseTensor.", "container": "An optional string. Defaults to \"\".\nThe container name for the SparseTensorsMap created by this op.", "shared_name": "An optional string. Defaults to \"\".\nThe shared name for the SparseTensorsMap created by this op.\nIf blank, the new Operation's unique name is used.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int64."}, "tf.raw_ops.AddV2": {"description": "Returns x &#43; y element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, uint8, uint16, uint32, uint64, int8, int16, int32, int64, complex64, complex128.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.AdjustContrast": {"description": "Deprecated. Disallowed in GraphDef version &gt;= 2.", "Args": {"images": "A Tensor. Must be one of the following types: uint8, int8, int16, int32, int64, float32, float64.", "contrast_factor": "A Tensor of type float32.", "min_value": "A Tensor of type float32.", "max_value": "A Tensor of type float32.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type float32."}, "tf.raw_ops.AdjustContrastv2": {"description": "Adjust the contrast of one or more images.", "Args": {"images": "A Tensor. Must be one of the following types: half, float32.\nImages to adjust.  At least 3-D.", "contrast_factor": "A Tensor of type float32.\nA float multiplier for adjusting contrast.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as images."}, "tf.raw_ops.AdjustHue": {"description": "Adjust the hue of one or more images.", "Args": {"images": "A Tensor. Must be one of the following types: half, float32.\nImages to adjust.  At least 3-D.", "delta": "A Tensor of type float32. A float delta to add to the hue.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as images."}, "tf.raw_ops.AdjustSaturation": {"description": "Adjust the saturation of one or more images.", "Args": {"images": "A Tensor. Must be one of the following types: half, float32.\nImages to adjust.  At least 3-D.", "scale": "A Tensor of type float32.\nA float scale to add to the saturation.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as images."}, "tf.raw_ops.All": {"description": "Computes the &#34;logical and&#34; of elements across dimensions of a tensor.", "Args": {"input": "A Tensor of type bool. The tensor to reduce.", "axis": "A Tensor. Must be one of the following types: int32, int64.\nThe dimensions to reduce. Must be in the range\n[-rank(input), rank(input)).", "keep_dims": "An optional bool. Defaults to False.\nIf true, retain reduced dimensions with length 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type bool."}, "tf.raw_ops.AllCandidateSampler": {"description": "Generates labels for candidate sampling with a learned unigram distribution.", "Args": {"true_classes": "A Tensor of type int64.\nA batch_size * num_true matrix, in which each row contains the\nIDs of the num_true target_classes in the corresponding original label.", "num_true": "An int that is >= 1. Number of true labels per context.", "num_sampled": "An int that is >= 1. Number of candidates to produce.", "unique": "A bool.\nIf unique is true, we sample with rejection, so that all sampled\ncandidates in a batch are unique. This requires some approximation to\nestimate the post-rejection sampling probabilities.", "seed": "An optional int. Defaults to 0.\nIf either seed or seed2 are set to be non-zero, the random number\ngenerator is seeded by the given seed.  Otherwise, it is seeded by a\nrandom seed.", "seed2": "An optional int. Defaults to 0.\nAn second seed to avoid seed collision.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (sampled_candidates, true_expected_count, sampled_expected_count)."}, "tf.raw_ops.AllToAll": {"description": "An Op to exchange data across TPU replicas.", "Args": {"input": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64, bool.\nThe local input to the sum.", "group_assignment": "A Tensor of type int32. An int32 tensor with shape\n[num_groups, num_replicas_per_group]. group_assignment[i] represents the\nreplica ids in the ith subgroup.", "concat_dimension": "An int. The dimension number to concatenate.", "split_dimension": "An int. The dimension number to split.", "split_count": "An int.\nThe number of splits, this number must equal to the sub-group\nsize(group_assignment.get_shape()[1])", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.Angle": {"description": "Returns the argument of a complex number.", "Args": {"input": "A Tensor. Must be one of the following types: complex64, complex128.", "Tout": "An optional tf.DType from: tf.float32, tf.float64. Defaults to tf.float32.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type Tout."}, "tf.raw_ops.AnonymousHashTable": {"description": "Creates a uninitialized anonymous hash table.", "Args": {"key_dtype": "A tf.DType. Type of the table keys.", "value_dtype": "A tf.DType. Type of the table values.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type resource."}, "tf.raw_ops.AnonymousIterator": {"description": "A container for an iterator resource.", "Args": {"output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type resource."}, "tf.raw_ops.AnonymousIteratorV2": {"description": "A container for an iterator resource.", "Args": {"output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (handle, deleter)."}, "tf.raw_ops.AnonymousIteratorV3": {"description": "A container for an iterator resource.", "Args": {"output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type resource."}, "tf.raw_ops.AnonymousMemoryCache": {"Args": {"name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (handle, deleter)."}, "tf.raw_ops.AnonymousMultiDeviceIterator": {"description": "A container for a multi device iterator resource.", "Args": {"devices": "A list of strings that has length >= 1.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (handle, deleter)."}, "tf.raw_ops.AnonymousMultiDeviceIteratorV3": {"description": "A container for a multi device iterator resource.", "Args": {"devices": "A list of strings that has length >= 1.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type resource."}, "tf.raw_ops.AnonymousMutableDenseHashTable": {"description": "Creates an empty anonymous mutable hash table that uses tensors as the backing store.", "Args": {"empty_key": "A Tensor.\nThe key used to represent empty key buckets internally. Must not\nbe used in insert or lookup operations.", "deleted_key": "A Tensor. Must have the same type as empty_key.", "value_dtype": "A tf.DType. Type of the table values.", "value_shape": "An optional tf.TensorShape or list of ints. Defaults to [].\nThe shape of each value.", "initial_num_buckets": "An optional int. Defaults to 131072.\nThe initial number of hash table buckets. Must be a power\nto 2.", "max_load_factor": "An optional float. Defaults to 0.8.\nThe maximum ratio between number of entries and number of\nbuckets before growing the table. Must be between 0 and 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type resource."}, "tf.raw_ops.AnonymousMutableHashTable": {"description": "Creates an empty anonymous mutable hash table.", "Args": {"key_dtype": "A tf.DType. Type of the table keys.", "value_dtype": "A tf.DType. Type of the table values.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type resource."}, "tf.raw_ops.AnonymousMutableHashTableOfTensors": {"description": "Creates an empty anonymous mutable hash table of vector values.", "Args": {"key_dtype": "A tf.DType. Type of the table keys.", "value_dtype": "A tf.DType. Type of the table values.", "value_shape": "An optional tf.TensorShape or list of ints. Defaults to [].", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type resource."}, "tf.raw_ops.AnonymousRandomSeedGenerator": {"Args": {"seed": "A Tensor of type int64.", "seed2": "A Tensor of type int64.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (handle, deleter)."}, "tf.raw_ops.AnonymousSeedGenerator": {"Args": {"seed": "A Tensor of type int64.", "seed2": "A Tensor of type int64.", "reshuffle": "A Tensor of type bool.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (handle, deleter)."}, "tf.raw_ops.Any": {"description": "Computes the &#34;logical or&#34; of elements across dimensions of a tensor.", "Args": {"input": "A Tensor of type bool. The tensor to reduce.", "axis": "A Tensor. Must be one of the following types: int32, int64.\nThe dimensions to reduce. Must be in the range\n[-rank(input), rank(input)).", "keep_dims": "An optional bool. Defaults to False.\nIf true, retain reduced dimensions with length 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type bool."}, "tf.raw_ops.ApplyAdaMax": {"description": "Update &#39;*var&#39; according to the AdaMax algorithm.", "Args": {"var": "A mutable Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nShould be from a Variable().", "m": "A mutable Tensor. Must have the same type as var.\nShould be from a Variable().", "v": "A mutable Tensor. Must have the same type as var.\nShould be from a Variable().", "beta1_power": "A Tensor. Must have the same type as var.\nMust be a scalar.", "lr": "A Tensor. Must have the same type as var.\nScaling factor. Must be a scalar.", "beta1": "A Tensor. Must have the same type as var.\nMomentum factor. Must be a scalar.", "beta2": "A Tensor. Must have the same type as var.\nMomentum factor. Must be a scalar.", "epsilon": "A Tensor. Must have the same type as var.\nRidge term. Must be a scalar.", "grad": "A Tensor. Must have the same type as var. The gradient.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var, m, and v tensors will be protected\nby a lock; otherwise the behavior is undefined, but may exhibit less\ncontention.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as var."}, "tf.raw_ops.ApplyAdadelta": {"description": "Update &#39;*var&#39; according to the adadelta scheme.", "Args": {"var": "A mutable Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nShould be from a Variable().", "accum": "A mutable Tensor. Must have the same type as var.\nShould be from a Variable().", "accum_update": "A mutable Tensor. Must have the same type as var.\nShould be from a Variable().", "lr": "A Tensor. Must have the same type as var.\nScaling factor. Must be a scalar.", "rho": "A Tensor. Must have the same type as var.\nDecay factor. Must be a scalar.", "epsilon": "A Tensor. Must have the same type as var.\nConstant factor. Must be a scalar.", "grad": "A Tensor. Must have the same type as var. The gradient.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var, accum and update_accum tensors will be protected by\na lock; otherwise the behavior is undefined, but may exhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as var."}, "tf.raw_ops.ApplyAdagrad": {"description": "Update &#39;*var&#39; according to the adagrad scheme.", "Args": {"var": "A mutable Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nShould be from a Variable().", "accum": "A mutable Tensor. Must have the same type as var.\nShould be from a Variable().", "lr": "A Tensor. Must have the same type as var.\nScaling factor. Must be a scalar.", "grad": "A Tensor. Must have the same type as var. The gradient.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var and accum tensors will be protected\nby a lock; otherwise the behavior is undefined, but may exhibit less\ncontention.", "update_slots": "An optional bool. Defaults to True.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as var."}, "tf.raw_ops.ApplyAdagradDA": {"description": "Update &#39;*var&#39; according to the proximal adagrad scheme.", "Args": {"var": "A mutable Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nShould be from a Variable().", "gradient_accumulator": "A mutable Tensor. Must have the same type as var.\nShould be from a Variable().", "gradient_squared_accumulator": "A mutable Tensor. Must have the same type as var.\nShould be from a Variable().", "grad": "A Tensor. Must have the same type as var. The gradient.", "lr": "A Tensor. Must have the same type as var.\nScaling factor. Must be a scalar.", "l1": "A Tensor. Must have the same type as var.\nL1 regularization. Must be a scalar.", "l2": "A Tensor. Must have the same type as var.\nL2 regularization. Must be a scalar.", "global_step": "A Tensor of type int64.\nTraining step number. Must be a scalar.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var and accum tensors will be protected by\na lock; otherwise the behavior is undefined, but may exhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as var."}, "tf.raw_ops.ApplyAdagradV2": {"description": "Update &#39;*var&#39; according to the adagrad scheme.", "Args": {"var": "A mutable Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nShould be from a Variable().", "accum": "A mutable Tensor. Must have the same type as var.\nShould be from a Variable().", "lr": "A Tensor. Must have the same type as var.\nScaling factor. Must be a scalar.", "epsilon": "A Tensor. Must have the same type as var.\nConstant factor. Must be a scalar.", "grad": "A Tensor. Must have the same type as var. The gradient.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var and accum tensors will be protected\nby a lock; otherwise the behavior is undefined, but may exhibit less\ncontention.", "update_slots": "An optional bool. Defaults to True.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as var."}, "tf.raw_ops.ApplyAdam": {"description": "Update &#39;*var&#39; according to the Adam algorithm.", "Args": {"var": "A mutable Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nShould be from a Variable().", "m": "A mutable Tensor. Must have the same type as var.\nShould be from a Variable().", "v": "A mutable Tensor. Must have the same type as var.\nShould be from a Variable().", "beta1_power": "A Tensor. Must have the same type as var.\nMust be a scalar.", "beta2_power": "A Tensor. Must have the same type as var.\nMust be a scalar.", "lr": "A Tensor. Must have the same type as var.\nScaling factor. Must be a scalar.", "beta1": "A Tensor. Must have the same type as var.\nMomentum factor. Must be a scalar.", "beta2": "A Tensor. Must have the same type as var.\nMomentum factor. Must be a scalar.", "epsilon": "A Tensor. Must have the same type as var.\nRidge term. Must be a scalar.", "grad": "A Tensor. Must have the same type as var. The gradient.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var, m, and v tensors will be protected\nby a lock; otherwise the behavior is undefined, but may exhibit less\ncontention.", "use_nesterov": "An optional bool. Defaults to False.\nIf True, uses the nesterov update.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as var."}, "tf.raw_ops.ApplyAddSign": {"description": "Update &#39;*var&#39; according to the AddSign update.", "Args": {"var": "A mutable Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nShould be from a Variable().", "m": "A mutable Tensor. Must have the same type as var.\nShould be from a Variable().", "lr": "A Tensor. Must have the same type as var.\nScaling factor. Must be a scalar.", "alpha": "A Tensor. Must have the same type as var. Must be a scalar.", "sign_decay": "A Tensor. Must have the same type as var.\nMust be a scalar.", "beta": "A Tensor. Must have the same type as var. Must be a scalar.", "grad": "A Tensor. Must have the same type as var. The gradient.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var and m tensors is\nprotected by a lock; otherwise the behavior is undefined, but may exhibit less\ncontention.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as var."}, "tf.raw_ops.ApplyCenteredRMSProp": {"description": "Update &#39;*var&#39; according to the centered RMSProp algorithm.", "Args": {"var": "A mutable Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nShould be from a Variable().", "mg": "A mutable Tensor. Must have the same type as var.\nShould be from a Variable().", "ms": "A mutable Tensor. Must have the same type as var.\nShould be from a Variable().", "mom": "A mutable Tensor. Must have the same type as var.\nShould be from a Variable().", "lr": "A Tensor. Must have the same type as var.\nScaling factor. Must be a scalar.", "rho": "A Tensor. Must have the same type as var.\nDecay rate. Must be a scalar.", "momentum": "A Tensor. Must have the same type as var.\nMomentum Scale. Must be a scalar.", "epsilon": "A Tensor. Must have the same type as var.\nRidge term. Must be a scalar.", "grad": "A Tensor. Must have the same type as var. The gradient.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var, mg, ms, and mom tensors is\nprotected by a lock; otherwise the behavior is undefined, but may exhibit less\ncontention.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as var."}, "tf.raw_ops.ApplyFtrl": {"description": "Update &#39;*var&#39; according to the Ftrl-proximal scheme.", "Args": {"var": "A mutable Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nShould be from a Variable().", "accum": "A mutable Tensor. Must have the same type as var.\nShould be from a Variable().", "linear": "A mutable Tensor. Must have the same type as var.\nShould be from a Variable().", "grad": "A Tensor. Must have the same type as var. The gradient.", "lr": "A Tensor. Must have the same type as var.\nScaling factor. Must be a scalar.", "l1": "A Tensor. Must have the same type as var.\nL1 regularization. Must be a scalar.", "l2": "A Tensor. Must have the same type as var.\nL2 regularization. Must be a scalar.", "lr_power": "A Tensor. Must have the same type as var.\nScaling factor. Must be a scalar.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var and accum tensors will be protected\nby a lock; otherwise the behavior is undefined, but may exhibit less\ncontention.", "multiply_linear_by_lr": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as var."}, "tf.raw_ops.ApplyFtrlV2": {"description": "Update &#39;*var&#39; according to the Ftrl-proximal scheme.", "Args": {"var": "A mutable Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nShould be from a Variable().", "accum": "A mutable Tensor. Must have the same type as var.\nShould be from a Variable().", "linear": "A mutable Tensor. Must have the same type as var.\nShould be from a Variable().", "grad": "A Tensor. Must have the same type as var. The gradient.", "lr": "A Tensor. Must have the same type as var.\nScaling factor. Must be a scalar.", "l1": "A Tensor. Must have the same type as var.\nL1 regularization. Must be a scalar.", "l2": "A Tensor. Must have the same type as var.\nL2 shrinkage regularization. Must be a scalar.", "l2_shrinkage": "A Tensor. Must have the same type as var.", "lr_power": "A Tensor. Must have the same type as var.\nScaling factor. Must be a scalar.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var and accum tensors will be protected\nby a lock; otherwise the behavior is undefined, but may exhibit less\ncontention.", "multiply_linear_by_lr": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as var."}, "tf.raw_ops.ApplyGradientDescent": {"description": "Update &#39;*var&#39; by subtracting &#39;alpha&#39; * &#39;delta&#39; from it.", "Args": {"var": "A mutable Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nShould be from a Variable().", "alpha": "A Tensor. Must have the same type as var.\nScaling factor. Must be a scalar.", "delta": "A Tensor. Must have the same type as var. The change.", "use_locking": "An optional bool. Defaults to False.\nIf True, the subtraction will be protected by a lock;\notherwise the behavior is undefined, but may exhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as var."}, "tf.raw_ops.ApplyMomentum": {"description": "Update &#39;*var&#39; according to the momentum scheme.", "Args": {"var": "A mutable Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nShould be from a Variable().", "accum": "A mutable Tensor. Must have the same type as var.\nShould be from a Variable().", "lr": "A Tensor. Must have the same type as var.\nScaling factor. Must be a scalar.", "grad": "A Tensor. Must have the same type as var. The gradient.", "momentum": "A Tensor. Must have the same type as var.\nMomentum. Must be a scalar.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var and accum tensors will be protected\nby a lock; otherwise the behavior is undefined, but may exhibit less\ncontention.", "use_nesterov": "An optional bool. Defaults to False.\nIf True, the tensor passed to compute grad will be\nvar - lr * momentum * accum, so in the end, the var you get is actually\nvar - lr * momentum * accum.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as var."}, "tf.raw_ops.ApplyPowerSign": {"description": "Update &#39;*var&#39; according to the AddSign update.", "Args": {"var": "A mutable Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nShould be from a Variable().", "m": "A mutable Tensor. Must have the same type as var.\nShould be from a Variable().", "lr": "A Tensor. Must have the same type as var.\nScaling factor. Must be a scalar.", "logbase": "A Tensor. Must have the same type as var. Must be a scalar.", "sign_decay": "A Tensor. Must have the same type as var.\nMust be a scalar.", "beta": "A Tensor. Must have the same type as var. Must be a scalar.", "grad": "A Tensor. Must have the same type as var. The gradient.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var and m tensors is\nprotected by a lock; otherwise the behavior is undefined, but may exhibit less\ncontention.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as var."}, "tf.raw_ops.ApplyProximalAdagrad": {"description": "Update &#39;*var&#39; and &#39;*accum&#39; according to FOBOS with Adagrad learning rate.", "Args": {"var": "A mutable Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nShould be from a Variable().", "accum": "A mutable Tensor. Must have the same type as var.\nShould be from a Variable().", "lr": "A Tensor. Must have the same type as var.\nScaling factor. Must be a scalar.", "l1": "A Tensor. Must have the same type as var.\nL1 regularization. Must be a scalar.", "l2": "A Tensor. Must have the same type as var.\nL2 regularization. Must be a scalar.", "grad": "A Tensor. Must have the same type as var. The gradient.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var and accum tensors will be protected by\na lock; otherwise the behavior is undefined, but may exhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as var."}, "tf.raw_ops.ApplyProximalGradientDescent": {"description": "Update &#39;*var&#39; as FOBOS algorithm with fixed learning rate.", "Args": {"var": "A mutable Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nShould be from a Variable().", "alpha": "A Tensor. Must have the same type as var.\nScaling factor. Must be a scalar.", "l1": "A Tensor. Must have the same type as var.\nL1 regularization. Must be a scalar.", "l2": "A Tensor. Must have the same type as var.\nL2 regularization. Must be a scalar.", "delta": "A Tensor. Must have the same type as var. The change.", "use_locking": "An optional bool. Defaults to False.\nIf True, the subtraction will be protected by a lock;\notherwise the behavior is undefined, but may exhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as var."}, "tf.raw_ops.ApplyRMSProp": {"description": "Update &#39;*var&#39; according to the RMSProp algorithm.", "Args": {"var": "A mutable Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nShould be from a Variable().", "ms": "A mutable Tensor. Must have the same type as var.\nShould be from a Variable().", "mom": "A mutable Tensor. Must have the same type as var.\nShould be from a Variable().", "lr": "A Tensor. Must have the same type as var.\nScaling factor. Must be a scalar.", "rho": "A Tensor. Must have the same type as var.\nDecay rate. Must be a scalar.", "momentum": "A Tensor. Must have the same type as var.", "epsilon": "A Tensor. Must have the same type as var.\nRidge term. Must be a scalar.", "grad": "A Tensor. Must have the same type as var. The gradient.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var, ms, and mom tensors is protected\nby a lock; otherwise the behavior is undefined, but may exhibit less\ncontention.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as var."}, "tf.raw_ops.ApproxTopK": {"description": "Returns min/max k values and their indices of the input operand in an approximate manner.", "Args": {"input": "A Tensor. Must be one of the following types: half, bfloat16, float32.\nArray to search. Must be at least 1-D of the floating type", "k": "An int that is >= 0. Specifies the number of min/max-k.", "reduction_dimension": "An optional int. Defaults to -1.\nInteger dimension along which to search. Default: -1.", "recall_target": "An optional float. Defaults to 0.95.\nRecall target for the approximation. Range in (0,1]", "is_max_k": "An optional bool. Defaults to True.\nWhen true, computes max-k; otherwise computes min-k.", "reduction_input_size_override": "An optional int. Defaults to -1.\nWhen set to a positive value, it overrides the size determined by\ninput[reduction_dim] for evaluating the recall. This option is useful when\nthe given input is only a subset of the overall computation in SPMD or\ndistributed pipelines, where the true input size cannot be deferred by the\ninput shape.", "aggregate_to_topk": "An optional bool. Defaults to True.\nWhen true, aggregates approximate results to top-k. When false, returns the\napproximate results. The number of the approximate results is implementation\ndefined and is greater equals to the specified k.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (values, indices)."}, "tf.raw_ops.ApproximateEqual": {"description": "Returns the truth value of abs(x-y) &lt; tolerance element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.", "y": "A Tensor. Must have the same type as x.", "tolerance": "An optional float. Defaults to 1e-05.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type bool."}, "tf.raw_ops.ArgMax": {"description": "Returns the index with the largest value across dimensions of a tensor.", "Args": {"input": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64, bool.", "dimension": "A Tensor. Must be one of the following types: int16, int32, int64.\nint16, int32 or int64, must be in the range [-rank(input), rank(input)).\nDescribes which dimension of the input Tensor to reduce across. For vectors,\nuse dimension = 0.", "output_type": "An optional tf.DType from: tf.int16, tf.uint16, tf.int32, tf.int64. Defaults to tf.int64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type output_type."}, "tf.raw_ops.ArgMin": {"description": "Returns the index with the smallest value across dimensions of a tensor.", "Args": {"input": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64, bool.", "dimension": "A Tensor. Must be one of the following types: int32, int64.\nint32 or int64, must be in the range [-rank(input), rank(input)).\nDescribes which dimension of the input Tensor to reduce across. For vectors,\nuse dimension = 0.", "output_type": "An optional tf.DType from: tf.int32, tf.int64. Defaults to tf.int64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type output_type."}, "tf.raw_ops.AsString": {"description": "Converts each entry in the given tensor to strings.", "Args": {"input": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64, complex64, complex128, bool, variant.", "precision": "An optional int. Defaults to -1.\nThe post-decimal precision to use for floating point numbers.\nOnly used if precision > -1.", "scientific": "An optional bool. Defaults to False.\nUse scientific notation for floating point numbers.", "shortest": "An optional bool. Defaults to False.\nUse shortest representation (either scientific or standard) for\nfloating point numbers.", "width": "An optional int. Defaults to -1.\nPad pre-decimal numbers to this width.\nApplies to both floating point and integer numbers.\nOnly used if width > -1.", "fill": "An optional string. Defaults to \"\".\nThe value to pad if width > -1.  If empty, pads with spaces.\nAnother typical value is '0'.  String cannot be longer than 1 character.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.Asin": {"description": "Computes the trignometric inverse sine of x element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, int8, int16, int32, int64, complex64, complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.Asinh": {"description": "Computes inverse hyperbolic sine of x element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, complex64, complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.Assert": {"description": "Asserts that the given condition is true.", "Args": {"condition": "A Tensor of type bool. The condition to evaluate.", "data": "A list of Tensor objects.\nThe tensors to print out when condition is false.", "summarize": "An optional int. Defaults to 3.\nPrint this many entries of each tensor.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.AssertCardinalityDataset": {"Args": {"input_dataset": "A Tensor of type variant.", "cardinality": "A Tensor of type int64.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.AssertNextDataset": {"description": "A transformation that asserts which transformations happen next.", "Args": {"input_dataset": "A Tensor of type variant.\nA variant tensor representing the input dataset.\nAssertNextDataset passes through the outputs of its input dataset.", "transformations": "A Tensor of type string.\nA tf.string vector tf.Tensor identifying the transformations that are\nexpected to happen next.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.AssertPrevDataset": {"description": "A transformation that asserts which transformations happened previously.", "Args": {"input_dataset": "A Tensor of type variant.\nA variant tensor representing the input dataset.\nAssertPrevDataset passes through the outputs of its input dataset.", "transformations": "A Tensor of type string.\nA tf.string vector tf.Tensor identifying the transformations, with optional\nattribute name-value pairs, that are expected to have happened previously.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.Assign": {"description": "Update &#39;ref&#39; by assigning &#39;value&#39; to it.", "Args": {"ref": "A mutable Tensor.\nShould be from a Variable node. May be uninitialized.", "value": "A Tensor. Must have the same type as ref.\nThe value to be assigned to the variable.", "validate_shape": "An optional bool. Defaults to True.\nIf true, the operation will validate that the shape\nof 'value' matches the shape of the Tensor being assigned to.  If false,\n'ref' will take on the shape of 'value'.", "use_locking": "An optional bool. Defaults to True.\nIf True, the assignment will be protected by a lock;\notherwise the behavior is undefined, but may exhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as ref."}, "tf.raw_ops.AssignAdd": {"description": "Update &#39;ref&#39; by adding &#39;value&#39; to it.", "Args": {"ref": "A mutable Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nShould be from a Variable node.", "value": "A Tensor. Must have the same type as ref.\nThe value to be added to the variable.", "use_locking": "An optional bool. Defaults to False.\nIf True, the addition will be protected by a lock;\notherwise the behavior is undefined, but may exhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as ref."}, "tf.raw_ops.AssignAddVariableOp": {"description": "Adds a value to the current value of a variable.", "Args": {"resource": "A Tensor of type resource.\nhandle to the resource in which to store the variable.", "value": "A Tensor. the value by which the variable will be incremented.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.AssignSub": {"description": "Update &#39;ref&#39; by subtracting &#39;value&#39; from it.", "Args": {"ref": "A mutable Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nShould be from a Variable node.", "value": "A Tensor. Must have the same type as ref.\nThe value to be subtracted to the variable.", "use_locking": "An optional bool. Defaults to False.\nIf True, the subtraction will be protected by a lock;\notherwise the behavior is undefined, but may exhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as ref."}, "tf.raw_ops.AssignSubVariableOp": {"description": "Subtracts a value from the current value of a variable.", "Args": {"resource": "A Tensor of type resource.\nhandle to the resource in which to store the variable.", "value": "A Tensor. the value by which the variable will be incremented.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.AssignVariableOp": {"description": "Assigns a new value to a variable.", "Args": {"resource": "A Tensor of type resource.\nhandle to the resource in which to store the variable.", "value": "A Tensor. the value to set the new tensor to use.", "validate_shape": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.AssignVariableXlaConcatND": {"description": "Concats input tensor across all dimensions.", "Args": {"resource": "A Tensor of type resource.\nResource variable for concatenated input tensors across all dimensions.\n  }\n  in_arg {\n    name: \"inputs\"\n    description: <", "inputs": "A list of at least 1 Tensor objects with the same type.", "num_concats": "A list of ints. Number of ways to merge per dimension.", "paddings": "An optional list of ints. Defaults to [].\nOptional list of right paddings per dimension to strip from the final merged\ntensor. These paddings must not exceed the dimension size of the merged result\nprior to stripping paddings.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.Atan": {"description": "Computes the trignometric inverse tangent of x element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, int8, int16, int32, int64, complex64, complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.Atan2": {"description": "Computes arctangent of y/x element-wise, respecting signs of the arguments.", "Args": {"y": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "x": "A Tensor. Must have the same type as y.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as y."}, "tf.raw_ops.Atanh": {"description": "Computes inverse hyperbolic tangent of x element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, complex64, complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.AudioSpectrogram": {"description": "Produces a visualization of audio data over time.", "Args": {"input": "A Tensor of type float32. Float representation of audio data.", "window_size": "An int.\nHow wide the input window is in samples. For the highest efficiency\nthis should be a power of two, but other values are accepted.", "stride": "An int.\nHow widely apart the center of adjacent sample windows should be.", "magnitude_squared": "An optional bool. Defaults to False.\nWhether to return the squared magnitude or just the\nmagnitude. Using squared magnitude can avoid extra calculations.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type float32."}, "tf.raw_ops.AudioSummary": {"description": "Outputs a Summary protocol buffer with audio.", "Args": {"tag": "A Tensor of type string.\nScalar. Used to build the tag attribute of the summary values.", "tensor": "A Tensor of type float32. 2-D of shape [batch_size, frames].", "sample_rate": "A float. The sample rate of the signal in hertz.", "max_outputs": "An optional int that is >= 1. Defaults to 3.\nMax number of batch elements to generate audio for.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.AudioSummaryV2": {"description": "Outputs a Summary protocol buffer with audio.", "Args": {"tag": "A Tensor of type string.\nScalar. Used to build the tag attribute of the summary values.", "tensor": "A Tensor of type float32. 2-D of shape [batch_size, frames].", "sample_rate": "A Tensor of type float32.\nThe sample rate of the signal in hertz.", "max_outputs": "An optional int that is >= 1. Defaults to 3.\nMax number of batch elements to generate audio for.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.AutoShardDataset": {"description": "Creates a dataset that shards the input dataset.", "Args": {"input_dataset": "A Tensor of type variant.\nA variant tensor representing the input dataset.", "num_workers": "A Tensor of type int64.\nA scalar representing the number of workers to distribute this dataset across.", "index": "A Tensor of type int64.\nA scalar representing the index of the current worker out of num_workers.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "auto_shard_policy": "An optional int. Defaults to 0.", "num_replicas": "An optional int. Defaults to 0.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.AvgPool": {"description": "Performs average pooling on the input.", "Args": {"value": "A Tensor. Must be one of the following types: half, bfloat16, float32, float64.\n4-D with shape [batch, height, width, channels].", "ksize": "A list of ints that has length >= 4.\nThe size of the sliding window for each dimension of value.", "strides": "A list of ints that has length >= 4.\nThe stride of the sliding window for each dimension of value.", "padding": "A string from: \"SAME\", \"VALID\".\nThe type of padding algorithm to use.", "data_format": "An optional string from: \"NHWC\", \"NCHW\". Defaults to \"NHWC\".\nSpecify the data format of the input and output data. With the\ndefault format \"NHWC\", the data is stored in the order of:\n    [batch, in_height, in_width, in_channels].\nAlternatively, the format could be \"NCHW\", the data storage order of:\n    [batch, in_channels, in_height, in_width].", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as value."}, "tf.raw_ops.AvgPool3D": {"description": "Performs 3D average pooling on the input.", "Args": {"input": "A Tensor. Must be one of the following types: half, bfloat16, float32, float64.\nShape [batch, depth, rows, cols, channels] tensor to pool over.", "ksize": "A list of ints that has length >= 5.\n1-D tensor of length 5. The size of the window for each dimension of\nthe input tensor. Must have ksize[0] = ksize[4] = 1.", "strides": "A list of ints that has length >= 5.\n1-D tensor of length 5. The stride of the sliding window for each\ndimension of input. Must have strides[0] = strides[4] = 1.", "padding": "A string from: \"SAME\", \"VALID\".\nThe type of padding algorithm to use.", "data_format": "An optional string from: \"NDHWC\", \"NCDHW\". Defaults to \"NDHWC\".\nThe data format of the input and output data. With the\ndefault format \"NDHWC\", the data is stored in the order of:\n    [batch, in_depth, in_height, in_width, in_channels].\nAlternatively, the format could be \"NCDHW\", the data storage order is:\n    [batch, in_channels, in_depth, in_height, in_width].", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.AvgPool3DGrad": {"description": "Computes gradients of average pooling function.", "Args": {"orig_input_shape": "A Tensor of type int32.\nThe original input dimensions.", "grad": "A Tensor. Must be one of the following types: half, bfloat16, float32, float64.\nOutput backprop of shape [batch, depth, rows, cols, channels].", "ksize": "A list of ints that has length >= 5.\n1-D tensor of length 5. The size of the window for each dimension of\nthe input tensor. Must have ksize[0] = ksize[4] = 1.", "strides": "A list of ints that has length >= 5.\n1-D tensor of length 5. The stride of the sliding window for each\ndimension of input. Must have strides[0] = strides[4] = 1.", "padding": "A string from: \"SAME\", \"VALID\".\nThe type of padding algorithm to use.", "data_format": "An optional string from: \"NDHWC\", \"NCDHW\". Defaults to \"NDHWC\".\nThe data format of the input and output data. With the\ndefault format \"NDHWC\", the data is stored in the order of:\n    [batch, in_depth, in_height, in_width, in_channels].\nAlternatively, the format could be \"NCDHW\", the data storage order is:\n    [batch, in_channels, in_depth, in_height, in_width].", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as grad."}, "tf.raw_ops.AvgPoolGrad": {"description": "Computes gradients of the average pooling function.", "Args": {"orig_input_shape": "A Tensor of type int32.\n1-D.  Shape of the original input to avg_pool.", "grad": "A Tensor. Must be one of the following types: half, bfloat16, float32, float64.\n4-D with shape [batch, height, width, channels].  Gradients w.r.t.\nthe output of avg_pool.", "ksize": "A list of ints that has length >= 4.\nThe size of the sliding window for each dimension of the input.", "strides": "A list of ints that has length >= 4.\nThe stride of the sliding window for each dimension of the input.", "padding": "A string from: \"SAME\", \"VALID\".\nThe type of padding algorithm to use.", "data_format": "An optional string from: \"NHWC\", \"NCHW\". Defaults to \"NHWC\".\nSpecify the data format of the input and output data. With the\ndefault format \"NHWC\", the data is stored in the order of:\n    [batch, in_height, in_width, in_channels].\nAlternatively, the format could be \"NCHW\", the data storage order of:\n    [batch, in_channels, in_height, in_width].", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as grad."}, "tf.raw_ops.BandedTriangularSolve": {"Args": {"matrix": "A Tensor. Must be one of the following types: float64, float32, half, complex64, complex128.", "rhs": "A Tensor. Must have the same type as matrix.", "lower": "An optional bool. Defaults to True.", "adjoint": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as matrix."}, "tf.raw_ops.Barrier": {"description": "Defines a barrier that persists across different graph executions.", "Args": {"component_types": "A list of tf.DTypes that has length >= 1.\nThe type of each component in a value.", "shapes": "An optional list of shapes (each a tf.TensorShape or list of ints). Defaults to [].\nThe shape of each component in a value. Each shape must be 1 in the\nfirst dimension. The length of this attr must be the same as the length of\ncomponent_types.", "capacity": "An optional int. Defaults to -1.\nThe capacity of the barrier.  The default capacity is MAX_INT32,\nwhich is the largest capacity of the underlying queue.", "container": "An optional string. Defaults to \"\".\nIf non-empty, this barrier is placed in the given container.\nOtherwise, a default container is used.", "shared_name": "An optional string. Defaults to \"\".\nIf non-empty, this barrier will be shared under the given name\nacross multiple sessions.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type mutable string."}, "tf.raw_ops.BarrierClose": {"description": "Closes the given barrier.", "Args": {"handle": "A Tensor of type mutable string. The handle to a barrier.", "cancel_pending_enqueues": "An optional bool. Defaults to False.\nIf true, all pending enqueue requests that are\nblocked on the barrier's queue will be canceled. InsertMany will fail, even\nif no new key is introduced.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.BarrierIncompleteSize": {"description": "Computes the number of incomplete elements in the given barrier.", "Args": {"handle": "A Tensor of type mutable string. The handle to a barrier.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int32."}, "tf.raw_ops.BarrierInsertMany": {"description": "For each key, assigns the respective value to the specified component.", "Args": {"handle": "A Tensor of type mutable string. The handle to a barrier.", "keys": "A Tensor of type string.\nA one-dimensional tensor of keys, with length n.", "values": "A Tensor.\nAn any-dimensional tensor of values, which are associated with the\nrespective keys. The 0th dimension must have length n.", "component_index": "An int.\nThe component of the barrier elements that is being assigned.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.BarrierReadySize": {"description": "Computes the number of complete elements in the given barrier.", "Args": {"handle": "A Tensor of type mutable string. The handle to a barrier.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int32."}, "tf.raw_ops.BarrierTakeMany": {"description": "Takes the given number of completed elements from a barrier.", "Args": {"handle": "A Tensor of type mutable string. The handle to a barrier.", "num_elements": "A Tensor of type int32.\nA single-element tensor containing the number of elements to\ntake.", "component_types": "A list of tf.DTypes that has length >= 1.\nThe type of each component in a value.", "allow_small_batch": "An optional bool. Defaults to False.\nAllow to return less than num_elements items if barrier is\nalready closed.", "wait_for_incomplete": "An optional bool. Defaults to False.", "timeout_ms": "An optional int. Defaults to -1.\nIf the queue is empty, this operation will block for up to\ntimeout_ms milliseconds.\nNote: This option is not supported yet.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (indices, keys, values)."}, "tf.raw_ops.Batch": {"description": "Batches all input tensors nondeterministically.", "Args": {"in_tensors": "A list of Tensor objects.", "num_batch_threads": "An int.", "max_batch_size": "An int.", "batch_timeout_micros": "An int.", "grad_timeout_micros": "An int.", "max_enqueued_batches": "An optional int. Defaults to 10.", "allowed_batch_sizes": "An optional list of ints. Defaults to [].", "container": "An optional string. Defaults to \"\".", "shared_name": "An optional string. Defaults to \"\".", "batching_queue": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (batched_tensors, batch_index, id)."}, "tf.raw_ops.BatchCholesky": {"Args": {"input": "A Tensor. Must be one of the following types: float64, float32.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.BatchCholeskyGrad": {"Args": {"l": "A Tensor. Must be one of the following types: float32, float64.", "grad": "A Tensor. Must have the same type as l.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as l."}, "tf.raw_ops.BatchDataset": {"description": "Creates a dataset that batches batch_size elements from input_dataset.", "Args": {"input_dataset": "A Tensor of type variant.", "batch_size": "A Tensor of type int64.\nA scalar representing the number of elements to accumulate in a\nbatch.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.BatchDatasetV2": {"description": "Creates a dataset that batches batch_size elements from input_dataset.", "Args": {"input_dataset": "A Tensor of type variant.", "batch_size": "A Tensor of type int64.\nA scalar representing the number of elements to accumulate in a batch.", "drop_remainder": "A Tensor of type bool.\nA scalar representing whether the last batch should be dropped in case its size\nis smaller than desired.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "parallel_copy": "An optional bool. Defaults to False.", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.BatchFFT": {"Args": {"input": "A Tensor of type complex64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type complex64."}, "tf.raw_ops.BatchFFT2D": {"Args": {"input": "A Tensor of type complex64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type complex64."}, "tf.raw_ops.BatchFFT3D": {"Args": {"input": "A Tensor of type complex64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type complex64."}, "tf.raw_ops.BatchFunction": {"description": "Batches all the inputs tensors to the computation done by the function.", "Args": {"in_tensors": "A list of Tensor objects. The tensors to be batched.", "captured_tensors": "A list of Tensor objects.\nThe tensors which are captured in the function, and don't need\nto be batched.", "f": "A function decorated with @Defun.", "num_batch_threads": "An int.\nNumber of scheduling threads for processing batches of work.\nDetermines the number of batches processed in parallel.", "max_batch_size": "An int. Batch sizes will never be bigger than this.", "batch_timeout_micros": "An int.\nMaximum number of microseconds to wait before outputting\nan incomplete batch.", "Tout": "A list of tf.DTypes that has length >= 1.\nthe types of the output tensors.", "max_enqueued_batches": "An optional int. Defaults to 10.\nMaximum number of batches enqueued. Default: 10.", "allowed_batch_sizes": "An optional list of ints. Defaults to [].\nOptional list of allowed batch sizes. If left empty, does\nnothing. Otherwise, supplies a list of batch sizes, causing the op to pad\nbatches up to one of those sizes. The entries must increase monotonically.\nIf enable_large_batch_splitting is false (i.e., large-input-split is not\nenabled) the final entry must equal max_batch_size.", "container": "An optional string. Defaults to \"\".\nControls the scope of sharing of this batch.", "shared_name": "An optional string. Defaults to \"\".\nConcurrently running instances of batch in the same device with the\nsame container and shared_name will batch their elements together. If left\nempty, the op name will be used as the shared name.", "batching_queue": "An optional string. Defaults to \"\".", "enable_large_batch_splitting": "An optional bool. Defaults to False.\ninput with a large size (i.e., larger than the largest value of\nallowed_batch_sizes) will be splitted into multiple batches with batch size.", "name": "A name for the operation (optional)."}, "Returns": "A list of Tensor objects of type Tout."}, "tf.raw_ops.BatchIFFT": {"Args": {"input": "A Tensor of type complex64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type complex64."}, "tf.raw_ops.BatchIFFT2D": {"Args": {"input": "A Tensor of type complex64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type complex64."}, "tf.raw_ops.BatchIFFT3D": {"Args": {"input": "A Tensor of type complex64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type complex64."}, "tf.raw_ops.BatchMatMul": {"description": "Multiplies slices of two tensors in batches.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, int32, int64, complex64, complex128.\n2-D or higher with shape [..., r_x, c_x].", "y": "A Tensor. Must have the same type as x.\n2-D or higher with shape [..., r_y, c_y].", "adj_x": "An optional bool. Defaults to False.\nIf True, adjoint the slices of x. Defaults to False.", "adj_y": "An optional bool. Defaults to False.\nIf True, adjoint the slices of y. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.BatchMatMulV2": {"description": "Multiplies slices of two tensors in batches.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, int16, int32, int64, complex64, complex128.\n2-D or higher with shape [..., r_x, c_x].", "y": "A Tensor. Must have the same type as x.\n2-D or higher with shape [..., r_y, c_y].", "adj_x": "An optional bool. Defaults to False.\nIf True, adjoint the slices of x. Defaults to False.", "adj_y": "An optional bool. Defaults to False.\nIf True, adjoint the slices of y. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.BatchMatMulV3": {"description": "Multiplies slices of two tensors in batches.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, uint8, int8, int16, int32, int64, complex64, complex128.\n2-D or higher with shape [..., r_x, c_x].", "y": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, uint8, int8, int16, int32, int64, complex64, complex128.\n2-D or higher with shape [..., r_y, c_y].", "Tout": "A tf.DType from: tf.bfloat16, tf.half, tf.float32, tf.float64, tf.int16, tf.int32, tf.int64, tf.complex64, tf.complex128.\nIf not spcified, Tout is the same type to input type.", "adj_x": "An optional bool. Defaults to False.\nIf True, adjoint the slices of x. Defaults to False.", "adj_y": "An optional bool. Defaults to False.\nIf True, adjoint the slices of y. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type Tout."}, "tf.raw_ops.BatchMatrixBandPart": {"Args": {"input": "A Tensor.", "num_lower": "A Tensor of type int64.", "num_upper": "A Tensor of type int64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.BatchMatrixDeterminant": {"Args": {"input": "A Tensor. Must be one of the following types: float32, float64, complex64, complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.BatchMatrixDiag": {"Args": {"diagonal": "A Tensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as diagonal."}, "tf.raw_ops.BatchMatrixDiagPart": {"Args": {"input": "A Tensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.BatchMatrixInverse": {"Args": {"input": "A Tensor. Must be one of the following types: float64, float32.", "adjoint": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.BatchMatrixSetDiag": {"Args": {"input": "A Tensor.", "diagonal": "A Tensor. Must have the same type as input.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.BatchMatrixSolve": {"Args": {"matrix": "A Tensor. Must be one of the following types: float64, float32.", "rhs": "A Tensor. Must have the same type as matrix.", "adjoint": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as matrix."}, "tf.raw_ops.BatchMatrixSolveLs": {"Args": {"matrix": "A Tensor. Must be one of the following types: float64, float32.", "rhs": "A Tensor. Must have the same type as matrix.", "l2_regularizer": "A Tensor of type float64.", "fast": "An optional bool. Defaults to True.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as matrix."}, "tf.raw_ops.BatchMatrixTriangularSolve": {"Args": {"matrix": "A Tensor. Must be one of the following types: float64, float32.", "rhs": "A Tensor. Must have the same type as matrix.", "lower": "An optional bool. Defaults to True.", "adjoint": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as matrix."}, "tf.raw_ops.BatchNormWithGlobalNormalization": {"description": "Batch normalization.", "Args": {"t": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nA 4D input Tensor.", "m": "A Tensor. Must have the same type as t.\nA 1D mean Tensor with size matching the last dimension of t.\nThis is the first output from tf.nn.moments,\nor a saved moving average thereof.", "v": "A Tensor. Must have the same type as t.\nA 1D variance Tensor with size matching the last dimension of t.\nThis is the second output from tf.nn.moments,\nor a saved moving average thereof.", "beta": "A Tensor. Must have the same type as t.\nA 1D beta Tensor with size matching the last dimension of t.\nAn offset to be added to the normalized tensor.", "gamma": "A Tensor. Must have the same type as t.\nA 1D gamma Tensor with size matching the last dimension of t.\nIf \"scale_after_normalization\" is true, this tensor will be multiplied\nwith the normalized tensor.", "variance_epsilon": "A float. A small float number to avoid dividing by 0.", "scale_after_normalization": "A bool.\nA bool indicating whether the resulted tensor\nneeds to be multiplied with gamma.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as t."}, "tf.raw_ops.BatchNormWithGlobalNormalizationGrad": {"description": "Gradients for batch normalization.", "Args": {"t": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nA 4D input Tensor.", "m": "A Tensor. Must have the same type as t.\nA 1D mean Tensor with size matching the last dimension of t.\nThis is the first output from tf.nn.moments,\nor a saved moving average thereof.", "v": "A Tensor. Must have the same type as t.\nA 1D variance Tensor with size matching the last dimension of t.\nThis is the second output from tf.nn.moments,\nor a saved moving average thereof.", "gamma": "A Tensor. Must have the same type as t.\nA 1D gamma Tensor with size matching the last dimension of t.\nIf \"scale_after_normalization\" is true, this Tensor will be multiplied\nwith the normalized Tensor.", "backprop": "A Tensor. Must have the same type as t. 4D backprop Tensor.", "variance_epsilon": "A float. A small float number to avoid dividing by 0.", "scale_after_normalization": "A bool.\nA bool indicating whether the resulted tensor\nneeds to be multiplied with gamma.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (dx, dm, dv, db, dg)."}, "tf.raw_ops.BatchSelfAdjointEig": {"Args": {"input": "A Tensor. Must be one of the following types: float64, float32.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.BatchSelfAdjointEigV2": {"Args": {"input": "A Tensor. Must be one of the following types: float64, float32.", "compute_v": "An optional bool. Defaults to True.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (e, v)."}, "tf.raw_ops.BatchSvd": {"Args": {"input": "A Tensor. Must be one of the following types: float64, float32, complex64, complex128.", "compute_uv": "An optional bool. Defaults to True.", "full_matrices": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (s, u, v)."}, "tf.raw_ops.BatchToSpace": {"description": "BatchToSpace for 4-D tensors of type T.", "Args": {"input": "A Tensor. 4-D tensor with shape\n[batch*block_size*block_size, height_pad/block_size, width_pad/block_size,\n  depth]. Note that the batch size of the input tensor must be divisible by\nblock_size * block_size.", "crops": "A Tensor. Must be one of the following types: int32, int64.\n2-D tensor of non-negative integers with shape [2, 2]. It specifies\nhow many elements to crop from the intermediate result across the spatial\ndimensions as follows:\ncrops = [[crop_top, crop_bottom], [crop_left, crop_right]]", "block_size": "An int that is >= 2.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.BatchToSpaceND": {"description": "BatchToSpace for N-D tensors of type T.", "Args": {"input": "A Tensor.\nN-D with shape input_shape = [batch] + spatial_shape + remaining_shape,\nwhere spatial_shape has M dimensions.", "block_shape": "A Tensor. Must be one of the following types: int32, int64.\n1-D with shape [M], all values must be >= 1.", "crops": "A Tensor. Must be one of the following types: int32, int64.\n2-D with shape [M, 2], all values must be >= 0.\n  crops[i] = [crop_start, crop_end] specifies the amount to crop from input\n  dimension i + 1, which corresponds to spatial dimension i.  It is\n  required that\n  crop_start[i] + crop_end[i] <= block_shape[i] * input_shape[i + 1].\nThis operation is equivalent to the following steps:\n\nReshape input to reshaped of shape:\n [block_shape[0], ..., block_shape[M-1],\n  batch / prod(block_shape),\n  input_shape[1], ..., input_shape[N-1]]\nPermute dimensions of reshaped to produce permuted of shape\n [batch / prod(block_shape),\ninput_shape[1], block_shape[0],\n  ...,\n  input_shape[M], block_shape[M-1],\ninput_shape[M+1], ..., input_shape[N-1]]\nReshape permuted to produce reshaped_permuted of shape\n [batch / prod(block_shape),\ninput_shape[1] * block_shape[0],\n  ...,\n  input_shape[M] * block_shape[M-1],\ninput_shape[M+1],\n  ...,\n  input_shape[N-1]]\nCrop the start and end of dimensions [1, ..., M] of\nreshaped_permuted according to crops to produce the output of shape:\n [batch / prod(block_shape),\ninput_shape[1] * block_shape[0] - crops[0,0] - crops[0,1],\n  ...,\n  input_shape[M] * block_shape[M-1] - crops[M-1,0] - crops[M-1,1],\ninput_shape[M+1], ..., input_shape[N-1]]\n\nSome examples:\n(1) For the following input of shape [4, 1, 1, 1], block_shape = [2, 2], and\n    crops = [[0, 0], [0, 0]]:\n[[[[1]]], [[[2]]], [[[3]]], [[[4]]]]\nThe output tensor has shape [1, 2, 2, 1] and value:\nx = [[[[1], [2]], [[3], [4]]]]\n(2) For the following input of shape [4, 1, 1, 3], block_shape = [2, 2], and\n    crops = [[0, 0], [0, 0]]:\n[[[[1, 2, 3]]], [[[4, 5, 6]]], [[[7, 8, 9]]], [[[10, 11, 12]]]]\nThe output tensor has shape [1, 2, 2, 3] and value:\nx = [[[[1, 2, 3], [4, 5, 6]],\u00a0 \u00a0 \u00a0 [[7, 8, 9], [10, 11, 12]]]]\n(3) For the following input of shape [4, 2, 2, 1], block_shape = [2, 2], and\n    crops = [[0, 0], [0, 0]]:\nx = [[[[1], [3]], [[9], [11]]],\u00a0 \u00a0 \u00a0[[[2], [4]], [[10], [12]]],\u00a0 \u00a0 \u00a0[[[5], [7]], [[13], [15]]],\u00a0 \u00a0 \u00a0[[[6], [8]], [[14], [16]]]]\nThe output tensor has shape [1, 4, 4, 1] and value:\nx = [[[[1], \u00a0 [2], \u00a0[3], \u00a0[4]],\u00a0 \u00a0 \u00a0[[5], \u00a0 [6], \u00a0[7], \u00a0[8]],\u00a0 \u00a0 \u00a0[[9], \u00a0[10], [11], \u00a0[12]],\u00a0 \u00a0 \u00a0[[13], [14], [15], \u00a0[16]]]]\n(4) For the following input of shape [8, 1, 3, 1], block_shape = [2, 2], and\n    crops = [[0, 0], [2, 0]]:\nx = [[[[0], [1], [3]]], [[[0], [9], [11]]],\u00a0 \u00a0 \u00a0[[[0], [2], [4]]], [[[0], [10], [12]]],\u00a0 \u00a0 \u00a0[[[0], [5], [7]]], [[[0], [13], [15]]],\u00a0 \u00a0 \u00a0[[[0], [6], [8]]], [[[0], [14], [16]]]]\nThe output tensor has shape [2, 2, 4, 1] and value:\nx = [[[[1], \u00a0 [2], \u00a0[3], \u00a0[4]],\u00a0 \u00a0 \u00a0 [[5], \u00a0 [6], \u00a0[7], \u00a0[8]]],\u00a0 \u00a0 \u00a0[[[9], \u00a0[10], [11], \u00a0[12]],\u00a0 \u00a0 \u00a0 [[13], [14], [15], \u00a0[16]]]]", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.BesselI0": {"Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.BesselI0e": {"Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.BesselI1": {"Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.BesselI1e": {"Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.BesselJ0": {"Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.BesselJ1": {"Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.BesselK0": {"Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.BesselK0e": {"Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.BesselK1": {"Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.BesselK1e": {"Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.BesselY0": {"Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.BesselY1": {"Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.Betainc": {"description": "Compute the regularized incomplete beta integral \\\\(I_x(a, b)\\\\).", "Args": {"a": "A Tensor. Must be one of the following types: float32, float64.", "b": "A Tensor. Must have the same type as a.", "x": "A Tensor. Must have the same type as a.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as a."}, "tf.raw_ops.BiasAdd": {"description": "Adds bias to value.", "Args": {"value": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nAny number of dimensions.", "bias": "A Tensor. Must have the same type as value.\n1-D with size the last dimension of value.", "data_format": "An optional string from: \"NHWC\", \"NCHW\". Defaults to \"NHWC\".\nSpecify the data format of the input and output data. With the\ndefault format \"NHWC\", the bias tensor will be added to the last dimension\nof the value tensor.\nAlternatively, the format could be \"NCHW\", the data storage order of:\n    [batch, in_channels, in_height, in_width].\nThe tensor will be added to \"in_channels\", the third-to-the-last\n    dimension.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as value."}, "tf.raw_ops.BiasAddGrad": {"description": "The backward operation for &#34;BiasAdd&#34; on the &#34;bias&#34; tensor.", "Args": {"out_backprop": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nAny number of dimensions.", "data_format": "An optional string from: \"NHWC\", \"NCHW\". Defaults to \"NHWC\".\nSpecify the data format of the input and output data. With the\ndefault format \"NHWC\", the bias tensor will be added to the last dimension\nof the value tensor.\nAlternatively, the format could be \"NCHW\", the data storage order of:\n    [batch, in_channels, in_height, in_width].\nThe tensor will be added to \"in_channels\", the third-to-the-last\n    dimension.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as out_backprop."}, "tf.raw_ops.BiasAddV1": {"description": "Adds bias to value.", "Args": {"value": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nAny number of dimensions.", "bias": "A Tensor. Must have the same type as value.\n1-D with size the last dimension of value.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as value."}, "tf.raw_ops.Bincount": {"description": "Counts the number of occurrences of each value in an integer array.", "Args": {"arr": "A Tensor of type int32. int32 Tensor.", "size": "A Tensor of type int32. non-negative int32 scalar Tensor.", "weights": "A Tensor. Must be one of the following types: int32, int64, float32, float64.\nis an int32, int64, float32, or float64 Tensor with the same\nshape as arr, or a length-0 Tensor, in which case it acts as all weights\nequal to 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as weights."}, "tf.raw_ops.Bitcast": {"description": "Bitcasts a tensor from one type to another without copying data.", "Args": {"input": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, int64, int32, uint8, uint16, uint32, uint64, int8, int16, complex64, complex128, qint8, quint8, qint16, quint16, qint32.", "type": "A tf.DType from: tf.bfloat16, tf.half, tf.float32, tf.float64, tf.int64, tf.int32, tf.uint8, tf.uint16, tf.uint32, tf.uint64, tf.int8, tf.int16, tf.complex64, tf.complex128, tf.qint8, tf.quint8, tf.qint16, tf.quint16, tf.qint32.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type type."}, "tf.raw_ops.BitwiseAnd": {"description": "Elementwise computes the bitwise AND of x and y.", "Args": {"x": "A Tensor. Must be one of the following types: int8, int16, int32, int64, uint8, uint16, uint32, uint64.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.BitwiseOr": {"description": "Elementwise computes the bitwise OR of x and y.", "Args": {"x": "A Tensor. Must be one of the following types: int8, int16, int32, int64, uint8, uint16, uint32, uint64.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.BitwiseXor": {"description": "Elementwise computes the bitwise XOR of x and y.", "Args": {"x": "A Tensor. Must be one of the following types: int8, int16, int32, int64, uint8, uint16, uint32, uint64.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.BlockLSTM": {"description": "Computes the LSTM cell forward propagation for all the time steps.", "Args": {"seq_len_max": "A Tensor of type int64.\nMaximum time length actually used by this input. Outputs are padded\nwith zeros beyond this length.", "x": "A Tensor. Must be one of the following types: half, float32.\nThe sequence input to the LSTM, shape (timelen, batch_size, num_inputs).", "cs_prev": "A Tensor. Must have the same type as x.\nValue of the initial cell state.", "h_prev": "A Tensor. Must have the same type as x.\nInitial output of cell (to be used for peephole).", "w": "A Tensor. Must have the same type as x. The weight matrix.", "wci": "A Tensor. Must have the same type as x.\nThe weight matrix for input gate peephole connection.", "wcf": "A Tensor. Must have the same type as x.\nThe weight matrix for forget gate peephole connection.", "wco": "A Tensor. Must have the same type as x.\nThe weight matrix for output gate peephole connection.", "b": "A Tensor. Must have the same type as x. The bias vector.", "forget_bias": "An optional float. Defaults to 1. The forget gate bias.", "cell_clip": "An optional float. Defaults to 3.\nValue to clip the 'cs' value to.", "use_peephole": "An optional bool. Defaults to False.\nWhether to use peephole weights.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (i, cs, f, o, ci, co, h)."}, "tf.raw_ops.BlockLSTMGrad": {"description": "Computes the LSTM cell backward propagation for the entire time sequence.", "Args": {"seq_len_max": "A Tensor of type int64.\nMaximum time length actually used by this input. Outputs are padded\nwith zeros beyond this length.", "x": "A Tensor. Must be one of the following types: half, float32.\nThe sequence input to the LSTM, shape (timelen, batch_size, num_inputs).", "cs_prev": "A Tensor. Must have the same type as x.\nValue of the initial cell state.", "h_prev": "A Tensor. Must have the same type as x.\nInitial output of cell (to be used for peephole).", "w": "A Tensor. Must have the same type as x. The weight matrix.", "wci": "A Tensor. Must have the same type as x.\nThe weight matrix for input gate peephole connection.", "wcf": "A Tensor. Must have the same type as x.\nThe weight matrix for forget gate peephole connection.", "wco": "A Tensor. Must have the same type as x.\nThe weight matrix for output gate peephole connection.", "b": "A Tensor. Must have the same type as x. The bias vector.", "i": "A Tensor. Must have the same type as x.\nThe input gate over the whole time sequence.", "cs": "A Tensor. Must have the same type as x.\nThe cell state before the tanh over the whole time sequence.", "f": "A Tensor. Must have the same type as x.\nThe forget gate over the whole time sequence.", "o": "A Tensor. Must have the same type as x.\nThe output gate over the whole time sequence.", "ci": "A Tensor. Must have the same type as x.\nThe cell input over the whole time sequence.", "co": "A Tensor. Must have the same type as x.\nThe cell after the tanh over the whole time sequence.", "h": "A Tensor. Must have the same type as x.\nThe output h vector over the whole time sequence.", "cs_grad": "A Tensor. Must have the same type as x.\nThe current gradient of cs.", "h_grad": "A Tensor. Must have the same type as x.\nThe gradient of h vector.", "use_peephole": "A bool. Whether to use peephole weights.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (x_grad, cs_prev_grad, h_prev_grad, w_grad, wci_grad, wcf_grad, wco_grad, b_grad)."}, "tf.raw_ops.BlockLSTMGradV2": {"description": "Computes the LSTM cell backward propagation for the entire time sequence.", "Args": {"seq_len_max": "A Tensor of type int64.\nMaximum time length actually used by this input. Outputs are padded\nwith zeros beyond this length.", "x": "A Tensor. Must be one of the following types: half, float32.\nThe sequence input to the LSTM, shape (timelen, batch_size, num_inputs).", "cs_prev": "A Tensor. Must have the same type as x.\nValue of the initial cell state.", "h_prev": "A Tensor. Must have the same type as x.\nInitial output of cell (to be used for peephole).", "w": "A Tensor. Must have the same type as x. The weight matrix.", "wci": "A Tensor. Must have the same type as x.\nThe weight matrix for input gate peephole connection.", "wcf": "A Tensor. Must have the same type as x.\nThe weight matrix for forget gate peephole connection.", "wco": "A Tensor. Must have the same type as x.\nThe weight matrix for output gate peephole connection.", "b": "A Tensor. Must have the same type as x. The bias vector.", "i": "A Tensor. Must have the same type as x.\nThe input gate over the whole time sequence.", "cs": "A Tensor. Must have the same type as x.\nThe cell state before the tanh over the whole time sequence.", "f": "A Tensor. Must have the same type as x.\nThe forget gate over the whole time sequence.", "o": "A Tensor. Must have the same type as x.\nThe output gate over the whole time sequence.", "ci": "A Tensor. Must have the same type as x.\nThe cell input over the whole time sequence.", "co": "A Tensor. Must have the same type as x.\nThe cell after the tanh over the whole time sequence.", "h": "A Tensor. Must have the same type as x.\nThe output h vector over the whole time sequence.", "cs_grad": "A Tensor. Must have the same type as x.\nThe current gradient of cs.", "h_grad": "A Tensor. Must have the same type as x.\nThe gradient of h vector.", "use_peephole": "A bool. Whether to use peephole weights.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (x_grad, cs_prev_grad, h_prev_grad, w_grad, wci_grad, wcf_grad, wco_grad, b_grad)."}, "tf.raw_ops.BlockLSTMV2": {"description": "Computes the LSTM cell forward propagation for all the time steps.", "Args": {"seq_len_max": "A Tensor of type int64.\nMaximum time length actually used by this input. Outputs are padded\nwith zeros beyond this length.", "x": "A Tensor. Must be one of the following types: half, float32.\nThe sequence input to the LSTM, shape (timelen, batch_size, num_inputs).", "cs_prev": "A Tensor. Must have the same type as x.\nValue of the initial cell state.", "h_prev": "A Tensor. Must have the same type as x.\nInitial output of cell (to be used for peephole).", "w": "A Tensor. Must have the same type as x. The weight matrix.", "wci": "A Tensor. Must have the same type as x.\nThe weight matrix for input gate peephole connection.", "wcf": "A Tensor. Must have the same type as x.\nThe weight matrix for forget gate peephole connection.", "wco": "A Tensor. Must have the same type as x.\nThe weight matrix for output gate peephole connection.", "b": "A Tensor. Must have the same type as x. The bias vector.", "cell_clip": "An optional float. Defaults to 0.\nValue to clip the 'cs' value to.", "use_peephole": "An optional bool. Defaults to False.\nWhether to use peephole weights.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (i, cs, f, o, ci, co, h)."}, "tf.raw_ops.BoostedTreesAggregateStats": {"description": "Aggregates the summary of accumulated stats for the batch.", "Args": {"node_ids": "A Tensor of type int32.\nint32; Rank 1 Tensor containing node ids for each example, shape [batch_size].", "gradients": "A Tensor of type float32.\nfloat32; Rank 2 Tensor (shape=[batch_size, logits_dimension]) with gradients for each example.", "hessians": "A Tensor of type float32.\nfloat32; Rank 2 Tensor (shape=[batch_size, hessian_dimension]) with hessians for each example.", "feature": "A Tensor of type int32.\nint32; Rank 2 feature Tensors (shape=[batch_size, feature_dimension]).", "max_splits": "An int that is >= 1.\nint; the maximum number of splits possible in the whole tree.", "num_buckets": "An int that is >= 1.\nint; equals to the maximum possible value of bucketized feature.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type float32."}, "tf.raw_ops.BoostedTreesBucketize": {"description": "Bucketize each feature based on bucket boundaries.", "Args": {"float_values": "A list of Tensor objects with type float32.\nfloat; List of Rank 1 Tensor each containing float values for a single feature.", "bucket_boundaries": "A list with the same length as float_values of Tensor objects with type float32.\nfloat; List of Rank 1 Tensors each containing the bucket boundaries for a single\nfeature.", "name": "A name for the operation (optional)."}, "Returns": "A list with the same length as float_values of Tensor objects with type int32."}, "tf.raw_ops.BoostedTreesCalculateBestFeatureSplit": {"description": "Calculates gains for each feature and returns the best possible split information for the feature.", "Args": {"node_id_range": "A Tensor of type int32.\nA Rank 1 tensor (shape=[2]) to specify the range [first, last) of node ids to process within stats_summary_list. The nodes are iterated between the two nodes specified by the tensor, as like for node_id in range(node_id_range[0], node_id_range[1]) (Note that the last index node_id_range[1] is exclusive).", "stats_summary": "A Tensor of type float32.\nA Rank 4 tensor (#shape=[max_splits, feature_dims, bucket, stats_dims]) for accumulated stats summary (gradient/hessian) per node, per dimension, per buckets for each feature.\nThe first dimension of the tensor is the maximum number of splits, and thus not all elements of it will be used, but only the indexes specified by node_ids will be used.", "l1": "A Tensor of type float32.\nl1 regularization factor on leaf weights, per instance based.", "l2": "A Tensor of type float32.\nl2 regularization factor on leaf weights, per instance based.", "tree_complexity": "A Tensor of type float32.\nadjustment to the gain, per leaf based.", "min_node_weight": "A Tensor of type float32.\nminimum avg of hessians in a node before required for the node to be considered for splitting.", "logits_dimension": "An int that is >= 1.\nThe dimension of logit, i.e., number of classes.", "split_type": "An optional string from: \"inequality\", \"equality\". Defaults to \"inequality\".\nA string indicating if this Op should perform inequality split or equality split.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (node_ids, gains, feature_dimensions, thresholds, left_node_contribs, right_node_contribs, split_with_default_directions)."}, "tf.raw_ops.BoostedTreesCalculateBestFeatureSplitV2": {"description": "Calculates gains for each feature and returns the best possible split information for each node. However, if no split is found, then no split information is returned for that node.", "Args": {"node_id_range": "A Tensor of type int32.\nA Rank 1 tensor (shape=[2]) to specify the range [first, last) of node ids to process within stats_summary_list. The nodes are iterated between the two nodes specified by the tensor, as like for node_id in range(node_id_range[0], node_id_range[1]) (Note that the last index node_id_range[1] is exclusive).", "stats_summaries_list": "A list of at least 1 Tensor objects with type float32.\nA list of Rank 4 tensor (#shape=[max_splits, feature_dims, bucket, stats_dims]) for accumulated stats summary (gradient/hessian) per node, per dimension, per buckets for each feature.\nThe first dimension of the tensor is the maximum number of splits, and thus not all elements of it will be used, but only the indexes specified by node_ids will be used.", "split_types": "A Tensor of type string.\nA Rank 1 tensor indicating if this Op should perform inequality split or equality split per feature.", "candidate_feature_ids": "A Tensor of type int32.\nRank 1 tensor with ids for each feature. This is the real id of the feature.", "l1": "A Tensor of type float32.\nl1 regularization factor on leaf weights, per instance based.", "l2": "A Tensor of type float32.\nl2 regularization factor on leaf weights, per instance based.", "tree_complexity": "A Tensor of type float32.\nadjustment to the gain, per leaf based.", "min_node_weight": "A Tensor of type float32.\nminimum avg of hessians in a node before required for the node to be considered for splitting.", "logits_dimension": "An int that is >= 1.\nThe dimension of logit, i.e., number of classes.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (node_ids, gains, feature_ids, feature_dimensions, thresholds, left_node_contribs, right_node_contribs, split_with_default_directions)."}, "tf.raw_ops.BoostedTreesCalculateBestGainsPerFeature": {"description": "Calculates gains for each feature and returns the best possible split information for the feature.", "Args": {"node_id_range": "A Tensor of type int32.\nA Rank 1 tensor (shape=[2]) to specify the range [first, last) of node ids to process within stats_summary_list. The nodes are iterated between the two nodes specified by the tensor, as like for node_id in range(node_id_range[0], node_id_range[1]) (Note that the last index node_id_range[1] is exclusive).", "stats_summary_list": "A list of at least 1 Tensor objects with type float32.\nA list of Rank 3 tensor (#shape=[max_splits, bucket, 2]) for accumulated stats summary (gradient/hessian) per node per buckets for each feature. The first dimension of the tensor is the maximum number of splits, and thus not all elements of it will be used, but only the indexes specified by node_ids will be used.", "l1": "A Tensor of type float32.\nl1 regularization factor on leaf weights, per instance based.", "l2": "A Tensor of type float32.\nl2 regularization factor on leaf weights, per instance based.", "tree_complexity": "A Tensor of type float32.\nadjustment to the gain, per leaf based.", "min_node_weight": "A Tensor of type float32.\nminimum avg of hessians in a node before required for the node to be considered for splitting.", "max_splits": "An int that is >= 1.\nthe number of nodes that can be split in the whole tree. Used as a dimension of output tensors.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (node_ids_list, gains_list, thresholds_list, left_node_contribs_list, right_node_contribs_list)."}, "tf.raw_ops.BoostedTreesCenterBias": {"description": "Calculates the prior from the training data (the bias) and fills in the first node with the logits&#39; prior. Returns a boolean indicating whether to continue centering.", "Args": {"tree_ensemble_handle": "A Tensor of type resource.\nHandle to the tree ensemble.", "mean_gradients": "A Tensor of type float32.\nA tensor with shape=[logits_dimension] with mean of gradients for a first node.", "mean_hessians": "A Tensor of type float32.\nA tensor with shape=[logits_dimension] mean of hessians for a first node.", "l1": "A Tensor of type float32.\nl1 regularization factor on leaf weights, per instance based.", "l2": "A Tensor of type float32.\nl2 regularization factor on leaf weights, per instance based.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type bool."}, "tf.raw_ops.BoostedTreesCreateEnsemble": {"description": "Creates a tree ensemble model and returns a handle to it.", "Args": {"tree_ensemble_handle": "A Tensor of type resource.\nHandle to the tree ensemble resource to be created.", "stamp_token": "A Tensor of type int64.\nToken to use as the initial value of the resource stamp.", "tree_ensemble_serialized": "A Tensor of type string.\nSerialized proto of the tree ensemble.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.BoostedTreesCreateQuantileStreamResource": {"description": "Create the Resource for Quantile Streams.", "Args": {"quantile_stream_resource_handle": "A Tensor of type resource.\nresource; Handle to quantile stream resource.", "epsilon": "A Tensor of type float32.\nfloat; The required approximation error of the stream resource.", "num_streams": "A Tensor of type int64.\nint; The number of streams managed by the resource that shares the same epsilon.", "max_elements": "An optional int. Defaults to 1099511627776.\nint; The maximum number of data points that can be fed to the stream.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.BoostedTreesDeserializeEnsemble": {"description": "Deserializes a serialized tree ensemble config and replaces current tree", "Args": {"tree_ensemble_handle": "A Tensor of type resource.\nHandle to the tree ensemble.", "stamp_token": "A Tensor of type int64.\nToken to use as the new value of the resource stamp.", "tree_ensemble_serialized": "A Tensor of type string.\nSerialized proto of the ensemble.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.BoostedTreesEnsembleResourceHandleOp": {"description": "Creates a handle to a BoostedTreesEnsembleResource", "Args": {"container": "An optional string. Defaults to \"\".", "shared_name": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type resource."}, "tf.raw_ops.BoostedTreesExampleDebugOutputs": {"description": "Debugging/model interpretability outputs for each example.", "Args": {"tree_ensemble_handle": "A Tensor of type resource.", "bucketized_features": "A list of at least 1 Tensor objects with type int32.\nA list of rank 1 Tensors containing bucket id for each\nfeature.", "logits_dimension": "An int.\nscalar, dimension of the logits, to be used for constructing the protos in\nexamples_debug_outputs_serialized.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.BoostedTreesFlushQuantileSummaries": {"description": "Flush the quantile summaries from each quantile stream resource.", "Args": {"quantile_stream_resource_handle": "A Tensor of type resource.\nresource handle referring to a QuantileStreamResource.", "num_features": "An int that is >= 0.", "name": "A name for the operation (optional)."}, "Returns": "A list of num_features Tensor objects with type float32."}, "tf.raw_ops.BoostedTreesGetEnsembleStates": {"description": "Retrieves the tree ensemble resource stamp token, number of trees and growing statistics.", "Args": {"tree_ensemble_handle": "A Tensor of type resource.\nHandle to the tree ensemble.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (stamp_token, num_trees, num_finalized_trees, num_attempted_layers, last_layer_nodes_range)."}, "tf.raw_ops.BoostedTreesMakeQuantileSummaries": {"description": "Makes the summary of quantiles for the batch.", "Args": {"float_values": "A list of Tensor objects with type float32.\nfloat; List of Rank 1 Tensors each containing values for a single feature.", "example_weights": "A Tensor of type float32.\nfloat; Rank 1 Tensor with weights per instance.", "epsilon": "A Tensor of type float32.\nfloat; The required maximum approximation error.", "name": "A name for the operation (optional)."}, "Returns": "A list with the same length as float_values of Tensor objects with type float32."}, "tf.raw_ops.BoostedTreesMakeStatsSummary": {"description": "Makes the summary of accumulated stats for the batch.", "Args": {"node_ids": "A Tensor of type int32.\nint32 Rank 1 Tensor containing node ids, which each example falls into for the requested layer.", "gradients": "A Tensor of type float32.\nfloat32; Rank 2 Tensor (shape=[#examples, 1]) for gradients.", "hessians": "A Tensor of type float32.\nfloat32; Rank 2 Tensor (shape=[#examples, 1]) for hessians.", "bucketized_features_list": "A list of at least 1 Tensor objects with type int32.\nint32 list of Rank 1 Tensors, each containing the bucketized feature (for each feature column).", "max_splits": "An int that is >= 1.\nint; the maximum number of splits possible in the whole tree.", "num_buckets": "An int that is >= 1.\nint; equals to the maximum possible value of bucketized feature.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type float32."}, "tf.raw_ops.BoostedTreesPredict": {"description": "Runs multiple additive regression ensemble predictors on input instances and", "Args": {"tree_ensemble_handle": "A Tensor of type resource.", "bucketized_features": "A list of at least 1 Tensor objects with type int32.\nA list of rank 1 Tensors containing bucket id for each\nfeature.", "logits_dimension": "An int.\nscalar, dimension of the logits, to be used for partial logits\nshape.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type float32."}, "tf.raw_ops.BoostedTreesQuantileStreamResourceAddSummaries": {"description": "Add the quantile summaries to each quantile stream resource.", "Args": {"quantile_stream_resource_handle": "A Tensor of type resource.\nresource handle referring to a QuantileStreamResource.", "summaries": "A list of Tensor objects with type float32.\nstring; List of Rank 2 Tensor each containing the summaries for a single feature.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.BoostedTreesQuantileStreamResourceDeserialize": {"description": "Deserialize bucket boundaries and ready flag into current QuantileAccumulator.", "Args": {"quantile_stream_resource_handle": "A Tensor of type resource.\nresource handle referring to a QuantileStreamResource.", "bucket_boundaries": "A list of at least 1 Tensor objects with type float32.\nfloat; List of Rank 1 Tensors each containing the bucket boundaries for a feature.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.BoostedTreesQuantileStreamResourceFlush": {"description": "Flush the summaries for a quantile stream resource.", "Args": {"quantile_stream_resource_handle": "A Tensor of type resource.\nresource handle referring to a QuantileStreamResource.", "num_buckets": "A Tensor of type int64.\nint; approximate number of buckets unless using generate_quantiles.", "generate_quantiles": "An optional bool. Defaults to False.\nbool; If True, the output will be the num_quantiles for each stream where the ith\nentry is the ith quantile of the input with an approximation error of epsilon.\nDuplicate values may be present.\nIf False, the output will be the points in the histogram that we got which roughly\ntranslates to 1/epsilon boundaries and without any duplicates.\nDefault to False.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.BoostedTreesQuantileStreamResourceGetBucketBoundaries": {"description": "Generate the bucket boundaries for each feature based on accumulated summaries.", "Args": {"quantile_stream_resource_handle": "A Tensor of type resource.\nresource handle referring to a QuantileStreamResource.", "num_features": "An int that is >= 0.\ninferred int; number of features to get bucket boundaries for.", "name": "A name for the operation (optional)."}, "Returns": "A list of num_features Tensor objects with type float32."}, "tf.raw_ops.BoostedTreesQuantileStreamResourceHandleOp": {"description": "Creates a handle to a BoostedTreesQuantileStreamResource.", "Args": {"container": "An optional string. Defaults to \"\".", "shared_name": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type resource."}, "tf.raw_ops.BoostedTreesSerializeEnsemble": {"description": "Serializes the tree ensemble to a proto.", "Args": {"tree_ensemble_handle": "A Tensor of type resource.\nHandle to the tree ensemble.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (stamp_token, tree_ensemble_serialized)."}, "tf.raw_ops.BoostedTreesSparseAggregateStats": {"description": "Aggregates the summary of accumulated stats for the batch.", "Args": {"node_ids": "A Tensor of type int32.\nint32; Rank 1 Tensor containing node ids for each example, shape [batch_size].", "gradients": "A Tensor of type float32.\nfloat32; Rank 2 Tensor (shape=[batch_size, logits_dimension]) with gradients for each example.", "hessians": "A Tensor of type float32.\nfloat32; Rank 2 Tensor (shape=[batch_size, hessian_dimension]) with hessians for each example.", "feature_indices": "A Tensor of type int32.\nint32; Rank 2 indices of feature sparse Tensors (shape=[number of sparse entries, 2]).\nNumber of sparse entries across all instances from the batch. The first value is\nthe index of the instance, the second is dimension of the feature. The second axis\ncan only have 2 values, i.e., the input dense version of Tensor can only be matrix.", "feature_values": "A Tensor of type int32.\nint32; Rank 1 values of feature sparse Tensors (shape=[number of sparse entries]).\nNumber of sparse entries across all instances from the batch. The first value is\nthe index of the instance, the second is dimension of the feature.", "feature_shape": "A Tensor of type int32.\nint32; Rank 1 dense shape of feature sparse Tensors (shape=[2]).\nThe first axis can only have 2 values, [batch_size, feature_dimension].", "max_splits": "An int that is >= 1.\nint; the maximum number of splits possible in the whole tree.", "num_buckets": "An int that is >= 1.\nint; equals to the maximum possible value of bucketized feature + 1.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (stats_summary_indices, stats_summary_values, stats_summary_shape)."}, "tf.raw_ops.BoostedTreesSparseCalculateBestFeatureSplit": {"description": "Calculates gains for each feature and returns the best possible split information for the feature.", "Args": {"node_id_range": "A Tensor of type int32.\nA Rank 1 tensor (shape=[2]) to specify the range [first, last) of node ids to process within stats_summary_list. The nodes are iterated between the two nodes specified by the tensor, as like for node_id in range(node_id_range[0], node_id_range[1]) (Note that the last index node_id_range[1] is exclusive).", "stats_summary_indices": "A Tensor of type int32.\nA Rank 2 int64 tensor of dense shape N, 4 for accumulated stats summary (gradient/hessian) per node per bucket for each feature. The second dimension contains node id, feature dimension, bucket id, and stats dim.\nstats dim is the sum of logits dimension and hessian dimension, hessian dimension can either be logits dimension if diagonal hessian is used, or logits dimension^2 if full hessian is used.", "stats_summary_values": "A Tensor of type float32.\nA Rank 1 float tensor of dense shape N, which supplies the values for each element in summary_indices.", "stats_summary_shape": "A Tensor of type int32.\nA Rank 1 float tensor of dense shape [4], which specifies the dense shape of the sparse tensor, which is [num tree nodes, feature dimensions, num buckets, stats dim].", "l1": "A Tensor of type float32.\nl1 regularization factor on leaf weights, per instance based.", "l2": "A Tensor of type float32.\nl2 regularization factor on leaf weights, per instance based.", "tree_complexity": "A Tensor of type float32.\nadjustment to the gain, per leaf based.", "min_node_weight": "A Tensor of type float32.\nminimum avg of hessians in a node before required for the node to be considered for splitting.", "logits_dimension": "An int that is >= 1.\nThe dimension of logit, i.e., number of classes.", "split_type": "An optional string from: \"inequality\". Defaults to \"inequality\".\nA string indicating if this Op should perform inequality split or equality split.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (node_ids, gains, feature_dimensions, thresholds, left_node_contribs, right_node_contribs, split_with_default_directions)."}, "tf.raw_ops.BoostedTreesTrainingPredict": {"description": "Runs multiple additive regression ensemble predictors on input instances and", "Args": {"tree_ensemble_handle": "A Tensor of type resource.", "cached_tree_ids": "A Tensor of type int32.\nRank 1 Tensor containing cached tree ids which is the starting\ntree of prediction.", "cached_node_ids": "A Tensor of type int32.\nRank 1 Tensor containing cached node id which is the starting\nnode of prediction.", "bucketized_features": "A list of at least 1 Tensor objects with type int32.\nA list of rank 1 Tensors containing bucket id for each\nfeature.", "logits_dimension": "An int.\nscalar, dimension of the logits, to be used for partial logits\nshape.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (partial_logits, tree_ids, node_ids)."}, "tf.raw_ops.BoostedTreesUpdateEnsemble": {"description": "Updates the tree ensemble by either adding a layer to the last tree being grown", "Args": {"tree_ensemble_handle": "A Tensor of type resource.\nHandle to the ensemble variable.", "feature_ids": "A Tensor of type int32.\nRank 1 tensor with ids for each feature. This is the real id of\nthe feature that will be used in the split.", "node_ids": "A list of Tensor objects with type int32.\nList of rank 1 tensors representing the nodes for which this feature\nhas a split.", "gains": "A list with the same length as node_ids of Tensor objects with type float32.\nList of rank 1 tensors representing the gains for each of the feature's\nsplit.", "thresholds": "A list with the same length as node_ids of Tensor objects with type int32.\nList of rank 1 tensors representing the thesholds for each of the\nfeature's split.", "left_node_contribs": "A list with the same length as node_ids of Tensor objects with type float32.\nList of rank 2 tensors with left leaf contribs for each of\nthe feature's splits. Will be added to the previous node values to constitute\nthe values of the left nodes.", "right_node_contribs": "A list with the same length as node_ids of Tensor objects with type float32.\nList of rank 2 tensors with right leaf contribs for each\nof the feature's splits. Will be added to the previous node values to constitute\nthe values of the right nodes.", "max_depth": "A Tensor of type int32. Max depth of the tree to build.", "learning_rate": "A Tensor of type float32.\nshrinkage const for each new tree.", "pruning_mode": "An int that is >= 0.\n0-No pruning, 1-Pre-pruning, 2-Post-pruning.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.BoostedTreesUpdateEnsembleV2": {"description": "Updates the tree ensemble by adding a layer to the last tree being grown", "Args": {"tree_ensemble_handle": "A Tensor of type resource.\nHandle to the ensemble variable.", "feature_ids": "A list of at least 1 Tensor objects with type int32.\nRank 1 tensor with ids for each feature. This is the real id of\nthe feature that will be used in the split.", "dimension_ids": "A list of Tensor objects with type int32.\nList of rank 1 tensors representing the dimension in each feature.", "node_ids": "A list with the same length as dimension_ids of Tensor objects with type int32.\nList of rank 1 tensors representing the nodes for which this feature\nhas a split.", "gains": "A list with the same length as dimension_ids of Tensor objects with type float32.\nList of rank 1 tensors representing the gains for each of the feature's\nsplit.", "thresholds": "A list with the same length as dimension_ids of Tensor objects with type int32.\nList of rank 1 tensors representing the thesholds for each of the\nfeature's split.", "left_node_contribs": "A list with the same length as dimension_ids of Tensor objects with type float32.\nList of rank 2 tensors with left leaf contribs for each of\nthe feature's splits. Will be added to the previous node values to constitute\nthe values of the left nodes.", "right_node_contribs": "A list with the same length as dimension_ids of Tensor objects with type float32.\nList of rank 2 tensors with right leaf contribs for each\nof the feature's splits. Will be added to the previous node values to constitute\nthe values of the right nodes.", "split_types": "A list with the same length as dimension_ids of Tensor objects with type string.\nList of rank 1 tensors representing the split type for each feature.", "max_depth": "A Tensor of type int32. Max depth of the tree to build.", "learning_rate": "A Tensor of type float32.\nshrinkage const for each new tree.", "pruning_mode": "A Tensor of type int32.\n0-No pruning, 1-Pre-pruning, 2-Post-pruning.", "logits_dimension": "An optional int. Defaults to 1.\nscalar, dimension of the logits", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.BroadcastArgs": {"description": "Return the shape of s0 op s1 with broadcast.", "Args": {"s0": "A Tensor. Must be one of the following types: int32, int64.", "s1": "A Tensor. Must have the same type as s0.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as s0."}, "tf.raw_ops.BroadcastGradientArgs": {"description": "Return the reduction indices for computing gradients of s0 op s1 with broadcast.", "Args": {"s0": "A Tensor. Must be one of the following types: int32, int64.", "s1": "A Tensor. Must have the same type as s0.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (r0, r1)."}, "tf.raw_ops.BroadcastTo": {"description": "Broadcast an array for a compatible shape.", "Args": {"input": "A Tensor. A Tensor to broadcast.", "shape": "A Tensor. Must be one of the following types: int32, int64.\nAn 1-D int Tensor. The shape of the desired output.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.Bucketize": {"description": "Bucketizes &#39;input&#39; based on &#39;boundaries&#39;.", "Args": {"input": "A Tensor. Must be one of the following types: int32, int64, float32, float64.\nAny shape of Tensor contains with int or float type.", "boundaries": "A list of floats.\nA sorted list of floats gives the boundary of the buckets.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int32."}, "tf.raw_ops.BytesProducedStatsDataset": {"description": "Records the bytes size of each element of input_dataset in a StatsAggregator.", "Args": {"input_dataset": "A Tensor of type variant.", "tag": "A Tensor of type string.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.CSRSparseMatrixComponents": {"description": "Reads out the CSR components at batch index.", "Args": {"csr_sparse_matrix": "A Tensor of type variant.\nA batched CSRSparseMatrix.", "index": "A Tensor of type int32.\nThe index in csr_sparse_matrix's batch.", "type": "A tf.DType from: tf.float32, tf.float64, tf.complex64, tf.complex128.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (row_ptrs, col_inds, values)."}, "tf.raw_ops.CSRSparseMatrixToDense": {"description": "Convert a (possibly batched) CSRSparseMatrix to dense.", "Args": {"sparse_input": "A Tensor of type variant. A batched CSRSparseMatrix.", "type": "A tf.DType from: tf.float32, tf.float64, tf.complex64, tf.complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type type."}, "tf.raw_ops.CSRSparseMatrixToSparseTensor": {"description": "Converts a (possibly batched) CSRSparesMatrix to a SparseTensor.", "Args": {"sparse_matrix": "A Tensor of type variant.\nA (possibly batched) CSRSparseMatrix.", "type": "A tf.DType from: tf.float32, tf.float64, tf.complex64, tf.complex128.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (indices, values, dense_shape)."}, "tf.raw_ops.CSVDataset": {"Args": {"filenames": "A Tensor of type string.", "compression_type": "A Tensor of type string.", "buffer_size": "A Tensor of type int64.", "header": "A Tensor of type bool.", "field_delim": "A Tensor of type string.", "use_quote_delim": "A Tensor of type bool.", "na_value": "A Tensor of type string.", "select_cols": "A Tensor of type int64.", "record_defaults": "A list of Tensor objects with types from: float32, float64, int32, int64, string.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.CSVDatasetV2": {"Args": {"filenames": "A Tensor of type string.", "compression_type": "A Tensor of type string.", "buffer_size": "A Tensor of type int64.", "header": "A Tensor of type bool.", "field_delim": "A Tensor of type string.", "use_quote_delim": "A Tensor of type bool.", "na_value": "A Tensor of type string.", "select_cols": "A Tensor of type int64.", "record_defaults": "A list of Tensor objects with types from: float32, float64, int32, int64, string.", "exclude_cols": "A Tensor of type int64.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.CTCBeamSearchDecoder": {"description": "Performs beam search decoding on the logits given in input.", "Args": {"inputs": "A Tensor. Must be one of the following types: float32, float64.\n3-D, shape: (max_time x batch_size x num_classes), the logits.", "sequence_length": "A Tensor of type int32.\nA vector containing sequence lengths, size (batch).", "beam_width": "An int that is >= 1.\nA scalar >= 0 (beam search beam width).", "top_paths": "An int that is >= 1.\nA scalar >= 0, <= beam_width (controls output size).", "merge_repeated": "An optional bool. Defaults to True.\nIf true, merge repeated classes in output.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (decoded_indices, decoded_values, decoded_shape, log_probability)."}, "tf.raw_ops.CTCGreedyDecoder": {"description": "Performs greedy decoding on the logits given in inputs.", "Args": {"inputs": "A Tensor. Must be one of the following types: float32, float64.\n3-D, shape: (max_time x batch_size x num_classes), the logits.", "sequence_length": "A Tensor of type int32.\nA vector containing sequence lengths, size (batch_size).", "merge_repeated": "An optional bool. Defaults to False.\nIf True, merge repeated classes in output.", "blank_index": "An optional int. Defaults to -1.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (decoded_indices, decoded_values, decoded_shape, log_probability)."}, "tf.raw_ops.CTCLoss": {"description": "Calculates the CTC Loss (log probability) for each batch entry. Also calculates", "Args": {"inputs": "A Tensor. Must be one of the following types: float32, float64.\n3-D, shape: (max_time x batch_size x num_classes), the logits.", "labels_indices": "A Tensor of type int64.\nThe indices of a SparseTensor<int32, 2>.\nlabels_indices(i, :) == [b, t] means labels_values(i) stores the id for\n(batch b, time t).", "labels_values": "A Tensor of type int32.\nThe values (labels) associated with the given batch and time.", "sequence_length": "A Tensor of type int32.\nA vector containing sequence lengths (batch).", "preprocess_collapse_repeated": "An optional bool. Defaults to False.\nScalar, if true then repeated labels are\ncollapsed prior to the CTC calculation.", "ctc_merge_repeated": "An optional bool. Defaults to True.\nScalar.  If set to false, during CTC calculation\nrepeated non-blank labels will not be merged and are interpreted as\nindividual labels.  This is a simplified version of CTC.", "ignore_longer_outputs_than_inputs": "An optional bool. Defaults to False.\nScalar. If set to true, during CTC\ncalculation, items that have longer output sequences than input sequences\nare skipped: they don't contribute to the loss term and have zero-gradient.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (loss, gradient)."}, "tf.raw_ops.CTCLossV2": {"description": "Calculates the CTC Loss (log probability) for each batch entry. Also calculates", "Args": {"inputs": "A Tensor of type float32.\n3-D, shape: (max_time x batch_size x num_classes), the logits. Default blank\nlabel is 0 rather num_classes - 1.", "labels_indices": "A Tensor of type int64.\nThe indices of a SparseTensor<int32, 2>.\nlabels_indices(i, :) == [b, t] means labels_values(i) stores the id for\n(batch b, time t).", "labels_values": "A Tensor of type int32.\nThe values (labels) associated with the given batch and time.", "sequence_length": "A Tensor of type int32.\nA vector containing sequence lengths (batch).", "preprocess_collapse_repeated": "An optional bool. Defaults to False.\nScalar, if true then repeated labels are\ncollapsed prior to the CTC calculation.", "ctc_merge_repeated": "An optional bool. Defaults to True.\nScalar.  If set to false, during CTC calculation\nrepeated non-blank labels will not be merged and are interpreted as\nindividual labels.  This is a simplified version of CTC.", "ignore_longer_outputs_than_inputs": "An optional bool. Defaults to False.\nScalar. If set to true, during CTC\ncalculation, items that have longer output sequences than input sequences\nare skipped: they don't contribute to the loss term and have zero-gradient.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (loss, gradient)."}, "tf.raw_ops.CacheDataset": {"description": "Creates a dataset that caches elements from input_dataset.", "Args": {"input_dataset": "A Tensor of type variant.", "filename": "A Tensor of type string.\nA path on the filesystem where we should cache the dataset. Note: this\nwill be a directory.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.CacheDatasetV2": {"Args": {"input_dataset": "A Tensor of type variant.", "filename": "A Tensor of type string.", "cache": "A Tensor of type resource.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.Case": {"description": "An n-way switch statement which calls a single branch function.", "Args": {"branch_index": "A Tensor of type int32.\nThe branch selector, an int32 Tensor.", "input": "A list of Tensor objects.\nA list of input tensors passed to the branch function.", "Tout": "A list of tf.DTypes. A list of output types.", "branches": "A list of functions decorated with @Defun that has length >= 1.\nA list of functions each of which takes 'inputs' and returns a list of\ntensors, whose types are the same as what every other branch returns.", "output_shapes": "An optional list of shapes (each a tf.TensorShape or list of ints). Defaults to [].", "name": "A name for the operation (optional)."}, "Returns": "A list of Tensor objects of type Tout."}, "tf.raw_ops.Cast": {"description": "Cast x of type SrcT to y of DstT.", "Args": {"x": "A Tensor.", "DstT": "A tf.DType.", "Truncate": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type DstT."}, "tf.raw_ops.Ceil": {"description": "Returns element-wise smallest integer not less than x.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.CheckNumerics": {"description": "Checks a tensor for NaN and Inf values.", "Args": {"tensor": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "message": "A string. Prefix of the error message.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as tensor."}, "tf.raw_ops.CheckNumericsV2": {"description": "Checks a tensor for NaN, -Inf and &#43;Inf values.", "Args": {"tensor": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "message": "A string. Prefix of the error message.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as tensor."}, "tf.raw_ops.Cholesky": {"description": "Computes the Cholesky decomposition of one or more square matrices.", "Args": {"input": "A Tensor. Must be one of the following types: float64, float32, half, complex64, complex128.\nShape is [..., M, M].", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.CholeskyGrad": {"description": "Computes the reverse mode backpropagated gradient of the Cholesky algorithm.", "Args": {"l": "A Tensor. Must be one of the following types: half, float32, float64.\nOutput of batch Cholesky algorithm l = cholesky(A). Shape is [..., M, M].\nAlgorithm depends only on lower triangular part of the innermost matrices of\nthis tensor.", "grad": "A Tensor. Must have the same type as l.\ndf/dl where f is some scalar function. Shape is [..., M, M].\nAlgorithm depends only on lower triangular part of the innermost matrices of\nthis tensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as l."}, "tf.raw_ops.ChooseFastestBranchDataset": {"Args": {"input_dataset": "A Tensor of type variant.", "ratio_numerator": "A Tensor of type int64.", "ratio_denominator": "A Tensor of type int64.", "other_arguments": "A list of Tensor objects.", "num_elements_per_branch": "An int that is >= 1.", "branches": "A list of functions decorated with @Defun that has length >= 1.", "other_arguments_lengths": "A list of ints that has length >= 1.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ChooseFastestDataset": {"Args": {"input_datasets": "A list of at least 2 Tensor objects with type variant.", "num_experiments": "An int.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ClipByValue": {"description": "Clips tensor values to a specified min and max.", "Args": {"t": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nA Tensor.", "clip_value_min": "A Tensor. Must have the same type as t.\nA 0-D (scalar) Tensor, or a Tensor with the same shape\nas t. The minimum value to clip by.", "clip_value_max": "A Tensor. Must have the same type as t.\nA 0-D (scalar) Tensor, or a Tensor with the same shape\nas t. The maximum value to clip by.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as t."}, "tf.raw_ops.CloseSummaryWriter": {"Args": {"writer": "A Tensor of type resource.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.CollectiveAllToAllV3": {"description": "Mutually exchanges multiple tensors of identical type and shape.", "Args": {"input": "A Tensor. Must be one of the following types: bfloat16, float32, half, float64, int32, int64.", "communicator": "A Tensor of type resource.", "group_assignment": "A Tensor of type int32.", "timeout_seconds": "An optional float. Defaults to 0.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.CollectiveAssignGroupV2": {"description": "Assign group keys based on group assignment.", "Args": {"group_assignment": "A Tensor of type int32.", "device_index": "A Tensor of type int32.", "base_key": "A Tensor of type int32.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (group_size, group_key)."}, "tf.raw_ops.CollectiveBcastRecv": {"description": "Receives a tensor value broadcast from another device.", "Args": {"T": "A tf.DType from: tf.bool, tf.float32, tf.half, tf.float64, tf.int32, tf.int64.", "group_size": "An int.", "group_key": "An int.", "instance_key": "An int.", "shape": "A tf.TensorShape or list of ints.", "communication_hint": "An optional string. Defaults to \"auto\".", "timeout_seconds": "An optional float. Defaults to 0.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type T."}, "tf.raw_ops.CollectiveBcastRecvV2": {"description": "Receives a tensor value broadcast from another device.", "Args": {"group_size": "A Tensor of type int32.", "group_key": "A Tensor of type int32.", "instance_key": "A Tensor of type int32.", "shape": "A Tensor. Must be one of the following types: int32, int64.", "T": "A tf.DType from: tf.bool, tf.float32, tf.half, tf.float64, tf.int32, tf.int64.", "communication_hint": "An optional string. Defaults to \"auto\".", "timeout_seconds": "An optional float. Defaults to 0.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type T."}, "tf.raw_ops.CollectiveBcastSend": {"description": "Broadcasts a tensor value to one or more other devices.", "Args": {"input": "A Tensor. Must be one of the following types: bool, float32, half, float64, int32, int64.", "group_size": "An int.", "group_key": "An int.", "instance_key": "An int.", "shape": "A tf.TensorShape or list of ints.", "communication_hint": "An optional string. Defaults to \"auto\".", "timeout_seconds": "An optional float. Defaults to 0.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.CollectiveBcastSendV2": {"description": "Broadcasts a tensor value to one or more other devices.", "Args": {"input": "A Tensor. Must be one of the following types: bool, float32, half, float64, int32, int64.", "group_size": "A Tensor of type int32.", "group_key": "A Tensor of type int32.", "instance_key": "A Tensor of type int32.", "communication_hint": "An optional string. Defaults to \"auto\".", "timeout_seconds": "An optional float. Defaults to 0.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.CollectiveGather": {"description": "Mutually accumulates multiple tensors of identical type and shape.", "Args": {"input": "A Tensor. Must be one of the following types: float32, half, float64, int32, int64.", "group_size": "An int.", "group_key": "An int.", "instance_key": "An int.", "shape": "A tf.TensorShape or list of ints.", "communication_hint": "An optional string. Defaults to \"auto\".", "timeout_seconds": "An optional float. Defaults to 0.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.CollectiveGatherV2": {"description": "Mutually accumulates multiple tensors of identical type and shape.", "Args": {"input": "A Tensor. Must be one of the following types: float32, half, float64, int32, int64.", "group_size": "A Tensor of type int32.", "group_key": "A Tensor of type int32.", "instance_key": "A Tensor of type int32.", "ordering_token": "A list of Tensor objects with type resource.", "communication_hint": "An optional string. Defaults to \"auto\".", "timeout_seconds": "An optional float. Defaults to 0.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.CollectiveInitializeCommunicator": {"description": "Initializes a group for collective operations.", "Args": {"group_key": "A Tensor of type int32.", "rank": "A Tensor of type int32.", "group_size": "A Tensor of type int32.", "communication_hint": "An optional string. Defaults to \"auto\".", "timeout_seconds": "An optional float. Defaults to 0.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type resource."}, "tf.raw_ops.CollectivePermute": {"description": "An Op to permute tensors across replicated TPU instances.", "Args": {"input": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nThe local input to be permuted. Currently only supports float and\nbfloat16.", "source_target_pairs": "A Tensor of type int32.\nA tensor with shape [num_pairs, 2].", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.CollectiveReduce": {"description": "Mutually reduces multiple tensors of identical type and shape.", "Args": {"input": "A Tensor. Must be one of the following types: bfloat16, float32, half, float64, int32, int64.", "group_size": "An int.", "group_key": "An int.", "instance_key": "An int.", "merge_op": "A string from: \"Min\", \"Max\", \"Mul\", \"Add\".", "final_op": "A string from: \"Id\", \"Div\".", "subdiv_offsets": "A list of ints.", "wait_for": "An optional list of ints. Defaults to [].", "communication_hint": "An optional string. Defaults to \"auto\".", "timeout_seconds": "An optional float. Defaults to 0.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.CollectiveReduceV2": {"description": "Mutually reduces multiple tensors of identical type and shape.", "Args": {"input": "A Tensor. Must be one of the following types: bfloat16, float32, half, float64, int32, int64.", "group_size": "A Tensor of type int32.", "group_key": "A Tensor of type int32.", "instance_key": "A Tensor of type int32.", "ordering_token": "A list of Tensor objects with type resource.", "merge_op": "A string from: \"Min\", \"Max\", \"Mul\", \"Add\".", "final_op": "A string from: \"Id\", \"Div\".", "communication_hint": "An optional string. Defaults to \"auto\".", "timeout_seconds": "An optional float. Defaults to 0.", "max_subdivs_per_device": "An optional int. Defaults to -1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.CollectiveReduceV3": {"description": "Mutually reduces multiple tensors of identical type and shape.", "Args": {"input": "A Tensor. Must be one of the following types: bfloat16, float32, half, float64, int32, int64.", "communicator": "A Tensor of type resource.", "group_assignment": "A Tensor of type int32.", "reduction": "A string from: \"Min\", \"Max\", \"Mul\", \"Add\".", "timeout_seconds": "An optional float. Defaults to 0.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.CombinedNonMaxSuppression": {"description": "Greedily selects a subset of bounding boxes in descending order of score,", "Args": {"boxes": "A Tensor of type float32.\nA 4-D float tensor of shape [batch_size, num_boxes, q, 4]. If q is 1 then\nsame boxes are used for all classes otherwise, if q is equal to number of\nclasses, class-specific boxes are used.", "scores": "A Tensor of type float32.\nA 3-D float tensor of shape [batch_size, num_boxes, num_classes]\nrepresenting a single score corresponding to each box (each row of boxes).", "max_output_size_per_class": "A Tensor of type int32.\nA scalar integer tensor representing the maximum number of\nboxes to be selected by non max suppression per class", "max_total_size": "A Tensor of type int32.\nAn int32 scalar representing the maximum number of boxes retained over all\nclasses. Note that setting this value to a large number may result in OOM error\ndepending on the system workload.", "iou_threshold": "A Tensor of type float32.\nA 0-D float tensor representing the threshold for deciding whether\nboxes overlap too much with respect to IOU.", "score_threshold": "A Tensor of type float32.\nA 0-D float tensor representing the threshold for deciding when to remove\nboxes based on score.", "pad_per_class": "An optional bool. Defaults to False.\nIf false, the output nmsed boxes, scores and classes\nare padded/clipped to max_total_size. If true, the\noutput nmsed boxes, scores and classes are padded to be of length\nmax_size_per_class*num_classes, unless it exceeds max_total_size in\nwhich case it is clipped to max_total_size. Defaults to false.", "clip_boxes": "An optional bool. Defaults to True.\nIf true, assume the box coordinates are between [0, 1] and clip the output boxes\nif they fall beyond [0, 1]. If false, do not do clipping and output the box\ncoordinates as it is.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (nmsed_boxes, nmsed_scores, nmsed_classes, valid_detections)."}, "tf.raw_ops.Complex": {"description": "Converts two real numbers to a complex number.", "Args": {"real": "A Tensor. Must be one of the following types: float32, float64.", "imag": "A Tensor. Must have the same type as real.", "Tout": "An optional tf.DType from: tf.complex64, tf.complex128. Defaults to tf.complex64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type Tout."}, "tf.raw_ops.ComplexAbs": {"description": "Computes the complex absolute value of a tensor.", "Args": {"x": "A Tensor. Must be one of the following types: complex64, complex128.", "Tout": "An optional tf.DType from: tf.float32, tf.float64. Defaults to tf.float32.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type Tout."}, "tf.raw_ops.CompositeTensorVariantFromComponents": {"description": "Encodes an ExtensionType value into a variant scalar Tensor.", "Args": {"components": "A list of Tensor objects.\nThe component tensors for the extension type value.", "metadata": "A string.\nString serialization for the TypeSpec.  (Note: the encoding for the TypeSpec\nmay change in future versions of TensorFlow.)", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.CompositeTensorVariantToComponents": {"description": "Decodes a variant scalar Tensor into an ExtensionType value.", "Args": {"encoded": "A Tensor of type variant.\nA scalar variant Tensor containing an encoded ExtensionType value.", "metadata": "A string.\nString serialization for the TypeSpec.  Must be compatible with the\nTypeSpec contained in encoded.  (Note: the encoding for the TypeSpec\nmay change in future versions of TensorFlow.)", "Tcomponents": "A list of tf.DTypes. Expected dtypes for components.", "name": "A name for the operation (optional)."}, "Returns": "A list of Tensor objects of type Tcomponents."}, "tf.raw_ops.CompressElement": {"description": "Compresses a dataset element.", "Args": {"components": "A list of Tensor objects.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ComputeAccidentalHits": {"description": "Computes the ids of the positions in sampled_candidates that match true_labels.", "Args": {"true_classes": "A Tensor of type int64.\nThe true_classes output of UnpackSparseLabels.", "sampled_candidates": "A Tensor of type int64.\nThe sampled_candidates output of CandidateSampler.", "num_true": "An int. Number of true labels per context.", "seed": "An optional int. Defaults to 0.\nIf either seed or seed2 are set to be non-zero, the random number\ngenerator is seeded by the given seed.  Otherwise, it is seeded by a\nrandom seed.", "seed2": "An optional int. Defaults to 0.\nAn second seed to avoid seed collision.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (indices, ids, weights)."}, "tf.raw_ops.ComputeBatchSize": {"description": "Computes the static batch size of a dataset sans partial batches.", "Args": {"input_dataset": "A Tensor of type variant.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int64."}, "tf.raw_ops.Concat": {"description": "Concatenates tensors along one dimension.", "Args": {"concat_dim": "A Tensor of type int32.\n0-D.  The dimension along which to concatenate.  Must be in the\nrange [0, rank(values)).", "values": "A list of at least 2 Tensor objects with the same type.\nThe N Tensors to concatenate. Their ranks and types must match,\nand their sizes must match in all dimensions except concat_dim.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as values."}, "tf.raw_ops.ConcatOffset": {"description": "Computes offsets of concat inputs within its output.", "Args": {"concat_dim": "A Tensor of type int32.\nThe dimension along which to concatenate.", "shape": "A list of at least 2 Tensor objects with type int32.\nThe N int32 vectors representing shape of tensors being concatenated.", "name": "A name for the operation (optional)."}, "Returns": "A list with the same length as shape of Tensor objects with type int32."}, "tf.raw_ops.ConcatV2": {"description": "Concatenates tensors along one dimension.", "Args": {"values": "A list of at least 2 Tensor objects with the same type.\nList of N Tensors to concatenate. Their ranks and types must match,\nand their sizes must match in all dimensions except concat_dim.", "axis": "A Tensor. Must be one of the following types: int32, int64.\n0-D.  The dimension along which to concatenate.  Must be in the\nrange [-rank(values), rank(values)).", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as values."}, "tf.raw_ops.ConcatenateDataset": {"description": "Creates a dataset that concatenates input_dataset with another_dataset.", "Args": {"input_dataset": "A Tensor of type variant.", "another_dataset": "A Tensor of type variant.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ConditionalAccumulator": {"description": "A conditional accumulator for aggregating gradients.", "Args": {"dtype": "A tf.DType from: tf.float32, tf.float64, tf.int32, tf.uint8, tf.int16, tf.int8, tf.complex64, tf.int64, tf.qint8, tf.quint8, tf.qint32, tf.bfloat16, tf.uint16, tf.complex128, tf.half, tf.uint32, tf.uint64.\nThe type of the value being accumulated.", "shape": "A tf.TensorShape or list of ints.\nThe shape of the values, can be [], in which case shape is unknown.", "container": "An optional string. Defaults to \"\".\nIf non-empty, this accumulator is placed in the given container.\nOtherwise, a default container is used.", "shared_name": "An optional string. Defaults to \"\".\nIf non-empty, this accumulator will be shared under the\ngiven name across multiple sessions.", "reduction_type": "An optional string from: \"MEAN\", \"SUM\". Defaults to \"MEAN\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type mutable string."}, "tf.raw_ops.ConfigureDistributedTPU": {"description": "Sets up the centralized structures for a distributed TPU system.", "Args": {"embedding_config": "An optional string. Defaults to \"\".\nReserved. Do not use.", "tpu_embedding_config": "An optional string. Defaults to \"\".\nSerialized tensorflow.tpu.TPUEmbeddingConfiguration that\ndescribes the embedding lookups of the program.", "is_global_init": "An optional bool. Defaults to False.\nReserved. Do not use.", "enable_whole_mesh_compilations": "An optional bool. Defaults to False.", "compilation_failure_closes_chips": "An optional bool. Defaults to True.", "tpu_cancellation_closes_chips": "An optional int. Defaults to 0.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.ConfigureTPUEmbedding": {"description": "Sets up TPUEmbedding in a distributed TPU system.", "Args": {"config": "A string.\nSerialized tensorflow.tpu.TPUEmbeddingConfiguration that\ndescribes the embedding lookups of the program.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.Conj": {"description": "Returns the complex conjugate of a complex number.", "Args": {"input": "A Tensor. Must be one of the following types: complex64, complex128, variant.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.ConjugateTranspose": {"description": "Shuffle dimensions of x according to a permutation and conjugate the result.", "Args": {"x": "A Tensor.", "perm": "A Tensor. Must be one of the following types: int32, int64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.Const": {"description": "Returns a constant tensor.", "Args": {"value": "A tf.TensorProto. Attr value is the tensor to return.", "dtype": "A tf.DType.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dtype."}, "tf.raw_ops.ConsumeMutexLock": {"description": "This op consumes a lock created by MutexLock.", "Args": {"mutex_lock": "A Tensor of type variant.\nA tensor returned by MutexLock.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ControlTrigger": {"description": "Does nothing. Serves as a control trigger for scheduling.", "Args": {"name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.Conv2D": {"description": "Computes a 2-D convolution given 4-D input and filter tensors.", "Args": {"input": "A Tensor. Must be one of the following types: half, bfloat16, float32, float64, int32.\nA 4-D tensor. The dimension order is interpreted according to the value\nof data_format, see below for details.", "filter": "A Tensor. Must have the same type as input.\nA 4-D tensor of shape\n[filter_height, filter_width, in_channels, out_channels]", "strides": "A list of ints.\n1-D tensor of length 4.  The stride of the sliding window for each\ndimension of input. The dimension order is determined by the value of\ndata_format, see below for details.", "padding": "A string from: \"SAME\", \"VALID\", \"EXPLICIT\".\nThe type of padding algorithm to use.", "use_cudnn_on_gpu": "An optional bool. Defaults to True.", "explicit_paddings": "An optional list of ints. Defaults to [].\nIf padding is \"EXPLICIT\", the list of explicit padding amounts. For the ith\ndimension, the amount of padding inserted before and after the dimension is\nexplicit_paddings[2 * i] and explicit_paddings[2 * i + 1], respectively. If\npadding is not \"EXPLICIT\", explicit_paddings must be empty.", "data_format": "An optional string from: \"NHWC\", \"NCHW\". Defaults to \"NHWC\".\nSpecify the data format of the input and output data. With the\ndefault format \"NHWC\", the data is stored in the order of:\n    [batch, height, width, channels].\nAlternatively, the format could be \"NCHW\", the data storage order of:\n    [batch, channels, height, width].", "dilations": "An optional list of ints. Defaults to [1, 1, 1, 1].\n1-D tensor of length 4.  The dilation factor for each dimension of\ninput. If set to k > 1, there will be k-1 skipped cells between each\nfilter element on that dimension. The dimension order is determined by the\nvalue of data_format, see above for details. Dilations in the batch and\ndepth dimensions must be 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.Conv2DBackpropFilter": {"description": "Computes the gradients of convolution with respect to the filter.", "Args": {"input": "A Tensor. Must be one of the following types: half, bfloat16, float32, float64.\n4-D with shape [batch, in_height, in_width, in_channels].", "filter_sizes": "A Tensor of type int32.\nAn integer vector representing the tensor shape of filter,\nwhere filter is a 4-D\n[filter_height, filter_width, in_channels, out_channels] tensor.", "out_backprop": "A Tensor. Must have the same type as input.\n4-D with shape [batch, out_height, out_width, out_channels].\nGradients w.r.t. the output of the convolution.", "strides": "A list of ints.\nThe stride of the sliding window for each dimension of the input\nof the convolution. Must be in the same order as the dimension specified with\nformat.", "padding": "A string from: \"SAME\", \"VALID\", \"EXPLICIT\".\nThe type of padding algorithm to use.", "use_cudnn_on_gpu": "An optional bool. Defaults to True.", "explicit_paddings": "An optional list of ints. Defaults to [].\nIf padding is \"EXPLICIT\", the list of explicit padding amounts. For the ith\ndimension, the amount of padding inserted before and after the dimension is\nexplicit_paddings[2 * i] and explicit_paddings[2 * i + 1], respectively. If\npadding is not \"EXPLICIT\", explicit_paddings must be empty.", "data_format": "An optional string from: \"NHWC\", \"NCHW\". Defaults to \"NHWC\".\nSpecify the data format of the input and output data. With the\ndefault format \"NHWC\", the data is stored in the order of:\n    [batch, in_height, in_width, in_channels].\nAlternatively, the format could be \"NCHW\", the data storage order of:\n    [batch, in_channels, in_height, in_width].", "dilations": "An optional list of ints. Defaults to [1, 1, 1, 1].\n1-D tensor of length 4.  The dilation factor for each dimension of\ninput. If set to k > 1, there will be k-1 skipped cells between each filter\nelement on that dimension. The dimension order is determined by the value of\ndata_format, see above for details. Dilations in the batch and depth\ndimensions must be 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.Conv2DBackpropInput": {"description": "Computes the gradients of convolution with respect to the input.", "Args": {"input_sizes": "A Tensor of type int32.\nAn integer vector representing the shape of input,\nwhere input is a 4-D [batch, height, width, channels] tensor.", "filter": "A Tensor. Must be one of the following types: half, bfloat16, float32, float64, int32.\n4-D with shape\n[filter_height, filter_width, in_channels, out_channels].", "out_backprop": "A Tensor. Must have the same type as filter.\n4-D with shape [batch, out_height, out_width, out_channels].\nGradients w.r.t. the output of the convolution.", "strides": "A list of ints.\nThe stride of the sliding window for each dimension of the input\nof the convolution. Must be in the same order as the dimension specified with\nformat.", "padding": "A string from: \"SAME\", \"VALID\", \"EXPLICIT\".\nThe type of padding algorithm to use.", "use_cudnn_on_gpu": "An optional bool. Defaults to True.", "explicit_paddings": "An optional list of ints. Defaults to [].\nIf padding is \"EXPLICIT\", the list of explicit padding amounts. For the ith\ndimension, the amount of padding inserted before and after the dimension is\nexplicit_paddings[2 * i] and explicit_paddings[2 * i + 1], respectively. If\npadding is not \"EXPLICIT\", explicit_paddings must be empty.", "data_format": "An optional string from: \"NHWC\", \"NCHW\". Defaults to \"NHWC\".\nSpecify the data format of the input and output data. With the\ndefault format \"NHWC\", the data is stored in the order of:\n    [batch, in_height, in_width, in_channels].\nAlternatively, the format could be \"NCHW\", the data storage order of:\n    [batch, in_channels, in_height, in_width].", "dilations": "An optional list of ints. Defaults to [1, 1, 1, 1].\n1-D tensor of length 4.  The dilation factor for each dimension of\ninput. If set to k > 1, there will be k-1 skipped cells between each filter\nelement on that dimension. The dimension order is determined by the value of\ndata_format, see above for details. Dilations in the batch and depth\ndimensions must be 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as filter."}, "tf.raw_ops.Conv3D": {"description": "Computes a 3-D convolution given 5-D input and filter tensors.", "Args": {"input": "A Tensor. Must be one of the following types: half, bfloat16, float32, float64.\nShape [batch, in_depth, in_height, in_width, in_channels].", "filter": "A Tensor. Must have the same type as input.\nShape [filter_depth, filter_height, filter_width, in_channels,\nout_channels]. in_channels must match between input and filter.", "strides": "A list of ints that has length >= 5.\n1-D tensor of length 5. The stride of the sliding window for each\ndimension of input. Must have strides[0] = strides[4] = 1.", "padding": "A string from: \"SAME\", \"VALID\".\nThe type of padding algorithm to use.", "data_format": "An optional string from: \"NDHWC\", \"NCDHW\". Defaults to \"NDHWC\".\nThe data format of the input and output data. With the\ndefault format \"NDHWC\", the data is stored in the order of:\n    [batch, in_depth, in_height, in_width, in_channels].\nAlternatively, the format could be \"NCDHW\", the data storage order is:\n    [batch, in_channels, in_depth, in_height, in_width].", "dilations": "An optional list of ints. Defaults to [1, 1, 1, 1, 1].\n1-D tensor of length 5.  The dilation factor for each dimension of\ninput. If set to k > 1, there will be k-1 skipped cells between each\nfilter element on that dimension. The dimension order is determined by the\nvalue of data_format, see above for details. Dilations in the batch and\ndepth dimensions must be 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.Conv3DBackpropFilter": {"description": "Computes the gradients of 3-D convolution with respect to the filter.", "Args": {"input": "A Tensor. Must be one of the following types: half, float32, float64.\nShape [batch, depth, rows, cols, in_channels].", "filter": "A Tensor. Must have the same type as input.\nShape [depth, rows, cols, in_channels, out_channels].\nin_channels must match between input and filter.", "out_backprop": "A Tensor. Must have the same type as input.\nBackprop signal of shape [batch, out_depth, out_rows, out_cols,\nout_channels].", "strides": "A list of ints that has length >= 5.\n1-D tensor of length 5. The stride of the sliding window for each\ndimension of input. Must have strides[0] = strides[4] = 1.", "padding": "A string from: \"SAME\", \"VALID\".\nThe type of padding algorithm to use.", "dilations": "An optional list of ints. Defaults to [1, 1, 1, 1, 1].", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.Conv3DBackpropFilterV2": {"description": "Computes the gradients of 3-D convolution with respect to the filter.", "Args": {"input": "A Tensor. Must be one of the following types: half, bfloat16, float32, float64.\nShape [batch, depth, rows, cols, in_channels].", "filter_sizes": "A Tensor of type int32.\nAn integer vector representing the tensor shape of filter,\nwhere filter is a 5-D\n[filter_depth, filter_height, filter_width, in_channels, out_channels]\ntensor.", "out_backprop": "A Tensor. Must have the same type as input.\nBackprop signal of shape [batch, out_depth, out_rows, out_cols,\nout_channels].", "strides": "A list of ints that has length >= 5.\n1-D tensor of length 5. The stride of the sliding window for each\ndimension of input. Must have strides[0] = strides[4] = 1.", "padding": "A string from: \"SAME\", \"VALID\".\nThe type of padding algorithm to use.", "data_format": "An optional string from: \"NDHWC\", \"NCDHW\". Defaults to \"NDHWC\".\nThe data format of the input and output data. With the\ndefault format \"NDHWC\", the data is stored in the order of:\n    [batch, in_depth, in_height, in_width, in_channels].\nAlternatively, the format could be \"NCDHW\", the data storage order is:\n    [batch, in_channels, in_depth, in_height, in_width].", "dilations": "An optional list of ints. Defaults to [1, 1, 1, 1, 1].\n1-D tensor of length 5.  The dilation factor for each dimension of\ninput. If set to k > 1, there will be k-1 skipped cells between each\nfilter element on that dimension. The dimension order is determined by the\nvalue of data_format, see above for details. Dilations in the batch and\ndepth dimensions must be 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.Conv3DBackpropInput": {"description": "Computes the gradients of 3-D convolution with respect to the input.", "Args": {"input": "A Tensor. Must be one of the following types: half, float32, float64.\nShape [batch, depth, rows, cols, in_channels].", "filter": "A Tensor. Must have the same type as input.\nShape [depth, rows, cols, in_channels, out_channels].\nin_channels must match between input and filter.", "out_backprop": "A Tensor. Must have the same type as input.\nBackprop signal of shape [batch, out_depth, out_rows, out_cols,\nout_channels].", "strides": "A list of ints that has length >= 5.\n1-D tensor of length 5. The stride of the sliding window for each\ndimension of input. Must have strides[0] = strides[4] = 1.", "padding": "A string from: \"SAME\", \"VALID\".\nThe type of padding algorithm to use.", "dilations": "An optional list of ints. Defaults to [1, 1, 1, 1, 1].", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.Conv3DBackpropInputV2": {"description": "Computes the gradients of 3-D convolution with respect to the input.", "Args": {"input_sizes": "A Tensor. Must be one of the following types: int32, int64.\nAn integer vector representing the tensor shape of input,\nwhere input is a 5-D\n[batch, depth, rows, cols, in_channels] tensor.", "filter": "A Tensor. Must be one of the following types: half, bfloat16, float32, float64.\nShape [depth, rows, cols, in_channels, out_channels].\nin_channels must match between input and filter.", "out_backprop": "A Tensor. Must have the same type as filter.\nBackprop signal of shape [batch, out_depth, out_rows, out_cols,\nout_channels].", "strides": "A list of ints that has length >= 5.\n1-D tensor of length 5. The stride of the sliding window for each\ndimension of input. Must have strides[0] = strides[4] = 1.", "padding": "A string from: \"SAME\", \"VALID\".\nThe type of padding algorithm to use.", "data_format": "An optional string from: \"NDHWC\", \"NCDHW\". Defaults to \"NDHWC\".\nThe data format of the input and output data. With the\ndefault format \"NDHWC\", the data is stored in the order of:\n    [batch, in_depth, in_height, in_width, in_channels].\nAlternatively, the format could be \"NCDHW\", the data storage order is:\n    [batch, in_channels, in_depth, in_height, in_width].", "dilations": "An optional list of ints. Defaults to [1, 1, 1, 1, 1].\n1-D tensor of length 5.  The dilation factor for each dimension of\ninput. If set to k > 1, there will be k-1 skipped cells between each\nfilter element on that dimension. The dimension order is determined by the\nvalue of data_format, see above for details. Dilations in the batch and\ndepth dimensions must be 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as filter."}, "tf.raw_ops.Copy": {"description": "Copy a tensor from CPU-to-CPU or GPU-to-GPU.", "Args": {"input": "A Tensor. Input tensor.", "tensor_name": "An optional string. Defaults to \"\".\nThe name of the input tensor.", "debug_ops_spec": "An optional list of strings. Defaults to [].\nA list of debug op spec (op, url, gated_grpc) for attached debug\nops. Each element of the list has the format\n;;, wherein gated_grpc is boolean represented\nas 0/1. E.g., \"DebugIdentity;grpc://foo:3333;1\",\n\"DebugIdentity;file:///tmp/tfdbg_1;0\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.CopyHost": {"description": "Copy a tensor to host.", "Args": {"input": "A Tensor. Input tensor.", "tensor_name": "An optional string. Defaults to \"\".\nThe name of the input tensor.", "debug_ops_spec": "An optional list of strings. Defaults to [].\nA list of debug op spec (op, url, gated_grpc) for attached debug\nops. Each element of the list has the format\n;;, wherein gated_grpc is boolean represented\nas 0/1. E.g., \"DebugIdentity;grpc://foo:3333;1\",\n\"DebugIdentity;file:///tmp/tfdbg_1;0\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.Cos": {"description": "Computes cos of x element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, complex64, complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.Cosh": {"description": "Computes hyperbolic cosine of x element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, complex64, complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.CountUpTo": {"description": "Increments &#39;ref&#39; until it reaches &#39;limit&#39;.", "Args": {"ref": "A mutable Tensor. Must be one of the following types: int32, int64.\nShould be from a scalar Variable node.", "limit": "An int.\nIf incrementing ref would bring it above limit, instead generates an\n'OutOfRange' error.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as ref."}, "tf.raw_ops.CreateSummaryDbWriter": {"Args": {"writer": "A Tensor of type resource.", "db_uri": "A Tensor of type string.", "experiment_name": "A Tensor of type string.", "run_name": "A Tensor of type string.", "user_name": "A Tensor of type string.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.CreateSummaryFileWriter": {"Args": {"writer": "A Tensor of type resource.", "logdir": "A Tensor of type string.", "max_queue": "A Tensor of type int32.", "flush_millis": "A Tensor of type int32.", "filename_suffix": "A Tensor of type string.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.CropAndResize": {"description": "Extracts crops from the input image tensor and resizes them.", "Args": {"image": "A Tensor. Must be one of the following types: uint8, uint16, int8, int16, int32, int64, half, float32, float64.\nA 4-D tensor of shape [batch, image_height, image_width, depth].\nBoth image_height and image_width need to be positive.", "boxes": "A Tensor of type float32.\nA 2-D tensor of shape [num_boxes, 4]. The i-th row of the tensor\nspecifies the coordinates of a box in the box_ind[i] image and is specified\nin normalized coordinates [y1, x1, y2, x2]. A normalized coordinate value of\ny is mapped to the image coordinate at y * (image_height - 1), so as the\n[0, 1] interval of normalized image height is mapped to\n[0, image_height - 1] in image height coordinates. We do allow y1 > y2, in\nwhich case the sampled crop is an up-down flipped version of the original\nimage. The width dimension is treated similarly. Normalized coordinates\noutside the [0, 1] range are allowed, in which case we use\nextrapolation_value to extrapolate the input image values.", "box_ind": "A Tensor of type int32.\nA 1-D tensor of shape [num_boxes] with int32 values in [0, batch).\nThe value of box_ind[i] specifies the image that the i-th box refers to.", "crop_size": "A Tensor of type int32.\nA 1-D tensor of 2 elements, size = [crop_height, crop_width]. All\ncropped image patches are resized to this size. The aspect ratio of the image\ncontent is not preserved. Both crop_height and crop_width need to be\npositive.", "method": "An optional string from: \"bilinear\", \"nearest\". Defaults to \"bilinear\".\nA string specifying the sampling method for resizing. It can be either\n\"bilinear\" or \"nearest\" and default to \"bilinear\". Currently two sampling\nmethods are supported: Bilinear and Nearest Neighbor.", "extrapolation_value": "An optional float. Defaults to 0.\nValue used for extrapolation, when applicable.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type float32."}, "tf.raw_ops.CropAndResizeGradBoxes": {"description": "Computes the gradient of the crop_and_resize op wrt the input boxes tensor.", "Args": {"grads": "A Tensor of type float32.\nA 4-D tensor of shape [num_boxes, crop_height, crop_width, depth].", "image": "A Tensor. Must be one of the following types: uint8, uint16, int8, int16, int32, int64, half, float32, float64.\nA 4-D tensor of shape [batch, image_height, image_width, depth].\nBoth image_height and image_width need to be positive."}, "Returns": "A Tensor of type float32."}, "tf.raw_ops.CropAndResizeGradImage": {"description": "Computes the gradient of the crop_and_resize op wrt the input image tensor.", "Args": {"grads": "A Tensor of type float32.\nA 4-D tensor of shape [num_boxes, crop_height, crop_width, depth]."}, "Returns": "A Tensor of type T."}, "tf.raw_ops.Cross": {"description": "Compute the pairwise cross product.", "Args": {"a": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.\nA tensor containing 3-element vectors.", "b": "A Tensor. Must have the same type as a.\nAnother tensor, of same type and shape as a.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as a."}, "tf.raw_ops.CrossReplicaSum": {"description": "An Op to sum inputs across replicated TPU instances.", "Args": {"input": "A Tensor. Must be one of the following types: half, bfloat16, float32, float64, int32, uint32.\nThe local input to the sum.", "group_assignment": "A Tensor of type int32. An int32 tensor with shape\n[num_groups, num_replicas_per_group]. group_assignment[i] represents the\nreplica ids in the ith subgroup.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.CudnnRNN": {"description": "A RNN backed by cuDNN.", "Args": {"input": "A Tensor. Must be one of the following types: half, float32, float64.", "input_h": "A Tensor. Must have the same type as input.", "input_c": "A Tensor. Must have the same type as input.", "params": "A Tensor. Must have the same type as input.", "rnn_mode": "An optional string from: \"rnn_relu\", \"rnn_tanh\", \"lstm\", \"gru\". Defaults to \"lstm\".", "input_mode": "An optional string from: \"linear_input\", \"skip_input\", \"auto_select\". Defaults to \"linear_input\".", "direction": "An optional string from: \"unidirectional\", \"bidirectional\". Defaults to \"unidirectional\".", "dropout": "An optional float. Defaults to 0.", "seed": "An optional int. Defaults to 0.", "seed2": "An optional int. Defaults to 0.", "is_training": "An optional bool. Defaults to True.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output, output_h, output_c, reserve_space)."}, "tf.raw_ops.CudnnRNNBackprop": {"description": "Backprop step of CudnnRNN.", "Args": {"input": "A Tensor. Must be one of the following types: half, float32, float64.", "input_h": "A Tensor. Must have the same type as input.", "input_c": "A Tensor. Must have the same type as input.", "params": "A Tensor. Must have the same type as input.", "output": "A Tensor. Must have the same type as input.", "output_h": "A Tensor. Must have the same type as input.", "output_c": "A Tensor. Must have the same type as input.", "output_backprop": "A Tensor. Must have the same type as input.", "output_h_backprop": "A Tensor. Must have the same type as input.", "output_c_backprop": "A Tensor. Must have the same type as input.", "reserve_space": "A Tensor. Must have the same type as input.", "rnn_mode": "An optional string from: \"rnn_relu\", \"rnn_tanh\", \"lstm\", \"gru\". Defaults to \"lstm\".", "input_mode": "An optional string from: \"linear_input\", \"skip_input\", \"auto_select\". Defaults to \"linear_input\".", "direction": "An optional string from: \"unidirectional\", \"bidirectional\". Defaults to \"unidirectional\".", "dropout": "An optional float. Defaults to 0.", "seed": "An optional int. Defaults to 0.", "seed2": "An optional int. Defaults to 0.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (input_backprop, input_h_backprop, input_c_backprop, params_backprop)."}, "tf.raw_ops.CudnnRNNBackpropV2": {"description": "Backprop step of CudnnRNN.", "Args": {"input": "A Tensor. Must be one of the following types: half, float32, float64.", "input_h": "A Tensor. Must have the same type as input.", "input_c": "A Tensor. Must have the same type as input.", "params": "A Tensor. Must have the same type as input.", "output": "A Tensor. Must have the same type as input.", "output_h": "A Tensor. Must have the same type as input.", "output_c": "A Tensor. Must have the same type as input.", "output_backprop": "A Tensor. Must have the same type as input.", "output_h_backprop": "A Tensor. Must have the same type as input.", "output_c_backprop": "A Tensor. Must have the same type as input.", "reserve_space": "A Tensor. Must have the same type as input.", "host_reserved": "A Tensor of type int8.", "rnn_mode": "An optional string from: \"rnn_relu\", \"rnn_tanh\", \"lstm\", \"gru\". Defaults to \"lstm\".", "input_mode": "An optional string from: \"linear_input\", \"skip_input\", \"auto_select\". Defaults to \"linear_input\".", "direction": "An optional string from: \"unidirectional\", \"bidirectional\". Defaults to \"unidirectional\".", "dropout": "An optional float. Defaults to 0.", "seed": "An optional int. Defaults to 0.", "seed2": "An optional int. Defaults to 0.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (input_backprop, input_h_backprop, input_c_backprop, params_backprop)."}, "tf.raw_ops.CudnnRNNBackpropV3": {"description": "Backprop step of CudnnRNNV3.", "Args": {"input": "A Tensor. Must be one of the following types: half, float32, float64.", "input_h": "A Tensor. Must have the same type as input.", "input_c": "A Tensor. Must have the same type as input.", "params": "A Tensor. Must have the same type as input.", "sequence_lengths": "A Tensor of type int32.", "output": "A Tensor. Must have the same type as input.", "output_h": "A Tensor. Must have the same type as input.", "output_c": "A Tensor. Must have the same type as input.", "output_backprop": "A Tensor. Must have the same type as input.", "output_h_backprop": "A Tensor. Must have the same type as input.", "output_c_backprop": "A Tensor. Must have the same type as input.", "reserve_space": "A Tensor. Must have the same type as input.", "host_reserved": "A Tensor of type int8.", "rnn_mode": "An optional string from: \"rnn_relu\", \"rnn_tanh\", \"lstm\", \"gru\". Defaults to \"lstm\".", "input_mode": "An optional string from: \"linear_input\", \"skip_input\", \"auto_select\". Defaults to \"linear_input\".", "direction": "An optional string from: \"unidirectional\", \"bidirectional\". Defaults to \"unidirectional\".", "dropout": "An optional float. Defaults to 0.", "seed": "An optional int. Defaults to 0.", "seed2": "An optional int. Defaults to 0.", "num_proj": "An optional int. Defaults to 0.", "time_major": "An optional bool. Defaults to True.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (input_backprop, input_h_backprop, input_c_backprop, params_backprop)."}, "tf.raw_ops.CudnnRNNCanonicalToParams": {"description": "Converts CudnnRNN params from canonical form to usable form.", "Args": {"num_layers": "A Tensor of type int32.", "num_units": "A Tensor of type int32.", "input_size": "A Tensor of type int32.", "weights": "A list of at least 1 Tensor objects with the same type in: half, float32, float64.", "biases": "A list with the same length as weights of Tensor objects with the same type as weights.", "rnn_mode": "An optional string from: \"rnn_relu\", \"rnn_tanh\", \"lstm\", \"gru\". Defaults to \"lstm\".", "input_mode": "An optional string from: \"linear_input\", \"skip_input\", \"auto_select\". Defaults to \"linear_input\".", "direction": "An optional string from: \"unidirectional\", \"bidirectional\". Defaults to \"unidirectional\".", "dropout": "An optional float. Defaults to 0.", "seed": "An optional int. Defaults to 0.", "seed2": "An optional int. Defaults to 0.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as weights."}, "tf.raw_ops.CudnnRNNCanonicalToParamsV2": {"description": "Converts CudnnRNN params from canonical form to usable form. It supports the projection in LSTM.", "Args": {"num_layers": "A Tensor of type int32.", "num_units": "A Tensor of type int32.", "input_size": "A Tensor of type int32.", "weights": "A list of at least 1 Tensor objects with the same type in: half, float32, float64.", "biases": "A list of at least 1 Tensor objects with the same type as weights.", "rnn_mode": "An optional string from: \"rnn_relu\", \"rnn_tanh\", \"lstm\", \"gru\". Defaults to \"lstm\".", "input_mode": "An optional string from: \"linear_input\", \"skip_input\", \"auto_select\". Defaults to \"linear_input\".", "direction": "An optional string from: \"unidirectional\", \"bidirectional\". Defaults to \"unidirectional\".", "dropout": "An optional float. Defaults to 0.", "seed": "An optional int. Defaults to 0.", "seed2": "An optional int. Defaults to 0.", "num_proj": "An optional int. Defaults to 0.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as weights."}, "tf.raw_ops.CudnnRNNParamsSize": {"description": "Computes size of weights that can be used by a Cudnn RNN model.", "Args": {"num_layers": "A Tensor of type int32.", "num_units": "A Tensor of type int32.", "input_size": "A Tensor of type int32.", "T": "A tf.DType from: tf.half, tf.float32, tf.float64.", "S": "A tf.DType from: tf.int32, tf.int64.", "rnn_mode": "An optional string from: \"rnn_relu\", \"rnn_tanh\", \"lstm\", \"gru\". Defaults to \"lstm\".", "input_mode": "An optional string from: \"linear_input\", \"skip_input\", \"auto_select\". Defaults to \"linear_input\".", "direction": "An optional string from: \"unidirectional\", \"bidirectional\". Defaults to \"unidirectional\".", "dropout": "An optional float. Defaults to 0.", "seed": "An optional int. Defaults to 0.", "seed2": "An optional int. Defaults to 0.", "num_proj": "An optional int. Defaults to 0.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type S."}, "tf.raw_ops.CudnnRNNParamsToCanonical": {"description": "Retrieves CudnnRNN params in canonical form.", "Args": {"num_layers": "A Tensor of type int32.", "num_units": "A Tensor of type int32.", "input_size": "A Tensor of type int32.", "params": "A Tensor. Must be one of the following types: half, float32, float64.", "num_params": "An int that is >= 1.", "rnn_mode": "An optional string from: \"rnn_relu\", \"rnn_tanh\", \"lstm\", \"gru\". Defaults to \"lstm\".", "input_mode": "An optional string from: \"linear_input\", \"skip_input\", \"auto_select\". Defaults to \"linear_input\".", "direction": "An optional string from: \"unidirectional\", \"bidirectional\". Defaults to \"unidirectional\".", "dropout": "An optional float. Defaults to 0.", "seed": "An optional int. Defaults to 0.", "seed2": "An optional int. Defaults to 0.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (weights, biases)."}, "tf.raw_ops.CudnnRNNParamsToCanonicalV2": {"description": "Retrieves CudnnRNN params in canonical form. It supports the projection in LSTM.", "Args": {"num_layers": "A Tensor of type int32.", "num_units": "A Tensor of type int32.", "input_size": "A Tensor of type int32.", "params": "A Tensor. Must be one of the following types: half, float32, float64.", "num_params_weights": "An int that is >= 1.", "num_params_biases": "An int that is >= 1.", "rnn_mode": "An optional string from: \"rnn_relu\", \"rnn_tanh\", \"lstm\", \"gru\". Defaults to \"lstm\".", "input_mode": "An optional string from: \"linear_input\", \"skip_input\", \"auto_select\". Defaults to \"linear_input\".", "direction": "An optional string from: \"unidirectional\", \"bidirectional\". Defaults to \"unidirectional\".", "dropout": "An optional float. Defaults to 0.", "seed": "An optional int. Defaults to 0.", "seed2": "An optional int. Defaults to 0.", "num_proj": "An optional int. Defaults to 0.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (weights, biases)."}, "tf.raw_ops.CudnnRNNV2": {"description": "A RNN backed by cuDNN.", "Args": {"input": "A Tensor. Must be one of the following types: half, float32, float64.", "input_h": "A Tensor. Must have the same type as input.", "input_c": "A Tensor. Must have the same type as input.", "params": "A Tensor. Must have the same type as input.", "rnn_mode": "An optional string from: \"rnn_relu\", \"rnn_tanh\", \"lstm\", \"gru\". Defaults to \"lstm\".", "input_mode": "An optional string from: \"linear_input\", \"skip_input\", \"auto_select\". Defaults to \"linear_input\".", "direction": "An optional string from: \"unidirectional\", \"bidirectional\". Defaults to \"unidirectional\".", "dropout": "An optional float. Defaults to 0.", "seed": "An optional int. Defaults to 0.", "seed2": "An optional int. Defaults to 0.", "is_training": "An optional bool. Defaults to True.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output, output_h, output_c, reserve_space, host_reserved)."}, "tf.raw_ops.CudnnRNNV3": {"description": "A RNN backed by cuDNN.", "Args": {"input": "A Tensor. Must be one of the following types: half, float32, float64.", "input_h": "A Tensor. Must have the same type as input.", "input_c": "A Tensor. Must have the same type as input.", "params": "A Tensor. Must have the same type as input.", "sequence_lengths": "A Tensor of type int32.", "rnn_mode": "An optional string from: \"rnn_relu\", \"rnn_tanh\", \"lstm\", \"gru\". Defaults to \"lstm\".", "input_mode": "An optional string from: \"linear_input\", \"skip_input\", \"auto_select\". Defaults to \"linear_input\".", "direction": "An optional string from: \"unidirectional\", \"bidirectional\". Defaults to \"unidirectional\".", "dropout": "An optional float. Defaults to 0.", "seed": "An optional int. Defaults to 0.", "seed2": "An optional int. Defaults to 0.", "num_proj": "An optional int. Defaults to 0.", "is_training": "An optional bool. Defaults to True.", "time_major": "An optional bool. Defaults to True.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output, output_h, output_c, reserve_space, host_reserved)."}, "tf.raw_ops.Cumprod": {"description": "Compute the cumulative product of the tensor x along axis.", "Args": {"x": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nA Tensor. Must be one of the following types: float32, float64,\nint64, int32, uint8, uint16, int16, int8, complex64,\ncomplex128, qint8, quint8, qint32, half.", "axis": "A Tensor. Must be one of the following types: int32, int64.\nA Tensor of type int32 (default: 0). Must be in the range\n[-rank(x), rank(x)).", "exclusive": "An optional bool. Defaults to False.\nIf True, perform exclusive cumprod.", "reverse": "An optional bool. Defaults to False.\nA bool (default: False).", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.Cumsum": {"description": "Compute the cumulative sum of the tensor x along axis.", "Args": {"x": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nA Tensor. Must be one of the following types: float32, float64,\nint64, int32, uint8, uint16, int16, int8, complex64,\ncomplex128, qint8, quint8, qint32, half.", "axis": "A Tensor. Must be one of the following types: int32, int64.\nA Tensor of type int32 (default: 0). Must be in the range\n[-rank(x), rank(x)).", "exclusive": "An optional bool. Defaults to False.\nIf True, perform exclusive cumsum.", "reverse": "An optional bool. Defaults to False.\nA bool (default: False).", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.CumulativeLogsumexp": {"description": "Compute the cumulative product of the tensor x along axis.", "Args": {"x": "A Tensor. Must be one of the following types: half, float32, float64.\nA Tensor. Must be one of the following types: float16, float32, float64.", "axis": "A Tensor. Must be one of the following types: int32, int64.\nA Tensor of type int32 (default: 0). Must be in the range\n[-rank(x), rank(x)).", "exclusive": "An optional bool. Defaults to False.\nIf True, perform exclusive cumulative log-sum-exp.", "reverse": "An optional bool. Defaults to False.\nA bool (default: False).", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.DataFormatDimMap": {"description": "Returns the dimension index in the destination data format given the one in", "Args": {"x": "A Tensor. Must be one of the following types: int32, int64.\nA Tensor with each element as a dimension index in source data format.\nMust be in the range [-4, 4).", "src_format": "An optional string. Defaults to \"NHWC\".\nsource data format.", "dst_format": "An optional string. Defaults to \"NCHW\".\ndestination data format.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.DataFormatVecPermute": {"description": "Permute input tensor from src_format to dst_format.", "Args": {"x": "A Tensor. Must be one of the following types: int32, int64.\nVector of size 4 or Tensor of shape (4, 2) in source data format.", "src_format": "An optional string. Defaults to \"NHWC\".\nsource data format.", "dst_format": "An optional string. Defaults to \"NCHW\".\ndestination data format.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.DataServiceDataset": {"description": "Creates a dataset that reads data from the tf.data service.", "Args": {"dataset_id": "A Tensor of type int64.", "processing_mode": "A Tensor of type string.", "address": "A Tensor of type string.", "protocol": "A Tensor of type string.", "job_name": "A Tensor of type string.", "max_outstanding_requests": "A Tensor of type int64.", "iteration_counter": "A Tensor of type resource.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "task_refresh_interval_hint_ms": "An optional int. Defaults to -1.", "data_transfer_protocol": "An optional string. Defaults to \"\".", "target_workers": "An optional string. Defaults to \"AUTO\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.DataServiceDatasetV2": {"description": "Creates a dataset that reads data from the tf.data service.", "Args": {"dataset_id": "A Tensor of type int64.", "processing_mode": "A Tensor of type string.", "address": "A Tensor of type string.", "protocol": "A Tensor of type string.", "job_name": "A Tensor of type string.", "consumer_index": "A Tensor of type int64.", "num_consumers": "A Tensor of type int64.", "max_outstanding_requests": "A Tensor of type int64.", "iteration_counter": "A Tensor of type resource.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "task_refresh_interval_hint_ms": "An optional int. Defaults to -1.", "data_transfer_protocol": "An optional string. Defaults to \"\".", "target_workers": "An optional string. Defaults to \"AUTO\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.DataServiceDatasetV3": {"description": "Creates a dataset that reads data from the tf.data service.", "Args": {"dataset_id": "A Tensor of type int64.", "processing_mode": "A Tensor of type string.", "address": "A Tensor of type string.", "protocol": "A Tensor of type string.", "job_name": "A Tensor of type string.", "consumer_index": "A Tensor of type int64.", "num_consumers": "A Tensor of type int64.", "max_outstanding_requests": "A Tensor of type int64.", "iteration_counter": "A Tensor of type resource.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "uncompress_fn": "A function decorated with @Defun.", "task_refresh_interval_hint_ms": "An optional int. Defaults to -1.", "data_transfer_protocol": "An optional string. Defaults to \"\".", "target_workers": "An optional string. Defaults to \"AUTO\".", "uncompress": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.DatasetCardinality": {"description": "Returns the cardinality of input_dataset.", "Args": {"input_dataset": "A Tensor of type variant.\nA variant tensor representing the dataset to return cardinality for.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int64."}, "tf.raw_ops.DatasetFromGraph": {"description": "Creates a dataset from the given graph_def.", "Args": {"graph_def": "A Tensor of type string.\nThe graph representation of the dataset (as serialized GraphDef).", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.DatasetToGraph": {"description": "Returns a serialized GraphDef representing input_dataset.", "Args": {"input_dataset": "A Tensor of type variant.\nA variant tensor representing the dataset to return the graph representation for.", "stateful_whitelist": "An optional list of strings. Defaults to [].", "allow_stateful": "An optional bool. Defaults to False.", "strip_device_assignment": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.DatasetToGraphV2": {"description": "Returns a serialized GraphDef representing input_dataset.", "Args": {"input_dataset": "A Tensor of type variant.\nA variant tensor representing the dataset to return the graph representation for.", "external_state_policy": "An optional int. Defaults to 0.", "strip_device_assignment": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.DatasetToSingleElement": {"description": "Outputs the single element from the given dataset.", "Args": {"dataset": "A Tensor of type variant.\nA handle to a dataset that contains a single element.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A list of Tensor objects of type output_types."}, "tf.raw_ops.DatasetToTFRecord": {"description": "Writes the given dataset to the given file using the TFRecord format.", "Args": {"input_dataset": "A Tensor of type variant.\nA variant tensor representing the dataset to write.", "filename": "A Tensor of type string.\nA scalar string tensor representing the filename to use.", "compression_type": "A Tensor of type string.\nA scalar string tensor containing either (i) the empty string (no\ncompression), (ii) \"ZLIB\", or (iii) \"GZIP\".", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.Dawsn": {"Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.DebugGradientIdentity": {"description": "Identity op for gradient debugging.", "Args": {"input": "A Tensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.DebugGradientRefIdentity": {"description": "Identity op for gradient debugging.", "Args": {"input": "A mutable Tensor.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as input."}, "tf.raw_ops.DebugIdentity": {"description": "Provides an identity mapping of the non-Ref type input tensor for debugging.", "Args": {"input": "A Tensor. Input tensor, non-Reference type", "device_name": "An optional string. Defaults to \"\".\nName of the device on which the tensor resides.", "tensor_name": "An optional string. Defaults to \"\".\nName of the input tensor.", "debug_urls": "An optional list of strings. Defaults to [].\nList of URLs to debug targets, e.g.,\n  file:///foo/tfdbg_dump, grpc:://localhost:11011", "gated_grpc": "An optional bool. Defaults to False.\nWhether this op will be gated. If any of the debug_urls of this\n  debug node is of the grpc:// scheme, when the value of this attribute is set\n  to True, the data will not actually be sent via the grpc stream unless this\n  debug op has been enabled at the debug_url. If all of the debug_urls of this\n  debug node are of the grpc:// scheme and the debug op is enabled at none of\n  them, the output will be an empty Tensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.DebugIdentityV2": {"description": "Debug Identity V2 Op.", "Args": {"input": "A Tensor. Input tensor, non-Reference type", "tfdbg_context_id": "An optional string. Defaults to \"\".\nA tfdbg-generated ID for the context that the op belongs to,\n  e.g., a concrete compiled tf.function.", "op_name": "An optional string. Defaults to \"\".\nOptional. Name of the op that the debug op is concerned with.\n  Used only for single-tensor trace.", "output_slot": "An optional int. Defaults to -1.\nOptional. Output slot index of the tensor that the debug op\n  is concerned with. Used only for single-tensor trace.", "tensor_debug_mode": "An optional int. Defaults to -1.\nTensorDebugMode enum value. See debug_event.proto for details.", "debug_urls": "An optional list of strings. Defaults to [].\nList of URLs to debug targets, e.g., file:///foo/tfdbg_dump.", "circular_buffer_size": "An optional int. Defaults to 1000.", "tfdbg_run_id": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.DebugNanCount": {"description": "Debug NaN Value Counter Op.", "Args": {"input": "A Tensor. Input tensor, non-Reference type.", "device_name": "An optional string. Defaults to \"\".", "tensor_name": "An optional string. Defaults to \"\".\nName of the input tensor.", "debug_urls": "An optional list of strings. Defaults to [].\nList of URLs to debug targets, e.g.,\n  file:///foo/tfdbg_dump, grpc:://localhost:11011.", "gated_grpc": "An optional bool. Defaults to False.\nWhether this op will be gated. If any of the debug_urls of this\n debug node is of the grpc:// scheme, when the value of this attribute is set\n to True, the data will not actually be sent via the grpc stream unless this\n debug op has been enabled at the debug_url. If all of the debug_urls of this\n debug node are of the grpc:// scheme and the debug op is enabled at none of\n them, the output will be an empty Tensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int64."}, "tf.raw_ops.DebugNumericSummary": {"description": "Debug Numeric Summary Op.", "Args": {"input": "A Tensor. Input tensor, non-Reference type.", "device_name": "An optional string. Defaults to \"\".", "tensor_name": "An optional string. Defaults to \"\".\nName of the input tensor.", "debug_urls": "An optional list of strings. Defaults to [].\nList of URLs to debug targets, e.g.,\n  file:///foo/tfdbg_dump, grpc:://localhost:11011.", "lower_bound": "An optional float. Defaults to float('-inf').\n(float) The lower bound <= which values will be included in the\n  generalized -inf count. Default: -inf.", "upper_bound": "An optional float. Defaults to float('inf').\n(float) The upper bound >= which values will be included in the\n  generalized +inf count. Default: +inf.", "mute_if_healthy": "An optional bool. Defaults to False.\n(bool) Do not send data to the debug URLs unless at least one\n  of elements [2], [3] and 7 is non-zero.", "gated_grpc": "An optional bool. Defaults to False.\nWhether this op will be gated. If any of the debug_urls of this\n  debug node is of the grpc:// scheme, when the value of this attribute is set\n  to True, the data will not actually be sent via the grpc stream unless this\n  debug op has been enabled at the debug_url. If all of the debug_urls of this\n  debug node are of the grpc:// scheme and the debug op is enabled at none of\n  them, the output will be an empty Tensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type float64."}, "tf.raw_ops.DebugNumericSummaryV2": {"description": "Debug Numeric Summary V2 Op.", "Args": {"input": "A Tensor. Input tensor, to be summarized by the op.", "output_dtype": "An optional tf.DType from: tf.float32, tf.float64. Defaults to tf.float32.\nOptional. The type of the output. Can be float32 or float64 (default: float32).", "tensor_debug_mode": "An optional int. Defaults to -1.\nTensor debug mode: the mode in which the input tensor is summarized\n  by the op. See the TensorDebugMode enum in\n  tensorflow/core/protobuf/debug_event.proto for details.\nSupported values:\n  2 (CURT_HEALTH): Output a float32/64 tensor of shape [2]. The 1st\n  element is the tensor_id, if provided, and -1 otherwise. The 2nd\n  element is a bit which is set to 1 if the input tensor has an\n  infinity or nan value, or zero otherwise.\n3 (CONCISE_HEALTH): Output a float32/64 tensor of shape [5]. The 1st\n  element is the tensor_id, if provided, and -1 otherwise. The\n  remaining four slots are the total number of elements, -infs,\n  +infs, and nans in the input tensor respectively.\n4 (FULL_HEALTH): Output a float32/64 tensor of shape [11]. The 1st\n  element is the tensor_id, if provided, and -1 otherwise. The 2nd\n  element is the device_id, if provided, and -1 otherwise. The 3rd\n  element holds the datatype value of the input tensor as according\n  to the enumerated type in tensorflow/core/framework/types.proto.\n  The remaining elements hold the total number of elements, -infs,\n  +infs, nans, negative finite numbers, zeros, and positive finite\n  numbers in the input tensor respectively.\n5 (SHAPE): Output a float32/64 tensor of shape [10]. The 1st\n  element is the tensor_id, if provided, and -1 otherwise. The 2nd\n  element holds the datatype value of the input tensor as according\n  to the enumerated type in tensorflow/core/framework/types.proto.\n  The 3rd element holds the rank of the tensor. The 4th element holds\n  the number of elements within the tensor. Finally the remaining 6\n  elements hold the shape of the tensor. If the rank of the tensor\n  is lower than 6, the shape is right padded with zeros. If the rank\n  is greater than 6, the head of the shape is truncated.\n6 (FULL_NUMERICS): Output a float32/64 tensor of shape [22]. The 1st\n  element is the tensor_id, if provided, and -1 otherwise. The 2nd\n  element is the device_id, if provided, and -1 otherwise. The 3rd\n  element holds the datatype value of the input tensor as according\n  to the enumerated type in tensorflow/core/framework/types.proto.\n  The 4th element holds the rank of the tensor. The 5th to 11th\n  elements hold the shape of the tensor. If the rank of the tensor\n  is lower than 6, the shape is right padded with zeros. If the rank\n  is greater than 6, the head of the shape is truncated. The 12th to\n  18th elements hold the number of elements, -infs, +infs, nans,\n  denormal floats, negative finite numbers, zeros, and positive\n  finite numbers in the input tensor respectively. The final four\n  elements hold the min value, max value, mean, and variance of the\n  input tensor.\n8 (REDUCE_INF_NAN_THREE_SLOTS): Output a float32/64 tensor of shape\n  [3]. The 1st element is -inf if any elements of the input tensor\n  is -inf, or zero otherwise. The 2nd element is +inf if any elements\n  of the input tensor is +inf, or zero otherwise.  The 3rd element is\n  nan if any element of the input tensor is nan, or zero otherwise.", "tensor_id": "An optional int. Defaults to -1.\nOptional. An integer identifier for the tensor being summarized by this op.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type output_dtype."}, "tf.raw_ops.DecodeAndCropJpeg": {"description": "Decode and Crop a JPEG-encoded image to a uint8 tensor.", "Args": {"contents": "A Tensor of type string. 0-D.  The JPEG-encoded image.", "crop_window": "A Tensor of type int32.\n1-D.  The crop window: [crop_y, crop_x, crop_height, crop_width].", "channels": "An optional int. Defaults to 0.\nNumber of color channels for the decoded image.", "ratio": "An optional int. Defaults to 1. Downscaling ratio.", "fancy_upscaling": "An optional bool. Defaults to True.\nIf true use a slower but nicer upscaling of the\nchroma planes (yuv420/422 only).", "try_recover_truncated": "An optional bool. Defaults to False.\nIf true try to recover an image from truncated input.", "acceptable_fraction": "An optional float. Defaults to 1.\nThe minimum required fraction of lines before a truncated\ninput is accepted.", "dct_method": "An optional string. Defaults to \"\".\nstring specifying a hint about the algorithm used for\ndecompression.  Defaults to \"\" which maps to a system-specific\ndefault.  Currently valid values are [\"INTEGER_FAST\",\n\"INTEGER_ACCURATE\"].  The hint may be ignored (e.g., the internal\njpeg library changes to a version that does not have that specific\noption.)", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type uint8."}, "tf.raw_ops.DecodeBase64": {"description": "Decode web-safe base64-encoded strings.", "Args": {"input": "A Tensor of type string. Base64 strings to decode.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.DecodeBmp": {"description": "Decode the first frame of a BMP-encoded image to a uint8 tensor.", "Args": {"contents": "A Tensor of type string. 0-D.  The BMP-encoded image.", "channels": "An optional int. Defaults to 0.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type uint8."}, "tf.raw_ops.DecodeCSV": {"description": "Convert CSV records to tensors. Each column maps to one tensor.", "Args": {"records": "A Tensor of type string.\nEach string is a record/row in the csv and all records should have\nthe same format.", "record_defaults": "A list of Tensor objects with types from: float32, float64, int32, int64, string.\nOne tensor per column of the input record, with either a\nscalar default value for that column or an empty vector if the column is\nrequired.", "field_delim": "An optional string. Defaults to \",\".\nchar delimiter to separate fields in a record.", "use_quote_delim": "An optional bool. Defaults to True.\nIf false, treats double quotation marks as regular\ncharacters inside of the string fields (ignoring RFC 4180, Section 2,\nBullet 5).", "na_value": "An optional string. Defaults to \"\".\nAdditional string to recognize as NA/NaN.", "select_cols": "An optional list of ints. Defaults to [].", "name": "A name for the operation (optional)."}, "Returns": "A list of Tensor objects. Has the same type as record_defaults."}, "tf.raw_ops.DecodeCompressed": {"description": "Decompress strings.", "Args": {"bytes": "A Tensor of type string.\nA Tensor of string which is compressed.", "compression_type": "An optional string. Defaults to \"\".\nA scalar containing either (i) the empty string (no\ncompression), (ii) \"ZLIB\", or (iii) \"GZIP\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.DecodeGif": {"description": "Decode the frame(s) of a GIF-encoded image to a uint8 tensor.", "Args": {"contents": "A Tensor of type string. 0-D.  The GIF-encoded image.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type uint8."}, "tf.raw_ops.DecodeImage": {"description": "Function for decode_bmp, decode_gif, decode_jpeg, and decode_png.", "Args": {"contents": "A Tensor of type string. 0-D. The encoded image bytes.", "channels": "An optional int. Defaults to 0.\nNumber of color channels for the decoded image.", "dtype": "An optional tf.DType from: tf.uint8, tf.uint16, tf.float32. Defaults to tf.uint8.\nThe desired DType of the returned Tensor.", "expand_animations": "An optional bool. Defaults to True.\nControls the output shape of the returned op. If True, the returned op will\nproduce a 3-D tensor for PNG, JPEG, and BMP files; and a 4-D tensor for all\nGIFs, whether animated or not. If, False, the returned op will produce a 3-D\ntensor for all file types and will truncate animated GIFs to the first frame.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dtype."}, "tf.raw_ops.DecodeJSONExample": {"description": "Convert JSON-encoded Example records to binary protocol buffer strings.", "Args": {"json_examples": "A Tensor of type string.\nEach string is a JSON object serialized according to the JSON\nmapping of the Example proto.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.DecodeJpeg": {"description": "Decode a JPEG-encoded image to a uint8 tensor.", "Args": {"contents": "A Tensor of type string. 0-D.  The JPEG-encoded image.", "channels": "An optional int. Defaults to 0.\nNumber of color channels for the decoded image.", "ratio": "An optional int. Defaults to 1. Downscaling ratio.", "fancy_upscaling": "An optional bool. Defaults to True.\nIf true use a slower but nicer upscaling of the\nchroma planes (yuv420/422 only).", "try_recover_truncated": "An optional bool. Defaults to False.\nIf true try to recover an image from truncated input.", "acceptable_fraction": "An optional float. Defaults to 1.\nThe minimum required fraction of lines before a truncated\ninput is accepted.", "dct_method": "An optional string. Defaults to \"\".\nstring specifying a hint about the algorithm used for\ndecompression.  Defaults to \"\" which maps to a system-specific\ndefault.  Currently valid values are [\"INTEGER_FAST\",\n\"INTEGER_ACCURATE\"].  The hint may be ignored (e.g., the internal\njpeg library changes to a version that does not have that specific\noption.)", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type uint8."}, "tf.raw_ops.DecodePaddedRaw": {"description": "Reinterpret the bytes of a string as a vector of numbers.", "Args": {"input_bytes": "A Tensor of type string. Tensor of string to be decoded.", "fixed_length": "A Tensor of type int32.\nLength in bytes for each element of the decoded output. Must be a multiple\nof the size of the output type.", "out_type": "A tf.DType from: tf.half, tf.float32, tf.float64, tf.int32, tf.uint16, tf.uint8, tf.int16, tf.int8, tf.int64, tf.bfloat16.", "little_endian": "An optional bool. Defaults to True.\nWhether the input input_bytes is in little-endian order. Ignored for\nout_type values that are stored in a single byte, like uint8", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type out_type."}, "tf.raw_ops.DecodePng": {"description": "Decode a PNG-encoded image to a uint8 or uint16 tensor.", "Args": {"contents": "A Tensor of type string. 0-D.  The PNG-encoded image.", "channels": "An optional int. Defaults to 0.\nNumber of color channels for the decoded image.", "dtype": "An optional tf.DType from: tf.uint8, tf.uint16. Defaults to tf.uint8.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dtype."}, "tf.raw_ops.DecodeProtoV2": {"description": "The op extracts fields from a serialized protocol buffers message into tensors.", "Args": {"bytes": "A Tensor of type string.\nTensor of serialized protos with shape batch_shape.", "message_type": "A string. Name of the proto message type to decode.", "field_names": "A list of strings.\nList of strings containing proto field names. An extension field can be decoded\nby using its full name, e.g. EXT_PACKAGE.EXT_FIELD_NAME.", "output_types": "A list of tf.DTypes.\nList of TF types to use for the respective field in field_names.", "descriptor_source": "An optional string. Defaults to \"local://\".\nEither the special value local:// or a path to a file containing\na serialized FileDescriptorSet.", "message_format": "An optional string. Defaults to \"binary\".\nEither binary or text.", "sanitize": "An optional bool. Defaults to False.\nWhether to sanitize the result or not.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (sizes, values)."}, "tf.raw_ops.DecodeRaw": {"description": "Reinterpret the bytes of a string as a vector of numbers.", "Args": {"bytes": "A Tensor of type string.\nAll the elements must have the same length.", "out_type": "A tf.DType from: tf.half, tf.float32, tf.float64, tf.int32, tf.uint16, tf.uint8, tf.int16, tf.int8, tf.int64, tf.complex64, tf.complex128, tf.bool, tf.bfloat16.", "little_endian": "An optional bool. Defaults to True.\nWhether the input bytes are in little-endian order.\nIgnored for out_type values that are stored in a single byte like\nuint8.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type out_type."}, "tf.raw_ops.DecodeWav": {"description": "Decode a 16-bit PCM WAV file to a float tensor.", "Args": {"contents": "A Tensor of type string.\nThe WAV-encoded audio, usually from a file.", "desired_channels": "An optional int. Defaults to -1.\nNumber of sample channels wanted.", "desired_samples": "An optional int. Defaults to -1.\nLength of audio requested.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (audio, sample_rate)."}, "tf.raw_ops.DeepCopy": {"description": "Makes a copy of x.", "Args": {"x": "A Tensor. The source tensor of type T.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.DeleteIterator": {"description": "A container for an iterator resource.", "Args": {"handle": "A Tensor of type resource. A handle to the iterator to delete.", "deleter": "A Tensor of type variant. A variant deleter.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.DeleteMemoryCache": {"Args": {"handle": "A Tensor of type resource.", "deleter": "A Tensor of type variant.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.DeleteMultiDeviceIterator": {"description": "A container for an iterator resource.", "Args": {"multi_device_iterator": "A Tensor of type resource.\nA handle to the multi device iterator to delete.", "iterators": "A list of Tensor objects with type resource.\nA list of iterator handles (unused). This is added so that automatic control dependencies get added during function tracing that ensure this op runs after all the dependent iterators are deleted.", "deleter": "A Tensor of type variant. A variant deleter.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.DeleteRandomSeedGenerator": {"Args": {"handle": "A Tensor of type resource.", "deleter": "A Tensor of type variant.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.DeleteSeedGenerator": {"Args": {"handle": "A Tensor of type resource.", "deleter": "A Tensor of type variant.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.DeleteSessionTensor": {"description": "Delete the tensor specified by its handle in the session.", "Args": {"handle": "A Tensor of type string.\nThe handle for a tensor stored in the session state.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.DenseBincount": {"description": "Counts the number of occurrences of each value in an integer array.", "Args": {"input": "A Tensor. Must be one of the following types: int32, int64.\n1D or 2D int Tensor.", "size": "A Tensor. Must have the same type as input.\nnon-negative int scalar Tensor.", "weights": "A Tensor. Must be one of the following types: int32, int64, float32, float64.\nis an int32, int64, float32, or float64 Tensor with the same\nshape as arr, or a length-0 Tensor, in which case it acts as all weights\nequal to 1.", "binary_output": "An optional bool. Defaults to False.\nbool; Whether the kernel should count the appearance or number of occurrences.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as weights."}, "tf.raw_ops.DenseCountSparseOutput": {"description": "Performs sparse-output bin counting for a tf.tensor input.", "Args": {"values": "A Tensor. Must be one of the following types: int32, int64.\nTensor containing data to count.", "weights": "A Tensor. Must be one of the following types: int32, int64, float32, float64.\nA Tensor of the same shape as indices containing per-index weight values. May\nalso be the empty tensor if no weights are used.", "binary_output": "A bool.\nWhether to output the number of occurrences of each value or 1.", "minlength": "An optional int that is >= -1. Defaults to -1.\nMinimum value to count. Can be set to -1 for no minimum.", "maxlength": "An optional int that is >= -1. Defaults to -1.\nMaximum value to count. Can be set to -1 for no maximum.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output_indices, output_values, output_dense_shape)."}, "tf.raw_ops.DenseToCSRSparseMatrix": {"description": "Converts a dense tensor to a (possibly batched) CSRSparseMatrix.", "Args": {"dense_input": "A Tensor. Must be one of the following types: float32, float64, complex64, complex128.\nA Dense tensor.", "indices": "A Tensor of type int64. Indices of nonzero elements.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.DenseToDenseSetOperation": {"description": "Applies set operation along last dimension of 2 Tensor inputs.", "Args": {"set1": "A Tensor. Must be one of the following types: int8, int16, int32, int64, uint8, uint16, string.\nTensor with rank n. 1st n-1 dimensions must be the same as set2.\nDimension n contains values in a set, duplicates are allowed but ignored.", "set2": "A Tensor. Must have the same type as set1.\nTensor with rank n. 1st n-1 dimensions must be the same as set1.\nDimension n contains values in a set, duplicates are allowed but ignored.", "set_operation": "A string.", "validate_indices": "An optional bool. Defaults to True.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (result_indices, result_values, result_shape)."}, "tf.raw_ops.DenseToSparseBatchDataset": {"description": "Creates a dataset that batches input elements into a SparseTensor.", "Args": {"input_dataset": "A Tensor of type variant.\nA handle to an input dataset. Must have a single component.", "batch_size": "A Tensor of type int64.\nA scalar representing the number of elements to accumulate in a\nbatch.", "row_shape": "A Tensor of type int64.\nA vector representing the dense shape of each row in the produced\nSparseTensor. The shape may be partially specified, using -1 to indicate\nthat a particular dimension should use the maximum size of all batch elements.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.DenseToSparseSetOperation": {"description": "Applies set operation along last dimension of Tensor and SparseTensor.", "Args": {"set1": "A Tensor. Must be one of the following types: int8, int16, int32, int64, uint8, uint16, string.\nTensor with rank n. 1st n-1 dimensions must be the same as set2.\nDimension n contains values in a set, duplicates are allowed but ignored.", "set2_indices": "A Tensor of type int64.\n2D Tensor, indices of a SparseTensor. Must be in row-major\norder.", "set2_values": "A Tensor. Must have the same type as set1.\n1D Tensor, values of a SparseTensor. Must be in row-major\norder.", "set2_shape": "A Tensor of type int64.\n1D Tensor, shape of a SparseTensor. set2_shape[0...n-1] must\nbe the same as the 1st n-1 dimensions of set1, result_shape[n] is the\nmax set size across n-1 dimensions.", "set_operation": "A string.", "validate_indices": "An optional bool. Defaults to True.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (result_indices, result_values, result_shape)."}, "tf.raw_ops.DepthToSpace": {"description": "DepthToSpace for tensors of type T.", "Args": {"input": "A Tensor.", "block_size": "An int that is >= 2.\nThe size of the spatial block, same as in Space2Depth.", "data_format": "An optional string from: \"NHWC\", \"NCHW\", \"NCHW_VECT_C\". Defaults to \"NHWC\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.DepthwiseConv2dNative": {"description": "Computes a 2-D depthwise convolution given 4-D input and filter tensors.", "Args": {"input": "A Tensor. Must be one of the following types: half, bfloat16, float32, float64.", "filter": "A Tensor. Must have the same type as input.", "strides": "A list of ints.\n1-D of length 4.  The stride of the sliding window for each dimension\nof input.", "padding": "A string from: \"SAME\", \"VALID\", \"EXPLICIT\".\nThe type of padding algorithm to use.", "explicit_paddings": "An optional list of ints. Defaults to [].", "data_format": "An optional string from: \"NHWC\", \"NCHW\". Defaults to \"NHWC\".\nSpecify the data format of the input and output data. With the\ndefault format \"NHWC\", the data is stored in the order of:\n    [batch, height, width, channels].\nAlternatively, the format could be \"NCHW\", the data storage order of:\n    [batch, channels, height, width].", "dilations": "An optional list of ints. Defaults to [1, 1, 1, 1].\n1-D tensor of length 4.  The dilation factor for each dimension of\ninput. If set to k > 1, there will be k-1 skipped cells between each filter\nelement on that dimension. The dimension order is determined by the value of\ndata_format, see above for details. Dilations in the batch and depth\ndimensions must be 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.DepthwiseConv2dNativeBackpropFilter": {"description": "Computes the gradients of depthwise convolution with respect to the filter.", "Args": {"input": "A Tensor. Must be one of the following types: half, bfloat16, float32, float64.\n4-D with shape based on data_format.  For example, if\ndata_format is 'NHWC' then input is a 4-D [batch, in_height,\nin_width, in_channels] tensor.", "filter_sizes": "A Tensor of type int32.\nAn integer vector representing the tensor shape of filter,\nwhere filter is a 4-D\n[filter_height, filter_width, in_channels, depthwise_multiplier] tensor.", "out_backprop": "A Tensor. Must have the same type as input.\n4-D with shape  based on data_format.\nFor example, if data_format is 'NHWC' then\nout_backprop shape is [batch, out_height, out_width, out_channels].\nGradients w.r.t. the output of the convolution.", "strides": "A list of ints.\nThe stride of the sliding window for each dimension of the input\nof the convolution.", "padding": "A string from: \"SAME\", \"VALID\", \"EXPLICIT\".\nThe type of padding algorithm to use.", "explicit_paddings": "An optional list of ints. Defaults to [].", "data_format": "An optional string from: \"NHWC\", \"NCHW\". Defaults to \"NHWC\".\nSpecify the data format of the input and output data. With the\ndefault format \"NHWC\", the data is stored in the order of:\n    [batch, height, width, channels].\nAlternatively, the format could be \"NCHW\", the data storage order of:\n    [batch, channels, height, width].", "dilations": "An optional list of ints. Defaults to [1, 1, 1, 1].\n1-D tensor of length 4.  The dilation factor for each dimension of\ninput. If set to k > 1, there will be k-1 skipped cells between each filter\nelement on that dimension. The dimension order is determined by the value of\ndata_format, see above for details. Dilations in the batch and depth\ndimensions must be 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.DepthwiseConv2dNativeBackpropInput": {"description": "Computes the gradients of depthwise convolution with respect to the input.", "Args": {"input_sizes": "A Tensor of type int32.\nAn integer vector representing the shape of input, based\non data_format.  For example, if data_format is 'NHWC' then\n input is a 4-D [batch, height, width, channels] tensor.", "filter": "A Tensor. Must be one of the following types: half, bfloat16, float32, float64.\n4-D with shape\n[filter_height, filter_width, in_channels, depthwise_multiplier].", "out_backprop": "A Tensor. Must have the same type as filter.\n4-D with shape  based on data_format.\nFor example, if data_format is 'NHWC' then\nout_backprop shape is [batch, out_height, out_width, out_channels].\nGradients w.r.t. the output of the convolution.", "strides": "A list of ints.\nThe stride of the sliding window for each dimension of the input\nof the convolution.", "padding": "A string from: \"SAME\", \"VALID\", \"EXPLICIT\".\nThe type of padding algorithm to use.", "explicit_paddings": "An optional list of ints. Defaults to [].", "data_format": "An optional string from: \"NHWC\", \"NCHW\". Defaults to \"NHWC\".\nSpecify the data format of the input and output data. With the\ndefault format \"NHWC\", the data is stored in the order of:\n    [batch, height, width, channels].\nAlternatively, the format could be \"NCHW\", the data storage order of:\n    [batch, channels, height, width].", "dilations": "An optional list of ints. Defaults to [1, 1, 1, 1].\n1-D tensor of length 4.  The dilation factor for each dimension of\ninput. If set to k > 1, there will be k-1 skipped cells between each filter\nelement on that dimension. The dimension order is determined by the value of\ndata_format, see above for details. Dilations in the batch and depth\ndimensions must be 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as filter."}, "tf.raw_ops.Dequantize": {"description": "Dequantize the &#39;input&#39; tensor into a float or bfloat16 Tensor.", "Args": {"input": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.", "min_range": "A Tensor of type float32.\nThe minimum scalar value possibly produced for the input.", "max_range": "A Tensor of type float32.\nThe maximum scalar value possibly produced for the input.", "mode": "An optional string from: \"MIN_COMBINED\", \"MIN_FIRST\", \"SCALED\". Defaults to \"MIN_COMBINED\".", "narrow_range": "An optional bool. Defaults to False.", "axis": "An optional int. Defaults to -1.", "dtype": "An optional tf.DType from: tf.bfloat16, tf.float32. Defaults to tf.float32.\nType of the output tensor. Currently Dequantize supports float and bfloat16.\nIf 'dtype' is 'bfloat16', it only supports 'MIN_COMBINED' mode.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dtype."}, "tf.raw_ops.DeserializeIterator": {"description": "Converts the given variant tensor to an iterator and stores it in the given resource.", "Args": {"resource_handle": "A Tensor of type resource.\nA handle to an iterator resource.", "serialized": "A Tensor of type variant.\nA variant tensor storing the state of the iterator contained in the\nresource.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.DeserializeManySparse": {"description": "Deserialize and concatenate SparseTensors from a serialized minibatch.", "Args": {"serialized_sparse": "A Tensor of type string.\n2-D, The N serialized SparseTensor objects.\nMust have 3 columns.", "dtype": "A tf.DType. The dtype of the serialized SparseTensor objects.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (sparse_indices, sparse_values, sparse_shape)."}, "tf.raw_ops.DeserializeSparse": {"description": "Deserialize SparseTensor objects.", "Args": {"serialized_sparse": "A Tensor. Must be one of the following types: string, variant.\nThe serialized SparseTensor objects. The last dimension\nmust have 3 columns.", "dtype": "A tf.DType. The dtype of the serialized SparseTensor objects.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (sparse_indices, sparse_values, sparse_shape)."}, "tf.raw_ops.DestroyResourceOp": {"description": "Deletes the resource specified by the handle.", "Args": {"resource": "A Tensor of type resource. handle to the resource to delete.", "ignore_lookup_error": "An optional bool. Defaults to True.\nwhether to ignore the error when the resource\ndoesn't exist.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.DestroyTemporaryVariable": {"description": "Destroys the temporary variable and returns its final value.", "Args": {"ref": "A mutable Tensor. A reference to the temporary variable tensor.", "var_name": "A string.\nName of the temporary variable, usually the name of the matching\n'TemporaryVariable' op.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as ref."}, "tf.raw_ops.DeviceIndex": {"description": "Return the index of device the op runs.", "Args": {"device_names": "A list of strings.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int32."}, "tf.raw_ops.Diag": {"description": "Returns a diagonal tensor with a given diagonal values.", "Args": {"diagonal": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, int32, int64, complex64, complex128.\nRank k tensor where k is at most 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as diagonal."}, "tf.raw_ops.DiagPart": {"description": "Returns the diagonal part of the tensor.", "Args": {"input": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, int32, int64, complex64, complex128.\nRank k tensor where k is even and not zero.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.Digamma": {"description": "Computes Psi, the derivative of Lgamma (the log of the absolute value of", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.Dilation2D": {"description": "Computes the grayscale dilation of 4-D input and 3-D filter tensors.", "Args": {"input": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.\n4-D with shape [batch, in_height, in_width, depth].", "filter": "A Tensor. Must have the same type as input.\n3-D with shape [filter_height, filter_width, depth].", "strides": "A list of ints that has length >= 4.\nThe stride of the sliding window for each dimension of the input\ntensor. Must be: [1, stride_height, stride_width, 1].", "rates": "A list of ints that has length >= 4.\nThe input stride for atrous morphological dilation. Must be:\n[1, rate_height, rate_width, 1].", "padding": "A string from: \"SAME\", \"VALID\".\nThe type of padding algorithm to use.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.Dilation2DBackpropFilter": {"description": "Computes the gradient of morphological 2-D dilation with respect to the filter.", "Args": {"input": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.\n4-D with shape [batch, in_height, in_width, depth].", "filter": "A Tensor. Must have the same type as input.\n3-D with shape [filter_height, filter_width, depth].", "out_backprop": "A Tensor. Must have the same type as input.\n4-D with shape [batch, out_height, out_width, depth].", "strides": "A list of ints that has length >= 4.\n1-D of length 4. The stride of the sliding window for each dimension of\nthe input tensor. Must be: [1, stride_height, stride_width, 1].", "rates": "A list of ints that has length >= 4.\n1-D of length 4. The input stride for atrous morphological dilation.\nMust be: [1, rate_height, rate_width, 1].", "padding": "A string from: \"SAME\", \"VALID\".\nThe type of padding algorithm to use.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.Dilation2DBackpropInput": {"description": "Computes the gradient of morphological 2-D dilation with respect to the input.", "Args": {"input": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.\n4-D with shape [batch, in_height, in_width, depth].", "filter": "A Tensor. Must have the same type as input.\n3-D with shape [filter_height, filter_width, depth].", "out_backprop": "A Tensor. Must have the same type as input.\n4-D with shape [batch, out_height, out_width, depth].", "strides": "A list of ints that has length >= 4.\n1-D of length 4. The stride of the sliding window for each dimension of\nthe input tensor. Must be: [1, stride_height, stride_width, 1].", "rates": "A list of ints that has length >= 4.\n1-D of length 4. The input stride for atrous morphological dilation.\nMust be: [1, rate_height, rate_width, 1].", "padding": "A string from: \"SAME\", \"VALID\".\nThe type of padding algorithm to use.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.DirectedInterleaveDataset": {"description": "A substitute for InterleaveDataset on a fixed list of N datasets.", "Args": {"selector_input_dataset": "A Tensor of type variant.\nA dataset of scalar DT_INT64 elements that determines which of the\nN data inputs should produce the next output element.", "data_input_datasets": "A list of at least 1 Tensor objects with type variant.\nN datasets with the same type that will be interleaved according to\nthe values of selector_input_dataset.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "stop_on_empty_dataset": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.DisableCopyOnRead": {"description": "Turns off the copy-on-read mode.", "Args": {"resource": "A Tensor of type resource.\nThe resource handle of the resource variable.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.Div": {"description": "Returns x / y element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, uint8, int8, uint16, int16, int32, uint32, uint64, int64, complex64, complex128.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.DivNoNan": {"description": "Returns 0 if the denominator is zero.", "Args": {"x": "A Tensor. Must be one of the following types: half, float32, bfloat16, float64, complex64, complex128.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.DrawBoundingBoxes": {"description": "Draw bounding boxes on a batch of images.", "Args": {"images": "A Tensor. Must be one of the following types: float32, half.\n4-D with shape [batch, height, width, depth]. A batch of images.", "boxes": "A Tensor of type float32.\n3-D with shape [batch, num_bounding_boxes, 4] containing bounding\nboxes.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as images."}, "tf.raw_ops.DrawBoundingBoxesV2": {"description": "Draw bounding boxes on a batch of images.", "Args": {"images": "A Tensor. Must be one of the following types: float32, half.\n4-D with shape [batch, height, width, depth]. A batch of images.", "boxes": "A Tensor of type float32.\n3-D with shape [batch, num_bounding_boxes, 4] containing bounding\nboxes.", "colors": "A Tensor of type float32.\n2-D. A list of RGBA colors to cycle through for the boxes.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as images."}, "tf.raw_ops.DummyIterationCounter": {"Args": {"name": "A name for the operation (optional)."}, "Returns": "A Tensor of type resource."}, "tf.raw_ops.DummyMemoryCache": {"Args": {"name": "A name for the operation (optional)."}, "Returns": "A Tensor of type resource."}, "tf.raw_ops.DummySeedGenerator": {"Args": {"name": "A name for the operation (optional)."}, "Returns": "A Tensor of type resource."}, "tf.raw_ops.DynamicEnqueueTPUEmbeddingArbitraryTensorBatch": {"description": "Eases the porting of code that uses tf.nn.embedding_lookup_sparse().", "Args": {"sample_indices_or_row_splits": "A list of at least 1 Tensor objects with the same type in: int32, int64.\nA list of rank 2 Tensors specifying the training example to which the\ncorresponding embedding_indices and aggregation_weights values belong.\nIf the size of its first dimension is 0, we assume each embedding_indices\nbelongs to a different sample. Both int32 and int64 are allowed and will\nbe converted to int32 internally.\nOr a list of rank 1 Tensors specifying the row splits for splitting\nembedding_indices and aggregation_weights into rows. It corresponds to\nids.row_splits in embedding_lookup(), when ids is a RaggedTensor. When\nenqueuing N-D ragged tensor, only the last dimension is allowed to be ragged.\nthe row splits is 1-D dense tensor. When empty, we assume a dense tensor is\npassed to the op Both int32 and int64 are allowed and will be converted to\nint32 internally.", "embedding_indices": "A list with the same length as sample_indices_or_row_splits of Tensor objects with the same type in: int32, int64.\nA list of rank 1 Tensors, indices into the embedding\ntables. Both int32 and int64 are allowed and will be converted to\nint32 internally.", "aggregation_weights": "A list with the same length as sample_indices_or_row_splits of Tensor objects with the same type in: float32, float64.\nA list of rank 1 Tensors containing per training\nexample aggregation weights. Both float32 and float64 are allowed and will\nbe converted to float32 internally.", "mode_override": "A Tensor of type string.\nA string input that overrides the mode specified in the\nTPUEmbeddingConfiguration. Supported values are {'unspecified', 'inference',\n'training', 'backward_pass_only'}. When set to 'unspecified', the mode set\nin TPUEmbeddingConfiguration is used, otherwise mode_override is used.", "device_ordinal": "A Tensor of type int32.\nThe TPU device to use. Should be >= 0 and less than the number\nof TPU cores in the task on which the node is placed.", "combiners": "An optional list of strings. Defaults to [].\nA list of string scalars, one for each embedding table that specify\nhow to normalize the embedding activations after weighted summation.\nSupported combiners are 'mean', 'sum', or 'sqrtn'. It is invalid to have\nthe sum of the weights be 0 for 'mean' or the sum of the squared weights be\n0 for 'sqrtn'. If combiners isn't passed, the default is to use 'sum' for\nall tables.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.DynamicPartition": {"description": "Partitions data into num_partitions tensors using indices from partitions.", "Args": {"data": "A Tensor.", "partitions": "A Tensor of type int32.\nAny shape.  Indices in the range [0, num_partitions).", "num_partitions": "An int that is >= 1.\nThe number of partitions to output.", "name": "A name for the operation (optional)."}, "Returns": "A list of num_partitions Tensor objects with the same type as data."}, "tf.raw_ops.DynamicStitch": {"description": "Interleave the values from the data tensors into a single tensor.", "Args": {"indices": "A list of at least 1 Tensor objects with type int32.", "data": "A list with the same length as indices of Tensor objects with the same type.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as data."}, "tf.raw_ops.EagerPyFunc": {"description": "Eagerly executes a python function to compute func(input)-&gt;output. The", "Args": {"input": "A list of Tensor objects.", "token": "A string.", "Tout": "A list of tf.DTypes.", "is_async": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "A list of Tensor objects of type Tout."}, "tf.raw_ops.EditDistance": {"description": "Computes the (possibly normalized) Levenshtein Edit Distance.", "Args": {"hypothesis_indices": "A Tensor of type int64.\nThe indices of the hypothesis list SparseTensor.\nThis is an N x R int64 matrix.", "hypothesis_values": "A Tensor.\nThe values of the hypothesis list SparseTensor.\nThis is an N-length vector.", "hypothesis_shape": "A Tensor of type int64.\nThe shape of the hypothesis list SparseTensor.\nThis is an R-length vector.", "truth_indices": "A Tensor of type int64.\nThe indices of the truth list SparseTensor.\nThis is an M x R int64 matrix.", "truth_values": "A Tensor. Must have the same type as hypothesis_values.\nThe values of the truth list SparseTensor.\nThis is an M-length vector.", "truth_shape": "A Tensor of type int64. truth indices, vector.", "normalize": "An optional bool. Defaults to True.\nboolean (if true, edit distances are normalized by length of truth).\nThe output is:", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type float32."}, "tf.raw_ops.Eig": {"description": "Computes the eigen decomposition of one or more square matrices.", "Args": {"input": "A Tensor. Must be one of the following types: float32, float64, complex64, complex128.\nTensor input of shape [N, N].", "Tout": "A tf.DType from: tf.complex64, tf.complex128.", "compute_v": "An optional bool. Defaults to True.\nIf True then eigenvectors will be computed and returned in v.\nOtherwise, only the eigenvalues will be computed.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (e, v)."}, "tf.raw_ops.Einsum": {"description": "Tensor contraction according to Einstein summation convention.", "Args": {"inputs": "A list of at least 1 Tensor objects with the same type.\nList of 1 or 2 Tensors.", "equation": "A string.\nString describing the Einstein Summation operation; in the format of np.einsum.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as inputs."}, "tf.raw_ops.Elu": {"description": "Computes the exponential linear function.", "Args": {"features": "A Tensor. Must be one of the following types: half, bfloat16, float32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as features."}, "tf.raw_ops.EluGrad": {"description": "Computes gradients for the exponential linear (Elu) operation.", "Args": {"gradients": "A Tensor. Must be one of the following types: half, bfloat16, float32, float64.\nThe backpropagated gradients to the corresponding Elu operation.", "outputs": "A Tensor. Must have the same type as gradients.\nThe outputs of the corresponding Elu operation.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as gradients."}, "tf.raw_ops.Empty": {"description": "Creates a tensor with the given shape."}, "tf.raw_ops.EmptyTensorList": {"description": "Creates and returns an empty tensor list.", "Args": {"element_shape": "A Tensor. Must be one of the following types: int32, int64.", "max_num_elements": "A Tensor of type int32.", "element_dtype": "A tf.DType.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.EncodeBase64": {"description": "Encode strings into web-safe base64 format.", "Args": {"input": "A Tensor of type string. Strings to be encoded.", "pad": "An optional bool. Defaults to False.\nBool whether padding is applied at the ends.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.EncodeJpeg": {"description": "JPEG-encode an image.", "Args": {"image": "A Tensor of type uint8.\n3-D with shape [height, width, channels].", "format": "An optional string from: \"\", \"grayscale\", \"rgb\". Defaults to \"\".\nPer pixel image format.", "quality": "An optional int. Defaults to 95.\nQuality of the compression from 0 to 100 (higher is better and slower).", "progressive": "An optional bool. Defaults to False.\nIf True, create a JPEG that loads progressively (coarse to fine).", "optimize_size": "An optional bool. Defaults to False.\nIf True, spend CPU/RAM to reduce size with no quality change.", "chroma_downsampling": "An optional bool. Defaults to True.\nSee http://en.wikipedia.org/wiki/Chroma_subsampling", "density_unit": "An optional string from: \"in\", \"cm\". Defaults to \"in\".\nUnit used to specify x_density and y_density:\npixels per inch ('in') or centimeter ('cm').", "x_density": "An optional int. Defaults to 300.\nHorizontal pixels per density unit.", "y_density": "An optional int. Defaults to 300.\nVertical pixels per density unit.", "xmp_metadata": "An optional string. Defaults to \"\".\nIf not empty, embed this XMP metadata in the image header.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.EncodeJpegVariableQuality": {"description": "JPEG encode input image with provided compression quality.", "Args": {"images": "A Tensor of type uint8. Images to adjust.  At least 3-D.", "quality": "A Tensor of type int32. An int quality to encode to.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.EncodePng": {"description": "PNG-encode an image.", "Args": {"image": "A Tensor. Must be one of the following types: uint8, uint16.\n3-D with shape [height, width, channels].", "compression": "An optional int. Defaults to -1. Compression level.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.EncodeProto": {"description": "The op serializes protobuf messages provided in the input tensors.", "Args": {"sizes": "A Tensor of type int32.\nTensor of int32 with shape [batch_shape, len(field_names)].", "values": "A list of Tensor objects.\nList of tensors containing values for the corresponding field.", "field_names": "A list of strings.\nList of strings containing proto field names.", "message_type": "A string. Name of the proto message type to decode.", "descriptor_source": "An optional string. Defaults to \"local://\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.EncodeWav": {"description": "Encode audio data using the WAV file format.", "Args": {"audio": "A Tensor of type float32. 2-D with shape [length, channels].", "sample_rate": "A Tensor of type int32.\nScalar containing the sample frequency.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.EnqueueTPUEmbeddingArbitraryTensorBatch": {"description": "Eases the porting of code that uses tf.nn.embedding_lookup_sparse().", "Args": {"sample_indices_or_row_splits": "A list of at least 1 Tensor objects with the same type in: int32, int64.\nA list of rank 2 Tensors specifying the training example to which the\ncorresponding embedding_indices and aggregation_weights values belong.\nIf the size of its first dimension is 0, we assume each embedding_indices\nbelongs to a different sample. Both int32 and int64 are allowed and will\nbe converted to int32 internally.\nOr a list of rank 1 Tensors specifying the row splits for splitting\nembedding_indices and aggregation_weights into rows. It corresponds to\nids.row_splits in embedding_lookup(), when ids is a RaggedTensor. When\nenqueuing N-D ragged tensor, only the last dimension is allowed to be ragged.\nthe row splits is 1-D dense tensor. When empty, we assume a dense tensor is\npassed to the op Both int32 and int64 are allowed and will be converted to\nint32 internally.", "embedding_indices": "A list with the same length as sample_indices_or_row_splits of Tensor objects with the same type in: int32, int64.\nA list of rank 1 Tensors, indices into the embedding\ntables. Both int32 and int64 are allowed and will be converted to\nint32 internally.", "aggregation_weights": "A list with the same length as sample_indices_or_row_splits of Tensor objects with the same type in: float32, float64.\nA list of rank 1 Tensors containing per training\nexample aggregation weights. Both float32 and float64 are allowed and will\nbe converted to float32 internally.", "mode_override": "A Tensor of type string.\nA string input that overrides the mode specified in the\nTPUEmbeddingConfiguration. Supported values are {'unspecified', 'inference',\n'training', 'backward_pass_only'}. When set to 'unspecified', the mode set\nin TPUEmbeddingConfiguration is used, otherwise mode_override is used.", "device_ordinal": "An optional int. Defaults to -1.\nThe TPU device to use. Should be >= 0 and less than the number\nof TPU cores in the task on which the node is placed.", "combiners": "An optional list of strings. Defaults to [].\nA list of string scalars, one for each embedding table that specify\nhow to normalize the embedding activations after weighted summation.\nSupported combiners are 'mean', 'sum', or 'sqrtn'. It is invalid to have\nthe sum of the weights be 0 for 'mean' or the sum of the squared weights be\n0 for 'sqrtn'. If combiners isn't passed, the default is to use 'sum' for\nall tables.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.EnqueueTPUEmbeddingIntegerBatch": {"description": "An op that enqueues a list of input batch tensors to TPUEmbedding.", "Args": {"batch": "A list of at least 1 Tensor objects with type int32.\nA list of 1D tensors, one for each embedding table, containing the\nindices into the tables.", "mode_override": "A Tensor of type string.\nA string input that overrides the mode specified in the\nTPUEmbeddingConfiguration. Supported values are {'unspecified', 'inference',\n'training', 'backward_pass_only'}. When set to 'unspecified', the mode set\nin TPUEmbeddingConfiguration is used, otherwise mode_override is used.", "device_ordinal": "An optional int. Defaults to -1.\nThe TPU device to use. Should be >= 0 and less than the number\nof TPU cores in the task on which the node is placed.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.EnqueueTPUEmbeddingRaggedTensorBatch": {"description": "Eases the porting of code that uses tf.nn.embedding_lookup().", "Args": {"sample_splits": "A list of at least 1 Tensor objects with the same type in: int32, int64.\nA list of rank 1 Tensors specifying the break points for splitting\nembedding_indices and aggregation_weights into rows.\nIt corresponds to ids.row_splits in embedding_lookup(), when ids is a\nRaggedTensor.", "embedding_indices": "A list with the same length as sample_splits of Tensor objects with the same type in: int32, int64.\nA list of rank 1 Tensors, indices into the embedding tables.\nIt corresponds to ids.values in embedding_lookup(), when ids is a RaggedTensor.", "aggregation_weights": "A list with the same length as sample_splits of Tensor objects with the same type in: float32, float64.\nA list of rank 1 Tensors containing per training example\naggregation weights. It corresponds to the values field of a RaggedTensor\nwith the same row_splits as ids in embedding_lookup(), when ids is a\nRaggedTensor.", "mode_override": "A Tensor of type string.\nA string input that overrides the mode specified in the\nTPUEmbeddingConfiguration. Supported values are {'unspecified', 'inference',\n'training', 'backward_pass_only'}. When set to 'unspecified', the mode set\nin TPUEmbeddingConfiguration is used, otherwise mode_override is used.", "table_ids": "A list of ints.\nA list of integers specifying the identifier of the embedding table\n(offset of TableDescriptor in the TPUEmbeddingConfiguration) to lookup the\ncorresponding input. The ith input is looked up using table_ids[i]. The size\nof the table_ids list must be equal to that of sample_indices,\nembedding_indices and aggregation_weights.", "device_ordinal": "An optional int. Defaults to -1.\nThe TPU device to use. Should be >= 0 and less than the number\nof TPU cores in the task on which the node is placed.", "combiners": "An optional list of strings. Defaults to [].\nA list of string scalars, one for each embedding table that specify\nhow to normalize the embedding activations after weighted summation.\nSupported combiners are 'mean', 'sum', or 'sqrtn'. It is invalid to have\nthe sum of the weights be 0 for 'mean' or the sum of the squared weights be\n0 for 'sqrtn'. If combiners isn't passed, the default is to use 'sum' for\nall tables.", "max_sequence_lengths": "An optional list of ints. Defaults to [].", "num_features": "An optional list of ints. Defaults to [].", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.EnqueueTPUEmbeddingSparseBatch": {"description": "An op that enqueues TPUEmbedding input indices from a SparseTensor.", "Args": {"sample_indices": "A list of at least 1 Tensor objects with the same type in: int32, int64.\nA list of rank 1 Tensors specifying the training example and\nfeature to which the corresponding embedding_indices and aggregation_weights\nvalues belong. sample_indices[i] must equal b * nf + f, where nf is the\nnumber of features from the corresponding table, f is in [0, nf), and\nb is in [0, batch size).", "embedding_indices": "A list with the same length as sample_indices of Tensor objects with the same type in: int32, int64.\nA list of rank 1 Tensors, indices into the embedding tables.", "aggregation_weights": "A list with the same length as sample_indices of Tensor objects with the same type in: float32, float64.\nA list of rank 1 Tensors containing per sample -- i.e. per\n(training example, feature) -- aggregation weights.", "mode_override": "A Tensor of type string.\nA string input that overrides the mode specified in the\nTPUEmbeddingConfiguration. Supported values are {'unspecified', 'inference',\n'training', 'backward_pass_only'}. When set to 'unspecified', the mode set\nin TPUEmbeddingConfiguration is used, otherwise mode_override is used.", "device_ordinal": "An optional int. Defaults to -1.\nThe TPU device to use. Should be >= 0 and less than the number\nof TPU cores in the task on which the node is placed.", "combiners": "An optional list of strings. Defaults to [].\nA list of string scalars, one for each embedding table that specify\nhow to normalize the embedding activations after weighted summation.\nSupported combiners are 'mean', 'sum', or 'sqrtn'. It is invalid to have\nthe sum of the weights be 0 for 'mean' or the sum of the squared weights be\n0 for 'sqrtn'. If combiners isn't passed, the default is to use 'sum' for\nall tables.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.EnqueueTPUEmbeddingSparseTensorBatch": {"description": "Eases the porting of code that uses tf.nn.embedding_lookup_sparse().", "Args": {"sample_indices": "A list of at least 1 Tensor objects with the same type in: int32, int64.\nA list of rank 1 Tensors specifying the training example to\nwhich the corresponding embedding_indices and aggregation_weights values\nbelong. It corresponds to sp_ids.indices[:,0] in  embedding_lookup_sparse().", "embedding_indices": "A list with the same length as sample_indices of Tensor objects with the same type in: int32, int64.\nA list of rank 1 Tensors, indices into the embedding tables.\nIt corresponds to sp_ids.values in embedding_lookup_sparse().", "aggregation_weights": "A list with the same length as sample_indices of Tensor objects with the same type in: float32, float64.\nA list of rank 1 Tensors containing per training example\naggregation weights. It corresponds to sp_weights.values in\nembedding_lookup_sparse().", "mode_override": "A Tensor of type string.\nA string input that overrides the mode specified in the\nTPUEmbeddingConfiguration. Supported values are {'unspecified', 'inference',\n'training', 'backward_pass_only'}. When set to 'unspecified', the mode set\nin TPUEmbeddingConfiguration is used, otherwise mode_override is used.", "table_ids": "A list of ints.\nA list of integers specifying the identifier of the embedding table\n(offset of TableDescriptor in the TPUEmbeddingConfiguration) to lookup the\ncorresponding input. The ith input is looked up using table_ids[i]. The size\nof the table_ids list must be equal to that of sample_indices,\nembedding_indices and aggregation_weights.", "device_ordinal": "An optional int. Defaults to -1.\nThe TPU device to use. Should be >= 0 and less than the number\nof TPU cores in the task on which the node is placed.", "combiners": "An optional list of strings. Defaults to [].\nA list of string scalars, one for each embedding table that specify\nhow to normalize the embedding activations after weighted summation.\nSupported combiners are 'mean', 'sum', or 'sqrtn'. It is invalid to have\nthe sum of the weights be 0 for 'mean' or the sum of the squared weights be\n0 for 'sqrtn'. If combiners isn't passed, the default is to use 'sum' for\nall tables.", "max_sequence_lengths": "An optional list of ints. Defaults to [].", "num_features": "An optional list of ints. Defaults to [].", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.EnsureShape": {"description": "Ensures that the tensor&#39;s shape matches the expected shape.", "Args": {"input": "A Tensor. A tensor, whose shape is to be validated.", "shape": "A tf.TensorShape or list of ints.\nThe expected (possibly partially specified) shape of the input tensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.Enter": {"description": "Creates or finds a child frame, and makes data available to the child frame.", "Args": {"data": "A Tensor. The tensor to be made available to the child frame.", "frame_name": "A string. The name of the child frame.", "is_constant": "An optional bool. Defaults to False.\nIf true, the output is constant within the child frame.", "parallel_iterations": "An optional int. Defaults to 10.\nThe number of iterations allowed to run in parallel.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as data."}, "tf.raw_ops.Equal": {"description": "Returns the truth value of (x == y) element-wise.", "Args": {"x": "A Tensor.", "y": "A Tensor. Must have the same type as x.", "incompatible_shape_error": "An optional bool. Defaults to True.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type bool."}, "tf.raw_ops.Erf": {"description": "Computes the [Gauss error function](https://en.wikipedia.org/wiki/Error_function) of x element-wise. In statistics, for non-negative values of \\\\(x\\\\), the error function has the following interpretation: for a random variable \\\\(Y\\\\) that is normally distributed with mean 0 and variance \\\\(1/\\sqrt{2}\\\\), \\\\(erf(x)\\\\) is the probability that \\\\(Y\\\\) falls in the range \\\\([\u2212x, x]\\\\).", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.Erfc": {"description": "Computes the complementary error function of x element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.Erfinv": {"Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.EuclideanNorm": {"description": "Computes the euclidean norm of elements across dimensions of a tensor.", "Args": {"input": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nThe tensor to reduce.", "axis": "A Tensor. Must be one of the following types: int32, int64.\nThe dimensions to reduce. Must be in the range\n[-rank(input), rank(input)).", "keep_dims": "An optional bool. Defaults to False.\nIf true, retain reduced dimensions with length 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.Exit": {"description": "Exits the current frame to its parent frame.", "Args": {"data": "A Tensor. The tensor to be made available to the parent frame.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as data."}, "tf.raw_ops.Exp": {"description": "Computes exponential of x element-wise. \\\\(y = e^x\\\\).", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, complex64, complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.ExpandDims": {"description": "Inserts a dimension of 1 into a tensor&#39;s shape.", "Args": {"input": "A Tensor.", "axis": "A Tensor. Must be one of the following types: int32, int64.\n0-D (scalar). Specifies the dimension index at which to\nexpand the shape of input. Must be in the range\n[-rank(input) - 1, rank(input)].", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.ExperimentalAssertNextDataset": {"Args": {"input_dataset": "A Tensor of type variant.", "transformations": "A Tensor of type string.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ExperimentalAutoShardDataset": {"description": "Creates a dataset that shards the input dataset.", "Args": {"input_dataset": "A Tensor of type variant.\nA variant tensor representing the input dataset.", "num_workers": "A Tensor of type int64.\nA scalar representing the number of workers to distribute this dataset across.", "index": "A Tensor of type int64.\nA scalar representing the index of the current worker out of num_workers.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "auto_shard_policy": "An optional int. Defaults to 0.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ExperimentalBytesProducedStatsDataset": {"description": "Records the bytes size of each element of input_dataset in a StatsAggregator.", "Args": {"input_dataset": "A Tensor of type variant.", "tag": "A Tensor of type string.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ExperimentalCSVDataset": {"Args": {"filenames": "A Tensor of type string.", "compression_type": "A Tensor of type string.", "buffer_size": "A Tensor of type int64.", "header": "A Tensor of type bool.", "field_delim": "A Tensor of type string.", "use_quote_delim": "A Tensor of type bool.", "na_value": "A Tensor of type string.", "select_cols": "A Tensor of type int64.", "record_defaults": "A list of Tensor objects with types from: float32, float64, int32, int64, string.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ExperimentalChooseFastestDataset": {"Args": {"input_datasets": "A list of at least 2 Tensor objects with type variant.", "num_experiments": "An int.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ExperimentalDatasetCardinality": {"description": "Returns the cardinality of input_dataset.", "Args": {"input_dataset": "A Tensor of type variant.\nA variant tensor representing the dataset to return cardinality for.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int64."}, "tf.raw_ops.ExperimentalDatasetToTFRecord": {"description": "Writes the given dataset to the given file using the TFRecord format.", "Args": {"input_dataset": "A Tensor of type variant.\nA variant tensor representing the dataset to write.", "filename": "A Tensor of type string.\nA scalar string tensor representing the filename to use.", "compression_type": "A Tensor of type string.\nA scalar string tensor containing either (i) the empty string (no\ncompression), (ii) \"ZLIB\", or (iii) \"GZIP\".", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ExperimentalDenseToSparseBatchDataset": {"description": "Creates a dataset that batches input elements into a SparseTensor.", "Args": {"input_dataset": "A Tensor of type variant.\nA handle to an input dataset. Must have a single component.", "batch_size": "A Tensor of type int64.\nA scalar representing the number of elements to accumulate in a\nbatch.", "row_shape": "A Tensor of type int64.\nA vector representing the dense shape of each row in the produced\nSparseTensor. The shape may be partially specified, using -1 to indicate\nthat a particular dimension should use the maximum size of all batch elements.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ExperimentalDirectedInterleaveDataset": {"description": "A substitute for InterleaveDataset on a fixed list of N datasets.", "Args": {"selector_input_dataset": "A Tensor of type variant.\nA dataset of scalar DT_INT64 elements that determines which of the\nN data inputs should produce the next output element.", "data_input_datasets": "A list of at least 1 Tensor objects with type variant.\nN datasets with the same type that will be interleaved according to\nthe values of selector_input_dataset.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ExperimentalGroupByReducerDataset": {"description": "Creates a dataset that computes a group-by on input_dataset.", "Args": {"input_dataset": "A Tensor of type variant.\nA variant tensor representing the input dataset.", "key_func_other_arguments": "A list of Tensor objects.\nA list of tensors, typically values that were captured when\nbuilding a closure for key_func.", "init_func_other_arguments": "A list of Tensor objects.\nA list of tensors, typically values that were captured when\nbuilding a closure for init_func.", "reduce_func_other_arguments": "A list of Tensor objects.\nA list of tensors, typically values that were captured when\nbuilding a closure for reduce_func.", "finalize_func_other_arguments": "A list of Tensor objects.\nA list of tensors, typically values that were captured when\nbuilding a closure for finalize_func.", "key_func": "A function decorated with @Defun.\nA function mapping an element of input_dataset, concatenated\nwith key_func_other_arguments to a scalar value of type DT_INT64.", "init_func": "A function decorated with @Defun.\nA function mapping a key of type DT_INT64, concatenated with\ninit_func_other_arguments to the initial reducer state.", "reduce_func": "A function decorated with @Defun.\nA function mapping the current reducer state and an element of input_dataset,\nconcatenated with reduce_func_other_arguments to a new reducer state.", "finalize_func": "A function decorated with @Defun.\nA function mapping the final reducer state to an output element.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ExperimentalGroupByWindowDataset": {"description": "Creates a dataset that computes a windowed group-by on input_dataset.", "Args": {"input_dataset": "A Tensor of type variant.", "key_func_other_arguments": "A list of Tensor objects.", "reduce_func_other_arguments": "A list of Tensor objects.", "window_size_func_other_arguments": "A list of Tensor objects.", "key_func": "A function decorated with @Defun.\nA function mapping an element of input_dataset, concatenated\nwith key_func_other_arguments to a scalar value of type DT_INT64.", "reduce_func": "A function decorated with @Defun.", "window_size_func": "A function decorated with @Defun.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ExperimentalIgnoreErrorsDataset": {"description": "Creates a dataset that contains the elements of input_dataset ignoring errors.", "Args": {"input_dataset": "A Tensor of type variant.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "log_warning": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ExperimentalIteratorGetDevice": {"description": "Returns the name of the device on which resource has been placed.", "Args": {"resource": "A Tensor of type resource.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.ExperimentalLMDBDataset": {"Args": {"filenames": "A Tensor of type string.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ExperimentalLatencyStatsDataset": {"description": "Records the latency of producing input_dataset elements in a StatsAggregator.", "Args": {"input_dataset": "A Tensor of type variant.", "tag": "A Tensor of type string.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ExperimentalMapAndBatchDataset": {"description": "Creates a dataset that fuses mapping with batching.", "Args": {"input_dataset": "A Tensor of type variant.\nA variant tensor representing the input dataset.", "other_arguments": "A list of Tensor objects.\nA list of tensors, typically values that were captured when building a closure\nfor f.", "batch_size": "A Tensor of type int64.\nA scalar representing the number of elements to accumulate in a\nbatch. It determines the number of concurrent invocations of f that process\nelements from input_dataset in parallel.", "num_parallel_calls": "A Tensor of type int64.\nA scalar representing the maximum number of parallel invocations of the map_fn\nfunction. Applying the map_fn on consecutive input elements in parallel has\nthe potential to improve input pipeline throughput.", "drop_remainder": "A Tensor of type bool.\nA scalar representing whether the last batch should be dropped in case its size\nis smaller than desired.", "f": "A function decorated with @Defun.\nA function to apply to the outputs of input_dataset.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "preserve_cardinality": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ExperimentalMapDataset": {"description": "Creates a dataset that applies f to the outputs of input_dataset.", "Args": {"input_dataset": "A Tensor of type variant.", "other_arguments": "A list of Tensor objects.", "f": "A function decorated with @Defun.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "use_inter_op_parallelism": "An optional bool. Defaults to True.", "preserve_cardinality": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ExperimentalMatchingFilesDataset": {"Args": {"patterns": "A Tensor of type string.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ExperimentalMaxIntraOpParallelismDataset": {"description": "Creates a dataset that overrides the maximum intra-op parallelism.", "Args": {"input_dataset": "A Tensor of type variant.", "max_intra_op_parallelism": "A Tensor of type int64.\nIdentifies the maximum intra-op parallelism to use.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ExperimentalNonSerializableDataset": {"Args": {"input_dataset": "A Tensor of type variant.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ExperimentalParallelInterleaveDataset": {"description": "Creates a dataset that applies f to the outputs of input_dataset.", "Args": {"input_dataset": "A Tensor of type variant.", "other_arguments": "A list of Tensor objects.", "cycle_length": "A Tensor of type int64.", "block_length": "A Tensor of type int64.", "sloppy": "A Tensor of type bool.", "buffer_output_elements": "A Tensor of type int64.", "prefetch_input_elements": "A Tensor of type int64.", "f": "A function decorated with @Defun.\nA function mapping elements of input_dataset, concatenated with\nother_arguments, to a Dataset variant that contains elements matching\noutput_types and output_shapes.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ExperimentalParseExampleDataset": {"description": "Transforms input_dataset containing Example protos as vectors of DT_STRING into a dataset of Tensor or SparseTensor objects representing the parsed features.", "Args": {"input_dataset": "A Tensor of type variant.", "num_parallel_calls": "A Tensor of type int64.", "dense_defaults": "A list of Tensor objects with types from: float32, int64, string.\nA dict mapping string keys to Tensors.\nThe keys of the dict must match the dense_keys of the feature.", "sparse_keys": "A list of strings.\nA list of string keys in the examples features.\nThe results for these keys will be returned as SparseTensor objects.", "dense_keys": "A list of strings.\nA list of Ndense string Tensors (scalars).\nThe keys expected in the Examples features associated with dense values.", "sparse_types": "A list of tf.DTypes from: tf.float32, tf.int64, tf.string.\nA list of DTypes of the same length as sparse_keys.\nOnly tf.float32 (FloatList), tf.int64 (Int64List),\nand tf.string (BytesList) are supported.", "dense_shapes": "A list of shapes (each a tf.TensorShape or list of ints).\nList of tuples with the same length as dense_keys.\nThe shape of the data for each dense feature referenced by dense_keys.\nRequired for any input tensors identified by dense_keys.  Must be\neither fully defined, or may contain an unknown first dimension.\nAn unknown first dimension means the feature is treated as having\na variable number of blocks, and the output shape along this dimension\nis considered unknown at graph build time.  Padding is applied for\nminibatch elements smaller than the maximum number of blocks for the\ngiven feature along this dimension.", "output_types": "A list of tf.DTypes that has length >= 1.\nThe type list for the return values.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.\nThe list of shapes being produced.", "sloppy": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ExperimentalPrivateThreadPoolDataset": {"description": "Creates a dataset that uses a custom thread pool to compute input_dataset.", "Args": {"input_dataset": "A Tensor of type variant.", "num_threads": "A Tensor of type int64.\nIdentifies the number of threads to use for the private threadpool.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ExperimentalRandomDataset": {"description": "Creates a Dataset that returns pseudorandom numbers.", "Args": {"seed": "A Tensor of type int64.\nA scalar seed for the random number generator. If either seed or\nseed2 is set to be non-zero, the random number generator is seeded\nby the given seed.  Otherwise, a random seed is used.", "seed2": "A Tensor of type int64.\nA second scalar seed to avoid seed collision.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ExperimentalRebatchDataset": {"description": "Creates a dataset that changes the batch size.", "Args": {"input_dataset": "A Tensor of type variant.\nA variant tensor representing the input dataset.", "num_replicas": "A Tensor of type int64.\nA scalar representing the number of replicas to distribute this batch across. As\na result of this transformation the current batch size would end up being\ndivided  by this parameter.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "use_fallback": "An optional bool. Defaults to True.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ExperimentalScanDataset": {"description": "Creates a dataset successively reduces f over the elements of input_dataset.", "Args": {"input_dataset": "A Tensor of type variant.", "initial_state": "A list of Tensor objects.", "other_arguments": "A list of Tensor objects.", "f": "A function decorated with @Defun.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "preserve_cardinality": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ExperimentalSetStatsAggregatorDataset": {"Args": {"input_dataset": "A Tensor of type variant.", "stats_aggregator": "A Tensor of type resource.", "tag": "A Tensor of type string.", "counter_prefix": "A Tensor of type string.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ExperimentalSleepDataset": {"Args": {"input_dataset": "A Tensor of type variant.", "sleep_microseconds": "A Tensor of type int64.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ExperimentalSlidingWindowDataset": {"description": "Creates a dataset that passes a sliding window over input_dataset.", "Args": {"input_dataset": "A Tensor of type variant.", "window_size": "A Tensor of type int64.\nA scalar representing the number of elements in the\nsliding window.", "window_shift": "A Tensor of type int64.\nA scalar representing the steps moving the sliding window\nforward in one iteration. It must be positive.", "window_stride": "A Tensor of type int64.\nA scalar representing the stride of the input elements of the sliding window.\nIt must be positive.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ExperimentalSqlDataset": {"description": "Creates a dataset that executes a SQL query and emits rows of the result set.", "Args": {"driver_name": "A Tensor of type string.\nThe database type. Currently, the only supported type is 'sqlite'.", "data_source_name": "A Tensor of type string.\nA connection string to connect to the database.", "query": "A Tensor of type string. A SQL query to execute.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ExperimentalStatsAggregatorHandle": {"description": "Creates a statistics manager resource.", "Args": {"container": "An optional string. Defaults to \"\".", "shared_name": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type resource."}, "tf.raw_ops.ExperimentalStatsAggregatorSummary": {"description": "Produces a summary of any statistics recorded by the given statistics manager.", "Args": {"iterator": "A Tensor of type resource.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.ExperimentalTakeWhileDataset": {"description": "Creates a dataset that stops iteration when predicate is false.", "Args": {"input_dataset": "A Tensor of type variant.", "other_arguments": "A list of Tensor objects.\nA list of tensors, typically values that were captured when\nbuilding a closure for predicate.", "predicate": "A function decorated with @Defun.\nA function returning a scalar boolean.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ExperimentalThreadPoolDataset": {"description": "Creates a dataset that uses a custom thread pool to compute input_dataset.", "Args": {"input_dataset": "A Tensor of type variant.", "thread_pool": "A Tensor of type resource.\nA resource produced by the ThreadPoolHandle op.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ExperimentalThreadPoolHandle": {"description": "Creates a dataset that uses a custom thread pool to compute input_dataset.", "Args": {"num_threads": "An int. The number of threads in the thread pool.", "display_name": "A string.\nA human-readable name for the threads that may be visible in some\nvisualizations.\nthreadpool.", "max_intra_op_parallelism": "An optional int. Defaults to 1.\nThe maximum degree of parallelism to use within operations that execute on this\nthreadpool.", "container": "An optional string. Defaults to \"\".", "shared_name": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type resource."}, "tf.raw_ops.ExperimentalUnbatchDataset": {"description": "A dataset that splits the elements of its input into multiple elements.", "Args": {"input_dataset": "A Tensor of type variant.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ExperimentalUniqueDataset": {"description": "Creates a dataset that contains the unique elements of input_dataset.", "Args": {"input_dataset": "A Tensor of type variant.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.Expint": {"Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.Expm1": {"description": "Computes exp(x) - 1 element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, complex64, complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.ExtractGlimpse": {"description": "Extracts a glimpse from the input tensor.", "Args": {"input": "A Tensor of type float32.\nA 4-D float tensor of shape [batch_size, height, width, channels].", "size": "A Tensor of type int32.\nA 1-D tensor of 2 elements containing the size of the glimpses\nto extract.  The glimpse height must be specified first, following\nby the glimpse width.", "offsets": "A Tensor of type float32.\nA 2-D integer tensor of shape [batch_size, 2] containing\nthe y, x locations of the center of each window.", "centered": "An optional bool. Defaults to True.\nindicates if the offset coordinates are centered relative to\nthe image, in which case the (0, 0) offset is relative to the center\nof the input images. If false, the (0,0) offset corresponds to the\nupper left corner of the input images.", "normalized": "An optional bool. Defaults to True.\nindicates if the offset coordinates are normalized.", "uniform_noise": "An optional bool. Defaults to True.\nindicates if the noise should be generated using a\nuniform distribution or a Gaussian distribution.", "noise": "An optional string. Defaults to \"uniform\".\nindicates if the noise should uniform, gaussian, or\nzero. The default is uniform which means the noise type\nwill be decided by uniform_noise.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type float32."}, "tf.raw_ops.ExtractGlimpseV2": {"description": "Extracts a glimpse from the input tensor.", "Args": {"input": "A Tensor of type float32.\nA 4-D float tensor of shape [batch_size, height, width, channels].", "size": "A Tensor of type int32.\nA 1-D tensor of 2 elements containing the size of the glimpses\nto extract.  The glimpse height must be specified first, following\nby the glimpse width.", "offsets": "A Tensor of type float32.\nA 2-D integer tensor of shape [batch_size, 2] containing\nthe y, x locations of the center of each window.", "centered": "An optional bool. Defaults to True.\nindicates if the offset coordinates are centered relative to\nthe image, in which case the (0, 0) offset is relative to the center\nof the input images. If false, the (0,0) offset corresponds to the\nupper left corner of the input images.", "normalized": "An optional bool. Defaults to True.\nindicates if the offset coordinates are normalized.", "uniform_noise": "An optional bool. Defaults to True.\nindicates if the noise should be generated using a\nuniform distribution or a Gaussian distribution.", "noise": "An optional string. Defaults to \"uniform\".\nindicates if the noise should uniform, gaussian, or\nzero. The default is uniform which means the noise type\nwill be decided by uniform_noise.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type float32."}, "tf.raw_ops.ExtractImagePatches": {"description": "Extract patches from images and put them in the &#34;depth&#34; output dimension.", "Args": {"images": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, int8, int16, int32, int64, uint8, uint16, uint32, uint64, complex64, complex128, bool.\n4-D Tensor with shape [batch, in_rows, in_cols, depth].", "ksizes": "A list of ints that has length >= 4.\nThe size of the sliding window for each dimension of images.", "strides": "A list of ints that has length >= 4.\nHow far the centers of two consecutive patches are in\nthe images. Must be: [1, stride_rows, stride_cols, 1].", "rates": "A list of ints that has length >= 4.\nMust be: [1, rate_rows, rate_cols, 1]. This is the\ninput stride, specifying how far two consecutive patch samples are in the\ninput. Equivalent to extracting patches with\npatch_sizes_eff = patch_sizes + (patch_sizes - 1) * (rates - 1), followed by\nsubsampling them spatially by a factor of rates. This is equivalent to\nrate in dilated (a.k.a. Atrous) convolutions.", "padding": "A string from: \"SAME\", \"VALID\".\nThe type of padding algorithm to use.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as images."}, "tf.raw_ops.ExtractJpegShape": {"description": "Extract the shape information of a JPEG-encoded image.", "Args": {"contents": "A Tensor of type string. 0-D. The JPEG-encoded image.", "output_type": "An optional tf.DType from: tf.int32, tf.int64. Defaults to tf.int32.\n(Optional) The output type of the operation (int32 or int64).\nDefaults to int32.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type output_type."}, "tf.raw_ops.ExtractVolumePatches": {"description": "Extract patches from input and put them in the &#34;depth&#34; output dimension. 3D extension of extract_image_patches.", "Args": {"input": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.\n5-D Tensor with shape [batch, in_planes, in_rows, in_cols, depth].", "ksizes": "A list of ints that has length >= 5.\nThe size of the sliding window for each dimension of input.", "strides": "A list of ints that has length >= 5.\n1-D of length 5. How far the centers of two consecutive patches are in\ninput. Must be: [1, stride_planes, stride_rows, stride_cols, 1].", "padding": "A string from: \"SAME\", \"VALID\".\nThe type of padding algorithm to use.\nThe size-related attributes are specified as follows:\nksizes = [1, ksize_planes, ksize_rows, ksize_cols, 1]strides = [1, stride_planes, strides_rows, strides_cols, 1]", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.FFT": {"description": "Fast Fourier transform.", "Args": {"input": "A Tensor. Must be one of the following types: complex64, complex128.\nA complex tensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.FFT2D": {"description": "2D fast Fourier transform.", "Args": {"input": "A Tensor. Must be one of the following types: complex64, complex128.\nA complex tensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.FFT3D": {"description": "3D fast Fourier transform.", "Args": {"input": "A Tensor. Must be one of the following types: complex64, complex128.\nA complex tensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.FIFOQueue": {"description": "A queue that produces elements in first-in first-out order.", "Args": {"component_types": "A list of tf.DTypes that has length >= 1.\nThe type of each component in a value.", "shapes": "An optional list of shapes (each a tf.TensorShape or list of ints). Defaults to [].\nThe shape of each component in a value. The length of this attr must\nbe either 0 or the same as the length of component_types. If the length of\nthis attr is 0, the shapes of queue elements are not constrained, and\nonly one element may be dequeued at a time.", "capacity": "An optional int. Defaults to -1.\nThe upper bound on the number of elements in this queue.\nNegative numbers mean no limit.", "container": "An optional string. Defaults to \"\".\nIf non-empty, this queue is placed in the given container.\nOtherwise, a default container is used.", "shared_name": "An optional string. Defaults to \"\".\nIf non-empty, this queue will be shared under the given name\nacross multiple sessions.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type mutable string."}, "tf.raw_ops.FIFOQueueV2": {"description": "A queue that produces elements in first-in first-out order.", "Args": {"component_types": "A list of tf.DTypes that has length >= 1.\nThe type of each component in a value.", "shapes": "An optional list of shapes (each a tf.TensorShape or list of ints). Defaults to [].\nThe shape of each component in a value. The length of this attr must\nbe either 0 or the same as the length of component_types. If the length of\nthis attr is 0, the shapes of queue elements are not constrained, and\nonly one element may be dequeued at a time.", "capacity": "An optional int. Defaults to -1.\nThe upper bound on the number of elements in this queue.\nNegative numbers mean no limit.", "container": "An optional string. Defaults to \"\".\nIf non-empty, this queue is placed in the given container.\nOtherwise, a default container is used.", "shared_name": "An optional string. Defaults to \"\".\nIf non-empty, this queue will be shared under the given name\nacross multiple sessions.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type resource."}, "tf.raw_ops.Fact": {"description": "Output a fact about factorials.", "Args": {"name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.FakeParam": {"description": "This op is used as a placeholder in If branch functions. It doesn&#39;t provide a", "Args": {"dtype": "A tf.DType. The type of the output.", "shape": "A tf.TensorShape or list of ints.\nThe purported shape of the output. This is only used for shape inference;\nthe output will not necessarily have this shape. Can be a partial shape.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dtype."}, "tf.raw_ops.FakeQuantWithMinMaxArgs": {"description": "Fake-quantize the &#39;inputs&#39; tensor, type float to &#39;outputs&#39; tensor of same type.", "Args": {"inputs": "A Tensor of type float32.", "min": "An optional float. Defaults to -6.", "max": "An optional float. Defaults to 6.", "num_bits": "An optional int. Defaults to 8.", "narrow_range": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type float32."}, "tf.raw_ops.FakeQuantWithMinMaxArgsGradient": {"description": "Compute gradients for a FakeQuantWithMinMaxArgs operation.", "Args": {"gradients": "A Tensor of type float32.\nBackpropagated gradients above the FakeQuantWithMinMaxArgs operation.", "inputs": "A Tensor of type float32.\nValues passed as inputs to the FakeQuantWithMinMaxArgs operation.", "min": "An optional float. Defaults to -6.", "max": "An optional float. Defaults to 6.", "num_bits": "An optional int. Defaults to 8.", "narrow_range": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type float32."}, "tf.raw_ops.FakeQuantWithMinMaxVars": {"description": "Fake-quantize the &#39;inputs&#39; tensor of type float via global float scalars", "Args": {"inputs": "A Tensor of type float32.", "min": "A Tensor of type float32.", "max": "A Tensor of type float32.", "num_bits": "An optional int. Defaults to 8.", "narrow_range": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type float32."}, "tf.raw_ops.FakeQuantWithMinMaxVarsGradient": {"description": "Compute gradients for a FakeQuantWithMinMaxVars operation.", "Args": {"gradients": "A Tensor of type float32.\nBackpropagated gradients above the FakeQuantWithMinMaxVars operation.", "inputs": "A Tensor of type float32.\nValues passed as inputs to the FakeQuantWithMinMaxVars operation.\nmin, max: Quantization interval, scalar floats.", "min": "A Tensor of type float32.", "max": "A Tensor of type float32.", "num_bits": "An optional int. Defaults to 8.\nThe bitwidth of the quantization; between 2 and 8, inclusive.", "narrow_range": "An optional bool. Defaults to False.\nWhether to quantize into 2^num_bits - 1 distinct values.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (backprops_wrt_input, backprop_wrt_min, backprop_wrt_max)."}, "tf.raw_ops.FakeQuantWithMinMaxVarsPerChannel": {"description": "Fake-quantize the &#39;inputs&#39; tensor of type float via per-channel floats", "Args": {"inputs": "A Tensor of type float32.", "min": "A Tensor of type float32.", "max": "A Tensor of type float32.", "num_bits": "An optional int. Defaults to 8.", "narrow_range": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type float32."}, "tf.raw_ops.FakeQuantWithMinMaxVarsPerChannelGradient": {"description": "Compute gradients for a FakeQuantWithMinMaxVarsPerChannel operation.", "Args": {"gradients": "A Tensor of type float32.\nBackpropagated gradients above the FakeQuantWithMinMaxVars operation,\nshape one of: [d], [b, d],  [b, h, w, d].", "inputs": "A Tensor of type float32.\nValues passed as inputs to the FakeQuantWithMinMaxVars operation, shape\n  same as gradients.\nmin, max: Quantization interval, floats of shape [d].", "min": "A Tensor of type float32.", "max": "A Tensor of type float32.", "num_bits": "An optional int. Defaults to 8.\nThe bitwidth of the quantization; between 2 and 16, inclusive.", "narrow_range": "An optional bool. Defaults to False.\nWhether to quantize into 2^num_bits - 1 distinct values.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (backprops_wrt_input, backprop_wrt_min, backprop_wrt_max)."}, "tf.raw_ops.FakeQueue": {"description": "Deprecated. Do not use.", "Args": {"resource": "A Tensor of type resource.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type mutable string."}, "tf.raw_ops.Fill": {"description": "Creates a tensor filled with a scalar value.", "Args": {"dims": "A Tensor. Must be one of the following types: int32, int64.\n1-D. Represents the shape of the output tensor.", "value": "A Tensor. 0-D (scalar). Value to fill the returned tensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as value."}, "tf.raw_ops.FilterByLastComponentDataset": {"description": "Creates a dataset containing elements of first component of input_dataset having true in the last component.", "Args": {"input_dataset": "A Tensor of type variant.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.FilterDataset": {"description": "Creates a dataset containing elements of input_dataset matching predicate.", "Args": {"input_dataset": "A Tensor of type variant.", "other_arguments": "A list of Tensor objects.\nA list of tensors, typically values that were captured when\nbuilding a closure for predicate.", "predicate": "A function decorated with @Defun.\nA function returning a scalar boolean.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.FinalizeDataset": {"description": "Creates a dataset by applying tf.data.Options to input_dataset.", "Args": {"input_dataset": "A Tensor of type variant.\nA variant tensor representing the input dataset.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "has_captured_ref": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.Fingerprint": {"description": "Generates fingerprint values.", "Args": {"data": "A Tensor. Must have rank 1 or higher.", "method": "A Tensor of type string.\nFingerprint method used by this op. Currently available method is\nfarmhash::fingerprint64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type uint8."}, "tf.raw_ops.FixedLengthRecordDataset": {"description": "Creates a dataset that emits the records from one or more binary files.", "Args": {"filenames": "A Tensor of type string.\nA scalar or a vector containing the name(s) of the file(s) to be\nread.", "header_bytes": "A Tensor of type int64.\nA scalar representing the number of bytes to skip at the\nbeginning of a file.", "record_bytes": "A Tensor of type int64.\nA scalar representing the number of bytes in each record.", "footer_bytes": "A Tensor of type int64.\nA scalar representing the number of bytes to skip at the end\nof a file.", "buffer_size": "A Tensor of type int64.\nA scalar representing the number of bytes to buffer. Must be > 0.", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.FixedLengthRecordDatasetV2": {"Args": {"filenames": "A Tensor of type string.", "header_bytes": "A Tensor of type int64.", "record_bytes": "A Tensor of type int64.", "footer_bytes": "A Tensor of type int64.", "buffer_size": "A Tensor of type int64.", "compression_type": "A Tensor of type string.", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.FixedLengthRecordReader": {"description": "A Reader that outputs fixed-length records from a file.", "Args": {"record_bytes": "An int. Number of bytes in the record.", "header_bytes": "An optional int. Defaults to 0.\nNumber of bytes in the header, defaults to 0.", "footer_bytes": "An optional int. Defaults to 0.\nNumber of bytes in the footer, defaults to 0.", "hop_bytes": "An optional int. Defaults to 0.\nNumber of bytes to hop before each read. Default of 0 means using\nrecord_bytes.", "container": "An optional string. Defaults to \"\".\nIf non-empty, this reader is placed in the given container.\nOtherwise, a default container is used.", "shared_name": "An optional string. Defaults to \"\".\nIf non-empty, this reader is named in the given bucket\nwith this shared_name. Otherwise, the node name is used instead.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type mutable string."}, "tf.raw_ops.FixedLengthRecordReaderV2": {"description": "A Reader that outputs fixed-length records from a file.", "Args": {"record_bytes": "An int. Number of bytes in the record.", "header_bytes": "An optional int. Defaults to 0.\nNumber of bytes in the header, defaults to 0.", "footer_bytes": "An optional int. Defaults to 0.\nNumber of bytes in the footer, defaults to 0.", "hop_bytes": "An optional int. Defaults to 0.\nNumber of bytes to hop before each read. Default of 0 means using\nrecord_bytes.", "container": "An optional string. Defaults to \"\".\nIf non-empty, this reader is placed in the given container.\nOtherwise, a default container is used.", "shared_name": "An optional string. Defaults to \"\".\nIf non-empty, this reader is named in the given bucket\nwith this shared_name. Otherwise, the node name is used instead.", "encoding": "An optional string. Defaults to \"\".\nThe type of encoding for the file. Currently ZLIB and GZIP\nare supported. Defaults to none.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type resource."}, "tf.raw_ops.FixedUnigramCandidateSampler": {"description": "Generates labels for candidate sampling with a learned unigram distribution.", "Args": {"true_classes": "A Tensor of type int64.\nA batch_size * num_true matrix, in which each row contains the\nIDs of the num_true target_classes in the corresponding original label.", "num_true": "An int that is >= 1. Number of true labels per context.", "num_sampled": "An int that is >= 1.\nNumber of candidates to randomly sample.", "unique": "A bool.\nIf unique is true, we sample with rejection, so that all sampled\ncandidates in a batch are unique. This requires some approximation to\nestimate the post-rejection sampling probabilities.", "range_max": "An int that is >= 1.\nThe sampler will sample integers from the interval [0, range_max).", "vocab_file": "An optional string. Defaults to \"\".\nEach valid line in this file (which should have a CSV-like format)\ncorresponds to a valid word ID. IDs are in sequential order, starting from\nnum_reserved_ids. The last entry in each line is expected to be a value\ncorresponding to the count or relative probability. Exactly one of vocab_file\nand unigrams needs to be passed to this op.", "distortion": "An optional float. Defaults to 1.\nThe distortion is used to skew the unigram probability distribution.\nEach weight is first raised to the distortion's power before adding to the\ninternal unigram distribution. As a result, distortion = 1.0 gives regular\nunigram sampling (as defined by the vocab file), and distortion = 0.0 gives\na uniform distribution.", "num_reserved_ids": "An optional int. Defaults to 0.\nOptionally some reserved IDs can be added in the range [0,\n..., num_reserved_ids) by the users. One use case is that a special unknown\nword token is used as ID 0. These IDs will have a sampling probability of 0.", "num_shards": "An optional int that is >= 1. Defaults to 1.\nA sampler can be used to sample from a subset of the original range\nin order to speed up the whole computation through parallelism. This parameter\n(together with 'shard') indicates the number of partitions that are being\nused in the overall computation.", "shard": "An optional int that is >= 0. Defaults to 0.\nA sampler can be used to sample from a subset of the original range\nin order to speed up the whole computation through parallelism. This parameter\n(together with 'num_shards') indicates the particular partition number of a\nsampler op, when partitioning is being used.", "unigrams": "An optional list of floats. Defaults to [].\nA list of unigram counts or probabilities, one per ID in sequential\norder. Exactly one of vocab_file and unigrams should be passed to this op.", "seed": "An optional int. Defaults to 0.\nIf either seed or seed2 are set to be non-zero, the random number\ngenerator is seeded by the given seed.  Otherwise, it is seeded by a\nrandom seed.", "seed2": "An optional int. Defaults to 0.\nAn second seed to avoid seed collision.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (sampled_candidates, true_expected_count, sampled_expected_count)."}, "tf.raw_ops.FlatMapDataset": {"description": "Creates a dataset that applies f to the outputs of input_dataset.", "Args": {"input_dataset": "A Tensor of type variant.", "other_arguments": "A list of Tensor objects.", "f": "A function decorated with @Defun.\nA function mapping elements of input_dataset, concatenated with\nother_arguments, to a Dataset variant that contains elements matching\noutput_types and output_shapes.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.Floor": {"description": "Returns element-wise largest integer not greater than x.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.FloorDiv": {"description": "Returns x // y element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, uint8, int8, uint16, int16, int32, uint32, uint64, int64, complex64, complex128.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.FloorMod": {"description": "Returns element-wise remainder of division. When x &lt; 0 xor y &lt; 0 is", "Args": {"x": "A Tensor. Must be one of the following types: int8, int16, int32, int64, uint8, uint16, uint32, uint64, bfloat16, half, float32, float64.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.FlushSummaryWriter": {"Args": {"writer": "A Tensor of type resource.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.For": {"description": "Applies a for loop.", "Args": {"start": "A Tensor of type int32. The lower bound. An int32", "limit": "A Tensor of type int32. The upper bound. An int32", "delta": "A Tensor of type int32. The increment. An int32", "input": "A list of Tensor objects.\nA list of input tensors whose types are T.", "body": "A function decorated with @Defun.\nA function that takes a list of tensors (int32, T) and returns another\nlist of tensors (T).", "name": "A name for the operation (optional)."}, "Returns": "A list of Tensor objects. Has the same type as input."}, "tf.raw_ops.FractionalAvgPool": {"description": "Performs fractional average pooling on the input.", "Args": {"value": "A Tensor. Must be one of the following types: float32, float64, int32, int64.\n4-D with shape [batch, height, width, channels].", "pooling_ratio": "A list of floats that has length >= 4.\nPooling ratio for each dimension of value, currently only\nsupports row and col dimension and should be >= 1.0. For example, a valid\npooling ratio looks like [1.0, 1.44, 1.73, 1.0]. The first and last elements\nmust be 1.0 because we don't allow pooling on batch and channels\ndimensions. 1.44 and 1.73 are pooling ratio on height and width dimensions\nrespectively.", "pseudo_random": "An optional bool. Defaults to False.\nWhen set to True, generates the pooling sequence in a\npseudorandom fashion, otherwise, in a random fashion. Check paper Benjamin\nGraham, Fractional Max-Pooling for\ndifference between pseudorandom and random.", "overlapping": "An optional bool. Defaults to False.\nWhen set to True, it means when pooling, the values at the boundary\nof adjacent pooling cells are used by both cells. For example:\nindex  0  1  2  3  4\nvalue  20 5  16 3  7\nIf the pooling sequence is [0, 2, 4], then 16, at index 2 will be used twice.\nThe result would be [41/3, 26/3] for fractional avg pooling.", "deterministic": "An optional bool. Defaults to False.\nWhen set to True, a fixed pooling region will be used when\niterating over a FractionalAvgPool node in the computation graph. Mainly used\nin unit test to make FractionalAvgPool deterministic.", "seed": "An optional int. Defaults to 0.\nIf either seed or seed2 are set to be non-zero, the random number\ngenerator is seeded by the given seed.  Otherwise, it is seeded by a\nrandom seed.", "seed2": "An optional int. Defaults to 0.\nAn second seed to avoid seed collision.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output, row_pooling_sequence, col_pooling_sequence)."}, "tf.raw_ops.FractionalAvgPoolGrad": {"description": "Computes gradient of the FractionalAvgPool function.", "Args": {"orig_input_tensor_shape": "A Tensor of type int64.\nOriginal input tensor shape for fractional_avg_pool", "out_backprop": "A Tensor. Must be one of the following types: float32, float64, int32, int64.\n4-D with shape [batch, height, width, channels].  Gradients\nw.r.t. the output of fractional_avg_pool.", "row_pooling_sequence": "A Tensor of type int64.\nrow pooling sequence, form pooling region with\ncol_pooling_sequence.", "col_pooling_sequence": "A Tensor of type int64.\ncolumn pooling sequence, form pooling region with\nrow_pooling sequence.", "overlapping": "An optional bool. Defaults to False.\nWhen set to True, it means when pooling, the values at the boundary\nof adjacent pooling cells are used by both cells. For example:\nindex  0  1  2  3  4\nvalue  20 5  16 3  7\nIf the pooling sequence is [0, 2, 4], then 16, at index 2 will be used twice.\nThe result would be [41/3, 26/3] for fractional avg pooling.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as out_backprop."}, "tf.raw_ops.FractionalMaxPool": {"description": "Performs fractional max pooling on the input.", "Args": {"value": "A Tensor. Must be one of the following types: float32, float64, int32, int64.\n4-D with shape [batch, height, width, channels].", "pooling_ratio": "A list of floats that has length >= 4.\nPooling ratio for each dimension of value, currently only\nsupports row and col dimension and should be >= 1.0. For example, a valid\npooling ratio looks like [1.0, 1.44, 1.73, 1.0]. The first and last elements\nmust be 1.0 because we don't allow pooling on batch and channels\ndimensions. 1.44 and 1.73 are pooling ratio on height and width dimensions\nrespectively.", "pseudo_random": "An optional bool. Defaults to False.\nWhen set to True, generates the pooling sequence in a\npseudorandom fashion, otherwise, in a random fashion. Check paper Benjamin\nGraham, Fractional Max-Pooling for\ndifference between pseudorandom and random.", "overlapping": "An optional bool. Defaults to False.\nWhen set to True, it means when pooling, the values at the boundary\nof adjacent pooling cells are used by both cells. For example:\nindex  0  1  2  3  4\nvalue  20 5  16 3  7\nIf the pooling sequence is [0, 2, 4], then 16, at index 2 will be used twice.\nThe result would be [20, 16] for fractional max pooling.", "deterministic": "An optional bool. Defaults to False.\nWhen set to True, a fixed pooling region will be used when\niterating over a FractionalMaxPool node in the computation graph. Mainly used\nin unit test to make FractionalMaxPool deterministic.", "seed": "An optional int. Defaults to 0.\nIf either seed or seed2 are set to be non-zero, the random number\ngenerator is seeded by the given seed.  Otherwise, it is seeded by a\nrandom seed.", "seed2": "An optional int. Defaults to 0.\nAn second seed to avoid seed collision.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output, row_pooling_sequence, col_pooling_sequence)."}, "tf.raw_ops.FractionalMaxPoolGrad": {"description": "Computes gradient of the FractionalMaxPool function.", "Args": {"orig_input": "A Tensor. Must be one of the following types: float32, float64, int32, int64.\nOriginal input for fractional_max_pool", "orig_output": "A Tensor. Must have the same type as orig_input.\nOriginal output for fractional_max_pool", "out_backprop": "A Tensor. Must have the same type as orig_input.\n4-D with shape [batch, height, width, channels].  Gradients\nw.r.t. the output of fractional_max_pool.", "row_pooling_sequence": "A Tensor of type int64.\nrow pooling sequence, form pooling region with\ncol_pooling_sequence.", "col_pooling_sequence": "A Tensor of type int64.\ncolumn pooling sequence, form pooling region with\nrow_pooling sequence.", "overlapping": "An optional bool. Defaults to False.\nWhen set to True, it means when pooling, the values at the boundary\nof adjacent pooling cells are used by both cells. For example:\nindex  0  1  2  3  4\nvalue  20 5  16 3  7\nIf the pooling sequence is [0, 2, 4], then 16, at index 2 will be used twice.\nThe result would be [20, 16] for fractional max pooling.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as orig_input."}, "tf.raw_ops.FresnelCos": {"Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.FresnelSin": {"Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.FusedBatchNorm": {"description": "Batch normalization.", "Args": {"x": "A Tensor. Must be one of the following types: float32.\nA 4D Tensor for input data.", "scale": "A Tensor. Must have the same type as x.\nA 1D Tensor for scaling factor, to scale the normalized x.", "offset": "A Tensor. Must have the same type as x.\nA 1D Tensor for offset, to shift to the normalized x.", "mean": "A Tensor. Must have the same type as x.\nA 1D Tensor for population mean. Used for inference only;\nmust be empty for training.", "variance": "A Tensor. Must have the same type as x.\nA 1D Tensor for population variance. Used for inference only;\nmust be empty for training.", "epsilon": "An optional float. Defaults to 0.0001.\nA small float number added to the variance of x.", "exponential_avg_factor": "An optional float. Defaults to 1.", "data_format": "An optional string from: \"NHWC\", \"NCHW\". Defaults to \"NHWC\".\nThe data format for x and y. Either \"NHWC\" (default) or \"NCHW\".", "is_training": "An optional bool. Defaults to True.\nA bool value to indicate the operation is for training (default)\nor inference.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (y, batch_mean, batch_variance, reserve_space_1, reserve_space_2)."}, "tf.raw_ops.FusedBatchNormGrad": {"description": "Gradient for batch normalization.", "Args": {"y_backprop": "A Tensor. Must be one of the following types: float32.\nA 4D Tensor for the gradient with respect to y.", "x": "A Tensor. Must have the same type as y_backprop.\nA 4D Tensor for input data.", "scale": "A Tensor. Must have the same type as y_backprop.\nA 1D Tensor for scaling factor, to scale the normalized x.", "reserve_space_1": "A Tensor. Must have the same type as y_backprop.\nWhen is_training is True, a 1D Tensor for the computed batch\nmean to be reused in gradient computation. When is_training is\nFalse, a 1D Tensor for the population mean to be reused in both\n1st and 2nd order gradient computation.", "reserve_space_2": "A Tensor. Must have the same type as y_backprop.\nWhen is_training is True, a 1D Tensor for the computed batch\nvariance (inverted variance in the cuDNN case) to be reused in\ngradient computation. When is_training is False, a 1D Tensor\nfor the population variance to be reused in both 1st and 2nd\norder gradient computation.", "epsilon": "An optional float. Defaults to 0.0001.\nA small float number added to the variance of x.", "data_format": "An optional string from: \"NHWC\", \"NCHW\". Defaults to \"NHWC\".\nThe data format for y_backprop, x, x_backprop.\nEither \"NHWC\" (default) or \"NCHW\".", "is_training": "An optional bool. Defaults to True.\nA bool value to indicate the operation is for training (default)\nor inference.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (x_backprop, scale_backprop, offset_backprop, reserve_space_3, reserve_space_4)."}, "tf.raw_ops.FusedBatchNormGradV2": {"description": "Gradient for batch normalization.", "Args": {"y_backprop": "A Tensor. Must be one of the following types: half, bfloat16, float32.\nA 4D Tensor for the gradient with respect to y.", "x": "A Tensor. Must have the same type as y_backprop.\nA 4D Tensor for input data.", "scale": "A Tensor of type float32.\nA 1D Tensor for scaling factor, to scale the normalized x.", "reserve_space_1": "A Tensor. Must be one of the following types: float32.\nWhen is_training is True, a 1D Tensor for the computed batch\nmean to be reused in gradient computation. When is_training is\nFalse, a 1D Tensor for the population mean to be reused in both\n1st and 2nd order gradient computation.", "reserve_space_2": "A Tensor. Must have the same type as reserve_space_1.\nWhen is_training is True, a 1D Tensor for the computed batch\nvariance (inverted variance in the cuDNN case) to be reused in\ngradient computation. When is_training is False, a 1D Tensor\nfor the population variance to be reused in both 1st and 2nd\norder gradient computation.", "epsilon": "An optional float. Defaults to 0.0001.\nA small float number added to the variance of x.", "data_format": "An optional string from: \"NHWC\", \"NCHW\". Defaults to \"NHWC\".\nThe data format for y_backprop, x, x_backprop.\nEither \"NHWC\" (default) or \"NCHW\".", "is_training": "An optional bool. Defaults to True.\nA bool value to indicate the operation is for training (default)\nor inference.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (x_backprop, scale_backprop, offset_backprop, reserve_space_3, reserve_space_4)."}, "tf.raw_ops.FusedBatchNormGradV3": {"description": "Gradient for batch normalization.", "Args": {"y_backprop": "A Tensor. Must be one of the following types: half, bfloat16, float32.\nA 4D Tensor for the gradient with respect to y.", "x": "A Tensor. Must have the same type as y_backprop.\nA 4D Tensor for input data.", "scale": "A Tensor of type float32.\nA 1D Tensor for scaling factor, to scale the normalized x.", "reserve_space_1": "A Tensor. Must be one of the following types: float32.\nWhen is_training is True, a 1D Tensor for the computed batch\nmean to be reused in gradient computation. When is_training is\nFalse, a 1D Tensor for the population mean to be reused in both\n1st and 2nd order gradient computation.", "reserve_space_2": "A Tensor. Must have the same type as reserve_space_1.\nWhen is_training is True, a 1D Tensor for the computed batch\nvariance (inverted variance in the cuDNN case) to be reused in\ngradient computation. When is_training is False, a 1D Tensor\nfor the population variance to be reused in both 1st and 2nd\norder gradient computation.", "reserve_space_3": "A Tensor. Must have the same type as reserve_space_1.\nWhen is_training is True, a 1D Tensor for some intermediate results to be reused\nin gradient computation. When is_training is False, a dummy empty Tensor will be\ncreated.", "epsilon": "An optional float. Defaults to 0.0001.\nA small float number added to the variance of x.", "data_format": "An optional string from: \"NHWC\", \"NCHW\", \"NDHWC\", \"NCDHW\". Defaults to \"NHWC\".\nThe data format for y_backprop, x, x_backprop.\nEither \"NHWC\" (default) or \"NCHW\".", "is_training": "An optional bool. Defaults to True.\nA bool value to indicate the operation is for training (default)\nor inference.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (x_backprop, scale_backprop, offset_backprop, reserve_space_4, reserve_space_5)."}, "tf.raw_ops.FusedBatchNormV2": {"description": "Batch normalization.", "Args": {"x": "A Tensor. Must be one of the following types: half, bfloat16, float32.\nA 4D Tensor for input data.", "scale": "A Tensor. Must be one of the following types: float32.\nA 1D Tensor for scaling factor, to scale the normalized x.", "offset": "A Tensor. Must have the same type as scale.\nA 1D Tensor for offset, to shift to the normalized x.", "mean": "A Tensor. Must have the same type as scale.\nA 1D Tensor for population mean. Used for inference only;\nmust be empty for training.", "variance": "A Tensor. Must have the same type as scale.\nA 1D Tensor for population variance. Used for inference only;\nmust be empty for training.", "epsilon": "An optional float. Defaults to 0.0001.\nA small float number added to the variance of x.", "exponential_avg_factor": "An optional float. Defaults to 1.", "data_format": "An optional string from: \"NHWC\", \"NCHW\". Defaults to \"NHWC\".\nThe data format for x and y. Either \"NHWC\" (default) or \"NCHW\".", "is_training": "An optional bool. Defaults to True.\nA bool value to indicate the operation is for training (default)\nor inference.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (y, batch_mean, batch_variance, reserve_space_1, reserve_space_2)."}, "tf.raw_ops.FusedBatchNormV3": {"description": "Batch normalization.", "Args": {"x": "A Tensor. Must be one of the following types: half, bfloat16, float32.\nA 4D Tensor for input data.", "scale": "A Tensor. Must be one of the following types: bfloat16, float32.\nA 1D Tensor for scaling factor, to scale the normalized x.", "offset": "A Tensor. Must have the same type as scale.\nA 1D Tensor for offset, to shift to the normalized x.", "mean": "A Tensor. Must have the same type as scale.\nA 1D Tensor for population mean. Used for inference only;\nmust be empty for training.", "variance": "A Tensor. Must have the same type as scale.\nA 1D Tensor for population variance. Used for inference only;\nmust be empty for training.", "epsilon": "An optional float. Defaults to 0.0001.\nA small float number added to the variance of x.", "exponential_avg_factor": "An optional float. Defaults to 1.", "data_format": "An optional string from: \"NHWC\", \"NCHW\", \"NDHWC\", \"NCDHW\". Defaults to \"NHWC\".\nThe data format for x and y. Either \"NHWC\" (default) or \"NCHW\".", "is_training": "An optional bool. Defaults to True.\nA bool value to indicate the operation is for training (default)\nor inference.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (y, batch_mean, batch_variance, reserve_space_1, reserve_space_2, reserve_space_3)."}, "tf.raw_ops.FusedPadConv2D": {"description": "Performs a padding as a preprocess during a convolution.", "Args": {"input": "A Tensor. Must be one of the following types: half, float32, float64.\n4-D with shape [batch, in_height, in_width, in_channels].", "paddings": "A Tensor of type int32.\nA two-column matrix specifying the padding sizes. The number of\nrows must be the same as the rank of input.", "filter": "A Tensor. Must have the same type as input. 4-D with shape\n[filter_height, filter_width, in_channels, out_channels].", "mode": "A string from: \"REFLECT\", \"SYMMETRIC\".", "strides": "A list of ints.\n1-D of length 4.  The stride of the sliding window for each dimension\nof input. Must be in the same order as the dimension specified with format.", "padding": "A string from: \"SAME\", \"VALID\".\nThe type of padding algorithm to use.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.FusedResizeAndPadConv2D": {"description": "Performs a resize and padding as a preprocess during a convolution.", "Args": {"input": "A Tensor. Must be one of the following types: half, float32, float64.\n4-D with shape [batch, in_height, in_width, in_channels].", "size": "A Tensor of type int32.\nA 1-D int32 Tensor of 2 elements: new_height, new_width.  The\nnew size for the images.", "paddings": "A Tensor of type int32.\nA two-column matrix specifying the padding sizes. The number of\nrows must be the same as the rank of input.", "filter": "A Tensor. Must have the same type as input. 4-D with shape\n[filter_height, filter_width, in_channels, out_channels].", "mode": "A string from: \"REFLECT\", \"SYMMETRIC\".", "strides": "A list of ints.\n1-D of length 4.  The stride of the sliding window for each dimension\nof input. Must be in the same order as the dimension specified with format.", "padding": "A string from: \"SAME\", \"VALID\".\nThe type of padding algorithm to use.", "resize_align_corners": "An optional bool. Defaults to False.\nIf true, the centers of the 4 corner pixels of the input and output tensors are\naligned, preserving the values at the corner pixels. Defaults to false.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.GRUBlockCell": {"description": "Computes the GRU cell forward propagation for 1 time step.", "Args": {"x": "A Tensor. Must be one of the following types: float32.", "h_prev": "A Tensor. Must have the same type as x.", "w_ru": "A Tensor. Must have the same type as x.", "w_c": "A Tensor. Must have the same type as x.", "b_ru": "A Tensor. Must have the same type as x.", "b_c": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (r, u, c, h)."}, "tf.raw_ops.GRUBlockCellGrad": {"description": "Computes the GRU cell back-propagation for 1 time step.", "Args": {"x": "A Tensor. Must be one of the following types: float32.", "h_prev": "A Tensor. Must have the same type as x.", "w_ru": "A Tensor. Must have the same type as x.", "w_c": "A Tensor. Must have the same type as x.", "b_ru": "A Tensor. Must have the same type as x.", "b_c": "A Tensor. Must have the same type as x.", "r": "A Tensor. Must have the same type as x.", "u": "A Tensor. Must have the same type as x.", "c": "A Tensor. Must have the same type as x.", "d_h": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (d_x, d_h_prev, d_c_bar, d_r_bar_u_bar)."}, "tf.raw_ops.Gather": {"description": "Gather slices from params according to indices.", "Args": {"params": "A Tensor.", "indices": "A Tensor. Must be one of the following types: int32, int64.", "validate_indices": "An optional bool. Defaults to True.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as params."}, "tf.raw_ops.GatherNd": {"description": "Gather slices from params into a Tensor with shape specified by indices.", "Args": {"params": "A Tensor. The tensor from which to gather values.", "indices": "A Tensor. Must be one of the following types: int16, int32, int64.\nIndex tensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as params."}, "tf.raw_ops.GatherV2": {"description": "Gather slices from params axis axis according to indices.", "Args": {"params": "A Tensor.\nThe tensor from which to gather values. Must be at least rank\naxis + 1.", "indices": "A Tensor. Must be one of the following types: int16, int32, int64.\nIndex tensor. Must be in range [0, params.shape[axis]).", "axis": "A Tensor. Must be one of the following types: int32, int64.\nThe axis in params to gather indices from. Defaults to the first\ndimension. Supports negative indexes.", "batch_dims": "An optional int. Defaults to 0.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as params."}, "tf.raw_ops.GenerateBoundingBoxProposals": {"description": "This op produces Region of Interests from given bounding boxes(bbox_deltas) encoded wrt anchors according to eq.2 in arXiv:1506.01497", "Args": {"scores": "A Tensor of type float32.\nA 4-D float tensor of shape [num_images, height, width, num_achors] containing scores of the boxes for given anchors, can be unsorted.", "bbox_deltas": "A Tensor of type float32.\nA 4-D float tensor of shape [num_images, height, width, 4 x num_anchors]. encoding boxes with respec to each anchor.\nCoordinates are given in the form [dy, dx, dh, dw].", "image_info": "A Tensor of type float32.\nA 2-D float tensor of shape [num_images, 5] containing image information Height, Width, Scale.", "anchors": "A Tensor of type float32.\nA 2-D float tensor of shape [num_anchors, 4] describing the anchor boxes. Boxes are formatted in the form [y1, x1, y2, x2].", "nms_threshold": "A Tensor of type float32.\nA scalar float tensor for non-maximal-suppression threshold.", "pre_nms_topn": "A Tensor of type int32.\nA scalar int tensor for the number of top scoring boxes to be used as input.", "min_size": "A Tensor of type float32.\nA scalar float tensor. Any box that has a smaller size than min_size will be discarded.", "post_nms_topn": "An optional int. Defaults to 300.\nAn integer. Maximum number of rois in the output.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (rois, roi_probabilities)."}, "tf.raw_ops.GenerateVocabRemapping": {"description": "Given a path to new and old vocabulary files, returns a remapping Tensor of", "Args": {"new_vocab_file": "A Tensor of type string. Path to the new vocab file.", "old_vocab_file": "A Tensor of type string. Path to the old vocab file.", "new_vocab_offset": "An int that is >= 0.\nHow many entries into the new vocab file to start reading.", "num_new_vocab": "An int that is >= 0.\nNumber of entries in the new vocab file to remap.", "old_vocab_size": "An optional int that is >= -1. Defaults to -1.\nNumber of entries in the old vocab file to consider.  If -1,\nuse the entire old vocabulary.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (remapping, num_present)."}, "tf.raw_ops.GeneratorDataset": {"description": "Creates a dataset that invokes a function to generate elements.", "Args": {"init_func_other_args": "A list of Tensor objects.", "next_func_other_args": "A list of Tensor objects.", "finalize_func_other_args": "A list of Tensor objects.", "init_func": "A function decorated with @Defun.", "next_func": "A function decorated with @Defun.", "finalize_func": "A function decorated with @Defun.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.GetElementAtIndex": {"description": "Gets the element at the specified index in a dataset.", "Args": {"dataset": "A Tensor of type variant.", "index": "A Tensor of type int64.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A list of Tensor objects of type output_types."}, "tf.raw_ops.GetOptions": {"description": "Returns the tf.data.Options attached to input_dataset.", "Args": {"input_dataset": "A Tensor of type variant.\nA variant tensor representing the input dataset.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.GetSessionHandle": {"description": "Store the input tensor in the state of the current session.", "Args": {"value": "A Tensor. The tensor to be stored.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.GetSessionHandleV2": {"description": "Store the input tensor in the state of the current session.", "Args": {"value": "A Tensor. The tensor to be stored.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type resource."}, "tf.raw_ops.GetSessionTensor": {"description": "Get the value of the tensor specified by its handle.", "Args": {"handle": "A Tensor of type string.\nThe handle for a tensor stored in the session state.", "dtype": "A tf.DType. The type of the output value.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dtype."}, "tf.raw_ops.Greater": {"description": "Returns the truth value of (x &gt; y) element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type bool."}, "tf.raw_ops.GreaterEqual": {"description": "Returns the truth value of (x &gt;= y) element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type bool."}, "tf.raw_ops.GroupByReducerDataset": {"description": "Creates a dataset that computes a group-by on input_dataset.", "Args": {"input_dataset": "A Tensor of type variant.\nA variant tensor representing the input dataset.", "key_func_other_arguments": "A list of Tensor objects.\nA list of tensors, typically values that were captured when\nbuilding a closure for key_func.", "init_func_other_arguments": "A list of Tensor objects.\nA list of tensors, typically values that were captured when\nbuilding a closure for init_func.", "reduce_func_other_arguments": "A list of Tensor objects.\nA list of tensors, typically values that were captured when\nbuilding a closure for reduce_func.", "finalize_func_other_arguments": "A list of Tensor objects.\nA list of tensors, typically values that were captured when\nbuilding a closure for finalize_func.", "key_func": "A function decorated with @Defun.\nA function mapping an element of input_dataset, concatenated\nwith key_func_other_arguments to a scalar value of type DT_INT64.", "init_func": "A function decorated with @Defun.\nA function mapping a key of type DT_INT64, concatenated with\ninit_func_other_arguments to the initial reducer state.", "reduce_func": "A function decorated with @Defun.\nA function mapping the current reducer state and an element of input_dataset,\nconcatenated with reduce_func_other_arguments to a new reducer state.", "finalize_func": "A function decorated with @Defun.\nA function mapping the final reducer state to an output element.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.GroupByWindowDataset": {"description": "Creates a dataset that computes a windowed group-by on input_dataset.", "Args": {"input_dataset": "A Tensor of type variant.", "key_func_other_arguments": "A list of Tensor objects.", "reduce_func_other_arguments": "A list of Tensor objects.", "window_size_func_other_arguments": "A list of Tensor objects.", "key_func": "A function decorated with @Defun.\nA function mapping an element of input_dataset, concatenated\nwith key_func_other_arguments to a scalar value of type DT_INT64.", "reduce_func": "A function decorated with @Defun.", "window_size_func": "A function decorated with @Defun.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.GuaranteeConst": {"description": "Gives a guarantee to the TF runtime that the input tensor is a constant.", "Args": {"input": "A Tensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.HSVToRGB": {"description": "Convert one or more images from HSV to RGB.", "Args": {"images": "A Tensor. Must be one of the following types: half, bfloat16, float32, float64.\n1-D or higher rank. HSV data to convert. Last dimension must be size 3.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as images."}, "tf.raw_ops.HashTable": {"description": "Creates a non-initialized hash table.", "Args": {"key_dtype": "A tf.DType. Type of the table keys.", "value_dtype": "A tf.DType. Type of the table values.", "container": "An optional string. Defaults to \"\".\nIf non-empty, this table is placed in the given container.\nOtherwise, a default container is used.", "shared_name": "An optional string. Defaults to \"\".\nIf non-empty, this table is shared under the given name across\nmultiple sessions.", "use_node_name_sharing": "An optional bool. Defaults to False.\nIf true and shared_name is empty, the table is shared\nusing the node name.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type mutable string."}, "tf.raw_ops.HashTableV2": {"description": "Creates a non-initialized hash table.", "Args": {"key_dtype": "A tf.DType. Type of the table keys.", "value_dtype": "A tf.DType. Type of the table values.", "container": "An optional string. Defaults to \"\".\nIf non-empty, this table is placed in the given container.\nOtherwise, a default container is used.", "shared_name": "An optional string. Defaults to \"\".\nIf non-empty, this table is shared under the given name across\nmultiple sessions.", "use_node_name_sharing": "An optional bool. Defaults to False.\nIf true and shared_name is empty, the table is shared\nusing the node name.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type resource."}, "tf.raw_ops.HistogramFixedWidth": {"description": "Return histogram of values.", "Args": {"values": "A Tensor. Must be one of the following types: int32, int64, float32, float64.\nNumeric Tensor.", "value_range": "A Tensor. Must have the same type as values.\nShape [2] Tensor of same dtype as values.\nvalues <= value_range[0] will be mapped to hist[0],\nvalues >= value_range[1] will be mapped to hist[-1].", "nbins": "A Tensor of type int32.\nScalar int32 Tensor.  Number of histogram bins.", "dtype": "An optional tf.DType from: tf.int32, tf.int64. Defaults to tf.int32.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dtype."}, "tf.raw_ops.HistogramSummary": {"description": "Outputs a Summary protocol buffer with a histogram.", "Args": {"tag": "A Tensor of type string.\nScalar.  Tag to use for the Summary.Value.", "values": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.\nAny shape. Values to use to build the histogram.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.IFFT": {"description": "Inverse fast Fourier transform.", "Args": {"input": "A Tensor. Must be one of the following types: complex64, complex128.\nA complex tensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.IFFT2D": {"description": "Inverse 2D fast Fourier transform.", "Args": {"input": "A Tensor. Must be one of the following types: complex64, complex128.\nA complex tensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.IFFT3D": {"description": "Inverse 3D fast Fourier transform.", "Args": {"input": "A Tensor. Must be one of the following types: complex64, complex128.\nA complex tensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.IRFFT": {"description": "Inverse real-valued fast Fourier transform.", "Args": {"input": "A Tensor. Must be one of the following types: complex64, complex128.\nA complex tensor.", "fft_length": "A Tensor of type int32.\nAn int32 tensor of shape [1]. The FFT length.", "Treal": "An optional tf.DType from: tf.float32, tf.float64. Defaults to tf.float32.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type Treal."}, "tf.raw_ops.IRFFT2D": {"description": "Inverse 2D real-valued fast Fourier transform.", "Args": {"input": "A Tensor. Must be one of the following types: complex64, complex128.\nA complex tensor.", "fft_length": "A Tensor of type int32.\nAn int32 tensor of shape [2]. The FFT length for each dimension.", "Treal": "An optional tf.DType from: tf.float32, tf.float64. Defaults to tf.float32.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type Treal."}, "tf.raw_ops.IRFFT3D": {"description": "Inverse 3D real-valued fast Fourier transform.", "Args": {"input": "A Tensor. Must be one of the following types: complex64, complex128.\nA complex tensor.", "fft_length": "A Tensor of type int32.\nAn int32 tensor of shape [3]. The FFT length for each dimension.", "Treal": "An optional tf.DType from: tf.float32, tf.float64. Defaults to tf.float32.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type Treal."}, "tf.raw_ops.Identity": {"description": "Return a tensor with the same shape and contents as the input tensor or value.", "Args": {"input": "A Tensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.IdentityN": {"description": "Returns a list of tensors with the same shapes and contents as the input", "Args": {"input": "A list of Tensor objects.", "name": "A name for the operation (optional)."}, "Returns": "A list of Tensor objects. Has the same type as input."}, "tf.raw_ops.IdentityReader": {"description": "A Reader that outputs the queued work as both the key and value.", "Args": {"container": "An optional string. Defaults to \"\".\nIf non-empty, this reader is placed in the given container.\nOtherwise, a default container is used.", "shared_name": "An optional string. Defaults to \"\".\nIf non-empty, this reader is named in the given bucket\nwith this shared_name. Otherwise, the node name is used instead.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type mutable string."}, "tf.raw_ops.IdentityReaderV2": {"description": "A Reader that outputs the queued work as both the key and value.", "Args": {"container": "An optional string. Defaults to \"\".\nIf non-empty, this reader is placed in the given container.\nOtherwise, a default container is used.", "shared_name": "An optional string. Defaults to \"\".\nIf non-empty, this reader is named in the given bucket\nwith this shared_name. Otherwise, the node name is used instead.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type resource."}, "tf.raw_ops.If": {"description": "output = cond ? then_branch(input) : else_branch(input)", "Args": {"cond": "A Tensor.\nA Tensor. If the tensor is a scalar of non-boolean type, the\nscalar is converted to a boolean according to the\nfollowing rule: if the scalar is a numerical value, non-zero means\nTrue and zero means False; if the scalar is a string, non-empty\nmeans True and empty means False. If the tensor is not a scalar,\nbeing empty means False and being non-empty means True.", "input": "A list of Tensor objects. A list of input tensors.", "Tout": "A list of tf.DTypes. A list of output types.", "then_branch": "A function decorated with @Defun.\nA function that takes 'inputs' and returns a list of tensors, whose\ntypes are the same as what else_branch returns.", "else_branch": "A function decorated with @Defun.\nA function that takes 'inputs' and returns a list of tensors, whose\ntypes are the same as what then_branch returns.", "output_shapes": "An optional list of shapes (each a tf.TensorShape or list of ints). Defaults to [].", "name": "A name for the operation (optional)."}, "Returns": "A list of Tensor objects of type Tout."}, "tf.raw_ops.Igamma": {"description": "Compute the lower regularized incomplete Gamma function P(a, x).", "Args": {"a": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "x": "A Tensor. Must have the same type as a.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as a."}, "tf.raw_ops.IgammaGradA": {"description": "Computes the gradient of igamma(a, x) wrt a.", "Args": {"a": "A Tensor. Must be one of the following types: float32, float64.", "x": "A Tensor. Must have the same type as a.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as a."}, "tf.raw_ops.Igammac": {"description": "Compute the upper regularized incomplete Gamma function Q(a, x).", "Args": {"a": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "x": "A Tensor. Must have the same type as a.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as a."}, "tf.raw_ops.IgnoreErrorsDataset": {"description": "Creates a dataset that contains the elements of input_dataset ignoring errors.", "Args": {"input_dataset": "A Tensor of type variant.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "log_warning": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.Imag": {"description": "Returns the imaginary part of a complex number.", "Args": {"input": "A Tensor. Must be one of the following types: complex64, complex128.", "Tout": "An optional tf.DType from: tf.float32, tf.float64. Defaults to tf.float32.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type Tout."}, "tf.raw_ops.ImageProjectiveTransformV2": {"description": "Applies the given transform to each of the images.", "Args": {"images": "A Tensor. Must be one of the following types: uint8, int32, int64, half, float32, float64.\n4-D with shape [batch, height, width, channels].", "transforms": "A Tensor of type float32.\n2-D Tensor, [batch, 8] or [1, 8] matrix, where each row corresponds to a 3 x 3\nprojective transformation matrix, with the last entry assumed to be 1. If there\nis one row, the same transformation will be applied to all images.", "output_shape": "A Tensor of type int32.\n1-D Tensor [new_height, new_width].", "interpolation": "A string. Interpolation method, \"NEAREST\" or \"BILINEAR\".", "fill_mode": "An optional string. Defaults to \"CONSTANT\".\nFill mode, \"REFLECT\", \"WRAP\", or \"CONSTANT\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as images."}, "tf.raw_ops.ImageProjectiveTransformV3": {"description": "Applies the given transform to each of the images.", "Args": {"images": "A Tensor. Must be one of the following types: uint8, int32, int64, half, float32, float64.\n4-D with shape [batch, height, width, channels].", "transforms": "A Tensor of type float32.\n2-D Tensor, [batch, 8] or [1, 8] matrix, where each row corresponds to a 3 x 3\nprojective transformation matrix, with the last entry assumed to be 1. If there\nis one row, the same transformation will be applied to all images.", "output_shape": "A Tensor of type int32.\n1-D Tensor [new_height, new_width].", "fill_value": "A Tensor of type float32.\nfloat, the value to be filled when fill_mode is constant\".", "interpolation": "A string. Interpolation method, \"NEAREST\" or \"BILINEAR\".", "fill_mode": "An optional string. Defaults to \"CONSTANT\".\nFill mode, \"REFLECT\", \"WRAP\", \"CONSTANT\", or \"NEAREST\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as images."}, "tf.raw_ops.ImageSummary": {"description": "Outputs a Summary protocol buffer with images.", "Args": {"tag": "A Tensor of type string.\nScalar. Used to build the tag attribute of the summary values.", "tensor": "A Tensor. Must be one of the following types: uint8, float32, half, float64.\n4-D of shape [batch_size, height, width, channels] where\nchannels is 1, 3, or 4.", "max_images": "An optional int that is >= 1. Defaults to 3.\nMax number of batch elements to generate images for.", "bad_color": "An optional tf.TensorProto. Defaults to dtype: DT_UINT8 tensor_shape { dim { size: 4 } } int_val: 255 int_val: 0 int_val: 0 int_val: 255.\nColor to use for pixels with non-finite values.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.ImmutableConst": {"description": "Returns immutable tensor from memory region.", "Args": {"dtype": "A tf.DType. Type of the returned tensor.", "shape": "A tf.TensorShape or list of ints. Shape of the returned tensor.", "memory_region_name": "A string.\nName of readonly memory region used by the tensor, see\nNewReadOnlyMemoryRegionFromFile in tensorflow::Env.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dtype."}, "tf.raw_ops.ImportEvent": {"Args": {"writer": "A Tensor of type resource.", "event": "A Tensor of type string.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.InTopK": {"description": "Says whether the targets are in the top K predictions.", "Args": {"predictions": "A Tensor of type float32.\nA batch_size x classes tensor.", "targets": "A Tensor. Must be one of the following types: int32, int64.\nA batch_size vector of class ids.", "k": "An int. Number of top elements to look at for computing precision.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type bool."}, "tf.raw_ops.InTopKV2": {"description": "Says whether the targets are in the top K predictions.", "Args": {"predictions": "A Tensor of type float32.\nA batch_size x classes tensor.", "targets": "A Tensor. Must be one of the following types: int32, int64.\nA batch_size vector of class ids.", "k": "A Tensor. Must have the same type as targets.\nNumber of top elements to look at for computing precision.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type bool."}, "tf.raw_ops.InfeedDequeue": {"description": "A placeholder op for a value that will be fed into the computation.", "Args": {"dtype": "A tf.DType. The type of elements in the tensor.", "shape": "A tf.TensorShape or list of ints. The shape of the tensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dtype."}, "tf.raw_ops.InfeedDequeueTuple": {"description": "Fetches multiple values from infeed as an XLA tuple.", "Args": {"dtypes": "A list of tf.DTypes that has length >= 1.\nThe element types of each element in outputs.", "shapes": "A list of shapes (each a tf.TensorShape or list of ints).\nThe shapes of each tensor in outputs.", "name": "A name for the operation (optional)."}, "Returns": "A list of Tensor objects of type dtypes."}, "tf.raw_ops.InfeedEnqueue": {"description": "An op which feeds a single Tensor value into the computation.", "Args": {"input": "A Tensor.\nA tensor that will be provided using the infeed mechanism.", "shape": "An optional tf.TensorShape or list of ints. Defaults to [].\nThe shape of the tensor.", "layout": "An optional list of ints. Defaults to [].\nA vector holding the requested layout in minor-to-major sequence.\nIf a layout attribute is passed, but its values are all -1, the layout will\nbe computed by the infeed operation.", "device_ordinal": "An optional int. Defaults to -1.\nThe TPU device to use. This should be -1 when the Op\nis running on a TPU device, and >= 0 when the Op is running on the CPU\ndevice.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.InfeedEnqueuePrelinearizedBuffer": {"description": "An op which enqueues prelinearized buffer into TPU infeed.", "Args": {"input": "A Tensor of type variant.\nA variant tensor representing linearized output.", "device_ordinal": "An optional int. Defaults to -1.\nThe TPU device to use. This should be -1 when the Op is running on a TPU device\nand = 0 when the Op is running on the CPU device.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.InfeedEnqueueTuple": {"description": "Feeds multiple Tensor values into the computation as an XLA tuple.", "Args": {"inputs": "A list of Tensor objects.\nA list of tensors that will be provided using the infeed mechanism.", "shapes": "A list of shapes (each a tf.TensorShape or list of ints).\nThe shapes of each tensor in inputs.", "layouts": "An optional list of ints. Defaults to [].\nA vector holding the requested layout in minor-to-major sequence for\nall the tuple shapes, in the order the shapes appear in the \"shapes\" input.\nThe layout elements for a sub-shape can be set to -1, in which case the\ncorresponding layout will be computed by the infeed operation.", "device_ordinal": "An optional int. Defaults to -1.\nThe TPU device to use. This should be -1 when the Op\nis running on a TPU device, and >= 0 when the Op is running on the CPU\ndevice.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.InitializeTable": {"description": "Table initializer that takes two tensors for keys and values respectively.", "Args": {"table_handle": "A Tensor of type mutable string.\nHandle to a table which will be initialized.", "keys": "A Tensor. Keys of type Tkey.", "values": "A Tensor. Values of type Tval.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.InitializeTableFromDataset": {"Args": {"table_handle": "A Tensor of type resource.", "dataset": "A Tensor of type variant.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.InitializeTableFromTextFile": {"description": "Initializes a table from a text file.", "Args": {"table_handle": "A Tensor of type mutable string.\nHandle to a table which will be initialized.", "filename": "A Tensor of type string. Filename of a vocabulary text file.", "key_index": "An int that is >= -2.\nColumn index in a line to get the table key values from.", "value_index": "An int that is >= -2.\nColumn index that represents information of a line to get the table\nvalue values from.", "vocab_size": "An optional int that is >= -1. Defaults to -1.\nNumber of elements of the file, use -1 if unknown.", "delimiter": "An optional string. Defaults to \"\\t\".\nDelimiter to separate fields in a line.", "offset": "An optional int. Defaults to 0.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.InitializeTableFromTextFileV2": {"description": "Initializes a table from a text file.", "Args": {"table_handle": "A Tensor of type resource.\nHandle to a table which will be initialized.", "filename": "A Tensor of type string. Filename of a vocabulary text file.", "key_index": "An int that is >= -2.\nColumn index in a line to get the table key values from.", "value_index": "An int that is >= -2.\nColumn index that represents information of a line to get the table\nvalue values from.", "vocab_size": "An optional int that is >= -1. Defaults to -1.\nNumber of elements of the file, use -1 if unknown.", "delimiter": "An optional string. Defaults to \"\\t\".\nDelimiter to separate fields in a line.", "offset": "An optional int. Defaults to 0.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.InitializeTableV2": {"description": "Table initializer that takes two tensors for keys and values respectively.", "Args": {"table_handle": "A Tensor of type resource.\nHandle to a table which will be initialized.", "keys": "A Tensor. Keys of type Tkey.", "values": "A Tensor. Values of type Tval.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.InplaceAdd": {"description": "Adds v into specified rows of x.", "Args": {"x": "A Tensor. A Tensor of type T.", "i": "A Tensor of type int32.\nA vector. Indices into the left-most dimension of x.", "v": "A Tensor. Must have the same type as x.\nA Tensor of type T. Same dimension sizes as x except the first dimension, which must be the same as i's size.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.InplaceSub": {"description": "Subtracts v into specified rows of x.", "Args": {"x": "A Tensor. A Tensor of type T.", "i": "A Tensor of type int32.\nA vector. Indices into the left-most dimension of x.", "v": "A Tensor. Must have the same type as x.\nA Tensor of type T. Same dimension sizes as x except the first dimension, which must be the same as i's size.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.InplaceUpdate": {"description": "Updates specified rows &#39;i&#39; with values &#39;v&#39;.", "Args": {"x": "A Tensor. A tensor of type T.", "i": "A Tensor of type int32.\nA vector. Indices into the left-most dimension of x.", "v": "A Tensor. Must have the same type as x.\nA Tensor of type T. Same dimension sizes as x except the first dimension, which must be the same as i's size.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.InterleaveDataset": {"description": "Creates a dataset that applies f to the outputs of input_dataset.", "Args": {"input_dataset": "A Tensor of type variant.", "other_arguments": "A list of Tensor objects.", "cycle_length": "A Tensor of type int64.", "block_length": "A Tensor of type int64.", "f": "A function decorated with @Defun.\nA function mapping elements of input_dataset, concatenated with\nother_arguments, to a Dataset variant that contains elements matching\noutput_types and output_shapes.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.Inv": {"description": "Computes the reciprocal of x element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, int8, int16, int32, int64, complex64, complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.InvGrad": {"description": "Computes the gradient for the inverse of x wrt its input.", "Args": {"y": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, complex64, complex128.", "dy": "A Tensor. Must have the same type as y.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as y."}, "tf.raw_ops.Invert": {"description": "Invert (flip) each bit of supported types; for example, type uint8 value 01010101 becomes 10101010.", "Args": {"x": "A Tensor. Must be one of the following types: int8, int16, int32, int64, uint8, uint16, uint32, uint64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.InvertPermutation": {"description": "Computes the inverse permutation of a tensor.", "Args": {"x": "A Tensor. Must be one of the following types: int32, int64. 1-D.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.IsBoostedTreesEnsembleInitialized": {"description": "Checks whether a tree ensemble has been initialized.", "Args": {"tree_ensemble_handle": "A Tensor of type resource.\nHandle to the tree ensemble resource.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type bool."}, "tf.raw_ops.IsBoostedTreesQuantileStreamResourceInitialized": {"description": "Checks whether a quantile stream has been initialized.", "Args": {"quantile_stream_resource_handle": "A Tensor of type resource.\nresource; The reference to quantile stream resource handle.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type bool."}, "tf.raw_ops.IsFinite": {"description": "Returns which elements of x are finite.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type bool."}, "tf.raw_ops.IsInf": {"description": "Returns which elements of x are Inf.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type bool."}, "tf.raw_ops.IsNan": {"description": "Returns which elements of x are NaN.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type bool."}, "tf.raw_ops.IsTPUEmbeddingInitialized": {"description": "Whether TPU Embedding is initialized in a distributed TPU system.", "Args": {"config": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type bool."}, "tf.raw_ops.IsVariableInitialized": {"description": "Checks whether a tensor has been initialized.", "Args": {"ref": "A mutable Tensor.\nShould be from a Variable node. May be uninitialized.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type bool."}, "tf.raw_ops.IsotonicRegression": {"description": "Solves a batch of isotonic regression problems.", "Args": {"input": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.\nA (batch_size, dim)-tensor holding a batch of inputs.", "output_dtype": "An optional tf.DType from: tf.half, tf.bfloat16, tf.float32, tf.float64. Defaults to tf.float32.\nDtype of output.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output, segments)."}, "tf.raw_ops.Iterator": {"description": "A container for an iterator resource.", "Args": {"shared_name": "A string.", "container": "A string.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type resource."}, "tf.raw_ops.IteratorFromStringHandle": {"description": "Converts the given string representing a handle to an iterator to a resource.", "Args": {"string_handle": "A Tensor of type string.\nA string representation of the given handle.", "output_types": "An optional list of tf.DTypes. Defaults to [].\nIf specified, defines the type of each tuple component in an\nelement produced by the resulting iterator.", "output_shapes": "An optional list of shapes (each a tf.TensorShape or list of ints). Defaults to [].\nIf specified, defines the shape of each tuple component in an\nelement produced by the resulting iterator.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type resource."}, "tf.raw_ops.IteratorFromStringHandleV2": {"Args": {"string_handle": "A Tensor of type string.", "output_types": "An optional list of tf.DTypes. Defaults to [].", "output_shapes": "An optional list of shapes (each a tf.TensorShape or list of ints). Defaults to [].", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type resource."}, "tf.raw_ops.IteratorGetDevice": {"description": "Returns the name of the device on which resource has been placed.", "Args": {"resource": "A Tensor of type resource.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.IteratorGetNext": {"description": "Gets the next output from the given iterator .", "Args": {"iterator": "A Tensor of type resource.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A list of Tensor objects of type output_types."}, "tf.raw_ops.IteratorGetNextAsOptional": {"description": "Gets the next output from the given iterator as an Optional variant.", "Args": {"iterator": "A Tensor of type resource.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.IteratorGetNextSync": {"description": "Gets the next output from the given iterator.", "Args": {"iterator": "A Tensor of type resource.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A list of Tensor objects of type output_types."}, "tf.raw_ops.IteratorToStringHandle": {"description": "Converts the given resource_handle representing an iterator to a string.", "Args": {"resource_handle": "A Tensor of type resource.\nA handle to an iterator resource.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.IteratorV2": {"Args": {"shared_name": "A string.", "container": "A string.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type resource."}, "tf.raw_ops.L2Loss": {"description": "L2 Loss.", "Args": {"t": "A Tensor. Must be one of the following types: half, bfloat16, float32, float64.\nTypically 2-D, but may have any dimensions.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as t."}, "tf.raw_ops.LMDBDataset": {"description": "Creates a dataset that emits the key-value pairs in one or more LMDB files.", "Args": {"filenames": "A Tensor of type string.\nA scalar or a vector containing the name(s) of the binary file(s) to be\nread.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.LMDBReader": {"description": "A Reader that outputs the records from a LMDB file.", "Args": {"container": "An optional string. Defaults to \"\".\nIf non-empty, this reader is placed in the given container.\nOtherwise, a default container is used.", "shared_name": "An optional string. Defaults to \"\".\nIf non-empty, this reader is named in the given bucket\nwith this shared_name. Otherwise, the node name is used instead.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type mutable string."}, "tf.raw_ops.LRN": {"description": "Local Response Normalization.", "Args": {"input": "A Tensor. Must be one of the following types: half, bfloat16, float32.\n4-D.", "depth_radius": "An optional int. Defaults to 5.\n0-D.  Half-width of the 1-D normalization window.", "bias": "An optional float. Defaults to 1.\nAn offset (usually positive to avoid dividing by 0).", "alpha": "An optional float. Defaults to 1.\nA scale factor, usually positive.", "beta": "An optional float. Defaults to 0.5. An exponent.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.LRNGrad": {"description": "Gradients for Local Response Normalization.", "Args": {"input_grads": "A Tensor. Must be one of the following types: half, bfloat16, float32.\n4-D with shape [batch, height, width, channels].", "input_image": "A Tensor. Must have the same type as input_grads.\n4-D with shape [batch, height, width, channels].", "output_image": "A Tensor. Must have the same type as input_grads.\n4-D with shape [batch, height, width, channels].", "depth_radius": "An optional int. Defaults to 5. A depth radius.", "bias": "An optional float. Defaults to 1.\nAn offset (usually > 0 to avoid dividing by 0).", "alpha": "An optional float. Defaults to 1.\nA scale factor, usually positive.", "beta": "An optional float. Defaults to 0.5. An exponent.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input_grads."}, "tf.raw_ops.LSTMBlockCell": {"description": "Computes the LSTM cell forward propagation for 1 time step.", "Args": {"x": "A Tensor. Must be one of the following types: half, float32.\nThe input to the LSTM cell, shape (batch_size, num_inputs).", "cs_prev": "A Tensor. Must have the same type as x.\nValue of the cell state at previous time step.", "h_prev": "A Tensor. Must have the same type as x.\nOutput of the previous cell at previous time step.", "w": "A Tensor. Must have the same type as x. The weight matrix.", "wci": "A Tensor. Must have the same type as x.\nThe weight matrix for input gate peephole connection.", "wcf": "A Tensor. Must have the same type as x.\nThe weight matrix for forget gate peephole connection.", "wco": "A Tensor. Must have the same type as x.\nThe weight matrix for output gate peephole connection.", "b": "A Tensor. Must have the same type as x. The bias vector.", "forget_bias": "An optional float. Defaults to 1. The forget gate bias.", "cell_clip": "An optional float. Defaults to 3.\nValue to clip the 'cs' value to.", "use_peephole": "An optional bool. Defaults to False.\nWhether to use peephole weights.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (i, cs, f, o, ci, co, h)."}, "tf.raw_ops.LSTMBlockCellGrad": {"description": "Computes the LSTM cell backward propagation for 1 timestep.", "Args": {"x": "A Tensor. Must be one of the following types: half, float32.\nThe input to the LSTM cell, shape (batch_size, num_inputs).", "cs_prev": "A Tensor. Must have the same type as x.\nThe previous cell state.", "h_prev": "A Tensor. Must have the same type as x. The previous h state.", "w": "A Tensor. Must have the same type as x. The weight matrix.", "wci": "A Tensor. Must have the same type as x.\nThe weight matrix for input gate peephole connection.", "wcf": "A Tensor. Must have the same type as x.\nThe weight matrix for forget gate peephole connection.", "wco": "A Tensor. Must have the same type as x.\nThe weight matrix for output gate peephole connection.", "b": "A Tensor. Must have the same type as x. The bias vector.", "i": "A Tensor. Must have the same type as x. The input gate.", "cs": "A Tensor. Must have the same type as x.\nThe cell state before the tanh.", "f": "A Tensor. Must have the same type as x. The forget gate.", "o": "A Tensor. Must have the same type as x. The output gate.", "ci": "A Tensor. Must have the same type as x. The cell input.", "co": "A Tensor. Must have the same type as x. The cell after the tanh.", "cs_grad": "A Tensor. Must have the same type as x.\nThe current gradient of cs.", "h_grad": "A Tensor. Must have the same type as x.\nThe gradient of h vector.", "use_peephole": "A bool. Whether the cell uses peephole connections.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (cs_prev_grad, dicfo, wci_grad, wcf_grad, wco_grad)."}, "tf.raw_ops.LatencyStatsDataset": {"description": "Records the latency of producing input_dataset elements in a StatsAggregator.", "Args": {"input_dataset": "A Tensor of type variant.", "tag": "A Tensor of type string.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.LeakyRelu": {"description": "Computes rectified linear: max(features, features * alpha).", "Args": {"features": "A Tensor. Must be one of the following types: half, bfloat16, float32, float64.", "alpha": "An optional float. Defaults to 0.2.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as features."}, "tf.raw_ops.LeakyReluGrad": {"description": "Computes rectified linear gradients for a LeakyRelu operation.", "Args": {"gradients": "A Tensor. Must be one of the following types: half, bfloat16, float32, float64.\nThe backpropagated gradients to the corresponding LeakyRelu operation.", "features": "A Tensor. Must have the same type as gradients.\nThe features passed as input to the corresponding LeakyRelu operation,\nOR the outputs of that operation (both work equivalently).", "alpha": "An optional float. Defaults to 0.2.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as gradients."}, "tf.raw_ops.LearnedUnigramCandidateSampler": {"description": "Generates labels for candidate sampling with a learned unigram distribution.", "Args": {"true_classes": "A Tensor of type int64.\nA batch_size * num_true matrix, in which each row contains the\nIDs of the num_true target_classes in the corresponding original label.", "num_true": "An int that is >= 1. Number of true labels per context.", "num_sampled": "An int that is >= 1.\nNumber of candidates to randomly sample.", "unique": "A bool.\nIf unique is true, we sample with rejection, so that all sampled\ncandidates in a batch are unique. This requires some approximation to\nestimate the post-rejection sampling probabilities.", "range_max": "An int that is >= 1.\nThe sampler will sample integers from the interval [0, range_max).", "seed": "An optional int. Defaults to 0.\nIf either seed or seed2 are set to be non-zero, the random number\ngenerator is seeded by the given seed.  Otherwise, it is seeded by a\nrandom seed.", "seed2": "An optional int. Defaults to 0.\nAn second seed to avoid seed collision.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (sampled_candidates, true_expected_count, sampled_expected_count)."}, "tf.raw_ops.LeftShift": {"description": "Elementwise computes the bitwise left-shift of x and y.", "Args": {"x": "A Tensor. Must be one of the following types: int8, int16, int32, int64, uint8, uint16, uint32, uint64.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.LegacyParallelInterleaveDatasetV2": {"description": "Creates a dataset that applies f to the outputs of input_dataset.", "Args": {"input_dataset": "A Tensor of type variant.", "other_arguments": "A list of Tensor objects.", "cycle_length": "A Tensor of type int64.", "block_length": "A Tensor of type int64.", "buffer_output_elements": "A Tensor of type int64.", "prefetch_input_elements": "A Tensor of type int64.", "f": "A function decorated with @Defun.\nA function mapping elements of input_dataset, concatenated with\nother_arguments, to a Dataset variant that contains elements matching\noutput_types and output_shapes.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "deterministic": "An optional string. Defaults to \"default\".", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.Less": {"description": "Returns the truth value of (x &lt; y) element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type bool."}, "tf.raw_ops.LessEqual": {"description": "Returns the truth value of (x &lt;= y) element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type bool."}, "tf.raw_ops.Lgamma": {"description": "Computes the log of the absolute value of Gamma(x) element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.LinSpace": {"description": "Generates values in an interval.", "Args": {"start": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.\n0-D tensor. First entry in the range.", "stop": "A Tensor. Must have the same type as start.\n0-D tensor. Last entry in the range.", "num": "A Tensor. Must be one of the following types: int32, int64.\n0-D tensor. Number of values to generate.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as start."}, "tf.raw_ops.ListDiff": {"description": "Computes the difference between two lists of numbers or strings.", "Args": {"x": "A Tensor. 1-D. Values to keep.", "y": "A Tensor. Must have the same type as x. 1-D. Values to remove.", "out_idx": "An optional tf.DType from: tf.int32, tf.int64. Defaults to tf.int32.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (out, idx)."}, "tf.raw_ops.LoadAndRemapMatrix": {"description": "Loads a 2-D (matrix) Tensor with name old_tensor_name from the checkpoint", "Args": {"ckpt_path": "A Tensor of type string.\nPath to the TensorFlow checkpoint (version 2, TensorBundle) from\nwhich the old matrix Tensor will be loaded.", "old_tensor_name": "A Tensor of type string.\nName of the 2-D Tensor to load from checkpoint.", "row_remapping": "A Tensor of type int64.\nAn int Tensor of row remappings (generally created by\ngenerate_vocab_remapping).  Even if no row remapping is needed, this must\nstill be an index-valued Tensor (e.g. [0, 1, 2, ...]), or a shifted\nindex-valued Tensor (e.g. [8, 9, 10, ...], for partitioned Variables).", "col_remapping": "A Tensor of type int64.\nAn int Tensor of column remappings (generally created by\ngenerate_vocab_remapping).  May be a size-0 Tensor if only row remapping\nis to be done (e.g. column ordering is the same).", "initializing_values": "A Tensor of type float32.\nA float Tensor containing  values to fill in for cells\nin the output matrix that are not loaded from the checkpoint. Length must be\nexactly the same as the number of missing / new cells.", "num_rows": "An int that is >= 0.\nNumber of rows (length of the 1st dimension) in the output matrix.", "num_cols": "An int that is >= 1.\nNumber of columns (length of the 2nd dimension) in the output matrix.", "max_rows_in_memory": "An optional int. Defaults to -1.\nThe maximum number of rows to load from the checkpoint at\nonce. If less than or equal to 0, the entire matrix will be loaded into\nmemory. Setting this arg trades increased disk reads for lower memory usage.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type float32."}, "tf.raw_ops.LoadDataset": {"Args": {"path": "A Tensor of type string.", "reader_func_other_args": "A list of Tensor objects.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "reader_func": "A function decorated with @Defun.", "compression": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.LoadTPUEmbeddingADAMParameters": {"description": "Load ADAM embedding parameters.", "Args": {"parameters": "A Tensor of type float32.\nValue of parameters used in the ADAM optimization algorithm.", "momenta": "A Tensor of type float32.\nValue of momenta used in the ADAM optimization algorithm.", "velocities": "A Tensor of type float32.\nValue of velocities used in the ADAM optimization algorithm.", "num_shards": "An int.", "shard_id": "An int.", "table_id": "An optional int. Defaults to -1.", "table_name": "An optional string. Defaults to \"\".", "config": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.LoadTPUEmbeddingAdadeltaParameters": {"description": "Load Adadelta embedding parameters.", "Args": {"parameters": "A Tensor of type float32.\nValue of parameters used in the Adadelta optimization algorithm.", "accumulators": "A Tensor of type float32.\nValue of accumulators used in the Adadelta optimization algorithm.", "updates": "A Tensor of type float32.\nValue of updates used in the Adadelta optimization algorithm.", "num_shards": "An int.", "shard_id": "An int.", "table_id": "An optional int. Defaults to -1.", "table_name": "An optional string. Defaults to \"\".", "config": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.LoadTPUEmbeddingAdagradMomentumParameters": {"description": "Load Adagrad Momentum embedding parameters.", "Args": {"parameters": "A Tensor of type float32.\nValue of parameters used in the Adagrad Momentum optimization algorithm.", "accumulators": "A Tensor of type float32.\nValue of accumulators used in the Adagrad Momentum optimization algorithm.", "momenta": "A Tensor of type float32.\nValue of momenta used in the Adagrad Momentum optimization algorithm.", "num_shards": "An int.", "shard_id": "An int.", "table_id": "An optional int. Defaults to -1.", "table_name": "An optional string. Defaults to \"\".", "config": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.LoadTPUEmbeddingAdagradParameters": {"description": "Load Adagrad embedding parameters.", "Args": {"parameters": "A Tensor of type float32.\nValue of parameters used in the Adagrad optimization algorithm.", "accumulators": "A Tensor of type float32.\nValue of accumulators used in the Adagrad optimization algorithm.", "num_shards": "An int.", "shard_id": "An int.", "table_id": "An optional int. Defaults to -1.", "table_name": "An optional string. Defaults to \"\".", "config": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.LoadTPUEmbeddingCenteredRMSPropParameters": {"description": "Load centered RMSProp embedding parameters.", "Args": {"parameters": "A Tensor of type float32.\nValue of parameters used in the centered RMSProp optimization algorithm.", "ms": "A Tensor of type float32.\nValue of ms used in the centered RMSProp optimization algorithm.", "mom": "A Tensor of type float32.\nValue of mom used in the centered RMSProp optimization algorithm.", "mg": "A Tensor of type float32.\nValue of mg used in the centered RMSProp optimization algorithm.", "num_shards": "An int.", "shard_id": "An int.", "table_id": "An optional int. Defaults to -1.", "table_name": "An optional string. Defaults to \"\".", "config": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.LoadTPUEmbeddingFTRLParameters": {"description": "Load FTRL embedding parameters.", "Args": {"parameters": "A Tensor of type float32.\nValue of parameters used in the FTRL optimization algorithm.", "accumulators": "A Tensor of type float32.\nValue of accumulators used in the FTRL optimization algorithm.", "linears": "A Tensor of type float32.\nValue of linears used in the FTRL optimization algorithm.", "num_shards": "An int.", "shard_id": "An int.", "table_id": "An optional int. Defaults to -1.", "table_name": "An optional string. Defaults to \"\".", "config": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.LoadTPUEmbeddingFrequencyEstimatorParameters": {"description": "Load frequency estimator embedding parameters.", "Args": {"parameters": "A Tensor of type float32.\nValue of parameters used in the frequency estimator optimization algorithm.", "last_hit_step": "A Tensor of type float32.\nValue of last_hit_step used in the frequency estimator optimization algorithm.", "num_shards": "An int.", "shard_id": "An int.", "table_id": "An optional int. Defaults to -1.", "table_name": "An optional string. Defaults to \"\".", "config": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.LoadTPUEmbeddingMDLAdagradLightParameters": {"description": "Load MDL Adagrad Light embedding parameters.", "Args": {"parameters": "A Tensor of type float32.\nValue of parameters used in the MDL Adagrad Light optimization algorithm.", "accumulators": "A Tensor of type float32.\nValue of accumulators used in the MDL Adagrad Light optimization algorithm.", "weights": "A Tensor of type float32.\nValue of weights used in the MDL Adagrad Light optimization algorithm.", "benefits": "A Tensor of type float32.\nValue of benefits used in the MDL Adagrad Light optimization algorithm.", "num_shards": "An int.", "shard_id": "An int.", "table_id": "An optional int. Defaults to -1.", "table_name": "An optional string. Defaults to \"\".", "config": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.LoadTPUEmbeddingMomentumParameters": {"description": "Load Momentum embedding parameters.", "Args": {"parameters": "A Tensor of type float32.\nValue of parameters used in the Momentum optimization algorithm.", "momenta": "A Tensor of type float32.\nValue of momenta used in the Momentum optimization algorithm.", "num_shards": "An int.", "shard_id": "An int.", "table_id": "An optional int. Defaults to -1.", "table_name": "An optional string. Defaults to \"\".", "config": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.LoadTPUEmbeddingProximalAdagradParameters": {"description": "Load proximal Adagrad embedding parameters.", "Args": {"parameters": "A Tensor of type float32.\nValue of parameters used in the proximal Adagrad optimization algorithm.", "accumulators": "A Tensor of type float32.\nValue of accumulators used in the proximal Adagrad optimization algorithm.", "num_shards": "An int.", "shard_id": "An int.", "table_id": "An optional int. Defaults to -1.", "table_name": "An optional string. Defaults to \"\".", "config": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.LoadTPUEmbeddingProximalYogiParameters": {"Args": {"parameters": "A Tensor of type float32.", "v": "A Tensor of type float32.", "m": "A Tensor of type float32.", "num_shards": "An int.", "shard_id": "An int.", "table_id": "An optional int. Defaults to -1.", "table_name": "An optional string. Defaults to \"\".", "config": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.LoadTPUEmbeddingRMSPropParameters": {"description": "Load RMSProp embedding parameters.", "Args": {"parameters": "A Tensor of type float32.\nValue of parameters used in the RMSProp optimization algorithm.", "ms": "A Tensor of type float32.\nValue of ms used in the RMSProp optimization algorithm.", "mom": "A Tensor of type float32.\nValue of mom used in the RMSProp optimization algorithm.", "num_shards": "An int.", "shard_id": "An int.", "table_id": "An optional int. Defaults to -1.", "table_name": "An optional string. Defaults to \"\".", "config": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.LoadTPUEmbeddingStochasticGradientDescentParameters": {"description": "Load SGD embedding parameters.", "Args": {"parameters": "A Tensor of type float32.\nValue of parameters used in the stochastic gradient descent optimization algorithm.", "num_shards": "An int.", "shard_id": "An int.", "table_id": "An optional int. Defaults to -1.", "table_name": "An optional string. Defaults to \"\".", "config": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.Log": {"description": "Computes natural logarithm of x element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, complex64, complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.Log1p": {"description": "Computes natural logarithm of (1 &#43; x) element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, complex64, complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.LogMatrixDeterminant": {"description": "Computes the sign and the log of the absolute value of the determinant of", "Args": {"input": "A Tensor. Must be one of the following types: half, float32, float64, complex64, complex128.\nShape is [N, M, M].", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (sign, log_abs_determinant)."}, "tf.raw_ops.LogSoftmax": {"description": "Computes log softmax activations.", "Args": {"logits": "A Tensor. Must be one of the following types: half, bfloat16, float32, float64.\n2-D with shape [batch_size, num_classes].", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as logits."}, "tf.raw_ops.LogUniformCandidateSampler": {"description": "Generates labels for candidate sampling with a log-uniform distribution.", "Args": {"true_classes": "A Tensor of type int64.\nA batch_size * num_true matrix, in which each row contains the\nIDs of the num_true target_classes in the corresponding original label.", "num_true": "An int that is >= 1. Number of true labels per context.", "num_sampled": "An int that is >= 1.\nNumber of candidates to randomly sample.", "unique": "A bool.\nIf unique is true, we sample with rejection, so that all sampled\ncandidates in a batch are unique. This requires some approximation to\nestimate the post-rejection sampling probabilities.", "range_max": "An int that is >= 1.\nThe sampler will sample integers from the interval [0, range_max).", "seed": "An optional int. Defaults to 0.\nIf either seed or seed2 are set to be non-zero, the random number\ngenerator is seeded by the given seed.  Otherwise, it is seeded by a\nrandom seed.", "seed2": "An optional int. Defaults to 0.\nAn second seed to avoid seed collision.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (sampled_candidates, true_expected_count, sampled_expected_count)."}, "tf.raw_ops.LogicalAnd": {"description": "Returns the truth value of x AND y element-wise.", "Args": {"x": "A Tensor of type bool.", "y": "A Tensor of type bool.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type bool."}, "tf.raw_ops.LogicalNot": {"description": "Returns the truth value of NOT x element-wise.", "Args": {"x": "A Tensor of type bool. A Tensor of type bool.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type bool."}, "tf.raw_ops.LogicalOr": {"description": "Returns the truth value of x OR y element-wise.", "Args": {"x": "A Tensor of type bool.", "y": "A Tensor of type bool.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type bool."}, "tf.raw_ops.LookupTableExport": {"description": "Outputs all keys and values in the table.", "Args": {"table_handle": "A Tensor of type mutable string. Handle to the table.", "Tkeys": "A tf.DType.", "Tvalues": "A tf.DType.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (keys, values)."}, "tf.raw_ops.LookupTableExportV2": {"description": "Outputs all keys and values in the table.", "Args": {"table_handle": "A Tensor of type resource. Handle to the table.", "Tkeys": "A tf.DType.", "Tvalues": "A tf.DType.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (keys, values)."}, "tf.raw_ops.LookupTableFind": {"description": "Looks up keys in a table, outputs the corresponding values.", "Args": {"table_handle": "A Tensor of type mutable string. Handle to the table.", "keys": "A Tensor. Any shape.  Keys to look up.", "default_value": "A Tensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as default_value."}, "tf.raw_ops.LookupTableFindV2": {"description": "Looks up keys in a table, outputs the corresponding values.", "Args": {"table_handle": "A Tensor of type resource. Handle to the table.", "keys": "A Tensor. Any shape.  Keys to look up.", "default_value": "A Tensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as default_value."}, "tf.raw_ops.LookupTableImport": {"description": "Replaces the contents of the table with the specified keys and values.", "Args": {"table_handle": "A Tensor of type mutable string. Handle to the table.", "keys": "A Tensor. Any shape.  Keys to look up.", "values": "A Tensor. Values to associate with keys.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.LookupTableImportV2": {"description": "Replaces the contents of the table with the specified keys and values.", "Args": {"table_handle": "A Tensor of type resource. Handle to the table.", "keys": "A Tensor. Any shape.  Keys to look up.", "values": "A Tensor. Values to associate with keys.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.LookupTableInsert": {"description": "Updates the table to associates keys with values.", "Args": {"table_handle": "A Tensor of type mutable string. Handle to the table.", "keys": "A Tensor. Any shape.  Keys to look up.", "values": "A Tensor. Values to associate with keys.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.LookupTableInsertV2": {"description": "Updates the table to associates keys with values.", "Args": {"table_handle": "A Tensor of type resource. Handle to the table.", "keys": "A Tensor. Any shape.  Keys to look up.", "values": "A Tensor. Values to associate with keys.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.LookupTableRemoveV2": {"description": "Removes keys and its associated values from a table.", "Args": {"table_handle": "A Tensor of type resource. Handle to the table.", "keys": "A Tensor. Any shape.  Keys of the elements to remove.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.LookupTableSize": {"description": "Computes the number of elements in the given table.", "Args": {"table_handle": "A Tensor of type mutable string. Handle to the table.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int64."}, "tf.raw_ops.LookupTableSizeV2": {"description": "Computes the number of elements in the given table.", "Args": {"table_handle": "A Tensor of type resource. Handle to the table.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int64."}, "tf.raw_ops.LoopCond": {"description": "Forwards the input to the output.", "Args": {"input": "A Tensor of type bool.\nA boolean scalar, representing the branch predicate of the Switch op.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type bool."}, "tf.raw_ops.LowerBound": {"description": "Applies lower_bound(sorted_search_values, values) along each row.", "Args": {"sorted_inputs": "A Tensor. 2-D Tensor where each row is ordered.", "values": "A Tensor. Must have the same type as sorted_inputs.\n2-D Tensor with the same numbers of rows as sorted_search_values. Contains\nthe values that will be searched for in sorted_search_values.", "out_type": "An optional tf.DType from: tf.int32, tf.int64. Defaults to tf.int32.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type out_type."}, "tf.raw_ops.Lu": {"description": "Computes the LU decomposition of one or more square matrices.", "Args": {"input": "A Tensor. Must be one of the following types: float64, float32, half, complex64, complex128.\nA tensor of shape [..., M, M] whose inner-most 2 dimensions form matrices of\nsize [M, M].", "output_idx_type": "An optional tf.DType from: tf.int32, tf.int64. Defaults to tf.int32.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (lu, p)."}, "tf.raw_ops.MakeIterator": {"description": "Makes a new iterator from the given dataset and stores it in iterator.", "Args": {"dataset": "A Tensor of type variant.", "iterator": "A Tensor of type resource.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.MapAndBatchDataset": {"description": "Creates a dataset that fuses mapping with batching.", "Args": {"input_dataset": "A Tensor of type variant.\nA variant tensor representing the input dataset.", "other_arguments": "A list of Tensor objects.\nA list of tensors, typically values that were captured when building a closure\nfor f.", "batch_size": "A Tensor of type int64.\nA scalar representing the number of elements to accumulate in a\nbatch. It determines the number of concurrent invocations of f that process\nelements from input_dataset in parallel.", "num_parallel_calls": "A Tensor of type int64.\nA scalar representing the maximum number of parallel invocations of the map_fn\nfunction. Applying the map_fn on consecutive input elements in parallel has\nthe potential to improve input pipeline throughput.", "drop_remainder": "A Tensor of type bool.\nA scalar representing whether the last batch should be dropped in case its size\nis smaller than desired.", "f": "A function decorated with @Defun.\nA function to apply to the outputs of input_dataset.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "preserve_cardinality": "An optional bool. Defaults to False.", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.MapClear": {"description": "Op removes all elements in the underlying container.", "Args": {"dtypes": "A list of tf.DTypes.", "capacity": "An optional int that is >= 0. Defaults to 0.", "memory_limit": "An optional int that is >= 0. Defaults to 0.", "container": "An optional string. Defaults to \"\".", "shared_name": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.MapDataset": {"description": "Creates a dataset that applies f to the outputs of input_dataset.", "Args": {"input_dataset": "A Tensor of type variant.", "other_arguments": "A list of Tensor objects.", "f": "A function decorated with @Defun.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "use_inter_op_parallelism": "An optional bool. Defaults to True.", "preserve_cardinality": "An optional bool. Defaults to False.", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.MapDefun": {"description": "Maps a function on the list of tensors unpacked from arguments on dimension 0.", "Args": {"arguments": "A list of Tensor objects.\nA list of tensors whose types are Targuments, corresponding to the inputs\nthe function should be mapped over.", "captured_inputs": "A list of Tensor objects.\nA list of tensors whose types are Tcaptured, corresponding to the captured\ninputs of the defun.", "output_types": "A list of tf.DTypes that has length >= 1.\nA list of types.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.\nA list of shapes.", "f": "A function decorated with @Defun.", "max_intra_op_parallelism": "An optional int. Defaults to 1.", "name": "A name for the operation (optional)."}, "Returns": "A list of Tensor objects of type output_types."}, "tf.raw_ops.MapIncompleteSize": {"description": "Op returns the number of incomplete elements in the underlying container.", "Args": {"dtypes": "A list of tf.DTypes.", "capacity": "An optional int that is >= 0. Defaults to 0.", "memory_limit": "An optional int that is >= 0. Defaults to 0.", "container": "An optional string. Defaults to \"\".", "shared_name": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int32."}, "tf.raw_ops.MapPeek": {"description": "Op peeks at the values at the specified key. If the", "Args": {"key": "A Tensor of type int64.", "indices": "A Tensor of type int32.", "dtypes": "A list of tf.DTypes that has length >= 1.", "capacity": "An optional int that is >= 0. Defaults to 0.", "memory_limit": "An optional int that is >= 0. Defaults to 0.", "container": "An optional string. Defaults to \"\".", "shared_name": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A list of Tensor objects of type dtypes."}, "tf.raw_ops.MapSize": {"description": "Op returns the number of elements in the underlying container.", "Args": {"dtypes": "A list of tf.DTypes.", "capacity": "An optional int that is >= 0. Defaults to 0.", "memory_limit": "An optional int that is >= 0. Defaults to 0.", "container": "An optional string. Defaults to \"\".", "shared_name": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int32."}, "tf.raw_ops.MapStage": {"description": "Stage (key, values) in the underlying container which behaves like a hashtable.", "Args": {"key": "A Tensor of type int64. int64", "indices": "A Tensor of type int32.", "values": "A list of Tensor objects. a list of tensors\ndtypes A list of data types that inserted values should adhere to.", "dtypes": "A list of tf.DTypes.", "capacity": "An optional int that is >= 0. Defaults to 0.\nMaximum number of elements in the Staging Area. If > 0, inserts\non the container will block when the capacity is reached.", "memory_limit": "An optional int that is >= 0. Defaults to 0.", "container": "An optional string. Defaults to \"\".\nIf non-empty, this queue is placed in the given container. Otherwise,\na default container is used.", "shared_name": "An optional string. Defaults to \"\".\nIt is necessary to match this name to the matching Unstage Op.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.MapUnstage": {"description": "Op removes and returns the values associated with the key", "Args": {"key": "A Tensor of type int64.", "indices": "A Tensor of type int32.", "dtypes": "A list of tf.DTypes that has length >= 1.", "capacity": "An optional int that is >= 0. Defaults to 0.", "memory_limit": "An optional int that is >= 0. Defaults to 0.", "container": "An optional string. Defaults to \"\".", "shared_name": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A list of Tensor objects of type dtypes."}, "tf.raw_ops.MapUnstageNoKey": {"description": "Op removes and returns a random (key, value)", "Args": {"indices": "A Tensor of type int32.", "dtypes": "A list of tf.DTypes that has length >= 1.", "capacity": "An optional int that is >= 0. Defaults to 0.", "memory_limit": "An optional int that is >= 0. Defaults to 0.", "container": "An optional string. Defaults to \"\".", "shared_name": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (key, values)."}, "tf.raw_ops.MatMul": {"description": "Multiply the matrix &#34;a&#34; by the matrix &#34;b&#34;.", "Args": {"a": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, int32, int64, complex64, complex128.", "b": "A Tensor. Must have the same type as a.", "transpose_a": "An optional bool. Defaults to False.\nIf true, \"a\" is transposed before multiplication.", "transpose_b": "An optional bool. Defaults to False.\nIf true, \"b\" is transposed before multiplication.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as a."}, "tf.raw_ops.MatchingFiles": {"description": "Returns the set of files matching one or more glob patterns.", "Args": {"pattern": "A Tensor of type string.\nShell wildcard pattern(s). Scalar or vector of type string.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.MatchingFilesDataset": {"Args": {"patterns": "A Tensor of type string.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.MatrixBandPart": {"description": "Copy a tensor setting everything outside a central band in each innermost matrix to zero.", "Args": {"input": "A Tensor. Rank k tensor.", "num_lower": "A Tensor. Must be one of the following types: int32, int64.\n0-D tensor. Number of subdiagonals to keep. If negative, keep entire\nlower triangle.", "num_upper": "A Tensor. Must have the same type as num_lower.\n0-D tensor. Number of superdiagonals to keep. If negative, keep\nentire upper triangle.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.MatrixDeterminant": {"description": "Computes the determinant of one or more square matrices.", "Args": {"input": "A Tensor. Must be one of the following types: half, float32, float64, complex64, complex128.\nShape is [..., M, M].", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.MatrixDiag": {"description": "Returns a batched diagonal tensor with a given batched diagonal values.", "Args": {"diagonal": "A Tensor. Rank k, where k >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as diagonal."}, "tf.raw_ops.MatrixDiagPart": {"description": "Returns the batched diagonal part of a batched tensor.", "Args": {"input": "A Tensor. Rank k tensor where k >= 2.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.MatrixDiagPartV2": {"description": "Returns the batched diagonal part of a batched tensor.", "Args": {"input": "A Tensor. Rank r tensor where r >= 2.", "k": "A Tensor of type int32.\nDiagonal offset(s). Positive value means superdiagonal, 0 refers to the main\ndiagonal, and negative value means subdiagonals. k can be a single integer\n(for a single diagonal) or a pair of integers specifying the low and high ends\nof a matrix band. k[0] must not be larger than k[1].", "padding_value": "A Tensor. Must have the same type as input.\nThe value to fill the area outside the specified diagonal band with.\nDefault is 0.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.MatrixDiagPartV3": {"description": "Returns the batched diagonal part of a batched tensor.", "Args": {"input": "A Tensor. Rank r tensor where r >= 2.", "k": "A Tensor of type int32.\nDiagonal offset(s). Positive value means superdiagonal, 0 refers to the main\ndiagonal, and negative value means subdiagonals. k can be a single integer\n(for a single diagonal) or a pair of integers specifying the low and high ends\nof a matrix band. k[0] must not be larger than k[1].", "padding_value": "A Tensor. Must have the same type as input.\nThe value to fill the area outside the specified diagonal band with.\nDefault is 0.", "align": "An optional string from: \"LEFT_RIGHT\", \"RIGHT_LEFT\", \"LEFT_LEFT\", \"RIGHT_RIGHT\". Defaults to \"RIGHT_LEFT\".\nSome diagonals are shorter than max_diag_len and need to be padded. align is\na string specifying how superdiagonals and subdiagonals should be aligned,\nrespectively. There are four possible alignments: \"RIGHT_LEFT\" (default),\n\"LEFT_RIGHT\", \"LEFT_LEFT\", and \"RIGHT_RIGHT\". \"RIGHT_LEFT\" aligns superdiagonals\nto the right (left-pads the row) and subdiagonals to the left (right-pads the\nrow). It is the packing format LAPACK uses. cuSPARSE uses \"LEFT_RIGHT\", which is\nthe opposite alignment.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.MatrixDiagV2": {"description": "Returns a batched diagonal tensor with given batched diagonal values.", "Args": {"diagonal": "A Tensor. Rank r, where r >= 1", "k": "A Tensor of type int32.\nDiagonal offset(s). Positive value means superdiagonal, 0 refers to the main\ndiagonal, and negative value means subdiagonals. k can be a single integer\n(for a single diagonal) or a pair of integers specifying the low and high ends\nof a matrix band. k[0] must not be larger than k[1].", "num_rows": "A Tensor of type int32.\nThe number of rows of the output matrix. If it is not provided, the op assumes\nthe output matrix is a square matrix and infers the matrix size from k and the\ninnermost dimension of diagonal.", "num_cols": "A Tensor of type int32.\nThe number of columns of the output matrix. If it is not provided, the op\nassumes the output matrix is a square matrix and infers the matrix size from\nk and the innermost dimension of diagonal.", "padding_value": "A Tensor. Must have the same type as diagonal.\nThe number to fill the area outside the specified diagonal band with.\nDefault is 0.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as diagonal."}, "tf.raw_ops.MatrixDiagV3": {"description": "Returns a batched diagonal tensor with given batched diagonal values.", "Args": {"diagonal": "A Tensor. Rank r, where r >= 1", "k": "A Tensor of type int32.\nDiagonal offset(s). Positive value means superdiagonal, 0 refers to the main\ndiagonal, and negative value means subdiagonals. k can be a single integer\n(for a single diagonal) or a pair of integers specifying the low and high ends\nof a matrix band. k[0] must not be larger than k[1].", "num_rows": "A Tensor of type int32.\nThe number of rows of the output matrix. If it is not provided, the op assumes\nthe output matrix is a square matrix and infers the matrix size from k and the\ninnermost dimension of diagonal.", "num_cols": "A Tensor of type int32.\nThe number of columns of the output matrix. If it is not provided, the op\nassumes the output matrix is a square matrix and infers the matrix size from\nk and the innermost dimension of diagonal.", "padding_value": "A Tensor. Must have the same type as diagonal.\nThe number to fill the area outside the specified diagonal band with.\nDefault is 0.", "align": "An optional string from: \"LEFT_RIGHT\", \"RIGHT_LEFT\", \"LEFT_LEFT\", \"RIGHT_RIGHT\". Defaults to \"RIGHT_LEFT\".\nSome diagonals are shorter than max_diag_len and need to be padded. align is\na string specifying how superdiagonals and subdiagonals should be aligned,\nrespectively. There are four possible alignments: \"RIGHT_LEFT\" (default),\n\"LEFT_RIGHT\", \"LEFT_LEFT\", and \"RIGHT_RIGHT\". \"RIGHT_LEFT\" aligns superdiagonals\nto the right (left-pads the row) and subdiagonals to the left (right-pads the\nrow). It is the packing format LAPACK uses. cuSPARSE uses \"LEFT_RIGHT\", which is\nthe opposite alignment.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as diagonal."}, "tf.raw_ops.MatrixExponential": {"description": "Deprecated, use python implementation tf.linalg.matrix_exponential.", "Args": {"input": "A Tensor. Must be one of the following types: float64, float32, half, complex64, complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.MatrixInverse": {"description": "Computes the inverse of one or more square invertible matrices or their adjoints (conjugate transposes).", "Args": {"input": "A Tensor. Must be one of the following types: float64, float32, half, complex64, complex128.\nShape is [..., M, M].", "adjoint": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.MatrixLogarithm": {"description": "Computes the matrix logarithm of one or more square matrices:", "Args": {"input": "A Tensor. Must be one of the following types: complex64, complex128.\nShape is [..., M, M].", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.MatrixSetDiag": {"description": "Returns a batched matrix tensor with new batched diagonal values.", "Args": {"input": "A Tensor. Rank k+1, where k >= 1.", "diagonal": "A Tensor. Must have the same type as input.\nRank k, where k >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.MatrixSetDiagV2": {"description": "Returns a batched matrix tensor with new batched diagonal values.", "Args": {"input": "A Tensor. Rank r+1, where r >= 1.", "diagonal": "A Tensor. Must have the same type as input.\nRank r when k is an integer or k[0] == k[1]. Otherwise, it has rank r+1.\nk >= 1.", "k": "A Tensor of type int32.\nDiagonal offset(s). Positive value means superdiagonal, 0 refers to the main\ndiagonal, and negative value means subdiagonals. k can be a single integer\n(for a single diagonal) or a pair of integers specifying the low and high ends\nof a matrix band. k[0] must not be larger than k[1].", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.MatrixSetDiagV3": {"description": "Returns a batched matrix tensor with new batched diagonal values.", "Args": {"input": "A Tensor. Rank r+1, where r >= 1.", "diagonal": "A Tensor. Must have the same type as input.\nRank r when k is an integer or k[0] == k[1]. Otherwise, it has rank r+1.\nk >= 1.", "k": "A Tensor of type int32.\nDiagonal offset(s). Positive value means superdiagonal, 0 refers to the main\ndiagonal, and negative value means subdiagonals. k can be a single integer\n(for a single diagonal) or a pair of integers specifying the low and high ends\nof a matrix band. k[0] must not be larger than k[1].", "align": "An optional string from: \"LEFT_RIGHT\", \"RIGHT_LEFT\", \"LEFT_LEFT\", \"RIGHT_RIGHT\". Defaults to \"RIGHT_LEFT\".\nSome diagonals are shorter than max_diag_len and need to be padded. align is\na string specifying how superdiagonals and subdiagonals should be aligned,\nrespectively. There are four possible alignments: \"RIGHT_LEFT\" (default),\n\"LEFT_RIGHT\", \"LEFT_LEFT\", and \"RIGHT_RIGHT\". \"RIGHT_LEFT\" aligns superdiagonals\nto the right (left-pads the row) and subdiagonals to the left (right-pads the\nrow). It is the packing format LAPACK uses. cuSPARSE uses \"LEFT_RIGHT\", which is\nthe opposite alignment.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.MatrixSolve": {"description": "Solves systems of linear equations.", "Args": {"matrix": "A Tensor. Must be one of the following types: float64, float32, half, complex64, complex128.\nShape is [..., M, M].", "rhs": "A Tensor. Must have the same type as matrix.\nShape is [..., M, K].", "adjoint": "An optional bool. Defaults to False.\nBoolean indicating whether to solve with matrix or its (block-wise)\nadjoint.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as matrix."}, "tf.raw_ops.MatrixSolveLs": {"description": "Solves one or more linear least-squares problems.", "Args": {"matrix": "A Tensor. Must be one of the following types: float64, float32, half, complex64, complex128.\nShape is [..., M, N].", "rhs": "A Tensor. Must have the same type as matrix.\nShape is [..., M, K].", "l2_regularizer": "A Tensor of type float64. Scalar tensor.", "fast": "An optional bool. Defaults to True.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as matrix."}, "tf.raw_ops.MatrixSquareRoot": {"description": "Computes the matrix square root of one or more square matrices:", "Args": {"input": "A Tensor. Must be one of the following types: float64, float32, half, complex64, complex128.\nShape is [..., M, M].", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.MatrixTriangularSolve": {"description": "Solves systems of linear equations with upper or lower triangular matrices by backsubstitution.", "Args": {"matrix": "A Tensor. Must be one of the following types: bfloat16, float64, float32, half, complex64, complex128.\nShape is [..., M, M].", "rhs": "A Tensor. Must have the same type as matrix.\nShape is [..., M, K].", "lower": "An optional bool. Defaults to True.\nBoolean indicating whether the innermost matrices in matrix are\nlower or upper triangular.", "adjoint": "An optional bool. Defaults to False.\nBoolean indicating whether to solve with matrix or its (block-wise)\n         adjoint.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as matrix."}, "tf.raw_ops.Max": {"description": "Computes the maximum of elements across dimensions of a tensor.", "Args": {"input": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64, qint8, quint8, qint32, qint16, quint16.\nThe tensor to reduce.", "axis": "A Tensor. Must be one of the following types: int32, int64.\nThe dimensions to reduce. Must be in the range\n[-rank(input), rank(input)).", "keep_dims": "An optional bool. Defaults to False.\nIf true, retain reduced dimensions with length 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.MaxIntraOpParallelismDataset": {"description": "Creates a dataset that overrides the maximum intra-op parallelism.", "Args": {"input_dataset": "A Tensor of type variant.", "max_intra_op_parallelism": "A Tensor of type int64.\nIdentifies the maximum intra-op parallelism to use.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.MaxPool": {"description": "Performs max pooling on the input.", "Args": {"input": "A Tensor. Must be one of the following types: half, bfloat16, float32, float64, int32, int64, uint8, int16, int8, uint16, qint8.\n4-D input to pool over.", "ksize": "A list of ints that has length >= 4.\nThe size of the window for each dimension of the input tensor.", "strides": "A list of ints that has length >= 4.\nThe stride of the sliding window for each dimension of the\ninput tensor.", "padding": "A string from: \"SAME\", \"VALID\", \"EXPLICIT\".\nThe type of padding algorithm to use.", "explicit_paddings": "An optional list of ints. Defaults to [].", "data_format": "An optional string from: \"NHWC\", \"NCHW\", \"NCHW_VECT_C\". Defaults to \"NHWC\".\nSpecify the data format of the input and output data. With the\ndefault format \"NHWC\", the data is stored in the order of:\n    [batch, in_height, in_width, in_channels].\nAlternatively, the format could be \"NCHW\", the data storage order of:\n    [batch, in_channels, in_height, in_width].", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.MaxPool3D": {"description": "Performs 3D max pooling on the input.", "Args": {"input": "A Tensor. Must be one of the following types: half, bfloat16, float32.\nShape [batch, depth, rows, cols, channels] tensor to pool over.", "ksize": "A list of ints that has length >= 5.\n1-D tensor of length 5. The size of the window for each dimension of\nthe input tensor. Must have ksize[0] = ksize[4] = 1.", "strides": "A list of ints that has length >= 5.\n1-D tensor of length 5. The stride of the sliding window for each\ndimension of input. Must have strides[0] = strides[4] = 1.", "padding": "A string from: \"SAME\", \"VALID\".\nThe type of padding algorithm to use.", "data_format": "An optional string from: \"NDHWC\", \"NCDHW\". Defaults to \"NDHWC\".\nThe data format of the input and output data. With the\ndefault format \"NDHWC\", the data is stored in the order of:\n    [batch, in_depth, in_height, in_width, in_channels].\nAlternatively, the format could be \"NCDHW\", the data storage order is:\n    [batch, in_channels, in_depth, in_height, in_width].", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.MaxPool3DGrad": {"description": "Computes gradients of 3D max pooling function.", "Args": {"orig_input": "A Tensor. Must be one of the following types: half, bfloat16, float32.\nThe original input tensor.", "orig_output": "A Tensor. Must have the same type as orig_input.\nThe original output tensor.", "grad": "A Tensor. Must be one of the following types: half, bfloat16, float32.\nOutput backprop of shape [batch, depth, rows, cols, channels].", "ksize": "A list of ints that has length >= 5.\n1-D tensor of length 5. The size of the window for each dimension of\nthe input tensor. Must have ksize[0] = ksize[4] = 1.", "strides": "A list of ints that has length >= 5.\n1-D tensor of length 5. The stride of the sliding window for each\ndimension of input. Must have strides[0] = strides[4] = 1.", "padding": "A string from: \"SAME\", \"VALID\".\nThe type of padding algorithm to use.", "data_format": "An optional string from: \"NDHWC\", \"NCDHW\". Defaults to \"NDHWC\".\nThe data format of the input and output data. With the\ndefault format \"NDHWC\", the data is stored in the order of:\n    [batch, in_depth, in_height, in_width, in_channels].\nAlternatively, the format could be \"NCDHW\", the data storage order is:\n    [batch, in_channels, in_depth, in_height, in_width].", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as grad."}, "tf.raw_ops.MaxPool3DGradGrad": {"description": "Computes second-order gradients of the maxpooling function.", "Args": {"orig_input": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.\nThe original input tensor.", "orig_output": "A Tensor. Must have the same type as orig_input.\nThe original output tensor.", "grad": "A Tensor. Must have the same type as orig_input.\nOutput backprop of shape [batch, depth, rows, cols, channels].", "ksize": "A list of ints that has length >= 5.\n1-D tensor of length 5. The size of the window for each dimension of\nthe input tensor. Must have ksize[0] = ksize[4] = 1.", "strides": "A list of ints that has length >= 5.\n1-D tensor of length 5. The stride of the sliding window for each\ndimension of input. Must have strides[0] = strides[4] = 1.", "padding": "A string from: \"SAME\", \"VALID\".\nThe type of padding algorithm to use.", "data_format": "An optional string from: \"NDHWC\", \"NCDHW\". Defaults to \"NDHWC\".\nThe data format of the input and output data. With the\ndefault format \"NDHWC\", the data is stored in the order of:\n    [batch, in_depth, in_height, in_width, in_channels].\nAlternatively, the format could be \"NCDHW\", the data storage order is:\n    [batch, in_channels, in_depth, in_height, in_width].", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as orig_input."}, "tf.raw_ops.MaxPoolGrad": {"description": "Computes gradients of the maxpooling function.", "Args": {"orig_input": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.\nThe original input tensor.", "orig_output": "A Tensor. Must have the same type as orig_input.\nThe original output tensor.", "grad": "A Tensor. Must have the same type as orig_input.\n4-D.  Gradients w.r.t. the output of max_pool.", "ksize": "A list of ints that has length >= 4.\nThe size of the window for each dimension of the input tensor.", "strides": "A list of ints that has length >= 4.\nThe stride of the sliding window for each dimension of the\ninput tensor.", "padding": "A string from: \"SAME\", \"VALID\", \"EXPLICIT\".\nThe type of padding algorithm to use.", "explicit_paddings": "An optional list of ints. Defaults to [].", "data_format": "An optional string from: \"NHWC\", \"NCHW\". Defaults to \"NHWC\".\nSpecify the data format of the input and output data. With the\ndefault format \"NHWC\", the data is stored in the order of:\n    [batch, in_height, in_width, in_channels].\nAlternatively, the format could be \"NCHW\", the data storage order of:\n    [batch, in_channels, in_height, in_width].", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as orig_input."}, "tf.raw_ops.MaxPoolGradGrad": {"description": "Computes second-order gradients of the maxpooling function.", "Args": {"orig_input": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.\nThe original input tensor.", "orig_output": "A Tensor. Must have the same type as orig_input.\nThe original output tensor.", "grad": "A Tensor. Must have the same type as orig_input.\n4-D.  Gradients of gradients w.r.t. the input of max_pool.", "ksize": "A list of ints that has length >= 4.\nThe size of the window for each dimension of the input tensor.", "strides": "A list of ints that has length >= 4.\nThe stride of the sliding window for each dimension of the\ninput tensor.", "padding": "A string from: \"SAME\", \"VALID\".\nThe type of padding algorithm to use.", "data_format": "An optional string from: \"NHWC\", \"NCHW\". Defaults to \"NHWC\".\nSpecify the data format of the input and output data. With the\ndefault format \"NHWC\", the data is stored in the order of:\n    [batch, in_height, in_width, in_channels].\nAlternatively, the format could be \"NCHW\", the data storage order of:\n    [batch, in_channels, in_height, in_width].", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as orig_input."}, "tf.raw_ops.MaxPoolGradGradV2": {"description": "Computes second-order gradients of the maxpooling function.", "Args": {"orig_input": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.\nThe original input tensor.", "orig_output": "A Tensor. Must have the same type as orig_input.\nThe original output tensor.", "grad": "A Tensor. Must have the same type as orig_input.\n4-D.  Gradients of gradients w.r.t. the input of max_pool.", "ksize": "A Tensor of type int32.\nThe size of the window for each dimension of the input tensor.", "strides": "A Tensor of type int32.\nThe stride of the sliding window for each dimension of the\ninput tensor.", "padding": "A string from: \"SAME\", \"VALID\".\nThe type of padding algorithm to use.", "data_format": "An optional string from: \"NHWC\", \"NCHW\". Defaults to \"NHWC\".\nSpecify the data format of the input and output data. With the\ndefault format \"NHWC\", the data is stored in the order of:\n    [batch, in_height, in_width, in_channels].\nAlternatively, the format could be \"NCHW\", the data storage order of:\n    [batch, in_channels, in_height, in_width].", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as orig_input."}, "tf.raw_ops.MaxPoolGradGradWithArgmax": {"description": "Computes second-order gradients of the maxpooling function.", "Args": {"input": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.\nThe original input.", "grad": "A Tensor. Must have the same type as input.\n4-D with shape [batch, height, width, channels].  Gradients w.r.t. the\ninput of max_pool.", "argmax": "A Tensor. Must be one of the following types: int32, int64.\nThe indices of the maximum values chosen for each output of max_pool.", "ksize": "A list of ints that has length >= 4.\nThe size of the window for each dimension of the input tensor.", "strides": "A list of ints that has length >= 4.\nThe stride of the sliding window for each dimension of the\ninput tensor.", "padding": "A string from: \"SAME\", \"VALID\".\nThe type of padding algorithm to use.", "include_batch_in_index": "An optional bool. Defaults to False.\nWhether to include batch dimension in flattened index of argmax.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.MaxPoolGradV2": {"description": "Computes gradients of the maxpooling function.", "Args": {"orig_input": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.\nThe original input tensor.", "orig_output": "A Tensor. Must have the same type as orig_input.\nThe original output tensor.", "grad": "A Tensor. Must have the same type as orig_input.\n4-D.  Gradients w.r.t. the output of max_pool.", "ksize": "A Tensor of type int32.\nThe size of the window for each dimension of the input tensor.", "strides": "A Tensor of type int32.\nThe stride of the sliding window for each dimension of the\ninput tensor.", "padding": "A string from: \"SAME\", \"VALID\".\nThe type of padding algorithm to use.", "data_format": "An optional string from: \"NHWC\", \"NCHW\". Defaults to \"NHWC\".\nSpecify the data format of the input and output data. With the\ndefault format \"NHWC\", the data is stored in the order of:\n    [batch, in_height, in_width, in_channels].\nAlternatively, the format could be \"NCHW\", the data storage order of:\n    [batch, in_channels, in_height, in_width].", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as orig_input."}, "tf.raw_ops.MaxPoolGradWithArgmax": {"description": "Computes gradients of the maxpooling function.", "Args": {"input": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.\nThe original input.", "grad": "A Tensor. Must have the same type as input.\n4-D with shape [batch, height, width, channels].  Gradients w.r.t. the\noutput of max_pool.", "argmax": "A Tensor. Must be one of the following types: int32, int64.\nThe indices of the maximum values chosen for each output of max_pool.", "ksize": "A list of ints that has length >= 4.\nThe size of the window for each dimension of the input tensor.", "strides": "A list of ints that has length >= 4.\nThe stride of the sliding window for each dimension of the\ninput tensor.", "padding": "A string from: \"SAME\", \"VALID\".\nThe type of padding algorithm to use.", "include_batch_in_index": "An optional bool. Defaults to False.\nWhether to include batch dimension in flattened index of argmax.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.MaxPoolV2": {"description": "Performs max pooling on the input.", "Args": {"input": "A Tensor. Must be one of the following types: half, bfloat16, float32, float64, int32, int64, uint8, int16, int8, uint16, qint8.\n4-D input to pool over.", "ksize": "A Tensor of type int32.\nThe size of the window for each dimension of the input tensor.", "strides": "A Tensor of type int32.\nThe stride of the sliding window for each dimension of the\ninput tensor.", "padding": "A string from: \"SAME\", \"VALID\".\nThe type of padding algorithm to use.", "data_format": "An optional string from: \"NHWC\", \"NCHW\", \"NCHW_VECT_C\". Defaults to \"NHWC\".\nSpecify the data format of the input and output data. With the\ndefault format \"NHWC\", the data is stored in the order of:\n    [batch, in_height, in_width, in_channels].\nAlternatively, the format could be \"NCHW\", the data storage order of:\n    [batch, in_channels, in_height, in_width].", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.MaxPoolWithArgmax": {"description": "Performs max pooling on the input and outputs both max values and indices.", "Args": {"input": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.\n4-D with shape [batch, height, width, channels].  Input to pool over.", "ksize": "A list of ints that has length >= 4.\nThe size of the window for each dimension of the input tensor.", "strides": "A list of ints that has length >= 4.\nThe stride of the sliding window for each dimension of the\ninput tensor.", "padding": "A string from: \"SAME\", \"VALID\".\nThe type of padding algorithm to use.", "Targmax": "An optional tf.DType from: tf.int32, tf.int64. Defaults to tf.int64.", "include_batch_in_index": "An optional bool. Defaults to False.\nWhether to include batch dimension in flattened index of argmax.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output, argmax)."}, "tf.raw_ops.Maximum": {"description": "Returns the max of x and y (i.e. x &gt; y ? x : y) element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, int8, uint8, int16, uint16, int32, uint32, int64, uint64.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.Mean": {"description": "Computes the mean of elements across dimensions of a tensor.", "Args": {"input": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nThe tensor to reduce.", "axis": "A Tensor. Must be one of the following types: int32, int64.\nThe dimensions to reduce. Must be in the range\n[-rank(input), rank(input)).", "keep_dims": "An optional bool. Defaults to False.\nIf true, retain reduced dimensions with length 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.Merge": {"description": "Forwards the value of an available tensor from inputs to output.", "Args": {"inputs": "A list of at least 1 Tensor objects with the same type.\nThe input tensors, exactly one of which will become available.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output, value_index)."}, "tf.raw_ops.MergeSummary": {"description": "Merges summaries.", "Args": {"inputs": "A list of at least 1 Tensor objects with type string.\nCan be of any shape.  Each must contain serialized Summary protocol\nbuffers.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.MergeV2Checkpoints": {"description": "V2 format specific: merges the metadata files of sharded checkpoints. The", "Args": {"checkpoint_prefixes": "A Tensor of type string.\nprefixes of V2 checkpoints to merge.", "destination_prefix": "A Tensor of type string.\nscalar.  The desired final prefix.  Allowed to be the same\nas one of the checkpoint_prefixes.", "delete_old_dirs": "An optional bool. Defaults to True. see above.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.Mfcc": {"description": "Transforms a spectrogram into a form that&#39;s useful for speech recognition.", "Args": {"spectrogram": "A Tensor of type float32.\nTypically produced by the Spectrogram op, with magnitude_squared\nset to true.", "sample_rate": "A Tensor of type int32.\nHow many samples per second the source audio used.", "upper_frequency_limit": "An optional float. Defaults to 4000.\nThe highest frequency to use when calculating the\nceptstrum.", "lower_frequency_limit": "An optional float. Defaults to 20.\nThe lowest frequency to use when calculating the\nceptstrum.", "filterbank_channel_count": "An optional int. Defaults to 40.\nResolution of the Mel bank used internally.", "dct_coefficient_count": "An optional int. Defaults to 13.\nHow many output channels to produce per time slice.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type float32."}, "tf.raw_ops.Min": {"description": "Computes the minimum of elements across dimensions of a tensor.", "Args": {"input": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64, qint8, quint8, qint32, qint16, quint16.\nThe tensor to reduce.", "axis": "A Tensor. Must be one of the following types: int32, int64.\nThe dimensions to reduce. Must be in the range\n[-rank(input), rank(input)).", "keep_dims": "An optional bool. Defaults to False.\nIf true, retain reduced dimensions with length 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.Minimum": {"description": "Returns the min of x and y (i.e. x &lt; y ? x : y) element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, int8, uint8, int16, uint16, int32, uint32, int64, uint64.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.MirrorPad": {"description": "Pads a tensor with mirrored values.", "Args": {"input": "A Tensor. The input tensor to be padded.", "paddings": "A Tensor. Must be one of the following types: int32, int64.\nA two-column matrix specifying the padding sizes. The number of\nrows must be the same as the rank of input.", "mode": "A string from: \"REFLECT\", \"SYMMETRIC\".\nEither REFLECT or SYMMETRIC. In reflect mode the padded regions\ndo not include the borders, while in symmetric mode the padded regions\ndo include the borders. For example, if input is [1, 2, 3] and paddings\nis [0, 2], then the output is [1, 2, 3, 2, 1] in reflect mode, and\nit is [1, 2, 3, 3, 2] in symmetric mode.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.MirrorPadGrad": {"description": "Gradient op for MirrorPad op. This op folds a mirror-padded tensor.", "Args": {"input": "A Tensor. The input tensor to be folded.", "paddings": "A Tensor. Must be one of the following types: int32, int64.\nA two-column matrix specifying the padding sizes. The number of\nrows must be the same as the rank of input.", "mode": "A string from: \"REFLECT\", \"SYMMETRIC\".\nThe mode used in the MirrorPad op.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.Mod": {"description": "Returns element-wise remainder of division. This emulates C semantics in that", "Args": {"x": "A Tensor. Must be one of the following types: int32, int64, half, half, bfloat16, float32, float64.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.ModelDataset": {"description": "Identity transformation that models performance.", "Args": {"input_dataset": "A Tensor of type variant.\nA variant tensor representing the input dataset.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "algorithm": "An optional int. Defaults to 0.", "cpu_budget": "An optional int. Defaults to 0.", "ram_budget": "An optional int. Defaults to 0.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.Mul": {"description": "Returns x * y element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, uint8, int8, uint16, int16, int32, uint32, uint64, int64, complex64, complex128.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.MulNoNan": {"description": "Returns x * y element-wise. Returns zero if y is zero, even if x if infinite or NaN.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, complex64, complex128.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.MultiDeviceIterator": {"description": "Creates a MultiDeviceIterator resource.", "Args": {"devices": "A list of strings that has length >= 1.\nA list of devices the iterator works across.", "shared_name": "A string.\nIf non-empty, this resource will be shared under the given name\nacross multiple sessions.", "container": "A string.\nIf non-empty, this resource is placed in the given container.\nOtherwise, a default container is used.", "output_types": "A list of tf.DTypes that has length >= 1.\nThe type list for the return values.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.\nThe list of shapes being produced.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type resource."}, "tf.raw_ops.MultiDeviceIteratorFromStringHandle": {"description": "Generates a MultiDeviceIterator resource from its provided string handle.", "Args": {"string_handle": "A Tensor of type string.\nString representing the resource.", "output_types": "An optional list of tf.DTypes. Defaults to [].\nThe type list for the return values.", "output_shapes": "An optional list of shapes (each a tf.TensorShape or list of ints). Defaults to [].\nThe list of shapes being produced.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type resource."}, "tf.raw_ops.MultiDeviceIteratorGetNextFromShard": {"description": "Gets next element for the provided shard number.", "Args": {"multi_device_iterator": "A Tensor of type resource.\nA MultiDeviceIterator resource.", "shard_num": "A Tensor of type int32.\nInteger representing which shard to fetch data for.", "incarnation_id": "A Tensor of type int64.\nWhich incarnation of the MultiDeviceIterator is running.", "output_types": "A list of tf.DTypes that has length >= 1.\nThe type list for the return values.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.\nThe list of shapes being produced.", "name": "A name for the operation (optional)."}, "Returns": "A list of Tensor objects of type output_types."}, "tf.raw_ops.MultiDeviceIteratorInit": {"description": "Initializes the multi device iterator with the given dataset.", "Args": {"dataset": "A Tensor of type variant. Dataset to be iterated upon.", "multi_device_iterator": "A Tensor of type resource.\nA MultiDeviceIteratorResource.", "max_buffer_size": "A Tensor of type int64.\nThe maximum size of the host side per device buffer to keep.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int64."}, "tf.raw_ops.MultiDeviceIteratorToStringHandle": {"description": "Produces a string handle for the given MultiDeviceIterator.", "Args": {"multi_device_iterator": "A Tensor of type resource.\nA MultiDeviceIterator resource.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.Multinomial": {"description": "Draws samples from a multinomial distribution.", "Args": {"logits": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.\n2-D Tensor with shape [batch_size, num_classes].  Each slice [i, :]\nrepresents the unnormalized log probabilities for all classes.", "num_samples": "A Tensor of type int32.\n0-D.  Number of independent samples to draw for each row slice.", "seed": "An optional int. Defaults to 0.\nIf either seed or seed2 is set to be non-zero, the internal random number\ngenerator is seeded by the given seed.  Otherwise, a random seed is used.", "seed2": "An optional int. Defaults to 0.\nA second seed to avoid seed collision.", "output_dtype": "An optional tf.DType from: tf.int32, tf.int64. Defaults to tf.int64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type output_dtype."}, "tf.raw_ops.MutableDenseHashTable": {"description": "Creates an empty hash table that uses tensors as the backing store.", "Args": {"empty_key": "A Tensor.\nThe key used to represent empty key buckets internally. Must not\nbe used in insert or lookup operations.", "value_dtype": "A tf.DType. Type of the table values.", "container": "An optional string. Defaults to \"\".\nIf non-empty, this table is placed in the given container.\nOtherwise, a default container is used.", "shared_name": "An optional string. Defaults to \"\".\nIf non-empty, this table is shared under the given name across\nmultiple sessions.", "use_node_name_sharing": "An optional bool. Defaults to False.", "value_shape": "An optional tf.TensorShape or list of ints. Defaults to [].\nThe shape of each value.", "initial_num_buckets": "An optional int. Defaults to 131072.\nThe initial number of hash table buckets. Must be a power\nto 2.", "max_load_factor": "An optional float. Defaults to 0.8.\nThe maximum ratio between number of entries and number of\nbuckets before growing the table. Must be between 0 and 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type mutable string."}, "tf.raw_ops.MutableDenseHashTableV2": {"description": "Creates an empty hash table that uses tensors as the backing store.", "Args": {"empty_key": "A Tensor.\nThe key used to represent empty key buckets internally. Must not\nbe used in insert or lookup operations.", "deleted_key": "A Tensor. Must have the same type as empty_key.", "value_dtype": "A tf.DType. Type of the table values.", "container": "An optional string. Defaults to \"\".\nIf non-empty, this table is placed in the given container.\nOtherwise, a default container is used.", "shared_name": "An optional string. Defaults to \"\".\nIf non-empty, this table is shared under the given name across\nmultiple sessions.", "use_node_name_sharing": "An optional bool. Defaults to False.", "value_shape": "An optional tf.TensorShape or list of ints. Defaults to [].\nThe shape of each value.", "initial_num_buckets": "An optional int. Defaults to 131072.\nThe initial number of hash table buckets. Must be a power\nto 2.", "max_load_factor": "An optional float. Defaults to 0.8.\nThe maximum ratio between number of entries and number of\nbuckets before growing the table. Must be between 0 and 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type resource."}, "tf.raw_ops.MutableHashTable": {"description": "Creates an empty hash table.", "Args": {"key_dtype": "A tf.DType. Type of the table keys.", "value_dtype": "A tf.DType. Type of the table values.", "container": "An optional string. Defaults to \"\".\nIf non-empty, this table is placed in the given container.\nOtherwise, a default container is used.", "shared_name": "An optional string. Defaults to \"\".\nIf non-empty, this table is shared under the given name across\nmultiple sessions.", "use_node_name_sharing": "An optional bool. Defaults to False.\nIf true and shared_name is empty, the table is shared\nusing the node name.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type mutable string."}, "tf.raw_ops.MutableHashTableOfTensors": {"description": "Creates an empty hash table.", "Args": {"key_dtype": "A tf.DType. Type of the table keys.", "value_dtype": "A tf.DType. Type of the table values.", "container": "An optional string. Defaults to \"\".\nIf non-empty, this table is placed in the given container.\nOtherwise, a default container is used.", "shared_name": "An optional string. Defaults to \"\".\nIf non-empty, this table is shared under the given name across\nmultiple sessions.", "use_node_name_sharing": "An optional bool. Defaults to False.", "value_shape": "An optional tf.TensorShape or list of ints. Defaults to [].", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type mutable string."}, "tf.raw_ops.MutableHashTableOfTensorsV2": {"description": "Creates an empty hash table.", "Args": {"key_dtype": "A tf.DType. Type of the table keys.", "value_dtype": "A tf.DType. Type of the table values.", "container": "An optional string. Defaults to \"\".\nIf non-empty, this table is placed in the given container.\nOtherwise, a default container is used.", "shared_name": "An optional string. Defaults to \"\".\nIf non-empty, this table is shared under the given name across\nmultiple sessions.", "use_node_name_sharing": "An optional bool. Defaults to False.", "value_shape": "An optional tf.TensorShape or list of ints. Defaults to [].", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type resource."}, "tf.raw_ops.MutableHashTableV2": {"description": "Creates an empty hash table.", "Args": {"key_dtype": "A tf.DType. Type of the table keys.", "value_dtype": "A tf.DType. Type of the table values.", "container": "An optional string. Defaults to \"\".\nIf non-empty, this table is placed in the given container.\nOtherwise, a default container is used.", "shared_name": "An optional string. Defaults to \"\".\nIf non-empty, this table is shared under the given name across\nmultiple sessions.", "use_node_name_sharing": "An optional bool. Defaults to False.\nIf true and shared_name is empty, the table is shared\nusing the node name.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type resource."}, "tf.raw_ops.MutexLock": {"description": "Locks a mutex resource. The output is the lock. So long as the lock tensor", "Args": {"mutex": "A Tensor of type resource. The mutex resource to lock.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.MutexV2": {"description": "Creates a Mutex resource that can be locked by MutexLock.", "Args": {"container": "An optional string. Defaults to \"\".\nIf non-empty, this variable is placed in the given container.\nOtherwise, a default container is used.", "shared_name": "An optional string. Defaults to \"\".\nIf non-empty, this variable is named in the given bucket\nwith this shared_name. Otherwise, the node name is used instead.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type resource."}, "tf.raw_ops.NcclAllReduce": {"description": "Outputs a tensor containing the reduction across all input tensors.", "Args": {"input": "A Tensor. Must be one of the following types: half, float32, float64, int32, int64.", "reduction": "A string from: \"min\", \"max\", \"prod\", \"sum\".", "num_devices": "An int.", "shared_name": "A string.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.NcclBroadcast": {"description": "Sends input to all devices that are connected to the output.", "Args": {"input": "A Tensor. Must be one of the following types: half, float32, float64, int32, int64.", "shape": "A tf.TensorShape or list of ints.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.NcclReduce": {"description": "Reduces input from num_devices using reduction to a single device.", "Args": {"input": "A list of at least 1 Tensor objects with the same type in: half, float32, float64, int32, int64.", "reduction": "A string from: \"min\", \"max\", \"prod\", \"sum\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.Ndtri": {"Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.Neg": {"description": "Computes numerical negative value element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, int8, int16, int32, int64, complex64, complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.NextAfter": {"description": "Returns the next representable value of x1 in the direction of x2, element-wise.", "Args": {"x1": "A Tensor. Must be one of the following types: float64, float32.", "x2": "A Tensor. Must have the same type as x1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x1."}, "tf.raw_ops.NextIteration": {"description": "Makes its input available to the next iteration.", "Args": {"data": "A Tensor. The tensor to be made available to the next iteration.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as data."}, "tf.raw_ops.NoOp": {"description": "Does nothing. Only useful as a placeholder for control edges.", "Args": {"name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.NonDeterministicInts": {"description": "Non-deterministically generates some integers.", "Args": {"shape": "A Tensor. The shape of the output tensor.", "dtype": "An optional tf.DType. Defaults to tf.int64.\nThe type of the output.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dtype."}, "tf.raw_ops.NonMaxSuppression": {"description": "Greedily selects a subset of bounding boxes in descending order of score,", "Args": {"boxes": "A Tensor of type float32.\nA 2-D float tensor of shape [num_boxes, 4].", "scores": "A Tensor of type float32.\nA 1-D float tensor of shape [num_boxes] representing a single\nscore corresponding to each box (each row of boxes).", "max_output_size": "A Tensor of type int32.\nA scalar integer tensor representing the maximum number of\nboxes to be selected by non max suppression.", "iou_threshold": "An optional float. Defaults to 0.5.\nA float representing the threshold for deciding whether boxes\noverlap too much with respect to IOU.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int32."}, "tf.raw_ops.NonMaxSuppressionV2": {"description": "Greedily selects a subset of bounding boxes in descending order of score,", "Args": {"boxes": "A Tensor. Must be one of the following types: half, float32.\nA 2-D float tensor of shape [num_boxes, 4].", "scores": "A Tensor. Must have the same type as boxes.\nA 1-D float tensor of shape [num_boxes] representing a single\nscore corresponding to each box (each row of boxes).", "max_output_size": "A Tensor of type int32.\nA scalar integer tensor representing the maximum number of\nboxes to be selected by non max suppression.", "iou_threshold": "A Tensor. Must be one of the following types: half, float32.\nA 0-D float tensor representing the threshold for deciding whether\nboxes overlap too much with respect to IOU.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int32."}, "tf.raw_ops.NonMaxSuppressionV3": {"description": "Greedily selects a subset of bounding boxes in descending order of score,", "Args": {"boxes": "A Tensor. Must be one of the following types: half, float32.\nA 2-D float tensor of shape [num_boxes, 4].", "scores": "A Tensor. Must have the same type as boxes.\nA 1-D float tensor of shape [num_boxes] representing a single\nscore corresponding to each box (each row of boxes).", "max_output_size": "A Tensor of type int32.\nA scalar integer tensor representing the maximum number of\nboxes to be selected by non max suppression.", "iou_threshold": "A Tensor. Must be one of the following types: half, float32.\nA 0-D float tensor representing the threshold for deciding whether\nboxes overlap too much with respect to IOU.", "score_threshold": "A Tensor. Must have the same type as iou_threshold.\nA 0-D float tensor representing the threshold for deciding when to remove\nboxes based on score.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int32."}, "tf.raw_ops.NonMaxSuppressionV4": {"description": "Greedily selects a subset of bounding boxes in descending order of score,", "Args": {"boxes": "A Tensor. Must be one of the following types: half, float32.\nA 2-D float tensor of shape [num_boxes, 4].", "scores": "A Tensor. Must have the same type as boxes.\nA 1-D float tensor of shape [num_boxes] representing a single\nscore corresponding to each box (each row of boxes).", "max_output_size": "A Tensor of type int32.\nA scalar integer tensor representing the maximum number of\nboxes to be selected by non max suppression.", "iou_threshold": "A Tensor. Must be one of the following types: half, float32.\nA 0-D float tensor representing the threshold for deciding whether\nboxes overlap too much with respect to IOU.", "score_threshold": "A Tensor. Must have the same type as iou_threshold.\nA 0-D float tensor representing the threshold for deciding when to remove\nboxes based on score.", "pad_to_max_output_size": "An optional bool. Defaults to False.\nIf true, the output selected_indices is padded to be of length\nmax_output_size. Defaults to false.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (selected_indices, valid_outputs)."}, "tf.raw_ops.NonMaxSuppressionV5": {"description": "Greedily selects a subset of bounding boxes in descending order of score,", "Args": {"boxes": "A Tensor. Must be one of the following types: half, float32.\nA 2-D float tensor of shape [num_boxes, 4].", "scores": "A Tensor. Must have the same type as boxes.\nA 1-D float tensor of shape [num_boxes] representing a single\nscore corresponding to each box (each row of boxes).", "max_output_size": "A Tensor of type int32.\nA scalar integer tensor representing the maximum number of\nboxes to be selected by non max suppression.", "iou_threshold": "A Tensor. Must have the same type as boxes.\nA 0-D float tensor representing the threshold for deciding whether\nboxes overlap too much with respect to IOU.", "score_threshold": "A Tensor. Must have the same type as boxes.\nA 0-D float tensor representing the threshold for deciding when to remove\nboxes based on score.", "soft_nms_sigma": "A Tensor. Must have the same type as boxes.\nA 0-D float tensor representing the sigma parameter for Soft NMS; see Bodla et\nal (c.f. https://arxiv.org/abs/1704.04503).  When soft_nms_sigma=0.0 (which\nis default), we fall back to standard (hard) NMS.", "pad_to_max_output_size": "An optional bool. Defaults to False.\nIf true, the output selected_indices is padded to be of length\nmax_output_size. Defaults to false.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (selected_indices, selected_scores, valid_outputs)."}, "tf.raw_ops.NonMaxSuppressionWithOverlaps": {"description": "Greedily selects a subset of bounding boxes in descending order of score,", "Args": {"overlaps": "A Tensor of type float32.\nA 2-D float tensor of shape [num_boxes, num_boxes] representing\nthe n-by-n box overlap values.", "scores": "A Tensor of type float32.\nA 1-D float tensor of shape [num_boxes] representing a single\nscore corresponding to each box (each row of boxes).", "max_output_size": "A Tensor of type int32.\nA scalar integer tensor representing the maximum number of\nboxes to be selected by non max suppression.", "overlap_threshold": "A Tensor of type float32.\nA 0-D float tensor representing the threshold for deciding whether\nboxes overlap too.", "score_threshold": "A Tensor of type float32.\nA 0-D float tensor representing the threshold for deciding when to remove\nboxes based on score.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int32."}, "tf.raw_ops.NonSerializableDataset": {"Args": {"input_dataset": "A Tensor of type variant.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.NotEqual": {"description": "Returns the truth value of (x != y) element-wise.", "Args": {"x": "A Tensor.", "y": "A Tensor. Must have the same type as x.", "incompatible_shape_error": "An optional bool. Defaults to True.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type bool."}, "tf.raw_ops.NthElement": {"description": "Finds values of the n-th order statistic for the last dimension.", "Args": {"input": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.\n1-D or higher with last dimension at least n+1.", "n": "A Tensor of type int32.\n0-D. Position of sorted vector to select along the last dimension (along\neach row for matrices). Valid range of n is [0, input.shape[:-1])", "reverse": "An optional bool. Defaults to False.\nWhen set to True, find the nth-largest value in the vector and vice\nversa.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.OneHot": {"description": "Returns a one-hot tensor.", "Args": {"indices": "A Tensor. Must be one of the following types: uint8, int32, int64.\nA tensor of indices.", "depth": "A Tensor of type int32.\nA scalar defining the depth of the one hot dimension.", "on_value": "A Tensor.\nA scalar defining the value to fill in output when indices[j] = i.", "off_value": "A Tensor. Must have the same type as on_value.\nA scalar defining the value to fill in output when indices[j] != i.", "axis": "An optional int. Defaults to -1.\nThe axis to fill (default: -1, a new inner-most axis).", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as on_value."}, "tf.raw_ops.OneShotIterator": {"description": "Makes a &#34;one-shot&#34; iterator that can be iterated only once.", "Args": {"dataset_factory": "A function decorated with @Defun.\nA function of type () -> DT_VARIANT, where the returned\nDT_VARIANT is a dataset.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "container": "An optional string. Defaults to \"\".", "shared_name": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type resource."}, "tf.raw_ops.OnesLike": {"description": "Returns a tensor of ones with the same shape and type as x.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, int8, uint8, int16, uint16, int32, uint32, int64, uint64, complex64, complex128, bool.\na tensor of type T.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.OptimizeDataset": {"description": "Creates a dataset by applying optimizations to input_dataset.", "Args": {"input_dataset": "A Tensor of type variant.\nA variant tensor representing the input dataset.", "optimizations": "A Tensor of type string.\nA tf.string vector tf.Tensor identifying optimizations to use.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "optimization_configs": "An optional list of strings. Defaults to [].", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.OptimizeDatasetV2": {"description": "Creates a dataset by applying related optimizations to input_dataset.", "Args": {"input_dataset": "A Tensor of type variant.\nA variant tensor representing the input dataset.", "optimizations_enabled": "A Tensor of type string.\nA tf.string vector tf.Tensor identifying user enabled optimizations.", "optimizations_disabled": "A Tensor of type string.\nA tf.string vector tf.Tensor identifying user disabled optimizations.", "optimizations_default": "A Tensor of type string.\nA tf.string vector tf.Tensor identifying optimizations by default.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "optimization_configs": "An optional list of strings. Defaults to [].", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.OptionalFromValue": {"description": "Constructs an Optional variant from a tuple of tensors.", "Args": {"components": "A list of Tensor objects.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.OptionalGetValue": {"description": "Returns the value stored in an Optional variant or raises an error if none exists.", "Args": {"optional": "A Tensor of type variant.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A list of Tensor objects of type output_types."}, "tf.raw_ops.OptionalHasValue": {"description": "Returns true if and only if the given Optional variant has a value.", "Args": {"optional": "A Tensor of type variant.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type bool."}, "tf.raw_ops.OptionalNone": {"description": "Creates an Optional variant with no value.", "Args": {"name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.OptionsDataset": {"description": "Creates a dataset by attaching tf.data.Options to input_dataset.", "Args": {"input_dataset": "A Tensor of type variant.\nA variant tensor representing the input dataset.", "serialized_options": "A string.\nA tf.string scalar tf.Tensor of serialized tf.data.Options protocol buffer.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.OrderedMapClear": {"description": "Op removes all elements in the underlying container.", "Args": {"dtypes": "A list of tf.DTypes.", "capacity": "An optional int that is >= 0. Defaults to 0.", "memory_limit": "An optional int that is >= 0. Defaults to 0.", "container": "An optional string. Defaults to \"\".", "shared_name": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.OrderedMapIncompleteSize": {"description": "Op returns the number of incomplete elements in the underlying container.", "Args": {"dtypes": "A list of tf.DTypes.", "capacity": "An optional int that is >= 0. Defaults to 0.", "memory_limit": "An optional int that is >= 0. Defaults to 0.", "container": "An optional string. Defaults to \"\".", "shared_name": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int32."}, "tf.raw_ops.OrderedMapPeek": {"description": "Op peeks at the values at the specified key. If the", "Args": {"key": "A Tensor of type int64.", "indices": "A Tensor of type int32.", "dtypes": "A list of tf.DTypes that has length >= 1.", "capacity": "An optional int that is >= 0. Defaults to 0.", "memory_limit": "An optional int that is >= 0. Defaults to 0.", "container": "An optional string. Defaults to \"\".", "shared_name": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A list of Tensor objects of type dtypes."}, "tf.raw_ops.OrderedMapSize": {"description": "Op returns the number of elements in the underlying container.", "Args": {"dtypes": "A list of tf.DTypes.", "capacity": "An optional int that is >= 0. Defaults to 0.", "memory_limit": "An optional int that is >= 0. Defaults to 0.", "container": "An optional string. Defaults to \"\".", "shared_name": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int32."}, "tf.raw_ops.OrderedMapStage": {"description": "Stage (key, values) in the underlying container which behaves like a ordered", "Args": {"key": "A Tensor of type int64. int64", "indices": "A Tensor of type int32.", "values": "A list of Tensor objects. a list of tensors\ndtypes A list of data types that inserted values should adhere to.", "dtypes": "A list of tf.DTypes.", "capacity": "An optional int that is >= 0. Defaults to 0.\nMaximum number of elements in the Staging Area. If > 0, inserts\non the container will block when the capacity is reached.", "memory_limit": "An optional int that is >= 0. Defaults to 0.", "container": "An optional string. Defaults to \"\".\nIf non-empty, this queue is placed in the given container. Otherwise,\na default container is used.", "shared_name": "An optional string. Defaults to \"\".\nIt is necessary to match this name to the matching Unstage Op.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.OrderedMapUnstage": {"description": "Op removes and returns the values associated with the key", "Args": {"key": "A Tensor of type int64.", "indices": "A Tensor of type int32.", "dtypes": "A list of tf.DTypes that has length >= 1.", "capacity": "An optional int that is >= 0. Defaults to 0.", "memory_limit": "An optional int that is >= 0. Defaults to 0.", "container": "An optional string. Defaults to \"\".", "shared_name": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A list of Tensor objects of type dtypes."}, "tf.raw_ops.OrderedMapUnstageNoKey": {"description": "Op removes and returns the (key, value) element with the smallest", "Args": {"indices": "A Tensor of type int32.", "dtypes": "A list of tf.DTypes that has length >= 1.", "capacity": "An optional int that is >= 0. Defaults to 0.", "memory_limit": "An optional int that is >= 0. Defaults to 0.", "container": "An optional string. Defaults to \"\".", "shared_name": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (key, values)."}, "tf.raw_ops.OutfeedDequeue": {"description": "Retrieves a single tensor from the computation outfeed.", "Args": {"dtype": "A tf.DType. The type of elements in the tensor.", "shape": "A tf.TensorShape or list of ints. The shape of the tensor.", "device_ordinal": "An optional int. Defaults to -1.\nThe TPU device to use. This should be -1 when the Op\nis running on a TPU device, and >= 0 when the Op is running on the CPU\ndevice.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dtype."}, "tf.raw_ops.OutfeedDequeueTuple": {"description": "Retrieve multiple values from the computation outfeed.", "Args": {"dtypes": "A list of tf.DTypes that has length >= 1.\nThe element types of each element in outputs.", "shapes": "A list of shapes (each a tf.TensorShape or list of ints).\nThe shapes of each tensor in outputs.", "device_ordinal": "An optional int. Defaults to -1.\nThe TPU device to use. This should be -1 when the Op\nis running on a TPU device, and >= 0 when the Op is running on the CPU\ndevice.", "name": "A name for the operation (optional)."}, "Returns": "A list of Tensor objects of type dtypes."}, "tf.raw_ops.OutfeedDequeueTupleV2": {"description": "Retrieve multiple values from the computation outfeed. Device ordinal is a"}, "tf.raw_ops.OutfeedDequeueV2": {"description": "Retrieves a single tensor from the computation outfeed. Device ordinal is a"}, "tf.raw_ops.OutfeedEnqueue": {"description": "Enqueue a Tensor on the computation outfeed.", "Args": {"input": "A Tensor. A tensor that will be inserted into the outfeed queue.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.OutfeedEnqueueTuple": {"description": "Enqueue multiple Tensor values on the computation outfeed.", "Args": {"inputs": "A list of Tensor objects.\nA list of tensors that will be inserted into the outfeed queue as an\nXLA tuple.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.Pack": {"description": "Packs a list of N rank-R tensors into one rank-(R&#43;1) tensor.", "Args": {"values": "A list of at least 1 Tensor objects with the same type.\nMust be of same shape and type.", "axis": "An optional int. Defaults to 0.\nDimension along which to pack.  Negative values wrap around, so the\nvalid range is [-(R+1), R+1).", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as values."}, "tf.raw_ops.Pad": {"description": "Pads a tensor with zeros.", "Args": {"input": "A Tensor.", "paddings": "A Tensor. Must be one of the following types: int32, int64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.PadV2": {"description": "Pads a tensor.", "Args": {"input": "A Tensor.", "paddings": "A Tensor. Must be one of the following types: int32, int64.", "constant_values": "A Tensor. Must have the same type as input.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.PaddedBatchDataset": {"description": "Creates a dataset that batches and pads batch_size elements from the input.", "Args": {"input_dataset": "A Tensor of type variant.", "batch_size": "A Tensor of type int64.\nA scalar representing the number of elements to accumulate in a\nbatch.", "padded_shapes": "A list of at least 1 Tensor objects with type int64.\nA list of int64 tensors representing the desired padded shapes\nof the corresponding output components. These shapes may be partially\nspecified, using -1 to indicate that a particular dimension should be\npadded to the maximum size of all batch elements.", "padding_values": "A list of Tensor objects.\nA list of scalars containing the padding value to use for\neach of the outputs.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.PaddedBatchDatasetV2": {"description": "Creates a dataset that batches and pads batch_size elements from the input.", "Args": {"input_dataset": "A Tensor of type variant.", "batch_size": "A Tensor of type int64.\nA scalar representing the number of elements to accumulate in a\nbatch.", "padded_shapes": "A list of at least 1 Tensor objects with type int64.\nA list of int64 tensors representing the desired padded shapes\nof the corresponding output components. These shapes may be partially\nspecified, using -1 to indicate that a particular dimension should be\npadded to the maximum size of all batch elements.", "padding_values": "A list of Tensor objects.\nA list of scalars containing the padding value to use for\neach of the outputs.", "drop_remainder": "A Tensor of type bool.\nA scalar representing whether the last batch should be dropped in case its size\nis smaller than desired.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "parallel_copy": "An optional bool. Defaults to False.", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.PaddingFIFOQueue": {"description": "A queue that produces elements in first-in first-out order.", "Args": {"component_types": "A list of tf.DTypes that has length >= 1.\nThe type of each component in a value.", "shapes": "An optional list of shapes (each a tf.TensorShape or list of ints). Defaults to [].\nThe shape of each component in a value. The length of this attr must\nbe either 0 or the same as the length of component_types.\nShapes of fixed rank but variable size are allowed by setting\nany shape dimension to -1.  In this case, the inputs' shape may vary along\nthe given dimension, and DequeueMany will pad the given dimension with\nzeros up to the maximum shape of all elements in the given batch.\nIf the length of this attr is 0, different queue elements may have\ndifferent ranks and shapes, but only one element may be dequeued at a time.", "capacity": "An optional int. Defaults to -1.\nThe upper bound on the number of elements in this queue.\nNegative numbers mean no limit.", "container": "An optional string. Defaults to \"\".\nIf non-empty, this queue is placed in the given container.\nOtherwise, a default container is used.", "shared_name": "An optional string. Defaults to \"\".\nIf non-empty, this queue will be shared under the given name\nacross multiple sessions.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type mutable string."}, "tf.raw_ops.PaddingFIFOQueueV2": {"description": "A queue that produces elements in first-in first-out order.", "Args": {"component_types": "A list of tf.DTypes that has length >= 1.\nThe type of each component in a value.", "shapes": "An optional list of shapes (each a tf.TensorShape or list of ints). Defaults to [].\nThe shape of each component in a value. The length of this attr must\nbe either 0 or the same as the length of component_types.\nShapes of fixed rank but variable size are allowed by setting\nany shape dimension to -1.  In this case, the inputs' shape may vary along\nthe given dimension, and DequeueMany will pad the given dimension with\nzeros up to the maximum shape of all elements in the given batch.\nIf the length of this attr is 0, different queue elements may have\ndifferent ranks and shapes, but only one element may be dequeued at a time.", "capacity": "An optional int. Defaults to -1.\nThe upper bound on the number of elements in this queue.\nNegative numbers mean no limit.", "container": "An optional string. Defaults to \"\".\nIf non-empty, this queue is placed in the given container.\nOtherwise, a default container is used.", "shared_name": "An optional string. Defaults to \"\".\nIf non-empty, this queue will be shared under the given name\nacross multiple sessions.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type resource."}, "tf.raw_ops.ParallelBatchDataset": {"Args": {"input_dataset": "A Tensor of type variant.", "batch_size": "A Tensor of type int64.", "num_parallel_calls": "A Tensor of type int64.", "drop_remainder": "A Tensor of type bool.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "parallel_copy": "An optional bool. Defaults to False.", "deterministic": "An optional string. Defaults to \"default\".", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ParallelConcat": {"description": "Concatenates a list of N tensors along the first dimension.", "Args": {"values": "A list of at least 1 Tensor objects with the same type.\nTensors to be concatenated. All must have size 1 in the first dimension\nand same shape.", "shape": "A tf.TensorShape or list of ints.\nthe final shape of the result; should be equal to the shapes of any input\nbut with the number of input values in the first dimension.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as values."}, "tf.raw_ops.ParallelDynamicStitch": {"description": "Interleave the values from the data tensors into a single tensor.", "Args": {"indices": "A list of at least 1 Tensor objects with type int32.", "data": "A list with the same length as indices of Tensor objects with the same type.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as data."}, "tf.raw_ops.ParallelFilterDataset": {"description": "Creates a dataset containing elements of input_dataset matching predicate.", "Args": {"input_dataset": "A Tensor of type variant.", "other_arguments": "A list of Tensor objects.\nA list of tensors, typically values that were captured when\nbuilding a closure for predicate.", "num_parallel_calls": "A Tensor of type int64.\nThe number of concurrent invocations of predicate that process\nelements from input_dataset in parallel.", "predicate": "A function decorated with @Defun.\nA function returning a scalar boolean.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "deterministic": "An optional string. Defaults to \"default\".\nA string indicating the op-level determinism to use. Deterministic controls\nwhether the interleave is allowed to return elements out of order if the next\nelement to be returned isn't available, but a later element is. Options are\n\"true\", \"false\", and \"default\". \"default\" indicates that determinism should be\ndecided by the experimental_deterministic parameter of tf.data.Options.", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ParallelInterleaveDataset": {"description": "Creates a dataset that applies f to the outputs of input_dataset.", "Args": {"input_dataset": "A Tensor of type variant.\nDataset that produces a stream of arguments for the function f.", "other_arguments": "A list of Tensor objects.\nAdditional arguments to pass to f beyond those produced by input_dataset.\nEvaluated once when the dataset is instantiated.", "cycle_length": "A Tensor of type int64.\nNumber of datasets (each created by applying f to the elements of\ninput_dataset) among which the ParallelInterleaveDataset will cycle in a\nround-robin fashion.", "block_length": "A Tensor of type int64.\nNumber of elements at a time to produce from each interleaved invocation of a\ndataset returned by f.", "sloppy": "A Tensor of type bool.\nIf True, return elements as they become available, even if that means returning\nthese elements in a non-deterministic order. Sloppy operation may result in better\nperformance in the presence of stragglers, but the dataset will still block if\nall of its open streams are blocked.\nIf False, always return elements in a deterministic order.", "buffer_output_elements": "A Tensor of type int64.\nThe number of elements each iterator being interleaved should buffer (similar\nto the .prefetch() transformation for each interleaved iterator).", "prefetch_input_elements": "A Tensor of type int64.\nDetermines the number of iterators to prefetch, allowing buffers to warm up and\ndata to be pre-fetched without blocking the main thread.", "f": "A function decorated with @Defun.\nA function mapping elements of input_dataset, concatenated with\nother_arguments, to a Dataset variant that contains elements matching\noutput_types and output_shapes.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ParallelInterleaveDatasetV2": {"description": "Creates a dataset that applies f to the outputs of input_dataset.", "Args": {"input_dataset": "A Tensor of type variant.\nDataset that produces a stream of arguments for the function f.", "other_arguments": "A list of Tensor objects.\nAdditional arguments to pass to f beyond those produced by input_dataset.\nEvaluated once when the dataset is instantiated.", "cycle_length": "A Tensor of type int64.\nNumber of datasets (each created by applying f to the elements of\ninput_dataset) among which the ParallelInterleaveDatasetV2 will cycle in a\nround-robin fashion.", "block_length": "A Tensor of type int64.\nNumber of elements at a time to produce from each interleaved invocation of a\ndataset returned by f.", "num_parallel_calls": "A Tensor of type int64.\nDetermines the number of threads that should be used for fetching data from\ninput datasets in parallel. The Python API tf.data.experimental.AUTOTUNE\nconstant can be used to indicate that the level of parallelism should be autotuned.", "f": "A function decorated with @Defun.\nA function mapping elements of input_dataset, concatenated with\nother_arguments, to a Dataset variant that contains elements matching\noutput_types and output_shapes.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "sloppy": "An optional bool. Defaults to False.", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ParallelInterleaveDatasetV3": {"description": "Creates a dataset that applies f to the outputs of input_dataset.", "Args": {"input_dataset": "A Tensor of type variant.\nDataset that produces a stream of arguments for the function f.", "other_arguments": "A list of Tensor objects.\nAdditional arguments to pass to f beyond those produced by input_dataset.\nEvaluated once when the dataset is instantiated.", "cycle_length": "A Tensor of type int64.\nNumber of datasets (each created by applying f to the elements of\ninput_dataset) among which the ParallelInterleaveDatasetV2 will cycle in a\nround-robin fashion.", "block_length": "A Tensor of type int64.\nNumber of elements at a time to produce from each interleaved invocation of a\ndataset returned by f.", "num_parallel_calls": "A Tensor of type int64.\nDetermines the number of threads that should be used for fetching data from\ninput datasets in parallel. The Python API tf.data.experimental.AUTOTUNE\nconstant can be used to indicate that the level of parallelism should be autotuned.", "f": "A function decorated with @Defun.\nA function mapping elements of input_dataset, concatenated with\nother_arguments, to a Dataset variant that contains elements matching\noutput_types and output_shapes.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "deterministic": "An optional string. Defaults to \"default\".\nA string indicating the op-level determinism to use. Deterministic controls\nwhether the interleave is allowed to return elements out of order if the next\nelement to be returned isn't available, but a later element is. Options are\n\"true\", \"false\", and \"default\". \"default\" indicates that determinism should be\ndecided by the experimental_deterministic parameter of tf.data.Options.", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ParallelInterleaveDatasetV4": {"description": "Creates a dataset that applies f to the outputs of input_dataset.", "Args": {"input_dataset": "A Tensor of type variant.\nDataset that produces a stream of arguments for the function f.", "other_arguments": "A list of Tensor objects.\nAdditional arguments to pass to f beyond those produced by input_dataset.\nEvaluated once when the dataset is instantiated.", "cycle_length": "A Tensor of type int64.\nNumber of datasets (each created by applying f to the elements of\ninput_dataset) among which the ParallelInterleaveDatasetV2 will cycle in a\nround-robin fashion.", "block_length": "A Tensor of type int64.\nNumber of elements at a time to produce from each interleaved invocation of a\ndataset returned by f.", "buffer_output_elements": "A Tensor of type int64.\nThe number of elements each iterator being interleaved should buffer (similar\nto the .prefetch() transformation for each interleaved iterator).", "prefetch_input_elements": "A Tensor of type int64.\nDetermines the number of iterators to prefetch, allowing buffers to warm up and\ndata to be pre-fetched without blocking the main thread.", "num_parallel_calls": "A Tensor of type int64.\nDetermines the number of threads that should be used for fetching data from\ninput datasets in parallel. The Python API tf.data.experimental.AUTOTUNE\nconstant can be used to indicate that the level of parallelism should be autotuned.", "f": "A function decorated with @Defun.\nA function mapping elements of input_dataset, concatenated with\nother_arguments, to a Dataset variant that contains elements matching\noutput_types and output_shapes.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "deterministic": "An optional string. Defaults to \"default\".\nA string indicating the op-level determinism to use. Deterministic controls\nwhether the interleave is allowed to return elements out of order if the next\nelement to be returned isn't available, but a later element is. Options are\n\"true\", \"false\", and \"default\". \"default\" indicates that determinism should be\ndecided by the experimental_deterministic parameter of tf.data.Options.", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ParallelMapDataset": {"description": "Creates a dataset that applies f to the outputs of input_dataset.", "Args": {"input_dataset": "A Tensor of type variant.", "other_arguments": "A list of Tensor objects.", "num_parallel_calls": "A Tensor of type int32.\nThe number of concurrent invocations of f that process\nelements from input_dataset in parallel.", "f": "A function decorated with @Defun.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "use_inter_op_parallelism": "An optional bool. Defaults to True.", "sloppy": "An optional bool. Defaults to False.", "preserve_cardinality": "An optional bool. Defaults to False.", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ParallelMapDatasetV2": {"description": "Creates a dataset that applies f to the outputs of input_dataset.", "Args": {"input_dataset": "A Tensor of type variant.", "other_arguments": "A list of Tensor objects.", "num_parallel_calls": "A Tensor of type int64.\nThe number of concurrent invocations of f that process\nelements from input_dataset in parallel.", "f": "A function decorated with @Defun.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "use_inter_op_parallelism": "An optional bool. Defaults to True.", "deterministic": "An optional string. Defaults to \"default\".", "preserve_cardinality": "An optional bool. Defaults to False.", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ParameterizedTruncatedNormal": {"description": "Outputs random values from a normal distribution. The parameters may each be a", "Args": {"shape": "A Tensor. Must be one of the following types: int32, int64.\nThe shape of the output tensor. Batches are indexed by the 0th dimension.", "means": "A Tensor. Must be one of the following types: half, bfloat16, float32, float64.\nThe mean parameter of each batch.", "stdevs": "A Tensor. Must have the same type as means.\nThe standard deviation parameter of each batch. Must be greater than 0.", "minvals": "A Tensor. Must have the same type as means.\nThe minimum cutoff. May be -infinity.", "maxvals": "A Tensor. Must have the same type as means.\nThe maximum cutoff. May be +infinity, and must be more than the minval\nfor each batch.", "seed": "An optional int. Defaults to 0.\nIf either seed or seed2 are set to be non-zero, the random number\ngenerator is seeded by the given seed.  Otherwise, it is seeded by a\nrandom seed.", "seed2": "An optional int. Defaults to 0.\nA second seed to avoid seed collision.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as means."}, "tf.raw_ops.ParseExample": {"description": "Transforms a vector of brain.Example protos (as strings) into typed tensors.", "Args": {"serialized": "A Tensor of type string.\nA vector containing a batch of binary serialized Example protos.", "names": "A Tensor of type string.\nA vector containing the names of the serialized protos.\nMay contain, for example, table key (descriptive) names for the\ncorresponding serialized protos.  These are purely useful for debugging\npurposes, and the presence of values here has no effect on the output.\nMay also be an empty vector if no names are available.\nIf non-empty, this vector must be the same length as \"serialized\".", "sparse_keys": "A list of Tensor objects with type string.\nA list of Nsparse string Tensors (scalars).\nThe keys expected in the Examples' features associated with sparse values.", "dense_keys": "A list of Tensor objects with type string.\nA list of Ndense string Tensors (scalars).\nThe keys expected in the Examples' features associated with dense values.", "dense_defaults": "A list of Tensor objects with types from: float32, int64, string.\nA list of Ndense Tensors (some may be empty).\ndense_defaults[j] provides default values\nwhen the example's feature_map lacks dense_key[j].  If an empty Tensor is\nprovided for dense_defaults[j], then the Feature dense_keys[j] is required.\nThe input type is inferred from dense_defaults[j], even when it's empty.\nIf dense_defaults[j] is not empty, and dense_shapes[j] is fully defined,\nthen the shape of dense_defaults[j] must match that of dense_shapes[j].\nIf dense_shapes[j] has an undefined major dimension (variable strides dense\nfeature), dense_defaults[j] must contain a single element:\nthe padding element.", "sparse_types": "A list of tf.DTypes from: tf.float32, tf.int64, tf.string.\nA list of Nsparse types; the data types of data in each Feature\ngiven in sparse_keys.\nCurrently the ParseExample supports DT_FLOAT (FloatList),\nDT_INT64 (Int64List), and DT_STRING (BytesList).", "dense_shapes": "A list of shapes (each a tf.TensorShape or list of ints).\nA list of Ndense shapes; the shapes of data in each Feature\ngiven in dense_keys.\nThe number of elements in the Feature corresponding to dense_key[j]\nmust always equal dense_shapes[j].NumEntries().\nIf dense_shapes[j] == (D0, D1, ..., DN) then the shape of output\nTensor dense_values[j] will be (|serialized|, D0, D1, ..., DN):\nThe dense outputs are just the inputs row-stacked by batch.\nThis works for dense_shapes[j] = (-1, D1, ..., DN).  In this case\nthe shape of the output Tensor dense_values[j] will be\n(|serialized|, M, D1, .., DN), where M is the maximum number of blocks\nof elements of length D1 * .... * DN, across all minibatch entries\nin the input.  Any minibatch entry with less than M blocks of elements of\nlength D1 * ... * DN will be padded with the corresponding default_value\nscalar element along the second dimension.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (sparse_indices, sparse_values, sparse_shapes, dense_values)."}, "tf.raw_ops.ParseExampleDataset": {"description": "Transforms input_dataset containing Example protos as vectors of DT_STRING into a dataset of Tensor or SparseTensor objects representing the parsed features.", "Args": {"input_dataset": "A Tensor of type variant.", "num_parallel_calls": "A Tensor of type int64.", "dense_defaults": "A list of Tensor objects with types from: float32, int64, string.\nA dict mapping string keys to Tensors.\nThe keys of the dict must match the dense_keys of the feature.", "sparse_keys": "A list of strings.\nA list of string keys in the examples features.\nThe results for these keys will be returned as SparseTensor objects.", "dense_keys": "A list of strings.\nA list of Ndense string Tensors (scalars).\nThe keys expected in the Examples features associated with dense values.", "sparse_types": "A list of tf.DTypes from: tf.float32, tf.int64, tf.string.\nA list of DTypes of the same length as sparse_keys.\nOnly tf.float32 (FloatList), tf.int64 (Int64List),\nand tf.string (BytesList) are supported.", "dense_shapes": "A list of shapes (each a tf.TensorShape or list of ints).\nList of tuples with the same length as dense_keys.\nThe shape of the data for each dense feature referenced by dense_keys.\nRequired for any input tensors identified by dense_keys.  Must be\neither fully defined, or may contain an unknown first dimension.\nAn unknown first dimension means the feature is treated as having\na variable number of blocks, and the output shape along this dimension\nis considered unknown at graph build time.  Padding is applied for\nminibatch elements smaller than the maximum number of blocks for the\ngiven feature along this dimension.", "output_types": "A list of tf.DTypes that has length >= 1.\nThe type list for the return values.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.\nThe list of shapes being produced.", "sloppy": "An optional bool. Defaults to False.", "ragged_keys": "An optional list of strings. Defaults to [].", "ragged_value_types": "An optional list of tf.DTypes from: tf.float32, tf.int64, tf.string. Defaults to [].", "ragged_split_types": "An optional list of tf.DTypes from: tf.int32, tf.int64. Defaults to [].", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ParseExampleDatasetV2": {"description": "Transforms input_dataset containing Example protos as vectors of DT_STRING into a dataset of Tensor or SparseTensor objects representing the parsed features.", "Args": {"input_dataset": "A Tensor of type variant.", "num_parallel_calls": "A Tensor of type int64.", "dense_defaults": "A list of Tensor objects with types from: float32, int64, string.\nA dict mapping string keys to Tensors.\nThe keys of the dict must match the dense_keys of the feature.", "sparse_keys": "A list of strings.\nA list of string keys in the examples features.\nThe results for these keys will be returned as SparseTensor objects.", "dense_keys": "A list of strings.\nA list of Ndense string Tensors (scalars).\nThe keys expected in the Examples features associated with dense values.", "sparse_types": "A list of tf.DTypes from: tf.float32, tf.int64, tf.string.\nA list of DTypes of the same length as sparse_keys.\nOnly tf.float32 (FloatList), tf.int64 (Int64List),\nand tf.string (BytesList) are supported.", "dense_shapes": "A list of shapes (each a tf.TensorShape or list of ints).\nList of tuples with the same length as dense_keys.\nThe shape of the data for each dense feature referenced by dense_keys.\nRequired for any input tensors identified by dense_keys.  Must be\neither fully defined, or may contain an unknown first dimension.\nAn unknown first dimension means the feature is treated as having\na variable number of blocks, and the output shape along this dimension\nis considered unknown at graph build time.  Padding is applied for\nminibatch elements smaller than the maximum number of blocks for the\ngiven feature along this dimension.", "output_types": "A list of tf.DTypes that has length >= 1.\nThe type list for the return values.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.\nThe list of shapes being produced.", "deterministic": "An optional string. Defaults to \"default\".\nA string indicating the op-level determinism to use. Deterministic controls\nwhether the dataset is allowed to return elements out of order if the next\nelement to be returned isn't available, but a later element is. Options are\n\"true\", \"false\", and \"default\". \"default\" indicates that determinism should be\ndecided by the experimental_deterministic parameter of tf.data.Options.", "ragged_keys": "An optional list of strings. Defaults to [].", "ragged_value_types": "An optional list of tf.DTypes from: tf.float32, tf.int64, tf.string. Defaults to [].", "ragged_split_types": "An optional list of tf.DTypes from: tf.int32, tf.int64. Defaults to [].", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ParseExampleV2": {"description": "Transforms a vector of tf.Example protos (as strings) into typed tensors.", "Args": {"serialized": "A Tensor of type string.\nA scalar or vector containing binary serialized Example protos.", "names": "A Tensor of type string.\nA tensor containing the names of the serialized protos.\nCorresponds 1:1 with the serialized tensor.\nMay contain, for example, table key (descriptive) names for the\ncorresponding serialized protos.  These are purely useful for debugging\npurposes, and the presence of values here has no effect on the output.\nMay also be an empty vector if no names are available.\nIf non-empty, this tensor must have the same shape as \"serialized\".", "sparse_keys": "A Tensor of type string. Vector of strings.\nThe keys expected in the Examples' features associated with sparse values.", "dense_keys": "A Tensor of type string. Vector of strings.\nThe keys expected in the Examples' features associated with dense values.", "ragged_keys": "A Tensor of type string. Vector of strings.\nThe keys expected in the Examples' features associated with ragged values.", "dense_defaults": "A list of Tensor objects with types from: float32, int64, string.\nA list of Tensors (some may be empty).  Corresponds 1:1 with dense_keys.\ndense_defaults[j] provides default values\nwhen the example's feature_map lacks dense_key[j].  If an empty Tensor is\nprovided for dense_defaults[j], then the Feature dense_keys[j] is required.\nThe input type is inferred from dense_defaults[j], even when it's empty.\nIf dense_defaults[j] is not empty, and dense_shapes[j] is fully defined,\nthen the shape of dense_defaults[j] must match that of dense_shapes[j].\nIf dense_shapes[j] has an undefined major dimension (variable strides dense\nfeature), dense_defaults[j] must contain a single element:\nthe padding element.", "num_sparse": "An int that is >= 0. The number of sparse keys.", "sparse_types": "A list of tf.DTypes from: tf.float32, tf.int64, tf.string.\nA list of num_sparse types; the data types of data in each Feature\ngiven in sparse_keys.\nCurrently the ParseExample supports DT_FLOAT (FloatList),\nDT_INT64 (Int64List), and DT_STRING (BytesList).", "ragged_value_types": "A list of tf.DTypes from: tf.float32, tf.int64, tf.string.\nA list of num_ragged types; the data types of data in each Feature\ngiven in ragged_keys (where num_ragged = sparse_keys.size()).\nCurrently the ParseExample supports DT_FLOAT (FloatList),\nDT_INT64 (Int64List), and DT_STRING (BytesList).", "ragged_split_types": "A list of tf.DTypes from: tf.int32, tf.int64.\nA list of num_ragged types; the data types of row_splits in each Feature\ngiven in ragged_keys (where num_ragged = sparse_keys.size()).\nMay be DT_INT32 or DT_INT64.", "dense_shapes": "A list of shapes (each a tf.TensorShape or list of ints).\nA list of num_dense shapes; the shapes of data in each Feature\ngiven in dense_keys (where num_dense = dense_keys.size()).\nThe number of elements in the Feature corresponding to dense_key[j]\nmust always equal dense_shapes[j].NumEntries().\nIf dense_shapes[j] == (D0, D1, ..., DN) then the shape of output\nTensor dense_values[j] will be (|serialized|, D0, D1, ..., DN):\nThe dense outputs are just the inputs row-stacked by batch.\nThis works for dense_shapes[j] = (-1, D1, ..., DN).  In this case\nthe shape of the output Tensor dense_values[j] will be\n(|serialized|, M, D1, .., DN), where M is the maximum number of blocks\nof elements of length D1 * .... * DN, across all minibatch entries\nin the input.  Any minibatch entry with less than M blocks of elements of\nlength D1 * ... * DN will be padded with the corresponding default_value\nscalar element along the second dimension.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (sparse_indices, sparse_values, sparse_shapes, dense_values, ragged_values, ragged_row_splits)."}, "tf.raw_ops.ParseSequenceExample": {"description": "Transforms a vector of brain.SequenceExample protos (as strings) into typed tensors.", "Args": {"serialized": "A Tensor of type string.\nA vector containing binary serialized SequenceExample protos.", "debug_name": "A Tensor of type string.\nA vector containing the names of the serialized protos.\nMay contain, for example, table key (descriptive) name for the\ncorresponding serialized proto.  This is purely useful for debugging\npurposes, and the presence of values here has no effect on the output.\nMay also be an empty vector if no name is available.", "context_dense_defaults": "A list of Tensor objects with types from: float32, int64, string.\nA list of Ncontext_dense Tensors (some may be empty).\ncontext_dense_defaults[j] provides default values\nwhen the SequenceExample's context map lacks context_dense_key[j].\nIf an empty Tensor is provided for context_dense_defaults[j],\nthen the Feature context_dense_keys[j] is required.\nThe input type is inferred from context_dense_defaults[j], even when it's\nempty.  If context_dense_defaults[j] is not empty, its shape must match\ncontext_dense_shapes[j].", "feature_list_dense_missing_assumed_empty": "A list of strings.\nA vector listing the\nFeatureList keys which may be missing from the SequenceExamples.  If the\nassociated FeatureList is missing, it is treated as empty.  By default,\nany FeatureList not listed in this vector must exist in the SequenceExamples.", "context_sparse_keys": "A list of strings.\nA list of Ncontext_sparse string Tensors (scalars).\nThe keys expected in the Examples' features associated with context_sparse\nvalues.", "context_dense_keys": "A list of strings.\nA list of Ncontext_dense string Tensors (scalars).\nThe keys expected in the SequenceExamples' context features associated with\ndense values.", "feature_list_sparse_keys": "A list of strings.\nA list of Nfeature_list_sparse string Tensors\n(scalars).  The keys expected in the FeatureLists associated with sparse\nvalues.", "feature_list_dense_keys": "A list of strings.\nA list of Nfeature_list_dense string Tensors (scalars).\nThe keys expected in the SequenceExamples' feature_lists associated\nwith lists of dense values.", "Ncontext_sparse": "An optional int that is >= 0. Defaults to 0.", "Ncontext_dense": "An optional int that is >= 0. Defaults to 0.", "Nfeature_list_sparse": "An optional int that is >= 0. Defaults to 0.", "Nfeature_list_dense": "An optional int that is >= 0. Defaults to 0.", "context_sparse_types": "An optional list of tf.DTypes from: tf.float32, tf.int64, tf.string. Defaults to [].\nA list of Ncontext_sparse types; the data types of data in\neach context Feature given in context_sparse_keys.\nCurrently the ParseSingleSequenceExample supports DT_FLOAT (FloatList),\nDT_INT64 (Int64List), and DT_STRING (BytesList).", "feature_list_dense_types": "An optional list of tf.DTypes from: tf.float32, tf.int64, tf.string. Defaults to [].", "context_dense_shapes": "An optional list of shapes (each a tf.TensorShape or list of ints). Defaults to [].\nA list of Ncontext_dense shapes; the shapes of data in\neach context Feature given in context_dense_keys.\nThe number of elements in the Feature corresponding to context_dense_key[j]\nmust always equal context_dense_shapes[j].NumEntries().\nThe shape of context_dense_values[j] will match context_dense_shapes[j].", "feature_list_sparse_types": "An optional list of tf.DTypes from: tf.float32, tf.int64, tf.string. Defaults to [].\nA list of Nfeature_list_sparse types; the data types\nof data in each FeatureList given in feature_list_sparse_keys.\nCurrently the ParseSingleSequenceExample supports DT_FLOAT (FloatList),\nDT_INT64 (Int64List), and DT_STRING (BytesList).", "feature_list_dense_shapes": "An optional list of shapes (each a tf.TensorShape or list of ints). Defaults to [].\nA list of Nfeature_list_dense shapes; the shapes of\ndata in each FeatureList given in feature_list_dense_keys.\nThe shape of each Feature in the FeatureList corresponding to\nfeature_list_dense_key[j] must always equal\nfeature_list_dense_shapes[j].NumEntries().", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (context_sparse_indices, context_sparse_values, context_sparse_shapes, context_dense_values, feature_list_sparse_indices, feature_list_sparse_values, feature_list_sparse_shapes, feature_list_dense_values, feature_list_dense_lengths)."}, "tf.raw_ops.ParseSequenceExampleV2": {"description": "Transforms a vector of tf.io.SequenceExample protos (as strings) into"}, "tf.raw_ops.ParseSingleExample": {"description": "Transforms a tf.Example proto (as a string) into typed tensors.", "Args": {"serialized": "A Tensor of type string.\nA vector containing a batch of binary serialized Example protos.", "dense_defaults": "A list of Tensor objects with types from: float32, int64, string.\nA list of Tensors (some may be empty), whose length matches\nthe length of dense_keys. dense_defaults[j] provides default values\nwhen the example's feature_map lacks dense_key[j].  If an empty Tensor is\nprovided for dense_defaults[j], then the Feature dense_keys[j] is required.\nThe input type is inferred from dense_defaults[j], even when it's empty.\nIf dense_defaults[j] is not empty, and dense_shapes[j] is fully defined,\nthen the shape of dense_defaults[j] must match that of dense_shapes[j].\nIf dense_shapes[j] has an undefined major dimension (variable strides dense\nfeature), dense_defaults[j] must contain a single element:\nthe padding element.", "num_sparse": "An int that is >= 0.\nThe number of sparse features to be parsed from the example. This\nmust match the lengths of sparse_keys and sparse_types.", "sparse_keys": "A list of strings. A list of num_sparse strings.\nThe keys expected in the Examples' features associated with sparse values.", "dense_keys": "A list of strings.\nThe keys expected in the Examples' features associated with dense\nvalues.", "sparse_types": "A list of tf.DTypes from: tf.float32, tf.int64, tf.string.\nA list of num_sparse types; the data types of data in each\nFeature given in sparse_keys.\nCurrently the ParseSingleExample op supports DT_FLOAT (FloatList),\nDT_INT64 (Int64List), and DT_STRING (BytesList).", "dense_shapes": "A list of shapes (each a tf.TensorShape or list of ints).\nThe shapes of data in each Feature given in dense_keys.\nThe length of this list must match the length of dense_keys.  The\nnumber of elements in the Feature corresponding to dense_key[j] must\nalways equal dense_shapes[j].NumEntries().  If dense_shapes[j] ==\n(D0, D1, ..., DN) then the shape of output Tensor dense_values[j]\nwill be (D0, D1, ..., DN): In the case dense_shapes[j] = (-1, D1,\n..., DN), the shape of the output Tensor dense_values[j] will be (M,\nD1, .., DN), where M is the number of blocks of elements of length\nD1 * .... * DN, in the input.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (sparse_indices, sparse_values, sparse_shapes, dense_values)."}, "tf.raw_ops.ParseSingleSequenceExample": {"description": "Transforms a scalar brain.SequenceExample proto (as strings) into typed tensors.", "Args": {"serialized": "A Tensor of type string.\nA scalar containing a binary serialized SequenceExample proto.", "feature_list_dense_missing_assumed_empty": "A Tensor of type string.\nA vector listing the\nFeatureList keys which may be missing from the SequenceExample.  If the\nassociated FeatureList is missing, it is treated as empty.  By default,\nany FeatureList not listed in this vector must exist in the SequenceExample.", "context_sparse_keys": "A list of Tensor objects with type string.\nA list of Ncontext_sparse string Tensors (scalars).\nThe keys expected in the Examples' features associated with context_sparse\nvalues.", "context_dense_keys": "A list of Tensor objects with type string.\nA list of Ncontext_dense string Tensors (scalars).\nThe keys expected in the SequenceExamples' context features associated with\ndense values.", "feature_list_sparse_keys": "A list of Tensor objects with type string.\nA list of Nfeature_list_sparse string Tensors\n(scalars).  The keys expected in the FeatureLists associated with sparse\nvalues.", "feature_list_dense_keys": "A list of Tensor objects with type string.\nA list of Nfeature_list_dense string Tensors (scalars).\nThe keys expected in the SequenceExamples' feature_lists associated\nwith lists of dense values.", "context_dense_defaults": "A list of Tensor objects with types from: float32, int64, string.\nA list of Ncontext_dense Tensors (some may be empty).\ncontext_dense_defaults[j] provides default values\nwhen the SequenceExample's context map lacks context_dense_key[j].\nIf an empty Tensor is provided for context_dense_defaults[j],\nthen the Feature context_dense_keys[j] is required.\nThe input type is inferred from context_dense_defaults[j], even when it's\nempty.  If context_dense_defaults[j] is not empty, its shape must match\ncontext_dense_shapes[j].", "debug_name": "A Tensor of type string.\nA scalar containing the name of the serialized proto.\nMay contain, for example, table key (descriptive) name for the\ncorresponding serialized proto.  This is purely useful for debugging\npurposes, and the presence of values here has no effect on the output.\nMay also be an empty scalar if no name is available.", "context_sparse_types": "An optional list of tf.DTypes from: tf.float32, tf.int64, tf.string. Defaults to [].\nA list of Ncontext_sparse types; the data types of data in\neach context Feature given in context_sparse_keys.\nCurrently the ParseSingleSequenceExample supports DT_FLOAT (FloatList),\nDT_INT64 (Int64List), and DT_STRING (BytesList).", "feature_list_dense_types": "An optional list of tf.DTypes from: tf.float32, tf.int64, tf.string. Defaults to [].", "context_dense_shapes": "An optional list of shapes (each a tf.TensorShape or list of ints). Defaults to [].\nA list of Ncontext_dense shapes; the shapes of data in\neach context Feature given in context_dense_keys.\nThe number of elements in the Feature corresponding to context_dense_key[j]\nmust always equal context_dense_shapes[j].NumEntries().\nThe shape of context_dense_values[j] will match context_dense_shapes[j].", "feature_list_sparse_types": "An optional list of tf.DTypes from: tf.float32, tf.int64, tf.string. Defaults to [].\nA list of Nfeature_list_sparse types; the data types\nof data in each FeatureList given in feature_list_sparse_keys.\nCurrently the ParseSingleSequenceExample supports DT_FLOAT (FloatList),\nDT_INT64 (Int64List), and DT_STRING (BytesList).", "feature_list_dense_shapes": "An optional list of shapes (each a tf.TensorShape or list of ints). Defaults to [].\nA list of Nfeature_list_dense shapes; the shapes of\ndata in each FeatureList given in feature_list_dense_keys.\nThe shape of each Feature in the FeatureList corresponding to\nfeature_list_dense_key[j] must always equal\nfeature_list_dense_shapes[j].NumEntries().", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (context_sparse_indices, context_sparse_values, context_sparse_shapes, context_dense_values, feature_list_sparse_indices, feature_list_sparse_values, feature_list_sparse_shapes, feature_list_dense_values)."}, "tf.raw_ops.ParseTensor": {"description": "Transforms a serialized tensorflow.TensorProto proto into a Tensor.", "Args": {"serialized": "A Tensor of type string.\nA scalar string containing a serialized TensorProto proto.", "out_type": "A tf.DType.\nThe type of the serialized tensor.  The provided type must match the\ntype of the serialized tensor and no implicit conversion will take place.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type out_type."}, "tf.raw_ops.PartitionedCall": {"description": "returns f(inputs), where f&#39;s body is placed and partitioned.", "Args": {"args": "A list of Tensor objects. A list of input tensors.", "Tout": "A list of tf.DTypes. A list of output types.", "f": "A function decorated with @Defun.\nA function that takes 'args', a list of tensors, and returns 'output',\nanother list of tensors. Input and output types are specified by 'Tin'\nand 'Tout'. The function body of f will be placed and partitioned across\ndevices, setting this op apart from the regular Call op.", "config": "An optional string. Defaults to \"\".", "config_proto": "An optional string. Defaults to \"\".", "executor_type": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A list of Tensor objects of type Tout."}, "tf.raw_ops.Placeholder": {"description": "A placeholder op for a value that will be fed into the computation.", "Args": {"dtype": "A tf.DType. The type of elements in the tensor.", "shape": "An optional tf.TensorShape or list of ints. Defaults to None.\n(Optional) The shape of the tensor. If the shape has 0 dimensions, the\nshape is unconstrained.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dtype."}, "tf.raw_ops.PlaceholderV2": {"description": "A placeholder op for a value that will be fed into the computation.", "Args": {"dtype": "A tf.DType. The type of elements in the tensor.", "shape": "A tf.TensorShape or list of ints.\nThe shape of the tensor. The shape can be any partially-specified\nshape.  To be unconstrained, pass in a shape with unknown rank.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dtype."}, "tf.raw_ops.PlaceholderWithDefault": {"description": "A placeholder op that passes through input when its output is not fed.", "Args": {"input": "A Tensor. The default value to produce when output is not fed.", "shape": "A tf.TensorShape or list of ints.\nThe (possibly partial) shape of the tensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.Polygamma": {"description": "Compute the polygamma function \\\\(\\psi^{(n)}(x)\\\\).", "Args": {"a": "A Tensor. Must be one of the following types: float32, float64.", "x": "A Tensor. Must have the same type as a.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as a."}, "tf.raw_ops.PopulationCount": {"description": "Computes element-wise population count (a.k.a. popcount, bitsum, bitcount).", "Args": {"x": "A Tensor. Must be one of the following types: int8, int16, int32, int64, uint8, uint16, uint32, uint64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type uint8."}, "tf.raw_ops.Pow": {"description": "Computes the power of one value to another.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, float32, half, float64, int8, int16, int32, int64, complex64, complex128.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.PrefetchDataset": {"description": "Creates a dataset that asynchronously prefetches elements from input_dataset.", "Args": {"input_dataset": "A Tensor of type variant.", "buffer_size": "A Tensor of type int64.\nThe maximum number of elements to buffer in an iterator over\nthis dataset.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "slack_period": "An optional int. Defaults to 0.", "legacy_autotune": "An optional bool. Defaults to True.", "buffer_size_min": "An optional int. Defaults to 0.", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.Prelinearize": {"description": "An op which linearizes one Tensor value to an opaque variant tensor.", "Args": {"input": "A Tensor. A tensor that will be linearized.", "shape": "An optional tf.TensorShape or list of ints. Defaults to [].\nThe shape of the tensor.", "layout": "An optional list of ints. Defaults to [].\nA vector holding the requested layout in minor-to-major sequence. If a layout\nattribute is passed but its values are all -1 the layout will be computed by\nthe infeed operation.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.PrelinearizeTuple": {"description": "An op which linearizes multiple Tensor values to an opaque variant tensor.", "Args": {"inputs": "A list of Tensor objects.\nA list of tensors that will be provided using the infeed mechanism.", "shapes": "A list of shapes (each a tf.TensorShape or list of ints).\nThe shapes of each tensor in inputs.", "layouts": "An optional list of ints. Defaults to [].\nA vector holding the requested layout in minor-to-major sequence for all the\ntuple shapes in the order the shapes appear in the \"shapes\" input. The layout\nelements for a sub-shape can be set to -1 in which case the corresponding layout\nwill be computed by the infeed operation.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.PreventGradient": {"description": "An identity op that triggers an error if a gradient is requested.", "Args": {"input": "A Tensor. any tensor.", "message": "An optional string. Defaults to \"\".\nWill be printed in the error when anyone tries to differentiate\nthis operation.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.Print": {"description": "Prints a list of tensors.", "Args": {"input": "A Tensor. The tensor passed to output", "data": "A list of Tensor objects.\nA list of tensors to print out when op is evaluated.", "message": "An optional string. Defaults to \"\".\nA string, prefix of the error message.", "first_n": "An optional int. Defaults to -1.\nOnly log first_n number of times. -1 disables logging.", "summarize": "An optional int. Defaults to 3.\nOnly print this many entries of each tensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.PrintV2": {"description": "Prints a string scalar.", "Args": {"input": "A Tensor of type string. The string scalar to print.", "output_stream": "An optional string. Defaults to \"stderr\".\nA string specifying the output stream or logging level to print to.", "end": "An optional string. Defaults to \"\\n\".", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.PriorityQueue": {"description": "A queue that produces elements sorted by the first component value.", "Args": {"shapes": "A list of shapes (each a tf.TensorShape or list of ints).\nThe shape of each component in a value. The length of this attr must\nbe either 0 or the same as the length of component_types. If the length of\nthis attr is 0, the shapes of queue elements are not constrained, and\nonly one element may be dequeued at a time.", "component_types": "An optional list of tf.DTypes. Defaults to [].\nThe type of each component in a value.", "capacity": "An optional int. Defaults to -1.\nThe upper bound on the number of elements in this queue.\nNegative numbers mean no limit.", "container": "An optional string. Defaults to \"\".\nIf non-empty, this queue is placed in the given container.\nOtherwise, a default container is used.", "shared_name": "An optional string. Defaults to \"\".\nIf non-empty, this queue will be shared under the given name\nacross multiple sessions.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type mutable string."}, "tf.raw_ops.PriorityQueueV2": {"description": "A queue that produces elements sorted by the first component value.", "Args": {"shapes": "A list of shapes (each a tf.TensorShape or list of ints).\nThe shape of each component in a value. The length of this attr must\nbe either 0 or the same as the length of component_types. If the length of\nthis attr is 0, the shapes of queue elements are not constrained, and\nonly one element may be dequeued at a time.", "component_types": "An optional list of tf.DTypes. Defaults to [].\nThe type of each component in a value.", "capacity": "An optional int. Defaults to -1.\nThe upper bound on the number of elements in this queue.\nNegative numbers mean no limit.", "container": "An optional string. Defaults to \"\".\nIf non-empty, this queue is placed in the given container.\nOtherwise, a default container is used.", "shared_name": "An optional string. Defaults to \"\".\nIf non-empty, this queue will be shared under the given name\nacross multiple sessions.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type resource."}, "tf.raw_ops.PrivateThreadPoolDataset": {"description": "Creates a dataset that uses a custom thread pool to compute input_dataset.", "Args": {"input_dataset": "A Tensor of type variant.", "num_threads": "A Tensor of type int64.\nIdentifies the number of threads to use for the private threadpool.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.Prod": {"description": "Computes the product of elements across dimensions of a tensor.", "Args": {"input": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nThe tensor to reduce.", "axis": "A Tensor. Must be one of the following types: int32, int64.\nThe dimensions to reduce. Must be in the range\n[-rank(input), rank(input)).", "keep_dims": "An optional bool. Defaults to False.\nIf true, retain reduced dimensions with length 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.PyFunc": {"description": "Invokes a python function to compute func(input)-&gt;output.", "Args": {"input": "A list of Tensor objects.\nList of Tensors that will provide input to the Op.", "token": "A string.\nA token representing a registered python function in this address space.", "Tout": "A list of tf.DTypes. Data types of the outputs from the op.\nThe length of the list specifies the number of outputs.", "name": "A name for the operation (optional)."}, "Returns": "A list of Tensor objects of type Tout."}, "tf.raw_ops.PyFuncStateless": {"description": "A stateless version of PyFunc.", "Args": {"input": "A list of Tensor objects.", "token": "A string.", "Tout": "A list of tf.DTypes.", "name": "A name for the operation (optional)."}, "Returns": "A list of Tensor objects of type Tout."}, "tf.raw_ops.Qr": {"description": "Computes the QR decompositions of one or more matrices.", "Args": {"input": "A Tensor. Must be one of the following types: float64, float32, half, complex64, complex128.\nA tensor of shape [..., M, N] whose inner-most 2 dimensions\nform matrices of size [M, N]. Let P be the minimum of M and N.", "full_matrices": "An optional bool. Defaults to False.\nIf true, compute full-sized q and r. If false\n(the default), compute only the leading P columns of q.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (q, r)."}, "tf.raw_ops.QuantizeAndDequantize": {"description": "Use QuantizeAndDequantizeV2 instead.", "Args": {"input": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "signed_input": "An optional bool. Defaults to True.", "num_bits": "An optional int. Defaults to 8.", "range_given": "An optional bool. Defaults to False.", "input_min": "An optional float. Defaults to 0.", "input_max": "An optional float. Defaults to 0.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.QuantizeAndDequantizeV2": {"description": "Quantizes then dequantizes a tensor.", "Args": {"input": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.\nTensor to quantize and then dequantize.", "input_min": "A Tensor. Must have the same type as input.\nIf range_given == True, this specifies the minimum input value that needs to\nbe represented, otherwise it is determined from the min value of the input\ntensor.", "input_max": "A Tensor. Must have the same type as input.\nIf range_given == True, this specifies the maximum input value that needs to\nbe represented, otherwise it is determined from the max value of the input\ntensor.", "signed_input": "An optional bool. Defaults to True.\nWhether the quantization is signed or unsigned. (actually this parameter should\nhave been called signed_output)", "num_bits": "An optional int. Defaults to 8.\nThe bitwidth of the quantization.", "range_given": "An optional bool. Defaults to False.\nWhether the range is given or should be determined from the input tensor.", "round_mode": "An optional string from: \"HALF_TO_EVEN\", \"HALF_UP\". Defaults to \"HALF_TO_EVEN\".\nThe 'round_mode' attribute controls which rounding tie-breaking algorithm is\nused when rounding float values to their quantized equivalents. The following\nrounding modes are currently supported:\n\nHALF_TO_EVEN: this is the default round_mode.\nHALF_UP: round towards positive. In this mode 7.5 rounds up to 8 and -7.5\nrounds up to -7.", "narrow_range": "An optional bool. Defaults to False.\nIf True, then the absolute value of the quantized minimum value is the same as\nthe quantized maximum value, instead of 1 greater.\ni.e. for 8 bit quantization, the minimum value is -127 instead of -128.", "axis": "An optional int. Defaults to -1.\nIf specified, this axis is treated as a channel or slice axis, and a separate\nquantization range is used for each channel or slice along this axis.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.QuantizeAndDequantizeV3": {"description": "Quantizes then dequantizes a tensor.", "Args": {"input": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "input_min": "A Tensor. Must have the same type as input.", "input_max": "A Tensor. Must have the same type as input.", "num_bits": "A Tensor of type int32.", "signed_input": "An optional bool. Defaults to True.", "range_given": "An optional bool. Defaults to True.", "narrow_range": "An optional bool. Defaults to False.", "axis": "An optional int. Defaults to -1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.QuantizeAndDequantizeV4": {"description": "Quantizes then dequantizes a tensor.", "Args": {"input": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.\nTensor to quantize and then dequantize.", "input_min": "A Tensor. Must have the same type as input.\nIf range_given == True, this specifies the minimum input value that needs to\nbe represented, otherwise it is determined from the min value of the input\ntensor.", "input_max": "A Tensor. Must have the same type as input.\nIf range_given == True, this specifies the maximum input value that needs to\nbe represented, otherwise it is determined from the max value of the input\ntensor.", "signed_input": "An optional bool. Defaults to True.\nWhether the quantization is signed or unsigned. (actually this parameter should\nhave been called signed_output)", "num_bits": "An optional int. Defaults to 8.\nThe bitwidth of the quantization.", "range_given": "An optional bool. Defaults to False.\nWhether the range is given or should be determined from the input tensor.", "round_mode": "An optional string from: \"HALF_TO_EVEN\", \"HALF_UP\". Defaults to \"HALF_TO_EVEN\".\nThe 'round_mode' attribute controls which rounding tie-breaking algorithm is\nused when rounding float values to their quantized equivalents. The following\nrounding modes are currently supported:\n\nHALF_TO_EVEN: this is the default round_mode.\nHALF_UP: round towards positive. In this mode 7.5 rounds up to 8 and -7.5\nrounds up to -7.", "narrow_range": "An optional bool. Defaults to False.\nIf True, then the absolute value of the quantized minimum value is the same as\nthe quantized maximum value, instead of 1 greater.\ni.e. for 8 bit quantization, the minimum value is -127 instead of -128.", "axis": "An optional int. Defaults to -1.\nIf specified, this axis is treated as a channel or slice axis, and a separate\nquantization range is used for each channel or slice along this axis.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.QuantizeAndDequantizeV4Grad": {"description": "Returns the gradient of QuantizeAndDequantizeV4.", "Args": {"gradients": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "input": "A Tensor. Must have the same type as gradients.", "input_min": "A Tensor. Must have the same type as gradients.", "input_max": "A Tensor. Must have the same type as gradients.", "axis": "An optional int. Defaults to -1.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (input_backprop, input_min_backprop, input_max_backprop)."}, "tf.raw_ops.QuantizeDownAndShrinkRange": {"description": "Convert the quantized &#39;input&#39; tensor into a lower-precision &#39;output&#39;, using the", "Args": {"input": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.", "input_min": "A Tensor of type float32.\nThe float value that the minimum quantized input value represents.", "input_max": "A Tensor of type float32.\nThe float value that the maximum quantized input value represents.", "out_type": "A tf.DType from: tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16.\nThe type of the output. Should be a lower bit depth than Tinput.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output, output_min, output_max)."}, "tf.raw_ops.QuantizeV2": {"description": "Quantize the &#39;input&#39; tensor of type float to &#39;output&#39; tensor of type &#39;T&#39;.", "Args": {"input": "A Tensor of type float32.", "min_range": "A Tensor of type float32.\nThe minimum value of the quantization range. This value may be adjusted by the\nop depending on other parameters. The adjusted value is written to output_min.\nIf the axis attribute is specified, this must be a 1-D tensor whose size\nmatches the axis dimension of the input and output tensors.", "max_range": "A Tensor of type float32.\nThe maximum value of the quantization range. This value may be adjusted by the\nop depending on other parameters. The adjusted value is written to output_max.\nIf the axis attribute is specified, this must be a 1-D tensor whose size\nmatches the axis dimension of the input and output tensors.", "T": "A tf.DType from: tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16.", "mode": "An optional string from: \"MIN_COMBINED\", \"MIN_FIRST\", \"SCALED\". Defaults to \"MIN_COMBINED\".", "round_mode": "An optional string from: \"HALF_AWAY_FROM_ZERO\", \"HALF_TO_EVEN\". Defaults to \"HALF_AWAY_FROM_ZERO\".", "narrow_range": "An optional bool. Defaults to False.", "axis": "An optional int. Defaults to -1.", "ensure_minimum_range": "An optional float. Defaults to 0.01.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output, output_min, output_max)."}, "tf.raw_ops.QuantizedAdd": {"description": "Returns x &#43; y element-wise, working on quantized buffers.", "Args": {"x": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.", "y": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.", "min_x": "A Tensor of type float32.\nThe float value that the lowest quantized x value represents.", "max_x": "A Tensor of type float32.\nThe float value that the highest quantized x value represents.", "min_y": "A Tensor of type float32.\nThe float value that the lowest quantized y value represents.", "max_y": "A Tensor of type float32.\nThe float value that the highest quantized y value represents.", "Toutput": "An optional tf.DType from: tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16. Defaults to tf.qint32.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (z, min_z, max_z)."}, "tf.raw_ops.QuantizedAvgPool": {"description": "Produces the average pool of the input tensor for quantized types.", "Args": {"input": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.\n4-D with shape [batch, height, width, channels].", "min_input": "A Tensor of type float32.\nThe float value that the lowest quantized input value represents.", "max_input": "A Tensor of type float32.\nThe float value that the highest quantized input value represents.", "ksize": "A list of ints.\nThe size of the window for each dimension of the input tensor.\nThe length must be 4 to match the number of dimensions of the input.", "strides": "A list of ints.\nThe stride of the sliding window for each dimension of the input\ntensor.  The length must be 4 to match the number of dimensions of the input.", "padding": "A string from: \"SAME\", \"VALID\".\nThe type of padding algorithm to use.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output, min_output, max_output)."}, "tf.raw_ops.QuantizedBatchNormWithGlobalNormalization": {"description": "Quantized Batch normalization.", "Args": {"t": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.\nA 4D input Tensor.", "t_min": "A Tensor of type float32.\nThe value represented by the lowest quantized input.", "t_max": "A Tensor of type float32.\nThe value represented by the highest quantized input.", "m": "A Tensor. Must have the same type as t.\nA 1D mean Tensor with size matching the last dimension of t.\nThis is the first output from tf.nn.moments,\nor a saved moving average thereof.", "m_min": "A Tensor of type float32.\nThe value represented by the lowest quantized mean.", "m_max": "A Tensor of type float32.\nThe value represented by the highest quantized mean.", "v": "A Tensor. Must have the same type as t.\nA 1D variance Tensor with size matching the last dimension of t.\nThis is the second output from tf.nn.moments,\nor a saved moving average thereof.", "v_min": "A Tensor of type float32.\nThe value represented by the lowest quantized variance.", "v_max": "A Tensor of type float32.\nThe value represented by the highest quantized variance.", "beta": "A Tensor. Must have the same type as t.\nA 1D beta Tensor with size matching the last dimension of t.\nAn offset to be added to the normalized tensor.", "beta_min": "A Tensor of type float32.\nThe value represented by the lowest quantized offset.", "beta_max": "A Tensor of type float32.\nThe value represented by the highest quantized offset.", "gamma": "A Tensor. Must have the same type as t.\nA 1D gamma Tensor with size matching the last dimension of t.\nIf \"scale_after_normalization\" is true, this tensor will be multiplied\nwith the normalized tensor.", "gamma_min": "A Tensor of type float32.\nThe value represented by the lowest quantized gamma.", "gamma_max": "A Tensor of type float32.\nThe value represented by the highest quantized gamma.", "out_type": "A tf.DType from: tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16.", "variance_epsilon": "A float. A small float number to avoid dividing by 0.", "scale_after_normalization": "A bool.\nA bool indicating whether the resulted tensor\nneeds to be multiplied with gamma.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (result, result_min, result_max)."}, "tf.raw_ops.QuantizedBiasAdd": {"description": "Adds Tensor &#39;bias&#39; to Tensor &#39;input&#39; for Quantized types.", "Args": {"input": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.", "bias": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.\nA 1D bias Tensor with size matching the last dimension of 'input'.", "min_input": "A Tensor of type float32.\nThe float value that the lowest quantized input value represents.", "max_input": "A Tensor of type float32.\nThe float value that the highest quantized input value represents.", "min_bias": "A Tensor of type float32.\nThe float value that the lowest quantized bias value represents.", "max_bias": "A Tensor of type float32.\nThe float value that the highest quantized bias value represents.", "out_type": "A tf.DType from: tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output, min_out, max_out)."}, "tf.raw_ops.QuantizedConcat": {"description": "Concatenates quantized tensors along one dimension.", "Args": {"concat_dim": "A Tensor of type int32.\n0-D.  The dimension along which to concatenate.  Must be in the\nrange [0, rank(values)).", "values": "A list of at least 2 Tensor objects with the same type.\nThe N Tensors to concatenate. Their ranks and types must match,\nand their sizes must match in all dimensions except concat_dim.", "input_mins": "A list with the same length as values of Tensor objects with type float32.\nThe minimum scalar values for each of the input tensors.", "input_maxes": "A list with the same length as values of Tensor objects with type float32.\nThe maximum scalar values for each of the input tensors.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output, output_min, output_max)."}, "tf.raw_ops.QuantizedConv2D": {"description": "Computes a 2D convolution given quantized 4D input and filter tensors.", "Args": {"input": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.", "filter": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.\nfilter's input_depth dimension must match input's depth dimensions.", "min_input": "A Tensor of type float32.\nThe float value that the lowest quantized input value represents.", "max_input": "A Tensor of type float32.\nThe float value that the highest quantized input value represents.", "min_filter": "A Tensor of type float32.\nThe float value that the lowest quantized filter value represents.", "max_filter": "A Tensor of type float32.\nThe float value that the highest quantized filter value represents.", "strides": "A list of ints.\nThe stride of the sliding window for each dimension of the input\ntensor.", "padding": "A string from: \"SAME\", \"VALID\".\nThe type of padding algorithm to use.", "out_type": "An optional tf.DType from: tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16. Defaults to tf.qint32.", "dilations": "An optional list of ints. Defaults to [1, 1, 1, 1].\n1-D tensor of length 4.  The dilation factor for each dimension of\ninput. If set to k > 1, there will be k-1 skipped cells between each\nfilter element on that dimension. The dimension order is determined by the\nvalue of data_format, see above for details. Dilations in the batch and\ndepth dimensions must be 1.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output, min_output, max_output)."}, "tf.raw_ops.QuantizedConv2DAndRelu": {"Args": {"input": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.", "filter": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.", "min_input": "A Tensor of type float32.", "max_input": "A Tensor of type float32.", "min_filter": "A Tensor of type float32.", "max_filter": "A Tensor of type float32.", "strides": "A list of ints.", "padding": "A string from: \"SAME\", \"VALID\".", "out_type": "An optional tf.DType from: tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16. Defaults to tf.qint32.", "dilations": "An optional list of ints. Defaults to [1, 1, 1, 1].", "padding_list": "An optional list of ints. Defaults to [].", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output, min_output, max_output)."}, "tf.raw_ops.QuantizedConv2DAndReluAndRequantize": {"Args": {"input": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.", "filter": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.", "min_input": "A Tensor of type float32.", "max_input": "A Tensor of type float32.", "min_filter": "A Tensor of type float32.", "max_filter": "A Tensor of type float32.", "min_freezed_output": "A Tensor of type float32.", "max_freezed_output": "A Tensor of type float32.", "strides": "A list of ints.", "padding": "A string from: \"SAME\", \"VALID\".", "out_type": "An optional tf.DType from: tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16. Defaults to tf.quint8.", "dilations": "An optional list of ints. Defaults to [1, 1, 1, 1].", "padding_list": "An optional list of ints. Defaults to [].", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output, min_output, max_output)."}, "tf.raw_ops.QuantizedConv2DAndRequantize": {"Args": {"input": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.", "filter": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.", "min_input": "A Tensor of type float32.", "max_input": "A Tensor of type float32.", "min_filter": "A Tensor of type float32.", "max_filter": "A Tensor of type float32.", "min_freezed_output": "A Tensor of type float32.", "max_freezed_output": "A Tensor of type float32.", "strides": "A list of ints.", "padding": "A string from: \"SAME\", \"VALID\".", "out_type": "An optional tf.DType from: tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16. Defaults to tf.qint8.", "dilations": "An optional list of ints. Defaults to [1, 1, 1, 1].", "padding_list": "An optional list of ints. Defaults to [].", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output, min_output, max_output)."}, "tf.raw_ops.QuantizedConv2DPerChannel": {"description": "Computes QuantizedConv2D per channel.", "Args": {"input": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.\nThe original input tensor.", "filter": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.\nThe original filter tensor.", "min_input": "A Tensor of type float32.\nThe minimum value of the input tensor", "max_input": "A Tensor of type float32.\nThe maximum value of the input tensor.", "min_filter": "A Tensor of type float32.\nThe minimum value of the filter tensor.", "max_filter": "A Tensor of type float32.\nThe maximum value of the filter tensor.", "strides": "A list of ints. list of stride values.", "padding": "A string from: \"SAME\", \"VALID\".", "out_type": "An optional tf.DType from: tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16. Defaults to tf.qint32.\nThe quantized type of output tensor that needs to be converted.", "dilations": "An optional list of ints. Defaults to [1, 1, 1, 1].\nlist of dilation values.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output, min_output, max_output)."}, "tf.raw_ops.QuantizedConv2DWithBias": {"Args": {"input": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.", "filter": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.", "bias": "A Tensor of type float32.", "min_input": "A Tensor of type float32.", "max_input": "A Tensor of type float32.", "min_filter": "A Tensor of type float32.", "max_filter": "A Tensor of type float32.", "strides": "A list of ints.", "padding": "A string from: \"SAME\", \"VALID\".", "out_type": "An optional tf.DType from: tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16. Defaults to tf.qint32.", "dilations": "An optional list of ints. Defaults to [1, 1, 1, 1].", "padding_list": "An optional list of ints. Defaults to [].", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output, min_output, max_output)."}, "tf.raw_ops.QuantizedConv2DWithBiasAndRelu": {"Args": {"input": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.", "filter": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.", "bias": "A Tensor of type float32.", "min_input": "A Tensor of type float32.", "max_input": "A Tensor of type float32.", "min_filter": "A Tensor of type float32.", "max_filter": "A Tensor of type float32.", "strides": "A list of ints.", "padding": "A string from: \"SAME\", \"VALID\".", "out_type": "An optional tf.DType from: tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16. Defaults to tf.qint32.", "dilations": "An optional list of ints. Defaults to [1, 1, 1, 1].", "padding_list": "An optional list of ints. Defaults to [].", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output, min_output, max_output)."}, "tf.raw_ops.QuantizedConv2DWithBiasAndReluAndRequantize": {"Args": {"input": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.", "filter": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.", "bias": "A Tensor. Must be one of the following types: float32, qint32.", "min_input": "A Tensor of type float32.", "max_input": "A Tensor of type float32.", "min_filter": "A Tensor of type float32.", "max_filter": "A Tensor of type float32.", "min_freezed_output": "A Tensor of type float32.", "max_freezed_output": "A Tensor of type float32.", "strides": "A list of ints.", "padding": "A string from: \"SAME\", \"VALID\".", "out_type": "An optional tf.DType from: tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16. Defaults to tf.quint8.", "dilations": "An optional list of ints. Defaults to [1, 1, 1, 1].", "padding_list": "An optional list of ints. Defaults to [].", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output, min_output, max_output)."}, "tf.raw_ops.QuantizedConv2DWithBiasAndRequantize": {"Args": {"input": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.", "filter": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.", "bias": "A Tensor. Must be one of the following types: float32, qint32.", "min_input": "A Tensor of type float32.", "max_input": "A Tensor of type float32.", "min_filter": "A Tensor of type float32.", "max_filter": "A Tensor of type float32.", "min_freezed_output": "A Tensor of type float32.", "max_freezed_output": "A Tensor of type float32.", "strides": "A list of ints.", "padding": "A string from: \"SAME\", \"VALID\".", "out_type": "An optional tf.DType from: tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16. Defaults to tf.qint8.", "dilations": "An optional list of ints. Defaults to [1, 1, 1, 1].", "padding_list": "An optional list of ints. Defaults to [].", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output, min_output, max_output)."}, "tf.raw_ops.QuantizedConv2DWithBiasSignedSumAndReluAndRequantize": {"Args": {"input": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.", "filter": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.", "bias": "A Tensor. Must be one of the following types: float32, qint32.", "min_input": "A Tensor of type float32.", "max_input": "A Tensor of type float32.", "min_filter": "A Tensor of type float32.", "max_filter": "A Tensor of type float32.", "min_freezed_output": "A Tensor of type float32.", "max_freezed_output": "A Tensor of type float32.", "summand": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.", "min_summand": "A Tensor of type float32.", "max_summand": "A Tensor of type float32.", "strides": "A list of ints.", "padding": "A string from: \"SAME\", \"VALID\".", "out_type": "An optional tf.DType from: tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16. Defaults to tf.quint8.", "dilations": "An optional list of ints. Defaults to [1, 1, 1, 1].", "padding_list": "An optional list of ints. Defaults to [].", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output, min_output, max_output)."}, "tf.raw_ops.QuantizedConv2DWithBiasSumAndRelu": {"Args": {"input": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.", "filter": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.", "bias": "A Tensor of type float32.", "min_input": "A Tensor of type float32.", "max_input": "A Tensor of type float32.", "min_filter": "A Tensor of type float32.", "max_filter": "A Tensor of type float32.", "summand": "A Tensor of type float32.", "strides": "A list of ints.", "padding": "A string from: \"SAME\", \"VALID\".", "out_type": "An optional tf.DType from: tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16. Defaults to tf.qint32.", "dilations": "An optional list of ints. Defaults to [1, 1, 1, 1].", "padding_list": "An optional list of ints. Defaults to [].", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output, min_output, max_output)."}, "tf.raw_ops.QuantizedConv2DWithBiasSumAndReluAndRequantize": {"Args": {"input": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.", "filter": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.", "bias": "A Tensor. Must be one of the following types: float32, qint32.", "min_input": "A Tensor of type float32.", "max_input": "A Tensor of type float32.", "min_filter": "A Tensor of type float32.", "max_filter": "A Tensor of type float32.", "min_freezed_output": "A Tensor of type float32.", "max_freezed_output": "A Tensor of type float32.", "summand": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.", "min_summand": "A Tensor of type float32.", "max_summand": "A Tensor of type float32.", "strides": "A list of ints.", "padding": "A string from: \"SAME\", \"VALID\".", "out_type": "An optional tf.DType from: tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16. Defaults to tf.quint8.", "dilations": "An optional list of ints. Defaults to [1, 1, 1, 1].", "padding_list": "An optional list of ints. Defaults to [].", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output, min_output, max_output)."}, "tf.raw_ops.QuantizedDepthwiseConv2D": {"description": "Computes quantized depthwise Conv2D.", "Args": {"input": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.\nThe original input tensor.", "filter": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.\nThe original filter tensor.", "min_input": "A Tensor of type float32.\nThe float value that the minimum quantized input value represents.", "max_input": "A Tensor of type float32.\nThe float value that the maximum quantized input value represents.", "min_filter": "A Tensor of type float32.\nThe float value that the minimum quantized filter value represents.", "max_filter": "A Tensor of type float32.\nThe float value that the maximum quantized filter value represents.", "strides": "A list of ints. List of stride values.", "padding": "A string from: \"SAME\", \"VALID\".", "out_type": "An optional tf.DType from: tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16. Defaults to tf.qint32.\nThe type of the output.", "dilations": "An optional list of ints. Defaults to [1, 1, 1, 1].\nList of dilation values.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output, min_output, max_output)."}, "tf.raw_ops.QuantizedDepthwiseConv2DWithBias": {"description": "Computes quantized depthwise Conv2D with Bias.", "Args": {"input": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.\nThe original input tensor.", "filter": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.\nThe original filter tensor.", "bias": "A Tensor of type float32. The original bias tensor.", "min_input": "A Tensor of type float32.\nThe float value that the minimum quantized input value represents.", "max_input": "A Tensor of type float32.\nThe float value that the maximum quantized input value represents.", "min_filter": "A Tensor of type float32.\nThe float value that the minimum quantized filter value represents.", "max_filter": "A Tensor of type float32.\nThe float value that the maximum quantized filter value represents.", "strides": "A list of ints. List of stride values.", "padding": "A string from: \"SAME\", \"VALID\".", "out_type": "An optional tf.DType from: tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16. Defaults to tf.qint32.\nThe type of the output.", "dilations": "An optional list of ints. Defaults to [1, 1, 1, 1].\nList of dilation values.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output, min_output, max_output)."}, "tf.raw_ops.QuantizedDepthwiseConv2DWithBiasAndRelu": {"description": "Computes quantized depthwise Conv2D with Bias and Relu.", "Args": {"input": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.\nThe original input tensor.", "filter": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.\nThe original filter tensor.", "bias": "A Tensor of type float32. The original bias tensor.", "min_input": "A Tensor of type float32.\nThe float value that the minimum quantized input value represents.", "max_input": "A Tensor of type float32.\nThe float value that the maximum quantized input value represents.", "min_filter": "A Tensor of type float32.\nThe float value that the minimum quantized filter value represents.", "max_filter": "A Tensor of type float32.\nThe float value that the maximum quantized filter value represents.", "strides": "A list of ints. List of stride values.", "padding": "A string from: \"SAME\", \"VALID\".", "out_type": "An optional tf.DType from: tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16. Defaults to tf.qint32.\nThe type of the output.", "dilations": "An optional list of ints. Defaults to [1, 1, 1, 1].\nList of dilation values.", "padding_list": "An optional list of ints. Defaults to [].", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output, min_output, max_output)."}, "tf.raw_ops.QuantizedDepthwiseConv2DWithBiasAndReluAndRequantize": {"description": "Computes quantized depthwise Conv2D with Bias, Relu and Requantize.", "Args": {"input": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.\nThe original input tensor.", "filter": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.\nThe original filter tensor.", "bias": "A Tensor. Must be one of the following types: float32, qint32.\nThe original bias tensor.", "min_input": "A Tensor of type float32.\nThe float value that the minimum quantized input value represents.", "max_input": "A Tensor of type float32.\nThe float value that the maximum quantized input value represents.", "min_filter": "A Tensor of type float32.\nThe float value that the minimum quantized filter value represents.", "max_filter": "A Tensor of type float32.\nThe float value that the maximum quantized filter value represents.", "min_freezed_output": "A Tensor of type float32.\nThe minimum float value of the output tensor.", "max_freezed_output": "A Tensor of type float32.\nThe maximum float value of the output tensor.", "strides": "A list of ints. List of stride values.", "padding": "A string from: \"SAME\", \"VALID\".", "out_type": "An optional tf.DType from: tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16. Defaults to tf.quint8.\nThe type of the output.", "dilations": "An optional list of ints. Defaults to [1, 1, 1, 1].\nList of dilation values.", "padding_list": "An optional list of ints. Defaults to [].", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output, min_output, max_output)."}, "tf.raw_ops.QuantizedInstanceNorm": {"description": "Quantized Instance normalization.", "Args": {"x": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.\nA 4D input Tensor.", "x_min": "A Tensor of type float32.\nThe value represented by the lowest quantized input.", "x_max": "A Tensor of type float32.\nThe value represented by the highest quantized input.", "output_range_given": "An optional bool. Defaults to False.\nIf True, given_y_min and given_y_min\nand given_y_max are used as the output range. Otherwise,\nthe implementation computes the output range.", "given_y_min": "An optional float. Defaults to 0.\nOutput in y_min if output_range_given is True.", "given_y_max": "An optional float. Defaults to 0.\nOutput in y_max if output_range_given is True.", "variance_epsilon": "An optional float. Defaults to 1e-05.\nA small float number to avoid dividing by 0.", "min_separation": "An optional float. Defaults to 0.001.\nMinimum value of y_max - y_min", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (y, y_min, y_max)."}, "tf.raw_ops.QuantizedMatMul": {"description": "Perform a quantized matrix multiplication of a by the matrix b.", "Args": {"a": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.\nMust be a two-dimensional tensor.", "b": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.\nMust be a two-dimensional tensor.", "min_a": "A Tensor of type float32.\nThe float value that the lowest quantized a value represents.", "max_a": "A Tensor of type float32.\nThe float value that the highest quantized a value represents.", "min_b": "A Tensor of type float32.\nThe float value that the lowest quantized b value represents.", "max_b": "A Tensor of type float32.\nThe float value that the highest quantized b value represents.", "Toutput": "An optional tf.DType from: tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16. Defaults to tf.qint32.", "transpose_a": "An optional bool. Defaults to False.\nIf true, a is transposed before multiplication.", "transpose_b": "An optional bool. Defaults to False.\nIf true, b is transposed before multiplication.", "Tactivation": "An optional tf.DType from: tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16. Defaults to tf.quint8.\nThe type of output produced by activation function\nfollowing this operation.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (out, min_out, max_out)."}, "tf.raw_ops.QuantizedMatMulWithBias": {"description": "Performs a quantized matrix multiplication of a by the matrix b with bias"}, "tf.raw_ops.QuantizedMatMulWithBiasAndDequantize": {"Args": {"a": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.", "b": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.", "bias": "A Tensor. Must be one of the following types: float32, qint32.", "min_a": "A Tensor of type float32.", "max_a": "A Tensor of type float32.", "min_b": "A Tensor of type float32.", "max_b": "A Tensor of type float32.", "min_freezed_output": "A Tensor of type float32.", "max_freezed_output": "A Tensor of type float32.", "Toutput": "A tf.DType from: tf.float32.", "transpose_a": "An optional bool. Defaults to False.", "transpose_b": "An optional bool. Defaults to False.", "input_quant_mode": "An optional string from: \"MIN_FIRST\", \"SCALED\". Defaults to \"MIN_FIRST\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type Toutput."}, "tf.raw_ops.QuantizedMatMulWithBiasAndRelu": {"description": "Perform a quantized matrix multiplication of a by the matrix b with bias"}, "tf.raw_ops.QuantizedMatMulWithBiasAndReluAndRequantize": {"description": "Perform a quantized matrix multiplication of a by the matrix b with bias"}, "tf.raw_ops.QuantizedMatMulWithBiasAndRequantize": {"Args": {"a": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.", "b": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.", "bias": "A Tensor. Must be one of the following types: float32, qint32.", "min_a": "A Tensor of type float32.", "max_a": "A Tensor of type float32.", "min_b": "A Tensor of type float32.", "max_b": "A Tensor of type float32.", "min_freezed_output": "A Tensor of type float32.", "max_freezed_output": "A Tensor of type float32.", "Toutput": "An optional tf.DType from: tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16. Defaults to tf.quint8.", "transpose_a": "An optional bool. Defaults to False.", "transpose_b": "An optional bool. Defaults to False.", "input_quant_mode": "An optional string from: \"MIN_FIRST\", \"SCALED\". Defaults to \"MIN_FIRST\".", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (out, min_out, max_out)."}, "tf.raw_ops.QuantizedMaxPool": {"description": "Produces the max pool of the input tensor for quantized types.", "Args": {"input": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.\nThe 4D (batch x rows x cols x depth) Tensor to MaxReduce over.", "min_input": "A Tensor of type float32.\nThe float value that the lowest quantized input value represents.", "max_input": "A Tensor of type float32.\nThe float value that the highest quantized input value represents.", "ksize": "A list of ints.\nThe size of the window for each dimension of the input tensor.\nThe length must be 4 to match the number of dimensions of the input.", "strides": "A list of ints.\nThe stride of the sliding window for each dimension of the input\ntensor. The length must be 4 to match the number of dimensions of the input.", "padding": "A string from: \"SAME\", \"VALID\".\nThe type of padding algorithm to use.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output, min_output, max_output)."}, "tf.raw_ops.QuantizedMul": {"description": "Returns x * y element-wise, working on quantized buffers.", "Args": {"x": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.", "y": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.", "min_x": "A Tensor of type float32.\nThe float value that the lowest quantized x value represents.", "max_x": "A Tensor of type float32.\nThe float value that the highest quantized x value represents.", "min_y": "A Tensor of type float32.\nThe float value that the lowest quantized y value represents.", "max_y": "A Tensor of type float32.\nThe float value that the highest quantized y value represents.", "Toutput": "An optional tf.DType from: tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16. Defaults to tf.qint32.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (z, min_z, max_z)."}, "tf.raw_ops.QuantizedRelu": {"description": "Computes Quantized Rectified Linear: max(features, 0)", "Args": {"features": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.", "min_features": "A Tensor of type float32.\nThe float value that the lowest quantized value represents.", "max_features": "A Tensor of type float32.\nThe float value that the highest quantized value represents.", "out_type": "An optional tf.DType from: tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16. Defaults to tf.quint8.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (activations, min_activations, max_activations)."}, "tf.raw_ops.QuantizedRelu6": {"description": "Computes Quantized Rectified Linear 6: min(max(features, 0), 6)", "Args": {"features": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.", "min_features": "A Tensor of type float32.\nThe float value that the lowest quantized value represents.", "max_features": "A Tensor of type float32.\nThe float value that the highest quantized value represents.", "out_type": "An optional tf.DType from: tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16. Defaults to tf.quint8.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (activations, min_activations, max_activations)."}, "tf.raw_ops.QuantizedReluX": {"description": "Computes Quantized Rectified Linear X: min(max(features, 0), max_value)", "Args": {"features": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.", "max_value": "A Tensor of type float32.", "min_features": "A Tensor of type float32.\nThe float value that the lowest quantized value represents.", "max_features": "A Tensor of type float32.\nThe float value that the highest quantized value represents.", "out_type": "An optional tf.DType from: tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16. Defaults to tf.quint8.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (activations, min_activations, max_activations)."}, "tf.raw_ops.QuantizedReshape": {"description": "Reshapes a quantized tensor as per the Reshape op."}, "tf.raw_ops.QuantizedResizeBilinear": {"description": "Resize quantized images to size using quantized bilinear interpolation.", "Args": {"images": "A Tensor. Must be one of the following types: quint8, qint32, float32.\n4-D with shape [batch, height, width, channels].", "size": "A 1-D int32 Tensor of 2 elements: new_height, new_width.  The\nnew size for the images.", "min": "A Tensor of type float32.", "max": "A Tensor of type float32.", "align_corners": "An optional bool. Defaults to False.\nIf true, the centers of the 4 corner pixels of the input and output tensors are\naligned, preserving the values at the corner pixels. Defaults to false.", "half_pixel_centers": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (resized_images, out_min, out_max)."}, "tf.raw_ops.QueueClose": {"description": "Closes the given queue.", "Args": {"handle": "A Tensor of type mutable string. The handle to a queue.", "cancel_pending_enqueues": "An optional bool. Defaults to False.\nIf true, all pending enqueue requests that are\nblocked on the given queue will be canceled.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.QueueCloseV2": {"description": "Closes the given queue.", "Args": {"handle": "A Tensor of type resource. The handle to a queue.", "cancel_pending_enqueues": "An optional bool. Defaults to False.\nIf true, all pending enqueue requests that are\nblocked on the given queue will be canceled.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.QueueDequeue": {"description": "Dequeues a tuple of one or more tensors from the given queue.", "Args": {"handle": "A Tensor of type mutable string. The handle to a queue.", "component_types": "A list of tf.DTypes that has length >= 1.\nThe type of each component in a tuple.", "timeout_ms": "An optional int. Defaults to -1.\nIf the queue is empty, this operation will block for up to\ntimeout_ms milliseconds.\nNote: This option is not supported yet.", "name": "A name for the operation (optional)."}, "Returns": "A list of Tensor objects of type component_types."}, "tf.raw_ops.QueueDequeueMany": {"description": "Dequeues n tuples of one or more tensors from the given queue.", "Args": {"handle": "A Tensor of type mutable string. The handle to a queue.", "n": "A Tensor of type int32. The number of tuples to dequeue.", "component_types": "A list of tf.DTypes that has length >= 1.\nThe type of each component in a tuple.", "timeout_ms": "An optional int. Defaults to -1.\nIf the queue has fewer than n elements, this operation\nwill block for up to timeout_ms milliseconds.\nNote: This option is not supported yet.", "name": "A name for the operation (optional)."}, "Returns": "A list of Tensor objects of type component_types."}, "tf.raw_ops.QueueDequeueManyV2": {"description": "Dequeues n tuples of one or more tensors from the given queue.", "Args": {"handle": "A Tensor of type resource. The handle to a queue.", "n": "A Tensor of type int32. The number of tuples to dequeue.", "component_types": "A list of tf.DTypes that has length >= 1.\nThe type of each component in a tuple.", "timeout_ms": "An optional int. Defaults to -1.\nIf the queue has fewer than n elements, this operation\nwill block for up to timeout_ms milliseconds.\nNote: This option is not supported yet.", "name": "A name for the operation (optional)."}, "Returns": "A list of Tensor objects of type component_types."}, "tf.raw_ops.QueueDequeueUpTo": {"description": "Dequeues n tuples of one or more tensors from the given queue.", "Args": {"handle": "A Tensor of type mutable string. The handle to a queue.", "n": "A Tensor of type int32. The number of tuples to dequeue.", "component_types": "A list of tf.DTypes that has length >= 1.\nThe type of each component in a tuple.", "timeout_ms": "An optional int. Defaults to -1.\nIf the queue has fewer than n elements, this operation\nwill block for up to timeout_ms milliseconds.\nNote: This option is not supported yet.", "name": "A name for the operation (optional)."}, "Returns": "A list of Tensor objects of type component_types."}, "tf.raw_ops.QueueDequeueUpToV2": {"description": "Dequeues n tuples of one or more tensors from the given queue.", "Args": {"handle": "A Tensor of type resource. The handle to a queue.", "n": "A Tensor of type int32. The number of tuples to dequeue.", "component_types": "A list of tf.DTypes that has length >= 1.\nThe type of each component in a tuple.", "timeout_ms": "An optional int. Defaults to -1.\nIf the queue has fewer than n elements, this operation\nwill block for up to timeout_ms milliseconds.\nNote: This option is not supported yet.", "name": "A name for the operation (optional)."}, "Returns": "A list of Tensor objects of type component_types."}, "tf.raw_ops.QueueDequeueV2": {"description": "Dequeues a tuple of one or more tensors from the given queue.", "Args": {"handle": "A Tensor of type resource. The handle to a queue.", "component_types": "A list of tf.DTypes that has length >= 1.\nThe type of each component in a tuple.", "timeout_ms": "An optional int. Defaults to -1.\nIf the queue is empty, this operation will block for up to\ntimeout_ms milliseconds.\nNote: This option is not supported yet.", "name": "A name for the operation (optional)."}, "Returns": "A list of Tensor objects of type component_types."}, "tf.raw_ops.QueueEnqueue": {"description": "Enqueues a tuple of one or more tensors in the given queue.", "Args": {"handle": "A Tensor of type mutable string. The handle to a queue.", "components": "A list of Tensor objects.\nOne or more tensors from which the enqueued tensors should be taken.", "timeout_ms": "An optional int. Defaults to -1.\nIf the queue is full, this operation will block for up to\ntimeout_ms milliseconds.\nNote: This option is not supported yet.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.QueueEnqueueMany": {"description": "Enqueues zero or more tuples of one or more tensors in the given queue.", "Args": {"handle": "A Tensor of type mutable string. The handle to a queue.", "components": "A list of Tensor objects.\nOne or more tensors from which the enqueued tensors should\nbe taken.", "timeout_ms": "An optional int. Defaults to -1.\nIf the queue is too full, this operation will block for up\nto timeout_ms milliseconds.\nNote: This option is not supported yet.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.QueueEnqueueManyV2": {"description": "Enqueues zero or more tuples of one or more tensors in the given queue.", "Args": {"handle": "A Tensor of type resource. The handle to a queue.", "components": "A list of Tensor objects.\nOne or more tensors from which the enqueued tensors should\nbe taken.", "timeout_ms": "An optional int. Defaults to -1.\nIf the queue is too full, this operation will block for up\nto timeout_ms milliseconds.\nNote: This option is not supported yet.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.QueueEnqueueV2": {"description": "Enqueues a tuple of one or more tensors in the given queue.", "Args": {"handle": "A Tensor of type resource. The handle to a queue.", "components": "A list of Tensor objects.\nOne or more tensors from which the enqueued tensors should be taken.", "timeout_ms": "An optional int. Defaults to -1.\nIf the queue is full, this operation will block for up to\ntimeout_ms milliseconds.\nNote: This option is not supported yet.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.QueueIsClosed": {"description": "Returns true if queue is closed.", "Args": {"handle": "A Tensor of type mutable string. The handle to a queue.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type bool."}, "tf.raw_ops.QueueIsClosedV2": {"description": "Returns true if queue is closed.", "Args": {"handle": "A Tensor of type resource. The handle to a queue.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type bool."}, "tf.raw_ops.QueueSize": {"description": "Computes the number of elements in the given queue.", "Args": {"handle": "A Tensor of type mutable string. The handle to a queue.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int32."}, "tf.raw_ops.QueueSizeV2": {"description": "Computes the number of elements in the given queue.", "Args": {"handle": "A Tensor of type resource. The handle to a queue.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int32."}, "tf.raw_ops.RFFT": {"description": "Real-valued fast Fourier transform.", "Args": {"input": "A Tensor. Must be one of the following types: float32, float64.\nA float32 tensor.", "fft_length": "A Tensor of type int32.\nAn int32 tensor of shape [1]. The FFT length.", "Tcomplex": "An optional tf.DType from: tf.complex64, tf.complex128. Defaults to tf.complex64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type Tcomplex."}, "tf.raw_ops.RFFT2D": {"description": "2D real-valued fast Fourier transform.", "Args": {"input": "A Tensor. Must be one of the following types: float32, float64.\nA float32 tensor.", "fft_length": "A Tensor of type int32.\nAn int32 tensor of shape [2]. The FFT length for each dimension.", "Tcomplex": "An optional tf.DType from: tf.complex64, tf.complex128. Defaults to tf.complex64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type Tcomplex."}, "tf.raw_ops.RFFT3D": {"description": "3D real-valued fast Fourier transform.", "Args": {"input": "A Tensor. Must be one of the following types: float32, float64.\nA float32 tensor.", "fft_length": "A Tensor of type int32.\nAn int32 tensor of shape [3]. The FFT length for each dimension.", "Tcomplex": "An optional tf.DType from: tf.complex64, tf.complex128. Defaults to tf.complex64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type Tcomplex."}, "tf.raw_ops.RGBToHSV": {"description": "Converts one or more images from RGB to HSV.", "Args": {"images": "A Tensor. Must be one of the following types: half, bfloat16, float32, float64.\n1-D or higher rank. RGB data to convert. Last dimension must be size 3.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as images."}, "tf.raw_ops.RaggedBincount": {"description": "Counts the number of occurrences of each value in an integer array.", "Args": {"splits": "A Tensor of type int64. 1D int64 Tensor.", "values": "A Tensor. Must be one of the following types: int32, int64.\n2D int Tensor.", "size": "A Tensor. Must have the same type as values.\nnon-negative int scalar Tensor.", "weights": "A Tensor. Must be one of the following types: int32, int64, float32, float64.\nis an int32, int64, float32, or float64 Tensor with the same\nshape as input, or a length-0 Tensor, in which case it acts as all weights\nequal to 1.", "binary_output": "An optional bool. Defaults to False.\nbool; Whether the kernel should count the appearance or number of occurrences.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as weights."}, "tf.raw_ops.RaggedCountSparseOutput": {"description": "Performs sparse-output bin counting for a ragged tensor input.", "Args": {"splits": "A Tensor of type int64.\nTensor containing the row splits of the ragged tensor to count.", "values": "A Tensor. Must be one of the following types: int32, int64.\nTensor containing values of the sparse tensor to count.", "weights": "A Tensor. Must be one of the following types: int32, int64, float32, float64.\nA Tensor of the same shape as indices containing per-index weight values.\nMay also be the empty tensor if no weights are used.", "binary_output": "A bool.\nWhether to output the number of occurrences of each value or 1.", "minlength": "An optional int that is >= -1. Defaults to -1.\nMinimum value to count. Can be set to -1 for no minimum.", "maxlength": "An optional int that is >= -1. Defaults to -1.\nMaximum value to count. Can be set to -1 for no maximum.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output_indices, output_values, output_dense_shape)."}, "tf.raw_ops.RaggedCross": {"description": "Generates a feature cross from a list of tensors, and returns it as a"}, "tf.raw_ops.RaggedGather": {"description": "Gather ragged slices from params axis 0 according to indices.", "Args": {"params_nested_splits": "A list of at least 1 Tensor objects with the same type in: int32, int64.\nThe nested_row_splits tensors that define the row-partitioning for the\nparams RaggedTensor input.", "params_dense_values": "A Tensor.\nThe flat_values for the params RaggedTensor. There was a terminology change\nat the python level from dense_values to flat_values, so dense_values is the\ndeprecated name.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nIndices in the outermost dimension of params of the values that should be\ngathered.", "OUTPUT_RAGGED_RANK": "An int that is >= 0.\nThe ragged rank of the output RaggedTensor. output_nested_splits will contain\nthis number of row_splits tensors. This value should equal\nindices.shape.ndims + params.ragged_rank - 1.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output_nested_splits, output_dense_values)."}, "tf.raw_ops.RaggedRange": {"description": "Returns a RaggedTensor containing the specified sequences of numbers.", "Args": {"starts": "A Tensor. Must be one of the following types: bfloat16, float32, float64, int32, int64.\nThe starts of each range.", "limits": "A Tensor. Must have the same type as starts.\nThe limits of each range.", "deltas": "A Tensor. Must have the same type as starts.\nThe deltas of each range.", "Tsplits": "An optional tf.DType from: tf.int32, tf.int64. Defaults to tf.int64.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (rt_nested_splits, rt_dense_values)."}, "tf.raw_ops.RaggedTensorFromVariant": {"description": "Decodes a variant Tensor into a RaggedTensor.", "Args": {"encoded_ragged": "A Tensor of type variant.\nA variant Tensor containing encoded RaggedTensors.", "input_ragged_rank": "An int that is >= -1.\nThe ragged rank of each encoded RaggedTensor component in the input. If set to\n-1, this is inferred as output_ragged_rank - rank(encoded_ragged)", "output_ragged_rank": "An int that is >= 0.\nThe expected ragged rank of the output RaggedTensor. The following must hold:\noutput_ragged_rank = rank(encoded_ragged) + input_ragged_rank.", "Tvalues": "A tf.DType.", "Tsplits": "An optional tf.DType from: tf.int32, tf.int64. Defaults to tf.int64.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output_nested_splits, output_dense_values)."}, "tf.raw_ops.RaggedTensorToSparse": {"description": "Converts a RaggedTensor into a SparseTensor with the same values.", "Args": {"rt_nested_splits": "A list of at least 1 Tensor objects with the same type in: int32, int64.\nThe row_splits for the RaggedTensor.", "rt_dense_values": "A Tensor. The flat_values for the RaggedTensor.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (sparse_indices, sparse_values, sparse_dense_shape)."}, "tf.raw_ops.RaggedTensorToTensor": {"description": "Create a dense tensor from a ragged tensor, possibly altering its shape.", "Args": {"shape": "A Tensor. Must be one of the following types: int64, int32.\nThe desired shape of the output tensor. If left unspecified (empty),\nthe minimal shape required to contain all the elements in the ragged tensor\n(the natural shape) will be used. If some dimensions are left unspecified, then\nthe size of the natural shape is used in that dimension.\nNote that dense dimensions cannot be modified by the shape argument. Trying to\nchange the size of a dense dimension will cause the op to fail.\nExamples:\nnatural shape: [4, 5, 6]\nshape: -1\noutput shape: [4, 5, 6]\nnatural shape: [4, 5, 6]\nshape: [3, -1, 2]\noutput shape: [3, 5, 2]\nnatural shape: [4, 5, 6]\nshape: [3, 7, 2]\noutput shape: [3, 7, 2]", "values": "A Tensor.\nA 1D tensor representing the values of the ragged tensor.", "default_value": "A Tensor. Must have the same type as values.\nThe default_value when the shape is larger than the ragged tensor. The\ndefault_value is broadcast until it is the shape of the output tensor, and\nthen overwritten by values in the ragged tensor. The default value must be\ncompatible with this broadcast operation, and must have fewer dimensions than\nthe value tensor.", "row_partition_tensors": "A list of at least 1 Tensor objects with the same type in: int64, int32.", "row_partition_types": "A list of strings.\nThe types of the row partition tensors. At present, these can be:\n\n\"ROW_SPLITS\": the row_splits tensor from the ragged tensor.\n\"VALUE_ROWIDS\": the value_rowids tensor from the ragged tensor.\n\"FIRST_DIM_SIZE\": if value_rowids is used for the first dimension, then it\nis preceeded by \"FIRST_DIM_SIZE\".\nThe tensors are in the order of the dimensions.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as values."}, "tf.raw_ops.RaggedTensorToVariant": {"description": "Encodes a RaggedTensor into a variant Tensor.", "Args": {"rt_nested_splits": "A list of Tensor objects with the same type in: int32, int64.\nA list of one or more Tensors representing the splits of the input\nRaggedTensor.", "rt_dense_values": "A Tensor.\nA Tensor representing the values of the input RaggedTensor.", "batched_input": "A bool.\nA bool denoting whether the input is a batched RaggedTensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.RaggedTensorToVariantGradient": {"description": "Helper used to compute the gradient for RaggedTensorToVariant.", "Args": {"encoded_ragged_grad": "A Tensor of type variant.\nA variant Tensor containing encoded RaggedTensor gradients.", "row_splits": "A Tensor. Must be one of the following types: int32, int64.\nOutermost row-splits that were used as input to the RaggedTensorToVariant op.", "dense_values_shape": "A Tensor of type int32.\nShape of the dense_values that was used as an input to the\nRaggedTensorToVariant op.", "Tvalues": "A tf.DType.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type Tvalues."}, "tf.raw_ops.RandomCrop": {"description": "Randomly crop image.", "Args": {"image": "A Tensor. Must be one of the following types: uint8, int8, int16, int32, int64, float32, float64.\n3-D of shape [height, width, channels].", "size": "A Tensor of type int64.\n1-D of length 2 containing: crop_height, crop_width..", "seed": "An optional int. Defaults to 0.\nIf either seed or seed2 are set to be non-zero, the random number\ngenerator is seeded by the given seed.  Otherwise, it is seeded by a\nrandom seed.", "seed2": "An optional int. Defaults to 0.\nAn second seed to avoid seed collision.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as image."}, "tf.raw_ops.RandomDataset": {"description": "Creates a Dataset that returns pseudorandom numbers.", "Args": {"seed": "A Tensor of type int64.\nA scalar seed for the random number generator. If either seed or\nseed2 is set to be non-zero, the random number generator is seeded\nby the given seed.  Otherwise, a random seed is used.", "seed2": "A Tensor of type int64.\nA second scalar seed to avoid seed collision.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.RandomGamma": {"description": "Outputs random values from the Gamma distribution(s) described by alpha.", "Args": {"shape": "A Tensor. Must be one of the following types: int32, int64.\n1-D integer tensor. Shape of independent samples to draw from each\ndistribution described by the shape parameters given in alpha.", "alpha": "A Tensor. Must be one of the following types: half, float32, float64.\nA tensor in which each scalar is a \"shape\" parameter describing the\nassociated gamma distribution.", "seed": "An optional int. Defaults to 0.\nIf either seed or seed2 are set to be non-zero, the random number\ngenerator is seeded by the given seed.  Otherwise, it is seeded by a\nrandom seed.", "seed2": "An optional int. Defaults to 0.\nA second seed to avoid seed collision.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as alpha."}, "tf.raw_ops.RandomGammaGrad": {"description": "Computes the derivative of a Gamma random sample w.r.t. alpha.", "Args": {"alpha": "A Tensor. Must be one of the following types: float32, float64.", "sample": "A Tensor. Must have the same type as alpha.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as alpha."}, "tf.raw_ops.RandomIndexShuffle": {"description": "Outputs the position of value in a permutation of [0, ..., max_index].", "Args": {"index": "A Tensor. Must be one of the following types: int32, uint32, int64, uint64.\nA scalar tensor or a vector of dtype dtype. The index (or indices) to be shuffled. Must be within [0, max_index].", "seed": "A Tensor. Must be one of the following types: int32, uint32, int64, uint64.\nA tensor of dtype Tseed and shape [3] or [n, 3]. The random seed.", "max_index": "A Tensor. Must have the same type as index.\nA scalar tensor or vector of dtype dtype. The upper bound(s) of the interval (inclusive).", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as index."}, "tf.raw_ops.RandomPoisson": {"description": "Use RandomPoissonV2 instead.", "Args": {"shape": "A Tensor. Must be one of the following types: int32, int64.", "rate": "A Tensor. Must be one of the following types: half, float32, float64.", "seed": "An optional int. Defaults to 0.", "seed2": "An optional int. Defaults to 0.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as rate."}, "tf.raw_ops.RandomPoissonV2": {"description": "Outputs random values from the Poisson distribution(s) described by rate.", "Args": {"shape": "A Tensor. Must be one of the following types: int32, int64.\n1-D integer tensor. Shape of independent samples to draw from each\ndistribution described by the shape parameters given in rate.", "rate": "A Tensor. Must be one of the following types: half, float32, float64, int32, int64.\nA tensor in which each scalar is a \"rate\" parameter describing the\nassociated poisson distribution.", "seed": "An optional int. Defaults to 0.\nIf either seed or seed2 are set to be non-zero, the random number\ngenerator is seeded by the given seed.  Otherwise, it is seeded by a\nrandom seed.", "seed2": "An optional int. Defaults to 0.\nA second seed to avoid seed collision.", "dtype": "An optional tf.DType from: tf.half, tf.float32, tf.float64, tf.int32, tf.int64. Defaults to tf.int64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dtype."}, "tf.raw_ops.RandomShuffle": {"description": "Randomly shuffles a tensor along its first dimension.", "Args": {"value": "A Tensor. The tensor to be shuffled.", "seed": "An optional int. Defaults to 0.\nIf either seed or seed2 are set to be non-zero, the random number\ngenerator is seeded by the given seed.  Otherwise, it is seeded by a\nrandom seed.", "seed2": "An optional int. Defaults to 0.\nA second seed to avoid seed collision.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as value."}, "tf.raw_ops.RandomShuffleQueue": {"description": "A queue that randomizes the order of elements.", "Args": {"component_types": "A list of tf.DTypes that has length >= 1.\nThe type of each component in a value.", "shapes": "An optional list of shapes (each a tf.TensorShape or list of ints). Defaults to [].\nThe shape of each component in a value. The length of this attr must\nbe either 0 or the same as the length of component_types. If the length of\nthis attr is 0, the shapes of queue elements are not constrained, and\nonly one element may be dequeued at a time.", "capacity": "An optional int. Defaults to -1.\nThe upper bound on the number of elements in this queue.\nNegative numbers mean no limit.", "min_after_dequeue": "An optional int. Defaults to 0.\nDequeue will block unless there would be this\nmany elements after the dequeue or the queue is closed. This\nensures a minimum level of mixing of elements.", "seed": "An optional int. Defaults to 0.\nIf either seed or seed2 is set to be non-zero, the random number\ngenerator is seeded by the given seed.  Otherwise, a random seed is used.", "seed2": "An optional int. Defaults to 0.\nA second seed to avoid seed collision.", "container": "An optional string. Defaults to \"\".\nIf non-empty, this queue is placed in the given container.\nOtherwise, a default container is used.", "shared_name": "An optional string. Defaults to \"\".\nIf non-empty, this queue will be shared under the given name\nacross multiple sessions.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type mutable string."}, "tf.raw_ops.RandomShuffleQueueV2": {"description": "A queue that randomizes the order of elements.", "Args": {"component_types": "A list of tf.DTypes that has length >= 1.\nThe type of each component in a value.", "shapes": "An optional list of shapes (each a tf.TensorShape or list of ints). Defaults to [].\nThe shape of each component in a value. The length of this attr must\nbe either 0 or the same as the length of component_types. If the length of\nthis attr is 0, the shapes of queue elements are not constrained, and\nonly one element may be dequeued at a time.", "capacity": "An optional int. Defaults to -1.\nThe upper bound on the number of elements in this queue.\nNegative numbers mean no limit.", "min_after_dequeue": "An optional int. Defaults to 0.\nDequeue will block unless there would be this\nmany elements after the dequeue or the queue is closed. This\nensures a minimum level of mixing of elements.", "seed": "An optional int. Defaults to 0.\nIf either seed or seed2 is set to be non-zero, the random number\ngenerator is seeded by the given seed.  Otherwise, a random seed is used.", "seed2": "An optional int. Defaults to 0.\nA second seed to avoid seed collision.", "container": "An optional string. Defaults to \"\".\nIf non-empty, this queue is placed in the given container.\nOtherwise, a default container is used.", "shared_name": "An optional string. Defaults to \"\".\nIf non-empty, this queue will be shared under the given name\nacross multiple sessions.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type resource."}, "tf.raw_ops.RandomStandardNormal": {"description": "Outputs random values from a normal distribution.", "Args": {"shape": "A Tensor. Must be one of the following types: int32, int64.\nThe shape of the output tensor.", "dtype": "A tf.DType from: tf.half, tf.bfloat16, tf.float32, tf.float64.\nThe type of the output.", "seed": "An optional int. Defaults to 0.\nIf either seed or seed2 are set to be non-zero, the random number\ngenerator is seeded by the given seed.  Otherwise, it is seeded by a\nrandom seed.", "seed2": "An optional int. Defaults to 0.\nA second seed to avoid seed collision.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dtype."}, "tf.raw_ops.RandomUniform": {"description": "Outputs random values from a uniform distribution.", "Args": {"shape": "A Tensor. Must be one of the following types: int32, int64.\nThe shape of the output tensor.", "dtype": "A tf.DType from: tf.half, tf.bfloat16, tf.float32, tf.float64.\nThe type of the output.", "seed": "An optional int. Defaults to 0.\nIf either seed or seed2 are set to be non-zero, the random number\ngenerator is seeded by the given seed.  Otherwise, it is seeded by a\nrandom seed.", "seed2": "An optional int. Defaults to 0.\nA second seed to avoid seed collision.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dtype."}, "tf.raw_ops.RandomUniformInt": {"description": "Outputs random integers from a uniform distribution.", "Args": {"shape": "A Tensor. Must be one of the following types: int32, int64.\nThe shape of the output tensor.", "minval": "A Tensor. Must be one of the following types: int32, int64.\n0-D.  Inclusive lower bound on the generated integers.", "maxval": "A Tensor. Must have the same type as minval.\n0-D.  Exclusive upper bound on the generated integers.", "seed": "An optional int. Defaults to 0.\nIf either seed or seed2 are set to be non-zero, the random number\ngenerator is seeded by the given seed.  Otherwise, it is seeded by a\nrandom seed.", "seed2": "An optional int. Defaults to 0.\nA second seed to avoid seed collision.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as minval."}, "tf.raw_ops.Range": {"description": "Creates a sequence of numbers.", "Args": {"start": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, int8, int16, int32, int64, uint16, uint32.\n0-D (scalar). First entry in the sequence.", "limit": "A Tensor. Must have the same type as start.\n0-D (scalar). Upper limit of sequence, exclusive.", "delta": "A Tensor. Must have the same type as start.\n0-D (scalar). Optional. Default is 1. Number that increments start.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as start."}, "tf.raw_ops.RangeDataset": {"description": "Creates a dataset with a range of values. Corresponds to python&#39;s xrange.", "Args": {"start": "A Tensor of type int64.\ncorresponds to start in python's xrange().", "stop": "A Tensor of type int64.\ncorresponds to stop in python's xrange().", "step": "A Tensor of type int64.\ncorresponds to step in python's xrange().", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.Rank": {"description": "Returns the rank of a tensor.", "Args": {"input": "A Tensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int32."}, "tf.raw_ops.ReadFile": {"description": "Reads and outputs the entire contents of the input filename.", "Args": {"filename": "A Tensor of type string.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.ReadVariableOp": {"description": "Reads the value of a variable.", "Args": {"resource": "A Tensor of type resource.\nhandle to the resource in which to store the variable.", "dtype": "A tf.DType. the dtype of the value.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dtype."}, "tf.raw_ops.ReadVariableXlaSplitND": {"description": "Splits resource variable input tensor across all dimensions.", "Args": {"resource": "A Tensor of type resource.\nResource variable of input tensor to split across all dimensions.\n  }\n  out_arg {\n    name: \"outputs\"\n    description: <", "T": "A tf.DType.", "N": "An int that is >= 1.", "num_splits": "A list of ints.\nNumber of ways to split per dimension. Shape dimensions must be evenly\ndivisible.", "paddings": "An optional list of ints. Defaults to [].\nOptional list of right paddings per dimension of input tensor to apply before\nsplitting. This can be used to make a dimension evenly divisible.", "name": "A name for the operation (optional)."}, "Returns": "A list of N Tensor objects with type T."}, "tf.raw_ops.ReaderNumRecordsProduced": {"description": "Returns the number of records this Reader has produced.", "Args": {"reader_handle": "A Tensor of type mutable string. Handle to a Reader.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int64."}, "tf.raw_ops.ReaderNumRecordsProducedV2": {"description": "Returns the number of records this Reader has produced.", "Args": {"reader_handle": "A Tensor of type resource. Handle to a Reader.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int64."}, "tf.raw_ops.ReaderNumWorkUnitsCompleted": {"description": "Returns the number of work units this Reader has finished processing.", "Args": {"reader_handle": "A Tensor of type mutable string. Handle to a Reader.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int64."}, "tf.raw_ops.ReaderNumWorkUnitsCompletedV2": {"description": "Returns the number of work units this Reader has finished processing.", "Args": {"reader_handle": "A Tensor of type resource. Handle to a Reader.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int64."}, "tf.raw_ops.ReaderRead": {"description": "Returns the next record (key, value pair) produced by a Reader.", "Args": {"reader_handle": "A Tensor of type mutable string. Handle to a Reader.", "queue_handle": "A Tensor of type mutable string.\nHandle to a Queue, with string work items.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (key, value)."}, "tf.raw_ops.ReaderReadUpTo": {"description": "Returns up to num_records (key, value) pairs produced by a Reader.", "Args": {"reader_handle": "A Tensor of type mutable string. Handle to a Reader.", "queue_handle": "A Tensor of type mutable string.\nHandle to a Queue, with string work items.", "num_records": "A Tensor of type int64.\nnumber of records to read from Reader.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (keys, values)."}, "tf.raw_ops.ReaderReadUpToV2": {"description": "Returns up to num_records (key, value) pairs produced by a Reader.", "Args": {"reader_handle": "A Tensor of type resource. Handle to a Reader.", "queue_handle": "A Tensor of type resource.\nHandle to a Queue, with string work items.", "num_records": "A Tensor of type int64.\nnumber of records to read from Reader.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (keys, values)."}, "tf.raw_ops.ReaderReadV2": {"description": "Returns the next record (key, value pair) produced by a Reader.", "Args": {"reader_handle": "A Tensor of type resource. Handle to a Reader.", "queue_handle": "A Tensor of type resource.\nHandle to a Queue, with string work items.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (key, value)."}, "tf.raw_ops.ReaderReset": {"description": "Restore a Reader to its initial clean state.", "Args": {"reader_handle": "A Tensor of type mutable string. Handle to a Reader.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ReaderResetV2": {"description": "Restore a Reader to its initial clean state.", "Args": {"reader_handle": "A Tensor of type resource. Handle to a Reader.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ReaderRestoreState": {"description": "Restore a reader to a previously saved state.", "Args": {"reader_handle": "A Tensor of type mutable string. Handle to a Reader.", "state": "A Tensor of type string.\nResult of a ReaderSerializeState of a Reader with type\nmatching reader_handle.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ReaderRestoreStateV2": {"description": "Restore a reader to a previously saved state.", "Args": {"reader_handle": "A Tensor of type resource. Handle to a Reader.", "state": "A Tensor of type string.\nResult of a ReaderSerializeState of a Reader with type\nmatching reader_handle.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ReaderSerializeState": {"description": "Produce a string tensor that encodes the state of a Reader.", "Args": {"reader_handle": "A Tensor of type mutable string. Handle to a Reader.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.ReaderSerializeStateV2": {"description": "Produce a string tensor that encodes the state of a Reader.", "Args": {"reader_handle": "A Tensor of type resource. Handle to a Reader.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.Real": {"description": "Returns the real part of a complex number.", "Args": {"input": "A Tensor. Must be one of the following types: complex64, complex128.", "Tout": "An optional tf.DType from: tf.float32, tf.float64. Defaults to tf.float32.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type Tout."}, "tf.raw_ops.RealDiv": {"description": "Returns x / y element-wise for real types.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, uint8, int8, uint16, int16, int32, uint32, uint64, int64, complex64, complex128.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.RebatchDataset": {"description": "Creates a dataset that changes the batch size.", "Args": {"input_dataset": "A Tensor of type variant.\nA variant tensor representing the input dataset.", "num_replicas": "A Tensor of type int64.\nA scalar representing the number of replicas to distribute this batch across. As\na result of this transformation the current batch size would end up being\ndivided  by this parameter.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "use_fallback": "An optional bool. Defaults to True.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.RebatchDatasetV2": {"description": "Creates a dataset that changes the batch size.", "Args": {"input_dataset": "A Tensor of type variant.\nA variant tensor representing the input dataset.", "batch_sizes": "A Tensor of type int64.\nA vector of integers representing the size of batches to produce. These values\nare cycled through in order.", "drop_remainder": "A Tensor of type bool.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.Reciprocal": {"description": "Computes the reciprocal of x element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, int8, int16, int32, int64, complex64, complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.ReciprocalGrad": {"description": "Computes the gradient for the inverse of x wrt its input.", "Args": {"y": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, complex64, complex128.", "dy": "A Tensor. Must have the same type as y.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as y."}, "tf.raw_ops.RecordInput": {"description": "Emits randomized records.", "Args": {"file_pattern": "A string. Glob pattern for the data files.", "file_random_seed": "An optional int. Defaults to 301.\nRandom seeds used to produce randomized records.", "file_shuffle_shift_ratio": "An optional float. Defaults to 0.\nShifts the list of files after the list is randomly\nshuffled.", "file_buffer_size": "An optional int. Defaults to 10000.\nThe randomization shuffling buffer.", "file_parallelism": "An optional int. Defaults to 16.\nHow many sstables are opened and concurrently iterated over.", "batch_size": "An optional int. Defaults to 32. The batch size.", "compression_type": "An optional string. Defaults to \"\".\nThe type of compression for the file. Currently ZLIB and\nGZIP are supported. Defaults to none.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.Recv": {"description": "Receives the named tensor from send_device on recv_device.", "Args": {"tensor_type": "A tf.DType.", "tensor_name": "A string. The name of the tensor to receive.", "send_device": "A string. The name of the device sending the tensor.", "send_device_incarnation": "An int. The current incarnation of send_device.", "recv_device": "A string. The name of the device receiving the tensor.", "client_terminated": "An optional bool. Defaults to False.\nIf set to true, this indicates that the node was added\nto the graph as a result of a client-side feed or fetch of Tensor data,\nin which case the corresponding send or recv is expected to be managed\nlocally by the caller.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type tensor_type."}, "tf.raw_ops.RecvTPUEmbeddingActivations": {"description": "An op that receives embedding activations on the TPU.", "Args": {"num_outputs": "An int that is >= 1.\nThe number of output activation tensors, equal to the number of\nembedding tables in the model.", "config": "A string. Serialized TPUEmbeddingConfiguration proto.", "name": "A name for the operation (optional)."}, "Returns": "A list of num_outputs Tensor objects with type float32."}, "tf.raw_ops.ReduceDataset": {"description": "Reduces the input dataset to a singleton using a reduce function.", "Args": {"input_dataset": "A Tensor of type variant.\nA variant tensor representing the input dataset.", "initial_state": "A list of Tensor objects.\nA nested structure of tensors, representing the initial state of the\ntransformation.", "other_arguments": "A list of Tensor objects.", "f": "A function decorated with @Defun.\nA function that maps (old_state, input_element) to new_state. It must take\ntwo arguments and return a nested structures of tensors. The structure of\nnew_state must match the structure of initial_state.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "use_inter_op_parallelism": "An optional bool. Defaults to True.", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A list of Tensor objects of type output_types."}, "tf.raw_ops.ReduceJoin": {"description": "Joins a string Tensor across the given dimensions.", "Args": {"inputs": "A Tensor of type string.\nThe input to be joined.  All reduced indices must have non-zero size.", "reduction_indices": "A Tensor of type int32.\nThe dimensions to reduce over.  Dimensions are reduced in the\norder specified.  Omitting reduction_indices is equivalent to passing\n[n-1, n-2, ..., 0].  Negative indices from -n to -1 are supported.", "keep_dims": "An optional bool. Defaults to False.\nIf True, retain reduced dimensions with length 1.", "separator": "An optional string. Defaults to \"\".\nThe separator to use when joining.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.RefEnter": {"description": "Creates or finds a child frame, and makes data available to the child frame.", "Args": {"data": "A mutable Tensor.\nThe tensor to be made available to the child frame.", "frame_name": "A string. The name of the child frame.", "is_constant": "An optional bool. Defaults to False.\nIf true, the output is constant within the child frame.", "parallel_iterations": "An optional int. Defaults to 10.\nThe number of iterations allowed to run in parallel.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as data."}, "tf.raw_ops.RefExit": {"description": "Exits the current frame to its parent frame.", "Args": {"data": "A mutable Tensor.\nThe tensor to be made available to the parent frame.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as data."}, "tf.raw_ops.RefIdentity": {"description": "Return the same ref tensor as the input ref tensor.", "Args": {"input": "A mutable Tensor.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as input."}, "tf.raw_ops.RefMerge": {"description": "Forwards the value of an available tensor from inputs to output.", "Args": {"inputs": "A list of at least 1 mutable Tensor objects with the same type.\nThe input tensors, exactly one of which will become available.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output, value_index)."}, "tf.raw_ops.RefNextIteration": {"description": "Makes its input available to the next iteration.", "Args": {"data": "A mutable Tensor.\nThe tensor to be made available to the next iteration.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as data."}, "tf.raw_ops.RefSelect": {"description": "Forwards the indexth element of inputs to output.", "Args": {"index": "A Tensor of type int32.\nA scalar that determines the input that gets selected.", "inputs": "A list of at least 1 mutable Tensor objects with the same type.\nA list of ref tensors, one of which will be forwarded to output.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as inputs."}, "tf.raw_ops.RefSwitch": {"description": "Forwards the ref tensor data to the output port determined by pred.", "Args": {"data": "A mutable Tensor.\nThe ref tensor to be forwarded to the appropriate output.", "pred": "A Tensor of type bool.\nA scalar that specifies which output port will receive data.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output_false, output_true)."}, "tf.raw_ops.RegexFullMatch": {"description": "Check if the input matches the regex pattern.", "Args": {"input": "A Tensor of type string.\nA string tensor of the text to be processed.", "pattern": "A Tensor of type string.\nA scalar string tensor containing the regular expression to match the input.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type bool."}, "tf.raw_ops.RegexReplace": {"description": "Replaces matches of the pattern regular expression in input with the"}, "tf.raw_ops.RegisterDataset": {"description": "Registers a dataset with the tf.data service.", "Args": {"dataset": "A Tensor of type variant.", "address": "A Tensor of type string.", "protocol": "A Tensor of type string.", "external_state_policy": "An int.", "element_spec": "An optional string. Defaults to \"\".", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int64."}, "tf.raw_ops.Relu": {"description": "Computes rectified linear: max(features, 0).", "Args": {"features": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64, qint8.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as features."}, "tf.raw_ops.Relu6": {"description": "Computes rectified linear 6: min(max(features, 0), 6).", "Args": {"features": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as features."}, "tf.raw_ops.Relu6Grad": {"description": "Computes rectified linear 6 gradients for a Relu6 operation.", "Args": {"gradients": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.\nThe backpropagated gradients to the corresponding Relu6 operation.", "features": "A Tensor. Must have the same type as gradients.\nThe features passed as input to the corresponding Relu6 operation, or\nits output; using either one produces the same result.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as gradients."}, "tf.raw_ops.ReluGrad": {"description": "Computes rectified linear gradients for a Relu operation.", "Args": {"gradients": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.\nThe backpropagated gradients to the corresponding Relu operation.", "features": "A Tensor. Must have the same type as gradients.\nThe features passed as input to the corresponding Relu operation, OR\nthe outputs of that operation (both work equivalently).", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as gradients."}, "tf.raw_ops.RemoteCall": {"description": "Runs function f on a remote device indicated by target.", "Args": {"target": "A Tensor of type string.\nA fully specified device name where we want to run the function.", "args": "A list of Tensor objects. A list of arguments for the function.", "Tout": "A list of tf.DTypes that has length >= 1.\nThe type list for the return values.", "f": "A function decorated with @Defun. The function to run remotely.", "name": "A name for the operation (optional)."}, "Returns": "A list of Tensor objects of type Tout."}, "tf.raw_ops.RepeatDataset": {"description": "Creates a dataset that emits the outputs of input_dataset count times.", "Args": {"input_dataset": "A Tensor of type variant.", "count": "A Tensor of type int64.\nA scalar representing the number of times that input_dataset should\nbe repeated. A value of -1 indicates that it should be repeated infinitely.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.RequantizationRange": {"description": "Computes a range that covers the actual values present in a quantized tensor.", "Args": {"input": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.", "input_min": "A Tensor of type float32.\nThe float value that the minimum quantized input value represents.", "input_max": "A Tensor of type float32.\nThe float value that the maximum quantized input value represents.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output_min, output_max)."}, "tf.raw_ops.RequantizationRangePerChannel": {"description": "Computes requantization range per channel.", "Args": {"input": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.\nThe original input tensor.", "input_min": "A Tensor of type float32.\nThe minimum value of the input tensor", "input_max": "A Tensor of type float32.\nThe maximum value of the input tensor.", "clip_value_max": "A float.\nThe maximum value of the output that needs to be clipped.\nExample: set this to 6 for Relu6.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output_min, output_max)."}, "tf.raw_ops.Requantize": {"description": "Converts the quantized input tensor into a lower-precision output.", "Args": {"input": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.", "input_min": "A Tensor of type float32.\nThe float value that the minimum quantized input value represents.", "input_max": "A Tensor of type float32.\nThe float value that the maximum quantized input value represents.", "requested_output_min": "A Tensor of type float32.\nThe float value that the minimum quantized output value represents.", "requested_output_max": "A Tensor of type float32.\nThe float value that the maximum quantized output value represents.", "out_type": "A tf.DType from: tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16.\nThe type of the output. Should be a lower bit depth than Tinput.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output, output_min, output_max)."}, "tf.raw_ops.RequantizePerChannel": {"description": "Requantizes input with min and max values known per channel.", "Args": {"input": "A Tensor. Must be one of the following types: qint8, quint8, qint32, qint16, quint16.\nThe original input tensor.", "input_min": "A Tensor of type float32.\nThe minimum value of the input tensor", "input_max": "A Tensor of type float32.\nThe maximum value of the input tensor.", "requested_output_min": "A Tensor of type float32.\nThe minimum value of the output tensor requested.", "requested_output_max": "A Tensor of type float32.\nThe maximum value of the output tensor requested.", "out_type": "An optional tf.DType from: tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16. Defaults to tf.quint8.\nThe quantized type of output tensor that needs to be converted.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output, output_min, output_max)."}, "tf.raw_ops.Reshape": {"description": "Reshapes a tensor.", "Args": {"tensor": "A Tensor.", "shape": "A Tensor. Must be one of the following types: int32, int64.\nDefines the shape of the output tensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as tensor."}, "tf.raw_ops.ResizeArea": {"description": "Resize images to size using area interpolation.", "Args": {"images": "A Tensor. Must be one of the following types: int8, uint8, int16, uint16, int32, int64, half, float32, float64, bfloat16.\n4-D with shape [batch, height, width, channels].", "size": "A 1-D int32 Tensor of 2 elements: new_height, new_width.  The\nnew size for the images.", "align_corners": "An optional bool. Defaults to False.\nIf true, the centers of the 4 corner pixels of the input and output tensors are\naligned, preserving the values at the corner pixels. Defaults to false.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type float32."}, "tf.raw_ops.ResizeBicubic": {"description": "Resize images to size using bicubic interpolation.", "Args": {"images": "A Tensor. Must be one of the following types: int8, uint8, int16, uint16, int32, int64, half, float32, float64, bfloat16.\n4-D with shape [batch, height, width, channels].", "size": "A 1-D int32 Tensor of 2 elements: new_height, new_width.  The\nnew size for the images.", "align_corners": "An optional bool. Defaults to False.\nIf true, the centers of the 4 corner pixels of the input and output tensors are\naligned, preserving the values at the corner pixels. Defaults to false.", "half_pixel_centers": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type float32."}, "tf.raw_ops.ResizeBicubicGrad": {"description": "Computes the gradient of bicubic interpolation.", "Args": {"grads": "A Tensor of type float32.\n4-D with shape [batch, height, width, channels].", "original_image": "A Tensor. Must be one of the following types: float32, float64.\n4-D with shape [batch, orig_height, orig_width, channels],\nThe image tensor that was resized.", "align_corners": "An optional bool. Defaults to False.\nIf true, the centers of the 4 corner pixels of the input and grad tensors are\naligned. Defaults to false.", "half_pixel_centers": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as original_image."}, "tf.raw_ops.ResizeBilinear": {"description": "Resize images to size using bilinear interpolation.", "Args": {"images": "A Tensor. Must be one of the following types: int8, uint8, int16, uint16, int32, int64, bfloat16, half, float32, float64, bfloat16.\n4-D with shape [batch, height, width, channels].", "size": "A 1-D int32 Tensor of 2 elements: new_height, new_width.  The\nnew size for the images.", "align_corners": "An optional bool. Defaults to False.\nIf true, the centers of the 4 corner pixels of the input and output tensors are\naligned, preserving the values at the corner pixels. Defaults to false.", "half_pixel_centers": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type float32."}, "tf.raw_ops.ResizeBilinearGrad": {"description": "Computes the gradient of bilinear interpolation.", "Args": {"grads": "A Tensor of type float32.\n4-D with shape [batch, height, width, channels].", "original_image": "A Tensor. Must be one of the following types: float32, bfloat16, half, float64.\n4-D with shape [batch, orig_height, orig_width, channels],\nThe image tensor that was resized.", "align_corners": "An optional bool. Defaults to False.\nIf true, the centers of the 4 corner pixels of the input and grad tensors are\naligned. Defaults to false.", "half_pixel_centers": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as original_image."}, "tf.raw_ops.ResizeNearestNeighbor": {"description": "Resize images to size using nearest neighbor interpolation.", "Args": {"images": "A Tensor. Must be one of the following types: int8, uint8, int16, uint16, int32, int64, half, float32, float64, bfloat16.\n4-D with shape [batch, height, width, channels].", "size": "A 1-D int32 Tensor of 2 elements: new_height, new_width.  The\nnew size for the images.", "align_corners": "An optional bool. Defaults to False.\nIf true, the centers of the 4 corner pixels of the input and output tensors are\naligned, preserving the values at the corner pixels. Defaults to false.", "half_pixel_centers": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as images."}, "tf.raw_ops.ResizeNearestNeighborGrad": {"description": "Computes the gradient of nearest neighbor interpolation.", "Args": {"grads": "A Tensor. Must be one of the following types: uint8, int8, int32, half, float32, float64, bfloat16.\n4-D with shape [batch, height, width, channels].", "size": "A 1-D int32 Tensor of 2 elements: orig_height, orig_width. The\noriginal input size.", "align_corners": "An optional bool. Defaults to False.\nIf true, the centers of the 4 corner pixels of the input and grad tensors are\naligned. Defaults to false.", "half_pixel_centers": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as grads."}, "tf.raw_ops.ResourceAccumulatorApplyGradient": {"description": "Applies a gradient to a given accumulator.", "Args": {"handle": "A Tensor of type resource. The handle to a accumulator.", "local_step": "A Tensor of type int64.\nThe local_step value at which the gradient was computed.", "gradient": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nA tensor of the gradient to be accumulated.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ResourceAccumulatorNumAccumulated": {"description": "Returns the number of gradients aggregated in the given accumulators.", "Args": {"handle": "A Tensor of type resource. The handle to an accumulator.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int32."}, "tf.raw_ops.ResourceAccumulatorSetGlobalStep": {"description": "Updates the accumulator with a new value for global_step.", "Args": {"handle": "A Tensor of type resource. The handle to an accumulator.", "new_global_step": "A Tensor of type int64.\nThe new global_step value to set.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ResourceAccumulatorTakeGradient": {"description": "Extracts the average gradient in the given ConditionalAccumulator.", "Args": {"handle": "A Tensor of type resource. The handle to an accumulator.", "num_required": "A Tensor of type int32.\nNumber of gradients required before we return an aggregate.", "dtype": "A tf.DType from: tf.float32, tf.float64, tf.int32, tf.uint8, tf.int16, tf.int8, tf.complex64, tf.int64, tf.qint8, tf.quint8, tf.qint32, tf.bfloat16, tf.uint16, tf.complex128, tf.half, tf.uint32, tf.uint64.\nThe data type of accumulated gradients. Needs to correspond to the type\nof the accumulator.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dtype."}, "tf.raw_ops.ResourceApplyAdaMax": {"description": "Update &#39;*var&#39; according to the AdaMax algorithm.", "Args": {"var": "A Tensor of type resource. Should be from a Variable().", "m": "A Tensor of type resource. Should be from a Variable().", "v": "A Tensor of type resource. Should be from a Variable().", "beta1_power": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nMust be a scalar.", "lr": "A Tensor. Must have the same type as beta1_power.\nScaling factor. Must be a scalar.", "beta1": "A Tensor. Must have the same type as beta1_power.\nMomentum factor. Must be a scalar.", "beta2": "A Tensor. Must have the same type as beta1_power.\nMomentum factor. Must be a scalar.", "epsilon": "A Tensor. Must have the same type as beta1_power.\nRidge term. Must be a scalar.", "grad": "A Tensor. Must have the same type as beta1_power. The gradient.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var, m, and v tensors will be protected\nby a lock; otherwise the behavior is undefined, but may exhibit less\ncontention.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ResourceApplyAdadelta": {"description": "Update &#39;*var&#39; according to the adadelta scheme.", "Args": {"var": "A Tensor of type resource. Should be from a Variable().", "accum": "A Tensor of type resource. Should be from a Variable().", "accum_update": "A Tensor of type resource. Should be from a Variable().", "lr": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nScaling factor. Must be a scalar.", "rho": "A Tensor. Must have the same type as lr.\nDecay factor. Must be a scalar.", "epsilon": "A Tensor. Must have the same type as lr.\nConstant factor. Must be a scalar.", "grad": "A Tensor. Must have the same type as lr. The gradient.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var, accum and update_accum tensors will be protected by\na lock; otherwise the behavior is undefined, but may exhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ResourceApplyAdagrad": {"description": "Update &#39;*var&#39; according to the adagrad scheme.", "Args": {"var": "A Tensor of type resource. Should be from a Variable().", "accum": "A Tensor of type resource. Should be from a Variable().", "lr": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nScaling factor. Must be a scalar.", "grad": "A Tensor. Must have the same type as lr. The gradient.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var and accum tensors will be protected\nby a lock; otherwise the behavior is undefined, but may exhibit less\ncontention.", "update_slots": "An optional bool. Defaults to True.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ResourceApplyAdagradDA": {"description": "Update &#39;*var&#39; according to the proximal adagrad scheme.", "Args": {"var": "A Tensor of type resource. Should be from a Variable().", "gradient_accumulator": "A Tensor of type resource.\nShould be from a Variable().", "gradient_squared_accumulator": "A Tensor of type resource.\nShould be from a Variable().", "grad": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nThe gradient.", "lr": "A Tensor. Must have the same type as grad.\nScaling factor. Must be a scalar.", "l1": "A Tensor. Must have the same type as grad.\nL1 regularization. Must be a scalar.", "l2": "A Tensor. Must have the same type as grad.\nL2 regularization. Must be a scalar.", "global_step": "A Tensor of type int64.\nTraining step number. Must be a scalar.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var and accum tensors will be protected by\na lock; otherwise the behavior is undefined, but may exhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ResourceApplyAdagradV2": {"description": "Update &#39;*var&#39; according to the adagrad scheme.", "Args": {"var": "A Tensor of type resource. Should be from a Variable().", "accum": "A Tensor of type resource. Should be from a Variable().", "lr": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nScaling factor. Must be a scalar.", "epsilon": "A Tensor. Must have the same type as lr.\nConstant factor. Must be a scalar.", "grad": "A Tensor. Must have the same type as lr. The gradient.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var and accum tensors will be protected\nby a lock; otherwise the behavior is undefined, but may exhibit less\ncontention.", "update_slots": "An optional bool. Defaults to True.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ResourceApplyAdam": {"description": "Update &#39;*var&#39; according to the Adam algorithm.", "Args": {"var": "A Tensor of type resource. Should be from a Variable().", "m": "A Tensor of type resource. Should be from a Variable().", "v": "A Tensor of type resource. Should be from a Variable().", "beta1_power": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nMust be a scalar.", "beta2_power": "A Tensor. Must have the same type as beta1_power.\nMust be a scalar.", "lr": "A Tensor. Must have the same type as beta1_power.\nScaling factor. Must be a scalar.", "beta1": "A Tensor. Must have the same type as beta1_power.\nMomentum factor. Must be a scalar.", "beta2": "A Tensor. Must have the same type as beta1_power.\nMomentum factor. Must be a scalar.", "epsilon": "A Tensor. Must have the same type as beta1_power.\nRidge term. Must be a scalar.", "grad": "A Tensor. Must have the same type as beta1_power. The gradient.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var, m, and v tensors will be protected\nby a lock; otherwise the behavior is undefined, but may exhibit less\ncontention.", "use_nesterov": "An optional bool. Defaults to False.\nIf True, uses the nesterov update.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ResourceApplyAdamWithAmsgrad": {"description": "Update &#39;*var&#39; according to the Adam algorithm.", "Args": {"var": "A Tensor of type resource. Should be from a Variable().", "m": "A Tensor of type resource. Should be from a Variable().", "v": "A Tensor of type resource. Should be from a Variable().", "vhat": "A Tensor of type resource. Should be from a Variable().", "beta1_power": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nMust be a scalar.", "beta2_power": "A Tensor. Must have the same type as beta1_power.\nMust be a scalar.", "lr": "A Tensor. Must have the same type as beta1_power.\nScaling factor. Must be a scalar.", "beta1": "A Tensor. Must have the same type as beta1_power.\nMomentum factor. Must be a scalar.", "beta2": "A Tensor. Must have the same type as beta1_power.\nMomentum factor. Must be a scalar.", "epsilon": "A Tensor. Must have the same type as beta1_power.\nRidge term. Must be a scalar.", "grad": "A Tensor. Must have the same type as beta1_power. The gradient.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var, m, and v tensors will be protected\nby a lock; otherwise the behavior is undefined, but may exhibit less\ncontention.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ResourceApplyAddSign": {"description": "Update &#39;*var&#39; according to the AddSign update.", "Args": {"var": "A Tensor of type resource. Should be from a Variable().", "m": "A Tensor of type resource. Should be from a Variable().", "lr": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nScaling factor. Must be a scalar.", "alpha": "A Tensor. Must have the same type as lr. Must be a scalar.", "sign_decay": "A Tensor. Must have the same type as lr. Must be a scalar.", "beta": "A Tensor. Must have the same type as lr. Must be a scalar.", "grad": "A Tensor. Must have the same type as lr. The gradient.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var and m tensors is\nprotected by a lock; otherwise the behavior is undefined, but may exhibit less\ncontention.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ResourceApplyCenteredRMSProp": {"description": "Update &#39;*var&#39; according to the centered RMSProp algorithm.", "Args": {"var": "A Tensor of type resource. Should be from a Variable().", "mg": "A Tensor of type resource. Should be from a Variable().", "ms": "A Tensor of type resource. Should be from a Variable().", "mom": "A Tensor of type resource. Should be from a Variable().", "lr": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nScaling factor. Must be a scalar.", "rho": "A Tensor. Must have the same type as lr.\nDecay rate. Must be a scalar.", "momentum": "A Tensor. Must have the same type as lr.\nMomentum Scale. Must be a scalar.", "epsilon": "A Tensor. Must have the same type as lr.\nRidge term. Must be a scalar.", "grad": "A Tensor. Must have the same type as lr. The gradient.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var, mg, ms, and mom tensors is\nprotected by a lock; otherwise the behavior is undefined, but may exhibit less\ncontention.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ResourceApplyFtrl": {"description": "Update &#39;*var&#39; according to the Ftrl-proximal scheme.", "Args": {"var": "A Tensor of type resource. Should be from a Variable().", "accum": "A Tensor of type resource. Should be from a Variable().", "linear": "A Tensor of type resource. Should be from a Variable().", "grad": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nThe gradient.", "lr": "A Tensor. Must have the same type as grad.\nScaling factor. Must be a scalar.", "l1": "A Tensor. Must have the same type as grad.\nL1 regularization. Must be a scalar.", "l2": "A Tensor. Must have the same type as grad.\nL2 regularization. Must be a scalar.", "lr_power": "A Tensor. Must have the same type as grad.\nScaling factor. Must be a scalar.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var and accum tensors will be protected\nby a lock; otherwise the behavior is undefined, but may exhibit less\ncontention.", "multiply_linear_by_lr": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ResourceApplyFtrlV2": {"description": "Update &#39;*var&#39; according to the Ftrl-proximal scheme.", "Args": {"var": "A Tensor of type resource. Should be from a Variable().", "accum": "A Tensor of type resource. Should be from a Variable().", "linear": "A Tensor of type resource. Should be from a Variable().", "grad": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nThe gradient.", "lr": "A Tensor. Must have the same type as grad.\nScaling factor. Must be a scalar.", "l1": "A Tensor. Must have the same type as grad.\nL1 regularization. Must be a scalar.", "l2": "A Tensor. Must have the same type as grad.\nL2 shrinkage regularization. Must be a scalar.", "l2_shrinkage": "A Tensor. Must have the same type as grad.", "lr_power": "A Tensor. Must have the same type as grad.\nScaling factor. Must be a scalar.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var and accum tensors will be protected\nby a lock; otherwise the behavior is undefined, but may exhibit less\ncontention.", "multiply_linear_by_lr": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ResourceApplyGradientDescent": {"description": "Update &#39;*var&#39; by subtracting &#39;alpha&#39; * &#39;delta&#39; from it.", "Args": {"var": "A Tensor of type resource. Should be from a Variable().", "alpha": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nScaling factor. Must be a scalar.", "delta": "A Tensor. Must have the same type as alpha. The change.", "use_locking": "An optional bool. Defaults to False.\nIf True, the subtraction will be protected by a lock;\notherwise the behavior is undefined, but may exhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ResourceApplyKerasMomentum": {"description": "Update &#39;*var&#39; according to the momentum scheme.", "Args": {"var": "A Tensor of type resource. Should be from a Variable().", "accum": "A Tensor of type resource. Should be from a Variable().", "lr": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nScaling factor. Must be a scalar.", "grad": "A Tensor. Must have the same type as lr. The gradient.", "momentum": "A Tensor. Must have the same type as lr.\nMomentum. Must be a scalar.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var and accum tensors will be protected\nby a lock; otherwise the behavior is undefined, but may exhibit less\ncontention.", "use_nesterov": "An optional bool. Defaults to False.\nIf True, the tensor passed to compute grad will be\nvar + momentum * accum, so in the end, the var you get is actually\nvar + momentum * accum.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ResourceApplyMomentum": {"description": "Update &#39;*var&#39; according to the momentum scheme.", "Args": {"var": "A Tensor of type resource. Should be from a Variable().", "accum": "A Tensor of type resource. Should be from a Variable().", "lr": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nScaling factor. Must be a scalar.", "grad": "A Tensor. Must have the same type as lr. The gradient.", "momentum": "A Tensor. Must have the same type as lr.\nMomentum. Must be a scalar.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var and accum tensors will be protected\nby a lock; otherwise the behavior is undefined, but may exhibit less\ncontention.", "use_nesterov": "An optional bool. Defaults to False.\nIf True, the tensor passed to compute grad will be\nvar - lr * momentum * accum, so in the end, the var you get is actually\nvar - lr * momentum * accum.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ResourceApplyPowerSign": {"description": "Update &#39;*var&#39; according to the AddSign update.", "Args": {"var": "A Tensor of type resource. Should be from a Variable().", "m": "A Tensor of type resource. Should be from a Variable().", "lr": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nScaling factor. Must be a scalar.", "logbase": "A Tensor. Must have the same type as lr. Must be a scalar.", "sign_decay": "A Tensor. Must have the same type as lr. Must be a scalar.", "beta": "A Tensor. Must have the same type as lr. Must be a scalar.", "grad": "A Tensor. Must have the same type as lr. The gradient.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var and m tensors is\nprotected by a lock; otherwise the behavior is undefined, but may exhibit less\ncontention.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ResourceApplyProximalAdagrad": {"description": "Update &#39;*var&#39; and &#39;*accum&#39; according to FOBOS with Adagrad learning rate.", "Args": {"var": "A Tensor of type resource. Should be from a Variable().", "accum": "A Tensor of type resource. Should be from a Variable().", "lr": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nScaling factor. Must be a scalar.", "l1": "A Tensor. Must have the same type as lr.\nL1 regularization. Must be a scalar.", "l2": "A Tensor. Must have the same type as lr.\nL2 regularization. Must be a scalar.", "grad": "A Tensor. Must have the same type as lr. The gradient.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var and accum tensors will be protected by\na lock; otherwise the behavior is undefined, but may exhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ResourceApplyProximalGradientDescent": {"description": "Update &#39;*var&#39; as FOBOS algorithm with fixed learning rate.", "Args": {"var": "A Tensor of type resource. Should be from a Variable().", "alpha": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nScaling factor. Must be a scalar.", "l1": "A Tensor. Must have the same type as alpha.\nL1 regularization. Must be a scalar.", "l2": "A Tensor. Must have the same type as alpha.\nL2 regularization. Must be a scalar.", "delta": "A Tensor. Must have the same type as alpha. The change.", "use_locking": "An optional bool. Defaults to False.\nIf True, the subtraction will be protected by a lock;\notherwise the behavior is undefined, but may exhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ResourceApplyRMSProp": {"description": "Update &#39;*var&#39; according to the RMSProp algorithm.", "Args": {"var": "A Tensor of type resource. Should be from a Variable().", "ms": "A Tensor of type resource. Should be from a Variable().", "mom": "A Tensor of type resource. Should be from a Variable().", "lr": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nScaling factor. Must be a scalar.", "rho": "A Tensor. Must have the same type as lr.\nDecay rate. Must be a scalar.", "momentum": "A Tensor. Must have the same type as lr.", "epsilon": "A Tensor. Must have the same type as lr.\nRidge term. Must be a scalar.", "grad": "A Tensor. Must have the same type as lr. The gradient.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var, ms, and mom tensors is protected\nby a lock; otherwise the behavior is undefined, but may exhibit less\ncontention.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ResourceConditionalAccumulator": {"description": "A conditional accumulator for aggregating gradients.", "Args": {"dtype": "A tf.DType from: tf.float32, tf.float64, tf.int32, tf.uint8, tf.int16, tf.int8, tf.complex64, tf.int64, tf.qint8, tf.quint8, tf.qint32, tf.bfloat16, tf.uint16, tf.complex128, tf.half, tf.uint32, tf.uint64.\nThe type of the value being accumulated.", "shape": "A tf.TensorShape or list of ints.\nThe shape of the values, can be [], in which case shape is unknown.", "container": "An optional string. Defaults to \"\".\nIf non-empty, this accumulator is placed in the given container.\nOtherwise, a default container is used.", "shared_name": "An optional string. Defaults to \"\".\nIf non-empty, this accumulator will be shared under the\ngiven name across multiple sessions.", "reduction_type": "An optional string from: \"MEAN\", \"SUM\". Defaults to \"MEAN\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type resource."}, "tf.raw_ops.ResourceCountUpTo": {"description": "Increments variable pointed to by &#39;resource&#39; until it reaches &#39;limit&#39;.", "Args": {"resource": "A Tensor of type resource.\nShould be from a scalar Variable node.", "limit": "An int.\nIf incrementing ref would bring it above limit, instead generates an\n'OutOfRange' error.", "T": "A tf.DType from: tf.int32, tf.int64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type T."}, "tf.raw_ops.ResourceGather": {"description": "Gather slices from the variable pointed to by resource according to indices.", "Args": {"resource": "A Tensor of type resource.", "indices": "A Tensor. Must be one of the following types: int32, int64.", "dtype": "A tf.DType.", "batch_dims": "An optional int. Defaults to 0.", "validate_indices": "An optional bool. Defaults to True.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dtype."}, "tf.raw_ops.ResourceGatherNd": {"Args": {"resource": "A Tensor of type resource.", "indices": "A Tensor. Must be one of the following types: int32, int64.", "dtype": "A tf.DType.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dtype."}, "tf.raw_ops.ResourceScatterAdd": {"description": "Adds sparse updates to the variable referenced by resource.", "Args": {"resource": "A Tensor of type resource. Should be from a Variable node.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA tensor of indices into the first dimension of ref.", "updates": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nA tensor of updated values to add to ref.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ResourceScatterDiv": {"description": "Divides sparse updates into the variable referenced by resource.", "Args": {"resource": "A Tensor of type resource. Should be from a Variable node.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA tensor of indices into the first dimension of ref.", "updates": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nA tensor of updated values to add to ref.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ResourceScatterMax": {"description": "Reduces sparse updates into the variable referenced by resource using the max operation.", "Args": {"resource": "A Tensor of type resource. Should be from a Variable node.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA tensor of indices into the first dimension of ref.", "updates": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nA tensor of updated values to add to ref.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ResourceScatterMin": {"description": "Reduces sparse updates into the variable referenced by resource using the min operation.", "Args": {"resource": "A Tensor of type resource. Should be from a Variable node.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA tensor of indices into the first dimension of ref.", "updates": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nA tensor of updated values to add to ref.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ResourceScatterMul": {"description": "Multiplies sparse updates into the variable referenced by resource.", "Args": {"resource": "A Tensor of type resource. Should be from a Variable node.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA tensor of indices into the first dimension of ref.", "updates": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nA tensor of updated values to add to ref.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ResourceScatterNdAdd": {"description": "Applies sparse addition to individual values or slices in a Variable.", "Args": {"ref": "A Tensor of type resource.\nA resource handle. Must be from a VarHandleOp.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA Tensor. Must be one of the following types: int32, int64.\nA tensor of indices into ref.", "updates": "A Tensor. A Tensor. Must have the same type as ref. A tensor of\nvalues to add to ref.", "use_locking": "An optional bool. Defaults to True.\nAn optional bool. Defaults to True. If True, the assignment will\nbe protected by a lock; otherwise the behavior is undefined,\nbut may exhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ResourceScatterNdMax": {"Args": {"ref": "A Tensor of type resource.\nA resource handle. Must be from a VarHandleOp.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA Tensor. Must be one of the following types: int32, int64.\nA tensor of indices into ref.", "updates": "A Tensor. A Tensor. Must have the same type as ref. A tensor of\nvalues whose element wise max is taken with ref", "use_locking": "An optional bool. Defaults to True.\nAn optional bool. Defaults to True. If True, the assignment will\nbe protected by a lock; otherwise the behavior is undefined,\nbut may exhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ResourceScatterNdMin": {"Args": {"ref": "A Tensor of type resource.\nA resource handle. Must be from a VarHandleOp.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA Tensor. Must be one of the following types: int32, int64.\nA tensor of indices into ref.", "updates": "A Tensor. A Tensor. Must have the same type as ref. A tensor of\nvalues whose element wise min is taken with ref.", "use_locking": "An optional bool. Defaults to True.\nAn optional bool. Defaults to True. If True, the assignment will\nbe protected by a lock; otherwise the behavior is undefined,\nbut may exhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ResourceScatterNdSub": {"description": "Applies sparse subtraction to individual values or slices in a Variable.", "Args": {"ref": "A Tensor of type resource.\nA resource handle. Must be from a VarHandleOp.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA Tensor. Must be one of the following types: int32, int64.\nA tensor of indices into ref.", "updates": "A Tensor. A Tensor. Must have the same type as ref. A tensor of\nvalues to add to ref.", "use_locking": "An optional bool. Defaults to True.\nAn optional bool. Defaults to True. If True, the assignment will\nbe protected by a lock; otherwise the behavior is undefined,\nbut may exhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ResourceScatterNdUpdate": {"description": "Applies sparse updates to individual values or slices within a given", "Args": {"ref": "A Tensor of type resource.\nA resource handle. Must be from a VarHandleOp.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA Tensor. Must be one of the following types: int32, int64.\nA tensor of indices into ref.", "updates": "A Tensor.\nA Tensor. Must have the same type as ref. A tensor of updated\nvalues to add to ref.", "use_locking": "An optional bool. Defaults to True.\nAn optional bool. Defaults to True. If True, the assignment will\nbe protected by a lock; otherwise the behavior is undefined,\nbut may exhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ResourceScatterSub": {"description": "Subtracts sparse updates from the variable referenced by resource.", "Args": {"resource": "A Tensor of type resource. Should be from a Variable node.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA tensor of indices into the first dimension of ref.", "updates": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nA tensor of updated values to add to ref.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ResourceScatterUpdate": {"description": "Assigns sparse updates to the variable referenced by resource.", "Args": {"resource": "A Tensor of type resource. Should be from a Variable node.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA tensor of indices into the first dimension of ref.", "updates": "A Tensor. A tensor of updated values to add to ref.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ResourceSparseApplyAdadelta": {"description": "var: Should be from a Variable().", "Args": {"var": "A Tensor of type resource.", "accum": "A Tensor of type resource. Should be from a Variable().", "accum_update": "A Tensor of type resource.\nShould be from a Variable().", "lr": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nLearning rate. Must be a scalar.", "rho": "A Tensor. Must have the same type as lr.\nDecay factor. Must be a scalar.", "epsilon": "A Tensor. Must have the same type as lr.\nConstant factor. Must be a scalar.", "grad": "A Tensor. Must have the same type as lr. The gradient.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA vector of indices into the first dimension of var and accum.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var and accum tensors will be protected by\na lock; otherwise the behavior is undefined, but may exhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ResourceSparseApplyAdagrad": {"description": "Update relevant entries in &#39;*var&#39; and &#39;*accum&#39; according to the adagrad scheme.", "Args": {"var": "A Tensor of type resource. Should be from a Variable().", "accum": "A Tensor of type resource. Should be from a Variable().", "lr": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nLearning rate. Must be a scalar.", "grad": "A Tensor. Must have the same type as lr. The gradient.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA vector of indices into the first dimension of var and accum.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var and accum tensors will be protected\nby a lock; otherwise the behavior is undefined, but may exhibit less\ncontention.", "update_slots": "An optional bool. Defaults to True.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ResourceSparseApplyAdagradDA": {"description": "Update entries in &#39;*var&#39; and &#39;*accum&#39; according to the proximal adagrad scheme.", "Args": {"var": "A Tensor of type resource. Should be from a Variable().", "gradient_accumulator": "A Tensor of type resource.\nShould be from a Variable().", "gradient_squared_accumulator": "A Tensor of type resource.\nShould be from a Variable().", "grad": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nThe gradient.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA vector of indices into the first dimension of var and accum.", "lr": "A Tensor. Must have the same type as grad.\nLearning rate. Must be a scalar.", "l1": "A Tensor. Must have the same type as grad.\nL1 regularization. Must be a scalar.", "l2": "A Tensor. Must have the same type as grad.\nL2 regularization. Must be a scalar.", "global_step": "A Tensor of type int64.\nTraining step number. Must be a scalar.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var and accum tensors will be protected by\na lock; otherwise the behavior is undefined, but may exhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ResourceSparseApplyAdagradV2": {"description": "Update relevant entries in &#39;*var&#39; and &#39;*accum&#39; according to the adagrad scheme.", "Args": {"var": "A Tensor of type resource. Should be from a Variable().", "accum": "A Tensor of type resource. Should be from a Variable().", "lr": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nLearning rate. Must be a scalar.", "epsilon": "A Tensor. Must have the same type as lr.\nConstant factor. Must be a scalar.", "grad": "A Tensor. Must have the same type as lr. The gradient.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA vector of indices into the first dimension of var and accum.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var and accum tensors will be protected\nby a lock; otherwise the behavior is undefined, but may exhibit less\ncontention.", "update_slots": "An optional bool. Defaults to True.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ResourceSparseApplyCenteredRMSProp": {"description": "Update &#39;*var&#39; according to the centered RMSProp algorithm.", "Args": {"var": "A Tensor of type resource. Should be from a Variable().", "mg": "A Tensor of type resource. Should be from a Variable().", "ms": "A Tensor of type resource. Should be from a Variable().", "mom": "A Tensor of type resource. Should be from a Variable().", "lr": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nScaling factor. Must be a scalar.", "rho": "A Tensor. Must have the same type as lr.\nDecay rate. Must be a scalar.", "momentum": "A Tensor. Must have the same type as lr.", "epsilon": "A Tensor. Must have the same type as lr.\nRidge term. Must be a scalar.", "grad": "A Tensor. Must have the same type as lr. The gradient.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA vector of indices into the first dimension of var, ms and mom.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var, mg, ms, and mom tensors is\nprotected by a lock; otherwise the behavior is undefined, but may exhibit less\ncontention.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ResourceSparseApplyFtrl": {"description": "Update relevant entries in &#39;*var&#39; according to the Ftrl-proximal scheme.", "Args": {"var": "A Tensor of type resource. Should be from a Variable().", "accum": "A Tensor of type resource. Should be from a Variable().", "linear": "A Tensor of type resource. Should be from a Variable().", "grad": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nThe gradient.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA vector of indices into the first dimension of var and accum.", "lr": "A Tensor. Must have the same type as grad.\nScaling factor. Must be a scalar.", "l1": "A Tensor. Must have the same type as grad.\nL1 regularization. Must be a scalar.", "l2": "A Tensor. Must have the same type as grad.\nL2 regularization. Must be a scalar.", "lr_power": "A Tensor. Must have the same type as grad.\nScaling factor. Must be a scalar.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var and accum tensors will be protected\nby a lock; otherwise the behavior is undefined, but may exhibit less\ncontention.", "multiply_linear_by_lr": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ResourceSparseApplyFtrlV2": {"description": "Update relevant entries in &#39;*var&#39; according to the Ftrl-proximal scheme.", "Args": {"var": "A Tensor of type resource. Should be from a Variable().", "accum": "A Tensor of type resource. Should be from a Variable().", "linear": "A Tensor of type resource. Should be from a Variable().", "grad": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nThe gradient.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA vector of indices into the first dimension of var and accum.", "lr": "A Tensor. Must have the same type as grad.\nScaling factor. Must be a scalar.", "l1": "A Tensor. Must have the same type as grad.\nL1 regularization. Must be a scalar.", "l2": "A Tensor. Must have the same type as grad.\nL2 shrinkage regularization. Must be a scalar.", "l2_shrinkage": "A Tensor. Must have the same type as grad.", "lr_power": "A Tensor. Must have the same type as grad.\nScaling factor. Must be a scalar.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var and accum tensors will be protected\nby a lock; otherwise the behavior is undefined, but may exhibit less\ncontention.", "multiply_linear_by_lr": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ResourceSparseApplyKerasMomentum": {"description": "Update relevant entries in &#39;*var&#39; and &#39;*accum&#39; according to the momentum scheme.", "Args": {"var": "A Tensor of type resource. Should be from a Variable().", "accum": "A Tensor of type resource. Should be from a Variable().", "lr": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nLearning rate. Must be a scalar.", "grad": "A Tensor. Must have the same type as lr. The gradient.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA vector of indices into the first dimension of var and accum.", "momentum": "A Tensor. Must have the same type as lr.\nMomentum. Must be a scalar.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var and accum tensors will be protected\nby a lock; otherwise the behavior is undefined, but may exhibit less\ncontention.", "use_nesterov": "An optional bool. Defaults to False.\nIf True, the tensor passed to compute grad will be\nvar + momentum * accum, so in the end, the var you get is actually\nvar + momentum * accum.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ResourceSparseApplyMomentum": {"description": "Update relevant entries in &#39;*var&#39; and &#39;*accum&#39; according to the momentum scheme.", "Args": {"var": "A Tensor of type resource. Should be from a Variable().", "accum": "A Tensor of type resource. Should be from a Variable().", "lr": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nLearning rate. Must be a scalar.", "grad": "A Tensor. Must have the same type as lr. The gradient.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA vector of indices into the first dimension of var and accum.", "momentum": "A Tensor. Must have the same type as lr.\nMomentum. Must be a scalar.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var and accum tensors will be protected\nby a lock; otherwise the behavior is undefined, but may exhibit less\ncontention.", "use_nesterov": "An optional bool. Defaults to False.\nIf True, the tensor passed to compute grad will be\nvar - lr * momentum * accum, so in the end, the var you get is actually\nvar - lr * momentum * accum.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ResourceSparseApplyProximalAdagrad": {"description": "Sparse update entries in &#39;*var&#39; and &#39;*accum&#39; according to FOBOS algorithm.", "Args": {"var": "A Tensor of type resource. Should be from a Variable().", "accum": "A Tensor of type resource. Should be from a Variable().", "lr": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nLearning rate. Must be a scalar.", "l1": "A Tensor. Must have the same type as lr.\nL1 regularization. Must be a scalar.", "l2": "A Tensor. Must have the same type as lr.\nL2 regularization. Must be a scalar.", "grad": "A Tensor. Must have the same type as lr. The gradient.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA vector of indices into the first dimension of var and accum.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var and accum tensors will be protected by\na lock; otherwise the behavior is undefined, but may exhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ResourceSparseApplyProximalGradientDescent": {"description": "Sparse update &#39;*var&#39; as FOBOS algorithm with fixed learning rate.", "Args": {"var": "A Tensor of type resource. Should be from a Variable().", "alpha": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nScaling factor. Must be a scalar.", "l1": "A Tensor. Must have the same type as alpha.\nL1 regularization. Must be a scalar.", "l2": "A Tensor. Must have the same type as alpha.\nL2 regularization. Must be a scalar.", "grad": "A Tensor. Must have the same type as alpha. The gradient.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA vector of indices into the first dimension of var and accum.", "use_locking": "An optional bool. Defaults to False.\nIf True, the subtraction will be protected by a lock;\notherwise the behavior is undefined, but may exhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ResourceSparseApplyRMSProp": {"description": "Update &#39;*var&#39; according to the RMSProp algorithm.", "Args": {"var": "A Tensor of type resource. Should be from a Variable().", "ms": "A Tensor of type resource. Should be from a Variable().", "mom": "A Tensor of type resource. Should be from a Variable().", "lr": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nScaling factor. Must be a scalar.", "rho": "A Tensor. Must have the same type as lr.\nDecay rate. Must be a scalar.", "momentum": "A Tensor. Must have the same type as lr.", "epsilon": "A Tensor. Must have the same type as lr.\nRidge term. Must be a scalar.", "grad": "A Tensor. Must have the same type as lr. The gradient.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA vector of indices into the first dimension of var, ms and mom.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var, ms, and mom tensors is protected\nby a lock; otherwise the behavior is undefined, but may exhibit less\ncontention.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ResourceStridedSliceAssign": {"description": "Assign value to the sliced l-value reference of ref.", "Args": {"ref": "A Tensor of type resource.", "begin": "A Tensor. Must be one of the following types: int32, int64.", "end": "A Tensor. Must have the same type as begin.", "strides": "A Tensor. Must have the same type as begin.", "value": "A Tensor.", "begin_mask": "An optional int. Defaults to 0.", "end_mask": "An optional int. Defaults to 0.", "ellipsis_mask": "An optional int. Defaults to 0.", "new_axis_mask": "An optional int. Defaults to 0.", "shrink_axis_mask": "An optional int. Defaults to 0.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.Restore": {"description": "Restores a tensor from checkpoint files.", "Args": {"file_pattern": "A Tensor of type string.\nMust have a single element. The pattern of the files from\nwhich we read the tensor.", "tensor_name": "A Tensor of type string.\nMust have a single element. The name of the tensor to be\nrestored.", "dt": "A tf.DType. The type of the tensor to be restored.", "preferred_shard": "An optional int. Defaults to -1.\nIndex of file to open first if multiple files match\nfile_pattern.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dt."}, "tf.raw_ops.RestoreSlice": {"description": "Restores a tensor from checkpoint files.", "Args": {"file_pattern": "A Tensor of type string.\nMust have a single element. The pattern of the files from\nwhich we read the tensor.", "tensor_name": "A Tensor of type string.\nMust have a single element. The name of the tensor to be\nrestored.", "shape_and_slice": "A Tensor of type string.\nScalar. The shapes and slice specifications to use when\nrestoring a tensors.", "dt": "A tf.DType. The type of the tensor to be restored.", "preferred_shard": "An optional int. Defaults to -1.\nIndex of file to open first if multiple files match\nfile_pattern. See the documentation for Restore.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dt."}, "tf.raw_ops.RestoreV2": {"description": "Restores tensors from a V2 checkpoint.", "Args": {"prefix": "A Tensor of type string.\nMust have a single element.  The prefix of a V2 checkpoint.", "tensor_names": "A Tensor of type string.\nshape {N}.  The names of the tensors to be restored.", "shape_and_slices": "A Tensor of type string.\nshape {N}.  The slice specs of the tensors to be restored.\nEmpty strings indicate that they are non-partitioned tensors.", "dtypes": "A list of tf.DTypes that has length >= 1.\nshape {N}.  The list of expected dtype for the tensors.  Must match\nthose stored in the checkpoint.", "name": "A name for the operation (optional)."}, "Returns": "A list of Tensor objects of type dtypes."}, "tf.raw_ops.RetrieveTPUEmbeddingADAMParameters": {"description": "Retrieve ADAM embedding parameters.", "Args": {"num_shards": "An int.", "shard_id": "An int.", "table_id": "An optional int. Defaults to -1.", "table_name": "An optional string. Defaults to \"\".", "config": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (parameters, momenta, velocities)."}, "tf.raw_ops.RetrieveTPUEmbeddingAdadeltaParameters": {"description": "Retrieve Adadelta embedding parameters.", "Args": {"num_shards": "An int.", "shard_id": "An int.", "table_id": "An optional int. Defaults to -1.", "table_name": "An optional string. Defaults to \"\".", "config": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (parameters, accumulators, updates)."}, "tf.raw_ops.RetrieveTPUEmbeddingAdagradMomentumParameters": {"description": "Retrieve Adagrad Momentum embedding parameters.", "Args": {"num_shards": "An int.", "shard_id": "An int.", "table_id": "An optional int. Defaults to -1.", "table_name": "An optional string. Defaults to \"\".", "config": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (parameters, accumulators, momenta)."}, "tf.raw_ops.RetrieveTPUEmbeddingAdagradParameters": {"description": "Retrieve Adagrad embedding parameters.", "Args": {"num_shards": "An int.", "shard_id": "An int.", "table_id": "An optional int. Defaults to -1.", "table_name": "An optional string. Defaults to \"\".", "config": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (parameters, accumulators)."}, "tf.raw_ops.RetrieveTPUEmbeddingCenteredRMSPropParameters": {"description": "Retrieve centered RMSProp embedding parameters.", "Args": {"num_shards": "An int.", "shard_id": "An int.", "table_id": "An optional int. Defaults to -1.", "table_name": "An optional string. Defaults to \"\".", "config": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (parameters, ms, mom, mg)."}, "tf.raw_ops.RetrieveTPUEmbeddingFTRLParameters": {"description": "Retrieve FTRL embedding parameters.", "Args": {"num_shards": "An int.", "shard_id": "An int.", "table_id": "An optional int. Defaults to -1.", "table_name": "An optional string. Defaults to \"\".", "config": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (parameters, accumulators, linears)."}, "tf.raw_ops.RetrieveTPUEmbeddingFrequencyEstimatorParameters": {"description": "Retrieve frequency estimator embedding parameters.", "Args": {"num_shards": "An int.", "shard_id": "An int.", "table_id": "An optional int. Defaults to -1.", "table_name": "An optional string. Defaults to \"\".", "config": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (parameters, last_hit_step)."}, "tf.raw_ops.RetrieveTPUEmbeddingMDLAdagradLightParameters": {"description": "Retrieve MDL Adagrad Light embedding parameters.", "Args": {"num_shards": "An int.", "shard_id": "An int.", "table_id": "An optional int. Defaults to -1.", "table_name": "An optional string. Defaults to \"\".", "config": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (parameters, accumulators, weights, benefits)."}, "tf.raw_ops.RetrieveTPUEmbeddingMomentumParameters": {"description": "Retrieve Momentum embedding parameters.", "Args": {"num_shards": "An int.", "shard_id": "An int.", "table_id": "An optional int. Defaults to -1.", "table_name": "An optional string. Defaults to \"\".", "config": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (parameters, momenta)."}, "tf.raw_ops.RetrieveTPUEmbeddingProximalAdagradParameters": {"description": "Retrieve proximal Adagrad embedding parameters.", "Args": {"num_shards": "An int.", "shard_id": "An int.", "table_id": "An optional int. Defaults to -1.", "table_name": "An optional string. Defaults to \"\".", "config": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (parameters, accumulators)."}, "tf.raw_ops.RetrieveTPUEmbeddingProximalYogiParameters": {"Args": {"num_shards": "An int.", "shard_id": "An int.", "table_id": "An optional int. Defaults to -1.", "table_name": "An optional string. Defaults to \"\".", "config": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (parameters, v, m)."}, "tf.raw_ops.RetrieveTPUEmbeddingRMSPropParameters": {"description": "Retrieve RMSProp embedding parameters.", "Args": {"num_shards": "An int.", "shard_id": "An int.", "table_id": "An optional int. Defaults to -1.", "table_name": "An optional string. Defaults to \"\".", "config": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (parameters, ms, mom)."}, "tf.raw_ops.RetrieveTPUEmbeddingStochasticGradientDescentParameters": {"description": "Retrieve SGD embedding parameters.", "Args": {"num_shards": "An int.", "shard_id": "An int.", "table_id": "An optional int. Defaults to -1.", "table_name": "An optional string. Defaults to \"\".", "config": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type float32."}, "tf.raw_ops.Reverse": {"description": "Reverses specific dimensions of a tensor.", "Args": {"tensor": "A Tensor. Must be one of the following types: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, bfloat16, half, float32, float64, complex64, complex128, string.\nUp to 8-D.", "dims": "A Tensor of type bool. 1-D. The dimensions to reverse.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as tensor."}, "tf.raw_ops.ReverseSequence": {"description": "Reverses variable length slices.", "Args": {"input": "A Tensor. The input to reverse.", "seq_lengths": "A Tensor. Must be one of the following types: int32, int64.\n1-D with length input.dims(batch_dim) and\nmax(seq_lengths) <= input.dims(seq_dim)", "seq_dim": "An int. The dimension which is partially reversed.", "batch_dim": "An optional int. Defaults to 0.\nThe dimension along which reversal is performed.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.ReverseV2": {"description": "Reverses specific dimensions of a tensor.", "Args": {"tensor": "A Tensor. Must be one of the following types: uint8, int8, uint16, int16, int32, uint32, int64, uint64, bool, bfloat16, half, float32, float64, complex64, complex128, string.\nUp to 8-D.", "axis": "A Tensor. Must be one of the following types: int32, int64.\n1-D. The indices of the dimensions to reverse. Must be in the range\n[-rank(tensor), rank(tensor)).", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as tensor."}, "tf.raw_ops.RewriteDataset": {"Args": {"input_dataset": "A Tensor of type variant.", "rewrite_name": "A Tensor of type string.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.RightShift": {"description": "Elementwise computes the bitwise right-shift of x and y.", "Args": {"x": "A Tensor. Must be one of the following types: int8, int16, int32, int64, uint8, uint16, uint32, uint64.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.Rint": {"description": "Returns element-wise integer closest to x.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.RngReadAndSkip": {"description": "Advance the counter of a counter-based RNG.", "Args": {"resource": "A Tensor of type resource.\nThe handle of the resource variable that stores the state of the RNG.", "alg": "A Tensor of type int32. The RNG algorithm.", "delta": "A Tensor of type uint64. The amount of advancement.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int64."}, "tf.raw_ops.RngSkip": {"description": "Advance the counter of a counter-based RNG.", "Args": {"resource": "A Tensor of type resource.\nThe handle of the resource variable that stores the state of the RNG.", "algorithm": "A Tensor of type int64. The RNG algorithm.", "delta": "A Tensor of type int64. The amount of advancement.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.Roll": {"description": "Rolls the elements of a tensor along an axis.", "Args": {"input": "A Tensor.", "shift": "A Tensor. Must be one of the following types: int32, int64.\nDimension must be 0-D or 1-D. shift[i] specifies the number of places by which\nelements are shifted positively (towards larger indices) along the dimension\nspecified by axis[i]. Negative shifts will roll the elements in the opposite\ndirection.", "axis": "A Tensor. Must be one of the following types: int32, int64.\nDimension must be 0-D or 1-D. axis[i] specifies the dimension that the shift\nshift[i] should occur. If the same axis is referenced more than once, the\ntotal shift for that axis will be the sum of all the shifts that belong to that\naxis.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.Round": {"description": "Rounds the values of a tensor to the nearest integer, element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, int8, int16, int32, int64, complex64, complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.Rsqrt": {"description": "Computes reciprocal of square root of x element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, complex64, complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.RsqrtGrad": {"description": "Computes the gradient for the rsqrt of x wrt its input.", "Args": {"y": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, complex64, complex128.", "dy": "A Tensor. Must have the same type as y.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as y."}, "tf.raw_ops.SampleDistortedBoundingBox": {"description": "Generate a single randomly distorted bounding box for an image.", "Args": {"image_size": "A Tensor. Must be one of the following types: uint8, int8, int16, int32, int64.\n1-D, containing [height, width, channels].", "bounding_boxes": "A Tensor of type float32.\n3-D with shape [batch, N, 4] describing the N bounding boxes\nassociated with the image.", "seed": "An optional int. Defaults to 0.\nIf either seed or seed2 are set to non-zero, the random number\ngenerator is seeded by the given seed.  Otherwise, it is seeded by a random\nseed.", "seed2": "An optional int. Defaults to 0.\nA second seed to avoid seed collision.", "min_object_covered": "An optional float. Defaults to 0.1.\nThe cropped area of the image must contain at least this\nfraction of any bounding box supplied. The value of this parameter should be\nnon-negative. In the case of 0, the cropped area does not need to overlap\nany of the bounding boxes supplied.", "aspect_ratio_range": "An optional list of floats. Defaults to [0.75, 1.33].\nThe cropped area of the image must have an aspect ratio =\nwidth / height within this range.", "area_range": "An optional list of floats. Defaults to [0.05, 1].\nThe cropped area of the image must contain a fraction of the\nsupplied image within this range.", "max_attempts": "An optional int. Defaults to 100.\nNumber of attempts at generating a cropped region of the image\nof the specified constraints. After max_attempts failures, return the entire\nimage.", "use_image_if_no_bounding_boxes": "An optional bool. Defaults to False.\nControls behavior if no bounding boxes supplied.\nIf true, assume an implicit bounding box covering the whole input. If false,\nraise an error.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (begin, size, bboxes)."}, "tf.raw_ops.SampleDistortedBoundingBoxV2": {"description": "Generate a single randomly distorted bounding box for an image.", "Args": {"image_size": "A Tensor. Must be one of the following types: uint8, int8, int16, int32, int64.\n1-D, containing [height, width, channels].", "bounding_boxes": "A Tensor of type float32.\n3-D with shape [batch, N, 4] describing the N bounding boxes\nassociated with the image.", "min_object_covered": "A Tensor of type float32.\nThe cropped area of the image must contain at least this\nfraction of any bounding box supplied. The value of this parameter should be\nnon-negative. In the case of 0, the cropped area does not need to overlap\nany of the bounding boxes supplied.", "seed": "An optional int. Defaults to 0.\nIf either seed or seed2 are set to non-zero, the random number\ngenerator is seeded by the given seed.  Otherwise, it is seeded by a random\nseed.", "seed2": "An optional int. Defaults to 0.\nA second seed to avoid seed collision.", "aspect_ratio_range": "An optional list of floats. Defaults to [0.75, 1.33].\nThe cropped area of the image must have an aspect ratio =\nwidth / height within this range.", "area_range": "An optional list of floats. Defaults to [0.05, 1].\nThe cropped area of the image must contain a fraction of the\nsupplied image within this range.", "max_attempts": "An optional int. Defaults to 100.\nNumber of attempts at generating a cropped region of the image\nof the specified constraints. After max_attempts failures, return the entire\nimage.", "use_image_if_no_bounding_boxes": "An optional bool. Defaults to False.\nControls behavior if no bounding boxes supplied.\nIf true, assume an implicit bounding box covering the whole input. If false,\nraise an error.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (begin, size, bboxes)."}, "tf.raw_ops.SamplingDataset": {"description": "Creates a dataset that takes a Bernoulli sample of the contents of another dataset.", "Args": {"input_dataset": "A Tensor of type variant.", "rate": "A Tensor of type float32.\nA scalar representing the sample rate. Each element of input_dataset is\nretained with this probability, independent of all other elements.", "seed": "A Tensor of type int64.\nA scalar representing seed of random number generator.", "seed2": "A Tensor of type int64.\nA scalar representing seed2 of random number generator.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.Save": {"description": "Saves the input tensors to disk.", "Args": {"filename": "A Tensor of type string.\nMust have a single element. The name of the file to which we write\nthe tensor.", "tensor_names": "A Tensor of type string.\nShape [N]. The names of the tensors to be saved.", "data": "A list of Tensor objects. N tensors to save.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.SaveDataset": {"Args": {"input_dataset": "A Tensor of type variant.", "path": "A Tensor of type string.", "shard_func_other_args": "A list of Tensor objects.", "shard_func": "A function decorated with @Defun.", "compression": "An optional string. Defaults to \"\".", "use_shard_func": "An optional bool. Defaults to True.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.SaveDatasetV2": {"Args": {"input_dataset": "A Tensor of type variant.", "path": "A Tensor of type string.", "shard_func_other_args": "A list of Tensor objects.", "shard_func": "A function decorated with @Defun.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "compression": "An optional string. Defaults to \"\".", "use_shard_func": "An optional bool. Defaults to True.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.SaveSlices": {"description": "Saves input tensors slices to disk.", "Args": {"filename": "A Tensor of type string.\nMust have a single element. The name of the file to which we write the\ntensor.", "tensor_names": "A Tensor of type string.\nShape [N]. The names of the tensors to be saved.", "shapes_and_slices": "A Tensor of type string.\nShape [N].  The shapes and slice specifications to use when\nsaving the tensors.", "data": "A list of Tensor objects. N tensors to save.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.SaveV2": {"description": "Saves tensors in V2 checkpoint format.", "Args": {"prefix": "A Tensor of type string.\nMust have a single element. The prefix of the V2 checkpoint to which we\nwrite the tensors.", "tensor_names": "A Tensor of type string.\nshape {N}. The names of the tensors to be saved.", "shape_and_slices": "A Tensor of type string.\nshape {N}.  The slice specs of the tensors to be saved.\nEmpty strings indicate that they are non-partitioned tensors.", "tensors": "A list of Tensor objects. N tensors to save.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.ScalarSummary": {"description": "Outputs a Summary protocol buffer with scalar values.", "Args": {"tags": "A Tensor of type string. Tags for the summary."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.ScaleAndTranslate": {"Args": {"images": "A Tensor. Must be one of the following types: int8, uint8, int16, uint16, int32, int64, bfloat16, half, float32, float64.", "size": "A Tensor of type int32.", "scale": "A Tensor of type float32.", "translation": "A Tensor of type float32.", "kernel_type": "An optional string. Defaults to \"lanczos3\".", "antialias": "An optional bool. Defaults to True.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type float32."}, "tf.raw_ops.ScaleAndTranslateGrad": {"Args": {"grads": "A Tensor. Must be one of the following types: float32.", "original_image": "A Tensor. Must have the same type as grads.", "scale": "A Tensor of type float32.", "translation": "A Tensor of type float32.", "kernel_type": "An optional string. Defaults to \"lanczos3\".", "antialias": "An optional bool. Defaults to True.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as grads."}, "tf.raw_ops.ScanDataset": {"description": "Creates a dataset successively reduces f over the elements of input_dataset.", "Args": {"input_dataset": "A Tensor of type variant.", "initial_state": "A list of Tensor objects.", "other_arguments": "A list of Tensor objects.", "f": "A function decorated with @Defun.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "preserve_cardinality": "An optional bool. Defaults to False.", "use_default_device": "An optional bool. Defaults to True.", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ScatterAdd": {"description": "Adds sparse updates to a variable reference.", "Args": {"ref": "A mutable Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nShould be from a Variable node.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA tensor of indices into the first dimension of ref.", "updates": "A Tensor. Must have the same type as ref.\nA tensor of updated values to add to ref.", "use_locking": "An optional bool. Defaults to False.\nIf True, the addition will be protected by a lock;\notherwise the behavior is undefined, but may exhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as ref."}, "tf.raw_ops.ScatterDiv": {"description": "Divides a variable reference by sparse updates.", "Args": {"ref": "A mutable Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nShould be from a Variable node.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA tensor of indices into the first dimension of ref.", "updates": "A Tensor. Must have the same type as ref.\nA tensor of values that ref is divided by.", "use_locking": "An optional bool. Defaults to False.\nIf True, the operation will be protected by a lock;\notherwise the behavior is undefined, but may exhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as ref."}, "tf.raw_ops.ScatterMax": {"description": "Reduces sparse updates into a variable reference using the max operation.", "Args": {"ref": "A mutable Tensor. Must be one of the following types: half, bfloat16, float32, float64, int32, int64.\nShould be from a Variable node.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA tensor of indices into the first dimension of ref.", "updates": "A Tensor. Must have the same type as ref.\nA tensor of updated values to reduce into ref.", "use_locking": "An optional bool. Defaults to False.\nIf True, the update will be protected by a lock;\notherwise the behavior is undefined, but may exhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as ref."}, "tf.raw_ops.ScatterMin": {"description": "Reduces sparse updates into a variable reference using the min operation.", "Args": {"ref": "A mutable Tensor. Must be one of the following types: half, bfloat16, float32, float64, int32, int64.\nShould be from a Variable node.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA tensor of indices into the first dimension of ref.", "updates": "A Tensor. Must have the same type as ref.\nA tensor of updated values to reduce into ref.", "use_locking": "An optional bool. Defaults to False.\nIf True, the update will be protected by a lock;\notherwise the behavior is undefined, but may exhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as ref."}, "tf.raw_ops.ScatterMul": {"description": "Multiplies sparse updates into a variable reference.", "Args": {"ref": "A mutable Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nShould be from a Variable node.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA tensor of indices into the first dimension of ref.", "updates": "A Tensor. Must have the same type as ref.\nA tensor of updated values to multiply to ref.", "use_locking": "An optional bool. Defaults to False.\nIf True, the operation will be protected by a lock;\notherwise the behavior is undefined, but may exhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as ref."}, "tf.raw_ops.ScatterNd": {"description": "Scatters updates into a tensor of shape shape according to indices.", "Args": {"indices": "A Tensor. Must be one of the following types: int16, int32, int64.\nTensor of indices.", "updates": "A Tensor. Values to scatter into the output tensor.", "shape": "A Tensor. Must have the same type as indices.\n1-D. The shape of the output tensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as updates."}, "tf.raw_ops.ScatterNdAdd": {"description": "Applies sparse addition to individual values or slices in a Variable.", "Args": {"ref": "A mutable Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nA mutable Tensor. Should be from a Variable node.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA Tensor. Must be one of the following types: int32, int64.\nA tensor of indices into ref.", "updates": "A Tensor. Must have the same type as ref.\nA Tensor. Must have the same type as ref. A tensor of updated values\nto add to ref.", "use_locking": "An optional bool. Defaults to False.\nAn optional bool. Defaults to True. If True, the assignment will\nbe protected by a lock; otherwise the behavior is undefined,\nbut may exhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as ref."}, "tf.raw_ops.ScatterNdMax": {"description": "Computes element-wise maximum.", "Args": {"ref": "A mutable Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nA mutable Tensor. Should be from a Variable node.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA Tensor. Must be one of the following types: int32, int64.\nA tensor of indices into ref.", "updates": "A Tensor. Must have the same type as ref.\nA Tensor. Must have the same type as ref. A tensor of updated values\nto add to ref.", "use_locking": "An optional bool. Defaults to False.\nAn optional bool. Defaults to True. If True, the assignment will\nbe protected by a lock; otherwise the behavior is undefined,\nbut may exhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as ref."}, "tf.raw_ops.ScatterNdMin": {"description": "Computes element-wise minimum.", "Args": {"ref": "A mutable Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nA mutable Tensor. Should be from a Variable node.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA Tensor. Must be one of the following types: int32, int64.\nA tensor of indices into ref.", "updates": "A Tensor. Must have the same type as ref.\nA Tensor. Must have the same type as ref. A tensor of updated values\nto add to ref.", "use_locking": "An optional bool. Defaults to False.\nAn optional bool. Defaults to True. If True, the assignment will\nbe protected by a lock; otherwise the behavior is undefined,\nbut may exhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as ref."}, "tf.raw_ops.ScatterNdNonAliasingAdd": {"description": "Applies sparse addition to input using individual values or slices", "Args": {"input": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64, bool.\nA Tensor.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA Tensor. Must be one of the following types: int32, int64.\nA tensor of indices into input.", "updates": "A Tensor. Must have the same type as input.\nA Tensor. Must have the same type as ref. A tensor of updated values\nto add to input.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.ScatterNdSub": {"description": "Applies sparse subtraction to individual values or slices in a Variable.", "Args": {"ref": "A mutable Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nA mutable Tensor. Should be from a Variable node.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA Tensor. Must be one of the following types: int32, int64.\nA tensor of indices into ref.", "updates": "A Tensor. Must have the same type as ref.\nA Tensor. Must have the same type as ref. A tensor of updated values\nto subtract from ref.", "use_locking": "An optional bool. Defaults to False.\nAn optional bool. Defaults to True. If True, the assignment will\nbe protected by a lock; otherwise the behavior is undefined,\nbut may exhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as ref."}, "tf.raw_ops.ScatterNdUpdate": {"description": "Applies sparse updates to individual values or slices within a given", "Args": {"ref": "A mutable Tensor. A mutable Tensor. Should be from a Variable node.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA Tensor. Must be one of the following types: int32, int64.\nA tensor of indices into ref.", "updates": "A Tensor. Must have the same type as ref.\nA Tensor. Must have the same type as ref. A tensor of updated\nvalues to add to ref.", "use_locking": "An optional bool. Defaults to True.\nAn optional bool. Defaults to True. If True, the assignment will\nbe protected by a lock; otherwise the behavior is undefined,\nbut may exhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as ref."}, "tf.raw_ops.ScatterSub": {"description": "Subtracts sparse updates to a variable reference.", "Args": {"ref": "A mutable Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nShould be from a Variable node.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA tensor of indices into the first dimension of ref.", "updates": "A Tensor. Must have the same type as ref.\nA tensor of updated values to subtract from ref.", "use_locking": "An optional bool. Defaults to False.\nIf True, the subtraction will be protected by a lock;\notherwise the behavior is undefined, but may exhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as ref."}, "tf.raw_ops.ScatterUpdate": {"description": "Applies sparse updates to a variable reference.", "Args": {"ref": "A mutable Tensor. Should be from a Variable node.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA tensor of indices into the first dimension of ref.", "updates": "A Tensor. Must have the same type as ref.\nA tensor of updated values to store in ref.", "use_locking": "An optional bool. Defaults to True.\nIf True, the assignment will be protected by a lock;\notherwise the behavior is undefined, but may exhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as ref."}, "tf.raw_ops.SdcaFprint": {"description": "Computes fingerprints of the input strings.", "Args": {"input": "A Tensor of type string.\nvector of strings to compute fingerprints on.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int64."}, "tf.raw_ops.SdcaOptimizer": {"description": "Distributed version of Stochastic Dual Coordinate Ascent (SDCA) optimizer for", "Args": {"sparse_example_indices": "A list of Tensor objects with type int64.\na list of vectors which contain example indices.", "sparse_feature_indices": "A list with the same length as sparse_example_indices of Tensor objects with type int64.\na list of vectors which contain feature indices.", "sparse_feature_values": "A list of Tensor objects with type float32.\na list of vectors which contains feature value\nassociated with each feature group.", "dense_features": "A list of Tensor objects with type float32.\na list of matrices which contains the dense feature values.", "example_weights": "A Tensor of type float32.\na vector which contains the weight associated with each\nexample.", "example_labels": "A Tensor of type float32.\na vector which contains the label/target associated with each\nexample.", "sparse_indices": "A list with the same length as sparse_example_indices of Tensor objects with type int64.\na list of vectors where each value is the indices which has\ncorresponding weights in sparse_weights. This field maybe omitted for the\ndense approach.", "sparse_weights": "A list with the same length as sparse_example_indices of Tensor objects with type float32.\na list of vectors where each value is the weight associated with\na sparse feature group.", "dense_weights": "A list with the same length as dense_features of Tensor objects with type float32.\na list of vectors where the values are the weights associated\nwith a dense feature group.", "example_state_data": "A Tensor of type float32.\na list of vectors containing the example state data.", "loss_type": "A string from: \"logistic_loss\", \"squared_loss\", \"hinge_loss\", \"smooth_hinge_loss\", \"poisson_loss\".\nType of the primal loss. Currently SdcaSolver supports logistic,\nsquared and hinge losses.", "l1": "A float. Symmetric l1 regularization strength.", "l2": "A float. Symmetric l2 regularization strength.", "num_loss_partitions": "An int that is >= 1.\nNumber of partitions of the global loss function.", "num_inner_iterations": "An int that is >= 1.\nNumber of iterations per mini-batch.", "adaptative": "An optional bool. Defaults to True.\nWhether to use Adaptive SDCA for the inner loop.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (out_example_state_data, out_delta_sparse_weights, out_delta_dense_weights)."}, "tf.raw_ops.SdcaOptimizerV2": {"description": "Distributed version of Stochastic Dual Coordinate Ascent (SDCA) optimizer for", "Args": {"sparse_example_indices": "A list of Tensor objects with type int64.\na list of vectors which contain example indices.", "sparse_feature_indices": "A list with the same length as sparse_example_indices of Tensor objects with type int64.\na list of vectors which contain feature indices.", "sparse_feature_values": "A list of Tensor objects with type float32.\na list of vectors which contains feature value\nassociated with each feature group.", "dense_features": "A list of Tensor objects with type float32.\na list of matrices which contains the dense feature values.", "example_weights": "A Tensor of type float32.\na vector which contains the weight associated with each\nexample.", "example_labels": "A Tensor of type float32.\na vector which contains the label/target associated with each\nexample.", "sparse_indices": "A list with the same length as sparse_example_indices of Tensor objects with type int64.\na list of vectors where each value is the indices which has\ncorresponding weights in sparse_weights. This field maybe omitted for the\ndense approach.", "sparse_weights": "A list with the same length as sparse_example_indices of Tensor objects with type float32.\na list of vectors where each value is the weight associated with\na sparse feature group.", "dense_weights": "A list with the same length as dense_features of Tensor objects with type float32.\na list of vectors where the values are the weights associated\nwith a dense feature group.", "example_state_data": "A Tensor of type float32.\na list of vectors containing the example state data.", "loss_type": "A string from: \"logistic_loss\", \"squared_loss\", \"hinge_loss\", \"smooth_hinge_loss\", \"poisson_loss\".\nType of the primal loss. Currently SdcaSolver supports logistic,\nsquared and hinge losses.", "l1": "A float. Symmetric l1 regularization strength.", "l2": "A float. Symmetric l2 regularization strength.", "num_loss_partitions": "An int that is >= 1.\nNumber of partitions of the global loss function.", "num_inner_iterations": "An int that is >= 1.\nNumber of iterations per mini-batch.", "adaptive": "An optional bool. Defaults to True.\nWhether to use Adaptive SDCA for the inner loop.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (out_example_state_data, out_delta_sparse_weights, out_delta_dense_weights)."}, "tf.raw_ops.SdcaShrinkL1": {"description": "Applies L1 regularization shrink step on the parameters.", "Args": {"weights": "A list of Tensor objects with type mutable float32.\na list of vectors where each value is the weight associated with a\nfeature group.", "l1": "A float. Symmetric l1 regularization strength.", "l2": "A float.\nSymmetric l2 regularization strength. Should be a positive float.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.SegmentMax": {"description": "Computes the maximum along segments of a tensor.", "Args": {"data": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.", "segment_ids": "A Tensor. Must be one of the following types: int32, int64.\nA 1-D tensor whose size is equal to the size of data's\nfirst dimension.  Values should be sorted and can be repeated.\nCaution: The values are always validated to be sorted on CPU, never validated\non GPU.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as data."}, "tf.raw_ops.SegmentMean": {"description": "Computes the mean along segments of a tensor.", "Args": {"data": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.", "segment_ids": "A Tensor. Must be one of the following types: int32, int64.\nA 1-D tensor whose size is equal to the size of data's\nfirst dimension.  Values should be sorted and can be repeated.\nCaution: The values are always validated to be sorted on CPU, never validated\non GPU.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as data."}, "tf.raw_ops.SegmentMin": {"description": "Computes the minimum along segments of a tensor.", "Args": {"data": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.", "segment_ids": "A Tensor. Must be one of the following types: int32, int64.\nA 1-D tensor whose size is equal to the size of data's\nfirst dimension.  Values should be sorted and can be repeated.\nCaution: The values are always validated to be sorted on CPU, never validated\non GPU.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as data."}, "tf.raw_ops.SegmentProd": {"description": "Computes the product along segments of a tensor.", "Args": {"data": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.", "segment_ids": "A Tensor. Must be one of the following types: int32, int64.\nA 1-D tensor whose size is equal to the size of data's\nfirst dimension.  Values should be sorted and can be repeated.\nCaution: The values are always validated to be sorted on CPU, never validated\non GPU.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as data."}, "tf.raw_ops.SegmentSum": {"description": "Computes the sum along segments of a tensor.", "Args": {"data": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.", "segment_ids": "A Tensor. Must be one of the following types: int32, int64.\nA 1-D tensor whose size is equal to the size of data's\nfirst dimension.  Values should be sorted and can be repeated.\nCaution: The values are always validated to be sorted on CPU, never validated\non GPU.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as data."}, "tf.raw_ops.Select": {"description": "Selects elements from x or y, depending on condition.", "Args": {"condition": "A Tensor of type bool.", "x": "A Tensor which may have the same shape as condition.\nIf condition is rank 1, x may have higher rank,\nbut its first dimension must match the size of condition.", "y": "A Tensor with the same type and shape as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as t."}, "tf.raw_ops.SelectV2": {"Args": {"condition": "A Tensor of type bool.", "t": "A Tensor.", "e": "A Tensor. Must have the same type as t.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as t."}, "tf.raw_ops.SelfAdjointEig": {"description": "Computes the Eigen Decomposition of a batch of square self-adjoint matrices.", "Args": {"input": "A Tensor. Must be one of the following types: float64, float32, half.\nShape is [..., M, M].", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.SelfAdjointEigV2": {"description": "Computes the eigen decomposition of one or more square self-adjoint matrices.", "Args": {"input": "A Tensor. Must be one of the following types: float64, float32, half, complex64, complex128.\nTensor input of shape [N, N].", "compute_v": "An optional bool. Defaults to True.\nIf True then eigenvectors will be computed and returned in v.\nOtherwise, only the eigenvalues will be computed.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (e, v)."}, "tf.raw_ops.Selu": {"description": "Computes scaled exponential linear: scale * alpha * (exp(features) - 1)", "Args": {"features": "A Tensor. Must be one of the following types: half, bfloat16, float32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as features."}, "tf.raw_ops.SeluGrad": {"description": "Computes gradients for the scaled exponential linear (Selu) operation.", "Args": {"gradients": "A Tensor. Must be one of the following types: half, bfloat16, float32, float64.\nThe backpropagated gradients to the corresponding Selu operation.", "outputs": "A Tensor. Must have the same type as gradients.\nThe outputs of the corresponding Selu operation.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as gradients."}, "tf.raw_ops.Send": {"description": "Sends the named tensor from send_device to recv_device.", "Args": {"tensor": "A Tensor. The tensor to send.", "tensor_name": "A string. The name of the tensor to send.", "send_device": "A string. The name of the device sending the tensor.", "send_device_incarnation": "An int. The current incarnation of send_device.", "recv_device": "A string. The name of the device receiving the tensor.", "client_terminated": "An optional bool. Defaults to False.\nIf set to true, this indicates that the node was added\nto the graph as a result of a client-side feed or fetch of Tensor data,\nin which case the corresponding send or recv is expected to be managed\nlocally by the caller.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.SendTPUEmbeddingGradients": {"description": "Performs gradient updates of embedding tables.", "Args": {"inputs": "A list of at least 1 Tensor objects with type float32.\nA TensorList of gradients with which to update embedding tables.\nThis argument has the same length and shapes as the return value of\nRecvTPUEmbeddingActivations, but contains gradients of the model's loss\nwith respect to the embedding activations. The embedding tables are updated\nfrom these gradients via the optimizer specified in the TPU embedding\nconfiguration given to tpu.initialize_system.", "learning_rates": "A list of Tensor objects with type float32.\nA TensorList of float32 scalars, one for each dynamic learning\nrate tag: see the comments in\n//third_party/tensorflow/core/protobuf/tpu/optimization_parameters.proto.\nMultiple tables can share the same dynamic learning rate tag as specified\nin the configuration. If the learning rates for all tables are constant,\nthis list should be empty.", "config": "A string. Serialized TPUEmbeddingConfiguration proto.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.SerializeIterator": {"description": "Converts the given resource_handle representing an iterator to a variant tensor.", "Args": {"resource_handle": "A Tensor of type resource.\nA handle to an iterator resource.", "external_state_policy": "An optional int. Defaults to 0.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.SerializeManySparse": {"description": "Serialize an N-minibatch SparseTensor into an [N, 3] Tensor object.", "Args": {"sparse_indices": "A Tensor of type int64.\n2-D.  The indices of the minibatch SparseTensor.", "sparse_values": "A Tensor.\n1-D.  The values of the minibatch SparseTensor.", "sparse_shape": "A Tensor of type int64.\n1-D.  The shape of the minibatch SparseTensor.", "out_type": "An optional tf.DType from: tf.string, tf.variant. Defaults to tf.string.\nThe dtype to use for serialization; the supported types are string\n(default) and variant.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type out_type."}, "tf.raw_ops.SerializeSparse": {"description": "Serialize a SparseTensor into a [3] Tensor object.", "Args": {"sparse_indices": "A Tensor of type int64.\n2-D.  The indices of the SparseTensor.", "sparse_values": "A Tensor. 1-D.  The values of the SparseTensor.", "sparse_shape": "A Tensor of type int64.\n1-D.  The shape of the SparseTensor.", "out_type": "An optional tf.DType from: tf.string, tf.variant. Defaults to tf.string.\nThe dtype to use for serialization; the supported types are string\n(default) and variant.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type out_type."}, "tf.raw_ops.SerializeTensor": {"description": "Transforms a Tensor into a serialized TensorProto proto.", "Args": {"tensor": "A Tensor. A Tensor of type T.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.SetSize": {"description": "Number of unique elements along last dimension of input set.", "Args": {"set_indices": "A Tensor of type int64.\n2D Tensor, indices of a SparseTensor.", "set_values": "A Tensor. Must be one of the following types: int8, int16, int32, int64, uint8, uint16, string.\n1D Tensor, values of a SparseTensor.", "set_shape": "A Tensor of type int64.\n1D Tensor, shape of a SparseTensor.", "validate_indices": "An optional bool. Defaults to True.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int32."}, "tf.raw_ops.SetStatsAggregatorDataset": {"Args": {"input_dataset": "A Tensor of type variant.", "stats_aggregator": "A Tensor of type resource.", "tag": "A Tensor of type string.", "counter_prefix": "A Tensor of type string.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.Shape": {"description": "Returns the shape of a tensor.", "Args": {"input": "A Tensor.", "out_type": "An optional tf.DType from: tf.int32, tf.int64. Defaults to tf.int32.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type out_type."}, "tf.raw_ops.ShapeN": {"description": "Returns shape of tensors.", "Args": {"input": "A list of at least 1 Tensor objects with the same type.", "out_type": "An optional tf.DType from: tf.int32, tf.int64. Defaults to tf.int32.", "name": "A name for the operation (optional)."}, "Returns": "A list with the same length as input of Tensor objects with type out_type."}, "tf.raw_ops.ShardDataset": {"description": "Creates a Dataset that includes only 1/num_shards of this dataset.", "Args": {"input_dataset": "A Tensor of type variant.", "num_shards": "A Tensor of type int64.\nAn integer representing the number of shards operating in parallel.", "index": "A Tensor of type int64.\nAn integer representing the current worker index.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "require_non_empty": "An optional bool. Defaults to False.", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ShardedFilename": {"description": "Generate a sharded filename. The filename is printf formatted as", "Args": {"basename": "A Tensor of type string.", "shard": "A Tensor of type int32.", "num_shards": "A Tensor of type int32.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.ShardedFilespec": {"description": "Generate a glob pattern matching all sharded file names.", "Args": {"basename": "A Tensor of type string.", "num_shards": "A Tensor of type int32.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.ShuffleAndRepeatDataset": {"description": "Creates a dataset that shuffles and repeats elements from input_dataset", "Args": {"input_dataset": "A Tensor of type variant.", "buffer_size": "A Tensor of type int64.\nThe number of output elements to buffer in an iterator over\nthis dataset. Compare with the min_after_dequeue attr when creating a\nRandomShuffleQueue.", "seed": "A Tensor of type int64.\nA scalar seed for the random number generator. If either seed or\nseed2 is set to be non-zero, the random number generator is seeded\nby the given seed.  Otherwise, a random seed is used.", "seed2": "A Tensor of type int64.\nA second scalar seed to avoid seed collision.", "count": "A Tensor of type int64.\nA scalar representing the number of times the underlying dataset\nshould be repeated. The default is -1, which results in infinite repetition.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "reshuffle_each_iteration": "An optional bool. Defaults to True.", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ShuffleAndRepeatDatasetV2": {"Args": {"input_dataset": "A Tensor of type variant.", "buffer_size": "A Tensor of type int64.", "seed": "A Tensor of type int64.", "seed2": "A Tensor of type int64.", "count": "A Tensor of type int64.", "seed_generator": "A Tensor of type resource.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "reshuffle_each_iteration": "An optional bool. Defaults to True.", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ShuffleDataset": {"description": "Creates a dataset that shuffles elements from input_dataset pseudorandomly.", "Args": {"input_dataset": "A Tensor of type variant.", "buffer_size": "A Tensor of type int64.\nThe number of output elements to buffer in an iterator over\nthis dataset. Compare with the min_after_dequeue attr when creating a\nRandomShuffleQueue.", "seed": "A Tensor of type int64.\nA scalar seed for the random number generator. If either seed or\nseed2 is set to be non-zero, the random number generator is seeded\nby the given seed.  Otherwise, a random seed is used.", "seed2": "A Tensor of type int64.\nA second scalar seed to avoid seed collision.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "reshuffle_each_iteration": "An optional bool. Defaults to True.\nIf true, each iterator over this dataset will be given\na different pseudorandomly generated seed, based on a sequence seeded by the\nseed and seed2 inputs. If false, each iterator will be given the same\nseed, and repeated iteration over this dataset will yield the exact same\nsequence of results.", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ShuffleDatasetV2": {"Args": {"input_dataset": "A Tensor of type variant.", "buffer_size": "A Tensor of type int64.", "seed_generator": "A Tensor of type resource.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ShuffleDatasetV3": {"Args": {"input_dataset": "A Tensor of type variant.", "buffer_size": "A Tensor of type int64.", "seed": "A Tensor of type int64.", "seed2": "A Tensor of type int64.", "seed_generator": "A Tensor of type resource.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "reshuffle_each_iteration": "An optional bool. Defaults to True.", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ShutdownDistributedTPU": {"description": "Shuts down a running distributed TPU system.", "Args": {"name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.Sigmoid": {"description": "Computes sigmoid of x element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, complex64, complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.SigmoidGrad": {"description": "Computes the gradient of the sigmoid of x wrt its input.", "Args": {"y": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, complex64, complex128.", "dy": "A Tensor. Must have the same type as y.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as y."}, "tf.raw_ops.Sign": {"description": "Returns an element-wise indication of the sign of a number.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, int8, int16, int32, int64, complex64, complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.Sin": {"description": "Computes sine of x element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, complex64, complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.Sinh": {"description": "Computes hyperbolic sine of x element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, complex64, complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.Size": {"description": "Returns the size of a tensor.", "Args": {"input": "A Tensor.", "out_type": "An optional tf.DType from: tf.int32, tf.int64. Defaults to tf.int32.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type out_type."}, "tf.raw_ops.SkipDataset": {"description": "Creates a dataset that skips count elements from the input_dataset.", "Args": {"input_dataset": "A Tensor of type variant.", "count": "A Tensor of type int64.\nA scalar representing the number of elements from the input_dataset\nthat should be skipped.  If count is -1, skips everything.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.SleepDataset": {"Args": {"input_dataset": "A Tensor of type variant.", "sleep_microseconds": "A Tensor of type int64.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.Slice": {"description": "Return a slice from &#39;input&#39;.", "Args": {"input": "A Tensor.", "begin": "A Tensor. Must be one of the following types: int32, int64.\nbegin[i] specifies the offset into the 'i'th dimension of\n'input' to slice from.", "size": "A Tensor. Must have the same type as begin.\nsize[i] specifies the number of elements of the 'i'th dimension\nof 'input' to slice. If size[i] is -1, all remaining elements in dimension\ni are included in the slice (i.e. this is equivalent to setting\nsize[i] = input.dim_size(i) - begin[i]).", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.SlidingWindowDataset": {"description": "Creates a dataset that passes a sliding window over input_dataset.", "Args": {"input_dataset": "A Tensor of type variant.", "window_size": "A Tensor of type int64.\nA scalar representing the number of elements in the\nsliding window.", "window_shift": "A Tensor of type int64.\nA scalar representing the steps moving the sliding window\nforward in one iteration. It must be positive.", "window_stride": "A Tensor of type int64.\nA scalar representing the stride of the input elements of the sliding window.\nIt must be positive.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "drop_remainder": "An optional bool. Defaults to True.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.Snapshot": {"description": "Returns a copy of the input tensor.", "Args": {"input": "A Tensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.SnapshotDataset": {"description": "Creates a dataset that will write to / read from a snapshot.", "Args": {"input_dataset": "A Tensor of type variant.\nA variant tensor representing the input dataset.", "path": "A Tensor of type string.\nThe path we should write snapshots to / read snapshots from.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "compression": "An optional string. Defaults to \"\".", "reader_path_prefix": "An optional string. Defaults to \"\".", "writer_path_prefix": "An optional string. Defaults to \"\".", "shard_size_bytes": "An optional int. Defaults to 10737418240.", "pending_snapshot_expiry_seconds": "An optional int. Defaults to 86400.", "num_reader_threads": "An optional int. Defaults to 1.", "reader_buffer_size": "An optional int. Defaults to 1.", "num_writer_threads": "An optional int. Defaults to 1.", "writer_buffer_size": "An optional int. Defaults to 1.", "shuffle_on_read": "An optional bool. Defaults to False.", "seed": "An optional int. Defaults to 0.", "seed2": "An optional int. Defaults to 0.", "mode": "An optional string. Defaults to \"auto\".", "snapshot_name": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.SnapshotDatasetReader": {"Args": {"shard_dir": "A Tensor of type string.", "start_index": "A Tensor of type int64.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "version": "An int.", "compression": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.SnapshotDatasetV2": {"description": "Creates a dataset that will write to / read from a snapshot.", "Args": {"input_dataset": "A Tensor of type variant.\nA variant tensor representing the input dataset.", "path": "A Tensor of type string.\nThe path we should write snapshots to / read snapshots from.", "reader_func_other_args": "A list of Tensor objects.", "shard_func_other_args": "A list of Tensor objects.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "reader_func": "A function decorated with @Defun.\nOptional. A function to control how to read data from snapshot shards.", "shard_func": "A function decorated with @Defun.\nOptional. A function to control how to shard data when writing a snapshot.", "compression": "An optional string. Defaults to \"\".\nThe type of compression to be applied to the saved snapshot files.", "reader_prefix": "An optional string. Defaults to \"\".", "writer_prefix": "An optional string. Defaults to \"\".", "hash_valid": "An optional bool. Defaults to False.", "hash": "An optional int. Defaults to 0.", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.SnapshotNestedDatasetReader": {"Args": {"inputs": "A list of at least 1 Tensor objects with type variant.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.SobolSample": {"description": "Generates points from the Sobol sequence.", "Args": {"dim": "A Tensor of type int32.\nPositive scalar Tensor representing each sample's dimension.", "num_results": "A Tensor of type int32.\nPositive scalar Tensor of dtype int32. The number of Sobol points to return\nin the output.", "skip": "A Tensor of type int32.\nPositive scalar Tensor of dtype int32. The number of initial points of the\nSobol sequence to skip.", "dtype": "An optional tf.DType from: tf.float32, tf.float64. Defaults to tf.float32.\nThe type of the sample. One of: float32 or float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dtype."}, "tf.raw_ops.Softmax": {"description": "Computes softmax activations.", "Args": {"logits": "A Tensor. Must be one of the following types: half, bfloat16, float32, float64.\n2-D with shape [batch_size, num_classes].", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as logits."}, "tf.raw_ops.SoftmaxCrossEntropyWithLogits": {"description": "Computes softmax cross entropy cost and gradients to backpropagate.", "Args": {"features": "A Tensor. Must be one of the following types: half, bfloat16, float32, float64.\nbatch_size x num_classes matrix", "labels": "A Tensor. Must have the same type as features.\nbatch_size x num_classes matrix\nThe caller must ensure that each batch of labels represents a valid\nprobability distribution.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (loss, backprop)."}, "tf.raw_ops.Softplus": {"Args": {"features": "A Tensor. Must be one of the following types: half, bfloat16, float32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as features."}, "tf.raw_ops.SoftplusGrad": {"description": "Computes softplus gradients for a softplus operation.", "Args": {"gradients": "A Tensor. Must be one of the following types: half, bfloat16, float32, float64.\nThe backpropagated gradients to the corresponding softplus operation.", "features": "A Tensor. Must have the same type as gradients.\nThe features passed as input to the corresponding softplus operation.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as gradients."}, "tf.raw_ops.Softsign": {"description": "Computes softsign: features / (abs(features) &#43; 1).", "Args": {"features": "A Tensor. Must be one of the following types: half, bfloat16, float32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as features."}, "tf.raw_ops.SoftsignGrad": {"description": "Computes softsign gradients for a softsign operation.", "Args": {"gradients": "A Tensor. Must be one of the following types: half, bfloat16, float32, float64.\nThe backpropagated gradients to the corresponding softsign operation.", "features": "A Tensor. Must have the same type as gradients.\nThe features passed as input to the corresponding softsign operation.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as gradients."}, "tf.raw_ops.SpaceToBatch": {"description": "SpaceToBatch for 4-D tensors of type T.", "Args": {"input": "A Tensor. 4-D with shape [batch, height, width, depth].", "paddings": "A Tensor. Must be one of the following types: int32, int64.\n2-D tensor of non-negative integers with shape [2, 2]. It specifies\n  the padding of the input with zeros across the spatial dimensions as follows:\n\u00a0 paddings = [[pad_top, pad_bottom], [pad_left, pad_right]]\nThe effective spatial dimensions of the zero-padded input tensor will be:\n\u00a0 height_pad = pad_top + height + pad_bottom\u00a0 width_pad = pad_left + width + pad_right", "block_size": "An int that is >= 2.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.SpaceToBatchND": {"description": "SpaceToBatch for N-D tensors of type T.", "Args": {"input": "A Tensor.\nN-D with shape input_shape = [batch] + spatial_shape + remaining_shape,\nwhere spatial_shape has M dimensions.", "block_shape": "A Tensor. Must be one of the following types: int32, int64.\n1-D with shape [M], all values must be >= 1.", "paddings": "A Tensor. Must be one of the following types: int32, int64.\n2-D with shape [M, 2], all values must be >= 0.\n  paddings[i] = [pad_start, pad_end] specifies the padding for input dimension\n  i + 1, which corresponds to spatial dimension i.  It is required that\n  block_shape[i] divides input_shape[i + 1] + pad_start + pad_end.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.SpaceToDepth": {"description": "SpaceToDepth for tensors of type T.", "Args": {"input": "A Tensor.", "block_size": "An int that is >= 2. The size of the spatial block.", "data_format": "An optional string from: \"NHWC\", \"NCHW\", \"NCHW_VECT_C\". Defaults to \"NHWC\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.SparseAccumulatorApplyGradient": {"description": "Applies a sparse gradient to a given accumulator.", "Args": {"handle": "A Tensor of type mutable string. The handle to a accumulator.", "local_step": "A Tensor of type int64.\nThe local_step value at which the sparse gradient was computed.", "gradient_indices": "A Tensor of type int64.\nIndices of the sparse gradient to be accumulated. Must be a\nvector.", "gradient_values": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nValues are the non-zero slices of the gradient, and must have\nthe same first dimension as indices, i.e., the nnz represented by indices and\nvalues must be consistent.", "gradient_shape": "A Tensor of type int64.\nShape of the sparse gradient to be accumulated.", "has_known_shape": "A bool.\nBoolean indicating whether gradient_shape is unknown, in which\ncase the input is ignored during validation.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.SparseAccumulatorTakeGradient": {"description": "Extracts the average sparse gradient in a SparseConditionalAccumulator.", "Args": {"handle": "A Tensor of type mutable string.\nThe handle to a SparseConditionalAccumulator.", "num_required": "A Tensor of type int32.\nNumber of gradients required before we return an aggregate.", "dtype": "A tf.DType from: tf.float32, tf.float64, tf.int32, tf.uint8, tf.int16, tf.int8, tf.complex64, tf.int64, tf.qint8, tf.quint8, tf.qint32, tf.bfloat16, tf.uint16, tf.complex128, tf.half, tf.uint32, tf.uint64.\nThe data type of accumulated gradients. Needs to correspond to the type\nof the accumulator.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (indices, values, shape)."}, "tf.raw_ops.SparseAdd": {"description": "Adds two SparseTensor objects to produce another SparseTensor.", "Args": {"a_indices": "A Tensor of type int64.\n2-D.  The indices of the first SparseTensor, size [nnz, ndims] Matrix.", "a_values": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\n1-D.  The values of the first SparseTensor, size [nnz] Vector.", "a_shape": "A Tensor of type int64.\n1-D.  The shape of the first SparseTensor, size [ndims] Vector.", "b_indices": "A Tensor of type int64.\n2-D.  The indices of the second SparseTensor, size [nnz, ndims] Matrix.", "b_values": "A Tensor. Must have the same type as a_values.\n1-D.  The values of the second SparseTensor, size [nnz] Vector.", "b_shape": "A Tensor of type int64.\n1-D.  The shape of the second SparseTensor, size [ndims] Vector.", "thresh": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.\n0-D.  The magnitude threshold that determines if an output value/index\npair takes space.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (sum_indices, sum_values, sum_shape)."}, "tf.raw_ops.SparseAddGrad": {"description": "The gradient operator for the SparseAdd op.", "Args": {"backprop_val_grad": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\n1-D with shape [nnz(sum)].  The gradient with respect to\nthe non-empty values of the sum.", "a_indices": "A Tensor of type int64.\n2-D.  The indices of the SparseTensor A, size [nnz(A), ndims].", "b_indices": "A Tensor of type int64.\n2-D.  The indices of the SparseTensor B, size [nnz(B), ndims].", "sum_indices": "A Tensor of type int64.\n2-D.  The indices of the sum SparseTensor, size\n[nnz(sum), ndims].", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (a_val_grad, b_val_grad)."}, "tf.raw_ops.SparseApplyAdadelta": {"description": "var: Should be from a Variable().", "Args": {"var": "A mutable Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.", "accum": "A mutable Tensor. Must have the same type as var.\nShould be from a Variable().", "accum_update": "A mutable Tensor. Must have the same type as var.\nShould be from a Variable().", "lr": "A Tensor. Must have the same type as var.\nLearning rate. Must be a scalar.", "rho": "A Tensor. Must have the same type as var.\nDecay factor. Must be a scalar.", "epsilon": "A Tensor. Must have the same type as var.\nConstant factor. Must be a scalar.", "grad": "A Tensor. Must have the same type as var. The gradient.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA vector of indices into the first dimension of var and accum.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var and accum tensors will be protected by\na lock; otherwise the behavior is undefined, but may exhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as var."}, "tf.raw_ops.SparseApplyAdagrad": {"description": "Update relevant entries in &#39;*var&#39; and &#39;*accum&#39; according to the adagrad scheme.", "Args": {"var": "A mutable Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nShould be from a Variable().", "accum": "A mutable Tensor. Must have the same type as var.\nShould be from a Variable().", "lr": "A Tensor. Must have the same type as var.\nLearning rate. Must be a scalar.", "grad": "A Tensor. Must have the same type as var. The gradient.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA vector of indices into the first dimension of var and accum.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var and accum tensors will be protected\nby a lock; otherwise the behavior is undefined, but may exhibit less\ncontention.", "update_slots": "An optional bool. Defaults to True.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as var."}, "tf.raw_ops.SparseApplyAdagradDA": {"description": "Update entries in &#39;*var&#39; and &#39;*accum&#39; according to the proximal adagrad scheme.", "Args": {"var": "A mutable Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nShould be from a Variable().", "gradient_accumulator": "A mutable Tensor. Must have the same type as var.\nShould be from a Variable().", "gradient_squared_accumulator": "A mutable Tensor. Must have the same type as var.\nShould be from a Variable().", "grad": "A Tensor. Must have the same type as var. The gradient.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA vector of indices into the first dimension of var and accum.", "lr": "A Tensor. Must have the same type as var.\nLearning rate. Must be a scalar.", "l1": "A Tensor. Must have the same type as var.\nL1 regularization. Must be a scalar.", "l2": "A Tensor. Must have the same type as var.\nL2 regularization. Must be a scalar.", "global_step": "A Tensor of type int64.\nTraining step number. Must be a scalar.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var and accum tensors will be protected by\na lock; otherwise the behavior is undefined, but may exhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as var."}, "tf.raw_ops.SparseApplyAdagradV2": {"description": "Update relevant entries in &#39;*var&#39; and &#39;*accum&#39; according to the adagrad scheme.", "Args": {"var": "A mutable Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nShould be from a Variable().", "accum": "A mutable Tensor. Must have the same type as var.\nShould be from a Variable().", "lr": "A Tensor. Must have the same type as var.\nLearning rate. Must be a scalar.", "epsilon": "A Tensor. Must have the same type as var.\nConstant factor. Must be a scalar.", "grad": "A Tensor. Must have the same type as var. The gradient.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA vector of indices into the first dimension of var and accum.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var and accum tensors will be protected\nby a lock; otherwise the behavior is undefined, but may exhibit less\ncontention.", "update_slots": "An optional bool. Defaults to True.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as var."}, "tf.raw_ops.SparseApplyCenteredRMSProp": {"description": "Update &#39;*var&#39; according to the centered RMSProp algorithm.", "Args": {"var": "A mutable Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nShould be from a Variable().", "mg": "A mutable Tensor. Must have the same type as var.\nShould be from a Variable().", "ms": "A mutable Tensor. Must have the same type as var.\nShould be from a Variable().", "mom": "A mutable Tensor. Must have the same type as var.\nShould be from a Variable().", "lr": "A Tensor. Must have the same type as var.\nScaling factor. Must be a scalar.", "rho": "A Tensor. Must have the same type as var.\nDecay rate. Must be a scalar.", "momentum": "A Tensor. Must have the same type as var.", "epsilon": "A Tensor. Must have the same type as var.\nRidge term. Must be a scalar.", "grad": "A Tensor. Must have the same type as var. The gradient.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA vector of indices into the first dimension of var, ms and mom.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var, mg, ms, and mom tensors is\nprotected by a lock; otherwise the behavior is undefined, but may exhibit less\ncontention.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as var."}, "tf.raw_ops.SparseApplyFtrl": {"description": "Update relevant entries in &#39;*var&#39; according to the Ftrl-proximal scheme.", "Args": {"var": "A mutable Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nShould be from a Variable().", "accum": "A mutable Tensor. Must have the same type as var.\nShould be from a Variable().", "linear": "A mutable Tensor. Must have the same type as var.\nShould be from a Variable().", "grad": "A Tensor. Must have the same type as var. The gradient.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA vector of indices into the first dimension of var and accum.", "lr": "A Tensor. Must have the same type as var.\nScaling factor. Must be a scalar.", "l1": "A Tensor. Must have the same type as var.\nL1 regularization. Must be a scalar.", "l2": "A Tensor. Must have the same type as var.\nL2 regularization. Must be a scalar.", "lr_power": "A Tensor. Must have the same type as var.\nScaling factor. Must be a scalar.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var and accum tensors will be protected\nby a lock; otherwise the behavior is undefined, but may exhibit less\ncontention.", "multiply_linear_by_lr": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as var."}, "tf.raw_ops.SparseApplyFtrlV2": {"description": "Update relevant entries in &#39;*var&#39; according to the Ftrl-proximal scheme.", "Args": {"var": "A mutable Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nShould be from a Variable().", "accum": "A mutable Tensor. Must have the same type as var.\nShould be from a Variable().", "linear": "A mutable Tensor. Must have the same type as var.\nShould be from a Variable().", "grad": "A Tensor. Must have the same type as var. The gradient.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA vector of indices into the first dimension of var and accum.", "lr": "A Tensor. Must have the same type as var.\nScaling factor. Must be a scalar.", "l1": "A Tensor. Must have the same type as var.\nL1 regularization. Must be a scalar.", "l2": "A Tensor. Must have the same type as var.\nL2 shrinkage regularization. Must be a scalar.", "l2_shrinkage": "A Tensor. Must have the same type as var.", "lr_power": "A Tensor. Must have the same type as var.\nScaling factor. Must be a scalar.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var and accum tensors will be protected\nby a lock; otherwise the behavior is undefined, but may exhibit less\ncontention.", "multiply_linear_by_lr": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as var."}, "tf.raw_ops.SparseApplyMomentum": {"description": "Update relevant entries in &#39;*var&#39; and &#39;*accum&#39; according to the momentum scheme.", "Args": {"var": "A mutable Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nShould be from a Variable().", "accum": "A mutable Tensor. Must have the same type as var.\nShould be from a Variable().", "lr": "A Tensor. Must have the same type as var.\nLearning rate. Must be a scalar.", "grad": "A Tensor. Must have the same type as var. The gradient.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA vector of indices into the first dimension of var and accum.", "momentum": "A Tensor. Must have the same type as var.\nMomentum. Must be a scalar.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var and accum tensors will be protected\nby a lock; otherwise the behavior is undefined, but may exhibit less\ncontention.", "use_nesterov": "An optional bool. Defaults to False.\nIf True, the tensor passed to compute grad will be\nvar - lr * momentum * accum, so in the end, the var you get is actually\nvar - lr * momentum * accum.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as var."}, "tf.raw_ops.SparseApplyProximalAdagrad": {"description": "Sparse update entries in &#39;*var&#39; and &#39;*accum&#39; according to FOBOS algorithm.", "Args": {"var": "A mutable Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nShould be from a Variable().", "accum": "A mutable Tensor. Must have the same type as var.\nShould be from a Variable().", "lr": "A Tensor. Must have the same type as var.\nLearning rate. Must be a scalar.", "l1": "A Tensor. Must have the same type as var.\nL1 regularization. Must be a scalar.", "l2": "A Tensor. Must have the same type as var.\nL2 regularization. Must be a scalar.", "grad": "A Tensor. Must have the same type as var. The gradient.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA vector of indices into the first dimension of var and accum.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var and accum tensors will be protected by\na lock; otherwise the behavior is undefined, but may exhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as var."}, "tf.raw_ops.SparseApplyProximalGradientDescent": {"description": "Sparse update &#39;*var&#39; as FOBOS algorithm with fixed learning rate.", "Args": {"var": "A mutable Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nShould be from a Variable().", "alpha": "A Tensor. Must have the same type as var.\nScaling factor. Must be a scalar.", "l1": "A Tensor. Must have the same type as var.\nL1 regularization. Must be a scalar.", "l2": "A Tensor. Must have the same type as var.\nL2 regularization. Must be a scalar.", "grad": "A Tensor. Must have the same type as var. The gradient.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA vector of indices into the first dimension of var and accum.", "use_locking": "An optional bool. Defaults to False.\nIf True, the subtraction will be protected by a lock;\notherwise the behavior is undefined, but may exhibit less contention.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as var."}, "tf.raw_ops.SparseApplyRMSProp": {"description": "Update &#39;*var&#39; according to the RMSProp algorithm.", "Args": {"var": "A mutable Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nShould be from a Variable().", "ms": "A mutable Tensor. Must have the same type as var.\nShould be from a Variable().", "mom": "A mutable Tensor. Must have the same type as var.\nShould be from a Variable().", "lr": "A Tensor. Must have the same type as var.\nScaling factor. Must be a scalar.", "rho": "A Tensor. Must have the same type as var.\nDecay rate. Must be a scalar.", "momentum": "A Tensor. Must have the same type as var.", "epsilon": "A Tensor. Must have the same type as var.\nRidge term. Must be a scalar.", "grad": "A Tensor. Must have the same type as var. The gradient.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA vector of indices into the first dimension of var, ms and mom.", "use_locking": "An optional bool. Defaults to False.\nIf True, updating of the var, ms, and mom tensors is protected\nby a lock; otherwise the behavior is undefined, but may exhibit less\ncontention.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as var."}, "tf.raw_ops.SparseBincount": {"description": "Counts the number of occurrences of each value in an integer array.", "Args": {"indices": "A Tensor of type int64. 2D int64 Tensor.", "values": "A Tensor. Must be one of the following types: int32, int64.\n1D int Tensor.", "dense_shape": "A Tensor of type int64. 1D int64 Tensor.", "size": "A Tensor. Must have the same type as values.\nnon-negative int scalar Tensor.", "weights": "A Tensor. Must be one of the following types: int32, int64, float32, float64.\nis an int32, int64, float32, or float64 Tensor with the same\nshape as input, or a length-0 Tensor, in which case it acts as all weights\nequal to 1.", "binary_output": "An optional bool. Defaults to False.\nbool; Whether the kernel should count the appearance or number of occurrences.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as weights."}, "tf.raw_ops.SparseConcat": {"description": "Concatenates a list of SparseTensor along the specified dimension.", "Args": {"indices": "A list of at least 2 Tensor objects with type int64.\n2-D.  Indices of each input SparseTensor.", "values": "A list with the same length as indices of Tensor objects with the same type.\n1-D.  Non-empty values of each SparseTensor.", "shapes": "A list with the same length as indices of Tensor objects with type int64.\n1-D.  Shapes of each SparseTensor.", "concat_dim": "An int.\nDimension to concatenate along. Must be in range [-rank, rank),\nwhere rank is the number of dimensions in each input SparseTensor.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output_indices, output_values, output_shape)."}, "tf.raw_ops.SparseConditionalAccumulator": {"description": "A conditional accumulator for aggregating sparse gradients.", "Args": {"dtype": "A tf.DType from: tf.float32, tf.float64, tf.int32, tf.uint8, tf.int16, tf.int8, tf.complex64, tf.int64, tf.qint8, tf.quint8, tf.qint32, tf.bfloat16, tf.uint16, tf.complex128, tf.half, tf.uint32, tf.uint64.\nThe type of the value being accumulated.", "shape": "A tf.TensorShape or list of ints. The shape of the values.", "container": "An optional string. Defaults to \"\".\nIf non-empty, this accumulator is placed in the given container.\nOtherwise, a default container is used.", "shared_name": "An optional string. Defaults to \"\".\nIf non-empty, this accumulator will be shared under the given name\nacross multiple sessions.", "reduction_type": "An optional string from: \"MEAN\", \"SUM\". Defaults to \"MEAN\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type mutable string."}, "tf.raw_ops.SparseCountSparseOutput": {"description": "Performs sparse-output bin counting for a sparse tensor input.", "Args": {"indices": "A Tensor of type int64.\nTensor containing the indices of the sparse tensor to count.", "values": "A Tensor. Must be one of the following types: int32, int64.\nTensor containing values of the sparse tensor to count.", "dense_shape": "A Tensor of type int64.\nTensor containing the dense shape of the sparse tensor to count.", "weights": "A Tensor. Must be one of the following types: int32, int64, float32, float64.\nA Tensor of the same shape as indices containing per-index weight values.\nMay also be the empty tensor if no weights are used.", "binary_output": "A bool.\nWhether to output the number of occurrences of each value or 1.", "minlength": "An optional int that is >= -1. Defaults to -1.\nMinimum value to count. Can be set to -1 for no minimum.", "maxlength": "An optional int that is >= -1. Defaults to -1.\nMaximum value to count. Can be set to -1 for no maximum.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output_indices, output_values, output_dense_shape)."}, "tf.raw_ops.SparseCross": {"description": "Generates sparse cross from a list of sparse and dense tensors.", "Args": {"indices": "A list of Tensor objects with type int64.\n2-D.  Indices of each input SparseTensor.", "values": "A list of Tensor objects with types from: int64, string.\n1-D.   values of each SparseTensor.", "shapes": "A list with the same length as indices of Tensor objects with type int64.\n1-D.   Shapes of each SparseTensor.", "dense_inputs": "A list of Tensor objects with types from: int64, string.\n2-D.    Columns represented by dense Tensor.", "hashed_output": "A bool.\nIf true, returns the hash of the cross instead of the string.\nThis will allow us avoiding string manipulations.", "num_buckets": "An int that is >= 0. It is used if hashed_output is true.\noutput = hashed_value%num_buckets if num_buckets > 0 else hashed_value.", "hash_key": "An int.\nSpecify the hash_key that will be used by the FingerprintCat64\nfunction to combine the crosses fingerprints.", "out_type": "A tf.DType from: tf.int64, tf.string.", "internal_type": "A tf.DType from: tf.int64, tf.string.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output_indices, output_values, output_shape)."}, "tf.raw_ops.SparseCrossHashed": {"description": "Generates sparse cross from a list of sparse and dense tensors.", "Args": {"indices": "A list of Tensor objects with type int64.\n2-D.  Indices of each input SparseTensor.", "values": "A list of Tensor objects with types from: int64, string.\n1-D.   values of each SparseTensor.", "shapes": "A list with the same length as indices of Tensor objects with type int64.\n1-D.   Shapes of each SparseTensor.", "dense_inputs": "A list of Tensor objects with types from: int64, string.\n2-D.    Columns represented by dense Tensor.", "num_buckets": "A Tensor of type int64.\nIt is used if hashed_output is true.\noutput = hashed_value%num_buckets if num_buckets > 0 else hashed_value.", "strong_hash": "A Tensor of type bool.\nboolean, if true, siphash with salt will be used instead of farmhash.", "salt": "A Tensor of type int64.\nSpecify the salt that will be used by the siphash function.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output_indices, output_values, output_shape)."}, "tf.raw_ops.SparseCrossV2": {"description": "Generates sparse cross from a list of sparse and dense tensors.", "Args": {"indices": "A list of Tensor objects with type int64.\n2-D.  Indices of each input SparseTensor.", "values": "A list of Tensor objects with types from: int64, string.\n1-D.   values of each SparseTensor.", "shapes": "A list with the same length as indices of Tensor objects with type int64.\n1-D.   Shapes of each SparseTensor.", "dense_inputs": "A list of Tensor objects with types from: int64, string.\n2-D.    Columns represented by dense Tensor.", "sep": "A Tensor of type string.\nstring used when joining a list of string inputs, can be used as separator later.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output_indices, output_values, output_shape)."}, "tf.raw_ops.SparseDenseCwiseAdd": {"description": "Adds up a SparseTensor and a dense Tensor, using these special rules:", "Args": {"sp_indices": "A Tensor of type int64.\n2-D.  N x R matrix with the indices of non-empty values in a\nSparseTensor, possibly not in canonical ordering.", "sp_values": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\n1-D.  N non-empty values corresponding to sp_indices.", "sp_shape": "A Tensor of type int64.\n1-D.  Shape of the input SparseTensor.", "dense": "A Tensor. Must have the same type as sp_values.\nR-D.  The dense Tensor operand.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as sp_values."}, "tf.raw_ops.SparseDenseCwiseDiv": {"description": "Component-wise divides a SparseTensor by a dense Tensor.", "Args": {"sp_indices": "A Tensor of type int64.\n2-D.  N x R matrix with the indices of non-empty values in a\nSparseTensor, possibly not in canonical ordering.", "sp_values": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\n1-D.  N non-empty values corresponding to sp_indices.", "sp_shape": "A Tensor of type int64.\n1-D.  Shape of the input SparseTensor.", "dense": "A Tensor. Must have the same type as sp_values.\nR-D.  The dense Tensor operand.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as sp_values."}, "tf.raw_ops.SparseDenseCwiseMul": {"description": "Component-wise multiplies a SparseTensor by a dense Tensor.", "Args": {"sp_indices": "A Tensor of type int64.\n2-D.  N x R matrix with the indices of non-empty values in a\nSparseTensor, possibly not in canonical ordering.", "sp_values": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\n1-D.  N non-empty values corresponding to sp_indices.", "sp_shape": "A Tensor of type int64.\n1-D.  Shape of the input SparseTensor.", "dense": "A Tensor. Must have the same type as sp_values.\nR-D.  The dense Tensor operand.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as sp_values."}, "tf.raw_ops.SparseFillEmptyRows": {"description": "Fills empty rows in the input 2-D SparseTensor with a default value.", "Args": {"indices": "A Tensor of type int64.\n2-D. the indices of the sparse tensor.", "values": "A Tensor. 1-D. the values of the sparse tensor.", "dense_shape": "A Tensor of type int64.\n1-D. the shape of the sparse tensor.", "default_value": "A Tensor. Must have the same type as values.\n0-D. default value to insert into location [row, 0, ..., 0]\n  for rows missing from the input sparse tensor.\noutput indices: 2-D. the indices of the filled sparse tensor.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output_indices, output_values, empty_row_indicator, reverse_index_map)."}, "tf.raw_ops.SparseFillEmptyRowsGrad": {"description": "The gradient of SparseFillEmptyRows.", "Args": {"reverse_index_map": "A Tensor of type int64.\n1-D.  The reverse index map from SparseFillEmptyRows.", "grad_values": "A Tensor. 1-D.  The gradients from backprop.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (d_values, d_default_value)."}, "tf.raw_ops.SparseMatMul": {"description": "Multiply matrix &#34;a&#34; by matrix &#34;b&#34;.", "Args": {"a": "A Tensor. Must be one of the following types: float32, bfloat16.", "b": "A Tensor. Must be one of the following types: float32, bfloat16.", "transpose_a": "An optional bool. Defaults to False.", "transpose_b": "An optional bool. Defaults to False.", "a_is_sparse": "An optional bool. Defaults to False.", "b_is_sparse": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type float32."}, "tf.raw_ops.SparseMatrixAdd": {"description": "Sparse addition of two CSR matrices, C = alpha * A &#43; beta * B.", "Args": {"a": "A Tensor of type variant. A CSRSparseMatrix.", "b": "A Tensor of type variant. A CSRSparseMatrix.", "alpha": "A Tensor. Must be one of the following types: float32, float64, complex64, complex128.\nA constant scalar.", "beta": "A Tensor. Must have the same type as alpha. A constant scalar.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.SparseMatrixMatMul": {"description": "Matrix-multiplies a sparse matrix with a dense matrix.", "Args": {"a": "A Tensor of type variant. A CSRSparseMatrix.", "b": "A Tensor. A dense tensor.", "transpose_a": "An optional bool. Defaults to False.\nIndicates whether a should be transposed.", "transpose_b": "An optional bool. Defaults to False.\nIndicates whether b should be transposed.", "adjoint_a": "An optional bool. Defaults to False.\nIndicates whether a should be conjugate-transposed.", "adjoint_b": "An optional bool. Defaults to False.\nIndicates whether b should be conjugate-transposed.", "transpose_output": "An optional bool. Defaults to False.\nTransposes the product of a and b.", "conjugate_output": "An optional bool. Defaults to False.\nConjugates the product of a and b.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as b."}, "tf.raw_ops.SparseMatrixMul": {"description": "Element-wise multiplication of a sparse matrix with a dense tensor.", "Args": {"a": "A Tensor of type variant. A CSRSparseMatrix.", "b": "A Tensor. A dense tensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.SparseMatrixNNZ": {"description": "Returns the number of nonzeroes of sparse_matrix.", "Args": {"sparse_matrix": "A Tensor of type variant. A CSRSparseMatrix.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int32."}, "tf.raw_ops.SparseMatrixOrderingAMD": {"description": "Computes the Approximate Minimum Degree (AMD) ordering of input.", "Args": {"input": "A Tensor of type variant. A CSRSparseMatrix.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int32."}, "tf.raw_ops.SparseMatrixSoftmax": {"description": "Calculates the softmax of a CSRSparseMatrix.", "Args": {"logits": "A Tensor of type variant. A CSRSparseMatrix.", "type": "A tf.DType from: tf.float32, tf.float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.SparseMatrixSoftmaxGrad": {"description": "Calculates the gradient of the SparseMatrixSoftmax op.", "Args": {"softmax": "A Tensor of type variant. A CSRSparseMatrix.", "grad_softmax": "A Tensor of type variant. The gradient of softmax.", "type": "A tf.DType from: tf.float32, tf.float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.SparseMatrixSparseCholesky": {"description": "Computes the sparse Cholesky decomposition of input.", "Args": {"input": "A Tensor of type variant. A CSRSparseMatrix.", "permutation": "A Tensor of type int32.\nA fill-in reducing permutation matrix.", "type": "A tf.DType from: tf.float32, tf.float64, tf.complex64, tf.complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.SparseMatrixSparseMatMul": {"description": "Sparse-matrix-multiplies two CSR matrices a and b.", "Args": {"a": "A Tensor of type variant. A CSRSparseMatrix.", "b": "A Tensor of type variant. A CSRSparseMatrix.", "type": "A tf.DType from: tf.float32, tf.float64, tf.complex64, tf.complex128.", "transpose_a": "An optional bool. Defaults to False.\nIndicates whether a should be transposed.", "transpose_b": "An optional bool. Defaults to False.\nIndicates whether b should be transposed.", "adjoint_a": "An optional bool. Defaults to False.\nIndicates whether a should be conjugate-transposed.", "adjoint_b": "An optional bool. Defaults to False.\nIndicates whether b should be conjugate-transposed.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.SparseMatrixTranspose": {"description": "Transposes the inner (matrix) dimensions of a CSRSparseMatrix.", "Args": {"input": "A Tensor of type variant. A CSRSparseMatrix.", "type": "A tf.DType from: tf.float32, tf.float64, tf.complex64, tf.complex128.", "conjugate": "An optional bool. Defaults to False.\nIndicates whether input should be conjugated.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.SparseMatrixZeros": {"description": "Creates an all-zeros CSRSparseMatrix with shape dense_shape.", "Args": {"dense_shape": "A Tensor of type int64. The desired matrix shape.", "type": "A tf.DType from: tf.float32, tf.float64, tf.complex64, tf.complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.SparseReduceMax": {"description": "Computes the max of elements across dimensions of a SparseTensor.", "Args": {"input_indices": "A Tensor of type int64.\n2-D.  N x R matrix with the indices of non-empty values in a\nSparseTensor, possibly not in canonical ordering.", "input_values": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.\n1-D.  N non-empty values corresponding to input_indices.", "input_shape": "A Tensor of type int64.\n1-D.  Shape of the input SparseTensor.", "reduction_axes": "A Tensor of type int32.\n1-D.  Length-K vector containing the reduction axes.", "keep_dims": "An optional bool. Defaults to False.\nIf true, retain reduced dimensions with length 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input_values."}, "tf.raw_ops.SparseReduceMaxSparse": {"description": "Computes the max of elements across dimensions of a SparseTensor.", "Args": {"input_indices": "A Tensor of type int64.\n2-D.  N x R matrix with the indices of non-empty values in a\nSparseTensor, possibly not in canonical ordering.", "input_values": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.\n1-D.  N non-empty values corresponding to input_indices.", "input_shape": "A Tensor of type int64.\n1-D.  Shape of the input SparseTensor.", "reduction_axes": "A Tensor of type int32.\n1-D.  Length-K vector containing the reduction axes.", "keep_dims": "An optional bool. Defaults to False.\nIf true, retain reduced dimensions with length 1.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output_indices, output_values, output_shape)."}, "tf.raw_ops.SparseReduceSum": {"description": "Computes the sum of elements across dimensions of a SparseTensor.", "Args": {"input_indices": "A Tensor of type int64.\n2-D.  N x R matrix with the indices of non-empty values in a\nSparseTensor, possibly not in canonical ordering.", "input_values": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\n1-D.  N non-empty values corresponding to input_indices.", "input_shape": "A Tensor of type int64.\n1-D.  Shape of the input SparseTensor.", "reduction_axes": "A Tensor of type int32.\n1-D.  Length-K vector containing the reduction axes.", "keep_dims": "An optional bool. Defaults to False.\nIf true, retain reduced dimensions with length 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input_values."}, "tf.raw_ops.SparseReduceSumSparse": {"description": "Computes the sum of elements across dimensions of a SparseTensor.", "Args": {"input_indices": "A Tensor of type int64.\n2-D.  N x R matrix with the indices of non-empty values in a\nSparseTensor, possibly not in canonical ordering.", "input_values": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\n1-D.  N non-empty values corresponding to input_indices.", "input_shape": "A Tensor of type int64.\n1-D.  Shape of the input SparseTensor.", "reduction_axes": "A Tensor of type int32.\n1-D.  Length-K vector containing the reduction axes.", "keep_dims": "An optional bool. Defaults to False.\nIf true, retain reduced dimensions with length 1.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output_indices, output_values, output_shape)."}, "tf.raw_ops.SparseReorder": {"description": "Reorders a SparseTensor into the canonical, row-major ordering.", "Args": {"input_indices": "A Tensor of type int64.\n2-D.  N x R matrix with the indices of non-empty values in a\nSparseTensor, possibly not in canonical ordering.", "input_values": "A Tensor.\n1-D.  N non-empty values corresponding to input_indices.", "input_shape": "A Tensor of type int64.\n1-D.  Shape of the input SparseTensor.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output_indices, output_values)."}, "tf.raw_ops.SparseReshape": {"description": "Reshapes a SparseTensor to represent values in a new dense shape.", "Args": {"input_indices": "A Tensor of type int64.\n2-D.  N x R_in matrix with the indices of non-empty values in a\nSparseTensor.", "input_shape": "A Tensor of type int64.\n1-D.  R_in vector with the input SparseTensor's dense shape.", "new_shape": "A Tensor of type int64.\n1-D.  R_out vector with the requested new dense shape.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output_indices, output_shape)."}, "tf.raw_ops.SparseSegmentMean": {"description": "Computes the mean along sparse segments of a tensor.", "Args": {"data": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA 1-D tensor. Has same rank as segment_ids.", "segment_ids": "A Tensor. Must be one of the following types: int32, int64.\nA 1-D tensor. Values should be sorted and can be repeated.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as data."}, "tf.raw_ops.SparseSegmentMeanGrad": {"description": "Computes gradients for SparseSegmentMean.", "Args": {"grad": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.\ngradient propagated to the SparseSegmentMean op.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nindices passed to the corresponding SparseSegmentMean op.", "segment_ids": "A Tensor. Must be one of the following types: int32, int64.\nsegment_ids passed to the corresponding SparseSegmentMean op.", "output_dim0": "A Tensor of type int32.\ndimension 0 of \"data\" passed to SparseSegmentMean op.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as grad."}, "tf.raw_ops.SparseSegmentMeanWithNumSegments": {"description": "Computes the mean along sparse segments of a tensor.", "Args": {"data": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA 1-D tensor. Has same rank as segment_ids.", "segment_ids": "A Tensor. Must be one of the following types: int32, int64.\nA 1-D tensor. Values should be sorted and can be repeated.", "num_segments": "A Tensor. Must be one of the following types: int32, int64.\nShould equal the number of distinct segment IDs.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as data."}, "tf.raw_ops.SparseSegmentSqrtN": {"description": "Computes the sum along sparse segments of a tensor divided by the sqrt of N.", "Args": {"data": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA 1-D tensor. Has same rank as segment_ids.", "segment_ids": "A Tensor. Must be one of the following types: int32, int64.\nA 1-D tensor. Values should be sorted and can be repeated.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as data."}, "tf.raw_ops.SparseSegmentSqrtNGrad": {"description": "Computes gradients for SparseSegmentSqrtN.", "Args": {"grad": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.\ngradient propagated to the SparseSegmentSqrtN op.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nindices passed to the corresponding SparseSegmentSqrtN op.", "segment_ids": "A Tensor. Must be one of the following types: int32, int64.\nsegment_ids passed to the corresponding SparseSegmentSqrtN op.", "output_dim0": "A Tensor of type int32.\ndimension 0 of \"data\" passed to SparseSegmentSqrtN op.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as grad."}, "tf.raw_ops.SparseSegmentSqrtNWithNumSegments": {"description": "Computes the sum along sparse segments of a tensor divided by the sqrt of N.", "Args": {"data": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA 1-D tensor. Has same rank as segment_ids.", "segment_ids": "A Tensor. Must be one of the following types: int32, int64.\nA 1-D tensor. Values should be sorted and can be repeated.", "num_segments": "A Tensor. Must be one of the following types: int32, int64.\nShould equal the number of distinct segment IDs.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as data."}, "tf.raw_ops.SparseSegmentSum": {"description": "Computes the sum along sparse segments of a tensor.", "Args": {"data": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA 1-D tensor. Has same rank as segment_ids.", "segment_ids": "A Tensor. Must be one of the following types: int32, int64.\nA 1-D tensor. Values should be sorted and can be repeated.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as data."}, "tf.raw_ops.SparseSegmentSumGrad": {"description": "Computes gradients for SparseSegmentSum.", "Args": {"grad": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.\ngradient propagated to the SparseSegmentSum op.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nindices passed to the corresponding SparseSegmentSum op.", "segment_ids": "A Tensor. Must be one of the following types: int32, int64.\nsegment_ids passed to the corresponding SparseSegmentSum op.", "output_dim0": "A Tensor of type int32.\ndimension 0 of \"data\" passed to SparseSegmentSum op.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as grad."}, "tf.raw_ops.SparseSegmentSumWithNumSegments": {"description": "Computes the sum along sparse segments of a tensor.", "Args": {"data": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nA 1-D tensor. Has same rank as segment_ids.", "segment_ids": "A Tensor. Must be one of the following types: int32, int64.\nA 1-D tensor. Values should be sorted and can be repeated.", "num_segments": "A Tensor. Must be one of the following types: int32, int64.\nShould equal the number of distinct segment IDs.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as data."}, "tf.raw_ops.SparseSlice": {"description": "Slice a SparseTensor based on the start and size.", "Args": {"indices": "A Tensor of type int64.\n2-D tensor represents the indices of the sparse tensor.", "values": "A Tensor. 1-D tensor represents the values of the sparse tensor.", "shape": "A Tensor of type int64.\n1-D. tensor represents the shape of the sparse tensor.", "start": "A Tensor of type int64.\n1-D. tensor represents the start of the slice.", "size": "A Tensor of type int64.\n1-D. tensor represents the size of the slice.\noutput indices: A list of 1-D tensors represents the indices of the output\nsparse tensors.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output_indices, output_values, output_shape)."}, "tf.raw_ops.SparseSliceGrad": {"description": "The gradient operator for the SparseSlice op.", "Args": {"backprop_val_grad": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\n1-D. The gradient with respect to\nthe non-empty values of the sliced SparseTensor.", "input_indices": "A Tensor of type int64.\n2-D.  The indices of the input SparseTensor.", "input_start": "A Tensor of type int64.\n1-D. tensor represents the start of the slice.", "output_indices": "A Tensor of type int64.\n2-D.  The indices of the sliced SparseTensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as backprop_val_grad."}, "tf.raw_ops.SparseSoftmax": {"description": "Applies softmax to a batched N-D SparseTensor.", "Args": {"sp_indices": "A Tensor of type int64.\n2-D.  NNZ x R matrix with the indices of non-empty values in a\nSparseTensor, in canonical ordering.", "sp_values": "A Tensor. Must be one of the following types: float32, float64.\n1-D.  NNZ non-empty values corresponding to sp_indices.", "sp_shape": "A Tensor of type int64.\n1-D.  Shape of the input SparseTensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as sp_values."}, "tf.raw_ops.SparseSoftmaxCrossEntropyWithLogits": {"description": "Computes softmax cross entropy cost and gradients to backpropagate.", "Args": {"features": "A Tensor. Must be one of the following types: half, bfloat16, float32, float64.\nbatch_size x num_classes matrix", "labels": "A Tensor. Must be one of the following types: int32, int64.\nbatch_size vector with values in [0, num_classes).\nThis is the label for the given minibatch entry.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (loss, backprop)."}, "tf.raw_ops.SparseSparseMaximum": {"description": "Returns the element-wise max of two SparseTensors.", "Args": {"a_indices": "A Tensor of type int64.\n2-D.  N x R matrix with the indices of non-empty values in a\nSparseTensor, in the canonical lexicographic ordering.", "a_values": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.\n1-D.  N non-empty values corresponding to a_indices.", "a_shape": "A Tensor of type int64.\n1-D.  Shape of the input SparseTensor.", "b_indices": "A Tensor of type int64.\ncounterpart to a_indices for the other operand.", "b_values": "A Tensor. Must have the same type as a_values.\ncounterpart to a_values for the other operand; must be of the same dtype.", "b_shape": "A Tensor of type int64.\ncounterpart to a_shape for the other operand; the two shapes must be equal.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output_indices, output_values)."}, "tf.raw_ops.SparseSparseMinimum": {"description": "Returns the element-wise min of two SparseTensors.", "Args": {"a_indices": "A Tensor of type int64.\n2-D.  N x R matrix with the indices of non-empty values in a\nSparseTensor, in the canonical lexicographic ordering.", "a_values": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\n1-D.  N non-empty values corresponding to a_indices.", "a_shape": "A Tensor of type int64.\n1-D.  Shape of the input SparseTensor.", "b_indices": "A Tensor of type int64.\ncounterpart to a_indices for the other operand.", "b_values": "A Tensor. Must have the same type as a_values.\ncounterpart to a_values for the other operand; must be of the same dtype.", "b_shape": "A Tensor of type int64.\ncounterpart to a_shape for the other operand; the two shapes must be equal.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output_indices, output_values)."}, "tf.raw_ops.SparseSplit": {"description": "Split a SparseTensor into num_split tensors along one dimension.", "Args": {"split_dim": "A Tensor of type int64.\n0-D.  The dimension along which to split.  Must be in the range\n[0, rank(shape)).", "indices": "A Tensor of type int64.\n2-D tensor represents the indices of the sparse tensor.", "values": "A Tensor. 1-D tensor represents the values of the sparse tensor.", "shape": "A Tensor of type int64.\n1-D. tensor represents the shape of the sparse tensor.\noutput indices: A list of 1-D tensors represents the indices of the output\nsparse tensors.", "num_split": "An int that is >= 1. The number of ways to split.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output_indices, output_values, output_shape)."}, "tf.raw_ops.SparseTensorDenseAdd": {"description": "Adds up a SparseTensor and a dense Tensor, producing a dense Tensor.", "Args": {"a_indices": "A Tensor. Must be one of the following types: int32, int64.\n2-D.  The indices of the SparseTensor, with shape [nnz, ndims].", "a_values": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\n1-D.  The values of the SparseTensor, with shape [nnz].", "a_shape": "A Tensor. Must have the same type as a_indices.\n1-D.  The shape of the SparseTensor, with shape [ndims].", "b": "A Tensor. Must have the same type as a_values.\nndims-D Tensor.  With shape a_shape.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as a_values."}, "tf.raw_ops.SparseTensorDenseMatMul": {"description": "Multiply SparseTensor (of rank 2) &#34;A&#34; by dense matrix &#34;B&#34;.", "Args": {"a_indices": "A Tensor. Must be one of the following types: int32, int64.\n2-D.  The indices of the SparseTensor, size [nnz, 2] Matrix.", "a_values": "A Tensor.\n1-D.  The values of the SparseTensor, size [nnz] Vector.", "a_shape": "A Tensor of type int64.\n1-D.  The shape of the SparseTensor, size [2] Vector.", "b": "A Tensor. Must have the same type as a_values.\n2-D.  A dense Matrix.", "adjoint_a": "An optional bool. Defaults to False.\nUse the adjoint of A in the matrix multiply.  If A is complex, this\nis transpose(conj(A)).  Otherwise it's transpose(A).", "adjoint_b": "An optional bool. Defaults to False.\nUse the adjoint of B in the matrix multiply.  If B is complex, this\nis transpose(conj(B)).  Otherwise it's transpose(B).", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as a_values."}, "tf.raw_ops.SparseTensorSliceDataset": {"description": "Creates a dataset that splits a SparseTensor into elements row-wise.", "Args": {"indices": "A Tensor of type int64.", "values": "A Tensor.", "dense_shape": "A Tensor of type int64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.SparseTensorToCSRSparseMatrix": {"description": "Converts a SparseTensor to a (possibly batched) CSRSparseMatrix.", "Args": {"indices": "A Tensor of type int64. SparseTensor indices.", "values": "A Tensor. Must be one of the following types: float32, float64, complex64, complex128.\nSparseTensor values.", "dense_shape": "A Tensor of type int64. SparseTensor dense shape.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.SparseToDense": {"description": "Converts a sparse representation into a dense tensor.", "Args": {"sparse_indices": "A Tensor. Must be one of the following types: int32, int64.\n0-D, 1-D, or 2-D.  sparse_indices[i] contains the complete\nindex where sparse_values[i] will be placed.", "output_shape": "A Tensor. Must have the same type as sparse_indices.\n1-D.  Shape of the dense output tensor.", "sparse_values": "A Tensor.\n1-D.  Values corresponding to each row of sparse_indices,\nor a scalar value to be used for all sparse indices.", "default_value": "A Tensor. Must have the same type as sparse_values.\nScalar value to set for indices not specified in\nsparse_indices.", "validate_indices": "An optional bool. Defaults to True.\nIf true, indices are checked to make sure they are sorted in\nlexicographic order and that there are no repeats.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as sparse_values."}, "tf.raw_ops.SparseToSparseSetOperation": {"description": "Applies set operation along last dimension of 2 SparseTensor inputs.", "Args": {"set1_indices": "A Tensor of type int64.\n2D Tensor, indices of a SparseTensor. Must be in row-major\norder.", "set1_values": "A Tensor. Must be one of the following types: int8, int16, int32, int64, uint8, uint16, string.\n1D Tensor, values of a SparseTensor. Must be in row-major\norder.", "set1_shape": "A Tensor of type int64.\n1D Tensor, shape of a SparseTensor. set1_shape[0...n-1] must\nbe the same as set2_shape[0...n-1], set1_shape[n] is the\nmax set size across 0...n-1 dimensions.", "set2_indices": "A Tensor of type int64.\n2D Tensor, indices of a SparseTensor. Must be in row-major\norder.", "set2_values": "A Tensor. Must have the same type as set1_values.\n1D Tensor, values of a SparseTensor. Must be in row-major\norder.", "set2_shape": "A Tensor of type int64.\n1D Tensor, shape of a SparseTensor. set2_shape[0...n-1] must\nbe the same as set1_shape[0...n-1], set2_shape[n] is the\nmax set size across 0...n-1 dimensions.", "set_operation": "A string.", "validate_indices": "An optional bool. Defaults to True.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (result_indices, result_values, result_shape)."}, "tf.raw_ops.Spence": {"Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.Split": {"description": "Splits a tensor into num_split tensors along one dimension.", "Args": {"axis": "A Tensor of type int32.\n0-D.  The dimension along which to split.  Must be in the range\n[-rank(value), rank(value)).", "value": "A Tensor. The tensor to split.", "num_split": "An int that is >= 1.\nThe number of ways to split.  Must evenly divide\nvalue.shape[split_dim].", "name": "A name for the operation (optional)."}, "Returns": "A list of num_split Tensor objects with the same type as value."}, "tf.raw_ops.SplitV": {"description": "Splits a tensor into num_split tensors along one dimension.", "Args": {"value": "A Tensor. The tensor to split.", "size_splits": "A Tensor. Must be one of the following types: int32, int64.\nlist containing the sizes of each output tensor along the split\ndimension. Must sum to the dimension of value along split_dim.\nCan contain one -1 indicating that dimension is to be inferred.", "axis": "A Tensor of type int32.\n0-D.  The dimension along which to split.  Must be in the range\n[-rank(value), rank(value)).", "num_split": "An int that is >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A list of num_split Tensor objects with the same type as value."}, "tf.raw_ops.SqlDataset": {"description": "Creates a dataset that executes a SQL query and emits rows of the result set.", "Args": {"driver_name": "A Tensor of type string.\nThe database type. Currently, the only supported type is 'sqlite'.", "data_source_name": "A Tensor of type string.\nA connection string to connect to the database.", "query": "A Tensor of type string. A SQL query to execute.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.Sqrt": {"description": "Computes square root of x element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, complex64, complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.SqrtGrad": {"description": "Computes the gradient for the sqrt of x wrt its input.", "Args": {"y": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, complex64, complex128.", "dy": "A Tensor. Must have the same type as y.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as y."}, "tf.raw_ops.Square": {"description": "Computes square of x element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, int8, int16, int32, int64, uint8, uint16, uint32, uint64, complex64, complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.SquaredDifference": {"description": "Returns conj(x - y)(x - y) element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, int32, int64, complex64, complex128.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.Squeeze": {"description": "Removes dimensions of size 1 from the shape of a tensor.", "Args": {"input": "A Tensor. The input to squeeze.", "axis": "An optional list of ints. Defaults to [].\nIf specified, only squeezes the dimensions listed. The dimension\nindex starts at 0. It is an error to squeeze a dimension that is not 1. Must\nbe in the range [-rank(input), rank(input)).", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.Stack": {"description": "Deprecated, use StackV2.", "Args": {"elem_type": "A tf.DType.", "stack_name": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type mutable string."}, "tf.raw_ops.StackClose": {"description": "Deprecated, use StackCloseV2.", "Args": {"handle": "A Tensor of type mutable string.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.StackCloseV2": {"description": "Delete the stack from its resource container.", "Args": {"handle": "A Tensor of type resource. The handle to a stack.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.StackPop": {"description": "Deprecated, use StackPopV2.", "Args": {"handle": "A Tensor of type mutable string.", "elem_type": "A tf.DType.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type elem_type."}, "tf.raw_ops.StackPopV2": {"description": "Pop the element at the top of the stack.", "Args": {"handle": "A Tensor of type resource. The handle to a stack.", "elem_type": "A tf.DType. The type of the elem that is popped.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type elem_type."}, "tf.raw_ops.StackPush": {"description": "Deprecated, use StackPushV2.", "Args": {"handle": "A Tensor of type mutable string.", "elem": "A Tensor.", "swap_memory": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as elem."}, "tf.raw_ops.StackPushV2": {"description": "Push an element onto the stack.", "Args": {"handle": "A Tensor of type resource. The handle to a stack.", "elem": "A Tensor. The tensor to be pushed onto the stack.", "swap_memory": "An optional bool. Defaults to False.\nSwap elem to CPU. Default to false.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as elem."}, "tf.raw_ops.StackV2": {"description": "A stack that produces elements in first-in last-out order.", "Args": {"max_size": "A Tensor of type int32.\nThe maximum size of the stack if non-negative. If negative, the stack\nsize is unlimited.", "elem_type": "A tf.DType. The type of the elements on the stack.", "stack_name": "An optional string. Defaults to \"\".\nOverrides the name used for the temporary stack resource. Default\nvalue is the name of the 'Stack' op (which is guaranteed unique).", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type resource."}, "tf.raw_ops.Stage": {"description": "Stage values similar to a lightweight Enqueue.", "Args": {"values": "A list of Tensor objects. a list of tensors\ndtypes A list of data types that inserted values should adhere to.", "capacity": "An optional int that is >= 0. Defaults to 0.\nMaximum number of elements in the Staging Area. If > 0, inserts\non the container will block when the capacity is reached.", "memory_limit": "An optional int that is >= 0. Defaults to 0.\nThe maximum number of bytes allowed for Tensors in the Staging Area.\nIf > 0, inserts will block until sufficient space is available.", "container": "An optional string. Defaults to \"\".\nIf non-empty, this queue is placed in the given container. Otherwise,\na default container is used.", "shared_name": "An optional string. Defaults to \"\".\nIt is necessary to match this name to the matching Unstage Op.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.StageClear": {"description": "Op removes all elements in the underlying container.", "Args": {"dtypes": "A list of tf.DTypes.", "capacity": "An optional int that is >= 0. Defaults to 0.", "memory_limit": "An optional int that is >= 0. Defaults to 0.", "container": "An optional string. Defaults to \"\".", "shared_name": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.StagePeek": {"description": "Op peeks at the values at the specified index. If the", "Args": {"index": "A Tensor of type int32.", "dtypes": "A list of tf.DTypes that has length >= 1.", "capacity": "An optional int that is >= 0. Defaults to 0.", "memory_limit": "An optional int that is >= 0. Defaults to 0.", "container": "An optional string. Defaults to \"\".", "shared_name": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A list of Tensor objects of type dtypes."}, "tf.raw_ops.StageSize": {"description": "Op returns the number of elements in the underlying container.", "Args": {"dtypes": "A list of tf.DTypes.", "capacity": "An optional int that is >= 0. Defaults to 0.", "memory_limit": "An optional int that is >= 0. Defaults to 0.", "container": "An optional string. Defaults to \"\".", "shared_name": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int32."}, "tf.raw_ops.StatefulPartitionedCall": {"description": "returns f(inputs), where f&#39;s body is placed and partitioned.", "Args": {"args": "A list of Tensor objects. A list of input tensors.", "Tout": "A list of tf.DTypes. A list of output types.", "f": "A function decorated with @Defun.\nA function that takes 'args', a list of tensors, and returns 'output',\nanother list of tensors. Input and output types are specified by 'Tin'\nand 'Tout'. The function body of f will be placed and partitioned across\ndevices, setting this op apart from the regular Call op. This op is\nstateful.", "config": "An optional string. Defaults to \"\".", "config_proto": "An optional string. Defaults to \"\".", "executor_type": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A list of Tensor objects of type Tout."}, "tf.raw_ops.StatefulRandomBinomial": {"Args": {"resource": "A Tensor of type resource.", "algorithm": "A Tensor of type int64.", "shape": "A Tensor. Must be one of the following types: int32, int64.", "counts": "A Tensor. Must be one of the following types: half, float32, float64, int32, int64.", "probs": "A Tensor. Must have the same type as counts.", "dtype": "An optional tf.DType from: tf.half, tf.float32, tf.float64, tf.int32, tf.int64. Defaults to tf.int64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dtype."}, "tf.raw_ops.StatefulStandardNormal": {"description": "Outputs random values from a normal distribution. This op is deprecated in favor of op &#39;StatefulStandardNormalV2&#39;", "Args": {"resource": "A Tensor of type resource.\nThe handle of the resource variable that stores the state of the RNG.", "shape": "A Tensor. The shape of the output tensor.", "dtype": "An optional tf.DType. Defaults to tf.float32.\nThe type of the output.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dtype."}, "tf.raw_ops.StatefulStandardNormalV2": {"description": "Outputs random values from a normal distribution.", "Args": {"resource": "A Tensor of type resource.\nThe handle of the resource variable that stores the state of the RNG.", "algorithm": "A Tensor of type int64. The RNG algorithm.", "shape": "A Tensor. The shape of the output tensor.", "dtype": "An optional tf.DType. Defaults to tf.float32.\nThe type of the output.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dtype."}, "tf.raw_ops.StatefulTruncatedNormal": {"description": "Outputs random values from a truncated normal distribution.", "Args": {"resource": "A Tensor of type resource.\nThe handle of the resource variable that stores the state of the RNG.", "algorithm": "A Tensor of type int64. The RNG algorithm.", "shape": "A Tensor. The shape of the output tensor.", "dtype": "An optional tf.DType. Defaults to tf.float32.\nThe type of the output.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dtype."}, "tf.raw_ops.StatefulUniform": {"description": "Outputs random values from a uniform distribution.", "Args": {"resource": "A Tensor of type resource.\nThe handle of the resource variable that stores the state of the RNG.", "algorithm": "A Tensor of type int64. The RNG algorithm.", "shape": "A Tensor. The shape of the output tensor.", "dtype": "An optional tf.DType. Defaults to tf.float32.\nThe type of the output.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dtype."}, "tf.raw_ops.StatefulUniformFullInt": {"description": "Outputs random integers from a uniform distribution.", "Args": {"resource": "A Tensor of type resource.\nThe handle of the resource variable that stores the state of the RNG.", "algorithm": "A Tensor of type int64. The RNG algorithm.", "shape": "A Tensor. The shape of the output tensor.", "dtype": "An optional tf.DType. Defaults to tf.uint64.\nThe type of the output.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dtype."}, "tf.raw_ops.StatefulUniformInt": {"description": "Outputs random integers from a uniform distribution.", "Args": {"resource": "A Tensor of type resource.\nThe handle of the resource variable that stores the state of the RNG.", "algorithm": "A Tensor of type int64. The RNG algorithm.", "shape": "A Tensor. The shape of the output tensor.", "minval": "A Tensor. Minimum value (inclusive, scalar).", "maxval": "A Tensor. Must have the same type as minval.\nMaximum value (exclusive, scalar).", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as minval."}, "tf.raw_ops.StatelessCase": {"description": "An n-way switch statement which calls a single branch function.", "Args": {"branch_index": "A Tensor of type int32.\nThe branch selector, an int32 Tensor.", "input": "A list of Tensor objects.\nA list of input tensors passed to the branch function.", "Tout": "A list of tf.DTypes. A list of output types.", "branches": "A list of functions decorated with @Defun that has length >= 1.\nA list of functions each of which takes 'inputs' and returns a list of\ntensors, whose types are the same as what every other branch returns.", "output_shapes": "An optional list of shapes (each a tf.TensorShape or list of ints). Defaults to [].", "name": "A name for the operation (optional)."}, "Returns": "A list of Tensor objects of type Tout."}, "tf.raw_ops.StatelessIf": {"description": "output = cond ? then_branch(input) : else_branch(input)", "Args": {"cond": "A Tensor.\nA Tensor. If the tensor is a scalar of non-boolean type, the\nscalar is converted to a boolean according to the\nfollowing rule: if the scalar is a numerical value, non-zero means\nTrue and zero means False; if the scalar is a string, non-empty\nmeans True and empty means False. If the tensor is not a scalar,\nbeing empty means False and being non-empty means True.\nThis should only be used when the if then/else body functions do not\nhave stateful ops.", "input": "A list of Tensor objects. A list of input tensors.", "Tout": "A list of tf.DTypes. A list of output types.", "then_branch": "A function decorated with @Defun.\nA function that takes 'inputs' and returns a list of tensors, whose\ntypes are the same as what else_branch returns.", "else_branch": "A function decorated with @Defun.\nA function that takes 'inputs' and returns a list of tensors, whose\ntypes are the same as what then_branch returns.", "output_shapes": "An optional list of shapes (each a tf.TensorShape or list of ints). Defaults to [].", "name": "A name for the operation (optional)."}, "Returns": "A list of Tensor objects of type Tout."}, "tf.raw_ops.StatelessMultinomial": {"description": "Draws samples from a multinomial distribution.", "Args": {"logits": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.\n2-D Tensor with shape [batch_size, num_classes].  Each slice [i, :]\nrepresents the unnormalized log probabilities for all classes.", "num_samples": "A Tensor of type int32.\n0-D.  Number of independent samples to draw for each row slice.", "seed": "A Tensor. Must be one of the following types: int32, int64.\n2 seeds (shape [2]).", "output_dtype": "An optional tf.DType from: tf.int32, tf.int64. Defaults to tf.int64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type output_dtype."}, "tf.raw_ops.StatelessParameterizedTruncatedNormal": {"Args": {"shape": "A Tensor. Must be one of the following types: int32, int64.\nThe shape of the output tensor.", "seed": "A Tensor. Must be one of the following types: int32, int64.\n2 seeds (shape [2]).", "means": "A Tensor. Must be one of the following types: half, float32, float64.\nThe mean parameter of each batch.", "stddevs": "A Tensor. Must have the same type as means.\nThe standard deviation parameter of each batch. Must be greater than 0.", "minvals": "A Tensor. Must have the same type as means.\nThe minimum cutoff. May be -infinity.", "maxvals": "A Tensor. Must have the same type as means.\nThe maximum cutoff. May be +infinity, and must be more than the minval\nfor each batch.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as means."}, "tf.raw_ops.StatelessRandomBinomial": {"description": "Outputs deterministic pseudorandom random numbers from a binomial distribution.", "Args": {"shape": "A Tensor. Must be one of the following types: int32, int64.\nThe shape of the output tensor.", "seed": "A Tensor. Must be one of the following types: int32, int64.\n2 seeds (shape [2]).", "counts": "A Tensor. Must be one of the following types: half, float32, float64, int32, int64.\nThe counts of the binomial distribution. Must be broadcastable with probs,\nand broadcastable with the rightmost dimensions of shape.", "probs": "A Tensor. Must have the same type as counts.\nThe probability of success for the binomial distribution. Must be broadcastable\nwith counts and broadcastable with the rightmost dimensions of shape.", "dtype": "An optional tf.DType from: tf.half, tf.float32, tf.float64, tf.int32, tf.int64. Defaults to tf.int64.\nThe type of the output.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dtype."}, "tf.raw_ops.StatelessRandomGammaV2": {"description": "Outputs deterministic pseudorandom random numbers from a gamma distribution.", "Args": {"shape": "A Tensor. Must be one of the following types: int32, int64.\nThe shape of the output tensor.", "seed": "A Tensor. Must be one of the following types: int32, int64.\n2 seeds (shape [2]).", "alpha": "A Tensor. Must be one of the following types: half, float32, float64.\nThe concentration of the gamma distribution. Shape must match the rightmost\ndimensions of shape.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as alpha."}, "tf.raw_ops.StatelessRandomGetAlg": {"description": "Picks the best counter-based RNG algorithm based on device.", "Args": {"name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int32."}, "tf.raw_ops.StatelessRandomGetKeyCounter": {"description": "Scrambles seed into key and counter, using the best algorithm based on device.", "Args": {"seed": "A Tensor. Must be one of the following types: int32, int64.\n2 seeds (shape [2]).", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (key, counter)."}, "tf.raw_ops.StatelessRandomGetKeyCounterAlg": {"description": "Picks the best algorithm based on device, and scrambles seed into key and counter.", "Args": {"seed": "A Tensor. Must be one of the following types: int32, int64.\n2 seeds (shape [2]).", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (key, counter, alg)."}, "tf.raw_ops.StatelessRandomNormal": {"description": "Outputs deterministic pseudorandom values from a normal distribution.", "Args": {"shape": "A Tensor. Must be one of the following types: int32, int64.\nThe shape of the output tensor.", "seed": "A Tensor. Must be one of the following types: int32, int64.\n2 seeds (shape [2]).", "dtype": "An optional tf.DType from: tf.half, tf.bfloat16, tf.float32, tf.float64. Defaults to tf.float32.\nThe type of the output.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dtype."}, "tf.raw_ops.StatelessRandomNormalV2": {"description": "Outputs deterministic pseudorandom values from a normal distribution.", "Args": {"shape": "A Tensor. Must be one of the following types: int32, int64.\nThe shape of the output tensor.", "key": "A Tensor of type uint64.\nKey for the counter-based RNG algorithm (shape uint64[1]).", "counter": "A Tensor of type uint64.\nInitial counter for the counter-based RNG algorithm (shape uint64[2] or uint64[1] depending on the algorithm). If a larger vector is given, only the needed portion on the left (i.e. [:N]) will be used.", "alg": "A Tensor of type int32. The RNG algorithm (shape int32[]).", "dtype": "An optional tf.DType from: tf.half, tf.bfloat16, tf.float32, tf.float64. Defaults to tf.float32.\nThe type of the output.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dtype."}, "tf.raw_ops.StatelessRandomPoisson": {"description": "Outputs deterministic pseudorandom random numbers from a Poisson distribution.", "Args": {"shape": "A Tensor. Must be one of the following types: int32, int64.\nThe shape of the output tensor.", "seed": "A Tensor. Must be one of the following types: int32, int64.\n2 seeds (shape [2]).", "lam": "A Tensor. Must be one of the following types: half, float32, float64, int32, int64.\nThe rate of the Poisson distribution. Shape must match the rightmost dimensions\nof shape.", "dtype": "A tf.DType from: tf.half, tf.float32, tf.float64, tf.int32, tf.int64.\nThe type of the output.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dtype."}, "tf.raw_ops.StatelessRandomUniform": {"description": "Outputs deterministic pseudorandom random values from a uniform distribution.", "Args": {"shape": "A Tensor. Must be one of the following types: int32, int64.\nThe shape of the output tensor.", "seed": "A Tensor. Must be one of the following types: int32, int64.\n2 seeds (shape [2]).", "dtype": "An optional tf.DType from: tf.half, tf.bfloat16, tf.float32, tf.float64. Defaults to tf.float32.\nThe type of the output.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dtype."}, "tf.raw_ops.StatelessRandomUniformFullInt": {"description": "Outputs deterministic pseudorandom random integers from a uniform distribution.", "Args": {"shape": "A Tensor. Must be one of the following types: int32, int64.\nThe shape of the output tensor.", "seed": "A Tensor. Must be one of the following types: int32, int64, uint32, uint64.\n2 seeds (shape [2]).", "dtype": "An optional tf.DType from: tf.int32, tf.int64, tf.uint32, tf.uint64. Defaults to tf.uint64.\nThe type of the output.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dtype."}, "tf.raw_ops.StatelessRandomUniformFullIntV2": {"description": "Outputs deterministic pseudorandom random integers from a uniform distribution.", "Args": {"shape": "A Tensor. Must be one of the following types: int32, int64.\nThe shape of the output tensor.", "key": "A Tensor of type uint64.\nKey for the counter-based RNG algorithm (shape uint64[1]).", "counter": "A Tensor of type uint64.\nInitial counter for the counter-based RNG algorithm (shape uint64[2] or uint64[1] depending on the algorithm). If a larger vector is given, only the needed portion on the left (i.e. [:N]) will be used.", "alg": "A Tensor of type int32. The RNG algorithm (shape int32[]).", "dtype": "An optional tf.DType from: tf.int32, tf.int64, tf.uint32, tf.uint64. Defaults to tf.uint64.\nThe type of the output.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dtype."}, "tf.raw_ops.StatelessRandomUniformInt": {"description": "Outputs deterministic pseudorandom random integers from a uniform distribution.", "Args": {"shape": "A Tensor. Must be one of the following types: int32, int64.\nThe shape of the output tensor.", "seed": "A Tensor. Must be one of the following types: int32, int64.\n2 seeds (shape [2]).", "minval": "A Tensor. Must be one of the following types: int32, int64.\nMinimum value (inclusive, scalar).", "maxval": "A Tensor. Must have the same type as minval.\nMaximum value (exclusive, scalar).", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as minval."}, "tf.raw_ops.StatelessRandomUniformIntV2": {"description": "Outputs deterministic pseudorandom random integers from a uniform distribution.", "Args": {"shape": "A Tensor. Must be one of the following types: int32, int64.\nThe shape of the output tensor.", "key": "A Tensor of type uint64.\nKey for the counter-based RNG algorithm (shape uint64[1]).", "counter": "A Tensor of type uint64.\nInitial counter for the counter-based RNG algorithm (shape uint64[2] or uint64[1] depending on the algorithm). If a larger vector is given, only the needed portion on the left (i.e. [:N]) will be used.", "alg": "A Tensor of type int32. The RNG algorithm (shape int32[]).", "minval": "A Tensor. Must be one of the following types: int32, int64, uint32, uint64.\nMinimum value (inclusive, scalar).", "maxval": "A Tensor. Must have the same type as minval.\nMaximum value (exclusive, scalar).", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as minval."}, "tf.raw_ops.StatelessRandomUniformV2": {"description": "Outputs deterministic pseudorandom random values from a uniform distribution.", "Args": {"shape": "A Tensor. Must be one of the following types: int32, int64.\nThe shape of the output tensor.", "key": "A Tensor of type uint64.\nKey for the counter-based RNG algorithm (shape uint64[1]).", "counter": "A Tensor of type uint64.\nInitial counter for the counter-based RNG algorithm (shape uint64[2] or uint64[1] depending on the algorithm). If a larger vector is given, only the needed portion on the left (i.e. [:N]) will be used.", "alg": "A Tensor of type int32. The RNG algorithm (shape int32[]).", "dtype": "An optional tf.DType from: tf.half, tf.bfloat16, tf.float32, tf.float64. Defaults to tf.float32.\nThe type of the output.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dtype."}, "tf.raw_ops.StatelessSampleDistortedBoundingBox": {"description": "Generate a randomly distorted bounding box for an image deterministically.", "Args": {"image_size": "A Tensor. Must be one of the following types: uint8, int8, int16, int32, int64.\n1-D, containing [height, width, channels].", "bounding_boxes": "A Tensor of type float32.\n3-D with shape [batch, N, 4] describing the N bounding boxes\nassociated with the image.", "min_object_covered": "A Tensor of type float32.\nThe cropped area of the image must contain at least this\nfraction of any bounding box supplied. The value of this parameter should be\nnon-negative. In the case of 0, the cropped area does not need to overlap\nany of the bounding boxes supplied.", "seed": "A Tensor. Must be one of the following types: int32, int64.\n1-D with shape [2]. The seed to the random number generator. Must have dtype\nint32 or int64. (When using XLA, only int32 is allowed.)", "aspect_ratio_range": "An optional list of floats. Defaults to [0.75, 1.33].\nThe cropped area of the image must have an aspect ratio =\nwidth / height within this range.", "area_range": "An optional list of floats. Defaults to [0.05, 1].\nThe cropped area of the image must contain a fraction of the\nsupplied image within this range.", "max_attempts": "An optional int. Defaults to 100.\nNumber of attempts at generating a cropped region of the image\nof the specified constraints. After max_attempts failures, return the entire\nimage.", "use_image_if_no_bounding_boxes": "An optional bool. Defaults to False.\nControls behavior if no bounding boxes supplied.\nIf true, assume an implicit bounding box covering the whole input. If false,\nraise an error.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (begin, size, bboxes)."}, "tf.raw_ops.StatelessShuffle": {"description": "Randomly and deterministically shuffles a tensor along its first dimension.", "Args": {"value": "A Tensor. The tensor to be shuffled.", "key": "A Tensor of type uint64.\nKey for the counter-based RNG algorithm (shape uint64[1]).", "counter": "A Tensor of type uint64.\nInitial counter for the counter-based RNG algorithm (shape uint64[2] or uint64[1] depending on the algorithm). If a larger vector is given, only the needed portion on the left (i.e. [:N]) will be used.", "alg": "A Tensor of type int32. The RNG algorithm (shape int32[]).", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as value."}, "tf.raw_ops.StatelessTruncatedNormal": {"description": "Outputs deterministic pseudorandom values from a truncated normal distribution.", "Args": {"shape": "A Tensor. Must be one of the following types: int32, int64.\nThe shape of the output tensor.", "seed": "A Tensor. Must be one of the following types: int32, int64.\n2 seeds (shape [2]).", "dtype": "An optional tf.DType from: tf.half, tf.bfloat16, tf.float32, tf.float64. Defaults to tf.float32.\nThe type of the output.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dtype."}, "tf.raw_ops.StatelessTruncatedNormalV2": {"description": "Outputs deterministic pseudorandom values from a truncated normal distribution.", "Args": {"shape": "A Tensor. Must be one of the following types: int32, int64.\nThe shape of the output tensor.", "key": "A Tensor of type uint64.\nKey for the counter-based RNG algorithm (shape uint64[1]).", "counter": "A Tensor of type uint64.\nInitial counter for the counter-based RNG algorithm (shape uint64[2] or uint64[1] depending on the algorithm). If a larger vector is given, only the needed portion on the left (i.e. [:N]) will be used.", "alg": "A Tensor of type int32. The RNG algorithm (shape int32[]).", "dtype": "An optional tf.DType from: tf.half, tf.bfloat16, tf.float32, tf.float64. Defaults to tf.float32.\nThe type of the output.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dtype."}, "tf.raw_ops.StatelessWhile": {"description": "output = input; While (Cond(output)) { output = Body(output) }", "Args": {"input": "A list of Tensor objects.\nA list of input tensors whose types are T.", "cond": "A function decorated with @Defun.\nA function takes 'input' and returns a tensor.  If the tensor is\na scalar of non-boolean, the scalar is converted to a boolean\naccording to the following rule: if the scalar is a numerical\nvalue, non-zero means True and zero means False; if the scalar is\na string, non-empty means True and empty means False. If the\ntensor is not a scalar, non-emptiness means True and False\notherwise.\nThis should only be used when the while condition and body functions\ndo not have stateful ops.", "body": "A function decorated with @Defun.\nA function that takes a list of tensors and returns another\nlist of tensors. Both lists have the same types as specified\nby T.", "output_shapes": "An optional list of shapes (each a tf.TensorShape or list of ints). Defaults to [].", "parallel_iterations": "An optional int. Defaults to 10.", "name": "A name for the operation (optional)."}, "Returns": "A list of Tensor objects. Has the same type as input."}, "tf.raw_ops.StaticRegexFullMatch": {"description": "Check if the input matches the regex pattern.", "Args": {"input": "A Tensor of type string.\nA string tensor of the text to be processed.", "pattern": "A string. The regular expression to match the input.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type bool."}, "tf.raw_ops.StaticRegexReplace": {"description": "Replaces the match of pattern in input with rewrite.", "Args": {"input": "A Tensor of type string. The text to be processed.", "pattern": "A string. The regular expression to match the input.", "rewrite": "A string. The rewrite to be applied to the matched expression.", "replace_global": "An optional bool. Defaults to True.\nIf True, the replacement is global, otherwise the replacement\nis done only on the first match.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.StatsAggregatorHandle": {"description": "Creates a statistics manager resource.", "Args": {"container": "An optional string. Defaults to \"\".", "shared_name": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type resource."}, "tf.raw_ops.StatsAggregatorHandleV2": {"Args": {"container": "An optional string. Defaults to \"\".", "shared_name": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type resource."}, "tf.raw_ops.StatsAggregatorSetSummaryWriter": {"description": "Set a summary_writer_interface to record statistics using given stats_aggregator.", "Args": {"stats_aggregator": "A Tensor of type resource.", "summary": "A Tensor of type resource.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.StatsAggregatorSummary": {"description": "Produces a summary of any statistics recorded by the given statistics manager.", "Args": {"iterator": "A Tensor of type resource.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.StopGradient": {"description": "Stops gradient computation.", "Args": {"input": "A Tensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.StridedSlice": {"description": "Return a strided slice from input.", "Args": {"input": "A Tensor.", "begin": "A Tensor. Must be one of the following types: int16, int32, int64.\nbegin[k] specifies the offset into the kth range specification.\nThe exact dimension this corresponds to will be determined by context.\nOut-of-bounds values will be silently clamped. If the kth bit of\nbegin_mask then begin[k] is ignored and the full range of the\nappropriate dimension is used instead. Negative values causes indexing\nto start from the highest element e.g. If foo==[1,2,3] then foo[-1]==3.", "end": "A Tensor. Must have the same type as begin.\nend[i] is like begin with the exception that end_mask is\nused to determine full ranges.", "strides": "A Tensor. Must have the same type as begin.\nstrides[i] specifies the increment in the ith specification\nafter extracting a given element. Negative indices will reverse\nthe original order. Out or range values are\nclamped to [0,dim[i]) if slice[i]>0 or [-1,dim[i]-1] if slice[i] < 0", "begin_mask": "An optional int. Defaults to 0.\na bitmask where a bit i being 1 means to ignore the begin\nvalue and instead use the largest interval possible. At runtime\nbegin[i] will be replaced with [0, n-1) if stride[i] > 0 or\n[-1, n-1] if stride[i] < 0", "end_mask": "An optional int. Defaults to 0. analogous to begin_mask", "ellipsis_mask": "An optional int. Defaults to 0.\na bitmask where bit i being 1 means the ith\nposition is actually an ellipsis. One bit at most can be 1.\nIf ellipsis_mask == 0, then an implicit ellipsis mask of 1 << (m+1)\nis provided. This means that foo[3:5] == foo[3:5, ...]. An ellipsis\nimplicitly creates as many range specifications as necessary to fully\nspecify the sliced range for every dimension. For example for a 4-dimensional\ntensor foo the slice foo[2, ..., 5:8] implies foo[2, :, :, 5:8].", "new_axis_mask": "An optional int. Defaults to 0.\na bitmask where bit i being 1 means the ith\nspecification creates a new shape 1 dimension. For example\nfoo[:4, tf.newaxis, :2] would produce a shape (4, 1, 2) tensor.", "shrink_axis_mask": "An optional int. Defaults to 0.\na bitmask where bit i implies that the ith\nspecification should shrink the dimensionality. begin and end\nmust imply a slice of size 1 in the dimension. For example in\npython one might do foo[:, 3, :] which would result in\nshrink_axis_mask being 2.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.StridedSliceAssign": {"description": "Assign value to the sliced l-value reference of ref.", "Args": {"ref": "A mutable Tensor.", "begin": "A Tensor. Must be one of the following types: int32, int64.", "end": "A Tensor. Must have the same type as begin.", "strides": "A Tensor. Must have the same type as begin.", "value": "A Tensor. Must have the same type as ref.", "begin_mask": "An optional int. Defaults to 0.", "end_mask": "An optional int. Defaults to 0.", "ellipsis_mask": "An optional int. Defaults to 0.", "new_axis_mask": "An optional int. Defaults to 0.", "shrink_axis_mask": "An optional int. Defaults to 0.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor. Has the same type as ref."}, "tf.raw_ops.StridedSliceGrad": {"description": "Returns the gradient of StridedSlice.", "Args": {"shape": "A Tensor. Must be one of the following types: int32, int64.", "begin": "A Tensor. Must have the same type as shape.", "end": "A Tensor. Must have the same type as shape.", "strides": "A Tensor. Must have the same type as shape.", "dy": "A Tensor.", "begin_mask": "An optional int. Defaults to 0.", "end_mask": "An optional int. Defaults to 0.", "ellipsis_mask": "An optional int. Defaults to 0.", "new_axis_mask": "An optional int. Defaults to 0.", "shrink_axis_mask": "An optional int. Defaults to 0.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as dy."}, "tf.raw_ops.StringFormat": {"description": "Formats a string template using a list of tensors.", "Args": {"inputs": "A list of Tensor objects.\nThe list of tensors to format into the placeholder string.", "template": "An optional string. Defaults to \"%s\".\nA string, the template to format tensor summaries into.", "placeholder": "An optional string. Defaults to \"%s\".\nA string, at each placeholder in the template a subsequent tensor summary will be inserted.", "summarize": "An optional int. Defaults to 3.\nWhen formatting the tensor summaries print the first and last summarize entries of each tensor dimension.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.StringJoin": {"description": "Joins the strings in the given list of string tensors into one tensor;", "Args": {"inputs": "A list of at least 1 Tensor objects with type string.\nA list of string tensors.  The tensors must all have the same shape,\nor be scalars.  Scalars may be mixed in; these will be broadcast to the shape\nof non-scalar inputs.", "separator": "An optional string. Defaults to \"\".\nstring, an optional join separator.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.StringLength": {"description": "String lengths of input.", "Args": {"input": "A Tensor of type string.\nThe strings for which to compute the length for each element.", "unit": "An optional string from: \"BYTE\", \"UTF8_CHAR\". Defaults to \"BYTE\".\nThe unit that is counted to compute string length.  One of: \"BYTE\" (for\nthe number of bytes in each string) or \"UTF8_CHAR\" (for the number of UTF-8\nencoded Unicode code points in each string).  Results are undefined\nif unit=UTF8_CHAR and the input strings do not contain structurally\nvalid UTF-8.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int32."}, "tf.raw_ops.StringLower": {"description": "Converts all uppercase characters into their respective lowercase replacements.", "Args": {"input": "A Tensor of type string. The input to be lower-cased.", "encoding": "An optional string. Defaults to \"\".\nCharacter encoding of input. Allowed values are '' and 'utf-8'.\nValue '' is interpreted as ASCII.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.StringNGrams": {"description": "Creates ngrams from ragged string data.", "Args": {"data": "A Tensor of type string.\nThe values tensor of the ragged string tensor to make ngrams out of. Must be a\n1D string tensor.", "data_splits": "A Tensor. Must be one of the following types: int32, int64.\nThe splits tensor of the ragged string tensor to make ngrams out of.", "separator": "A string.\nThe string to append between elements of the token. Use \"\" for no separator.", "ngram_widths": "A list of ints. The sizes of the ngrams to create.", "left_pad": "A string.\nThe string to use to pad the left side of the ngram sequence. Only used if\npad_width != 0.", "right_pad": "A string.\nThe string to use to pad the right side of the ngram sequence. Only used if\npad_width != 0.", "pad_width": "An int.\nThe number of padding elements to add to each side of each\nsequence. Note that padding will never be greater than 'ngram_widths'-1\nregardless of this value. If pad_width=-1, then add max(ngram_widths)-1\nelements.", "preserve_short_sequences": "A bool.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (ngrams, ngrams_splits)."}, "tf.raw_ops.StringSplit": {"description": "Split elements of input based on delimiter into a SparseTensor.", "Args": {"input": "A Tensor of type string. 1-D. Strings to split.", "delimiter": "A Tensor of type string.\n0-D. Delimiter characters (bytes), or empty string.", "skip_empty": "An optional bool. Defaults to True.\nA bool. If True, skip the empty strings from the result.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (indices, values, shape)."}, "tf.raw_ops.StringSplitV2": {"description": "Split elements of source based on sep into a SparseTensor.", "Args": {"input": "A Tensor of type string.\n1-D string Tensor, the strings to split.", "sep": "A Tensor of type string.\n0-D string Tensor, the delimiter character.", "maxsplit": "An optional int. Defaults to -1.\nAn int. If maxsplit > 0, limit of the split of the result.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (indices, values, shape)."}, "tf.raw_ops.StringStrip": {"description": "Strip leading and trailing whitespaces from the Tensor.", "Args": {"input": "A Tensor of type string. A string Tensor of any shape.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.StringToHashBucket": {"description": "Converts each string in the input Tensor to its hash mod by a number of buckets.", "Args": {"string_tensor": "A Tensor of type string.", "num_buckets": "An int that is >= 1. The number of buckets.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int64."}, "tf.raw_ops.StringToHashBucketFast": {"description": "Converts each string in the input Tensor to its hash mod by a number of buckets.", "Args": {"input": "A Tensor of type string. The strings to assign a hash bucket.", "num_buckets": "An int that is >= 1. The number of buckets.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int64."}, "tf.raw_ops.StringToHashBucketStrong": {"description": "Converts each string in the input Tensor to its hash mod by a number of buckets.", "Args": {"input": "A Tensor of type string. The strings to assign a hash bucket.", "num_buckets": "An int that is >= 1. The number of buckets.", "key": "A list of ints.\nThe key used to seed the hash function, passed as a list of two uint64\nelements.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int64."}, "tf.raw_ops.StringToNumber": {"description": "Converts each string in the input Tensor to the specified numeric type.", "Args": {"string_tensor": "A Tensor of type string.", "out_type": "An optional tf.DType from: tf.float32, tf.float64, tf.int32, tf.int64. Defaults to tf.float32.\nThe numeric type to interpret each string in string_tensor as.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type out_type."}, "tf.raw_ops.StringUpper": {"description": "Converts all lowercase characters into their respective uppercase replacements.", "Args": {"input": "A Tensor of type string. The input to be upper-cased.", "encoding": "An optional string. Defaults to \"\".\nCharacter encoding of input. Allowed values are '' and 'utf-8'.\nValue '' is interpreted as ASCII.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.Sub": {"description": "Returns x - y element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, uint8, int8, uint16, int16, int32, int64, complex64, complex128, uint32, uint64.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.Substr": {"description": "Return substrings from Tensor of strings.", "Raises": {}, "Args": {"input": "A Tensor of type string. Tensor of strings", "pos": "A Tensor. Must be one of the following types: int32, int64.\nScalar defining the position of first character in each substring", "len": "A Tensor. Must have the same type as pos.\nScalar defining the number of characters to include in each substring", "unit": "An optional string from: \"BYTE\", \"UTF8_CHAR\". Defaults to \"BYTE\".\nThe unit that is used to create the substring.  One of: \"BYTE\" (for\ndefining position and length by bytes) or \"UTF8_CHAR\" (for the UTF-8\nencoded Unicode code points).  The default is \"BYTE\". Results are undefined if\nunit=UTF8_CHAR and the input strings do not contain structurally valid\nUTF-8.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.Sum": {"description": "Computes the sum of elements across dimensions of a tensor.", "Args": {"input": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.\nThe tensor to reduce.", "axis": "A Tensor. Must be one of the following types: int32, int64.\nThe dimensions to reduce. Must be in the range\n[-rank(input), rank(input)).", "keep_dims": "An optional bool. Defaults to False.\nIf true, retain reduced dimensions with length 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.SummaryWriter": {"Args": {"shared_name": "An optional string. Defaults to \"\".", "container": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type resource."}, "tf.raw_ops.Svd": {"description": "Computes the singular value decompositions of one or more matrices.", "Args": {"input": "A Tensor. Must be one of the following types: float64, float32, half, complex64, complex128.\nA tensor of shape [..., M, N] whose inner-most 2 dimensions\nform matrices of size [M, N]. Let P be the minimum of M and N.", "compute_uv": "An optional bool. Defaults to True.\nIf true, left and right singular vectors will be\ncomputed and returned in u and v, respectively.\nIf false, u and v are not set and should never referenced.", "full_matrices": "An optional bool. Defaults to False.\nIf true, compute full-sized u and v. If false\n(the default), compute only the leading P singular vectors.\nIgnored if compute_uv is False.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (s, u, v)."}, "tf.raw_ops.Switch": {"description": "Forwards data to the output port determined by pred.", "Args": {"data": "A Tensor. The tensor to be forwarded to the appropriate output.", "pred": "A Tensor of type bool.\nA scalar that specifies which output port will receive data.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output_false, output_true)."}, "tf.raw_ops.SymbolicGradient": {"description": "Computes the gradient function for function f via backpropagation.", "Args": {"input": "A list of Tensor objects. a list of input tensors of size N + M;", "Tout": "A list of tf.DTypes that has length >= 1.\nthe type list for the input list.", "f": "A function decorated with @Defun.\nThe function we want to compute the gradient for.\nThe function 'f' must be a numerical function which takes N inputs and\nproduces M outputs. Its gradient function 'g', which is computed by\nthis SymbolicGradient op is a function taking N + M inputs and\nproduces N outputs.\nI.e. if we have\n   (y1, y2, ..., y_M) = f(x1, x2, ..., x_N),\nthen, g is\n   (dL/dx1, dL/dx2, ..., dL/dx_N) = g(x1, x2, ..., x_N,\n                                     dL/dy1, dL/dy2, ..., dL/dy_M),\nwhere L is a scalar-value function of (x1, x2, ..., xN) (e.g., the\nloss function). dL/dx_i is the partial derivative of L with respect\nto x_i.\n(Needs some math expert to say the comment above better.)", "name": "A name for the operation (optional)."}, "Returns": "A list of Tensor objects of type Tout."}, "tf.raw_ops.TFRecordDataset": {"description": "Creates a dataset that emits the records from one or more TFRecord files.", "Args": {"filenames": "A Tensor of type string.\nA scalar or vector containing the name(s) of the file(s) to be\nread.", "compression_type": "A Tensor of type string.\nA scalar containing either (i) the empty string (no\ncompression), (ii) \"ZLIB\", or (iii) \"GZIP\".", "buffer_size": "A Tensor of type int64.\nA scalar representing the number of bytes to buffer. A value of\n0 means no buffering will be performed.", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.TFRecordReader": {"description": "A Reader that outputs the records from a TensorFlow Records file.", "Args": {"container": "An optional string. Defaults to \"\".\nIf non-empty, this reader is placed in the given container.\nOtherwise, a default container is used.", "shared_name": "An optional string. Defaults to \"\".\nIf non-empty, this reader is named in the given bucket\nwith this shared_name. Otherwise, the node name is used instead.", "compression_type": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type mutable string."}, "tf.raw_ops.TFRecordReaderV2": {"description": "A Reader that outputs the records from a TensorFlow Records file.", "Args": {"container": "An optional string. Defaults to \"\".\nIf non-empty, this reader is placed in the given container.\nOtherwise, a default container is used.", "shared_name": "An optional string. Defaults to \"\".\nIf non-empty, this reader is named in the given bucket\nwith this shared_name. Otherwise, the node name is used instead.", "compression_type": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type resource."}, "tf.raw_ops.TPUCompilationResult": {"description": "Returns the result of a TPU compilation.", "Args": {"name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.TPUEmbeddingActivations": {"description": "An op enabling differentiation of TPU Embeddings.", "Args": {"embedding_variable": "A Tensor of type float32.\nA trainable variable, enabling optimizers to find this op.", "sliced_activations": "A Tensor of type float32.\nThe embedding activations Tensor to return.", "table_id": "An int that is >= 0.\nThe id of the table in the embedding layer configuration from which\nthese activations were computed.", "lookup_id": "An int that is >= 0.\nIdentifier of the set of embedding indices which produced these\nactivations.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type float32."}, "tf.raw_ops.TPUOrdinalSelector": {"description": "A TPU core selector Op.", "Args": {"name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int32."}, "tf.raw_ops.TPUPartitionedCall": {"description": "Calls a function placed on a specified TPU device.", "Args": {"args": "A list of Tensor objects. The arguments to the function.", "device_ordinal": "A Tensor of type int32.\nThe TPU device ordinal to run the function on.", "Tout": "A list of tf.DTypes. The types of the outputs of the function.", "f": "A function decorated with @Defun. The function to call.", "autotuner_thresh": "An optional int. Defaults to 0.", "name": "A name for the operation (optional)."}, "Returns": "A list of Tensor objects of type Tout."}, "tf.raw_ops.TPUPartitionedInput": {"description": "An op that groups a list of partitioned inputs together. This op", "Args": {"inputs": "A list of at least 1 Tensor objects with the same type.\nA list of partitioned inputs which must have the same shape.", "partition_dim": "An optional int. Defaults to 0.\nAn integer describles which dimension is partitioned. -1 means\nthose inputs are replicated.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as inputs."}, "tf.raw_ops.TPUPartitionedOutput": {"description": "An op that demultiplexes a tensor to be sharded by XLA to a list of partitioned", "Args": {"inputs": "A Tensor.\nA tensor which represents the full shape of partitioned tensors.", "num_splits": "An int that is >= 1.", "partition_dim": "An optional int. Defaults to 0.\nAn integer describles which dimension is partitioned.", "name": "A name for the operation (optional)."}, "Returns": "A list of num_splits Tensor objects with the same type as inputs."}, "tf.raw_ops.TPUReplicateMetadata": {"description": "Metadata indicating how the TPU computation should be replicated.", "Args": {"num_replicas": "An int that is >= 0.\nNumber of replicas of the computation", "num_cores_per_replica": "An optional int. Defaults to 1.\nNumber of cores per replica. Used for model parallelism.", "topology": "An optional string. Defaults to \"\".\nTopologyProto indicating the topology of the TPU pod slice.", "use_tpu": "An optional bool. Defaults to True.\nWhether to place the computation on the TPU.", "device_assignment": "An optional list of ints. Defaults to [].\nThe assignment of devices for the computation.", "computation_shape": "An optional list of ints. Defaults to [].\nDEPRECATED. Use num_cores_per_replica instead.", "host_compute_core": "An optional list of strings. Defaults to [].", "padding_map": "An optional list of strings. Defaults to [].", "step_marker_location": "An optional string. Defaults to \"STEP_MARK_AT_ENTRY\".", "allow_soft_placement": "An optional bool. Defaults to False.", "use_spmd_for_xla_partitioning": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.TPUReplicatedInput": {"description": "Connects N inputs to an N-way replicated TPU computation.", "Args": {"inputs": "A list of at least 1 Tensor objects with the same type.", "is_mirrored_variable": "An optional bool. Defaults to False.", "index": "An optional int. Defaults to -1.", "is_packed": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as inputs."}, "tf.raw_ops.TPUReplicatedOutput": {"description": "Connects N outputs from an N-way replicated TPU computation.", "Args": {"input": "A Tensor.", "num_replicas": "An int that is >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A list of num_replicas Tensor objects with the same type as input."}, "tf.raw_ops.TakeDataset": {"description": "Creates a dataset that contains count elements from the input_dataset.", "Args": {"input_dataset": "A Tensor of type variant.", "count": "A Tensor of type int64.\nA scalar representing the number of elements from the input_dataset\nthat should be taken. A value of -1 indicates that all of input_dataset\nis taken.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.TakeManySparseFromTensorsMap": {"description": "Read SparseTensors from a SparseTensorsMap and concatenate them.", "Args": {"sparse_handles": "A Tensor of type int64.\n1-D, The N serialized SparseTensor objects.\nShape: [N].", "dtype": "A tf.DType.\nThe dtype of the SparseTensor objects stored in the\nSparseTensorsMap.", "container": "An optional string. Defaults to \"\".\nThe container name for the SparseTensorsMap read by this op.", "shared_name": "An optional string. Defaults to \"\".\nThe shared name for the SparseTensorsMap read by this op.\nIt should not be blank; rather the shared_name or unique Operation name\nof the Op that created the original SparseTensorsMap should be used.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (sparse_indices, sparse_values, sparse_shape)."}, "tf.raw_ops.TakeWhileDataset": {"description": "Creates a dataset that stops iteration when predicate is false.", "Args": {"input_dataset": "A Tensor of type variant.", "other_arguments": "A list of Tensor objects.\nA list of tensors, typically values that were captured when\nbuilding a closure for predicate.", "predicate": "A function decorated with @Defun.\nA function returning a scalar boolean.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.Tan": {"description": "Computes tan of x element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, int8, int16, int32, int64, complex64, complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.Tanh": {"description": "Computes hyperbolic tangent of x element-wise.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, complex64, complex128.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.TanhGrad": {"description": "Computes the gradient for the tanh of x wrt its input.", "Args": {"y": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, complex64, complex128.", "dy": "A Tensor. Must have the same type as y.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as y."}, "tf.raw_ops.TemporaryVariable": {"description": "Returns a tensor that may be mutated, but only persists within a single step.", "Args": {"shape": "A tf.TensorShape or list of ints.\nThe shape of the variable tensor.", "dtype": "A tf.DType. The type of elements in the variable tensor.", "var_name": "An optional string. Defaults to \"\".\nOverrides the name used for the temporary variable resource. Default\nvalue is the name of the 'TemporaryVariable' op (which is guaranteed unique).", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor of type dtype."}, "tf.raw_ops.TensorArray": {"Args": {"size": "A Tensor of type int32.", "dtype": "A tf.DType.", "dynamic_size": "An optional bool. Defaults to False.", "clear_after_read": "An optional bool. Defaults to True.", "tensor_array_name": "An optional string. Defaults to \"\".", "element_shape": "An optional tf.TensorShape or list of ints. Defaults to None.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type mutable string."}, "tf.raw_ops.TensorArrayClose": {"Args": {"handle": "A Tensor of type mutable string.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.TensorArrayCloseV2": {"description": "Deprecated. Use TensorArrayCloseV3", "Args": {"handle": "A Tensor of type string.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.TensorArrayCloseV3": {"description": "Delete the TensorArray from its resource container.", "Args": {"handle": "A Tensor of type resource.\nThe handle to a TensorArray (output of TensorArray or TensorArrayGrad).", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.TensorArrayConcat": {"Args": {"handle": "A Tensor of type mutable string.", "flow_in": "A Tensor of type float32.", "dtype": "A tf.DType.", "element_shape_except0": "An optional tf.TensorShape or list of ints. Defaults to None.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (value, lengths)."}, "tf.raw_ops.TensorArrayConcatV2": {"description": "Deprecated. Use TensorArrayConcatV3", "Args": {"handle": "A Tensor of type string.", "flow_in": "A Tensor of type float32.", "dtype": "A tf.DType.", "element_shape_except0": "An optional tf.TensorShape or list of ints. Defaults to None.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (value, lengths)."}, "tf.raw_ops.TensorArrayConcatV3": {"description": "Concat the elements from the TensorArray into value value.", "Args": {"handle": "A Tensor of type resource. The handle to a TensorArray.", "flow_in": "A Tensor of type float32.\n  A float scalar that enforces proper chaining of operations.", "dtype": "A tf.DType. The type of the elem that is returned.", "element_shape_except0": "An optional tf.TensorShape or list of ints. Defaults to None.\n  The expected shape of an element, if known,\n  excluding the first dimension. Used to validate the shapes of\n  TensorArray elements. If this shape is not fully specified, concatenating\n  zero-size TensorArrays is an error.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (value, lengths)."}, "tf.raw_ops.TensorArrayGather": {"Args": {"handle": "A Tensor of type mutable string.", "indices": "A Tensor of type int32.", "flow_in": "A Tensor of type float32.", "dtype": "A tf.DType.", "element_shape": "An optional tf.TensorShape or list of ints. Defaults to None.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dtype."}, "tf.raw_ops.TensorArrayGatherV2": {"description": "Deprecated. Use TensorArrayGatherV3", "Args": {"handle": "A Tensor of type string.", "indices": "A Tensor of type int32.", "flow_in": "A Tensor of type float32.", "dtype": "A tf.DType.", "element_shape": "An optional tf.TensorShape or list of ints. Defaults to None.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dtype."}, "tf.raw_ops.TensorArrayGatherV3": {"description": "Gather specific elements from the TensorArray into output value.", "Args": {"handle": "A Tensor of type resource. The handle to a TensorArray.", "indices": "A Tensor of type int32.\nThe locations in the TensorArray from which to read tensor elements.", "flow_in": "A Tensor of type float32.\nA float scalar that enforces proper chaining of operations.", "dtype": "A tf.DType. The type of the elem that is returned.", "element_shape": "An optional tf.TensorShape or list of ints. Defaults to None.\nThe expected shape of an element, if known. Used to\nvalidate the shapes of TensorArray elements. If this shape is not\nfully specified, gathering zero-size TensorArrays is an error.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dtype."}, "tf.raw_ops.TensorArrayGrad": {"Args": {"handle": "A Tensor of type string.", "flow_in": "A Tensor of type float32.", "source": "A string.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type mutable string."}, "tf.raw_ops.TensorArrayGradV2": {"description": "Deprecated. Use TensorArrayGradV3", "Args": {"handle": "A Tensor of type string.", "flow_in": "A Tensor of type float32.", "source": "A string.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.TensorArrayGradV3": {"description": "Creates a TensorArray for storing the gradients of values in the given handle.", "Args": {"handle": "A Tensor of type resource.\nThe handle to the forward TensorArray.", "flow_in": "A Tensor of type float32.\nA float scalar that enforces proper chaining of operations.", "source": "A string.\nThe gradient source string, used to decide which gradient TensorArray\nto return.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (grad_handle, flow_out)."}, "tf.raw_ops.TensorArrayGradWithShape": {"description": "Creates a TensorArray for storing multiple gradients of values in the given handle.", "Args": {"handle": "A Tensor of type resource.\nThe handle to the forward TensorArray.", "flow_in": "A Tensor of type float32.\nA float scalar that enforces proper chaining of operations.", "shape_to_prepend": "A Tensor of type int32.\nAn int32 vector representing a shape. Elements in the gradient accumulator will\nhave shape which is this shape_to_prepend value concatenated with shape of the\nelements in the TensorArray corresponding to the input handle.", "source": "A string.\nThe gradient source string, used to decide which gradient TensorArray\nto return.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (grad_handle, flow_out)."}, "tf.raw_ops.TensorArrayPack": {"Args": {"handle": "A Tensor of type mutable string.", "flow_in": "A Tensor of type float32.", "dtype": "A tf.DType.", "element_shape": "An optional tf.TensorShape or list of ints. Defaults to None.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dtype."}, "tf.raw_ops.TensorArrayRead": {"Args": {"handle": "A Tensor of type mutable string.", "index": "A Tensor of type int32.", "flow_in": "A Tensor of type float32.", "dtype": "A tf.DType.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dtype."}, "tf.raw_ops.TensorArrayReadV2": {"description": "Deprecated. Use TensorArrayReadV3", "Args": {"handle": "A Tensor of type string.", "index": "A Tensor of type int32.", "flow_in": "A Tensor of type float32.", "dtype": "A tf.DType.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dtype."}, "tf.raw_ops.TensorArrayReadV3": {"description": "Read an element from the TensorArray into output value.", "Args": {"handle": "A Tensor of type resource. The handle to a TensorArray.", "index": "A Tensor of type int32.", "flow_in": "A Tensor of type float32.\nA float scalar that enforces proper chaining of operations.", "dtype": "A tf.DType. The type of the elem that is returned.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dtype."}, "tf.raw_ops.TensorArrayScatter": {"Args": {"handle": "A Tensor of type mutable string.", "indices": "A Tensor of type int32.", "value": "A Tensor.", "flow_in": "A Tensor of type float32.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type float32."}, "tf.raw_ops.TensorArrayScatterV2": {"description": "Deprecated. Use TensorArrayScatterV3", "Args": {"handle": "A Tensor of type string.", "indices": "A Tensor of type int32.", "value": "A Tensor.", "flow_in": "A Tensor of type float32.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type float32."}, "tf.raw_ops.TensorArrayScatterV3": {"description": "Scatter the data from the input value into specific TensorArray elements.", "Args": {"handle": "A Tensor of type resource. The handle to a TensorArray.", "indices": "A Tensor of type int32.\nThe locations at which to write the tensor elements.", "value": "A Tensor. The concatenated tensor to write to the TensorArray.", "flow_in": "A Tensor of type float32.\nA float scalar that enforces proper chaining of operations.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type float32."}, "tf.raw_ops.TensorArraySize": {"Args": {"handle": "A Tensor of type mutable string.", "flow_in": "A Tensor of type float32.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int32."}, "tf.raw_ops.TensorArraySizeV2": {"description": "Deprecated. Use TensorArraySizeV3", "Args": {"handle": "A Tensor of type string.", "flow_in": "A Tensor of type float32.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int32."}, "tf.raw_ops.TensorArraySizeV3": {"description": "Get the current size of the TensorArray.", "Args": {"handle": "A Tensor of type resource.\nThe handle to a TensorArray (output of TensorArray or TensorArrayGrad).", "flow_in": "A Tensor of type float32.\nA float scalar that enforces proper chaining of operations.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int32."}, "tf.raw_ops.TensorArraySplit": {"Args": {"handle": "A Tensor of type mutable string.", "value": "A Tensor.", "lengths": "A Tensor of type int64.", "flow_in": "A Tensor of type float32.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type float32."}, "tf.raw_ops.TensorArraySplitV2": {"description": "Deprecated. Use TensorArraySplitV3", "Args": {"handle": "A Tensor of type string.", "value": "A Tensor.", "lengths": "A Tensor of type int64.", "flow_in": "A Tensor of type float32.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type float32."}, "tf.raw_ops.TensorArraySplitV3": {"description": "Split the data from the input value into TensorArray elements.", "Args": {"handle": "A Tensor of type resource. The handle to a TensorArray.", "value": "A Tensor. The concatenated tensor to write to the TensorArray.", "lengths": "A Tensor of type int64.\n  The vector of lengths, how to split the rows of value into the\n  TensorArray.", "flow_in": "A Tensor of type float32.\n  A float scalar that enforces proper chaining of operations.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type float32."}, "tf.raw_ops.TensorArrayUnpack": {"Args": {"handle": "A Tensor of type mutable string.", "value": "A Tensor.", "flow_in": "A Tensor of type float32.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type float32."}, "tf.raw_ops.TensorArrayV2": {"description": "Deprecated. Use TensorArrayV3", "Args": {"size": "A Tensor of type int32.", "dtype": "A tf.DType.", "element_shape": "An optional tf.TensorShape or list of ints. Defaults to None.", "dynamic_size": "An optional bool. Defaults to False.", "clear_after_read": "An optional bool. Defaults to True.", "tensor_array_name": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.TensorArrayV3": {"description": "An array of Tensors of given size.", "Args": {"size": "A Tensor of type int32. The size of the array.", "dtype": "A tf.DType. The type of the elements on the tensor_array.", "element_shape": "An optional tf.TensorShape or list of ints. Defaults to None.\nThe expected shape of an element, if known. Used to\nvalidate the shapes of TensorArray elements. If this shape is not\nfully specified, gathering zero-size TensorArrays is an error.", "dynamic_size": "An optional bool. Defaults to False.\nA boolean that determines whether writes to the TensorArray\nare allowed to grow the size.  By default, this is not allowed.", "clear_after_read": "An optional bool. Defaults to True.\nIf true (default), Tensors in the TensorArray are cleared\nafter being read.  This disables multiple read semantics but allows early\nrelease of memory.", "identical_element_shapes": "An optional bool. Defaults to False.\nIf true (default is false), then all\nelements in the TensorArray will be expected to have identical shapes.\nThis allows certain behaviors, like dynamically checking for\nconsistent shapes on write, and being able to fill in properly\nshaped zero tensors on stack -- even if the element_shape attribute\nis not fully defined.", "tensor_array_name": "An optional string. Defaults to \"\".\nOverrides the name used for the temporary tensor_array\nresource. Default value is the name of the 'TensorArray' op (which\nis guaranteed unique).", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (handle, flow)."}, "tf.raw_ops.TensorArrayWrite": {"Args": {"handle": "A Tensor of type mutable string.", "index": "A Tensor of type int32.", "value": "A Tensor.", "flow_in": "A Tensor of type float32.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type float32."}, "tf.raw_ops.TensorArrayWriteV2": {"description": "Deprecated. Use TensorArrayGradV3", "Args": {"handle": "A Tensor of type string.", "index": "A Tensor of type int32.", "value": "A Tensor.", "flow_in": "A Tensor of type float32.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type float32."}, "tf.raw_ops.TensorArrayWriteV3": {"description": "Push an element onto the tensor_array.", "Args": {"handle": "A Tensor of type resource. The handle to a TensorArray.", "index": "A Tensor of type int32.\nThe position to write to inside the TensorArray.", "value": "A Tensor. The tensor to write to the TensorArray.", "flow_in": "A Tensor of type float32.\nA float scalar that enforces proper chaining of operations.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type float32."}, "tf.raw_ops.TensorDataset": {"description": "Creates a dataset that emits components as a tuple of tensors once.", "Args": {"components": "A list of Tensor objects.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.TensorListConcat": {"description": "Concats all tensors in the list along the 0th dimension.", "Args": {"input_handle": "A Tensor of type variant.", "element_dtype": "A tf.DType.", "element_shape": "An optional tf.TensorShape or list of ints. Defaults to None.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (tensor, lengths)."}, "tf.raw_ops.TensorListConcatLists": {"Args": {"input_a": "A Tensor of type variant.", "input_b": "A Tensor of type variant.", "element_dtype": "A tf.DType.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.TensorListConcatV2": {"description": "Concats all tensors in the list along the 0th dimension.", "Args": {"input_handle": "A Tensor of type variant.", "element_shape": "A Tensor. Must be one of the following types: int32, int64.", "leading_dims": "A Tensor of type int64.", "element_dtype": "A tf.DType.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (tensor, lengths)."}, "tf.raw_ops.TensorListElementShape": {"description": "The shape of the elements of the given list, as a tensor.", "Args": {"input_handle": "A Tensor of type variant.", "shape_type": "A tf.DType from: tf.int32, tf.int64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type shape_type."}, "tf.raw_ops.TensorListFromTensor": {"description": "Creates a TensorList which, when stacked, has the value of tensor.", "Args": {"tensor": "A Tensor.", "element_shape": "A Tensor. Must be one of the following types: int32, int64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.TensorListGather": {"description": "Creates a Tensor by indexing into the TensorList.", "Args": {"input_handle": "A Tensor of type variant.", "indices": "A Tensor of type int32.", "element_shape": "A Tensor of type int32.", "element_dtype": "A tf.DType.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type element_dtype."}, "tf.raw_ops.TensorListGetItem": {"description": "Returns the item in the list with the given index.", "Args": {"input_handle": "A Tensor of type variant.", "index": "A Tensor of type int32.", "element_shape": "A Tensor of type int32.", "element_dtype": "A tf.DType.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type element_dtype."}, "tf.raw_ops.TensorListLength": {"description": "Returns the number of tensors in the input tensor list.", "Args": {"input_handle": "A Tensor of type variant.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int32."}, "tf.raw_ops.TensorListPopBack": {"description": "Returns the last element of the input list as well as a list with all but that element.", "Args": {"input_handle": "A Tensor of type variant.", "element_shape": "A Tensor of type int32.", "element_dtype": "A tf.DType.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (output_handle, tensor)."}, "tf.raw_ops.TensorListPushBack": {"description": "Returns a list which has the passed-in Tensor as last element and the other elements of the given list in input_handle.", "Args": {"input_handle": "A Tensor of type variant.", "tensor": "A Tensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.TensorListPushBackBatch": {"Args": {"input_handles": "A Tensor of type variant.", "tensor": "A Tensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.TensorListReserve": {"description": "List of the given size with empty elements.", "Args": {"element_shape": "A Tensor. Must be one of the following types: int32, int64.", "num_elements": "A Tensor of type int32.", "element_dtype": "A tf.DType.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.TensorListResize": {"description": "Resizes the list.", "Args": {"input_handle": "A Tensor of type variant.", "size": "A Tensor of type int32.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.TensorListScatter": {"description": "Creates a TensorList by indexing into a Tensor.", "Args": {"tensor": "A Tensor.", "indices": "A Tensor of type int32.", "element_shape": "A Tensor. Must be one of the following types: int32, int64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.TensorListScatterIntoExistingList": {"description": "Scatters tensor at indices in an input list.", "Args": {"input_handle": "A Tensor of type variant.", "tensor": "A Tensor.", "indices": "A Tensor of type int32.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.TensorListScatterV2": {"description": "Creates a TensorList by indexing into a Tensor.", "Args": {"tensor": "A Tensor.", "indices": "A Tensor of type int32.", "element_shape": "A Tensor. Must be one of the following types: int32, int64.", "num_elements": "A Tensor of type int32.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.TensorListSetItem": {"description": "Sets the index-th position of the list to contain the given tensor.", "Args": {"input_handle": "A Tensor of type variant.", "index": "A Tensor of type int32.", "item": "A Tensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.TensorListSplit": {"description": "Splits a tensor into a list.", "Args": {"tensor": "A Tensor.", "element_shape": "A Tensor. Must be one of the following types: int32, int64.", "lengths": "A Tensor of type int64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.TensorListStack": {"description": "Stacks all tensors in the list.", "Args": {"input_handle": "A Tensor of type variant.", "element_shape": "A Tensor of type int32.", "element_dtype": "A tf.DType.", "num_elements": "An optional int. Defaults to -1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type element_dtype."}, "tf.raw_ops.TensorScatterAdd": {"description": "Adds sparse updates to an existing tensor according to indices.", "Args": {"tensor": "A Tensor. Tensor to copy/update.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nIndex tensor.", "updates": "A Tensor. Must have the same type as tensor.\nUpdates to scatter into output.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as tensor."}, "tf.raw_ops.TensorScatterMax": {"description": "Apply a sparse update to a tensor taking the element-wise maximum.", "Args": {"tensor": "A Tensor. Tensor to update.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nIndex tensor.", "updates": "A Tensor. Must have the same type as tensor.\nUpdates to scatter into output.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as tensor."}, "tf.raw_ops.TensorScatterMin": {"Args": {"tensor": "A Tensor. Tensor to update.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nIndex tensor.", "updates": "A Tensor. Must have the same type as tensor.\nUpdates to scatter into output.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as tensor."}, "tf.raw_ops.TensorScatterSub": {"description": "Subtracts sparse updates from an existing tensor according to indices.", "Args": {"tensor": "A Tensor. Tensor to copy/update.", "indices": "A Tensor. Must be one of the following types: int32, int64.\nIndex tensor.", "updates": "A Tensor. Must have the same type as tensor.\nUpdates to scatter into output.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as tensor."}, "tf.raw_ops.TensorScatterUpdate": {"description": "Scatter updates into an existing tensor according to indices.", "Args": {"tensor": "A Tensor. Tensor to copy/update.", "indices": "A Tensor. Must be one of the following types: int16, int32, int64, uint16.\nIndex tensor.", "updates": "A Tensor. Must have the same type as tensor.\nUpdates to scatter into output.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as tensor."}, "tf.raw_ops.TensorSliceDataset": {"description": "Creates a dataset that emits each dim-0 slice of components once.", "Args": {"components": "A list of Tensor objects.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "is_files": "An optional bool. Defaults to False.", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.TensorStridedSliceUpdate": {"description": "Assign value to the sliced l-value reference of input.", "Args": {"input": "A Tensor.", "begin": "A Tensor. Must be one of the following types: int32, int64.", "end": "A Tensor. Must have the same type as begin.", "strides": "A Tensor. Must have the same type as begin.", "value": "A Tensor. Must have the same type as input.", "begin_mask": "An optional int. Defaults to 0.", "end_mask": "An optional int. Defaults to 0.", "ellipsis_mask": "An optional int. Defaults to 0.", "new_axis_mask": "An optional int. Defaults to 0.", "shrink_axis_mask": "An optional int. Defaults to 0.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.TensorSummary": {"description": "Outputs a Summary protocol buffer with a tensor.", "Args": {"tensor": "A Tensor. A tensor to serialize.", "description": "An optional string. Defaults to \"\".\nA json-encoded SummaryDescription proto.", "labels": "An optional list of strings. Defaults to [].\nAn unused list of strings.", "display_name": "An optional string. Defaults to \"\". An unused string.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.TensorSummaryV2": {"description": "Outputs a Summary protocol buffer with a tensor and per-plugin data.", "Args": {"tag": "A Tensor of type string.\nA string attached to this summary. Used for organization in TensorBoard.", "tensor": "A Tensor. A tensor to serialize.", "serialized_summary_metadata": "A Tensor of type string.\nA serialized SummaryMetadata proto. Contains plugin\ndata.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.TextLineDataset": {"description": "Creates a dataset that emits the lines of one or more text files.", "Args": {"filenames": "A Tensor of type string.\nA scalar or a vector containing the name(s) of the file(s) to be\nread.", "compression_type": "A Tensor of type string.\nA scalar containing either (i) the empty string (no\ncompression), (ii) \"ZLIB\", or (iii) \"GZIP\".", "buffer_size": "A Tensor of type int64.\nA scalar containing the number of bytes to buffer.", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.TextLineReader": {"description": "A Reader that outputs the lines of a file delimited by &#39;\\n&#39;.", "Args": {"skip_header_lines": "An optional int. Defaults to 0.\nNumber of lines to skip from the beginning of every file.", "container": "An optional string. Defaults to \"\".\nIf non-empty, this reader is placed in the given container.\nOtherwise, a default container is used.", "shared_name": "An optional string. Defaults to \"\".\nIf non-empty, this reader is named in the given bucket\nwith this shared_name. Otherwise, the node name is used instead.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type mutable string."}, "tf.raw_ops.TextLineReaderV2": {"description": "A Reader that outputs the lines of a file delimited by &#39;\\n&#39;.", "Args": {"skip_header_lines": "An optional int. Defaults to 0.\nNumber of lines to skip from the beginning of every file.", "container": "An optional string. Defaults to \"\".\nIf non-empty, this reader is placed in the given container.\nOtherwise, a default container is used.", "shared_name": "An optional string. Defaults to \"\".\nIf non-empty, this reader is named in the given bucket\nwith this shared_name. Otherwise, the node name is used instead.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type resource."}, "tf.raw_ops.ThreadPoolDataset": {"description": "Creates a dataset that uses a custom thread pool to compute input_dataset.", "Args": {"input_dataset": "A Tensor of type variant.", "thread_pool": "A Tensor of type resource.\nA resource produced by the ThreadPoolHandle op.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.ThreadPoolHandle": {"description": "Creates a dataset that uses a custom thread pool to compute input_dataset.", "Args": {"num_threads": "An int. The number of threads in the thread pool.", "display_name": "A string.\nA human-readable name for the threads that may be visible in some\nvisualizations.\nthreadpool.", "max_intra_op_parallelism": "An optional int. Defaults to 1.\nThe maximum degree of parallelism to use within operations that execute on this\nthreadpool.", "container": "An optional string. Defaults to \"\".", "shared_name": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type resource."}, "tf.raw_ops.ThreadUnsafeUnigramCandidateSampler": {"description": "Generates labels for candidate sampling with a learned unigram distribution.", "Args": {"true_classes": "A Tensor of type int64.\nA batch_size * num_true matrix, in which each row contains the\nIDs of the num_true target_classes in the corresponding original label.", "num_true": "An int that is >= 1. Number of true labels per context.", "num_sampled": "An int that is >= 1.\nNumber of candidates to randomly sample.", "unique": "A bool.\nIf unique is true, we sample with rejection, so that all sampled\ncandidates in a batch are unique. This requires some approximation to\nestimate the post-rejection sampling probabilities.", "range_max": "An int that is >= 1.\nThe sampler will sample integers from the interval [0, range_max).", "seed": "An optional int. Defaults to 0.\nIf either seed or seed2 are set to be non-zero, the random number\ngenerator is seeded by the given seed.  Otherwise, it is seeded by a\nrandom seed.", "seed2": "An optional int. Defaults to 0.\nAn second seed to avoid seed collision.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (sampled_candidates, true_expected_count, sampled_expected_count)."}, "tf.raw_ops.Tile": {"description": "Constructs a tensor by tiling a given tensor.", "Args": {"input": "A Tensor. 1-D or higher.", "multiples": "A Tensor. Must be one of the following types: int32, int64.\n1-D. Length must be the same as the number of dimensions in input", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.TileGrad": {"description": "Returns the gradient of Tile.", "Args": {"input": "A Tensor.", "multiples": "A Tensor of type int32.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.raw_ops.Timestamp": {"description": "Provides the time since epoch in seconds.", "Args": {"name": "A name for the operation (optional)."}, "Returns": "A Tensor of type float64."}, "tf.raw_ops.ToBool": {"description": "Converts a tensor to a scalar predicate.", "Args": {"input": "A Tensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type bool."}, "tf.raw_ops.TopK": {"description": "Finds values and indices of the k largest elements for the last dimension.", "Args": {"input": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.\n1-D or higher with last dimension at least k.", "k": "An int that is >= 0.\nNumber of top elements to look for along the last dimension (along each\nrow for matrices).", "sorted": "An optional bool. Defaults to True.\nIf true the resulting k elements will be sorted by the values in\ndescending order.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (values, indices)."}, "tf.raw_ops.TopKV2": {"description": "Finds values and indices of the k largest elements for the last dimension.", "Args": {"input": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.\n1-D or higher with last dimension at least k.", "k": "A Tensor of type int32.\n0-D.  Number of top elements to look for along the last dimension (along each\nrow for matrices).", "sorted": "An optional bool. Defaults to True.\nIf true the resulting k elements will be sorted by the values in\ndescending order.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (values, indices)."}, "tf.raw_ops.Transpose": {"description": "Shuffle dimensions of x according to a permutation.", "Args": {"x": "A Tensor.", "perm": "A Tensor. Must be one of the following types: int32, int64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.TridiagonalMatMul": {"description": "Calculate product with tridiagonal matrix.", "Args": {"superdiag": "A Tensor. Must be one of the following types: float64, float32, complex64, complex128.\nTensor of shape [..., 1, M], representing superdiagonals of\ntri-diagonal matrices to the left of multiplication. Last element is ignored.", "maindiag": "A Tensor. Must have the same type as superdiag.\nTensor of shape [..., 1, M], representing main diagonals of tri-diagonal\nmatrices to the left of multiplication.", "subdiag": "A Tensor. Must have the same type as superdiag.\nTensor of shape [..., 1, M], representing subdiagonals of tri-diagonal\nmatrices to the left of multiplication. First element is ignored.", "rhs": "A Tensor. Must have the same type as superdiag.\nTensor of shape [..., M, N], representing MxN matrices to the right of\nmultiplication.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as superdiag."}, "tf.raw_ops.TridiagonalSolve": {"description": "Solves tridiagonal systems of equations.", "Args": {"diagonals": "A Tensor. Must be one of the following types: float64, float32, complex64, complex128.\nTensor of shape [..., 3, M] whose innermost 2 dimensions represent the\ntridiagonal matrices with three rows being the superdiagonal, diagonals, and\nsubdiagonals, in order. The last element of the superdiagonal and the first\nelement of the subdiagonal is ignored.", "rhs": "A Tensor. Must have the same type as diagonals.\nTensor of shape [..., M, K], representing K right-hand sides per each\nleft-hand side.", "partial_pivoting": "An optional bool. Defaults to True.\nWhether to apply partial pivoting. Partial pivoting makes the procedure more\nstable, but slower.", "perturb_singular": "An optional bool. Defaults to False.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as diagonals."}, "tf.raw_ops.TruncateDiv": {"description": "Returns x / y element-wise for integer types.", "Args": {"x": "A Tensor. Must be one of the following types: bfloat16, half, float32, float64, uint8, int8, uint16, int16, int32, uint32, uint64, int64, complex64, complex128.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.TruncateMod": {"description": "Returns element-wise remainder of division. This emulates C semantics in that", "Args": {"x": "A Tensor. Must be one of the following types: int32, int64, bfloat16, half, float32, float64.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.TruncatedNormal": {"description": "Outputs random values from a truncated normal distribution.", "Args": {"shape": "A Tensor. Must be one of the following types: int32, int64.\nThe shape of the output tensor.", "dtype": "A tf.DType from: tf.half, tf.bfloat16, tf.float32, tf.float64.\nThe type of the output.", "seed": "An optional int. Defaults to 0.\nIf either seed or seed2 are set to be non-zero, the random number\ngenerator is seeded by the given seed.  Otherwise, it is seeded by a\nrandom seed.", "seed2": "An optional int. Defaults to 0.\nA second seed to avoid seed collision.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type dtype."}, "tf.raw_ops.Unbatch": {"description": "Reverses the operation of Batch for a single output Tensor.", "Args": {"batched_tensor": "A Tensor.", "batch_index": "A Tensor of type int64.", "id": "A Tensor of type int64.", "timeout_micros": "An int.", "container": "An optional string. Defaults to \"\".", "shared_name": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as batched_tensor."}, "tf.raw_ops.UnbatchDataset": {"description": "A dataset that splits the elements of its input into multiple elements.", "Args": {"input_dataset": "A Tensor of type variant.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.UnbatchGrad": {"description": "Gradient of Unbatch.", "Args": {"original_input": "A Tensor.", "batch_index": "A Tensor of type int64.", "grad": "A Tensor. Must have the same type as original_input.", "id": "A Tensor of type int64.", "container": "An optional string. Defaults to \"\".", "shared_name": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as original_input."}, "tf.raw_ops.UncompressElement": {"description": "Uncompresses a compressed dataset element.", "Args": {"compressed": "A Tensor of type variant.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A list of Tensor objects of type output_types."}, "tf.raw_ops.UnicodeDecode": {"description": "Decodes each string in input into a sequence of Unicode code points.", "Args": {"input": "A Tensor of type string.\nThe text to be decoded. Can have any shape. Note that the output is flattened\nto a vector of char values.", "input_encoding": "A string.\nText encoding of the input strings. This is any of the encodings supported\nby ICU ucnv algorithmic converters. Examples: \"UTF-16\", \"US ASCII\", \"UTF-8\".", "errors": "An optional string from: \"strict\", \"replace\", \"ignore\". Defaults to \"replace\".\nError handling policy when there is invalid formatting found in the input.\nThe value of 'strict' will cause the operation to produce a InvalidArgument\nerror on any invalid input formatting. A value of 'replace' (the default) will\ncause the operation to replace any invalid formatting in the input with the\nreplacement_char codepoint. A value of 'ignore' will cause the operation to\nskip any invalid formatting in the input and produce no corresponding output\ncharacter.", "replacement_char": "An optional int. Defaults to 65533.\nThe replacement character codepoint to be used in place of any invalid\nformatting in the input when errors='replace'. Any valid unicode codepoint may\nbe used. The default value is the default unicode replacement character is\n0xFFFD or U+65533.)", "replace_control_characters": "An optional bool. Defaults to False.\nWhether to replace the C0 control characters (00-1F) with the\nreplacement_char. Default is false.", "Tsplits": "An optional tf.DType from: tf.int32, tf.int64. Defaults to tf.int64.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (row_splits, char_values)."}, "tf.raw_ops.UnicodeDecodeWithOffsets": {"description": "Decodes each string in input into a sequence of Unicode code points.", "Args": {"input": "A Tensor of type string.\nThe text to be decoded. Can have any shape. Note that the output is flattened\nto a vector of char values.", "input_encoding": "A string.\nText encoding of the input strings. This is any of the encodings supported\nby ICU ucnv algorithmic converters. Examples: \"UTF-16\", \"US ASCII\", \"UTF-8\".", "errors": "An optional string from: \"strict\", \"replace\", \"ignore\". Defaults to \"replace\".\nError handling policy when there is invalid formatting found in the input.\nThe value of 'strict' will cause the operation to produce a InvalidArgument\nerror on any invalid input formatting. A value of 'replace' (the default) will\ncause the operation to replace any invalid formatting in the input with the\nreplacement_char codepoint. A value of 'ignore' will cause the operation to\nskip any invalid formatting in the input and produce no corresponding output\ncharacter.", "replacement_char": "An optional int. Defaults to 65533.\nThe replacement character codepoint to be used in place of any invalid\nformatting in the input when errors='replace'. Any valid unicode codepoint may\nbe used. The default value is the default unicode replacement character is\n0xFFFD or U+65533.)", "replace_control_characters": "An optional bool. Defaults to False.\nWhether to replace the C0 control characters (00-1F) with the\nreplacement_char. Default is false.", "Tsplits": "An optional tf.DType from: tf.int32, tf.int64. Defaults to tf.int64.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (row_splits, char_values, char_to_byte_starts)."}, "tf.raw_ops.UnicodeEncode": {"description": "Encode a tensor of ints into unicode strings.", "Args": {"input_values": "A Tensor of type int32.\nA 1D tensor containing the unicode codepoints that should be encoded.", "input_splits": "A Tensor. Must be one of the following types: int32, int64.\nA 1D tensor specifying how the unicode codepoints should be split into strings.\nIn particular, output[i] is constructed by encoding the codepoints in the\nslice input_values[input_splits[i]:input_splits[i+1]].", "output_encoding": "A string from: \"UTF-8\", \"UTF-16-BE\", \"UTF-32-BE\".\nUnicode encoding of the output strings. Valid encodings are: \"UTF-8\",\n\"UTF-16-BE\", and \"UTF-32-BE\".", "errors": "An optional string from: \"ignore\", \"replace\", \"strict\". Defaults to \"replace\".\nError handling policy when there is invalid formatting found in the input.\nThe value of 'strict' will cause the operation to produce a InvalidArgument\nerror on any invalid input formatting. A value of 'replace' (the default) will\ncause the operation to replace any invalid formatting in the input with the\nreplacement_char codepoint. A value of 'ignore' will cause the operation to\nskip any invalid formatting in the input and produce no corresponding output\ncharacter.", "replacement_char": "An optional int. Defaults to 65533.\nThe replacement character codepoint to be used in place of any invalid\nformatting in the input when errors='replace'. Any valid unicode codepoint may\nbe used. The default value is the default unicode replacement character is\n0xFFFD (U+65533).", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.UnicodeScript": {"description": "Determine the script codes of a given tensor of Unicode integer code points.", "Args": {"input": "A Tensor of type int32. A Tensor of int32 Unicode code points.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int32."}, "tf.raw_ops.UnicodeTranscode": {"description": "Transcode the input text from a source encoding to a destination encoding.", "Args": {"input": "A Tensor of type string.\nThe text to be processed. Can have any shape.", "input_encoding": "A string.\nText encoding of the input strings. This is any of the encodings supported\nby ICU ucnv algorithmic converters. Examples: \"UTF-16\", \"US ASCII\", \"UTF-8\".", "output_encoding": "A string from: \"UTF-8\", \"UTF-16-BE\", \"UTF-32-BE\".\nThe unicode encoding to use in the output. Must be one of\n\"UTF-8\", \"UTF-16-BE\", \"UTF-32-BE\". Multi-byte encodings will be big-endian.", "errors": "An optional string from: \"strict\", \"replace\", \"ignore\". Defaults to \"replace\".\nError handling policy when there is invalid formatting found in the input.\nThe value of 'strict' will cause the operation to produce a InvalidArgument\nerror on any invalid input formatting. A value of 'replace' (the default) will\ncause the operation to replace any invalid formatting in the input with the\nreplacement_char codepoint. A value of 'ignore' will cause the operation to\nskip any invalid formatting in the input and produce no corresponding output\ncharacter.", "replacement_char": "An optional int. Defaults to 65533.\nThe replacement character codepoint to be used in place of any invalid\nformatting in the input when errors='replace'. Any valid unicode codepoint may\nbe used. The default value is the default unicode replacement character is\n0xFFFD or U+65533.)\nNote that for UTF-8, passing a replacement character expressible in 1 byte, such\nas ' ', will preserve string alignment to the source since invalid bytes will be\nreplaced with a 1-byte replacement. For UTF-16-BE and UTF-16-LE, any 1 or 2 byte\nreplacement character will preserve byte alignment to the source.", "replace_control_characters": "An optional bool. Defaults to False.\nWhether to replace the C0 control characters (00-1F) with the\nreplacement_char. Default is false.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.UniformCandidateSampler": {"description": "Generates labels for candidate sampling with a uniform distribution.", "Args": {"true_classes": "A Tensor of type int64.\nA batch_size * num_true matrix, in which each row contains the\nIDs of the num_true target_classes in the corresponding original label.", "num_true": "An int that is >= 1. Number of true labels per context.", "num_sampled": "An int that is >= 1.\nNumber of candidates to randomly sample.", "unique": "A bool.\nIf unique is true, we sample with rejection, so that all sampled\ncandidates in a batch are unique. This requires some approximation to\nestimate the post-rejection sampling probabilities.", "range_max": "An int that is >= 1.\nThe sampler will sample integers from the interval [0, range_max).", "seed": "An optional int. Defaults to 0.\nIf either seed or seed2 are set to be non-zero, the random number\ngenerator is seeded by the given seed.  Otherwise, it is seeded by a\nrandom seed.", "seed2": "An optional int. Defaults to 0.\nAn second seed to avoid seed collision.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (sampled_candidates, true_expected_count, sampled_expected_count)."}, "tf.raw_ops.Unique": {"description": "Finds unique elements in a 1-D tensor.", "Args": {"x": "A Tensor. 1-D.", "out_idx": "An optional tf.DType from: tf.int32, tf.int64. Defaults to tf.int32.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (y, idx)."}, "tf.raw_ops.UniqueDataset": {"description": "Creates a dataset that contains the unique elements of input_dataset.", "Args": {"input_dataset": "A Tensor of type variant.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.UniqueV2": {"description": "Finds unique elements along an axis of a tensor.", "Args": {"x": "A Tensor. A Tensor.", "axis": "A Tensor. Must be one of the following types: int32, int64.\nA Tensor of type int32 (default: None). The axis of the Tensor to\nfind the unique elements.", "out_idx": "An optional tf.DType from: tf.int32, tf.int64. Defaults to tf.int32.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (y, idx)."}, "tf.raw_ops.UniqueWithCounts": {"description": "Finds unique elements in a 1-D tensor.", "Args": {"x": "A Tensor. 1-D.", "out_idx": "An optional tf.DType from: tf.int32, tf.int64. Defaults to tf.int32.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (y, idx, count)."}, "tf.raw_ops.UniqueWithCountsV2": {"description": "Finds unique elements along an axis of a tensor.", "Args": {"x": "A Tensor. A Tensor.", "axis": "A Tensor. Must be one of the following types: int32, int64.\nA Tensor of type int32 (default: None). The axis of the Tensor to\nfind the unique elements.", "out_idx": "An optional tf.DType from: tf.int32, tf.int64. Defaults to tf.int32.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of Tensor objects (y, idx, count)."}, "tf.raw_ops.Unpack": {"description": "Unpacks a given dimension of a rank-R tensor into num rank-(R-1) tensors.", "Args": {"value": "A Tensor.\n1-D or higher, with axis dimension size equal to num.", "num": "An int that is >= 0.", "axis": "An optional int. Defaults to 0.\nDimension along which to unpack.  Negative values wrap around, so the\nvalid range is [-R, R).", "name": "A name for the operation (optional)."}, "Returns": "A list of num Tensor objects with the same type as value."}, "tf.raw_ops.UnravelIndex": {"description": "Converts an array of flat indices into a tuple of coordinate arrays.", "Args": {"indices": "A Tensor. Must be one of the following types: int32, int64.\nAn 0-D or 1-D int Tensor whose elements are indices into the\nflattened version of an array of dimensions dims.", "dims": "A Tensor. Must have the same type as indices.\nAn 1-D int Tensor. The shape of the array to use for unraveling\nindices.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as indices."}, "tf.raw_ops.UnsortedSegmentJoin": {"description": "Joins the elements of inputs based on segment_ids.", "Args": {"inputs": "A Tensor of type string. The input to be joined.", "segment_ids": "A Tensor. Must be one of the following types: int32, int64.\nA tensor whose shape is a prefix of data.shape.  Negative segment ids are not\nsupported.", "num_segments": "A Tensor. Must be one of the following types: int32, int64.\nA scalar.", "separator": "An optional string. Defaults to \"\".\nThe separator to use when joining.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.UnsortedSegmentMax": {"description": "Computes the maximum along segments of a tensor.", "Args": {"data": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.", "segment_ids": "A Tensor. Must be one of the following types: int32, int64.\nA tensor whose shape is a prefix of data.shape.\nThe values must be less than num_segments.\nCaution: The values are always validated to be in range on CPU, never validated\non GPU.", "num_segments": "A Tensor. Must be one of the following types: int32, int64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as data."}, "tf.raw_ops.UnsortedSegmentMin": {"description": "Computes the minimum along segments of a tensor.", "Args": {"data": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.", "segment_ids": "A Tensor. Must be one of the following types: int32, int64.\nA tensor whose shape is a prefix of data.shape.\nThe values must be less than num_segments.\nCaution: The values are always validated to be in range on CPU, never validated\non GPU.", "num_segments": "A Tensor. Must be one of the following types: int32, int64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as data."}, "tf.raw_ops.UnsortedSegmentProd": {"description": "Computes the product along segments of a tensor.", "Args": {"data": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.", "segment_ids": "A Tensor. Must be one of the following types: int32, int64.\nA tensor whose shape is a prefix of data.shape.\nThe values must be less than num_segments.\nCaution: The values are always validated to be in range on CPU, never validated\non GPU.", "num_segments": "A Tensor. Must be one of the following types: int32, int64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as data."}, "tf.raw_ops.UnsortedSegmentSum": {"description": "Computes the sum along segments of a tensor.", "Args": {"data": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64.", "segment_ids": "A Tensor. Must be one of the following types: int32, int64.\nA tensor whose shape is a prefix of data.shape.\nThe values must be less than num_segments.\nCaution: The values are always validated to be in range on CPU, never validated\non GPU.", "num_segments": "A Tensor. Must be one of the following types: int32, int64.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as data."}, "tf.raw_ops.Unstage": {"description": "Op is similar to a lightweight Dequeue.", "Args": {"dtypes": "A list of tf.DTypes that has length >= 1.", "capacity": "An optional int that is >= 0. Defaults to 0.", "memory_limit": "An optional int that is >= 0. Defaults to 0.", "container": "An optional string. Defaults to \"\".", "shared_name": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A list of Tensor objects of type dtypes."}, "tf.raw_ops.UnwrapDatasetVariant": {"Args": {"input_handle": "A Tensor of type variant.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.UpperBound": {"description": "Applies upper_bound(sorted_search_values, values) along each row.", "Args": {"sorted_inputs": "A Tensor. 2-D Tensor where each row is ordered.", "values": "A Tensor. Must have the same type as sorted_inputs.\n2-D Tensor with the same numbers of rows as sorted_search_values. Contains\nthe values that will be searched for in sorted_search_values.", "out_type": "An optional tf.DType from: tf.int32, tf.int64. Defaults to tf.int32.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type out_type."}, "tf.raw_ops.VarHandleOp": {"description": "Creates a handle to a Variable resource.", "Args": {"dtype": "A tf.DType. the type of this variable. Must agree with the dtypes\nof all ops using this variable.", "shape": "A tf.TensorShape or list of ints.\nThe (possibly partially specified) shape of this variable.", "container": "An optional string. Defaults to \"\".\nthe container this variable is placed in.", "shared_name": "An optional string. Defaults to \"\".\nthe name by which this variable is referred to.", "allowed_devices": "An optional list of strings. Defaults to [].\nDEPRECATED. The allowed devices containing the resource variable. Set when the\noutput ResourceHandle represents a per-replica/partitioned resource variable.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type resource."}, "tf.raw_ops.VarIsInitializedOp": {"description": "Checks whether a resource handle-based variable has been initialized.", "Args": {"resource": "A Tensor of type resource. the input resource handle.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type bool."}, "tf.raw_ops.Variable": {"description": "Use VariableV2 instead.", "Args": {"shape": "A tf.TensorShape or list of ints.", "dtype": "A tf.DType.", "container": "An optional string. Defaults to \"\".", "shared_name": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor of type dtype."}, "tf.raw_ops.VariableShape": {"description": "Returns the shape of the variable pointed to by resource.", "Args": {"input": "A Tensor of type resource.", "out_type": "An optional tf.DType from: tf.int32, tf.int64. Defaults to tf.int32.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type out_type."}, "tf.raw_ops.VariableV2": {"description": "Holds state in the form of a tensor that persists across steps.", "Args": {"shape": "A tf.TensorShape or list of ints.\nThe shape of the variable tensor.", "dtype": "A tf.DType. The type of elements in the variable tensor.", "container": "An optional string. Defaults to \"\".\nIf non-empty, this variable is placed in the given container.\nOtherwise, a default container is used.", "shared_name": "An optional string. Defaults to \"\".\nIf non-empty, this variable is named in the given bucket\nwith this shared_name. Otherwise, the node name is used instead.", "name": "A name for the operation (optional)."}, "Returns": "A mutable Tensor of type dtype."}, "tf.raw_ops.Where": {"description": "Returns locations of nonzero / true values in a tensor.", "Args": {"condition": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64, bool.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int64."}, "tf.raw_ops.While": {"description": "output = input; While (Cond(output)) { output = Body(output) }", "Args": {"input": "A list of Tensor objects.\nA list of input tensors whose types are T.", "cond": "A function decorated with @Defun.\nA function takes 'input' and returns a tensor.  If the tensor is\na scalar of non-boolean, the scalar is converted to a boolean\naccording to the following rule: if the scalar is a numerical\nvalue, non-zero means True and zero means False; if the scalar is\na string, non-empty means True and empty means False. If the\ntensor is not a scalar, non-emptiness means True and False\notherwise.", "body": "A function decorated with @Defun.\nA function that takes a list of tensors and returns another\nlist of tensors. Both lists have the same types as specified\nby T.", "output_shapes": "An optional list of shapes (each a tf.TensorShape or list of ints). Defaults to [].", "parallel_iterations": "An optional int. Defaults to 10.", "name": "A name for the operation (optional)."}, "Returns": "A list of Tensor objects. Has the same type as input."}, "tf.raw_ops.WholeFileReader": {"description": "A Reader that outputs the entire contents of a file as a value.", "Args": {"container": "An optional string. Defaults to \"\".\nIf non-empty, this reader is placed in the given container.\nOtherwise, a default container is used.", "shared_name": "An optional string. Defaults to \"\".\nIf non-empty, this reader is named in the given bucket\nwith this shared_name. Otherwise, the node name is used instead.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type mutable string."}, "tf.raw_ops.WholeFileReaderV2": {"description": "A Reader that outputs the entire contents of a file as a value.", "Args": {"container": "An optional string. Defaults to \"\".\nIf non-empty, this reader is placed in the given container.\nOtherwise, a default container is used.", "shared_name": "An optional string. Defaults to \"\".\nIf non-empty, this reader is named in the given bucket\nwith this shared_name. Otherwise, the node name is used instead.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type resource."}, "tf.raw_ops.WindowDataset": {"description": "Combines (nests of) input elements into a dataset of (nests of) windows.", "Args": {"input_dataset": "A Tensor of type variant.", "size": "A Tensor of type int64.\nAn integer scalar, representing the number of elements\nof the input dataset to combine into a window. Must be positive.", "shift": "A Tensor of type int64.\nAn integer scalar, representing the number of input elements\nby which the window moves in each iteration.  Defaults to size.\nMust be positive.", "stride": "A Tensor of type int64.\nAn integer scalar, representing the stride of the input elements\nin the sliding window. Must be positive. The default value of 1 means\n\"retain every input element\".", "drop_remainder": "A Tensor of type bool.\nA Boolean scalar, representing whether the last window should be\ndropped if its size is smaller than window_size.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.WindowOp": {"Args": {"inputs": "A list of Tensor objects.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.WorkerHeartbeat": {"description": "Worker heartbeat op.", "Args": {"request": "A Tensor of type string.\nA string tensor containing a serialized WorkerHeartbeatRequest", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.raw_ops.WrapDatasetVariant": {"Args": {"input_handle": "A Tensor of type variant.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}, "tf.raw_ops.WriteAudioSummary": {"description": "Writes an audio summary.", "Args": {"writer": "A Tensor of type resource.", "step": "A Tensor of type int64.", "tag": "A Tensor of type string.", "tensor": "A Tensor of type float32.", "sample_rate": "A Tensor of type float32.", "max_outputs": "An optional int that is >= 1. Defaults to 3.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.WriteFile": {"description": "Writes contents to the file at input filename.", "Args": {"filename": "A Tensor of type string.\nscalar. The name of the file to which we write the contents.", "contents": "A Tensor of type string.\nscalar. The content to be written to the output file.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.WriteGraphSummary": {"description": "Writes a graph summary.", "Args": {"writer": "A Tensor of type resource.", "step": "A Tensor of type int64.", "tensor": "A Tensor of type string.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.WriteHistogramSummary": {"description": "Writes a histogram summary.", "Args": {"writer": "A Tensor of type resource.", "step": "A Tensor of type int64.", "tag": "A Tensor of type string.", "values": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64, bool.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.WriteImageSummary": {"description": "Writes an image summary.", "Args": {"writer": "A Tensor of type resource.", "step": "A Tensor of type int64.", "tag": "A Tensor of type string.", "tensor": "A Tensor. Must be one of the following types: uint8, float64, float32, half.", "bad_color": "A Tensor of type uint8.", "max_images": "An optional int that is >= 1. Defaults to 3.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.WriteRawProtoSummary": {"description": "Writes a serialized proto summary.", "Args": {"writer": "A Tensor of type resource.", "step": "A Tensor of type int64.", "tensor": "A Tensor of type string.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.WriteScalarSummary": {"description": "Writes a scalar summary.", "Args": {"writer": "A Tensor of type resource.", "step": "A Tensor of type int64.", "tag": "A Tensor of type string.", "value": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.WriteSummary": {"description": "Writes a tensor summary.", "Args": {"writer": "A Tensor of type resource.", "step": "A Tensor of type int64.", "tensor": "A Tensor.", "tag": "A Tensor of type string.", "summary_metadata": "A Tensor of type string.", "name": "A name for the operation (optional)."}, "Returns": "The created Operation."}, "tf.raw_ops.Xdivy": {"description": "Returns 0 if x == 0, and x / y otherwise, elementwise.", "Args": {"x": "A Tensor. Must be one of the following types: half, float32, float64, complex64, complex128.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.XlaConcatND": {"description": "Concats input tensor across all dimensions.", "Args": {"inputs": "A list of at least 1 Tensor objects with the same type.\nInput tensor slices in row-major order to merge across all dimensions. All\ninputs must have the same shape.\n  }\n  out_arg {\n    name: \"output\"\n    description: <", "num_concats": "A list of ints. Number of ways to merge per dimension.", "paddings": "An optional list of ints. Defaults to [].\nOptional list of right paddings per dimension to strip from the final merged\ntensor. These paddings must not exceed the dimension size of the merged result\nprior to stripping paddings.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as inputs."}, "tf.raw_ops.XlaSplitND": {"description": "Splits input tensor across all dimensions.", "Args": {"input": "A Tensor. Input tensor to split across all dimensions.\n  }\n  out_arg {\n    name: \"outputs\"\n    description: <", "N": "An int that is >= 1.", "num_splits": "A list of ints.\nNumber of ways to split per dimension. Shape dimensions must be evenly\ndivisible.", "paddings": "An optional list of ints. Defaults to [].\nOptional list of right paddings per dimension of input tensor to apply before\nsplitting. This can be used to make a dimension evenly divisible.", "name": "A name for the operation (optional)."}, "Returns": "A list of N Tensor objects with the same type as input."}, "tf.raw_ops.Xlog1py": {"description": "Returns 0 if x == 0, and x * log1p(y) otherwise, elementwise.", "Args": {"x": "A Tensor. Must be one of the following types: half, float32, float64, complex64, complex128.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.Xlogy": {"description": "Returns 0 if x == 0, and x * log(y) otherwise, elementwise.", "Args": {"x": "A Tensor. Must be one of the following types: half, float32, float64, complex64, complex128.", "y": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.ZerosLike": {"description": "Returns a tensor of zeros with the same shape and type as x.", "Args": {"x": "A Tensor. a tensor of type T.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.Zeta": {"description": "Compute the Hurwitz zeta function \\\\(\\zeta(x, q)\\\\).", "Args": {"x": "A Tensor. Must be one of the following types: float32, float64.", "q": "A Tensor. Must have the same type as x.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as x."}, "tf.raw_ops.ZipDataset": {"description": "Creates a dataset that zips together input_datasets.", "Args": {"input_datasets": "A list of at least 1 Tensor objects with type variant.\nList of N variant Tensors representing datasets to be zipped together.", "output_types": "A list of tf.DTypes that has length >= 1.", "output_shapes": "A list of shapes (each a tf.TensorShape or list of ints) that has length >= 1.", "metadata": "An optional string. Defaults to \"\".", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type variant."}}, "tf.saved_model": {"tf.saved_model.Asset": {"description": "Represents a file asset to hermetically include in a SavedModel.", "Attributes": {"asset_path": "A path, or a 0-D tf.string tensor with path to the asset."}}, "tf.saved_model.LoadOptions": {"description": "Options for loading a SavedModel.", "Args": {"allow_partial_checkpoint": "bool. Defaults to False. When enabled, allows\nthe SavedModel checkpoint to not entirely match the loaded object.", "experimental_io_device": "string. Applies in a distributed setting.\nTensorflow device to use to access the filesystem. If None (default)\nthen for each variable the filesystem is accessed from the CPU:0 device\nof the host where that variable is assigned. If specified, the\nfilesystem is instead accessed from that device for all variables.\nThis is for example useful if you want to load from a local directory,\nsuch as \"/tmp\" when running in a distributed setting. In that case\npass a device for the host where the \"/tmp\" directory is accessible.", "experimental_skip_checkpoint": "bool. Defaults to False. If set to True,\ncheckpoints will not be restored. Note that this in the majority of\ncases will generate an unusable model."}, "Attributes": {"allow_partial_checkpoint": "", "experimental_io_device": "", "experimental_skip_checkpoint": ""}}, "tf.saved_model.SaveOptions": {"description": "Options for saving to SavedModel.", "Args": {"namespace_whitelist": "List of strings containing op namespaces to whitelist\nwhen saving a model. Saving an object that uses namespaced ops must\nexplicitly add all namespaces to the whitelist. The namespaced ops must\nbe registered into the framework when loading the SavedModel. If no\nwhitelist is provided, all namespaced ops will be allowed.", "save_debug_info": "Boolean indicating whether debug information is saved. If\nTrue, then a debug/saved_model_debug_info.pb file will be written with\nthe contents of a GraphDebugInfo binary protocol buffer containing stack\ntrace information for all ops and functions that are saved.", "function_aliases": "Python dict. Mapping from string to object returned by\n@tf.function. A single tf.function can generate many ConcreteFunctions.\nIf a downstream tool wants to refer to all concrete functions generated\nby a single tf.function you can use the function_aliases argument to\nstore a map from the alias name to all concrete function names.\nE.g.\nclass Adder(tf.Module):\u00a0 @tf.function\u00a0 def double(self, x):\u00a0 \u00a0 return x + x\nmodel = Adder()model.double.get_concrete_function(\u00a0 tf.TensorSpec(shape=[], dtype=tf.float32, name=\"float_input\"))model.double.get_concrete_function(\u00a0 tf.TensorSpec(shape=[], dtype=tf.string, name=\"string_input\"))\noptions = tf.saved_model.SaveOptions(\u00a0 function_aliases={'double': model.double})tf.saved_model.save(model, '/tmp/adder', options=options)", "experimental_io_device": "string. Applies in a distributed setting.\nTensorflow device to use to access the filesystem. If None (default)\nthen for each variable the filesystem is accessed from the CPU:0 device\nof the host where that variable is assigned. If specified, the\nfilesystem is instead accessed from that device for all variables.\nThis is for example useful if you want to save to a local directory,\nsuch as \"/tmp\" when running in a distributed setting. In that case pass\na device for the host where the \"/tmp\" directory is accessible.", "experimental_variable_policy": "The policy to apply to variables when\nsaving. This is either a saved_model.experimental.VariablePolicy enum\ninstance or one of its value strings (case is not important). See that\nenum documentation for details. A value of None corresponds to the\ndefault policy.", "experimental_custom_gradients": "Boolean. When True, will save traced\ngradient functions for the functions decorated by tf.custom_gradient.\nDefaults to True."}, "Attributes": {"experimental_custom_gradients": "", "experimental_io_device": "", "experimental_variable_policy": "", "function_aliases": "", "namespace_whitelist": "", "save_debug_info": ""}}, "tf.saved_model.contains_saved_model": {"description": "Checks whether the provided export directory could contain a SavedModel.", "Args": {"export_dir": "Absolute path to possible export location. For example,\n'/my/foo/model'."}, "Returns": "True if the export directory contains SavedModel files, False otherwise."}, "tf.saved_model.load": {"description": "Load a SavedModel from export_dir.", "Args": {"export_dir": "The SavedModel directory to load from.", "tags": "A tag or sequence of tags identifying the MetaGraph to load. Optional\nif the SavedModel contains a single MetaGraph, as for those exported from\ntf.saved_model.save.", "options": "tf.saved_model.LoadOptions object that specifies options for\nloading."}, "Returns": "A trackable object with a signatures attribute mapping from signature\nkeys to functions. If the SavedModel was exported by tf.saved_model.save,\nit also points to trackable objects, functions, debug info which it has been\nsaved.", "Raises": {"ValueError": "If tags don't match a MetaGraph in the SavedModel."}}, "tf.saved_model.save": {"description": "Exports a [tf.Module](https://www.tensorflow.org/api_docs/python/tf/Module) (and subclasses) obj to [SavedModel format](https://www.tensorflow.org/guide/saved_model#the_savedmodel_format_on_disk).", "Args": {"obj": "A trackable object (e.g. tf.Module or tf.train.Checkpoint) to export.", "export_dir": "A directory in which to write the SavedModel.", "signatures": "Optional, one of three types:\n\na tf.function with an input signature specified, which will use the\ndefault serving signature key,\nthe result of f.get_concrete_function on a @tf.function-decorated\nfunction f, in which case f will be used to generate a signature for\nthe SavedModel under the default serving signature key,\na dictionary, which maps signature keys to either tf.function\ninstances with input signatures or concrete functions. Keys of such a\ndictionary may be arbitrary strings, but will typically be from the\ntf.saved_model.signature_constants module.", "options": "tf.saved_model.SaveOptions object for configuring save options."}, "Raises": {"ValueError": "If obj is not trackable."}}}, "tf.sets": {"tf.sets.difference": {"description": "Compute set difference of elements in last dimension of a and b.", "Args": {"a": "Tensor or SparseTensor of the same type as b. If sparse, indices\nmust be sorted in row-major order.", "b": "Tensor or SparseTensor of the same type as a. If sparse, indices\nmust be sorted in row-major order.", "aminusb": "Whether to subtract b from a, vs vice versa.", "validate_indices": "Whether to validate the order and range of sparse indices\nin a and b."}, "Returns": "A SparseTensor whose shape is the same rank as a and b, and all but\nthe last dimension the same. Elements along the last dimension contain the\ndifferences.", "Raises": {"TypeError": "If inputs are invalid types, or if a and b have\ndifferent types.", "ValueError": "If a is sparse and b is dense.", "errors_impl.InvalidArgumentError": "If the shapes of a and b do not\nmatch in any dimension other than the last dimension."}}, "tf.sets.intersection": {"description": "Compute set intersection of elements in last dimension of a and b.", "Args": {"a": "Tensor or SparseTensor of the same type as b. If sparse, indices\nmust be sorted in row-major order.", "b": "Tensor or SparseTensor of the same type as a. If sparse, indices\nmust be sorted in row-major order.", "validate_indices": "Whether to validate the order and range of sparse indices\nin a and b."}, "Returns": "A SparseTensor whose shape is the same rank as a and b, and all but\nthe last dimension the same. Elements along the last dimension contain the\nintersections."}, "tf.sets.size": {"description": "Compute number of unique elements along last dimension of a.", "Args": {"a": "SparseTensor, with indices sorted in row-major order.", "validate_indices": "Whether to validate the order and range of sparse indices\nin a."}, "Returns": "int32 Tensor of set sizes. For a ranked n, this is a Tensor with\nrank n-1, and the same 1st n-1 dimensions as a. Each value is the\nnumber of unique elements in the corresponding [0...n-1] dimension of a.", "Raises": {"TypeError": "If a is an invalid types."}}, "tf.sets.union": {"description": "Compute set union of elements in last dimension of a and b.", "Args": {"a": "Tensor or SparseTensor of the same type as b. If sparse, indices\nmust be sorted in row-major order.", "b": "Tensor or SparseTensor of the same type as a. If sparse, indices\nmust be sorted in row-major order.", "validate_indices": "Whether to validate the order and range of sparse indices\nin a and b."}, "Returns": "A SparseTensor whose shape is the same rank as a and b, and all but\nthe last dimension the same. Elements along the last dimension contain the\nunions."}}, "tf.signal": {"tf.signal.dct": {"description": "Computes the 1D [Discrete Cosine Transform (DCT)][dct] of input.", "Args": {"input": "A [..., samples] float32/float64 Tensor containing the\nsignals to take the DCT of.", "type": "The DCT type to perform. Must be 1, 2, 3 or 4.", "n": "The length of the transform. If length is less than sequence length,\nonly the first n elements of the sequence are considered for the DCT.\nIf n is greater than the sequence length, zeros are padded and then\nthe DCT is computed as usual.", "axis": "For future expansion. The axis to compute the DCT along. Must be -1.", "norm": "The normalization to apply. None for no normalization or 'ortho'\nfor orthonormal normalization.", "name": "An optional name for the operation."}, "Returns": "A [..., samples] float32/float64 Tensor containing the DCT of\ninput.", "Raises": {"ValueError": "If type is 1 and norm is ortho."}}, "tf.signal.fft": {"description": "Fast Fourier transform.", "Args": {"input": "A Tensor. Must be one of the following types: complex64, complex128.\nA complex tensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.signal.fft2d": {"description": "2D fast Fourier transform.", "Args": {"input": "A Tensor. Must be one of the following types: complex64, complex128.\nA complex tensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.signal.fft3d": {"description": "3D fast Fourier transform.", "Args": {"input": "A Tensor. Must be one of the following types: complex64, complex128.\nA complex tensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.signal.fftshift": {"description": "Shift the zero-frequency component to the center of the spectrum.", "Args": {"x": "Tensor, input tensor.", "axes": "int or shape tuple, optional Axes over which to shift.  Default is\nNone, which shifts all axes.", "name": "An optional name for the operation."}, "Returns": "A Tensor, The shifted tensor."}, "tf.signal.frame": {"description": "Expands signal&#39;s axis dimension into frames of frame_length.", "Args": {"signal": "A [..., samples, ...] Tensor. The rank and dimensions\nmay be unknown. Rank must be at least 1.", "frame_length": "The frame length in samples. An integer or scalar Tensor.", "frame_step": "The frame hop size in samples. An integer or scalar Tensor.", "pad_end": "Whether to pad the end of signal with pad_value.", "pad_value": "An optional scalar Tensor to use where the input signal\ndoes not exist when pad_end is True.", "axis": "A scalar integer Tensor indicating the axis to frame. Defaults to\nthe last axis. Supports negative values for indexing from the end.", "name": "An optional name for the operation."}, "Returns": "A Tensor of frames with shape [..., num_frames, frame_length, ...].", "Raises": {"ValueError": "If frame_length, frame_step, pad_value, or axis are not\nscalar."}}, "tf.signal.hamming_window": {"description": "Generate a [Hamming][hamming] window.", "Args": {"window_length": "A scalar Tensor indicating the window length to generate.", "periodic": "A bool Tensor indicating whether to generate a periodic or\nsymmetric window. Periodic windows are typically used for spectral\nanalysis while symmetric windows are typically used for digital\nfilter design.", "dtype": "The data type to produce. Must be a floating point type.", "name": "An optional name for the operation."}, "Returns": "A Tensor of shape [window_length] of type dtype.", "Raises": {"ValueError": "If dtype is not a floating point type."}}, "tf.signal.hann_window": {"description": "Generate a [Hann window][hann].", "Args": {"window_length": "A scalar Tensor indicating the window length to generate.", "periodic": "A bool Tensor indicating whether to generate a periodic or\nsymmetric window. Periodic windows are typically used for spectral\nanalysis while symmetric windows are typically used for digital\nfilter design.", "dtype": "The data type to produce. Must be a floating point type.", "name": "An optional name for the operation."}, "Returns": "A Tensor of shape [window_length] of type dtype.", "Raises": {"ValueError": "If dtype is not a floating point type."}}, "tf.signal.idct": {"description": "Computes the 1D [Inverse Discrete Cosine Transform (DCT)][idct] of input.", "Args": {"input": "A [..., samples] float32/float64 Tensor containing the\nsignals to take the DCT of.", "type": "The IDCT type to perform. Must be 1, 2, 3 or 4.", "n": "For future expansion. The length of the transform. Must be None.", "axis": "For future expansion. The axis to compute the DCT along. Must be -1.", "norm": "The normalization to apply. None for no normalization or 'ortho'\nfor orthonormal normalization.", "name": "An optional name for the operation."}, "Returns": "A [..., samples] float32/float64 Tensor containing the IDCT of\ninput.", "Raises": {"ValueError": "If type is not 1, 2 or 3, n is not None,axisis\nnot-1, ornormis notNoneor'ortho'`."}}, "tf.signal.ifft": {"description": "Inverse fast Fourier transform.", "Args": {"input": "A Tensor. Must be one of the following types: complex64, complex128.\nA complex tensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.signal.ifft2d": {"description": "Inverse 2D fast Fourier transform.", "Args": {"input": "A Tensor. Must be one of the following types: complex64, complex128.\nA complex tensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.signal.ifft3d": {"description": "Inverse 3D fast Fourier transform.", "Args": {"input": "A Tensor. Must be one of the following types: complex64, complex128.\nA complex tensor.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor. Has the same type as input."}, "tf.signal.ifftshift": {"description": "The inverse of fftshift.", "Args": {"x": "Tensor, input tensor.", "axes": "int or shape tuple Axes over which to calculate. Defaults to None,\nwhich shifts all axes.", "name": "An optional name for the operation."}, "Returns": "A Tensor, The shifted tensor."}, "tf.signal.inverse_mdct": {"description": "Computes the inverse modified DCT of mdcts.", "Args": {"mdcts": "A float32/float64 [..., frames, frame_length // 2]\nTensor of MDCT bins representing a batch of frame_length // 2-point\nMDCTs.", "window_fn": "A callable that takes a frame_length and a dtype keyword\nargument and returns a [frame_length] Tensor of samples in the\nprovided datatype. If set to None, a rectangular window with a scale of\n1/sqrt(2) is used. For perfect reconstruction of a signal from mdct\nfollowed by inverse_mdct, please use tf.signal.vorbis_window,\ntf.signal.kaiser_bessel_derived_window or None. If using another\nwindow function, make sure that w[n]^2 + w[n + frame_length // 2]^2 = 1\nand w[n] = w[frame_length - n - 1] for n = 0,...,frame_length // 2 - 1 to\nachieve perfect reconstruction.", "norm": "If \"ortho\", orthonormal inverse DCT4 is performed, if it is None,\na regular dct4 followed by scaling of 1/frame_length is performed.", "name": "An optional name for the operation."}, "Returns": "A [..., samples] Tensor of float32/float64 signals representing\nthe inverse MDCT for each input MDCT in mdcts where samples is\n(frames - 1) * (frame_length // 2) + frame_length.", "Raises": {"ValueError": "If mdcts is not at least rank 2."}}, "tf.signal.inverse_stft": {"description": "Computes the inverse [Short-time Fourier Transform][stft] of stfts.", "Args": {"stfts": "A complex64/complex128 [..., frames, fft_unique_bins]\nTensor of STFT bins representing a batch of fft_length-point STFTs\nwhere fft_unique_bins is fft_length // 2 + 1", "frame_length": "An integer scalar Tensor. The window length in samples.", "frame_step": "An integer scalar Tensor. The number of samples to step.", "fft_length": "An integer scalar Tensor. The size of the FFT that produced\nstfts. If not provided, uses the smallest power of 2 enclosing\nframe_length.", "window_fn": "A callable that takes a window length and a dtype keyword\nargument and returns a [window_length] Tensor of samples in the\nprovided datatype. If set to None, no windowing is used.", "name": "An optional name for the operation."}, "Returns": "A [..., samples] Tensor of float32/float64 signals representing\nthe inverse STFT for each input STFT in stfts.", "Raises": {"ValueError": "If stfts is not at least rank 2, frame_length is not scalar,\nframe_step is not scalar, or fft_length is not scalar."}}, "tf.signal.inverse_stft_window_fn": {"description": "Generates a window function that can be used in inverse_stft.", "Args": {"frame_step": "An integer scalar Tensor. The number of samples to step.", "forward_window_fn": "window_fn used in the forward transform, stft.", "name": "An optional name for the operation."}, "Returns": "A callable that takes a window length and a dtype keyword argument and\nreturns a [window_length] Tensor of samples in the provided datatype.\nThe returned window is suitable for reconstructing original waveform in\ninverse_stft."}, "tf.signal.irfft": {"description": "Inverse real-valued fast Fourier transform.", "Args": {"input": "A Tensor. Must be one of the following types: complex64, complex128.\nA complex tensor.", "fft_length": "A Tensor of type int32.\nAn int32 tensor of shape [1]. The FFT length.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type Treal."}, "tf.signal.irfft2d": {"description": "Inverse 2D real-valued fast Fourier transform.", "Args": {"input": "A Tensor. Must be one of the following types: complex64, complex128.\nA complex tensor.", "fft_length": "A Tensor of type int32.\nAn int32 tensor of shape [2]. The FFT length for each dimension.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type Treal."}, "tf.signal.irfft3d": {"description": "Inverse 3D real-valued fast Fourier transform.", "Args": {"input": "A Tensor. Must be one of the following types: complex64, complex128.\nA complex tensor.", "fft_length": "A Tensor of type int32.\nAn int32 tensor of shape [3]. The FFT length for each dimension.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type Treal."}, "tf.signal.kaiser_bessel_derived_window": {"description": "Generate a [Kaiser Bessel derived window][kbd].", "Args": {"window_length": "A scalar Tensor indicating the window length to generate.", "beta": "Beta parameter for Kaiser window.", "dtype": "The data type to produce. Must be a floating point type.", "name": "An optional name for the operation."}, "Returns": "A Tensor of shape [window_length] of type dtype."}, "tf.signal.kaiser_window": {"description": "Generate a [Kaiser window][kaiser].", "Args": {"window_length": "A scalar Tensor indicating the window length to generate.", "beta": "Beta parameter for Kaiser window, see reference below.", "dtype": "The data type to produce. Must be a floating point type.", "name": "An optional name for the operation."}, "Returns": "A Tensor of shape [window_length] of type dtype."}, "tf.signal.linear_to_mel_weight_matrix": {"description": "Returns a matrix to warp linear scale spectrograms to the [mel scale][mel].", "Args": {"num_mel_bins": "Python int. How many bands in the resulting mel spectrum.", "num_spectrogram_bins": "An integer Tensor. How many bins there are in the\nsource spectrogram data, which is understood to be fft_size // 2 + 1,\ni.e. the spectrogram only contains the nonredundant FFT bins.", "sample_rate": "An integer or float Tensor. Samples per second of the input\nsignal used to create the spectrogram. Used to figure out the frequencies\ncorresponding to each spectrogram bin, which dictates how they are mapped\ninto the mel scale.", "lower_edge_hertz": "Python float. Lower bound on the frequencies to be\nincluded in the mel spectrum. This corresponds to the lower edge of the\nlowest triangular band.", "upper_edge_hertz": "Python float. The desired top edge of the highest\nfrequency band.", "dtype": "The DType of the result matrix. Must be a floating point type.", "name": "An optional name for the operation."}, "Returns": "A Tensor of shape [num_spectrogram_bins, num_mel_bins].", "Raises": {"ValueError": "If num_mel_bins/num_spectrogram_bins/sample_rate are not\npositive, lower_edge_hertz is negative, frequency edges are incorrectly\nordered, upper_edge_hertz is larger than the Nyquist frequency."}}, "tf.signal.mdct": {"description": "Computes the [Modified Discrete Cosine Transform][mdct] of signals.", "Args": {"signals": "A [..., samples] float32/float64 Tensor of real-valued\nsignals.", "frame_length": "An integer scalar Tensor. The window length in samples\nwhich must be divisible by 4.", "window_fn": "A callable that takes a frame_length and a dtype keyword\nargument and returns a [frame_length] Tensor of samples in the\nprovided datatype. If set to None, a rectangular window with a scale of\n1/sqrt(2) is used. For perfect reconstruction of a signal from mdct\nfollowed by inverse_mdct, please use tf.signal.vorbis_window,\ntf.signal.kaiser_bessel_derived_window or None. If using another\nwindow function, make sure that w[n]^2 + w[n + frame_length // 2]^2 = 1\nand w[n] = w[frame_length - n - 1] for n = 0,...,frame_length // 2 - 1 to\nachieve perfect reconstruction.", "pad_end": "Whether to pad the end of signals with zeros when the provided\nframe length and step produces a frame that lies partially past its end.", "norm": "If it is None, unnormalized dct4 is used, if it is \"ortho\"\northonormal dct4 is used.", "name": "An optional name for the operation."}, "Returns": "A [..., frames, frame_length // 2] Tensor of float32/float64\nMDCT values where frames is roughly samples // (frame_length // 2)\nwhen pad_end=False.", "Raises": {"ValueError": "If signals is not at least rank 1, frame_length is\nnot scalar, or frame_length is not a multiple of 4."}}, "tf.signal.mfccs_from_log_mel_spectrograms": {"description": "Computes [MFCCs][mfcc] of log_mel_spectrograms.", "Args": {"log_mel_spectrograms": "A [..., num_mel_bins] float32/float64 Tensor\nof log-magnitude mel-scale spectrograms.", "name": "An optional name for the operation."}, "Returns": "A [..., num_mel_bins] float32/float64 Tensor of the MFCCs of\nlog_mel_spectrograms.", "Raises": {"ValueError": "If num_mel_bins is not positive."}}, "tf.signal.overlap_and_add": {"description": "Reconstructs a signal from a framed representation.", "Args": {"signal": "A [..., frames, frame_length] Tensor. All dimensions may be\nunknown, and rank must be at least 2.", "frame_step": "An integer or scalar Tensor denoting overlap offsets. Must be\nless than or equal to frame_length.", "name": "An optional name for the operation."}, "Returns": "A Tensor with shape [..., output_size] containing the overlap-added\nframes of signal's inner-most two dimensions.", "Raises": {"ValueError": "If signal's rank is less than 2, or frame_step is not a\nscalar integer."}}, "tf.signal.rfft": {"description": "Real-valued fast Fourier transform.", "Args": {"input": "A Tensor. Must be one of the following types: float32, float64.\nA float32 tensor.", "fft_length": "A Tensor of type int32.\nAn int32 tensor of shape [1]. The FFT length.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type Tcomplex."}, "tf.signal.rfft2d": {"description": "2D real-valued fast Fourier transform.", "Args": {"input": "A Tensor. Must be one of the following types: float32, float64.\nA float32 tensor.", "fft_length": "A Tensor of type int32.\nAn int32 tensor of shape [2]. The FFT length for each dimension.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type Tcomplex."}, "tf.signal.rfft3d": {"description": "3D real-valued fast Fourier transform.", "Args": {"input": "A Tensor. Must be one of the following types: float32, float64.\nA float32 tensor.", "fft_length": "A Tensor of type int32.\nAn int32 tensor of shape [3]. The FFT length for each dimension.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type Tcomplex."}, "tf.signal.stft": {"description": "Computes the [Short-time Fourier Transform][stft] of signals.", "Args": {"signals": "A [..., samples] float32/float64 Tensor of real-valued\nsignals.", "frame_length": "An integer scalar Tensor. The window length in samples.", "frame_step": "An integer scalar Tensor. The number of samples to step.", "fft_length": "An integer scalar Tensor. The size of the FFT to apply.\nIf not provided, uses the smallest power of 2 enclosing frame_length.", "window_fn": "A callable that takes a window length and a dtype keyword\nargument and returns a [window_length] Tensor of samples in the\nprovided datatype. If set to None, no windowing is used.", "pad_end": "Whether to pad the end of signals with zeros when the provided\nframe length and step produces a frame that lies partially past its end.", "name": "An optional name for the operation."}, "Returns": "A [..., frames, fft_unique_bins] Tensor of complex64/complex128\nSTFT values where fft_unique_bins is fft_length // 2 + 1 (the unique\ncomponents of the FFT).", "Raises": {"ValueError": "If signals is not at least rank 1, frame_length is\nnot scalar, or frame_step is not scalar."}}, "tf.signal.vorbis_window": {"description": "Generate a [Vorbis power complementary window][vorbis].", "Args": {"window_length": "A scalar Tensor indicating the window length to generate.", "dtype": "The data type to produce. Must be a floating point type.", "name": "An optional name for the operation."}, "Returns": "A Tensor of shape [window_length] of type dtype."}}, "tf.sparse": {"tf.sparse.SparseTensor": {"description": "Represents a sparse tensor.", "Args": {"indices": "A 2-D int64 tensor of shape [N, ndims].", "values": "A 1-D tensor of any type and shape [N].", "dense_shape": "A 1-D int64 tensor of shape [ndims]."}, "Raises": {"ValueError": "When building an eager SparseTensor if dense_shape is\nunknown or contains unknown elements (None or -1)."}, "Attributes": {"dense_shape": "A 1-D Tensor of int64 representing the shape of the dense tensor.", "dtype": "The DType of elements in this tensor.", "graph": "The Graph that contains the index, value, and dense_shape tensors.", "indices": "The indices of non-zero values in the represented dense tensor.", "op": "The Operation that produces values as an output.", "shape": "Get the TensorShape representing the shape of the dense tensor.", "values": "The non-zero values in the represented dense tensor."}}, "tf.sparse.add": {"description": "Adds two tensors, at least one of each is a SparseTensor.", "Args": {"a": "The first operand; SparseTensor or Tensor.", "b": "The second operand; SparseTensor or Tensor. At least one operand\nmust be sparse.", "threshold": "A 0-D Tensor. The magnitude threshold that determines if an\noutput value/index pair takes space. Its dtype should match that of the\nvalues if they are real; if the latter are complex64/complex128, then the\ndtype should be float32/float64, correspondingly."}, "Returns": "A SparseTensor or a Tensor, representing the sum.", "Raises": {"TypeError": "If both a and b are Tensors.  Use tf.add() instead."}}, "tf.sparse.bincount": {"description": "Count the number of times an integer value appears in a tensor.", "Args": {"values": "A Tensor, RaggedTensor, or SparseTensor whose values should be\ncounted. These tensors must have a rank of 2 if axis=-1.", "weights": "If non-None, must be the same shape as arr. For each value in\nvalue, the bin will be incremented by the corresponding weight instead\nof 1.", "axis": "The axis to slice over. Axes at and below axis will be flattened\nbefore bin counting. Currently, only 0, and -1 are supported. If None,\nall axes will be flattened (identical to passing 0).", "minlength": "If given, ensures the output has length at least minlength,\npadding with zeros at the end if necessary.", "maxlength": "If given, skips values in values that are equal or greater than\nmaxlength, ensuring that the output has length at most maxlength.", "binary_output": "If True, this op will output 1 instead of the number of times\na token appears (equivalent to one_hot + reduce_any instead of one_hot +\nreduce_add). Defaults to False.", "name": "A name for this op."}, "Returns": "A SparseTensor with output.shape = values.shape[:axis] + [N], where N is\n\nmaxlength (if set);\nminlength (if set, and minlength > reduce_max(values));\n0 (if values is empty);\nreduce_max(values) + 1 otherwise."}, "tf.sparse.concat": {"description": "Concatenates a list of SparseTensor along the specified dimension. (deprecated arguments)", "Args": {"axis": "Dimension to concatenate along. Must be in range [-rank, rank),\nwhere rank is the number of dimensions in each input SparseTensor.", "sp_inputs": "List of SparseTensor to concatenate.", "name": "A name prefix for the returned tensors (optional).", "expand_nonconcat_dim": "Whether to allow the expansion in the non-concat\ndimensions. Defaulted to False.", "concat_dim": "The old (deprecated) name for axis.", "expand_nonconcat_dims": "alias for expand_nonconcat_dim"}, "Returns": "A SparseTensor with the concatenated output.", "Raises": {"TypeError": "If sp_inputs is not a list of SparseTensor."}}, "tf.sparse.cross": {"description": "Generates sparse cross from a list of sparse and dense tensors.", "Args": {"inputs": "An iterable of Tensor or SparseTensor.", "name": "Optional name for the op.", "separator": "A string added between each string being joined. Defaults to\n'X'."}, "Returns": "A SparseTensor of type string."}, "tf.sparse.cross_hashed": {"description": "Generates hashed sparse cross from a list of sparse and dense tensors.", "Args": {"inputs": "An iterable of Tensor or SparseTensor.", "num_buckets": "An int that is >= 0.\noutput = hashed_value%num_buckets if num_buckets > 0 else hashed_value.", "hash_key": "Integer hash_key that will be used by the FingerprintCat64\nfunction. If not given, will use a default key.", "name": "Optional name for the op."}, "Returns": "A SparseTensor of type int64."}, "tf.sparse.expand_dims": {"description": "Returns a tensor with an length 1 axis inserted at index axis.", "Args": {"sp_input": "A SparseTensor.", "axis": "0-D (scalar). Specifies the dimension index at which to expand the\nshape of input. Must be in the range [-rank(sp_input) - 1,\nrank(sp_input)]. Defaults to -1.", "name": "The name of the output SparseTensor."}, "Returns": "A SparseTensor with the same data as sp_input, but its shape has an\nadditional dimension of size 1 added."}, "tf.sparse.eye": {"description": "Creates a two-dimensional sparse tensor with ones along the diagonal.", "Args": {"num_rows": "Non-negative integer or int32 scalar tensor giving the number\nof rows in the resulting matrix.", "num_columns": "Optional non-negative integer or int32 scalar tensor giving\nthe number of columns in the resulting matrix. Defaults to num_rows.", "dtype": "The type of element in the resulting Tensor.", "name": "A name for this Op. Defaults to \"eye\"."}, "Returns": "A SparseTensor of shape [num_rows, num_columns] with ones along the\ndiagonal."}, "tf.sparse.fill_empty_rows": {"description": "Fills empty rows in the input 2-D SparseTensor with a default value.", "Args": {"sp_input": "A SparseTensor with shape [N, M].", "default_value": "The value to fill for empty rows, with the same type as\nsp_input.", "name": "A name prefix for the returned tensors (optional)"}, "Returns": "sp_ordered_output\n\n\nA SparseTensor with shape [N, M], and with all empty\nrows filled in with default_value.", "Raises": {"TypeError": "If sp_input is not a SparseTensor."}}, "tf.sparse.from_dense": {"description": "Converts a dense tensor into a sparse tensor.", "Args": {"tensor": "A dense Tensor to be converted to a SparseTensor.", "name": "Optional name for the op."}, "Returns": "The SparseTensor."}, "tf.sparse.map_values": {"description": "Applies op to the .values tensor of one or more SparseTensors.", "Args": {"op": "The operation that should be applied to the SparseTensor values. op\nis typically an element-wise operation (such as math_ops.add), but any\noperation that preserves the shape can be used.", "*args": "Arguments for op.", "**kwargs": "Keyword arguments for op."}, "Returns": "A SparseTensor whose indices and dense_shape matches the indices\nand dense_shape of all input SparseTensors.", "Raises": {"ValueError": "If args contains no SparseTensor, or if the indices\nor dense_shapes of the input SparseTensors are not equal."}}, "tf.sparse.mask": {"description": "Masks elements of IndexedSlices.", "Args": {"a": "An IndexedSlices instance.", "mask_indices": "Indices of elements to mask.", "name": "A name for the operation (optional)."}, "Returns": "The masked IndexedSlices instance."}, "tf.sparse.maximum": {"description": "Returns the element-wise max of two SparseTensors.", "Args": {"sp_a": "a SparseTensor operand whose dtype is real, and indices\nlexicographically ordered.", "sp_b": "the other SparseTensor operand with the same requirements (and the\nsame shape).", "name": "optional name of the operation."}, "Returns": "output\n\n\nthe output SparseTensor."}, "tf.sparse.minimum": {"description": "Returns the element-wise min of two SparseTensors.", "Args": {"sp_a": "a SparseTensor operand whose dtype is real, and indices\nlexicographically ordered.", "sp_b": "the other SparseTensor operand with the same requirements (and the\nsame shape).", "name": "optional name of the operation."}, "Returns": "output\n\n\nthe output SparseTensor."}, "tf.sparse.reduce_max": {"description": "Computes tf.sparse.maximum of elements across dimensions of a SparseTensor.", "Args": {"sp_input": "The SparseTensor to reduce. Should have numeric type.", "axis": "The dimensions to reduce; list or scalar. If None (the\ndefault), reduces all dimensions.", "keepdims": "If true, retain reduced dimensions with length 1.", "output_is_sparse": "If true, returns a SparseTensor instead of a dense\nTensor (the default).", "name": "A name for the operation (optional)."}, "Returns": "The reduced Tensor or the reduced SparseTensor if output_is_sparse is\nTrue."}, "tf.sparse.reduce_sum": {"description": "Computes tf.sparse.add of elements across dimensions of a SparseTensor.", "Args": {"sp_input": "The SparseTensor to reduce. Should have numeric type.", "axis": "The dimensions to reduce; list or scalar. If None (the\ndefault), reduces all dimensions.", "keepdims": "If true, retain reduced dimensions with length 1.", "output_is_sparse": "If true, returns a SparseTensor instead of a dense\nTensor (the default).", "name": "A name for the operation (optional)."}, "Returns": "The reduced Tensor or the reduced SparseTensor if output_is_sparse is\nTrue."}, "tf.sparse.reorder": {"description": "Reorders a SparseTensor into the canonical, row-major ordering.", "Args": {"sp_input": "The input SparseTensor.", "name": "A name prefix for the returned tensors (optional)"}, "Returns": "A SparseTensor with the same shape and non-empty values, but in\ncanonical ordering.", "Raises": {"TypeError": "If sp_input is not a SparseTensor."}}, "tf.sparse.reset_shape": {"description": "Resets the shape of a SparseTensor with indices and values unchanged.", "Args": {"sp_input": "The input SparseTensor.", "new_shape": "None or a vector representing the new shape for the returned\nSparseTensor."}, "Returns": "A SparseTensor indices and values unchanged from sp_input. Its shape is\nnew_shape if that is set. Otherwise it is the tight bounding box of\n sp_input", "Raises": {"TypeError": "If sp_input is not a SparseTensor.", "ValueError": "If new_shape is determined during graph build to have\ndimension sizes that are too small.", "OpError": "If new_shape has dimension sizes that are too small.\nIf shapes are not known during graph construction time, and during run\ntime it is found out that the ranks do not match."}}, "tf.sparse.reshape": {"description": "Reshapes a SparseTensor to represent values in a new dense shape.", "Args": {"sp_input": "The input SparseTensor.", "shape": "A 1-D (vector) int64 Tensor specifying the new dense shape of the\nrepresented SparseTensor.", "name": "A name prefix for the returned tensors (optional)"}, "Returns": "A SparseTensor with the same non-empty values but with indices calculated\nby the new dense shape.", "Raises": {"TypeError": "If sp_input is not a SparseTensor.", "ValueError": "If shape has more than one inferred (== -1) dimension."}}, "tf.sparse.retain": {"description": "Retains specified non-empty values within a SparseTensor.", "Args": {"sp_input": "The input SparseTensor with N non-empty elements.", "to_retain": "A bool vector of length N with M true values."}, "Returns": "A SparseTensor with the same shape as the input and M non-empty\nelements corresponding to the true positions in to_retain.", "Raises": {"TypeError": "If sp_input is not a SparseTensor."}}, "tf.sparse.segment_mean": {"description": "Computes the mean along sparse segments of a tensor.", "Args": {"data": "A Tensor with data that will be assembled in the output.", "indices": "A 1-D Tensor with indices into data. Has same rank as\nsegment_ids.", "segment_ids": "A 1-D Tensor with indices into the output Tensor. Values\nshould be sorted and can be repeated.", "num_segments": "An optional int32 scalar. Indicates the size of the output\nTensor.", "name": "A name for the operation (optional)."}, "Returns": "A tensor of the shape as data, except for dimension 0 which\nhas size k, the number of segments specified via num_segments or\ninferred for the last element in segments_ids."}, "tf.sparse.segment_sqrt_n": {"description": "Computes the sum along sparse segments of a tensor divided by the sqrt(N).", "Args": {"data": "A Tensor with data that will be assembled in the output.", "indices": "A 1-D Tensor with indices into data. Has same rank as\nsegment_ids.", "segment_ids": "A 1-D Tensor with indices into the output Tensor. Values\nshould be sorted and can be repeated.", "num_segments": "An optional int32 scalar. Indicates the size of the output\nTensor.", "name": "A name for the operation (optional)."}, "Returns": "A tensor of the shape as data, except for dimension 0 which\nhas size k, the number of segments specified via num_segments or\ninferred for the last element in segments_ids."}, "tf.sparse.segment_sum": {"description": "Computes the sum along sparse segments of a tensor.", "Args": {"data": "A Tensor with data that will be assembled in the output.", "indices": "A 1-D Tensor with indices into data. Has same rank as\nsegment_ids.", "segment_ids": "A 1-D Tensor with indices into the output Tensor. Values\nshould be sorted and can be repeated.", "num_segments": "An optional int32 scalar. Indicates the size of the output\nTensor.", "name": "A name for the operation (optional)."}, "Returns": "A tensor of the shape as data, except for dimension 0 which\nhas size k, the number of segments specified via num_segments or\ninferred for the last element in segments_ids."}, "tf.sparse.slice": {"description": "Slice a SparseTensor based on the start and size.", "Args": {"sp_input": "The SparseTensor to split.", "start": "1-D. tensor represents the start of the slice.", "size": "1-D. tensor represents the size of the slice.", "name": "A name for the operation (optional)."}, "Returns": "A SparseTensor objects resulting from splicing.", "Raises": {"TypeError": "If sp_input is not a SparseTensor."}}, "tf.sparse.softmax": {"description": "Applies softmax to a batched N-D SparseTensor.", "Args": {"sp_input": "N-D SparseTensor, where N >= 2.", "name": "optional name of the operation."}, "Returns": "output\n\n\nN-D SparseTensor representing the results."}, "tf.sparse.sparse_dense_matmul": {"description": "Multiply SparseTensor (or dense Matrix) (of rank 2) &#34;A&#34; by dense matrix", "Args": {"sp_a": "SparseTensor (or dense Matrix) A, of rank 2.", "b": "dense Matrix (or SparseTensor) B, with the same dtype as sp_a.", "adjoint_a": "Use the adjoint of A in the matrix multiply.  If A is complex,\nthis is transpose(conj(A)).  Otherwise it's transpose(A).", "adjoint_b": "Use the adjoint of B in the matrix multiply.  If B is complex,\nthis is transpose(conj(B)).  Otherwise it's transpose(B).", "name": "A name prefix for the returned tensors (optional)"}, "Returns": "A dense matrix (pseudo-code in dense np.matrix notation):\nA = A.H if adjoint_a else A\nB = B.H if adjoint_b else B\nreturn A*B"}, "tf.sparse.split": {"description": "Split a SparseTensor into num_split tensors along axis.", "Args": {"sp_input": "The SparseTensor to split.", "num_split": "A Python integer. The number of ways to split.", "axis": "A 0-D int32 Tensor. The dimension along which to split. Must be in\nrange [-rank, rank), where rank is the number of dimensions in the input\nSparseTensor.", "name": "A name for the operation (optional)."}, "Returns": "num_split SparseTensor objects resulting from splitting value.", "Raises": {"TypeError": "If sp_input is not a SparseTensor."}}, "tf.sparse.to_dense": {"description": "Converts a SparseTensor into a dense tensor.", "Args": {"sp_input": "The input SparseTensor.", "default_value": "Scalar value to set for indices not specified in\nsp_input.  Defaults to zero.", "validate_indices": "A boolean value.  If True, indices are checked to make\nsure they are sorted in lexicographic order and that there are no repeats.", "name": "A name prefix for the returned tensors (optional)."}, "Returns": "A dense tensor with shape sp_input.dense_shape and values specified by\nthe non-empty values in sp_input. Indices not in sp_input are assigned\ndefault_value.", "Raises": {"TypeError": "If sp_input is not a SparseTensor."}}, "tf.sparse.to_indicator": {"description": "Converts a SparseTensor of ids into a dense bool indicator tensor.", "Args": {"sp_input": "A SparseTensor with values property of type int32 or\nint64.", "vocab_size": "A scalar int64 Tensor (or Python int) containing the new size\nof the last dimension, all(0 <= sp_input.values < vocab_size).", "name": "A name prefix for the returned tensors (optional)"}, "Returns": "A dense bool indicator tensor representing the indices with specified value.", "Raises": {"TypeError": "If sp_input is not a SparseTensor."}}, "tf.sparse.transpose": {"description": "Transposes a SparseTensor", "Args": {"sp_input": "The input SparseTensor.", "perm": "A permutation of the dimensions of sp_input.", "name": "A name prefix for the returned tensors (optional)"}, "Returns": "A transposed SparseTensor.", "Raises": {"TypeError": "If sp_input is not a SparseTensor."}}}, "tf.strings": {"tf.strings.as_string": {"description": "Converts each entry in the given tensor to strings.", "Args": {"input": "A Tensor. Must be one of the following types: float32, float64, int32, uint8, int16, int8, int64, bfloat16, uint16, half, uint32, uint64, complex64, complex128, bool, variant.", "precision": "An optional int. Defaults to -1.\nThe post-decimal precision to use for floating point numbers.\nOnly used if precision > -1.", "scientific": "An optional bool. Defaults to False.\nUse scientific notation for floating point numbers.", "shortest": "An optional bool. Defaults to False.\nUse shortest representation (either scientific or standard) for\nfloating point numbers.", "width": "An optional int. Defaults to -1.\nPad pre-decimal numbers to this width.\nApplies to both floating point and integer numbers.\nOnly used if width > -1.", "fill": "An optional string. Defaults to \"\".\nThe value to pad if width > -1.  If empty, pads with spaces.\nAnother typical value is '0'.  String cannot be longer than 1 character.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.strings.bytes_split": {"description": "Split string elements of input into bytes.", "Args": {"input": "A string Tensor or RaggedTensor: the strings to split.  Must\nhave a statically known rank (N).", "name": "A name for the operation (optional)."}, "Returns": "A RaggedTensor of rank N+1: the bytes that make up the source strings."}, "tf.strings.format": {"description": "Formats a string template using a list of tensors.", "Args": {"template": "A string template to format tensor values into.", "inputs": "A list of Tensor objects, or a single Tensor.\nThe list of tensors to format into the template string. If a solitary\ntensor is passed in, the input tensor will automatically be wrapped as a\nlist.", "placeholder": "An optional string. Defaults to {}.\nAt each placeholder occurring in the template, a subsequent tensor\nwill be inserted.", "summarize": "An optional int. Defaults to 3.\nWhen formatting the tensors, show the first and last summarize\nentries of each tensor dimension (recursively). If set to -1, all\nelements of the tensor will be shown.", "name": "A name for the operation (optional)."}, "Returns": "A scalar Tensor of type string.", "Raises": {"ValueError": "if the number of placeholders does not match the number of\ninputs."}}, "tf.strings.join": {"description": "Perform element-wise concatenation of a list of string tensors.", "Args": {"inputs": "A list of tf.Tensor objects of same size and tf.string dtype.", "separator": "A string added between each string being joined.", "name": "A name for the operation (optional)."}, "Returns": "A tf.string tensor."}, "tf.strings.length": {"description": "String lengths of input.", "Args": {"input": "A Tensor of type string.\nThe strings for which to compute the length for each element.", "unit": "An optional string from: \"BYTE\", \"UTF8_CHAR\". Defaults to \"BYTE\".\nThe unit that is counted to compute string length.  One of: \"BYTE\" (for\nthe number of bytes in each string) or \"UTF8_CHAR\" (for the number of UTF-8\nencoded Unicode code points in each string).  Results are undefined\nif unit=UTF8_CHAR and the input strings do not contain structurally\nvalid UTF-8.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int32."}, "tf.strings.lower": {"description": "Converts all uppercase characters into their respective lowercase replacements.", "Args": {"input": "A Tensor of type string. The input to be lower-cased.", "encoding": "An optional string. Defaults to \"\".\nCharacter encoding of input. Allowed values are '' and 'utf-8'.\nValue '' is interpreted as ASCII.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.strings.ngrams": {"description": "Create a tensor of n-grams based on data.", "Args": {"data": "A Tensor or RaggedTensor containing the source data for the ngrams.", "ngram_width": "The width(s) of the ngrams to create. If this is a list or\ntuple, the op will return ngrams of all specified arities in list order.\nValues must be non-Tensor integers greater than 0.", "separator": "The separator string used between ngram elements. Must be a\nstring constant, not a Tensor.", "pad_values": "A tuple of (left_pad_value, right_pad_value), a single string,\nor None. If None, no padding will be added; if a single string, then that\nstring will be used for both left and right padding. Values must be Python\nstrings.", "padding_width": "If set, padding_width pad values will be added to both\nsides of each sequence. Defaults to ngram_width-1. Must be greater than\n\n(Note that 1-grams are never padded, regardless of this value.)", "preserve_short_sequences": "If true, then ensure that at least one ngram is\ngenerated for each input sequence.  In particular, if an input sequence is\nshorter than min(ngram_width) + 2*pad_width, then generate a single\nngram containing the entire sequence.  If false, then no ngrams are\ngenerated for these short input sequences.", "name": "The op name."}, "Returns": "A RaggedTensor of ngrams. If data.shape=[D1...DN, S], then\noutput.shape=[D1...DN, NUM_NGRAMS], where\nNUM_NGRAMS=S-ngram_width+1+2*padding_width.", "Raises": {"TypeError": "if pad_values is set to an invalid type.", "ValueError": "if pad_values, padding_width, or ngram_width is set to an\ninvalid value."}}, "tf.strings.reduce_join": {"description": "Joins all strings into a single string, or joins along an axis.", "Args": {"inputs": "A tf.string tensor.", "axis": "Which axis to join along. The default behavior is to join all\nelements, producing a scalar.", "keepdims": "If true, retains reduced dimensions with length 1.", "separator": "a string added between each string being joined.", "name": "A name for the operation (optional)."}, "Returns": "A tf.string tensor."}, "tf.strings.regex_full_match": {"description": "Check if the input matches the regex pattern.", "Args": {"input": "A Tensor of type string.\nA string tensor of the text to be processed.", "pattern": "A Tensor of type string.\nA scalar string tensor containing the regular expression to match the input.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type bool."}, "tf.strings.regex_replace": {"description": "Replace elements of input matching regex pattern with rewrite.", "Args": {"input": "string Tensor, the source strings to process.", "pattern": "string or scalar string Tensor, regular expression to use,\nsee more details at https://github.com/google/re2/wiki/Syntax", "rewrite": "string or scalar string Tensor, value to use in match\nreplacement, supports backslash-escaped digits (\\1 to \\9) can be to insert\ntext matching corresponding parenthesized group.", "replace_global": "bool, if True replace all non-overlapping matches,\nelse replace only the first match.", "name": "A name for the operation (optional)."}, "Returns": "string Tensor of the same shape as input with specified replacements."}, "tf.strings.split": {"description": "Split elements of input based on sep into a RaggedTensor.", "Args": {"input": "A string Tensor of rank N, the strings to split.  If\nrank(input) is not known statically, then it is assumed to be 1.", "sep": "0-D string Tensor, the delimiter string.", "maxsplit": "An int. If maxsplit > 0, limit of the split of the result.", "name": "A name for the operation (optional)."}, "Raises": {"ValueError": "If sep is not a string."}, "Returns": "A RaggedTensor of rank N+1, the strings split according to the\ndelimiter."}, "tf.strings.strip": {"description": "Strip leading and trailing whitespaces from the Tensor.", "Args": {"input": "A Tensor of type string. A string Tensor of any shape.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.strings.substr": {"description": "Return substrings from Tensor of strings.", "Raises": {}, "Args": {"input": "A Tensor of type string. Tensor of strings", "pos": "A Tensor. Must be one of the following types: int32, int64.\nScalar defining the position of first character in each substring", "len": "A Tensor. Must have the same type as pos.\nScalar defining the number of characters to include in each substring", "unit": "An optional string from: \"BYTE\", \"UTF8_CHAR\". Defaults to \"BYTE\".\nThe unit that is used to create the substring.  One of: \"BYTE\" (for\ndefining position and length by bytes) or \"UTF8_CHAR\" (for the UTF-8\nencoded Unicode code points).  The default is \"BYTE\". Results are undefined if\nunit=UTF8_CHAR and the input strings do not contain structurally valid\nUTF-8.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.strings.to_hash_bucket": {"description": "Converts each string in the input Tensor to its hash mod by a number of buckets.", "Args": {"input": "A Tensor of type string.", "num_buckets": "An int that is >= 1. The number of buckets.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int64."}, "tf.strings.to_hash_bucket_fast": {"description": "Converts each string in the input Tensor to its hash mod by a number of buckets.", "Args": {"input": "A Tensor of type string. The strings to assign a hash bucket.", "num_buckets": "An int that is >= 1. The number of buckets.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int64."}, "tf.strings.to_hash_bucket_strong": {"description": "Converts each string in the input Tensor to its hash mod by a number of buckets.", "Args": {"input": "A Tensor of type string. The strings to assign a hash bucket.", "num_buckets": "An int that is >= 1. The number of buckets.", "key": "A list of ints.\nThe key used to seed the hash function, passed as a list of two uint64\nelements.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int64."}, "tf.strings.to_number": {"description": "Converts each string in the input Tensor to the specified numeric type.", "Args": {"input": "A Tensor of type string.", "out_type": "An optional tf.DType from: tf.float32, tf.float64, tf.int32,\ntf.int64. Defaults to tf.float32.\nThe numeric type to interpret each string in string_tensor as.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type out_type."}, "tf.strings.unicode_decode": {"description": "Decodes each string in input into a sequence of Unicode code points.", "Args": {"input": "An N dimensional potentially ragged string tensor with shape\n[D1...DN].  N must be statically known.", "input_encoding": "String name for the unicode encoding that should be used to\ndecode each string.", "errors": "Specifies the response when an input string can't be converted\nusing the indicated encoding. One of:\n\n'strict': Raise an exception for any illegal substrings.\n'replace': Replace illegal substrings with replacement_char.\n'ignore': Skip illegal substrings.", "replacement_char": "The replacement codepoint to be used in place of invalid\nsubstrings in input when errors='replace'; and in place of C0 control\ncharacters in input when replace_control_characters=True.", "replace_control_characters": "Whether to replace the C0 control characters\n(U+0000 - U+001F) with the replacement_char.", "name": "A name for the operation (optional)."}, "Returns": "A N+1 dimensional int32 tensor with shape [D1...DN, (num_chars)].\nThe returned tensor is a tf.Tensor if input is a scalar, or a\ntf.RaggedTensor otherwise."}, "tf.strings.unicode_decode_with_offsets": {"description": "Decodes each string into a sequence of code points with start offsets.", "Args": {"input": "An N dimensional potentially ragged string tensor with shape\n[D1...DN].  N must be statically known.", "input_encoding": "String name for the unicode encoding that should be used to\ndecode each string.", "errors": "Specifies the response when an input string can't be converted\nusing the indicated encoding. One of:\n\n'strict': Raise an exception for any illegal substrings.\n'replace': Replace illegal substrings with replacement_char.\n'ignore': Skip illegal substrings.", "replacement_char": "The replacement codepoint to be used in place of invalid\nsubstrings in input when errors='replace'; and in place of C0 control\ncharacters in input when replace_control_characters=True.", "replace_control_characters": "Whether to replace the C0 control characters\n(U+0000 - U+001F) with the replacement_char.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of N+1 dimensional tensors (codepoints, start_offsets).\n\ncodepoints is an int32 tensor with shape [D1...DN, (num_chars)].\noffsets is an int64 tensor with shape [D1...DN, (num_chars)].\n\nThe returned tensors are tf.Tensors if input is a scalar, or\ntf.RaggedTensors otherwise."}, "tf.strings.unicode_encode": {"description": "Encodes each sequence of Unicode code points in input into a string.", "Args": {"input": "An N+1 dimensional potentially ragged integer tensor with shape\n[D1...DN, num_chars].", "output_encoding": "Unicode encoding that should be used to encode each\ncodepoint sequence.  Can be \"UTF-8\", \"UTF-16-BE\", or \"UTF-32-BE\".", "errors": "Specifies the response when an invalid codepoint is encountered\n(optional). One of:\n\u00a0 * `'replace'`: Replace invalid codepoint with the\u00a0 \u00a0 `replacement_char`. (default)\u00a0 * `'ignore'`: Skip invalid codepoints.\u00a0 * `'strict'`: Raise an exception for any invalid codepoint.", "replacement_char": "The replacement character codepoint to be used in place of\nany invalid input when errors='replace'. Any valid unicode codepoint may\nbe used. The default value is the default unicode replacement character\nwhich is 0xFFFD (U+65533).", "name": "A name for the operation (optional)."}, "Returns": "A N dimensional string tensor with shape [D1...DN]."}, "tf.strings.unicode_script": {"description": "Determine the script codes of a given tensor of Unicode integer code points.", "Args": {"input": "A Tensor of type int32. A Tensor of int32 Unicode code points.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type int32."}, "tf.strings.unicode_split": {"description": "Splits each string in input into a sequence of Unicode code points.", "Args": {"input": "An N dimensional potentially ragged string tensor with shape\n[D1...DN].  N must be statically known.", "input_encoding": "String name for the unicode encoding that should be used to\ndecode each string.", "errors": "Specifies the response when an input string can't be converted\nusing the indicated encoding. One of:\n\n'strict': Raise an exception for any illegal substrings.\n'replace': Replace illegal substrings with replacement_char.\n'ignore': Skip illegal substrings.", "replacement_char": "The replacement codepoint to be used in place of invalid\nsubstrings in input when errors='replace'.", "name": "A name for the operation (optional)."}, "Returns": "A N+1 dimensional int32 tensor with shape [D1...DN, (num_chars)].\nThe returned tensor is a tf.Tensor if input is a scalar, or a\ntf.RaggedTensor otherwise."}, "tf.strings.unicode_split_with_offsets": {"description": "Splits each string into a sequence of code points with start offsets.", "Args": {"input": "An N dimensional potentially ragged string tensor with shape\n[D1...DN].  N must be statically known.", "input_encoding": "String name for the unicode encoding that should be used to\ndecode each string.", "errors": "Specifies the response when an input string can't be converted\nusing the indicated encoding. One of:\n\n'strict': Raise an exception for any illegal substrings.\n'replace': Replace illegal substrings with replacement_char.\n'ignore': Skip illegal substrings.", "replacement_char": "The replacement codepoint to be used in place of invalid\nsubstrings in input when errors='replace'.", "name": "A name for the operation (optional)."}, "Returns": "A tuple of N+1 dimensional tensors (codepoints, start_offsets).\n\ncodepoints is an int32 tensor with shape [D1...DN, (num_chars)].\noffsets is an int64 tensor with shape [D1...DN, (num_chars)].\n\nThe returned tensors are tf.Tensors if input is a scalar, or\ntf.RaggedTensors otherwise."}, "tf.strings.unicode_transcode": {"description": "Transcode the input text from a source encoding to a destination encoding.", "Args": {"input": "A Tensor of type string.\nThe text to be processed. Can have any shape.", "input_encoding": "A string.\nText encoding of the input strings. This is any of the encodings supported\nby ICU ucnv algorithmic converters. Examples: \"UTF-16\", \"US ASCII\", \"UTF-8\".", "output_encoding": "A string from: \"UTF-8\", \"UTF-16-BE\", \"UTF-32-BE\".\nThe unicode encoding to use in the output. Must be one of\n\"UTF-8\", \"UTF-16-BE\", \"UTF-32-BE\". Multi-byte encodings will be big-endian.", "errors": "An optional string from: \"strict\", \"replace\", \"ignore\". Defaults to \"replace\".\nError handling policy when there is invalid formatting found in the input.\nThe value of 'strict' will cause the operation to produce a InvalidArgument\nerror on any invalid input formatting. A value of 'replace' (the default) will\ncause the operation to replace any invalid formatting in the input with the\nreplacement_char codepoint. A value of 'ignore' will cause the operation to\nskip any invalid formatting in the input and produce no corresponding output\ncharacter.", "replacement_char": "An optional int. Defaults to 65533.\nThe replacement character codepoint to be used in place of any invalid\nformatting in the input when errors='replace'. Any valid unicode codepoint may\nbe used. The default value is the default unicode replacement character is\n0xFFFD or U+65533.)\nNote that for UTF-8, passing a replacement character expressible in 1 byte, such\nas ' ', will preserve string alignment to the source since invalid bytes will be\nreplaced with a 1-byte replacement. For UTF-16-BE and UTF-16-LE, any 1 or 2 byte\nreplacement character will preserve byte alignment to the source.", "replace_control_characters": "An optional bool. Defaults to False.\nWhether to replace the C0 control characters (00-1F) with the\nreplacement_char. Default is false.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.strings.unsorted_segment_join": {"description": "Joins the elements of inputs based on segment_ids.", "Args": {"inputs": "A Tensor of type string. The input to be joined.", "segment_ids": "A Tensor. Must be one of the following types: int32, int64.\nA tensor whose shape is a prefix of data.shape.  Negative segment ids are not\nsupported.", "num_segments": "A Tensor. Must be one of the following types: int32, int64.\nA scalar.", "separator": "An optional string. Defaults to \"\".\nThe separator to use when joining.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}, "tf.strings.upper": {"description": "Converts all lowercase characters into their respective uppercase replacements.", "Args": {"input": "A Tensor of type string. The input to be upper-cased.", "encoding": "An optional string. Defaults to \"\".\nCharacter encoding of input. Allowed values are '' and 'utf-8'.\nValue '' is interpreted as ASCII.", "name": "A name for the operation (optional)."}, "Returns": "A Tensor of type string."}}, "tf.summary": {"tf.summary.SummaryWriter": {"description": "Interface representing a stateful summary writer object."}, "tf.summary.audio": {"description": "Write an audio summary.", "Returns": "True on success, or false if no summary was emitted because no default\nsummary writer was available.", "Raises": {"ValueError": "if a default writer exists, but no step was provided and\ntf.summary.experimental.get_step() is None."}}, "tf.summary.create_file_writer": {"description": "Creates a summary file writer for the given log directory.", "Args": {"logdir": "a string specifying the directory in which to write an event file.", "max_queue": "the largest number of summaries to keep in a queue; will\nflush once the queue gets bigger than this. Defaults to 10.", "flush_millis": "the largest interval between flushes. Defaults to 120,000.", "filename_suffix": "optional suffix for the event file name. Defaults to .v2.", "name": "a name for the op that creates the writer.", "experimental_trackable": "a boolean that controls whether the returned writer\nwill be a TrackableResource, which makes it compatible with SavedModel\nwhen used as a tf.Module property."}, "Returns": "A SummaryWriter object."}, "tf.summary.create_noop_writer": {"description": "Returns a summary writer that does nothing."}, "tf.summary.flush": {"description": "Forces summary writer to send any buffered data to storage.", "Args": {"writer": "The tf.summary.SummaryWriter to flush. If None, the current\ndefault writer will be used instead; if there is no current writer, this\nreturns tf.no_op.", "name": "Ignored legacy argument for a name for the operation."}, "Returns": "The created tf.Operation."}, "tf.summary.graph": {"description": "Writes a TensorFlow graph summary.", "Args": {"graph_data": "The TensorFlow graph to write, as a tf.Graph or a\ntf.compat.v1.GraphDef."}, "Returns": "True on success, or False if no summary was written because no default\nsummary writer was available.", "Raises": {"ValueError": "graph summary API is invoked in a graph mode."}}, "tf.summary.histogram": {"description": "Write a histogram summary.", "Returns": "True on success, or false if no summary was emitted because no default\nsummary writer was available.", "Raises": {"ValueError": "if a default writer exists, but no step was provided and\ntf.summary.experimental.get_step() is None."}}, "tf.summary.image": {"description": "Write an image summary.", "Returns": "True on success, or false if no summary was emitted because no default\nsummary writer was available.", "Raises": {"ValueError": "if a default writer exists, but no step was provided and\ntf.summary.experimental.get_step() is None."}}, "tf.summary.record_if": {"description": "Sets summary recording on or off per the provided boolean value.", "Args": {"condition": "can be True, False, a bool Tensor, or a callable providing such."}}, "tf.summary.scalar": {"description": "Write a scalar summary.", "Returns": "True on success, or false if no summary was written because no default\nsummary writer was available.", "Raises": {"ValueError": "if a default writer exists, but no step was provided and\ntf.summary.experimental.get_step() is None."}}, "tf.summary.should_record_summaries": {"description": "Returns boolean Tensor which is True if summaries will be recorded."}, "tf.summary.text": {"description": "Write a text summary.", "Returns": "True on success, or false if no summary was emitted because no default\nsummary writer was available.", "Raises": {"ValueError": "if a default writer exists, but no step was provided and\ntf.summary.experimental.get_step() is None."}}, "tf.summary.trace_export": {"description": "Stops and exports the active trace as a Summary and/or profile file.", "Args": {"name": "A name for the summary to be written.", "step": "Explicit int64-castable monotonic step value for this summary. If\nomitted, this defaults to tf.summary.experimental.get_step(), which must\nnot be None.", "profiler_outdir": "Output directory for profiler. It is required when profiler\nis enabled when trace was started. Otherwise, it is ignored."}, "Raises": {"ValueError": "if a default writer exists, but no step was provided and\ntf.summary.experimental.get_step() is None."}}, "tf.summary.trace_off": {"description": "Stops the current trace and discards any collected information."}, "tf.summary.trace_on": {"description": "Starts a trace to record computation graphs and profiling information.", "Args": {"graph": "If True, enables collection of executed graphs. It includes ones from\ntf.function invocation and ones from the legacy graph mode. The default\nis True.", "profiler": "If True, enables the advanced profiler. Enabling profiler\nimplicitly enables the graph collection. The profiler may incur a high\nmemory overhead. The default is False."}}, "tf.summary.write": {"description": "Writes a generic summary to the default SummaryWriter if one exists.", "Args": {"tag": "string tag used to identify the summary (e.g. in TensorBoard), usually\ngenerated with tf.summary.summary_scope", "tensor": "the Tensor holding the summary data to write or a callable that\nreturns this Tensor. If a callable is passed, it will only be called when\na default SummaryWriter exists and the recording condition specified by\nrecord_if() is met.", "step": "Explicit int64-castable monotonic step value for this summary. If\nomitted, this defaults to tf.summary.experimental.get_step(), which must\nnot be None.", "metadata": "Optional SummaryMetadata, as a proto or serialized bytes", "name": "Optional string name for this op."}, "Returns": "True on success, or false if no summary was written because no default\nsummary writer was available.", "Raises": {"ValueError": "if a default writer exists, but no step was provided and\ntf.summary.experimental.get_step() is None."}}}, "tf.sysconfig": {"tf.sysconfig.get_build_info": {"description": "Get a dictionary describing TensorFlow&#39;s build environment.", "Returns": "A Dictionary describing TensorFlow's build environment."}, "tf.sysconfig.get_compile_flags": {"description": "Get the compilation flags for custom operators.", "Returns": "The compilation flags."}, "tf.sysconfig.get_include": {"description": "Get the directory containing the TensorFlow C&#43;&#43; header files.", "Returns": "The directory as string."}, "tf.sysconfig.get_lib": {"description": "Get the directory containing the TensorFlow framework library.", "Returns": "The directory as string."}, "tf.sysconfig.get_link_flags": {"description": "Get the link flags for custom operators.", "Returns": "The link flags."}}, "tf.test": {"tf.test.Benchmark": {"description": "Abstract class that provides helpers for TensorFlow benchmarks."}, "tf.test.TestCase": {"description": "Base class for tests that need to test TensorFlow.", "Class Variables": {"longMessage": "True", "maxDiff": "1600", "tempfile_cleanup": "<TempFileCleanup.ALWAYS: 'always'>"}}, "tf.test.TestCase.failureException": {"description": "Assertion failed."}, "tf.test.assert_equal_graph_def": {"description": "Asserts that two GraphDefs are (mostly) the same.", "Args": {"expected": "The GraphDef we expected.", "actual": "The GraphDef we have."}, "Raises": {"AssertionError": "If the GraphDefs do not match.", "TypeError": "If either argument is not a GraphDef."}}, "tf.test.benchmark_config": {"description": "Returns a tf.compat.v1.ConfigProto for disabling the dependency optimizer.", "Returns": "A TensorFlow ConfigProto object."}, "tf.test.compute_gradient": {"description": "Computes the theoretical and numeric Jacobian of f.", "Args": {"f": "the function.", "x": "the arguments for the function as a list or tuple of values convertible\nto a Tensor.", "delta": "(optional) perturbation used to compute numeric Jacobian."}, "Returns": "A pair of lists, where the first is a list of 2-d numpy arrays representing\nthe theoretical Jacobians for each argument, and the second list is the\nnumerical ones. Each 2-d array has \"y_size\" rows\nand \"x_size\" columns where \"x_size\" is the number of elements in the\ncorresponding argument and \"y_size\" is the number of elements in f(x).", "Raises": {"ValueError": "If x is not list, but any other type."}}, "tf.test.create_local_cluster": {"description": "Create and start local servers and return the associated Server objects.", "Args": {"num_workers": "Number of worker servers to start.", "num_ps": "Number of PS servers to start.", "protocol": "Communication protocol. Allowed values are documented in the\ndocumentation of tf.distribute.Server.", "worker_config": "(optional) tf.ConfigProto to initialize workers. Can be\nused to instantiate multiple devices etc.", "ps_config": "(optional) tf.ConfigProto to initialize PS servers."}, "Returns": "A tuple (worker_servers, ps_servers).  worker_servers is a list\nof num_workers objects of type tf.distribute.Server (all running\nlocally);\nand ps_servers is a list of num_ps objects of similar type.", "Raises": {"ImportError": "if portpicker module was not found at load time"}}, "tf.test.disable_with_predicate": {"description": "Disables the test if pred is true."}, "tf.test.gpu_device_name": {"description": "Returns the name of a GPU device if available or a empty string."}, "tf.test.is_built_with_cuda": {"description": "Returns whether TensorFlow was built with CUDA (GPU) support."}, "tf.test.is_built_with_gpu_support": {"description": "Returns whether TensorFlow was built with GPU (CUDA or ROCm) support."}, "tf.test.is_built_with_rocm": {"description": "Returns whether TensorFlow was built with ROCm (GPU) support."}, "tf.test.is_built_with_xla": {"description": "Returns whether TensorFlow was built with XLA support."}, "tf.test.is_gpu_available": {"description": "Returns whether TensorFlow can access a GPU. (deprecated)", "Args": {"cuda_only": "limit the search to CUDA GPUs.", "min_cuda_compute_capability": "a (major,minor) pair that indicates the minimum\nCUDA compute capability required, or None if no requirement."}, "Returns": "True if a GPU device of the requested kind is available."}, "tf.test.main": {"description": "Runs all unit tests."}, "tf.test.with_eager_op_as_function": {"description": "Adds methods that call original methods with eager_op_as_function enabled.", "Args": {"cls": "class to decorate.", "only_as_function": "whether to run all the tests in the TestCase in eager mode\nand in eager_op_as_function mode. By default it will run all tests in both\nmodes. When only_as_function=True tests will not be run in eager mode."}, "Returns": "cls with new test methods added."}}, "tf.tpu": {"tf.tpu.XLAOptions": {"description": "XLA compilation options.", "Attributes": {"use_spmd_for_xla_partitioning": "Boolean. Whether to use XLA's SPMD\npartitioner instead of MPMD partitioner when compiler partitioning is\nrequested.", "enable_xla_dynamic_padder": "Boolean. Whether to enable XLA dynamic padder\ninfrastructure to handle dynamic shapes inputs inside XLA. True by\ndefault. Disabling this may cause correctness issues with dynamic shapes\ninputs, as XLA will just assume the inputs are with padded shapes. However\nusers can optionally set it to False to improve device time if masking is\nalready handled in the user side."}}}, "tf.tpu.experimental.embedding": {"tf.tpu.experimental.embedding.Adagrad": {"description": "Optimization parameters for Adagrad with TPU embeddings.", "Args": {"learning_rate": "The learning rate. It should be a floating point value or a\ncallable taking no arguments for a dynamic learning rate.", "initial_accumulator_value": "initial accumulator for Adagrad.", "use_gradient_accumulation": "setting this to False makes embedding\ngradients calculation less accurate but faster.", "clip_weight_min": "the minimum value to clip by; None means -infinity.", "clip_weight_max": "the maximum value to clip by; None means +infinity.", "weight_decay_factor": "amount of weight decay to apply; None means that the\nweights are not decayed.", "multiply_weight_decay_factor_by_learning_rate": "if true,\nweight_decay_factor is multiplied by the current learning rate.", "slot_variable_creation_fn": "If you wish do directly control the creation of\nthe slot variables, set this to a callable taking three parameters: a\n  table variable, a list of slot names to create for it, and a list of\n  initializers. This function should return a dict with the slot names\n  as keys and the created variables as values with types matching the\n  table variable. When set to None (the default), uses the built-in\n  variable creation.", "clipvalue": "Controls clipping of the gradient. Set to either a single\npositive scalar value to get clipping or a tuple of scalar values (min,\nmax) to set a separate maximum or minimum. If one of the two entries is\nNone, then there will be no clipping that direction."}}, "tf.tpu.experimental.embedding.AdagradMomentum": {"description": "Optimization parameters for Adagrad &#43; Momentum with TPU embeddings.", "Args": {"learning_rate": "The learning rate. It should be a floating point value or a\ncallable taking no arguments for a dynamic learning rate.", "momentum": "Moving average parameter for the momentum accumulator.", "use_nesterov": "Whether to use the Nesterov variant of momentum. See\nSutskever et al., 2013.", "exponent": "Exponent for the Adagrad accumulator.", "beta2": "Moving average parameter for the Adagrad accumulator.", "epsilon": "initial accumulator for Adagrad accumulator.", "use_gradient_accumulation": "setting this to False makes embedding\ngradients calculation less accurate but faster.", "clip_weight_min": "the minimum value to clip by; None means -infinity.", "clip_weight_max": "the maximum value to clip by; None means +infinity.", "weight_decay_factor": "amount of weight decay to apply; None means that the\nweights are not decayed.", "multiply_weight_decay_factor_by_learning_rate": "if true,\nweight_decay_factor is multiplied by the current learning rate.", "slot_variable_creation_fn": "If you wish do directly control the creation of\nthe slot variables, set this to a callable taking three parameters: a\n  table variable, a list of slot names to create for it, and a list of\n  initializers. This function should return a dict with the slot names\n  as keys and the created variables as values with types matching the\n  table variable. When set to None (the default), uses the built-in\n  variable creation.", "clipvalue": "Controls clipping of the gradient. Set to either a single\npositive scalar value to get clipping or a tuple of scalar values (min,\nmax) to set a separate maximum or minimum. If one of the two entries is\nNone, then there will be no clipping that direction."}}, "tf.tpu.experimental.embedding.Adam": {"description": "Optimization parameters for Adam with TPU embeddings.", "Args": {"learning_rate": "The learning rate. It should be a floating point value or a\ncallable taking no arguments for a dynamic learning rate.", "beta_1": "A float value. The exponential decay rate for the 1st moment\nestimates.", "beta_2": "A float value. The exponential decay rate for the 2nd moment\nestimates.", "epsilon": "A small constant for numerical stability.", "lazy_adam": "Use lazy Adam instead of Adam. Lazy Adam trains faster.", "sum_inside_sqrt": "When this is true, the Adam update formula is changed\nfrom m / (sqrt(v) + epsilon) to m / sqrt(v + epsilon**2). This\noption improves the performance of TPU training and is not expected to\nharm model quality.", "use_gradient_accumulation": "Setting this to False makes embedding\ngradients calculation less accurate but faster.", "clip_weight_min": "the minimum value to clip by; None means -infinity.", "clip_weight_max": "the maximum value to clip by; None means +infinity.", "weight_decay_factor": "amount of weight decay to apply; None means that the\nweights are not decayed.", "multiply_weight_decay_factor_by_learning_rate": "if true,\nweight_decay_factor is multiplied by the current learning rate.", "slot_variable_creation_fn": "If you wish do directly control the creation of\nthe slot variables, set this to a callable taking three parameters: a\n  table variable, a list of slot names to create for it, and a list of\n  initializers. This function should return a dict with the slot names\n  as keys and the created variables as values with types matching the\n  table variable. When set to None (the default), uses the built-in\n  variable creation.", "clipvalue": "Controls clipping of the gradient. Set to either a single\npositive scalar value to get clipping or a tiple of scalar values (min,\nmax) to set a separate maximum or minimum. If one of the two entries is\nNone, then there will be no clipping that direction."}}, "tf.tpu.experimental.embedding.FTRL": {"description": "Optimization parameters for FTRL with TPU embeddings.", "Args": {"learning_rate": "The learning rate. It should be a floating point value or a\ncallable taking no arguments for a dynamic learning rate.", "learning_rate_power": "A float value, must be less or equal to zero.\nControls how the learning rate decreases during training. Use zero for a\nfixed learning rate.", "l1_regularization_strength": "A float value, must be greater than or equal\nto zero.", "l2_regularization_strength": "A float value, must be greater than or equal\nto zero.", "beta": "A float value, representing the beta value from the paper.", "initial_accumulator_value": "The starting value for accumulators. Only zero\nor positive values are allowed.", "use_gradient_accumulation": "setting this to False makes embedding\ngradients calculation less accurate but faster.", "clip_weight_min": "the minimum value to clip by; None means -infinity.", "clip_weight_max": "the maximum value to clip by; None means +infinity.", "weight_decay_factor": "amount of weight decay to apply; None means that the\nweights are not decayed.", "multiply_weight_decay_factor_by_learning_rate": "if true,\nweight_decay_factor is multiplied by the current learning rate.", "slot_variable_creation_fn": "If you wish do directly control the creation of\nthe slot variables, set this to a callable taking three parameters: a\n  table variable, a list of slot names to create for it, and a list of\n  initializers. This function should return a dict with the slot names\n  as keys and the created variables as values with types matching the\n  table variable. When set to None (the default), uses the built-in\n  variable creation.", "clipvalue": "Controls clipping of the gradient. Set to either a single\npositive scalar value to get clipping or a tuple of scalar values (min,\nmax) to set a separate maximum or minimum. If one of the two entries is\nNone, then there will be no clipping that direction.", "multiply_linear_by_learning_rate": "If set to True, a modified formula is\nused for FTRL that treats the \"linear\" accumulator as being\npre-multiplied by the learning rate (i.e., the accumulator named\n\"linear\" actually stores \"linear * learning_rate\"). Other than\ncheckpoint compatibility, this is mathematically equivalent for a static\nlearning rate; for a dynamic learning rate, it is nearly the same as\nlong as the learning rate does not change quickly. The benefit of this\nis that the modified formula handles zero and near-zero learning rates\nwithout producing NaNs, improving flexibility for learning rate ramp-up.", "allow_zero_accumulator": "If set to True, changes some internal formulas to\nallow zero and near-zero accumulator values at the cost of some\nperformance; this only needs to be set if you are using an initial\naccumulator value of zero, which is uncommon."}}, "tf.tpu.experimental.embedding.FeatureConfig": {"description": "Configuration data for one embedding feature.", "Args": {"table": "An instance of tf.tpu.experimental.embedding.TableConfig,\ndescribing the table in which this feature should be looked up.", "max_sequence_length": "If positive, the feature is a sequence feature with\nthe corresponding maximum sequence length. If the sequence is longer\nthan this, it will be truncated. If 0, the feature is not a sequence\nfeature.", "validate_weights_and_indices": "If true, uses safe_embedding_lookup during\nserving which ensures there are no empty rows and all weights and ids\nare positive at the expense of extra compute cost.", "output_shape": "Optional argument to config the output shape of the feature\nactivation. If provided, the feature feeding to the embedding.enqueue\nhas to match the shape (for ragged tensor, the input shape and output\nshape can mismatch). If not provided, the shape can be either provided\nto the embedding.build or auto detected at the runtime.", "name": "An optional name for the feature, useful for debugging."}, "Raises": {"ValueError": "if max_sequence_length not an integer or is negative."}}, "tf.tpu.experimental.embedding.SGD": {"description": "Optimization parameters for stochastic gradient descent for TPU embeddings.", "Args": {"learning_rate": "The learning rate. It should be a floating point value or a\ncallable taking no arguments for a dynamic learning rate.", "use_gradient_accumulation": "setting this to False makes embedding\ngradients calculation less accurate but faster.", "clip_weight_min": "the minimum value to clip by; None means -infinity.", "clip_weight_max": "the maximum value to clip by; None means +infinity.", "weight_decay_factor": "amount of weight decay to apply; None means that the\nweights are not decayed. Weights are decayed by multiplying the weight\nby this factor each step.", "multiply_weight_decay_factor_by_learning_rate": "if true,\nweight_decay_factor is multiplied by the current learning rate.", "clipvalue": "Controls clipping of the gradient. Set to either a single\npositive scalar value to get clipping or a tiple of scalar values (min,\nmax) to set a separate maximum or minimum. If one of the two entries is\nNone, then there will be no clipping that direction. Note if this is\nset, you may see a decrease in performance as  gradient accumulation\nwill be enabled (it is normally off for SGD as it has no affect on\naccuracy). See\n'tensorflow/core/protobuf/tpu/optimization_parameters.proto' for more\ninformation on gradient accumulation and its impact on tpu embeddings."}}, "tf.tpu.experimental.embedding.TPUEmbedding": {"description": "The TPUEmbedding mid level API.", "Args": {"feature_config": "A nested structure of\ntf.tpu.experimental.embedding.FeatureConfig configs.", "optimizer": "An instance of one of tf.tpu.experimental.embedding.SGD,\ntf.tpu.experimental.embedding.Adagrad or\ntf.tpu.experimental.embedding.Adam. When not created under\nTPUStrategy may be set to None to avoid the creation of the optimizer\nslot variables, useful for optimizing memory consumption when exporting\nthe model for serving where slot variables aren't needed.", "pipeline_execution_with_tensor_core": "If True, the TPU embedding\ncomputations will overlap with the TensorCore computations (and hence\nwill be one step old). Set to True for improved performance."}, "Raises": {"ValueError": "If optimizer is not one of tf.tpu.experimental.embedding.(SGD,\nAdam or Adagrad) or None when created under a TPUStrategy."}, "Attributes": {"embedding_tables": "Returns a dict of embedding tables, keyed by TableConfig.\nThis property only works when the TPUEmbedding object is created under a\nnon-TPU strategy. This is intended to be used to for CPU based lookup when\ncreating a serving checkpoint."}}, "tf.tpu.experimental.embedding.TPUEmbeddingForServing": {"description": "The TPUEmbedding mid level API running on CPU for serving.", "Args": {"feature_config": "A nested structure of\ntf.tpu.experimental.embedding.FeatureConfig configs.", "optimizer": "An instance of one of tf.tpu.experimental.embedding.SGD,\ntf.tpu.experimental.embedding.Adagrad or\ntf.tpu.experimental.embedding.Adam. When not created under TPUStrategy\nmay be set to None to avoid the creation of the optimizer slot\nvariables, useful for optimizing memory consumption when exporting the\nmodel for serving where slot variables aren't needed."}, "Raises": {"RuntimeError": "If created under TPUStrategy."}, "Attributes": {"embedding_tables": "Returns a dict of embedding tables, keyed by TableConfig."}}, "tf.tpu.experimental.embedding.TPUEmbeddingV0": {"description": "The TPUEmbedding mid level API running on TPU without Embedding accelerator.", "Attributes": {"embedding_tables": "Returns a dict of embedding tables, keyed by TableConfig."}}, "tf.tpu.experimental.embedding.TableConfig": {"description": "Configuration data for one embedding table.", "Args": {"vocabulary_size": "Size of the table's vocabulary (number of rows).", "dim": "The embedding dimension (width) of the table.", "initializer": "A callable initializer taking one parameter, the shape of the\nvariable that will be initialized. Will be called once per task, to\ninitialize that task's shard of the embedding table. If not specified,\ndefaults to truncated_normal_initializer with mean 0.0 and standard\ndeviation 1/sqrt(dim).", "optimizer": "An optional instance of an optimizer parameters class, instance\nof one of tf.tpu.experimental.embedding.SGD,\ntf.tpu.experimental.embedding.Adagrad or\ntf.tpu.experimental.embedding.Adam. It set will override the global\noptimizer passed to tf.tpu.experimental.embedding.TPUEmbedding.", "combiner": "A string specifying how to reduce if there are multiple entries\nin a single row. Currently 'mean', 'sqrtn', 'sum' are supported, with\n'mean' the default. 'sqrtn' often achieves good accuracy, in particular\nwith bag-of-words columns. For more information, see\ntf.nn.embedding_lookup_sparse.", "name": "An optional string used to name the table. Useful for debugging."}, "Raises": {"ValueError": "if combiner is not supported."}}, "tf.tpu.experimental.embedding.serving_embedding_lookup": {"description": "Apply standard lookup ops with tf.tpu.experimental.embedding configs.", "Args": {"inputs": "a nested structure of Tensors, SparseTensors or RaggedTensors.", "weights": "a nested structure of Tensors, SparseTensors or RaggedTensors or\nNone for no weights. If not None, structure must match that of inputs, but\nentries are allowed to be None.", "tables": "a dict of mapping TableConfig objects to Variables.", "feature_config": "a nested structure of FeatureConfig objects with the same\nstructure as inputs."}, "Returns": "A nested structure of Tensors with the same structure as inputs."}}, "tf.train": {"tf.train.BytesList": {"description": "Used in tf.train.Example protos. Holds a list of byte-strings.", "Attributes": {"value": "repeated bytes value"}}, "tf.train.Checkpoint": {"description": "Manages saving/restoring trackable values to disk.", "Args": {"root": "The root object to checkpoint. root may be a trackable object or\nWeakRef of a trackable object.", "**kwargs": "Keyword arguments are set as attributes of this object, and are\nsaved with the checkpoint. All kwargs must be trackable objects, or a\nnested structure of trackable objects (list, dict, or tuple)."}, "Raises": {"ValueError": "If root or the objects in kwargs are not trackable. A\nValueError is also raised if the root object tracks different\nobjects from the ones listed in attributes in kwargs (e.g.\nroot.child = A and tf.train.Checkpoint(root, child=B) are\nincompatible)."}, "Attributes": {"save_counter": "Incremented when save() is called. Used to number\ncheckpoints."}}, "tf.train.CheckpointManager": {"description": "Manages multiple checkpoints by keeping some and deleting unneeded ones.", "Args": {"checkpoint": "The tf.train.Checkpoint instance to save and manage\ncheckpoints for.", "directory": "The path to a directory in which to write checkpoints. A\nspecial file named \"checkpoint\" is also written to this directory (in a\nhuman-readable text format) which contains the state of the\nCheckpointManager.", "max_to_keep": "An integer, the number of checkpoints to keep. Unless\npreserved by keep_checkpoint_every_n_hours, checkpoints will be\ndeleted from the active set, oldest first, until only max_to_keep\ncheckpoints remain. If None, no checkpoints are deleted and everything\nstays in the active set. Note that max_to_keep=None will keep all\ncheckpoint paths in memory and in the checkpoint state protocol buffer\non disk.", "keep_checkpoint_every_n_hours": "Upon removal from the active set, a\ncheckpoint will be preserved if it has been at least\nkeep_checkpoint_every_n_hours since the last preserved checkpoint. The\ndefault setting of None does not preserve any checkpoints in this way.", "checkpoint_name": "Custom name for the checkpoint file.", "step_counter": "A tf.Variable instance for checking the current step\ncounter value, in case users want to save checkpoints every N steps.", "checkpoint_interval": "An integer, indicates the minimum step interval\nbetween two checkpoints.", "init_fn": "Callable. A function to do customized intialization if no\ncheckpoints are in the directory."}, "Raises": {"ValueError": "If max_to_keep is not a positive integer."}, "Attributes": {"checkpoint": "Returns the tf.train.Checkpoint object.", "checkpoint_interval": "", "checkpoints": "A list of managed checkpoints.\nNote that checkpoints saved due to keep_checkpoint_every_n_hours will not\nshow up in this list (to avoid ever-growing filename lists).", "directory": "", "latest_checkpoint": "The prefix of the most recent checkpoint in directory.\nEquivalent to tf.train.latest_checkpoint(directory) where directory is\nthe constructor argument to CheckpointManager.\nSuitable for passing to tf.train.Checkpoint.restore to resume training."}}, "tf.train.CheckpointOptions": {"description": "Options for constructing a Checkpoint.", "Args": {"experimental_io_device": "string. Applies in a distributed setting.\nTensorflow device to use to access the filesystem. If None (default)\nthen for each variable the filesystem is accessed from the CPU:0 device\nof the host where that variable is assigned. If specified, the\nfilesystem is instead accessed from that device for all variables.\nThis is for example useful if you want to save to a local directory,\nsuch as \"/tmp\" when running in a distributed setting. In that case pass\na device for the host where the \"/tmp\" directory is accessible.", "experimental_enable_async_checkpoint": "bool Type. Indicates whether async\ncheckpoint is enabled. Default is False, i.e., no async checkpoint.\nAsync checkpoint moves the checkpoint file writing off the main thread,\nso that the model can continue to train while the checkpoing file\nwriting runs in the background. Async checkpoint reduces TPU device idle\ncycles and speeds up model training process, while memory consumption\nmay increase."}, "Attributes": {"experimental_enable_async_checkpoint": "", "experimental_io_device": ""}}, "tf.train.ClusterDef": {"description": "A ProtocolMessage"}, "tf.train.ClusterSpec": {"description": "Represents a cluster as a set of &#34;tasks&#34;, organized into &#34;jobs&#34;.", "Args": {"cluster": "A dictionary mapping one or more job names to (i) a list of\nnetwork addresses, or (ii) a dictionary mapping integer task indices to\nnetwork addresses; or a tf.train.ClusterDef protocol buffer."}, "Raises": {"TypeError": "If cluster is not a dictionary mapping strings to lists\nof strings, and not a tf.train.ClusterDef protobuf."}, "Attributes": {"jobs": "Returns a list of job names in this cluster."}}, "tf.train.Coordinator": {"description": "A coordinator for threads.", "Args": {"clean_stop_exception_types": "Optional tuple of Exception types that should\ncause a clean stop of the coordinator. If an exception of one of these\ntypes is reported to request_stop(ex) the coordinator will behave as\nif request_stop(None) was called.  Defaults to\n(tf.errors.OutOfRangeError,) which is used by input queues to signal\nthe end of input. When feeding training data from a Python iterator it\nis common to add StopIteration to this list."}, "Attributes": {"joined": ""}}, "tf.train.Example": {"description": "An Example is a standard proto storing data for training and inference.", "Attributes": {"features": "Features features"}}, "tf.train.ExponentialMovingAverage": {"description": "Maintains moving averages of variables by employing an exponential decay.", "Args": {"decay": "A scalar float value, Tensor, or Variable. The decay parameter.", "num_updates": "Optional count of number of updates applied to variables.", "zero_debias": "If True, zero debias moving-averages that are initialized\nwith tensors. (Note: moving averages may not be initialized with\nnon-variable tensors when eager execution is enabled).", "name": "String. Optional prefix name to use for the name of ops added in\napply()."}, "Attributes": {"name": "The name of this ExponentialMovingAverage object."}}, "tf.train.Feature": {"description": "Used in tf.train.Example protos. Contains a list of values.", "Attributes": {"bytes_list": "BytesList bytes_list", "float_list": "FloatList float_list", "int64_list": "Int64List int64_list"}}, "tf.train.FeatureList": {"description": "Mainly used as part of a tf.train.SequenceExample.", "Attributes": {"feature": "repeated Feature feature"}}, "tf.train.FeatureLists": {"description": "Mainly used as part of a tf.train.SequenceExample.", "Attributes": {"feature_list": "repeated FeatureListEntry feature_list"}}, "tf.train.FeatureLists.FeatureListEntry": {"description": "A ProtocolMessage"}, "tf.train.Features": {"description": "Used in tf.train.Example protos. Contains the mapping from keys to Feature.", "Attributes": {"feature": "repeated FeatureEntry feature"}}, "tf.train.Features.FeatureEntry": {"description": "A ProtocolMessage"}, "tf.train.FloatList": {"description": "Used in tf.train.Example protos. Holds a list of floats.", "Attributes": {"value": "repeated float value"}}, "tf.train.Int64List": {"description": "Used in tf.train.Example protos. Holds a list of Int64s.", "Attributes": {"value": "repeated int64 value"}}, "tf.train.JobDef": {"description": "A ProtocolMessage", "Attributes": {"name": "string name", "tasks": "repeated TasksEntry tasks"}}, "tf.train.JobDef.TasksEntry": {"description": "A ProtocolMessage"}, "tf.train.SequenceExample": {"description": "A SequenceExample is a format a sequences and some context.", "Attributes": {"context": "Features context", "feature_lists": "FeatureLists feature_lists"}}, "tf.train.ServerDef": {"description": "A ProtocolMessage"}, "tf.train.checkpoints_iterator": {"description": "Continuously yield new checkpoint files as they appear.", "Args": {"checkpoint_dir": "The directory in which checkpoints are saved.", "min_interval_secs": "The minimum number of seconds between yielding\ncheckpoints.", "timeout": "The maximum number of seconds to wait between checkpoints. If left\nas None, then the process will wait indefinitely.", "timeout_fn": "Optional function to call after a timeout.  If the function\nreturns True, then it means that no new checkpoints will be generated and\nthe iterator will exit.  The function is called with no arguments."}}, "tf.train.get_checkpoint_state": {"description": "Returns CheckpointState proto from the &#34;checkpoint&#34; file.", "Args": {"checkpoint_dir": "The directory of checkpoints.", "latest_filename": "Optional name of the checkpoint file.  Default to\n'checkpoint'."}, "Returns": "A CheckpointState if the state was available, None\notherwise.", "Raises": {"ValueError": "if the checkpoint read doesn't have model_checkpoint_path set."}}, "tf.train.latest_checkpoint": {"description": "Finds the filename of latest saved checkpoint file.", "Args": {"checkpoint_dir": "Directory where the variables were saved.", "latest_filename": "Optional name for the protocol buffer file that\ncontains the list of most recent checkpoint filenames.\nSee the corresponding argument to v1.train.Saver.save."}, "Returns": "The full path to the latest checkpoint or None if no checkpoint was found."}, "tf.train.list_variables": {"description": "Lists the checkpoint keys and shapes of variables in a checkpoint.", "Args": {"ckpt_dir_or_file": "Directory with checkpoints file or path to checkpoint."}, "Returns": "List of tuples (key, shape)."}, "tf.train.load_checkpoint": {"description": "Returns CheckpointReader for checkpoint found in ckpt_dir_or_file.", "Args": {"ckpt_dir_or_file": "Directory with checkpoints file or path to checkpoint\nfile."}, "Returns": "CheckpointReader object.", "Raises": {"ValueError": "If ckpt_dir_or_file resolves to a directory with no\ncheckpoints."}}, "tf.train.load_variable": {"description": "Returns the tensor value of the given variable in the checkpoint.", "Args": {"ckpt_dir_or_file": "Directory with checkpoints file or path to checkpoint.", "name": "Name of the variable to return."}, "Returns": "A numpy ndarray with a copy of the value of this variable."}}, "tf.types": {}, "tf.version": {}, "tf.xla": {}, "tf.compat.v1.keras.applications.convnext": {}, "tf.initializers": {}, "tf.keras.applications.convnext": {"tf.keras.applications.convnext.ConvNeXtBase": {"description": "Instantiates the ConvNeXtBase architecture.", "Args": {"include_top": "Whether to include the fully-connected\nlayer at the top of the network. Defaults to True.", "weights": "One of None (random initialization),\n\"imagenet\" (pre-training on ImageNet-1k), or the path to the weights\nfile to be loaded. Defaults to \"imagenet\".", "input_tensor": "Optional Keras tensor\n(i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "Optional shape tuple, only to be specified\nif include_top is False.\nIt should have exactly 3 inputs channels.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False. Defaults to None.\n\nNone means that the output of the model will be\nthe 4D tensor output of the last convolutional layer.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "Optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified. Defaults to 1000 (number of\nImageNet classes).", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nDefaults to \"softmax\".\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.convnext.ConvNeXtLarge": {"description": "Instantiates the ConvNeXtLarge architecture.", "Args": {"include_top": "Whether to include the fully-connected\nlayer at the top of the network. Defaults to True.", "weights": "One of None (random initialization),\n\"imagenet\" (pre-training on ImageNet-1k), or the path to the weights\nfile to be loaded. Defaults to \"imagenet\".", "input_tensor": "Optional Keras tensor\n(i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "Optional shape tuple, only to be specified\nif include_top is False.\nIt should have exactly 3 inputs channels.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False. Defaults to None.\n\nNone means that the output of the model will be\nthe 4D tensor output of the last convolutional layer.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "Optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified. Defaults to 1000 (number of\nImageNet classes).", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nDefaults to \"softmax\".\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.convnext.ConvNeXtSmall": {"description": "Instantiates the ConvNeXtSmall architecture.", "Args": {"include_top": "Whether to include the fully-connected\nlayer at the top of the network. Defaults to True.", "weights": "One of None (random initialization),\n\"imagenet\" (pre-training on ImageNet-1k), or the path to the weights\nfile to be loaded. Defaults to \"imagenet\".", "input_tensor": "Optional Keras tensor\n(i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "Optional shape tuple, only to be specified\nif include_top is False.\nIt should have exactly 3 inputs channels.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False. Defaults to None.\n\nNone means that the output of the model will be\nthe 4D tensor output of the last convolutional layer.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "Optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified. Defaults to 1000 (number of\nImageNet classes).", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nDefaults to \"softmax\".\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.convnext.ConvNeXtTiny": {"description": "Instantiates the ConvNeXtTiny architecture.", "Args": {"include_top": "Whether to include the fully-connected\nlayer at the top of the network. Defaults to True.", "weights": "One of None (random initialization),\n\"imagenet\" (pre-training on ImageNet-1k), or the path to the weights\nfile to be loaded. Defaults to \"imagenet\".", "input_tensor": "Optional Keras tensor\n(i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "Optional shape tuple, only to be specified\nif include_top is False.\nIt should have exactly 3 inputs channels.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False. Defaults to None.\n\nNone means that the output of the model will be\nthe 4D tensor output of the last convolutional layer.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "Optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified. Defaults to 1000 (number of\nImageNet classes).", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nDefaults to \"softmax\".\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.convnext.ConvNeXtXLarge": {"description": "Instantiates the ConvNeXtXLarge architecture.", "Args": {"include_top": "Whether to include the fully-connected\nlayer at the top of the network. Defaults to True.", "weights": "One of None (random initialization),\n\"imagenet\" (pre-training on ImageNet-1k), or the path to the weights\nfile to be loaded. Defaults to \"imagenet\".", "input_tensor": "Optional Keras tensor\n(i.e. output of layers.Input())\nto use as image input for the model.", "input_shape": "Optional shape tuple, only to be specified\nif include_top is False.\nIt should have exactly 3 inputs channels.", "pooling": "Optional pooling mode for feature extraction\nwhen include_top is False. Defaults to None.\n\nNone means that the output of the model will be\nthe 4D tensor output of the last convolutional layer.\navg means that global average pooling\nwill be applied to the output of the\nlast convolutional layer, and thus\nthe output of the model will be a 2D tensor.\nmax means that global max pooling will\nbe applied.", "classes": "Optional number of classes to classify images\ninto, only to be specified if include_top is True, and\nif no weights argument is specified. Defaults to 1000 (number of\nImageNet classes).", "classifier_activation": "A str or callable. The activation function to use\non the \"top\" layer. Ignored unless include_top=True. Set\nclassifier_activation=None to return the logits of the \"top\" layer.\nDefaults to \"softmax\".\nWhen loading pretrained weights, classifier_activation can only\nbe None or \"softmax\"."}, "Returns": "A keras.Model instance."}, "tf.keras.applications.convnext.decode_predictions": {"description": "Decodes the prediction of an ImageNet model.", "Args": {"preds": "Numpy array encoding a batch of predictions.", "top": "Integer, how many top-guesses to return. Defaults to 5."}, "Returns": "A list of lists of top class prediction tuples\n(class_name, class_description, score).\nOne list of tuples per sample in batch input.", "Raises": {"ValueError": "In case of invalid shape of the pred array\n(must be 2D)."}}, "tf.keras.applications.convnext.preprocess_input": {"description": "A placeholder method for backward compatibility.", "Args": {"x": "A floating point numpy.array or a tf.Tensor.", "data_format": "Optional data format of the image tensor/array. Defaults to\nNone, in which case the global setting\ntf.keras.backend.image_data_format() is used (unless you changed it,\nit defaults to \"channels_last\").{mode}"}, "Returns": "Unchanged numpy.array or tf.Tensor."}}, "tf.keras.models.experimental": {"tf.keras.models.experimental.SharpnessAwareMinimization": {"description": "Sharpness aware minimization (SAM) training flow.", "Args": {"model": "tf.keras.Model instance. The inner model that does the\nforward-backward pass.", "rho": "float, defaults to 0.05. The gradients scaling factor.", "num_batch_splits": "int, defaults to None. The number of mini batches to\nsplit into from each data batch. If None, batches are not split into\nsub-batches.", "name": "string, defaults to None. The name of the SAM model."}, "Attributes": {"distribute_strategy": "The tf.distribute.Strategy this model was created under.", "layers": "", "metrics_names": "Returns the model's display labels for all outputs.\nNote: metrics_names are available only after a keras.Model has been\ntrained/evaluated on actual data.\ninputs = tf.keras.layers.Input(shape=(3,))outputs = tf.keras.layers.Dense(2)(inputs)model = tf.keras.models.Model(inputs=inputs, outputs=outputs)model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])model.metrics_names[]\nx = np.random.random((2, 3))y = np.random.randint(0, 2, (2, 2))model.fit(x, y)model.metrics_names['loss', 'mae']\ninputs = tf.keras.layers.Input(shape=(3,))d = tf.keras.layers.Dense(2, name='out')output_1 = d(inputs)output_2 = d(inputs)model = tf.keras.models.Model(\u00a0 \u00a0inputs=inputs, outputs=[output_1, output_2])model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\", \"acc\"])model.fit(x, (y, y))model.metrics_names['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae','out_1_acc']", "run_eagerly": "Settable attribute indicating whether the model should run eagerly.\nRunning eagerly means that your model will be run step by step,\nlike Python code. Your model might run slower, but it should become\neasier for you to debug it by stepping into individual layer calls.\nBy default, we will attempt to compile your model to a static graph to\ndeliver the best execution performance."}}}, "tf.metrics": {}, "tf.optimizers": {}, "tf.types.experimental.distributed": {"tf.types.experimental.distributed.Mirrored": {"description": "Holds a distributed value: a map from replica id to synchronized values."}, "tf.types.experimental.distributed.PerReplica": {"description": "Holds a distributed value: a map from replica id to unsynchronized values."}}, "tf.autograph.experimental": {"tf.autograph.experimental.Feature": {"description": "This enumeration represents optional conversion options.", "Attributes": {"ALL": "Enable all features.", "AUTO_CONTROL_DEPS": "Insert of control dependencies in the generated code.", "ASSERT_STATEMENTS": "Convert Tensor-dependent assert statements to tf.Assert.", "BUILTIN_FUNCTIONS": "Convert builtin functions applied to Tensors to\ntheir TF counterparts.", "EQUALITY_OPERATORS": "Whether to convert the comparison operators, like\nequality. This is soon to be deprecated as support is being added to the\nTensor class.", "LISTS": "Convert list idioms, like initializers, slices, append, etc.", "NAME_SCOPES": "Insert name scopes that name ops according to context, like the\nfunction they were defined in."}, "Class Variables": {"ALL": "<Feature.ALL: 'ALL'>", "ASSERT_STATEMENTS": "<Feature.ASSERT_STATEMENTS: 'ASSERT_STATEMENTS'>", "AUTO_CONTROL_DEPS": "<Feature.AUTO_CONTROL_DEPS: 'AUTO_CONTROL_DEPS'>", "BUILTIN_FUNCTIONS": "<Feature.BUILTIN_FUNCTIONS: 'BUILTIN_FUNCTIONS'>", "EQUALITY_OPERATORS": "<Feature.EQUALITY_OPERATORS: 'EQUALITY_OPERATORS'>", "LISTS": "<Feature.LISTS: 'LISTS'>", "NAME_SCOPES": "<Feature.NAME_SCOPES: 'NAME_SCOPES'>"}}, "tf.autograph.experimental.do_not_convert": {"description": "Decorator that suppresses the conversion of a function.", "Args": {"func": "function to decorate."}, "Returns": "If func is not None, returns a Callable which is equivalent to\nfunc, but is not converted by AutoGraph.\nIf func is None, returns a decorator that, when invoked with a\nsingle func argument, returns a Callable equivalent to the\nabove case."}, "tf.autograph.experimental.set_loop_options": {"description": "Specifies additional arguments to be passed to the enclosing while_loop.", "Args": {"parallel_iterations": "The maximum number of iterations allowed to run in\nparallel at any given time. Note that this does not guarantee parallel\nexecution.", "swap_memory": "Whether to store intermediate values needed for\ngradients on the CPU instead of GPU.", "maximum_iterations": "Allows limiting the total number of iterations executed\nby the loop.", "shape_invariants": "Allows controlling the argument with the same name passed\nto tf.while_loop. Unlike tf.while_loop, this is a list of\n(tensor, shape) pairs."}}}, "tf.compat.v1.autograph.experimental": {}, "tf.compat.v1.config.experimental": {}, "tf.compat.v1.data.experimental": {"tf.compat.v1.data.experimental.Counter": {"description": "Creates a Dataset that counts from start in steps of size step.", "Args": {"start": "(Optional.) The starting value for the counter. Defaults to 0.", "step": "(Optional.) The step size for the counter. Defaults to 1.", "dtype": "(Optional.) The data type for counter elements. Defaults to\ntf.int64."}, "Returns": "A Dataset of scalar dtype elements."}, "tf.compat.v1.data.experimental.CsvDataset": {"description": "A Dataset comprising lines from one or more CSV files.", "Args": {"filenames": "A tf.string tensor containing one or more filenames.", "record_defaults": "A list of default values for the CSV fields. Each item in\nthe list is either a valid CSV DType (float32, float64, int32, int64,\nstring), or a Tensor object with one of the above types. One per\ncolumn of CSV data, with either a scalar Tensor default value for the\ncolumn if it is optional, or DType or empty Tensor if required. If\nboth this and select_columns are specified, these must have the same\nlengths, and column_defaults is assumed to be sorted in order of\nincreasing column index. If both this and 'exclude_cols' are specified,\nthe sum of lengths of record_defaults and exclude_cols should equal the\ntotal number of columns in the CSV file.", "compression_type": "(Optional.) A tf.string scalar evaluating to one of\n\"\" (no compression), \"ZLIB\", or \"GZIP\". Defaults to no\ncompression.", "buffer_size": "(Optional.) A tf.int64 scalar denoting the number of bytes\nto buffer while reading files. Defaults to 4MB.", "header": "(Optional.) A tf.bool scalar indicating whether the CSV file(s)\nhave header line(s) that should be skipped when parsing. Defaults to\nFalse.", "field_delim": "(Optional.) A tf.string scalar containing the delimiter\ncharacter that separates fields in a record. Defaults to \",\".", "use_quote_delim": "(Optional.) A tf.bool scalar. If False, treats double\nquotation marks as regular characters inside of string fields (ignoring\nRFC 4180, Section 2, Bullet 5). Defaults to True.", "na_value": "(Optional.) A tf.string scalar indicating a value that will be\ntreated as NA/NaN.", "select_cols": "(Optional.) A sorted list of column indices to select from\nthe input data. If specified, only this subset of columns will be\nparsed. Defaults to parsing all columns. At most one of select_cols\nand exclude_cols can be specified."}, "Attributes": {"element_spec": "The type specification of an element of this dataset.\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])dataset.element_specTensorSpec(shape=(), dtype=tf.int32, name=None)\nFor more information,\nread this guide.", "output_classes": "Returns the class of each component of an element of this dataset. (deprecated)\nDeprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\nInstructions for updating:\nUse tf.compat.v1.data.get_output_classes(dataset).", "output_shapes": "Returns the shape of each component of an element of this dataset. (deprecated)Deprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\nInstructions for updating:\nUse tf.compat.v1.data.get_output_shapes(dataset).", "output_types": "Returns the type of each component of an element of this dataset. (deprecated)Deprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\nInstructions for updating:\nUse tf.compat.v1.data.get_output_types(dataset)."}}, "tf.compat.v1.data.experimental.RaggedTensorStructure": {"description": "DEPRECATED FUNCTION"}, "tf.compat.v1.data.experimental.RandomDataset": {"description": "A Dataset of pseudorandom values. (deprecated)", "Attributes": {"element_spec": "The type specification of an element of this dataset.\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])dataset.element_specTensorSpec(shape=(), dtype=tf.int32, name=None)\nFor more information,\nread this guide.", "output_classes": "Returns the class of each component of an element of this dataset. (deprecated)\nDeprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\nInstructions for updating:\nUse tf.compat.v1.data.get_output_classes(dataset).", "output_shapes": "Returns the shape of each component of an element of this dataset. (deprecated)Deprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\nInstructions for updating:\nUse tf.compat.v1.data.get_output_shapes(dataset).", "output_types": "Returns the type of each component of an element of this dataset. (deprecated)Deprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\nInstructions for updating:\nUse tf.compat.v1.data.get_output_types(dataset)."}}, "tf.compat.v1.data.experimental.SparseTensorStructure": {"description": "DEPRECATED FUNCTION"}, "tf.compat.v1.data.experimental.SqlDataset": {"description": "A Dataset consisting of the results from a SQL query.", "Args": {"driver_name": "A 0-D tf.string tensor containing the database type.\nCurrently, the only supported value is 'sqlite'.", "data_source_name": "A 0-D tf.string tensor containing a connection string\nto connect to the database.", "query": "A 0-D tf.string tensor containing the SQL query to execute.", "output_types": "A tuple of tf.DType objects representing the types of the\ncolumns returned by query."}, "Attributes": {"element_spec": "The type specification of an element of this dataset.\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])dataset.element_specTensorSpec(shape=(), dtype=tf.int32, name=None)\nFor more information,\nread this guide.", "output_classes": "Returns the class of each component of an element of this dataset. (deprecated)\nDeprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\nInstructions for updating:\nUse tf.compat.v1.data.get_output_classes(dataset).", "output_shapes": "Returns the shape of each component of an element of this dataset. (deprecated)Deprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\nInstructions for updating:\nUse tf.compat.v1.data.get_output_shapes(dataset).", "output_types": "Returns the type of each component of an element of this dataset. (deprecated)Deprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\nInstructions for updating:\nUse tf.compat.v1.data.get_output_types(dataset)."}}, "tf.compat.v1.data.experimental.TensorArrayStructure": {"description": "DEPRECATED FUNCTION"}, "tf.compat.v1.data.experimental.TensorStructure": {"description": "DEPRECATED FUNCTION"}, "tf.compat.v1.data.experimental.choose_from_datasets": {"description": "Creates a dataset that deterministically chooses elements from datasets. (deprecated)", "Args": {"datasets": "A non-empty list of tf.data.Dataset objects with compatible\nstructure.", "choice_dataset": "A tf.data.Dataset of scalar tf.int64 tensors between 0\nand len(datasets) - 1.", "stop_on_empty_dataset": "If True, selection stops if it encounters an empty\ndataset. If False, it skips empty datasets. It is recommended to set it\nto True. Otherwise, the selected elements start off as the user intends,\nbut may change as input datasets become empty. This can be difficult to\ndetect since the dataset starts off looking correct. Default to False\nfor backward compatibility."}, "Returns": "A dataset that interleaves elements from datasets according to the values\nof choice_dataset.", "Raises": {"TypeError": "If datasets or choice_dataset has the wrong type.", "ValueError": "If datasets is empty."}}, "tf.compat.v1.data.experimental.make_batched_features_dataset": {"description": "Returns a Dataset of feature dictionaries from Example protos.", "Args": {"file_pattern": "List of files or patterns of file paths containing\nExample records. See tf.io.gfile.glob for pattern rules.", "batch_size": "An int representing the number of records to combine\nin a single batch.", "features": "A dict mapping feature keys to FixedLenFeature or\nVarLenFeature values. See tf.io.parse_example.", "reader": "A function or class that can be\ncalled with a filenames tensor and (optional) reader_args and returns\na Dataset of Example tensors. Defaults to tf.data.TFRecordDataset.", "label_key": "(Optional) A string corresponding to the key labels are stored in\ntf.Examples. If provided, it must be one of the features key,\notherwise results in ValueError.", "reader_args": "Additional arguments to pass to the reader class.", "num_epochs": "Integer specifying the number of times to read through the\ndataset. If None, cycles through the dataset forever. Defaults to None.", "shuffle": "A boolean, indicates whether the input should be shuffled. Defaults\nto True.", "shuffle_buffer_size": "Buffer size of the ShuffleDataset. A large capacity\nensures better shuffling but would increase memory usage and startup time.", "shuffle_seed": "Randomization seed to use for shuffling.", "prefetch_buffer_size": "Number of feature batches to prefetch in order to\nimprove performance. Recommended value is the number of batches consumed\nper training step. Defaults to auto-tune.", "reader_num_threads": "Number of threads used to read Example records. If >1,\nthe results will be interleaved. Defaults to 1.", "parser_num_threads": "Number of threads to use for parsing Example tensors\ninto a dictionary of Feature tensors. Defaults to 2.", "sloppy_ordering": "If True, reading performance will be improved at\nthe cost of non-deterministic ordering. If False, the order of elements\nproduced is deterministic prior to shuffling (elements are still\nrandomized if shuffle=True. Note that if the seed is set, then order\nof elements after shuffling is deterministic). Defaults to False.", "drop_final_batch": "If True, and the batch size does not evenly divide the\ninput dataset size, the final smaller batch will be dropped. Defaults to\nFalse."}, "Returns": "A dataset of dict elements, (or a tuple of dict elements and label).\nEach dict maps feature keys to Tensor or SparseTensor objects.", "Raises": {"TypeError": "If reader is of the wrong type.", "ValueError": "If label_key is not one of the features keys."}}, "tf.compat.v1.data.experimental.make_csv_dataset": {"description": "Reads CSV files into a dataset.", "Args": {"file_pattern": "List of files or patterns of file paths containing CSV\nrecords. See tf.io.gfile.glob for pattern rules.", "batch_size": "An int representing the number of records to combine\nin a single batch.", "column_names": "An optional list of strings that corresponds to the CSV\ncolumns, in order. One per column of the input record. If this is not\nprovided, infers the column names from the first row of the records.\nThese names will be the keys of the features dict of each dataset element.", "column_defaults": "A optional list of default values for the CSV fields. One\nitem per selected column of the input record. Each item in the list is\neither a valid CSV dtype (float32, float64, int32, int64, or string), or a\nTensor with one of the aforementioned types. The tensor can either be\na scalar default value (if the column is optional), or an empty tensor (if\nthe column is required). If a dtype is provided instead of a tensor, the\ncolumn is also treated as required. If this list is not provided, tries\nto infer types based on reading the first num_rows_for_inference rows of\nfiles specified, and assumes all columns are optional, defaulting to 0\nfor numeric values and \"\" for string values. If both this and\nselect_columns are specified, these must have the same lengths, and\ncolumn_defaults is assumed to be sorted in order of increasing column\nindex.", "label_name": "A optional string corresponding to the label column. If\nprovided, the data for this column is returned as a separate Tensor from\nthe features dictionary, so that the dataset complies with the format\nexpected by a tf.Estimator.train or tf.Estimator.evaluate input\nfunction.", "select_columns": "An optional list of integer indices or string column\nnames, that specifies a subset of columns of CSV data to select. If\ncolumn names are provided, these must correspond to names provided in\ncolumn_names or inferred from the file header lines. When this argument\nis specified, only a subset of CSV columns will be parsed and returned,\ncorresponding to the columns specified. Using this results in faster\nparsing and lower memory usage. If both this and column_defaults are\nspecified, these must have the same lengths, and column_defaults is\nassumed to be sorted in order of increasing column index.", "field_delim": "An optional string. Defaults to \",\". Char delimiter to\nseparate fields in a record.", "use_quote_delim": "An optional bool. Defaults to True. If false, treats\ndouble quotation marks as regular characters inside of the string fields.", "na_value": "Additional string to recognize as NA/NaN.", "header": "A bool that indicates whether the first rows of provided CSV files\ncorrespond to header lines with column names, and should not be included\nin the data.", "num_epochs": "An int specifying the number of times this dataset is repeated.\nIf None, cycles through the dataset forever.", "shuffle": "A bool that indicates whether the input should be shuffled.", "shuffle_buffer_size": "Buffer size to use for shuffling. A large buffer size\nensures better shuffling, but increases memory usage and startup time.", "shuffle_seed": "Randomization seed to use for shuffling.", "prefetch_buffer_size": "An int specifying the number of feature\nbatches to prefetch for performance improvement. Recommended value is the\nnumber of batches consumed per training step. Defaults to auto-tune.", "num_parallel_reads": "Number of threads used to read CSV records from files.\nIf >1, the results will be interleaved. Defaults to 1.", "sloppy": "If True, reading performance will be improved at\nthe cost of non-deterministic ordering. If False, the order of elements\nproduced is deterministic prior to shuffling (elements are still\nrandomized if shuffle=True. Note that if the seed is set, then order\nof elements after shuffling is deterministic). Defaults to False.", "num_rows_for_inference": "Number of rows of a file to use for type inference\nif record_defaults is not provided. If None, reads all the rows of all\nthe files. Defaults to 100.", "compression_type": "(Optional.) A tf.string scalar evaluating to one of\n\"\" (no compression), \"ZLIB\", or \"GZIP\". Defaults to no compression.", "ignore_errors": "(Optional.) If True, ignores errors with CSV file parsing,\nsuch as malformed data or empty lines, and moves on to the next valid\nCSV record. Otherwise, the dataset raises an error and stops processing\nwhen encountering any invalid records. Defaults to False."}, "Returns": "A dataset, where each element is a (features, labels) tuple that corresponds\nto a batch of batch_size CSV rows. The features dictionary maps feature\ncolumn names to Tensors containing the corresponding column data, and\nlabels is a Tensor containing the column data for the label column\nspecified by label_name.", "Raises": {"ValueError": "If any of the arguments is malformed."}}, "tf.compat.v1.data.experimental.map_and_batch_with_legacy_function": {"description": "Fused implementation of map and batch. (deprecated)", "Args": {"map_func": "A function mapping a nested structure of tensors to another\nnested structure of tensors.", "batch_size": "A tf.int64 scalar tf.Tensor, representing the number of\nconsecutive elements of this dataset to combine in a single batch.", "num_parallel_batches": "(Optional.) A tf.int64 scalar tf.Tensor,\nrepresenting the number of batches to create in parallel. On one hand,\nhigher values can help mitigate the effect of stragglers. On the other\nhand, higher values can increase contention if CPU is scarce.", "drop_remainder": "(Optional.) A tf.bool scalar tf.Tensor, representing\nwhether the last batch should be dropped in case its size is smaller than\ndesired; the default behavior is not to drop the smaller batch.", "num_parallel_calls": "(Optional.) A tf.int32 scalar tf.Tensor,\nrepresenting the number of elements to process in parallel. If not\nspecified, batch_size * num_parallel_batches elements will be processed\nin parallel. If the value tf.data.AUTOTUNE is used, then\nthe number of parallel calls is set dynamically based on available CPU."}, "Returns": "A Dataset transformation function, which can be passed to\ntf.data.Dataset.apply.", "Raises": {"ValueError": "If both num_parallel_batches and num_parallel_calls are\nspecified."}}, "tf.compat.v1.data.experimental.sample_from_datasets": {"description": "Samples elements at random from the datasets in datasets. (deprecated)", "Args": {"datasets": "A non-empty list of tf.data.Dataset objects with compatible\nstructure.", "weights": "(Optional.) A list or Tensor of len(datasets) floating-point\nvalues where weights[i] represents the probability to sample from\ndatasets[i], or a tf.data.Dataset object where each element is such a\nlist. Defaults to a uniform distribution across datasets.", "seed": "(Optional.) A tf.int64 scalar tf.Tensor, representing the random\nseed that will be used to create the distribution. See\ntf.random.set_seed for behavior.", "stop_on_empty_dataset": "If True, sampling stops if it encounters an empty\ndataset. If False, it skips empty datasets. It is recommended to set it\nto True. Otherwise, the distribution of samples starts off as the user\nintends, but may change as input datasets become empty. This can be\ndifficult to detect since the dataset starts off looking correct. Default\nto False for backward compatibility."}, "Returns": "A dataset that interleaves elements from datasets at random, according to\nweights if provided, otherwise with uniform probability.", "Raises": {"TypeError": "If the datasets or weights arguments have the wrong type.", "ValueError": "If datasets is empty, or\nIf weights is specified and does not match the length of datasets."}}}, "tf.compat.v1.debugging.experimental": {}, "tf.compat.v1.distribute.experimental": {"tf.compat.v1.distribute.experimental.CentralStorageStrategy": {"description": "A one-machine strategy that puts all variables on a single device.", "Attributes": {"cluster_resolver": "Returns the cluster resolver associated with this strategy.\nIn general, when using a multi-worker tf.distribute strategy such as\ntf.distribute.experimental.MultiWorkerMirroredStrategy or\ntf.distribute.TPUStrategy(), there is a\ntf.distribute.cluster_resolver.ClusterResolver associated with the\nstrategy used, and such an instance is returned by this property.\nStrategies that intend to have an associated\ntf.distribute.cluster_resolver.ClusterResolver must set the\nrelevant attribute, or override this property; otherwise, None is returned\nby default. Those strategies should also provide information regarding what\nis returned by this property.\nSingle-worker strategies usually do not have a\ntf.distribute.cluster_resolver.ClusterResolver, and in those cases this\nproperty will return None.\nThe tf.distribute.cluster_resolver.ClusterResolver may be useful when the\nuser needs to access information such as the cluster spec, task type or task\nid. For example,\nos.environ['TF_CONFIG'] = json.dumps({\u00a0 'cluster': {\u00a0 \u00a0 \u00a0 'worker': [\"localhost:12345\", \"localhost:23456\"],\u00a0 \u00a0 \u00a0 'ps': [\"localhost:34567\"]\u00a0 },\u00a0 'task': {'type': 'worker', 'index': 0}})# This implicitly uses TF_CONFIG for the cluster and current task info.strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()...if strategy.cluster_resolver.task_type == 'worker':\u00a0 # Perform something that's only applicable on workers. Since we set this\u00a0 # as a worker above, this block will run on this particular instance.elif strategy.cluster_resolver.task_type == 'ps':\u00a0 # Perform something that's only applicable on parameter servers. Since we\u00a0 # set this as a worker above, this block will not run on this particular\u00a0 # instance.\nFor more information, please see\ntf.distribute.cluster_resolver.ClusterResolver's API docstring.", "extended": "tf.distribute.StrategyExtended with additional methods.", "num_replicas_in_sync": "Returns number of replicas over which gradients are aggregated."}}, "tf.compat.v1.distribute.experimental.MultiWorkerMirroredStrategy": {"description": "A distribution strategy for synchronous training on multiple workers.", "Attributes": {"cluster_resolver": "Returns the cluster resolver associated with this strategy.\nIn general, when using a multi-worker tf.distribute strategy such as\ntf.distribute.experimental.MultiWorkerMirroredStrategy or\ntf.distribute.TPUStrategy(), there is a\ntf.distribute.cluster_resolver.ClusterResolver associated with the\nstrategy used, and such an instance is returned by this property.\nStrategies that intend to have an associated\ntf.distribute.cluster_resolver.ClusterResolver must set the\nrelevant attribute, or override this property; otherwise, None is returned\nby default. Those strategies should also provide information regarding what\nis returned by this property.\nSingle-worker strategies usually do not have a\ntf.distribute.cluster_resolver.ClusterResolver, and in those cases this\nproperty will return None.\nThe tf.distribute.cluster_resolver.ClusterResolver may be useful when the\nuser needs to access information such as the cluster spec, task type or task\nid. For example,\nos.environ['TF_CONFIG'] = json.dumps({\u00a0 'cluster': {\u00a0 \u00a0 \u00a0 'worker': [\"localhost:12345\", \"localhost:23456\"],\u00a0 \u00a0 \u00a0 'ps': [\"localhost:34567\"]\u00a0 },\u00a0 'task': {'type': 'worker', 'index': 0}})# This implicitly uses TF_CONFIG for the cluster and current task info.strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()...if strategy.cluster_resolver.task_type == 'worker':\u00a0 # Perform something that's only applicable on workers. Since we set this\u00a0 # as a worker above, this block will run on this particular instance.elif strategy.cluster_resolver.task_type == 'ps':\u00a0 # Perform something that's only applicable on parameter servers. Since we\u00a0 # set this as a worker above, this block will not run on this particular\u00a0 # instance.\nFor more information, please see\ntf.distribute.cluster_resolver.ClusterResolver's API docstring.", "extended": "tf.distribute.StrategyExtended with additional methods.", "num_replicas_in_sync": "Returns number of replicas over which gradients are aggregated."}}, "tf.compat.v1.distribute.experimental.ParameterServerStrategy": {"description": "An asynchronous multi-worker parameter server tf.distribute strategy.", "Args": {"cluster_resolver": "Optional\ntf.distribute.cluster_resolver.ClusterResolver object. Defaults to a\ntf.distribute.cluster_resolver.TFConfigClusterResolver."}, "Attributes": {"cluster_resolver": "Returns the cluster resolver associated with this strategy.\nIn general, when using a multi-worker tf.distribute strategy such as\ntf.distribute.experimental.MultiWorkerMirroredStrategy or\ntf.distribute.TPUStrategy(), there is a\ntf.distribute.cluster_resolver.ClusterResolver associated with the\nstrategy used, and such an instance is returned by this property.\nStrategies that intend to have an associated\ntf.distribute.cluster_resolver.ClusterResolver must set the\nrelevant attribute, or override this property; otherwise, None is returned\nby default. Those strategies should also provide information regarding what\nis returned by this property.\nSingle-worker strategies usually do not have a\ntf.distribute.cluster_resolver.ClusterResolver, and in those cases this\nproperty will return None.\nThe tf.distribute.cluster_resolver.ClusterResolver may be useful when the\nuser needs to access information such as the cluster spec, task type or task\nid. For example,\nos.environ['TF_CONFIG'] = json.dumps({\u00a0 'cluster': {\u00a0 \u00a0 \u00a0 'worker': [\"localhost:12345\", \"localhost:23456\"],\u00a0 \u00a0 \u00a0 'ps': [\"localhost:34567\"]\u00a0 },\u00a0 'task': {'type': 'worker', 'index': 0}})# This implicitly uses TF_CONFIG for the cluster and current task info.strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()...if strategy.cluster_resolver.task_type == 'worker':\u00a0 # Perform something that's only applicable on workers. Since we set this\u00a0 # as a worker above, this block will run on this particular instance.elif strategy.cluster_resolver.task_type == 'ps':\u00a0 # Perform something that's only applicable on parameter servers. Since we\u00a0 # set this as a worker above, this block will not run on this particular\u00a0 # instance.\nFor more information, please see\ntf.distribute.cluster_resolver.ClusterResolver's API docstring.", "extended": "tf.distribute.StrategyExtended with additional methods.", "num_replicas_in_sync": "Returns number of replicas over which gradients are aggregated."}}, "tf.compat.v1.distribute.experimental.TPUStrategy": {"description": "TPU distribution strategy implementation.", "Args": {"tpu_cluster_resolver": "A tf.distribute.cluster_resolver.TPUClusterResolver,\nwhich provides information about the TPU cluster.", "steps_per_run": "Number of steps to run on device before returning to the\nhost. Note that this can have side-effects on performance, hooks,\nmetrics, summaries etc.\nThis parameter is only used when Distribution Strategy is used with\nestimator or keras.", "device_assignment": "Optional tf.tpu.experimental.DeviceAssignment to\nspecify the placement of replicas on the TPU cluster. Currently only\nsupports the usecase of using a single core within a TPU cluster."}, "Attributes": {"cluster_resolver": "Returns the cluster resolver associated with this strategy.\nIn general, when using a multi-worker tf.distribute strategy such as\ntf.distribute.experimental.MultiWorkerMirroredStrategy or\ntf.distribute.TPUStrategy(), there is a\ntf.distribute.cluster_resolver.ClusterResolver associated with the\nstrategy used, and such an instance is returned by this property.\nStrategies that intend to have an associated\ntf.distribute.cluster_resolver.ClusterResolver must set the\nrelevant attribute, or override this property; otherwise, None is returned\nby default. Those strategies should also provide information regarding what\nis returned by this property.\nSingle-worker strategies usually do not have a\ntf.distribute.cluster_resolver.ClusterResolver, and in those cases this\nproperty will return None.\nThe tf.distribute.cluster_resolver.ClusterResolver may be useful when the\nuser needs to access information such as the cluster spec, task type or task\nid. For example,\nos.environ['TF_CONFIG'] = json.dumps({\u00a0 'cluster': {\u00a0 \u00a0 \u00a0 'worker': [\"localhost:12345\", \"localhost:23456\"],\u00a0 \u00a0 \u00a0 'ps': [\"localhost:34567\"]\u00a0 },\u00a0 'task': {'type': 'worker', 'index': 0}})# This implicitly uses TF_CONFIG for the cluster and current task info.strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()...if strategy.cluster_resolver.task_type == 'worker':\u00a0 # Perform something that's only applicable on workers. Since we set this\u00a0 # as a worker above, this block will run on this particular instance.elif strategy.cluster_resolver.task_type == 'ps':\u00a0 # Perform something that's only applicable on parameter servers. Since we\u00a0 # set this as a worker above, this block will not run on this particular\u00a0 # instance.\nFor more information, please see\ntf.distribute.cluster_resolver.ClusterResolver's API docstring.", "extended": "tf.distribute.StrategyExtended with additional methods.", "num_replicas_in_sync": "Returns number of replicas over which gradients are aggregated.", "steps_per_run": "DEPRECATED: use .extended.steps_per_run instead."}}}, "tf.compat.v1.estimator.experimental": {"tf.compat.v1.estimator.experimental.KMeans": {"description": "An Estimator for K-Means clustering.", "Args": {"num_clusters": "An integer tensor specifying the number of clusters. This\nargument is ignored if initial_clusters is a tensor or numpy array.", "model_dir": "The directory to save the model results and log files.", "initial_clusters": "Specifies how the initial cluster centers are chosen.\nOne of the following: * a tensor or numpy array with the initial cluster\n  centers. * a callable f(inputs, k) that selects and returns up to\n  k centers from an input batch. f is free to return any number of\n  centers from 0 to k. It will be invoked on successive input\n  batches as necessary until all num_clusters centers are chosen.\n\nKMeansClustering.RANDOM_INIT: Choose centers randomly from an input\nbatch. If the batch size is less than num_clusters then the entire\nbatch is chosen to be initial cluster centers and the remaining\ncenters are chosen from successive input batches.\nKMeansClustering.KMEANS_PLUS_PLUS_INIT: Use kmeans++ to choose\ncenters from the first input batch. If the batch size is less than\nnum_clusters, a TensorFlow runtime error occurs.", "distance_metric": "The distance metric used for clustering. One of:\nKMeansClustering.SQUARED_EUCLIDEAN_DISTANCE: Euclidean distance\nbetween vectors u and v is defined as ||u\u2212v||2 which is\nthe square root of the sum of the absolute squares of the elements'\ndifference.\nKMeansClustering.COSINE_DISTANCE: Cosine distance between vectors\nu and v is defined as 1\u2212(u.v)/(||u||2||v||2).", "seed": "Python integer. Seed for PRNG used to initialize centers.", "use_mini_batch": "A boolean specifying whether to use the mini-batch k-means\nalgorithm. See explanation above.", "mini_batch_steps_per_iteration": "The number of steps after which the\nupdated cluster centers are synced back to a master copy. Used only if\nuse_mini_batch=True. See explanation above.", "kmeans_plus_plus_num_retries": "For each point that is sampled during\nkmeans++ initialization, this parameter specifies the number of\nadditional points to draw from the current distribution before selecting\nthe best. If a negative value is specified, a heuristic is used to\nsample O(log(num_to_sample)) additional points. Used only if\ninitial_clusters=KMeansClustering.KMEANS_PLUS_PLUS_INIT.", "relative_tolerance": "A relative tolerance of change in the loss between\niterations. Stops learning if the loss changes less than this amount.\nThis may not work correctly if use_mini_batch=True.", "config": "See tf.estimator.Estimator.", "feature_columns": "An optionable iterable containing all the feature columns\nused by the model. All items in the set should be feature column\ninstances that can be passed to tf.feature_column.input_layer. If this\nis None, all features will be used."}, "Raises": {"ValueError": "An invalid argument was passed to initial_clusters or\ndistance_metric."}, "Attributes": {"config": "", "model_dir": "", "model_fn": "Returns the model_fn which is bound to self.params.", "params": ""}, "Class Variables": {"ALL_DISTANCES": "'all_distances'", "CLUSTER_CENTERS_VAR_NAME": "'clusters'", "CLUSTER_INDEX": "'cluster_index'", "COSINE_DISTANCE": "'cosine'", "KMEANS_PLUS_PLUS_INIT": "'kmeans_plus_plus'", "RANDOM_INIT": "'random'", "SCORE": "'score'", "SQUARED_EUCLIDEAN_DISTANCE": "'squared_euclidean'"}}, "tf.compat.v1.estimator.experimental.dnn_logit_fn_builder": {"description": "Function builder for a dnn logit_fn.", "Args": {"units": "An int indicating the dimension of the logit layer.  In the MultiHead\ncase, this should be the sum of all component Heads' logit dimensions.", "hidden_units": "Iterable of integer number of hidden units per layer.", "feature_columns": "Iterable of feature_column._FeatureColumn model inputs.", "activation_fn": "Activation function applied to each layer.", "dropout": "When not None, the probability we will drop out a given\ncoordinate.", "input_layer_partitioner": "Partitioner for input layer.", "batch_norm": "Whether to use batch normalization after each hidden layer."}, "Returns": "A logit_fn (see below).", "Raises": {"ValueError": "If units is not an int."}}, "tf.compat.v1.estimator.experimental.linear_logit_fn_builder": {"description": "Function builder for a linear logit_fn.", "Args": {"units": "An int indicating the dimension of the logit layer.", "feature_columns": "An iterable containing all the feature columns used by the\nmodel.", "sparse_combiner": "A string specifying how to reduce if a categorical column\nis multivalent.  One of \"mean\", \"sqrtn\", and \"sum\"."}, "Returns": "A logit_fn (see below)."}}, "tf.compat.v1.estimator.tpu.experimental": {"tf.compat.v1.estimator.tpu.experimental.EmbeddingConfigSpec": {"description": "Class to keep track of the specification for TPU embeddings.", "Args": {"feature_columns": "All embedding FeatureColumns used by model.", "optimization_parameters": "An instance of AdagradParameters,\nAdamParameters or StochasticGradientDescentParameters. This\noptimizer will be applied to all embedding variables specified by\nfeature_columns.", "clipping_limit": "(Optional) Clipping limit (absolute value).", "pipeline_execution_with_tensor_core": "setting this to True makes training\nfaster, but trained model will be different if step N and step N+1\ninvolve the same set of embedding IDs. Please see\ntpu_embedding_configuration.proto for details.", "experimental_gradient_multiplier_fn": "(Optional) A Fn taking global step as\ninput returning the current multiplier for all embedding gradients.", "feature_to_config_dict": "A dictionary mapping feature names to instances of\nthe class FeatureConfig. Either features_columns or the pair of\nfeature_to_config_dict and table_to_config_dict must be specified.", "table_to_config_dict": "A dictionary mapping feature names to instances of\nthe class TableConfig. Either features_columns or the pair of\nfeature_to_config_dict and table_to_config_dict must be specified."}, "Raises": {"ValueError": "If optimization_parameters is not one of the required types.", "TypeError": "If the feature columns are not of ths correct type (one of\n_SUPPORTED_FEATURE_COLUMNS, _TPU_EMBEDDING_COLUMN_CLASSES OR\n_EMBEDDING_COLUMN_CLASSES)."}, "Attributes": {"feature_columns": "A namedtuple alias for field number 0", "tensor_core_feature_columns": "A namedtuple alias for field number 1", "optimization_parameters": "A namedtuple alias for field number 2", "clipping_limit": "A namedtuple alias for field number 3", "pipeline_execution_with_tensor_core": "A namedtuple alias for field number 4", "experimental_gradient_multiplier_fn": "A namedtuple alias for field number 5", "feature_to_config_dict": "A namedtuple alias for field number 6", "table_to_config_dict": "A namedtuple alias for field number 7", "partition_strategy": "A namedtuple alias for field number 8", "profile_data_directory": "A namedtuple alias for field number 9"}}}, "tf.compat.v1.experimental": {"tf.compat.v1.experimental.output_all_intermediates": {"description": "Whether to output all intermediates from functional control flow ops.", "Args": {"state": "True, False or None. None restores the default behavior."}}}, "tf.compat.v1.keras.experimental": {"tf.compat.v1.keras.experimental.export_saved_model": {"description": "Exports a tf.keras.Model as a Tensorflow SavedModel.", "Args": {"model": "A tf.keras.Model to be saved. If the model is subclassed, the flag\nserving_only must be set to True.", "saved_model_path": "a string specifying the path to the SavedModel directory.", "custom_objects": "Optional dictionary mapping string names to custom classes\nor functions (e.g. custom loss functions).", "as_text": "bool, False by default. Whether to write the SavedModel proto\nin text format. Currently unavailable in serving-only mode.", "input_signature": "A possibly nested sequence of tf.TensorSpec objects, used\nto specify the expected model inputs. See tf.function for more details.", "serving_only": "bool, False by default. When this is true, only the\nprediction graph is saved."}, "Raises": {"NotImplementedError": "If the model is a subclassed model, and serving_only is\nFalse.", "ValueError": "If the input signature cannot be inferred from the model.", "AssertionError": "If the SavedModel directory already exists and isn't empty."}}, "tf.compat.v1.keras.experimental.load_from_saved_model": {"description": "Loads a keras Model from a SavedModel created by export_saved_model().", "Args": {"saved_model_path": "a string specifying the path to an existing SavedModel.", "custom_objects": "Optional dictionary mapping names\n(strings) to custom classes or functions to be\nconsidered during deserialization."}, "Returns": "a keras.Model instance."}}, "tf.compat.v1.keras.layers.experimental": {}, "tf.compat.v1.layers.experimental": {"tf.compat.v1.layers.experimental.keras_style_scope": {"description": "Use Keras-style variable management."}, "tf.compat.v1.layers.experimental.set_keras_style": {"description": "Use Keras-style variable management."}}, "tf.compat.v1.linalg.experimental": {}, "tf.compat.v1.lite.experimental": {"tf.compat.v1.lite.experimental.convert_op_hints_to_stubs": {"description": "Converts a graphdef with LiteOp hints into stub operations. (deprecated)", "Args": {"session": "A TensorFlow session that contains the graph to convert.", "graph_def": "A graph def that we should convert.", "write_callback": "A function pointer that can be used to write intermediate\nsteps of graph transformation (optional)."}, "Returns": "A new graphdef with all ops contained in OpHints being replaced by\na single op call with the right parameters.", "Raises": {"ValueError": "If both session and graph_def are provided."}}}, "tf.compat.v1.lookup.experimental": {}, "tf.compat.v1.mixed_precision.experimental": {}, "tf.compat.v1.mlir.experimental": {}, "tf.compat.v1.nn.experimental": {}, "tf.compat.v1.random.experimental": {}, "tf.compat.v1.saved_model.experimental": {}, "tf.compat.v1.tpu.experimental": {"tf.compat.v1.tpu.experimental.AdagradParameters": {"description": "Optimization parameters for Adagrad with TPU embeddings.", "Args": {"learning_rate": "used for updating embedding table.", "initial_accumulator": "initial accumulator for Adagrad.", "use_gradient_accumulation": "setting this to False makes embedding\ngradients calculation less accurate but faster. Please see\noptimization_parameters.proto for details.", "clip_weight_min": "the minimum value to clip by; None means -infinity.", "clip_weight_max": "the maximum value to clip by; None means +infinity.", "weight_decay_factor": "amount of weight decay to apply; None means that the\nweights are not decayed.", "multiply_weight_decay_factor_by_learning_rate": "if true,\nweight_decay_factor is multiplied by the current learning rate.", "clip_gradient_min": "the minimum value to clip by; None means -infinity.\nGradient accumulation must be set to true if this is set.", "clip_gradient_max": "the maximum value to clip by; None means +infinity.\nGradient accumulation must be set to true if this is set."}}, "tf.compat.v1.tpu.experimental.AdamParameters": {"description": "Optimization parameters for Adam with TPU embeddings.", "Args": {"learning_rate": "a floating point value. The learning rate.", "beta1": "A float value. The exponential decay rate for the 1st moment\nestimates.", "beta2": "A float value. The exponential decay rate for the 2nd moment\nestimates.", "epsilon": "A small constant for numerical stability.", "lazy_adam": "Use lazy Adam instead of Adam. Lazy Adam trains faster. See\noptimization_parameters.proto for details.", "sum_inside_sqrt": "This improves training speed. Please see\noptimization_parameters.proto for details.", "use_gradient_accumulation": "setting this to False makes embedding\ngradients calculation less accurate but faster. Please see\noptimization_parameters.proto for details.", "clip_weight_min": "the minimum value to clip by; None means -infinity.", "clip_weight_max": "the maximum value to clip by; None means +infinity.", "weight_decay_factor": "amount of weight decay to apply; None means that the\nweights are not decayed.", "multiply_weight_decay_factor_by_learning_rate": "if true,\nweight_decay_factor is multiplied by the current learning rate.", "clip_gradient_min": "the minimum value to clip by; None means -infinity.\nGradient accumulation must be set to true if this is set.", "clip_gradient_max": "the maximum value to clip by; None means +infinity.\nGradient accumulation must be set to true if this is set."}}, "tf.compat.v1.tpu.experimental.FtrlParameters": {"description": "Optimization parameters for Ftrl with TPU embeddings.", "Args": {"learning_rate": "a floating point value. The learning rate.", "learning_rate_power": "A float value, must be less or equal to zero.\nControls how the learning rate decreases during training. Use zero for a\nfixed learning rate. See section 3.1 in the\npaper.", "initial_accumulator_value": "The starting value for accumulators. Only zero\nor positive values are allowed.", "l1_regularization_strength": "A float value, must be greater than or equal\nto zero.", "l2_regularization_strength": "A float value, must be greater than or equal\nto zero.", "use_gradient_accumulation": "setting this to False makes embedding\ngradients calculation less accurate but faster. Please see\noptimization_parameters.proto for details. for details.", "clip_weight_min": "the minimum value to clip by; None means -infinity.", "clip_weight_max": "the maximum value to clip by; None means +infinity.", "weight_decay_factor": "amount of weight decay to apply; None means that the\nweights are not decayed.", "multiply_weight_decay_factor_by_learning_rate": "if true,\nweight_decay_factor is multiplied by the current learning rate.", "multiply_linear_by_learning_rate": "When true, multiplies the usages of the\nlinear slot in the weight update by the learning rate. This is useful\nwhen ramping up learning rate from 0 (which would normally produce\nNaNs).", "beta": "The beta parameter for FTRL.", "allow_zero_accumulator": "Changes the implementation of the square root to\nallow for the case of initial_accumulator_value being zero. This will\ncause a slight performance drop.", "clip_gradient_min": "the minimum value to clip by; None means -infinity.\nGradient accumulation must be set to true if this is set.", "clip_gradient_max": "the maximum value to clip by; None means +infinity.\nGradient accumulation must be set to true if this is set."}}, "tf.compat.v1.tpu.experimental.StochasticGradientDescentParameters": {"description": "Optimization parameters for stochastic gradient descent for TPU embeddings.", "Args": {"learning_rate": "a floating point value. The learning rate.", "use_gradient_accumulation": "setting this to False makes embedding\ngradients calculation less accurate but faster. Please see\noptimization_parameters.proto for details.", "clip_weight_min": "the minimum value to clip by; None means -infinity.", "clip_weight_max": "the maximum value to clip by; None means +infinity.", "weight_decay_factor": "amount of weight decay to apply; None means that the\nweights are not decayed.", "multiply_weight_decay_factor_by_learning_rate": "if true,\nweight_decay_factor is multiplied by the current learning rate.", "clip_gradient_min": "the minimum value to clip by; None means -infinity.", "clip_gradient_max": "the maximum value to clip by; None means +infinity."}}, "tf.compat.v1.tpu.experimental.embedding_column": {"description": "TPU version of tf.compat.v1.feature_column.embedding_column.", "Args": {"categorical_column": "A categorical column returned from\ncategorical_column_with_identity, weighted_categorical_column,\ncategorical_column_with_vocabulary_file,\ncategorical_column_with_vocabulary_list,\nsequence_categorical_column_with_identity,\nsequence_categorical_column_with_vocabulary_file,\nsequence_categorical_column_with_vocabulary_list", "dimension": "An integer specifying dimension of the embedding, must be > 0.", "combiner": "A string specifying how to reduce if there are multiple entries\nin a single row for a non-sequence column. For more information, see\ntf.feature_column.embedding_column.", "initializer": "A variable initializer function to be used in embedding\nvariable initialization. If not specified, defaults to\ntf.compat.v1.truncated_normal_initializer with mean 0.0 and\nstandard deviation 1/sqrt(dimension).", "max_sequence_length": "An non-negative integer specifying the max sequence\nlength. Any sequence shorter then this will be padded with 0 embeddings\nand any sequence longer will be truncated. This must be positive for\nsequence features and 0 for non-sequence features.", "learning_rate_fn": "A function that takes global step and returns learning\nrate for the embedding table. If you intend to use the same learning rate\nfor multiple embedding tables, please ensure that you pass the exact same\npython function to all calls of embedding_column, otherwise performence\nmay suffer.", "embedding_lookup_device": "The device on which to run the embedding lookup.\nValid options are \"cpu\", \"tpu_tensor_core\", and \"tpu_embedding_core\".\nIf specifying \"tpu_tensor_core\", a tensor_core_shape must be supplied.\nIf not specified, the default behavior is embedding lookup on\n\"tpu_embedding_core\" for training and \"cpu\" for inference.\nValid options for training : [\"tpu_embedding_core\", \"tpu_tensor_core\"]\nValid options for serving :  [\"cpu\", \"tpu_tensor_core\"]\nFor training, tpu_embedding_core is good for large embedding vocab (>1M),\notherwise, tpu_tensor_core is often sufficient.\nFor serving, doing embedding lookup on tpu_tensor_core during serving is\na way to reduce host cpu usage in cases where that is a bottleneck.", "tensor_core_shape": "If supplied, a list of integers which specifies\nthe intended dense shape to run embedding lookup for this feature on\nTensorCore. The batch dimension can be left None or -1 to indicate\na dynamic shape. Only rank 2 shapes currently supported.", "use_safe_embedding_lookup": "If true, uses safe_embedding_lookup_sparse\ninstead of embedding_lookup_sparse. safe_embedding_lookup_sparse ensures\nthere are no empty rows and all weights and ids are positive at the\nexpense of extra compute cost. This only applies to rank 2 (NxM) shaped\ninput tensors. Defaults to true, consider turning off if the above checks\nare not needed. Note that having empty rows will not trigger any error\nthough the output result might be 0 or omitted."}, "Returns": "A  _TPUEmbeddingColumnV2.", "Raises": {"ValueError": "if initializer is specified but not callable."}}, "tf.compat.v1.tpu.experimental.shared_embedding_columns": {"description": "TPU version of tf.compat.v1.feature_column.shared_embedding_columns.", "Args": {"categorical_columns": "A list of categorical columns returned from\ncategorical_column_with_identity, weighted_categorical_column,\ncategorical_column_with_vocabulary_file,\ncategorical_column_with_vocabulary_list,\nsequence_categorical_column_with_identity,\nsequence_categorical_column_with_vocabulary_file,\nsequence_categorical_column_with_vocabulary_list", "dimension": "An integer specifying dimension of the embedding, must be > 0.", "combiner": "A string specifying how to reduce if there are multiple entries in\na single row for a non-sequence column. For more information, see\ntf.feature_column.embedding_column.", "initializer": "A variable initializer function to be used in embedding\nvariable initialization. If not specified, defaults to\ntf.truncated_normal_initializer with mean 0.0 and standard deviation\n1/sqrt(dimension).", "shared_embedding_collection_name": "Optional name of the collection where\nshared embedding weights are added. If not given, a reasonable name will\nbe chosen based on the names of categorical_columns. This is also used\nin variable_scope when creating shared embedding weights.", "max_sequence_lengths": "An list of non-negative integers, either None or empty\nor the same length as the argument categorical_columns. Entries\ncorresponding to non-sequence columns must be 0 and entries corresponding\nto sequence columns specify the max sequence length for the column. Any\nsequence shorter then this will be padded with 0 embeddings and any\nsequence longer will be truncated.", "learning_rate_fn": "A function that takes global step and returns learning\nrate for the embedding table. If you intend to use the same learning rate\nfor multiple embedding tables, please ensure that you pass the exact same\npython function to all calls of shared_embedding_columns, otherwise\nperformence may suffer.", "embedding_lookup_device": "The device on which to run the embedding lookup.\nValid options are \"cpu\", \"tpu_tensor_core\", and \"tpu_embedding_core\". If\nspecifying \"tpu_tensor_core\", a tensor_core_shape must be supplied.\nDefaults to \"cpu\". If not specified, the default behavior is embedding\nlookup on \"tpu_embedding_core\" for training and \"cpu\" for inference.\nValid options for training : [\"tpu_embedding_core\", \"tpu_tensor_core\"]\nValid options for serving :  [\"cpu\", \"tpu_tensor_core\"]\nFor training, tpu_embedding_core is good for large embedding vocab (>1M),\notherwise, tpu_tensor_core is often sufficient.\nFor serving, doing embedding lookup on tpu_tensor_core during serving is\na way to reduce host cpu usage in cases where that is a bottleneck.", "tensor_core_shape": "If supplied, a list of integers which specifies the\nintended dense shape to run embedding lookup for this feature on\nTensorCore. The batch dimension can be left None or -1 to indicate a\ndynamic shape. Only rank 2 shapes currently supported.", "use_safe_embedding_lookup": "If true, uses safe_embedding_lookup_sparse\ninstead of embedding_lookup_sparse. safe_embedding_lookup_sparse ensures\nthere are no empty rows and all weights and ids are positive at the\nexpense of extra compute cost. This only applies to rank 2 (NxM) shaped\ninput tensors. Defaults to true, consider turning off if the above checks\nare not needed. Note that having empty rows will not trigger any error\nthough the output result might be 0 or omitted."}, "Returns": "A  list of _TPUSharedEmbeddingColumnV2.", "Raises": {"ValueError": "if max_sequence_lengths is positive for a non sequence column\nor 0 for a sequence column."}}}, "tf.compat.v1.train.experimental": {}, "tf.compat.v1.types.experimental": {}, "tf.compat.v1.xla.experimental": {}, "tf.config.experimental": {"tf.config.experimental.ClusterDeviceFilters": {"description": "Represent a collection of device filters for the remote workers in cluster."}, "tf.config.experimental.disable_mlir_bridge": {"description": "Disables experimental MLIR-Based TensorFlow Compiler Bridge."}, "tf.config.experimental.disable_mlir_graph_optimization": {"description": "Disables experimental MLIR-Based TensorFlow Compiler Optimizations."}, "tf.config.experimental.enable_mlir_bridge": {"description": "Enables experimental MLIR-Based TensorFlow Compiler Bridge."}, "tf.config.experimental.enable_mlir_graph_optimization": {"description": "Enables experimental MLIR-Based TensorFlow Compiler Optimizations."}, "tf.config.experimental.enable_op_determinism": {"description": "Configures TensorFlow ops to run deterministically."}, "tf.config.experimental.enable_tensor_float_32_execution": {"description": "Enable or disable the use of TensorFloat-32 on supported hardware.", "Args": {"enabled": "Bool indicating whether to enable TensorFloat-32 execution."}}, "tf.config.experimental.get_device_details": {"description": "Returns details about a physical devices.", "Args": {"device": "A tf.config.PhysicalDevice returned by\ntf.config.list_physical_devices or tf.config.get_visible_devices."}, "Returns": "A dict with string keys."}, "tf.config.experimental.get_device_policy": {"description": "Gets the current device policy.", "Returns": "Current thread device policy"}, "tf.config.experimental.get_memory_growth": {"description": "Get if memory growth is enabled for a PhysicalDevice.", "Args": {"device": "PhysicalDevice to query"}, "Returns": "A boolean indicating the memory growth setting for the PhysicalDevice.", "Raises": {"ValueError": "Invalid PhysicalDevice specified."}}, "tf.config.experimental.get_memory_info": {"description": "Get memory info for the chosen device, as a dict.", "Args": {"device": "Device string to get the memory information for, e.g. \"GPU:0\",\n\"TPU:0\". See https://www.tensorflow.org/api_docs/python/tf/device for\n  specifying device strings."}, "Returns": "A dict with keys 'current' and 'peak', specifying the current and peak\nmemory usage respectively.", "Raises": {"ValueError": "Memory statistics not tracked, like '\"CPU:0\"'."}}, "tf.config.experimental.get_memory_usage": {"description": "Get the current memory usage, in bytes, for the chosen device. (deprecated)", "Args": {"device": "Device string to get the bytes in use for, e.g. \"GPU:0\""}, "Returns": "Total memory usage in bytes.", "Raises": {"ValueError": "Non-existent or CPU device specified."}}, "tf.config.experimental.get_synchronous_execution": {"description": "Gets whether operations are executed synchronously or asynchronously.", "Returns": "Current thread execution mode"}, "tf.config.experimental.reset_memory_stats": {"description": "Resets the tracked memory stats for the chosen device.", "Args": {"device": "Device string to reset the memory stats, e.g. \"GPU:0\", \"TPU:0\".\nSee https://www.tensorflow.org/api_docs/python/tf/device for specifying\ndevice strings."}, "Raises": {"ValueError": "Memory statistics not tracked or clearing memory statistics not\nsupported, like '\"CPU:0\"'."}}, "tf.config.experimental.set_device_policy": {"description": "Sets the current thread device policy.", "Args": {"device_policy": "A device policy.\nValid values:\n\nNone: Switch to a system default.\n'warn': Copies the tensors which are not on the right device and logs a\nwarning.\n'explicit': Raises an error if the placement is not as required.\n'silent': Silently copies the tensors. Note that this may hide\nperformance problems as there is no notification provided when\noperations are blocked on the tensor being copied between devices.\n'silent_for_int32': silently copies int32 tensors, raising errors on\nthe other ones."}, "Raises": {"ValueError": "If an invalid device_policy is passed."}}, "tf.config.experimental.set_memory_growth": {"description": "Set if memory growth should be enabled for a PhysicalDevice.", "Args": {"device": "PhysicalDevice to configure", "enable": "(Boolean) Whether to enable or disable memory growth"}, "Raises": {"ValueError": "Invalid PhysicalDevice specified.", "RuntimeError": "Runtime is already initialized."}}, "tf.config.experimental.set_synchronous_execution": {"description": "Specifies whether operations are executed synchronously or asynchronously.", "Args": {"enable": "Whether operations should be dispatched synchronously.\nValid values:\n\nNone: sets the system default.\nTrue: executes each operation synchronously.\nFalse: executes each operation asynchronously."}}, "tf.config.experimental.tensor_float_32_execution_enabled": {"description": "Returns whether TensorFloat-32 is enabled.", "Returns": "True if TensorFloat-32 is enabled (the default) and False otherwise"}}, "tf.data.experimental": {"tf.data.experimental.AutoShardPolicy": {"description": "Represents the type of auto-sharding to use.", "Class Variables": {"AUTO": "<AutoShardPolicy.AUTO: 0>", "DATA": "<AutoShardPolicy.DATA: 2>", "FILE": "<AutoShardPolicy.FILE: 1>", "HINT": "<AutoShardPolicy.HINT: 3>", "OFF": "<AutoShardPolicy.OFF: -1>"}}, "tf.data.experimental.AutotuneAlgorithm": {"description": "Represents the type of autotuning algorithm to use.", "Class Variables": {"DEFAULT": "<AutotuneAlgorithm.DEFAULT: 0>", "GRADIENT_DESCENT": "<AutotuneAlgorithm.GRADIENT_DESCENT: 2>", "HILL_CLIMB": "<AutotuneAlgorithm.HILL_CLIMB: 1>", "MAX_PARALLELISM": "<AutotuneAlgorithm.MAX_PARALLELISM: 3>"}}, "tf.data.experimental.AutotuneOptions": {"description": "Represents options for autotuning dataset performance.", "Attributes": {"autotune_algorithm": "When autotuning is enabled (through autotune), determines the algorithm to use.", "cpu_budget": "When autotuning is enabled (through autotune), determines the CPU budget to use. Values greater than the number of schedulable CPU cores are allowed but may result in CPU contention. If None, defaults to the number of schedulable CPU cores.", "enabled": "Whether to automatically tune performance knobs. If None, defaults to True.", "ram_budget": "When autotuning is enabled (through autotune), determines the RAM budget to use. Values greater than the available RAM in bytes may result in OOM. If None, defaults to half of the available RAM in bytes."}}, "tf.data.experimental.CheckpointInputPipelineHook": {"description": "Checkpoints input pipeline state every N steps or seconds.", "Args": {"estimator": "Estimator.", "external_state_policy": "A string that identifies how to handle input\npipelines that depend on external state. Possible values are\n'ignore': The external state is silently ignored.\n'warn': The external state is ignored, logging a warning.\n'fail': The operation fails upon encountering external state.\nBy default we set it to 'fail'."}, "Raises": {"ValueError": "If external_state_policy is not one of 'warn', 'ignore' or\n'fail'."}}, "tf.data.experimental.Counter": {"description": "Creates a Dataset that counts from start in steps of size step.", "Args": {"start": "(Optional.) The starting value for the counter. Defaults to 0.", "step": "(Optional.) The step size for the counter. Defaults to 1.", "dtype": "(Optional.) The data type for counter elements. Defaults to\ntf.int64."}, "Returns": "A Dataset of scalar dtype elements."}, "tf.data.experimental.CsvDataset": {"description": "A Dataset comprising lines from one or more CSV files.", "Args": {"filenames": "A tf.string tensor containing one or more filenames.", "record_defaults": "A list of default values for the CSV fields. Each item in\nthe list is either a valid CSV DType (float32, float64, int32, int64,\nstring), or a Tensor object with one of the above types. One per\ncolumn of CSV data, with either a scalar Tensor default value for the\ncolumn if it is optional, or DType or empty Tensor if required. If\nboth this and select_columns are specified, these must have the same\nlengths, and column_defaults is assumed to be sorted in order of\nincreasing column index. If both this and 'exclude_cols' are specified,\nthe sum of lengths of record_defaults and exclude_cols should equal\nthe total number of columns in the CSV file.", "compression_type": "(Optional.) A tf.string scalar evaluating to one of\n\"\" (no compression), \"ZLIB\", or \"GZIP\". Defaults to no\ncompression.", "buffer_size": "(Optional.) A tf.int64 scalar denoting the number of bytes\nto buffer while reading files. Defaults to 4MB.", "header": "(Optional.) A tf.bool scalar indicating whether the CSV file(s)\nhave header line(s) that should be skipped when parsing. Defaults to\nFalse.", "field_delim": "(Optional.) A tf.string scalar containing the delimiter\ncharacter that separates fields in a record. Defaults to \",\".", "use_quote_delim": "(Optional.) A tf.bool scalar. If False, treats\ndouble quotation marks as regular characters inside of string fields\n(ignoring RFC 4180, Section 2, Bullet 5). Defaults to True.", "na_value": "(Optional.) A tf.string scalar indicating a value that will\nbe treated as NA/NaN.", "select_cols": "(Optional.) A sorted list of column indices to select from\nthe input data. If specified, only this subset of columns will be\nparsed. Defaults to parsing all columns. At most one of select_cols\nand exclude_cols can be specified.", "exclude_cols": "(Optional.) A sorted list of column indices to exclude from\nthe input data. If specified, only the complement of this set of column\nwill be parsed. Defaults to parsing all columns. At most one of\nselect_cols and exclude_cols can be specified."}, "Raises": {"InvalidArgumentError": "If exclude_cols is not None and\nlen(exclude_cols) + len(record_defaults) does not match the total\nnumber of columns in the file(s)"}, "Attributes": {"element_spec": "The type specification of an element of this dataset.\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])dataset.element_specTensorSpec(shape=(), dtype=tf.int32, name=None)\nFor more information,\nread this guide."}}, "tf.data.experimental.DatasetInitializer": {"description": "Creates a table initializer from a tf.data.Dataset.", "Args": {"dataset": "A tf.data.Dataset object that produces tuples of scalars. The\nfirst scalar is treated as a key and the second as value."}, "Attributes": {"dataset": "A tf.data.Dataset object that produces tuples of scalars. The\nfirst scalar is treated as a key and the second as value.", "key_dtype": "The expected table key dtype.", "value_dtype": "The expected table value dtype."}}, "tf.data.experimental.DistributeOptions": {"description": "Represents options for distributed data processing.", "Attributes": {"auto_shard_policy": "The type of sharding to use. See tf.data.experimental.AutoShardPolicy for additional information.", "num_devices": "The number of devices attached to this input pipeline. This will be automatically set by MultiDeviceIterator."}}, "tf.data.experimental.ExternalStatePolicy": {"description": "Represents how to handle external state during serialization.", "Class Variables": {"FAIL": "<ExternalStatePolicy.FAIL: 2>", "IGNORE": "<ExternalStatePolicy.IGNORE: 1>", "WARN": "<ExternalStatePolicy.WARN: 0>"}}, "tf.data.experimental.OptimizationOptions": {"description": "Represents options for dataset optimizations.", "Attributes": {"apply_default_optimizations": "Whether to apply default graph optimizations. If False, only graph optimizations that have been explicitly enabled will be applied.", "filter_fusion": "Whether to fuse filter transformations. If None, defaults to False.", "filter_parallelization": "Whether to parallelize stateless filter transformations. If None, defaults to False.", "map_and_batch_fusion": "Whether to fuse map and batch transformations. If None, defaults to True.", "map_and_filter_fusion": "Whether to fuse map and filter transformations. If None, defaults to False.", "map_fusion": "Whether to fuse map transformations. If None, defaults to False.", "map_parallelization": "Whether to parallelize stateless map transformations. If None, defaults to True.", "noop_elimination": "Whether to eliminate no-op transformations. If None, defaults to True.", "parallel_batch": "Whether to parallelize copying of batch elements. If None, defaults to True.", "shuffle_and_repeat_fusion": "Whether to fuse shuffle and repeat transformations. If None, defaults to True."}}, "tf.data.experimental.RandomDataset": {"description": "A Dataset of pseudorandom values. (deprecated)", "Attributes": {"element_spec": "The type specification of an element of this dataset.\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])dataset.element_specTensorSpec(shape=(), dtype=tf.int32, name=None)\nFor more information,\nread this guide."}}, "tf.data.experimental.Reducer": {"description": "A reducer is used for reducing a set of elements.", "Attributes": {"finalize_func": "", "init_func": "", "reduce_func": ""}}, "tf.data.experimental.SqlDataset": {"description": "A Dataset consisting of the results from a SQL query.", "Args": {"driver_name": "A 0-D tf.string tensor containing the database type.\nCurrently, the only supported value is 'sqlite'.", "data_source_name": "A 0-D tf.string tensor containing a connection string\nto connect to the database.", "query": "A 0-D tf.string tensor containing the SQL query to execute.", "output_types": "A tuple of tf.DType objects representing the types of the\ncolumns returned by query."}, "Attributes": {"element_spec": "The type specification of an element of this dataset.\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])dataset.element_specTensorSpec(shape=(), dtype=tf.int32, name=None)\nFor more information,\nread this guide."}}, "tf.data.experimental.TFRecordWriter": {"description": "Writes a dataset to a TFRecord file. (deprecated)", "Args": {"filename": "a string path indicating where to write the TFRecord data.", "compression_type": "(Optional.) a string indicating what type of compression\nto use when writing the file. See tf.io.TFRecordCompressionType for\nwhat types of compression are available. Defaults to None."}}, "tf.data.experimental.assert_cardinality": {"description": "Asserts the cardinality of the input dataset.", "Args": {"expected_cardinality": "The expected cardinality of the input dataset."}, "Returns": "A Dataset transformation function, which can be passed to\ntf.data.Dataset.apply.", "Raises": {"FailedPreconditionError": "The assertion is checked at runtime (when iterating\nthe dataset) and an error is raised if the actual and expected cardinality\ndiffer."}}, "tf.data.experimental.bucket_by_sequence_length": {"description": "A transformation that buckets elements in a Dataset by length. (deprecated)", "Args": {"element_length_func": "function from element in Dataset to tf.int32,\ndetermines the length of the element, which will determine the bucket it\ngoes into.", "bucket_boundaries": "list<int>, upper length boundaries of the buckets.", "bucket_batch_sizes": "list<int>, batch size per bucket. Length should be\nlen(bucket_boundaries) + 1.", "padded_shapes": "Nested structure of tf.TensorShape to pass to\ntf.data.Dataset.padded_batch. If not provided, will use\ndataset.output_shapes, which will result in variable length dimensions\nbeing padded out to the maximum length in each batch.", "padding_values": "Values to pad with, passed to\ntf.data.Dataset.padded_batch. Defaults to padding with 0.", "pad_to_bucket_boundary": "bool, if False, will pad dimensions with unknown\nsize to maximum length in batch. If True, will pad dimensions with\nunknown size to bucket boundary minus 1 (i.e., the maximum length in each\nbucket), and caller must ensure that the source Dataset does not contain\nany elements with length longer than max(bucket_boundaries).", "no_padding": "bool, indicates whether to pad the batch features (features\nneed to be either of type tf.sparse.SparseTensor or of same shape).", "drop_remainder": "(Optional.) A tf.bool scalar tf.Tensor, representing\nwhether the last batch should be dropped in the case it has fewer than\nbatch_size elements; the default behavior is not to drop the smaller\nbatch."}, "Returns": "A Dataset transformation function, which can be passed to\ntf.data.Dataset.apply.", "Raises": {"ValueError": "if len(bucket_batch_sizes) != len(bucket_boundaries) + 1."}}, "tf.data.experimental.cardinality": {"description": "Returns the cardinality of dataset, if known.", "Args": {"dataset": "A tf.data.Dataset for which to determine cardinality."}, "Returns": "A scalar tf.int64 Tensor representing the cardinality of dataset. If\nthe cardinality is infinite or unknown, the operation returns the named\nconstant INFINITE_CARDINALITY and UNKNOWN_CARDINALITY respectively."}, "tf.data.experimental.choose_from_datasets": {"description": "Creates a dataset that deterministically chooses elements from datasets. (deprecated)", "Args": {"datasets": "A non-empty list of tf.data.Dataset objects with compatible\nstructure.", "choice_dataset": "A tf.data.Dataset of scalar tf.int64 tensors between 0\nand len(datasets) - 1.", "stop_on_empty_dataset": "If True, selection stops if it encounters an empty\ndataset. If False, it skips empty datasets. It is recommended to set it\nto True. Otherwise, the selected elements start off as the user intends,\nbut may change as input datasets become empty. This can be difficult to\ndetect since the dataset starts off looking correct. Default to False\nfor backward compatibility."}, "Returns": "A dataset that interleaves elements from datasets according to the values\nof choice_dataset.", "Raises": {"TypeError": "If datasets or choice_dataset has the wrong type.", "ValueError": "If datasets is empty."}}, "tf.data.experimental.copy_to_device": {"description": "A transformation that copies dataset elements to the given target_device.", "Args": {"target_device": "The name of a device to which elements will be copied.", "source_device": "The original device on which input_dataset will be placed."}, "Returns": "A Dataset transformation function, which can be passed to\ntf.data.Dataset.apply."}, "tf.data.experimental.dense_to_ragged_batch": {"description": "A transformation that batches ragged elements into tf.RaggedTensors.", "Args": {"batch_size": "A tf.int64 scalar tf.Tensor, representing the number of\nconsecutive elements of this dataset to combine in a single batch.", "drop_remainder": "(Optional.) A tf.bool scalar tf.Tensor, representing\nwhether the last batch should be dropped in the case it has fewer than\nbatch_size elements; the default behavior is not to drop the smaller\nbatch.", "row_splits_dtype": "The dtype that should be used for the row_splits of any\nnew ragged tensors.  Existing tf.RaggedTensor elements do not have their\nrow_splits dtype changed."}, "Returns": "Dataset\n\n\nA Dataset."}, "tf.data.experimental.dense_to_sparse_batch": {"description": "A transformation that batches ragged elements into tf.sparse.SparseTensors.", "Args": {"batch_size": "A tf.int64 scalar tf.Tensor, representing the number of\nconsecutive elements of this dataset to combine in a single batch.", "row_shape": "A tf.TensorShape or tf.int64 vector tensor-like object\nrepresenting the equivalent dense shape of a row in the resulting\ntf.sparse.SparseTensor. Each element of this dataset must have the same\nrank as row_shape, and must have size less than or equal to row_shape\nin each dimension."}, "Returns": "A Dataset transformation function, which can be passed to\ntf.data.Dataset.apply."}, "tf.data.experimental.enable_debug_mode": {"description": "Enables debug mode for tf.data.", "Raises": {"ValueError": "When invoked from graph mode."}}, "tf.data.experimental.enumerate_dataset": {"description": "A transformation that enumerates the elements of a dataset. (deprecated)", "Args": {"start": "A tf.int64 scalar tf.Tensor, representing the start value for\nenumeration."}, "Returns": "A Dataset transformation function, which can be passed to\ntf.data.Dataset.apply."}, "tf.data.experimental.from_variant": {"description": "Constructs a dataset from the given variant and (nested) structure.", "Args": {"variant": "A scalar tf.variant tensor representing a dataset.", "structure": "A (nested) structure of tf.TypeSpec objects representing the\nstructure of each element in the dataset."}, "Returns": "A tf.data.Dataset instance."}, "tf.data.experimental.get_next_as_optional": {"description": "Returns a tf.experimental.Optional with the next element of the iterator. (deprecated)", "Args": {"iterator": "A tf.data.Iterator."}, "Returns": "A tf.experimental.Optional object which either contains the next element\nof the iterator (if it exists) or no value."}, "tf.data.experimental.get_single_element": {"description": "Returns the single element of the dataset as a nested structure of tensors. (deprecated)", "Args": {"dataset": "A tf.data.Dataset object containing a single element."}, "Returns": "A nested structure of tf.Tensor objects, corresponding to the single\nelement of dataset.", "Raises": {"TypeError": "if dataset is not a tf.data.Dataset object.", "InvalidArgumentError": "(at runtime) if dataset does not contain exactly\none element."}}, "tf.data.experimental.get_structure": {"description": "Returns the type signature for elements of the input dataset / iterator.", "Args": {"dataset_or_iterator": "A tf.data.Dataset or an tf.data.Iterator."}, "Returns": "A (nested) structure of tf.TypeSpec objects matching the structure of an\nelement of dataset_or_iterator and specifying the type of individual\ncomponents.", "Raises": {"TypeError": "If input is not a tf.data.Dataset or an tf.data.Iterator\nobject."}}, "tf.data.experimental.group_by_reducer": {"description": "A transformation that groups elements and performs a reduction.", "Args": {"key_func": "A function mapping a nested structure of tensors\n(having shapes and types defined by self.output_shapes and\nself.output_types) to a scalar tf.int64 tensor.", "reducer": "An instance of Reducer, which captures the reduction logic using\nthe init_func, reduce_func, and finalize_func functions."}, "Returns": "A Dataset transformation function, which can be passed to\ntf.data.Dataset.apply."}, "tf.data.experimental.group_by_window": {"description": "A transformation that groups windows of elements by key and reduces them. (deprecated)", "Args": {"key_func": "A function mapping a nested structure of tensors\n(having shapes and types defined by self.output_shapes and\nself.output_types) to a scalar tf.int64 tensor.", "reduce_func": "A function mapping a key and a dataset of up to window_size\nconsecutive elements matching that key to another dataset.", "window_size": "A tf.int64 scalar tf.Tensor, representing the number of\nconsecutive elements matching the same key to combine in a single\nbatch, which will be passed to reduce_func. Mutually exclusive with\nwindow_size_func.", "window_size_func": "A function mapping a key to a tf.int64 scalar\ntf.Tensor, representing the number of consecutive elements matching\nthe same key to combine in a single batch, which will be passed to\nreduce_func. Mutually exclusive with window_size."}, "Returns": "A Dataset transformation function, which can be passed to\ntf.data.Dataset.apply.", "Raises": {"ValueError": "if neither or both of {window_size, window_size_func} are\npassed."}}, "tf.data.experimental.ignore_errors": {"description": "Creates a Dataset from another Dataset and silently ignores any errors.", "Returns": "A Dataset transformation function, which can be passed to\ntf.data.Dataset.apply."}, "tf.data.experimental.index_table_from_dataset": {"description": "Returns an index lookup table based on the given dataset.", "Args": {"dataset": "A dataset of keys.", "num_oov_buckets": "The number of out-of-vocabulary buckets.", "vocab_size": "Number of the elements in the vocabulary, if known.", "default_value": "The value to use for out-of-vocabulary feature values.\nDefaults to -1.", "hasher_spec": "A HasherSpec to specify the hash function to use for\nassignation of out-of-vocabulary buckets.", "key_dtype": "The key data type.", "name": "A name for this op (optional)."}, "Returns": "The lookup table based on the given dataset.", "Raises": {"ValueError": "If\n\nnum_oov_buckets is negative\nvocab_size is not greater than zero\nThe key_dtype is not integer or string"}}, "tf.data.experimental.load": {"description": "Loads a previously saved dataset.", "Args": {"path": "Required. A path pointing to a previously saved dataset.", "element_spec": "Optional. A nested structure of tf.TypeSpec objects matching\nthe structure of an element of the saved dataset and specifying the type\nof individual element components. If not provided, the nested structure of\ntf.TypeSpec saved with the saved dataset is used. This argument needs to\nbe provided if the method is executed in graph mode.", "compression": "Optional. The algorithm to use to decompress the data when\nreading it. Supported options are GZIP and NONE. Defaults to NONE.", "reader_func": "Optional. A function to control how to read data from shards.\nIf present, the function will be traced and executed as graph computation."}, "Returns": "A tf.data.Dataset instance.", "Raises": {"FileNotFoundError": "If element_spec is not specified and the saved nested\nstructure of tf.TypeSpec can not be located with the saved dataset.", "ValueError": "If element_spec is not specified and the method is executed\nin graph mode."}}, "tf.data.experimental.make_batched_features_dataset": {"description": "Returns a Dataset of feature dictionaries from Example protos.", "Args": {"file_pattern": "List of files or patterns of file paths containing\nExample records. See tf.io.gfile.glob for pattern rules.", "batch_size": "An int representing the number of records to combine\nin a single batch.", "features": "A dict mapping feature keys to FixedLenFeature or\nVarLenFeature values. See tf.io.parse_example.", "reader": "A function or class that can be\ncalled with a filenames tensor and (optional) reader_args and returns\na Dataset of Example tensors. Defaults to tf.data.TFRecordDataset.", "label_key": "(Optional) A string corresponding to the key labels are stored in\ntf.Examples. If provided, it must be one of the features key,\notherwise results in ValueError.", "reader_args": "Additional arguments to pass to the reader class.", "num_epochs": "Integer specifying the number of times to read through the\ndataset. If None, cycles through the dataset forever. Defaults to None.", "shuffle": "A boolean, indicates whether the input should be shuffled. Defaults\nto True.", "shuffle_buffer_size": "Buffer size of the ShuffleDataset. A large capacity\nensures better shuffling but would increase memory usage and startup time.", "shuffle_seed": "Randomization seed to use for shuffling.", "prefetch_buffer_size": "Number of feature batches to prefetch in order to\nimprove performance. Recommended value is the number of batches consumed\nper training step. Defaults to auto-tune.", "reader_num_threads": "Number of threads used to read Example records. If >1,\nthe results will be interleaved. Defaults to 1.", "parser_num_threads": "Number of threads to use for parsing Example tensors\ninto a dictionary of Feature tensors. Defaults to 2.", "sloppy_ordering": "If True, reading performance will be improved at\nthe cost of non-deterministic ordering. If False, the order of elements\nproduced is deterministic prior to shuffling (elements are still\nrandomized if shuffle=True. Note that if the seed is set, then order\nof elements after shuffling is deterministic). Defaults to False.", "drop_final_batch": "If True, and the batch size does not evenly divide the\ninput dataset size, the final smaller batch will be dropped. Defaults to\nFalse."}, "Returns": "A dataset of dict elements, (or a tuple of dict elements and label).\nEach dict maps feature keys to Tensor or SparseTensor objects.", "Raises": {"TypeError": "If reader is of the wrong type.", "ValueError": "If label_key is not one of the features keys."}}, "tf.data.experimental.make_csv_dataset": {"description": "Reads CSV files into a dataset.", "Args": {"file_pattern": "List of files or patterns of file paths containing CSV\nrecords. See tf.io.gfile.glob for pattern rules.", "batch_size": "An int representing the number of records to combine\nin a single batch.", "column_names": "An optional list of strings that corresponds to the CSV\ncolumns, in order. One per column of the input record. If this is not\nprovided, infers the column names from the first row of the records.\nThese names will be the keys of the features dict of each dataset element.", "column_defaults": "A optional list of default values for the CSV fields. One\nitem per selected column of the input record. Each item in the list is\neither a valid CSV dtype (float32, float64, int32, int64, or string), or a\nTensor with one of the aforementioned types. The tensor can either be\na scalar default value (if the column is optional), or an empty tensor (if\nthe column is required). If a dtype is provided instead of a tensor, the\ncolumn is also treated as required. If this list is not provided, tries\nto infer types based on reading the first num_rows_for_inference rows of\nfiles specified, and assumes all columns are optional, defaulting to 0\nfor numeric values and \"\" for string values. If both this and\nselect_columns are specified, these must have the same lengths, and\ncolumn_defaults is assumed to be sorted in order of increasing column\nindex.", "label_name": "A optional string corresponding to the label column. If\nprovided, the data for this column is returned as a separate Tensor from\nthe features dictionary, so that the dataset complies with the format\nexpected by a tf.Estimator.train or tf.Estimator.evaluate input\nfunction.", "select_columns": "An optional list of integer indices or string column\nnames, that specifies a subset of columns of CSV data to select. If\ncolumn names are provided, these must correspond to names provided in\ncolumn_names or inferred from the file header lines. When this argument\nis specified, only a subset of CSV columns will be parsed and returned,\ncorresponding to the columns specified. Using this results in faster\nparsing and lower memory usage. If both this and column_defaults are\nspecified, these must have the same lengths, and column_defaults is\nassumed to be sorted in order of increasing column index.", "field_delim": "An optional string. Defaults to \",\". Char delimiter to\nseparate fields in a record.", "use_quote_delim": "An optional bool. Defaults to True. If false, treats\ndouble quotation marks as regular characters inside of the string fields.", "na_value": "Additional string to recognize as NA/NaN.", "header": "A bool that indicates whether the first rows of provided CSV files\ncorrespond to header lines with column names, and should not be included\nin the data.", "num_epochs": "An int specifying the number of times this dataset is repeated.\nIf None, cycles through the dataset forever.", "shuffle": "A bool that indicates whether the input should be shuffled.", "shuffle_buffer_size": "Buffer size to use for shuffling. A large buffer size\nensures better shuffling, but increases memory usage and startup time.", "shuffle_seed": "Randomization seed to use for shuffling.", "prefetch_buffer_size": "An int specifying the number of feature\nbatches to prefetch for performance improvement. Recommended value is the\nnumber of batches consumed per training step. Defaults to auto-tune.", "num_parallel_reads": "Number of threads used to read CSV records from files.\nIf >1, the results will be interleaved. Defaults to 1.", "sloppy": "If True, reading performance will be improved at\nthe cost of non-deterministic ordering. If False, the order of elements\nproduced is deterministic prior to shuffling (elements are still\nrandomized if shuffle=True. Note that if the seed is set, then order\nof elements after shuffling is deterministic). Defaults to False.", "num_rows_for_inference": "Number of rows of a file to use for type inference\nif record_defaults is not provided. If None, reads all the rows of all\nthe files. Defaults to 100.", "compression_type": "(Optional.) A tf.string scalar evaluating to one of\n\"\" (no compression), \"ZLIB\", or \"GZIP\". Defaults to no compression.", "ignore_errors": "(Optional.) If True, ignores errors with CSV file parsing,\nsuch as malformed data or empty lines, and moves on to the next valid\nCSV record. Otherwise, the dataset raises an error and stops processing\nwhen encountering any invalid records. Defaults to False."}, "Returns": "A dataset, where each element is a (features, labels) tuple that corresponds\nto a batch of batch_size CSV rows. The features dictionary maps feature\ncolumn names to Tensors containing the corresponding column data, and\nlabels is a Tensor containing the column data for the label column\nspecified by label_name.", "Raises": {"ValueError": "If any of the arguments is malformed."}}, "tf.data.experimental.make_saveable_from_iterator": {"description": "Returns a SaveableObject for saving/restoring iterator state using Saver. (deprecated)", "Args": {"iterator": "Iterator.", "external_state_policy": "A string that identifies how to handle input\npipelines that depend on external state. Possible values are\n'ignore': The external state is silently ignored.\n'warn': The external state is ignored, logging a warning.\n'fail': The operation fails upon encountering external state.\nBy default we set it to 'fail'."}, "Returns": "A SaveableObject for saving/restoring iterator state using Saver.", "Raises": {"ValueError": "If external_state_policy is not one of 'warn', 'ignore' or\n'fail'."}}, "tf.data.experimental.map_and_batch": {"description": "Fused implementation of map and batch. (deprecated)", "Args": {"map_func": "A function mapping a nested structure of tensors to another\nnested structure of tensors.", "batch_size": "A tf.int64 scalar tf.Tensor, representing the number of\nconsecutive elements of this dataset to combine in a single batch.", "num_parallel_batches": "(Optional.) A tf.int64 scalar tf.Tensor,\nrepresenting the number of batches to create in parallel. On one hand,\nhigher values can help mitigate the effect of stragglers. On the other\nhand, higher values can increase contention if CPU is scarce.", "drop_remainder": "(Optional.) A tf.bool scalar tf.Tensor, representing\nwhether the last batch should be dropped in case its size is smaller than\ndesired; the default behavior is not to drop the smaller batch.", "num_parallel_calls": "(Optional.) A tf.int32 scalar tf.Tensor,\nrepresenting the number of elements to process in parallel. If not\nspecified, batch_size * num_parallel_batches elements will be processed\nin parallel. If the value tf.data.AUTOTUNE is used, then\nthe number of parallel calls is set dynamically based on available CPU."}, "Returns": "A Dataset transformation function, which can be passed to\ntf.data.Dataset.apply.", "Raises": {"ValueError": "If both num_parallel_batches and num_parallel_calls are\nspecified."}}, "tf.data.experimental.parallel_interleave": {"description": "A parallel version of the Dataset.interleave() transformation. (deprecated)", "Args": {"map_func": "A function mapping a nested structure of tensors to a Dataset.", "cycle_length": "The number of input Datasets to interleave from in parallel.", "block_length": "The number of consecutive elements to pull from an input\nDataset before advancing to the next input Dataset.", "sloppy": "A boolean controlling whether determinism should be traded for\nperformance by allowing elements to be produced out of order.  If sloppy\nis None, the tf.data.Options.deterministic dataset option (True by\ndefault) is used to decide whether to enforce a deterministic order.", "buffer_output_elements": "The number of elements each iterator being\ninterleaved should buffer (similar to the .prefetch() transformation for\neach interleaved iterator).", "prefetch_input_elements": "The number of input elements to transform to\niterators before they are needed for interleaving."}, "Returns": "A Dataset transformation function, which can be passed to\ntf.data.Dataset.apply."}, "tf.data.experimental.parse_example_dataset": {"description": "A transformation that parses Example protos into a dict of tensors.", "Args": {"features": "A dict mapping feature keys to FixedLenFeature,\nVarLenFeature, RaggedFeature, and SparseFeature values.", "num_parallel_calls": "(Optional.) A tf.int32 scalar tf.Tensor,\nrepresenting the number of parsing processes to call in parallel.", "deterministic": "(Optional.) A boolean controlling whether determinism\nshould be traded for performance by allowing elements to be produced out\nof order if some parsing calls complete faster than others. If\ndeterministic is None, the\ntf.data.Options.deterministic dataset option (True by default) is used\nto decide whether to produce elements deterministically."}, "Returns": "A dataset transformation function, which can be passed to\ntf.data.Dataset.apply.", "Raises": {"ValueError": "if features argument is None."}}, "tf.data.experimental.prefetch_to_device": {"description": "A transformation that prefetches dataset values to the given device.", "Args": {"device": "A string. The name of a device to which elements will be prefetched.", "buffer_size": "(Optional.) The number of elements to buffer on device.\nDefaults to an automatically chosen value."}, "Returns": "A Dataset transformation function, which can be passed to\ntf.data.Dataset.apply."}, "tf.data.experimental.rejection_resample": {"description": "A transformation that resamples a dataset to achieve a target distribution. (deprecated)", "Args": {"class_func": "A function mapping an element of the input dataset to a scalar\ntf.int32 tensor. Values should be in [0, num_classes).", "target_dist": "A floating point type tensor, shaped [num_classes].", "initial_dist": "(Optional.)  A floating point type tensor, shaped\n[num_classes].  If not provided, the true class distribution is\nestimated live in a streaming fashion.", "seed": "(Optional.) Python integer seed for the resampler."}, "Returns": "A Dataset transformation function, which can be passed to\ntf.data.Dataset.apply."}, "tf.data.experimental.sample_from_datasets": {"description": "Samples elements at random from the datasets in datasets. (deprecated)", "Args": {"datasets": "A non-empty list of tf.data.Dataset objects with compatible\nstructure.", "weights": "(Optional.) A list or Tensor of len(datasets) floating-point\nvalues where weights[i] represents the probability to sample from\ndatasets[i], or a tf.data.Dataset object where each element is such a\nlist. Defaults to a uniform distribution across datasets.", "seed": "(Optional.) A tf.int64 scalar tf.Tensor, representing the random\nseed that will be used to create the distribution. See\ntf.random.set_seed for behavior.", "stop_on_empty_dataset": "If True, sampling stops if it encounters an empty\ndataset. If False, it skips empty datasets. It is recommended to set it\nto True. Otherwise, the distribution of samples starts off as the user\nintends, but may change as input datasets become empty. This can be\ndifficult to detect since the dataset starts off looking correct. Default\nto False for backward compatibility."}, "Returns": "A dataset that interleaves elements from datasets at random, according to\nweights if provided, otherwise with uniform probability.", "Raises": {"TypeError": "If the datasets or weights arguments have the wrong type.", "ValueError": "If datasets is empty, or\nIf weights is specified and does not match the length of datasets."}}, "tf.data.experimental.save": {"description": "Saves the content of the given dataset.", "Args": {"dataset": "The dataset to save.", "path": "Required. A directory to use for saving the dataset.", "compression": "Optional. The algorithm to use to compress data when writing\nit. Supported options are GZIP and NONE. Defaults to NONE.", "shard_func": "Optional. A function to control the mapping of dataset elements\nto file shards. The function is expected to map elements of the input\ndataset to int64 shard IDs. If present, the function will be traced and\nexecuted as graph computation.", "checkpoint_args": "Optional args for checkpointing which will be passed into\nthe tf.train.CheckpointManager. If checkpoint_args are not specified,\nthen checkpointing will not be performed. The save() implementation\ncreates a tf.train.Checkpoint object internally, so users should not\nset the checkpoint argument in checkpoint_args."}, "Raises": {}}, "tf.data.experimental.scan": {"description": "A transformation that scans a function across an input dataset. (deprecated)", "Args": {"initial_state": "A nested structure of tensors, representing the initial state\nof the accumulator.", "scan_func": "A function that maps (old_state, input_element) to\n(new_state, output_element). It must take two arguments and return a\npair of nested structures of tensors. The new_state must match the\nstructure of initial_state."}, "Returns": "A Dataset transformation function, which can be passed to\ntf.data.Dataset.apply."}, "tf.data.experimental.shuffle_and_repeat": {"description": "Shuffles and repeats a Dataset, reshuffling with each repetition. (deprecated)", "Args": {"buffer_size": "A tf.int64 scalar tf.Tensor, representing the maximum\nnumber elements that will be buffered when prefetching.", "count": "(Optional.) A tf.int64 scalar tf.Tensor, representing the number\nof times the dataset should be repeated. The default behavior (if count\nis None or -1) is for the dataset be repeated indefinitely.", "seed": "(Optional.) A tf.int64 scalar tf.Tensor, representing the random\nseed that will be used to create the distribution. See\ntf.random.set_seed for behavior."}, "Returns": "A Dataset transformation function, which can be passed to\ntf.data.Dataset.apply."}, "tf.data.experimental.snapshot": {"description": "API to persist the output of the input dataset. (deprecated)", "Args": {"path": "Required. A directory to use for storing / loading the snapshot to /\nfrom.", "compression": "Optional. The type of compression to apply to the snapshot\nwritten to disk. Supported options are GZIP, SNAPPY, AUTO or None.\nDefaults to AUTO, which attempts to pick an appropriate compression\nalgorithm for the dataset.", "reader_func": "Optional. A function to control how to read data from snapshot\nshards.", "shard_func": "Optional. A function to control how to shard data when writing a\nsnapshot."}, "Returns": "A Dataset transformation function, which can be passed to\ntf.data.Dataset.apply."}, "tf.data.experimental.table_from_dataset": {"description": "Returns a lookup table based on the given dataset.", "Args": {"dataset": "A dataset containing (key, value) pairs.", "num_oov_buckets": "The number of out-of-vocabulary buckets.", "vocab_size": "Number of the elements in the vocabulary, if known.", "default_value": "The value to use for out-of-vocabulary feature values.\nDefaults to -1.", "hasher_spec": "A HasherSpec to specify the hash function to use for\nassignation of out-of-vocabulary buckets.", "key_dtype": "The key data type.", "name": "A name for this op (optional)."}, "Returns": "The lookup table based on the given dataset.", "Raises": {"ValueError": "If\n\ndataset does not contain pairs\nThe 2nd item in the dataset pairs has a dtype which is incompatible\nwith default_value\nnum_oov_buckets is negative\nvocab_size is not greater than zero\nThe key_dtype is not integer or string"}}, "tf.data.experimental.take_while": {"description": "A transformation that stops dataset iteration based on a predicate. (deprecated)", "Args": {"predicate": "A function that maps a nested structure of tensors (having shapes\nand types defined by self.output_shapes and self.output_types) to a\nscalar tf.bool tensor."}, "Returns": "A Dataset transformation function, which can be passed to\ntf.data.Dataset.apply."}, "tf.data.experimental.to_variant": {"description": "Returns a variant representing the given dataset.", "Args": {"dataset": "A tf.data.Dataset."}, "Returns": "A scalar tf.variant tensor representing the given dataset."}, "tf.data.experimental.unbatch": {"description": "Splits elements of a dataset into multiple elements on the batch dimension. (deprecated)", "Returns": "A Dataset transformation function, which can be passed to\ntf.data.Dataset.apply."}, "tf.data.experimental.unique": {"description": "Creates a Dataset from another Dataset, discarding duplicates. (deprecated)", "Returns": "A Dataset transformation function, which can be passed to\ntf.data.Dataset.apply."}}, "tf.debugging.experimental": {"tf.debugging.experimental.disable_dump_debug_info": {"description": "Disable the currently-enabled debugging dumping."}, "tf.debugging.experimental.enable_dump_debug_info": {"description": "Enable dumping debugging information from a TensorFlow program.", "Args": {"dump_root": "The directory path where the dumping information will be written.", "tensor_debug_mode": "Debug mode for tensor values, as a string.\nThe currently supported options are:\n\n\"NO_TENSOR\": (Default) Only traces the output tensors of all executed\nops (including those executed eagerly at the Python level or as a part\nof a TensorFlow graph) and functions, while not extracting any\ninformation from the values of the tensors.\n\"CURT_HEALTH\": For each floating-dtype tensor (e.g., tensors of dtypes\nsuch as float32, float64 and bfloat16), extracts a binary bit\nindicating whether it contains any -infinity, +infinity or NaN.\n\"CONCISE_HEALTH\": For each floating-dtype tensor, extract total\nelement count, and counts of -infinity, +infinity and NaN elements.\n\"FULL_HEALTH\": For each floating-dtype tensor, extracts the dtype,\nrank (number of dimensions), total element count, and counts of\n-infinity, +infinity and NaN elements.\n\"SHAPE\": For each tensor (regardless of dtype), extracts its dtype,\nrank, total element count and shape.", "circular_buffer_size": "Size of the circular buffers for execution events.\nThese circular buffers are designed to reduce the overhead of debugging\ndumping. They hold the most recent debug events concerning eager execution\nof ops and tf.functions and traces of tensor values computed inside\ntf.functions. They are written to the file system only when the proper\nflushing method is called (see description of return values below).\nExpected to be an integer. If <= 0, the circular-buffer behavior will be\ndisabled, i.e., the execution debug events will be written to the file\nwriters in the same way as non-execution events such as op creations and\nsource-file snapshots.", "op_regex": "Dump data from only the tensors from op types that matches to the\nregular expression (through Python's re.match()).\n\"Op type\" refers to the names of the TensorFlow operations (e.g.,\n\"MatMul\", \"LogSoftmax\"), which may repeat in a TensorFlow\nfunction. It does not refer to the names of nodes (e.g.,\n\"dense/MatMul\", \"dense_1/MatMul_1\") which are unique within a function.\nExample 1: Dump tensor data from only MatMul and Relu ops\nop_regex=\"^(MatMul|Relu)$\".\nExample 2: Dump tensors from all ops except Relu:\nop_regex=\"(?!^Relu$)\".\nThis filter operates in a logical AND relation with tensor_dtypes.", "tensor_dtypes": "Dump data from only the tensors of which the specified\ndtypes. This optional argument can be in any of the following format:\na list or tuple of DType objects or strings that can be converted\nto DType objects via tf.as_dtype(). Examples:\n\ntensor_dtype=[tf.float32, tf.float64],\ntensor_dtype=[\"float32\", \"float64\"],\ntensor_dtypes=(tf.int32, tf.bool),\ntensor_dtypes=(\"int32\", \"bool\")\n\na callable that takes a single DType argument and returns a Python\nboolean indicating whether the dtype is to be included in the data\ndumping. Examples:\n\ntensor_dtype=lambda dtype: dtype.is_integer.\nThis filter operates in a logical AND relation with op_regex."}, "Returns": "A DebugEventsWriter instance used by the dumping callback. The caller\nmay use its flushing methods, including FlushNonExecutionFiles() and\nFlushExecutionFiles()."}}, "tf.distribute.experimental": {"tf.distribute.experimental.CentralStorageStrategy": {"description": "A one-machine strategy that puts all variables on a single device.", "Attributes": {"cluster_resolver": "Returns the cluster resolver associated with this strategy.\nIn general, when using a multi-worker tf.distribute strategy such as\ntf.distribute.experimental.MultiWorkerMirroredStrategy or\ntf.distribute.TPUStrategy(), there is a\ntf.distribute.cluster_resolver.ClusterResolver associated with the\nstrategy used, and such an instance is returned by this property.\nStrategies that intend to have an associated\ntf.distribute.cluster_resolver.ClusterResolver must set the\nrelevant attribute, or override this property; otherwise, None is returned\nby default. Those strategies should also provide information regarding what\nis returned by this property.\nSingle-worker strategies usually do not have a\ntf.distribute.cluster_resolver.ClusterResolver, and in those cases this\nproperty will return None.\nThe tf.distribute.cluster_resolver.ClusterResolver may be useful when the\nuser needs to access information such as the cluster spec, task type or task\nid. For example,\nos.environ['TF_CONFIG'] = json.dumps({\u00a0 'cluster': {\u00a0 \u00a0 \u00a0 'worker': [\"localhost:12345\", \"localhost:23456\"],\u00a0 \u00a0 \u00a0 'ps': [\"localhost:34567\"]\u00a0 },\u00a0 'task': {'type': 'worker', 'index': 0}})# This implicitly uses TF_CONFIG for the cluster and current task info.strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()...if strategy.cluster_resolver.task_type == 'worker':\u00a0 # Perform something that's only applicable on workers. Since we set this\u00a0 # as a worker above, this block will run on this particular instance.elif strategy.cluster_resolver.task_type == 'ps':\u00a0 # Perform something that's only applicable on parameter servers. Since we\u00a0 # set this as a worker above, this block will not run on this particular\u00a0 # instance.\nFor more information, please see\ntf.distribute.cluster_resolver.ClusterResolver's API docstring.", "extended": "tf.distribute.StrategyExtended with additional methods.", "num_replicas_in_sync": "Returns number of replicas over which gradients are aggregated."}}, "tf.distribute.experimental.CollectiveHints": {"description": "Hints for collective operations like AllReduce.", "Args": {"bytes_per_pack": "a non-negative integer. Breaks collective operations into\npacks of certain size. If it's zero, the value is determined\nautomatically. This only applies to all-reduce with\nMultiWorkerMirroredStrategy currently.", "timeout_seconds": "a float or None, timeout in seconds. If not None, the\ncollective raises tf.errors.DeadlineExceededError if it takes longer\nthan this timeout. This can be useful when debugging hanging issues.\nThis should only be used for debugging since it creates a new thread for\neach collective, i.e. an overhead of timeout_seconds *\nnum_collectives_per_second more threads.  This only works for\ntf.distribute.experimental.MultiWorkerMirroredStrategy."}, "Raises": {"ValueError": "When arguments have invalid value."}}, "tf.distribute.experimental.CommunicationImplementation": {"description": "Cross device communication implementation.", "Class Variables": {"AUTO": "<CommunicationImplementation.AUTO: 'AUTO'>", "NCCL": "<CommunicationImplementation.NCCL: 'NCCL'>", "RING": "<CommunicationImplementation.RING: 'RING'>"}}, "tf.distribute.experimental.CommunicationOptions": {"description": "Options for cross device communications like All-reduce.", "Args": {"bytes_per_pack": "a non-negative integer. Breaks collective operations into\npacks of certain size. If it's zero, the value is determined\nautomatically. This hint is respected by all multi-replica strategies\nexcept TPUStrategy.", "timeout_seconds": "a float or None, timeout in seconds. If not None, the\ncollective raises tf.errors.DeadlineExceededError if it takes longer\nthan this timeout. Zero disables timeout. This can be useful when\ndebugging hanging issues.  This should only be used for debugging since\nit creates a new thread for each collective, i.e. an overhead of\ntimeout_seconds * num_collectives_per_second more threads. This only\nworks for tf.distribute.experimental.MultiWorkerMirroredStrategy.", "implementation": "a\ntf.distribute.experimental.CommunicationImplementation. This is a hint\non the preferred communication implementation. Possible values include\nAUTO, RING, and NCCL. NCCL is generally more performant for GPU,\nbut doesn't work for CPU. This only works for\ntf.distribute.experimental.MultiWorkerMirroredStrategy."}, "Raises": {"ValueError": "When arguments have invalid value."}}, "tf.distribute.experimental.MultiWorkerMirroredStrategy": {"description": "A distribution strategy for synchronous training on multiple workers.", "Args": {"communication": "optional\ntf.distribute.experimental.CommunicationImplementation. This is a hint\non the preferred collective communication implementation. Possible\nvalues include AUTO, RING, and NCCL.", "cluster_resolver": "optional\ntf.distribute.cluster_resolver.ClusterResolver. If None,\ntf.distribute.cluster_resolver.TFConfigClusterResolver is used."}, "Attributes": {"cluster_resolver": "Returns the cluster resolver associated with this strategy.\nAs a multi-worker strategy, tf.distribute.MultiWorkerMirroredStrategy\nprovides the associated tf.distribute.cluster_resolver.ClusterResolver. If\nthe user provides one in __init__, that instance is returned; if the user\ndoes not, a default TFConfigClusterResolver is provided.", "extended": "tf.distribute.StrategyExtended with additional methods.", "num_replicas_in_sync": "Returns number of replicas over which gradients are aggregated."}}, "tf.distribute.experimental.ParameterServerStrategy": {"description": "An multi-worker tf.distribute strategy with parameter servers.", "Args": {"cluster_resolver": "a tf.distribute.cluster_resolver.ClusterResolver\nobject.", "variable_partitioner": "a distribute.experimental.partitioners.Partitioner that specifies\nhow to partition variables. If None, variables will not be\npartitioned.\n\nPredefined partitioners in tf.distribute.experimental.partitioners\ncan be used for this argument. A commonly used partitioner is\nMinSizePartitioner(min_shard_bytes = 256 << 10, max_shards = num_ps),\nwhich allocates at least 256K per shard, and each ps gets at most one\nshard.\nvariable_partitioner will be called for each variable created under\nstrategy scope to instruct how the variable should be partitioned.\nVariables that have only one partition along the partitioning axis\n(i.e., no need for partition) will be created as a normal tf.Variable.\nOnly the first / outermost axis partitioning is supported.\nDiv partition strategy is used to partition variables. Assuming we\nassign consecutive integer ids along the first axis of a variable, then\nids are assigned to shards in a contiguous manner, while attempting to\nkeep each shard size identical. If the ids do not evenly divide the\nnumber of shards, each of the first several shards will be assigned one\nmore id. For instance, a variable whose first dimension is 13 has 13\nids, and they are split across 5 shards as:\n[[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10], [11, 12]].\nVariables created under strategy.extended.colocate_vars_with will\nnot be partitioned."}, "Attributes": {"cluster_resolver": "Returns the cluster resolver associated with this strategy.\nIn general, when using a multi-worker tf.distribute strategy such as\ntf.distribute.experimental.MultiWorkerMirroredStrategy or\ntf.distribute.TPUStrategy(), there is a\ntf.distribute.cluster_resolver.ClusterResolver associated with the\nstrategy used, and such an instance is returned by this property.\nStrategies that intend to have an associated\ntf.distribute.cluster_resolver.ClusterResolver must set the\nrelevant attribute, or override this property; otherwise, None is returned\nby default. Those strategies should also provide information regarding what\nis returned by this property.\nSingle-worker strategies usually do not have a\ntf.distribute.cluster_resolver.ClusterResolver, and in those cases this\nproperty will return None.\nThe tf.distribute.cluster_resolver.ClusterResolver may be useful when the\nuser needs to access information such as the cluster spec, task type or task\nid. For example,\nos.environ['TF_CONFIG'] = json.dumps({\u00a0 'cluster': {\u00a0 \u00a0 \u00a0 'worker': [\"localhost:12345\", \"localhost:23456\"],\u00a0 \u00a0 \u00a0 'ps': [\"localhost:34567\"]\u00a0 },\u00a0 'task': {'type': 'worker', 'index': 0}})# This implicitly uses TF_CONFIG for the cluster and current task info.strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()...if strategy.cluster_resolver.task_type == 'worker':\u00a0 # Perform something that's only applicable on workers. Since we set this\u00a0 # as a worker above, this block will run on this particular instance.elif strategy.cluster_resolver.task_type == 'ps':\u00a0 # Perform something that's only applicable on parameter servers. Since we\u00a0 # set this as a worker above, this block will not run on this particular\u00a0 # instance.\nFor more information, please see\ntf.distribute.cluster_resolver.ClusterResolver's API docstring.", "extended": "tf.distribute.StrategyExtended with additional methods.", "num_replicas_in_sync": "Returns number of replicas over which gradients are aggregated."}}, "tf.distribute.experimental.PreemptionCheckpointHandler": {"description": "Preemption and error handler for synchronous training.", "Args": {"cluster_resolver": "a tf.distribute.cluster_resolver.ClusterResolver\nobject. You may also obtain it through the cluster_resolver attribute\nof the distribution strategy in use.", "checkpoint_or_checkpoint_manager": "a tf.train.CheckpointManager or a\ntf.train.Checkpoint. If you are using a tf.train.CheckpointManager\nto manage checkpoints outside the PreemptionCheckpointHandler for\nbackup purpose as well, pass it as checkpoint_or_checkpoint_manager\nargument. Otherwise, pass a tf.train.Checkpoint and the\nPreemptionCheckpointHandler will create\na tf.train.CheckpointManager to manage it in the checkpoint_dir.", "checkpoint_dir": "a directory where the PreemptionCheckpointHandler saves\nand restores checkpoints. When a PreemptionCheckpointHandler is\ncreated, the latest checkpoint in the checkpoint_dir will be restored.\n(This is not needed if a tf.train.CheckpointManager instead of a\ntf.train.Checkpoint is passed as the\ncheckpoint_or_checkpoint_manager argument.)", "termination_config": "optional, a\ntf.distribute.experimental.TerminationConfig object to configure for a\nplatform other than Google Borg or GCP."}, "Attributes": {"total_run_calls": "Returns the number of times PreemptionCheckpointHandler.run is called.\nThis value tracks the number of all calls to\nPreemptionCheckpointHandler.run including those before the program is\nrestarted and the training is restored, by saving and reading the value in\nthe checkpoint. A user can compute their total number of iterations\nby PreemptionCheckpointHandler.total_run_calls *\nnumber_of_steps_in_train_function,\nwhile number_of_steps_in_train_function should be one for\ntf.distribute.MultiWorkerMirroredStrategy users. They can also use this\nvalue to infer the starting epoch and step after training restores, as shown\nin the example above."}}, "tf.distribute.experimental.TPUStrategy": {"description": "Synchronous training on TPUs and TPU Pods.", "Args": {"tpu_cluster_resolver": "A tf.distribute.cluster_resolver.TPUClusterResolver,\nwhich provides information about the TPU cluster.", "device_assignment": "Optional tf.tpu.experimental.DeviceAssignment to\nspecify the placement of replicas on the TPU cluster."}, "Attributes": {"cluster_resolver": "Returns the cluster resolver associated with this strategy.\ntf.distribute.experimental.TPUStrategy provides the\nassociated tf.distribute.cluster_resolver.ClusterResolver. If the user\nprovides one in __init__, that instance is returned; if the user does\nnot, a default\ntf.distribute.cluster_resolver.TPUClusterResolver is provided.", "extended": "tf.distribute.StrategyExtended with additional methods.", "num_replicas_in_sync": "Returns number of replicas over which gradients are aggregated."}}, "tf.distribute.experimental.TerminationConfig": {"description": "Customization of PreemptionCheckpointHandler for various platforms.", "Args": {"termination_watcher_fn": "a function to execute repeatedly that returns\nTrue if a preemption signal is available and False otherwise. The\nfunction cannot block until a preemption signal is available, which\nprevents proper cleanup of the program. A change is NOT recommended\nfor users on Google Borg or Google Cloud Platform.", "exit_fn": "a function to execute after a checkpoint is saved and before the\npreemption happens. Usually, it should be in the form of\nlambda: sys.exit(RESTART_CODE), where RESTART_CODE varies by\nplatform. A change is NOT recommended for users on Google Borg.\nUsers on Google Cloud Platform may configure it to use a customized\nRESTART_CODE.", "grace_period": "the length of time between receiving a preemption signal and\nthe actual preemption. A change is NOT recommended for users on\nGoogle Borg, Google Cloud Platform, or users with a short grace period."}}, "tf.distribute.experimental.ValueContext": {"description": "A class wrapping information needed by a distribute function.", "Args": {"replica_id_in_sync_group": "the current replica_id, should be an int in\n[0,num_replicas_in_sync).", "num_replicas_in_sync": "the number of replicas that are in sync."}, "Attributes": {"num_replicas_in_sync": "Returns the number of compute replicas in sync.", "replica_id_in_sync_group": "Returns the replica ID."}}}, "tf.estimator.experimental": {"tf.estimator.experimental.InMemoryEvaluatorHook": {"description": "Hook to run evaluation in training without a checkpoint.", "Args": {"estimator": "A tf.estimator.Estimator instance to call evaluate.", "input_fn": "Equivalent to the input_fn arg to estimator.evaluate. A\nfunction that constructs the input data for evaluation. See Creating\ninput functions\n  for more information. The function should construct and return one of\nthe following:\n\nA 'tf.data.Dataset' object: Outputs of Dataset object must be a\ntuple (features, labels) with same constraints as below.\nA tuple (features, labels): Where features is a Tensor or a\ndictionary of string feature name to Tensor and labels is a\nTensor or a dictionary of string label name to Tensor. Both\nfeatures and labels are consumed by model_fn. They should\nsatisfy the expectation of model_fn from inputs.", "steps": "Equivalent to the steps arg to estimator.evaluate.  Number of\nsteps for which to evaluate model. If None, evaluates until input_fn\nraises an end-of-input exception.", "hooks": "Equivalent to the hooks arg to estimator.evaluate. List of\nSessionRunHook subclass instances. Used for callbacks inside the\nevaluation call.", "name": "Equivalent to the name arg to estimator.evaluate. Name of the\nevaluation if user needs to run multiple evaluations on different data\nsets, such as on training data vs test data. Metrics for different\nevaluations are saved in separate folders, and appear separately in\ntensorboard.", "every_n_iter": "int, runs the evaluator once every N training iteration."}, "Raises": {"ValueError": "if every_n_iter is non-positive or it's not a single machine\ntraining"}}, "tf.estimator.experimental.LinearSDCA": {"description": "Stochastic Dual Coordinate Ascent helper for linear estimators.", "Args": {"example_id_column": "The column name containing the example ids.", "num_loss_partitions": "Number of workers.", "num_table_shards": "Number of shards of the internal state table, typically\nset to match the number of parameter servers.", "symmetric_l1_regularization": "A float value, must be greater than or equal\nto zero.", "symmetric_l2_regularization": "A float value, must be greater than zero and\nshould typically be greater than 1.", "adaptive": "A boolean indicating whether to use adaptive sampling."}}, "tf.estimator.experimental.RNNClassifier": {"description": "A classifier for TensorFlow RNN models.", "Args": {"sequence_feature_columns": "An iterable containing the FeatureColumns that\nrepresent sequential input. All items in the set should either be\nsequence columns (e.g. sequence_numeric_column) or constructed from\none (e.g. embedding_column with sequence_categorical_column_* as\ninput).", "context_feature_columns": "An iterable containing the FeatureColumns for\ncontextual input. The data represented by these columns will be\nreplicated and given to the RNN at each timestep. These columns must be\ninstances of classes derived from DenseColumn such as\nnumeric_column, not the sequential variants.", "units": "Iterable of integer number of hidden units per RNN layer. If set,\ncell_type must also be specified and rnn_cell_fn must be None.", "cell_type": "A class producing a RNN cell or a string specifying the cell\ntype. Supported strings are: 'simple_rnn', 'lstm', and 'gru'. If\n  set, units must also be specified and rnn_cell_fn must be None.", "rnn_cell_fn": "A function that returns a RNN cell instance that will be used\nto construct the RNN. If set, units and cell_type cannot be set.\nThis is for advanced users who need additional customization beyond\nunits and cell_type. Note that tf.keras.layers.StackedRNNCells is\nneeded for stacked RNNs.", "return_sequences": "A boolean indicating whether to return the last output\nin the output sequence, or the full sequence. Note that if True,\nweight_column must be None or a string.", "model_dir": "Directory to save model parameters, graph and etc. This can\nalso be used to load checkpoints from the directory into a estimator to\ncontinue training a previously saved model.", "n_classes": "Number of label classes. Defaults to 2, namely binary\nclassification. Must be > 1.", "weight_column": "A string or a NumericColumn created by\ntf.feature_column.numeric_column defining feature column representing\nweights. It is used to down weight or boost examples during training. It\nwill be multiplied by the loss of the example. If it is a string, it is\nused as a key to fetch weight tensor from the features. If it is a\nNumericColumn, raw tensor is fetched by key weight_column.key, then\nweight_column.normalizer_fn is applied on it to get weight tensor.", "label_vocabulary": "A list of strings represents possible label values. If\ngiven, labels must be string type and have any value in\nlabel_vocabulary. If it is not given, that means labels are already\nencoded as integer or float within [0, 1] for n_classes=2 and encoded\nas integer values in {0, 1,..., n_classes-1} for n_classes>2 . Also\nthere will be errors if vocabulary is not provided and labels are\nstring.", "optimizer": "An instance of tf.Optimizer or string specifying optimizer\ntype. Defaults to Adagrad optimizer.", "loss_reduction": "One of tf.losses.Reduction except NONE. Describes how\nto reduce training loss over batch. Defaults to SUM_OVER_BATCH_SIZE.", "sequence_mask": "A string with the name of the sequence mask tensor. If\nsequence_mask is in the features dictionary, the provided tensor is\nused, otherwise the sequence mask is computed from the length of\nsequential features. The sequence mask is used in evaluation and\ntraining mode to aggregate loss and metrics computation while excluding\npadding steps. It is also added to the predictions dictionary in\nprediction mode to indicate which steps are padding.", "config": "RunConfig object to configure the runtime settings."}, "Raises": {"ValueError": "If units, cell_type, and rnn_cell_fn are not\ncompatible."}, "Attributes": {"config": "", "model_dir": "", "model_fn": "Returns the model_fn which is bound to self.params.", "params": ""}}, "tf.estimator.experimental.RNNEstimator": {"description": "An Estimator for TensorFlow RNN models with user-specified head.", "Args": {"head": "A Head instance. This specifies the model's output and loss\nfunction to be optimized.", "sequence_feature_columns": "An iterable containing the FeatureColumns that\nrepresent sequential input. All items in the set should either be\nsequence columns (e.g. sequence_numeric_column) or constructed from\none (e.g. embedding_column with sequence_categorical_column_* as\ninput).", "context_feature_columns": "An iterable containing the FeatureColumns for\ncontextual input. The data represented by these columns will be\nreplicated and given to the RNN at each timestep. These columns must be\ninstances of classes derived from DenseColumn such as\nnumeric_column, not the sequential variants.", "units": "Iterable of integer number of hidden units per RNN layer. If set,\ncell_type must also be specified and rnn_cell_fn must be None.", "cell_type": "A class producing a RNN cell or a string specifying the cell\ntype. Supported strings are: 'simple_rnn', 'lstm', and 'gru'. If\n  set, units must also be specified and rnn_cell_fn must be None.", "rnn_cell_fn": "A function that returns a RNN cell instance that will be used\nto construct the RNN. If set, units and cell_type cannot be set.\nThis is for advanced users who need additional customization beyond\nunits and cell_type. Note that tf.keras.layers.StackedRNNCells is\nneeded for stacked RNNs.", "return_sequences": "A boolean indicating whether to return the last output\nin the output sequence, or the full sequence.", "model_dir": "Directory to save model parameters, graph and etc. This can\nalso be used to load checkpoints from the directory into a estimator to\ncontinue training a previously saved model.", "optimizer": "An instance of tf.Optimizer or string specifying optimizer\ntype. Defaults to Adagrad optimizer.", "config": "RunConfig object to configure the runtime settings."}, "Raises": {"ValueError": "If units, cell_type, and rnn_cell_fn are not\ncompatible."}, "Attributes": {"config": "", "model_dir": "", "model_fn": "Returns the model_fn which is bound to self.params.", "params": ""}}, "tf.estimator.experimental.build_raw_supervised_input_receiver_fn": {"description": "Build a supervised_input_receiver_fn for raw features and labels.", "Args": {"features": "a dict of string to Tensor or Tensor.", "labels": "a dict of string to Tensor or Tensor.", "default_batch_size": "the number of query examples expected per batch. Leave\nunset for variable batch size (recommended)."}, "Returns": "A supervised_input_receiver_fn.", "Raises": {"ValueError": "if features and labels have overlapping keys."}}, "tf.estimator.experimental.call_logit_fn": {"description": "Calls logit_fn (experimental).", "Args": {"logit_fn": "A logit_fn as defined above.", "features": "The features dict.", "mode": "TRAIN / EVAL / PREDICT ModeKeys.", "params": "The hyperparameter dict.", "config": "The configuration object."}, "Returns": "A logit Tensor, the output of logit_fn.", "Raises": {"ValueError": "if logit_fn does not return a Tensor or a dictionary mapping\nstrings to Tensors."}}, "tf.estimator.experimental.make_early_stopping_hook": {"description": "Creates early-stopping hook.", "Args": {"estimator": "A tf.estimator.Estimator instance.", "should_stop_fn": "callable, function that takes no arguments and returns a\nbool. If the function returns True, stopping will be initiated by the\nchief.", "run_every_secs": "If specified, calls should_stop_fn at an interval of\nrun_every_secs seconds. Defaults to 60 seconds. Either this or\nrun_every_steps must be set.", "run_every_steps": "If specified, calls should_stop_fn every\nrun_every_steps steps. Either this or run_every_secs must be set."}, "Returns": "A SessionRunHook that periodically executes should_stop_fn and initiates\nearly stopping if the function returns True.", "Raises": {"TypeError": "If estimator is not of type tf.estimator.Estimator.", "ValueError": "If both run_every_secs and run_every_steps are set."}}, "tf.estimator.experimental.make_stop_at_checkpoint_step_hook": {"description": "Creates a proper StopAtCheckpointStepHook based on chief status."}, "tf.estimator.experimental.stop_if_higher_hook": {"description": "Creates hook to stop if the given metric is higher than the threshold.", "Args": {"estimator": "A tf.estimator.Estimator instance.", "metric_name": "str, metric to track. \"loss\", \"accuracy\", etc.", "threshold": "Numeric threshold for the given metric.", "eval_dir": "If set, directory containing summary files with eval metrics. By\ndefault, estimator.eval_dir() will be used.", "min_steps": "int, stop is never requested if global step is less than this\nvalue. Defaults to 0.", "run_every_secs": "If specified, calls should_stop_fn at an interval of\nrun_every_secs seconds. Defaults to 60 seconds. Either this or\nrun_every_steps must be set.", "run_every_steps": "If specified, calls should_stop_fn every\nrun_every_steps steps. Either this or run_every_secs must be set."}, "Returns": "An early-stopping hook of type SessionRunHook that periodically checks\nif the given metric is higher than specified threshold and initiates\nearly stopping if true."}, "tf.estimator.experimental.stop_if_lower_hook": {"description": "Creates hook to stop if the given metric is lower than the threshold.", "Args": {"estimator": "A tf.estimator.Estimator instance.", "metric_name": "str, metric to track. \"loss\", \"accuracy\", etc.", "threshold": "Numeric threshold for the given metric.", "eval_dir": "If set, directory containing summary files with eval metrics. By\ndefault, estimator.eval_dir() will be used.", "min_steps": "int, stop is never requested if global step is less than this\nvalue. Defaults to 0.", "run_every_secs": "If specified, calls should_stop_fn at an interval of\nrun_every_secs seconds. Defaults to 60 seconds. Either this or\nrun_every_steps must be set.", "run_every_steps": "If specified, calls should_stop_fn every\nrun_every_steps steps. Either this or run_every_secs must be set."}, "Returns": "An early-stopping hook of type SessionRunHook that periodically checks\nif the given metric is lower than specified threshold and initiates\nearly stopping if true."}, "tf.estimator.experimental.stop_if_no_decrease_hook": {"description": "Creates hook to stop if metric does not decrease within given max steps.", "Args": {"estimator": "A tf.estimator.Estimator instance.", "metric_name": "str, metric to track. \"loss\", \"accuracy\", etc.", "max_steps_without_decrease": "int, maximum number of training steps with no\ndecrease in the given metric.", "eval_dir": "If set, directory containing summary files with eval metrics. By\ndefault, estimator.eval_dir() will be used.", "min_steps": "int, stop is never requested if global step is less than this\nvalue. Defaults to 0.", "run_every_secs": "If specified, calls should_stop_fn at an interval of\nrun_every_secs seconds. Defaults to 60 seconds. Either this or\nrun_every_steps must be set.", "run_every_steps": "If specified, calls should_stop_fn every\nrun_every_steps steps. Either this or run_every_secs must be set."}, "Returns": "An early-stopping hook of type SessionRunHook that periodically checks\nif the given metric shows no decrease over given maximum number of\ntraining steps, and initiates early stopping if true."}, "tf.estimator.experimental.stop_if_no_increase_hook": {"description": "Creates hook to stop if metric does not increase within given max steps.", "Args": {"estimator": "A tf.estimator.Estimator instance.", "metric_name": "str, metric to track. \"loss\", \"accuracy\", etc.", "max_steps_without_increase": "int, maximum number of training steps with no\nincrease in the given metric.", "eval_dir": "If set, directory containing summary files with eval metrics. By\ndefault, estimator.eval_dir() will be used.", "min_steps": "int, stop is never requested if global step is less than this\nvalue. Defaults to 0.", "run_every_secs": "If specified, calls should_stop_fn at an interval of\nrun_every_secs seconds. Defaults to 60 seconds. Either this or\nrun_every_steps must be set.", "run_every_steps": "If specified, calls should_stop_fn every\nrun_every_steps steps. Either this or run_every_secs must be set."}, "Returns": "An early-stopping hook of type SessionRunHook that periodically checks\nif the given metric shows no increase over given maximum number of\ntraining steps, and initiates early stopping if true."}}, "tf.experimental": {"tf.experimental.BatchableExtensionType": {"description": "An ExtensionType that can be batched and unbatched."}, "tf.experimental.DynamicRaggedShape": {"description": "The shape of a ragged or dense tensor.", "Args": {"row_partitions": "the row_partitions of the shape.", "inner_shape": "if len(row_partitions) > 0, the shape of the flat_values.\nOtherwise, the shape of the tensor.", "dtype": "tf.int64, tf.int32, or None representing the preferred dtype.", "validate": "if true, dynamic validation is applied to the shape."}, "Attributes": {"dtype": "The dtype of the shape -- one of tf.int32 or tf.int64.", "inner_rank": "The rank of inner_shape.", "inner_shape": "The inner dimension sizes for this shape.", "num_row_partitions": "The number of row_partitions of the shape.", "rank": "The number of dimensions in this shape, or None if unknown.", "row_partitions": "The row_partitions of the shape."}}, "tf.experimental.DynamicRaggedShape.Spec": {"description": "A Spec for DynamicRaggedShape: similar to a static shape.", "Attributes": {"dtype": "", "inner_rank": "", "num_row_partitions": "", "rank": ""}}, "tf.experimental.ExtensionType": {"description": "Base class for TensorFlow ExtensionType classes."}, "tf.experimental.ExtensionTypeBatchEncoder": {"description": "Class used to encode and decode extension type values for batching."}, "tf.experimental.Optional": {"description": "Represents a value that may or may not be present.", "Attributes": {"element_spec": "The type specification of an element of this optional.\noptional = tf.experimental.Optional.from_value(42)print(optional.element_spec)tf.TensorSpec(shape=(), dtype=tf.int32, name=None)"}}, "tf.experimental.RowPartition": {"description": "Partitioning of a sequence of values into contiguous subsequences (&#34;rows&#34;).", "Args": {"row_splits": "A 1-D integer tensor with shape [nrows+1].", "row_lengths": "A 1-D integer tensor with shape [nrows]", "value_rowids": "A 1-D integer tensor with shape [nvals].", "nrows": "A 1-D integer scalar tensor.", "uniform_row_length": "A scalar tensor.", "nvals": "A scalar tensor.", "internal": "Private key value, required to ensure that this private\nconstructor is only called from the factory methods."}, "Raises": {"TypeError": "If exactly one row partitioning argument was not specified.", "ValueError": "If nrows is specified but value_rowids is not None."}, "Attributes": {"dtype": "The DType used to encode the row partition (either int32 or int64).", "static_nrows": "The number of rows in this partition, if statically known.\nself.row_lengths().shape == [self.static_nrows]self.row_starts().shape == [self.static_nrows]self.row_limits().shape == [self.static_nrows]self.row_splits().shape == [self.static_nrows + 1]", "static_nvals": "The number of values in this partition, if statically known.\nself.value_rowids().shape == [self.static_vals]", "static_uniform_row_length": "The number of values in each row of this partition, if statically known."}}, "tf.experimental.async_clear_error": {"description": "Clear pending operations and error statuses in async execution."}, "tf.experimental.async_scope": {"description": "Context manager for grouping async operations."}, "tf.experimental.dispatch_for_api": {"description": "Decorator that overrides the default implementation for a TensorFlow API.", "Args": {"api": "The TensorFlow API to override.", "*signatures": "Dictionaries mapping parameter names or indices to type\nannotations, specifying when the dispatch target should be called.  In\nparticular, the dispatch target will be called if any signature matches;\nand a signature matches if all of the specified parameters have types that\nmatch with the indicated type annotations.  If no signatures are\nspecified, then a signature will be read from the dispatch target\nfunction's type annotations."}, "Returns": "A decorator that overrides the default implementation for api."}, "tf.experimental.dispatch_for_binary_elementwise_apis": {"description": "Decorator to override default implementation for binary elementwise APIs.", "Args": {"x_type": "A type annotation indicating when the api handler should be called.", "y_type": "A type annotation indicating when the api handler should be called."}, "Returns": "A decorator."}, "tf.experimental.dispatch_for_binary_elementwise_assert_apis": {"description": "Decorator to override default implementation for binary elementwise assert APIs.", "Args": {"x_type": "A type annotation indicating when the api handler should be called.", "y_type": "A type annotation indicating when the api handler should be called."}, "Returns": "A decorator."}, "tf.experimental.dispatch_for_unary_elementwise_apis": {"description": "Decorator to override default implementation for unary elementwise APIs.", "Args": {"x_type": "A type annotation indicating when the api handler should be called.\nSee dispatch_for_api for a list of supported annotation types."}, "Returns": "A decorator."}, "tf.experimental.function_executor_type": {"description": "Context manager for setting the executor of eager defined functions.", "Args": {"executor_type": "a string for the name of the executor to be used to execute\nfunctions defined by tf.contrib.eager.defun."}}, "tf.experimental.register_filesystem_plugin": {"description": "Loads a TensorFlow FileSystem plugin.", "Args": {"plugin_location": "Path to the plugin. Relative or absolute filesystem plugin\npath to a dynamic library file."}, "Returns": "None", "Raises": {"OSError": "When the file to be loaded is not found.", "RuntimeError": "when unable to load the library."}}, "tf.experimental.unregister_dispatch_for": {"description": "Unregisters a function that was registered with @dispatch_for_*.", "Args": {"dispatch_target": "The function to unregister."}, "Raises": {"ValueError": "If dispatch_target was not registered using @dispatch_for,\n@dispatch_for_unary_elementwise_apis, or\n@dispatch_for_binary_elementwise_apis."}}}, "tf.keras.backend.experimental": {"tf.keras.backend.experimental.disable_tf_random_generator": {"description": "Disable the tf.random.Generator as the RNG for Keras."}, "tf.keras.backend.experimental.enable_tf_random_generator": {"description": "Enable the tf.random.Generator as the RNG for Keras."}, "tf.keras.backend.experimental.is_tf_random_generator_enabled": {"description": "Check whether tf.random.Generator is used for RNG in Keras.", "Returns": "boolean\n\n\nwhether tf.random.Generator is used for random number generation\nin Keras."}}, "tf.keras.callbacks.experimental": {"tf.keras.callbacks.experimental.BackupAndRestore": {"description": "Deprecated. Please use tf.keras.callbacks.BackupAndRestore instead."}}, "tf.keras.dtensor.experimental": {"tf.keras.dtensor.experimental.LayoutMap": {"description": "A dict-like object that maps string to Layout instances.", "Args": {"mesh": "An optional Mesh that can be used to create all replicated\nlayout as default when there isn't a layout found based on the input\nstring query."}}, "tf.keras.dtensor.experimental.layout_map_scope": {"description": "Apply the layout to all the tf.Variables created under the scope.", "Args": {"layout_map": "a LayoutMap which contains the variable_object_path (string) ->\nLayout. When a layout is not found for the variable, a default all\nreplicated layout will be created for the variable."}}}, "tf.keras.experimental": {"tf.keras.experimental.LinearModel": {"description": "Linear Model for regression and classification problems.", "Args": {"units": "Positive integer, output dimension without the batch size.", "activation": "Activation function to use.\nIf you don't specify anything, no activation is applied.", "use_bias": "whether to calculate the bias/intercept for this model. If set\nto False, no bias/intercept will be used in calculations, e.g., the data\nis already centered.", "kernel_initializer": "Initializer for the kernel weights matrices.", "bias_initializer": "Initializer for the bias vector.", "kernel_regularizer": "regularizer for kernel vectors.", "bias_regularizer": "regularizer for bias vector.", "**kwargs": "The keyword arguments that are passed on to BaseLayer.init."}, "Attributes": {"distribute_strategy": "The tf.distribute.Strategy this model was created under.", "layers": "", "metrics_names": "Returns the model's display labels for all outputs.\nNote: metrics_names are available only after a keras.Model has been\ntrained/evaluated on actual data.\ninputs = tf.keras.layers.Input(shape=(3,))outputs = tf.keras.layers.Dense(2)(inputs)model = tf.keras.models.Model(inputs=inputs, outputs=outputs)model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])model.metrics_names[]\nx = np.random.random((2, 3))y = np.random.randint(0, 2, (2, 2))model.fit(x, y)model.metrics_names['loss', 'mae']\ninputs = tf.keras.layers.Input(shape=(3,))d = tf.keras.layers.Dense(2, name='out')output_1 = d(inputs)output_2 = d(inputs)model = tf.keras.models.Model(\u00a0 \u00a0inputs=inputs, outputs=[output_1, output_2])model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\", \"acc\"])model.fit(x, (y, y))model.metrics_names['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae','out_1_acc']", "run_eagerly": "Settable attribute indicating whether the model should run eagerly.\nRunning eagerly means that your model will be run step by step,\nlike Python code. Your model might run slower, but it should become easier\nfor you to debug it by stepping into individual layer calls.\nBy default, we will attempt to compile your model to a static graph to\ndeliver the best execution performance."}}, "tf.keras.experimental.SequenceFeatures": {"description": "A layer for sequence input.", "Args": {"feature_columns": "An iterable of dense sequence columns. Valid columns are\n\nembedding_column that wraps a sequence_categorical_column_with_*\nsequence_numeric_column.", "trainable": "Boolean, whether the layer's variables will be updated via\ngradient descent during training.", "name": "Name to give to the SequenceFeatures.", "**kwargs": "Keyword arguments to construct a layer."}, "Raises": {"ValueError": "If any of the feature_columns is not a\nSequenceDenseColumn."}}, "tf.keras.experimental.SidecarEvaluator": {"description": "Deprecated. Please use tf.keras.utils.SidecarEvaluator instead.", "Args": {"model": "Model to use for evaluation. The model object used here should be a\ntf.keras.Model, and should be the same as the one that is used in\ntraining, where tf.keras.Models are checkpointed. The model should\nhave one or more metrics compiled before using SidecarEvaluator.", "data": "The input data for evaluation. SidecarEvaluator supports all data\ntypes that Keras model.evaluate supports as the input data x, such\nas a tf.data.Dataset.", "checkpoint_dir": "Directory where checkpoint files are saved.", "steps": "Number of steps to perform evaluation for, when evaluating a single\ncheckpoint file. If None, evaluation continues until the dataset is\nexhausted. For repeated evaluation dataset, user must specify steps to\navoid infinite evaluation loop.", "max_evaluations": "Maximum number of the checkpoint file to be evaluated,\nfor SidecarEvaluator to know when to stop. The evaluator will stop\nafter it evaluates a checkpoint filepath ending with\n'-'. If using\ntf.train.CheckpointManager.save for saving checkpoints, the kth saved\ncheckpoint has the filepath suffix '-' (k=1 for the first\nsaved), and if checkpoints are saved every epoch after training, the\nfilepath saved at the kth epoch would end with '-. Thus,\nif training runs for n epochs, and the evaluator should end after the\ntraining finishes, use n for this parameter. Note that this is not\nnecessarily equal to the number of total evaluations, since some\ncheckpoints may be skipped if evaluation is slower than checkpoint\ncreation. If None, SidecarEvaluator will evaluate indefinitely, and\nthe user must terminate evaluator program themselves.", "callbacks": "List of keras.callbacks.Callback instances to apply during\nevaluation. See callbacks."}}, "tf.keras.experimental.WideDeepModel": {"description": "Wide & Deep Model for regression and classification problems.", "Args": {"linear_model": "a premade LinearModel, its output must match the output of\nthe dnn model.", "dnn_model": "a tf.keras.Model, its output must match the output of the\nlinear model.", "activation": "Activation function. Set it to None to maintain a linear\nactivation.", "**kwargs": "The keyword arguments that are passed on to BaseLayer.init.\nAllowed keyword arguments include name."}, "Attributes": {"distribute_strategy": "The tf.distribute.Strategy this model was created under.", "layers": "", "metrics_names": "Returns the model's display labels for all outputs.\nNote: metrics_names are available only after a keras.Model has been\ntrained/evaluated on actual data.\ninputs = tf.keras.layers.Input(shape=(3,))outputs = tf.keras.layers.Dense(2)(inputs)model = tf.keras.models.Model(inputs=inputs, outputs=outputs)model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])model.metrics_names[]\nx = np.random.random((2, 3))y = np.random.randint(0, 2, (2, 2))model.fit(x, y)model.metrics_names['loss', 'mae']\ninputs = tf.keras.layers.Input(shape=(3,))d = tf.keras.layers.Dense(2, name='out')output_1 = d(inputs)output_2 = d(inputs)model = tf.keras.models.Model(\u00a0 \u00a0inputs=inputs, outputs=[output_1, output_2])model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\", \"acc\"])model.fit(x, (y, y))model.metrics_names['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae','out_1_acc']", "run_eagerly": "Settable attribute indicating whether the model should run eagerly.\nRunning eagerly means that your model will be run step by step,\nlike Python code. Your model might run slower, but it should become easier\nfor you to debug it by stepping into individual layer calls.\nBy default, we will attempt to compile your model to a static graph to\ndeliver the best execution performance."}}}, "tf.keras.layers.experimental": {"tf.keras.layers.experimental.EinsumDense": {"description": "A layer that uses tf.einsum as the backing computation.", "Args": {"equation": "An equation describing the einsum to perform. This equation must\nbe a valid einsum string of the form ab,bc->ac, ...ab,bc->...ac, or\nab...,bc->ac... where 'ab', 'bc', and 'ac' can be any valid einsum axis\nexpression sequence.", "output_shape": "The expected shape of the output tensor (excluding the batch\ndimension and any dimensions represented by ellipses). You can specify\nNone for any dimension that is unknown or can be inferred from the input\nshape.", "activation": "Activation function to use. If you don't specify anything, no\nactivation is applied (that is, a \"linear\" activation: a(x) = x).", "bias_axes": "A string containing the output dimension(s) to apply a bias to.\nEach character in the bias_axes string should correspond to a character\nin the output portion of the equation string.", "kernel_initializer": "Initializer for the kernel weights matrix.", "bias_initializer": "Initializer for the bias vector.", "kernel_regularizer": "Regularizer function applied to the kernel weights\nmatrix.", "bias_regularizer": "Regularizer function applied to the bias vector.", "activity_regularizer": "Regularizer function applied to the output of the\nlayer (its \"activation\")..", "kernel_constraint": "Constraint function applied to the kernel weights\nmatrix.", "bias_constraint": "Constraint function applied to the bias vector."}}, "tf.keras.layers.experimental.RandomFourierFeatures": {"description": "Layer that projects its inputs into a random feature space.", "Args": {"output_dim": "Positive integer, the dimension of the layer's output, i.e., the\nnumber of random features used to approximate the kernel.", "kernel_initializer": "Determines the distribution of the parameters of the\nrandom features map (and therefore the kernel approximated by the layer).\nIt can be either a string identifier or a Keras Initializer instance.\nCurrently only 'gaussian' and 'laplacian' are supported string\nidentifiers (case insensitive). Note that the kernel matrix is not\ntrainable.", "scale": "For Gaussian and Laplacian kernels, this corresponds to a scaling\nfactor of the corresponding kernel approximated by the layer (see concrete\ndefinitions above). When provided, it should be a positive float. If None,\na default value is used: if the kernel initializer is set to \"gaussian\",\nscale defaults to sqrt(input_dim / 2), otherwise, it defaults to 1.0.\nBoth the approximation error of the kernel and the classification quality\nare sensitive to this parameter. If trainable is set to True, this\nparameter is learned end-to-end during training and the provided value\nserves as the initial value.\nNote: When features from this layer are fed to a linear model,\n  by making scale trainable, the resulting optimization problem is\n  no longer convex (even if the loss function used by the linear model\n  is convex).", "trainable": "Whether the scaling parameter of the layer should be trainable.\nDefaults to False.", "name": "String, name to use for this layer."}}, "tf.keras.layers.experimental.SyncBatchNormalization": {"description": "Normalize and scale inputs or activations synchronously across replicas.", "Args": {"axis": "Integer, the axis that should be normalized\n(typically the features axis).\nFor instance, after a Conv2D layer with\ndata_format=\"channels_first\",\nset axis=1 in BatchNormalization.", "momentum": "Momentum for the moving average.", "epsilon": "Small float added to variance to avoid dividing by zero.", "center": "If True, add offset of beta to normalized tensor.\nIf False, beta is ignored.", "scale": "If True, multiply by gamma.\nIf False, gamma is not used.\nWhen the next layer is linear (also e.g. nn.relu),\nthis can be disabled since the scaling\nwill be done by the next layer.", "beta_initializer": "Initializer for the beta weight.", "gamma_initializer": "Initializer for the gamma weight.", "moving_mean_initializer": "Initializer for the moving mean.", "moving_variance_initializer": "Initializer for the moving variance.", "beta_regularizer": "Optional regularizer for the beta weight.", "gamma_regularizer": "Optional regularizer for the gamma weight.", "beta_constraint": "Optional constraint for the beta weight.", "gamma_constraint": "Optional constraint for the gamma weight."}}}, "tf.keras.optimizers.experimental": {"tf.keras.optimizers.experimental.Adadelta": {"description": "Optimizer that implements the Adadelta algorithm.", "Args": {"learning_rate": "Initial value for the learning rate:\neither a floating point value,\nor a tf.keras.optimizers.schedules.LearningRateSchedule instance.\nDefaults to 0.001.\nNote that Adadelta tends to benefit from higher initial learning rate\nvalues compared to other optimizers.\nTo match the exact form in the original paper, use 1.0.", "rho": "A Tensor or a floating point value. The decay rate. Defaults to 0.95.", "epsilon": "Small floating point value used to maintain numerical stability.\nDefaults to 1e-7.", "name": "String. The name to use\nfor momentum accumulator weights created by\nthe optimizer.", "clipnorm": "Float. If set, the gradient of each weight is individually\nclipped so that its norm is no higher than this value.", "clipvalue": "Float. If set, the gradient of each weight is clipped to be no\nhigher than this value.", "global_clipnorm": "Float. If set, the gradient of all weights is clipped so\nthat their global norm is no higher than this value.", "use_ema": "Boolean, defaults to False. If True, exponential moving average\n(EMA) is applied. EMA consists of computing an exponential moving\naverage of the weights of the model (as the weight values change after\neach training batch), and periodically overwriting the weights with\ntheir moving average.", "ema_momentum": "Float, defaults to 0.99. Only used if use_ema=True. This is\nthe momentum to use when computing the EMA of the model's weights:\nnew_average = ema_momentum * old_average + (1 - ema_momentum) *\ncurrent_variable_value.", "ema_overwrite_frequency": "Int or None, defaults to None. Only used if\nuse_ema=True. Every ema_overwrite_frequency steps of iterations, we\noverwrite the model variable by its moving average. If None, the optimizer\n does not overwrite model variables in the middle of training, and you\nneed to explicitly overwrite the variables at the end of training\nby calling optimizer.finalize_variable_values() (which updates the model\nvariables in-place). When using the built-in fit() training loop, this\nhappens automatically after the last epoch, and you don't need to do\nanything.", "jit_compile": "Boolean, defaults to True. If True, the optimizer will use XLA\ncompilation. jit_compile cannot be True when training with\ntf.distribute.experimental.ParameterServerStrategy. Additionally,\nif no GPU device is found, this flag will be ignored.", "**kwargs": "keyword arguments only used for backward compatibility."}, "Attributes": {"iterations": "The number of training steps this optimizer has run.\nBy default, iterations would be incremented by one every time\napply_gradients() is called.", "learning_rate": ""}}, "tf.keras.optimizers.experimental.Adagrad": {"description": "Optimizer that implements the Adagrad algorithm.", "Args": {"learning_rate": "Initial value for the learning rate:\neither a floating point value,\nor a tf.keras.optimizers.schedules.LearningRateSchedule instance.\nDefaults to 0.001.\nNote that Adagrad tends to benefit from higher initial learning rate\nvalues compared to other optimizers.\nTo match the exact form in the original paper, use 1.0.", "initial_accumulator_value": "Floating point value.\nStarting value for the accumulators (per-parameter momentum values).\nMust be non-negative.", "epsilon": "Small floating point value used to maintain numerical stability.", "name": "String. The name to use\nfor momentum accumulator weights created by\nthe optimizer.", "clipnorm": "Float. If set, the gradient of each weight is individually\nclipped so that its norm is no higher than this value.", "clipvalue": "Float. If set, the gradient of each weight is clipped to be no\nhigher than this value.", "global_clipnorm": "Float. If set, the gradient of all weights is clipped so\nthat their global norm is no higher than this value.", "use_ema": "Boolean, defaults to False. If True, exponential moving average\n(EMA) is applied. EMA consists of computing an exponential moving\naverage of the weights of the model (as the weight values change after\neach training batch), and periodically overwriting the weights with\ntheir moving average.", "ema_momentum": "Float, defaults to 0.99. Only used if use_ema=True. This is\nthe momentum to use when computing the EMA of the model's weights:\nnew_average = ema_momentum * old_average + (1 - ema_momentum) *\ncurrent_variable_value.", "ema_overwrite_frequency": "Int or None, defaults to None. Only used if\nuse_ema=True. Every ema_overwrite_frequency steps of iterations, we\noverwrite the model variable by its moving average. If None, the optimizer\n does not overwrite model variables in the middle of training, and you\nneed to explicitly overwrite the variables at the end of training\nby calling optimizer.finalize_variable_values() (which updates the model\nvariables in-place). When using the built-in fit() training loop, this\nhappens automatically after the last epoch, and you don't need to do\nanything.", "jit_compile": "Boolean, defaults to True. If True, the optimizer will use XLA\ncompilation. jit_compile cannot be True when training with\ntf.distribute.experimental.ParameterServerStrategy. Additionally,\nif no GPU device is found, this flag will be ignored.", "**kwargs": "keyword arguments only used for backward compatibility."}, "Attributes": {"iterations": "The number of training steps this optimizer has run.\nBy default, iterations would be incremented by one every time\napply_gradients() is called.", "learning_rate": ""}}, "tf.keras.optimizers.experimental.Adam": {"description": "Optimizer that implements the Adam algorithm.", "Args": {"learning_rate": "A tf.Tensor, floating point value, a schedule that is a\ntf.keras.optimizers.schedules.LearningRateSchedule, or a callable\nthat takes no arguments and returns the actual value to use. The\nlearning rate. Defaults to 0.001.", "beta_1": "A float value or a constant float tensor, or a callable\nthat takes no arguments and returns the actual value to use. The\nexponential decay rate for the 1st moment estimates. Defaults to 0.9.", "beta_2": "A float value or a constant float tensor, or a callable\nthat takes no arguments and returns the actual value to use. The\nexponential decay rate for the 2nd moment estimates. Defaults to 0.999.", "epsilon": "A small constant for numerical stability. This epsilon is\n\"epsilon hat\" in the Kingma and Ba paper (in the formula just before\nSection 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to\n1e-7.", "amsgrad": "Boolean. Whether to apply AMSGrad variant of this algorithm from\nthe paper \"On the Convergence of Adam and beyond\". Defaults to False.", "name": "String. The name to use\nfor momentum accumulator weights created by\nthe optimizer.", "clipnorm": "Float. If set, the gradient of each weight is individually\nclipped so that its norm is no higher than this value.", "clipvalue": "Float. If set, the gradient of each weight is clipped to be no\nhigher than this value.", "global_clipnorm": "Float. If set, the gradient of all weights is clipped so\nthat their global norm is no higher than this value.", "use_ema": "Boolean, defaults to False. If True, exponential moving average\n(EMA) is applied. EMA consists of computing an exponential moving\naverage of the weights of the model (as the weight values change after\neach training batch), and periodically overwriting the weights with\ntheir moving average.", "ema_momentum": "Float, defaults to 0.99. Only used if use_ema=True. This is\nthe momentum to use when computing the EMA of the model's weights:\nnew_average = ema_momentum * old_average + (1 - ema_momentum) *\ncurrent_variable_value.", "ema_overwrite_frequency": "Int or None, defaults to None. Only used if\nuse_ema=True. Every ema_overwrite_frequency steps of iterations, we\noverwrite the model variable by its moving average. If None, the optimizer\n does not overwrite model variables in the middle of training, and you\nneed to explicitly overwrite the variables at the end of training\nby calling optimizer.finalize_variable_values() (which updates the model\nvariables in-place). When using the built-in fit() training loop, this\nhappens automatically after the last epoch, and you don't need to do\nanything.", "jit_compile": "Boolean, defaults to True. If True, the optimizer will use XLA\ncompilation. jit_compile cannot be True when training with\ntf.distribute.experimental.ParameterServerStrategy. Additionally,\nif no GPU device is found, this flag will be ignored.", "**kwargs": "keyword arguments only used for backward compatibility."}, "Attributes": {"iterations": "The number of training steps this optimizer has run.\nBy default, iterations would be incremented by one every time\napply_gradients() is called.", "learning_rate": ""}}, "tf.keras.optimizers.experimental.AdamW": {"description": "Optimizer that implements the AdamW algorithm.", "Args": {"learning_rate": "A tf.Tensor, floating point value, a schedule that is a\ntf.keras.optimizers.schedules.LearningRateSchedule, or a callable\nthat takes no arguments and returns the actual value to use. The\nlearning rate. Defaults to 0.001.", "weight_decay": "A tf.Tensor, floating point value. The weight decay.\nDefaults to 0.004.", "beta_1": "A float value or a constant float tensor, or a callable\nthat takes no arguments and returns the actual value to use. The\nexponential decay rate for the 1st moment estimates. Defaults to 0.9.", "beta_2": "A float value or a constant float tensor, or a callable\nthat takes no arguments and returns the actual value to use. The\nexponential decay rate for the 2nd moment estimates. Defaults to 0.999.", "epsilon": "A small constant for numerical stability. This epsilon is\n\"epsilon hat\" in the Kingma and Ba paper (in the formula just before\nSection 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to\n1e-7.", "amsgrad": "Boolean. Whether to apply AMSGrad variant of this algorithm from\nthe paper \"On the Convergence of Adam and beyond\". Defaults to False.", "name": "String. The name to use\nfor momentum accumulator weights created by\nthe optimizer.", "clipnorm": "Float. If set, the gradient of each weight is individually\nclipped so that its norm is no higher than this value.", "clipvalue": "Float. If set, the gradient of each weight is clipped to be no\nhigher than this value.", "global_clipnorm": "Float. If set, the gradient of all weights is clipped so\nthat their global norm is no higher than this value.", "use_ema": "Boolean, defaults to False. If True, exponential moving average\n(EMA) is applied. EMA consists of computing an exponential moving\naverage of the weights of the model (as the weight values change after\neach training batch), and periodically overwriting the weights with\ntheir moving average.", "ema_momentum": "Float, defaults to 0.99. Only used if use_ema=True. This is\nthe momentum to use when computing the EMA of the model's weights:\nnew_average = ema_momentum * old_average + (1 - ema_momentum) *\ncurrent_variable_value.", "ema_overwrite_frequency": "Int or None, defaults to None. Only used if\nuse_ema=True. Every ema_overwrite_frequency steps of iterations, we\noverwrite the model variable by its moving average. If None, the optimizer\n does not overwrite model variables in the middle of training, and you\nneed to explicitly overwrite the variables at the end of training\nby calling optimizer.finalize_variable_values() (which updates the model\nvariables in-place). When using the built-in fit() training loop, this\nhappens automatically after the last epoch, and you don't need to do\nanything.", "jit_compile": "Boolean, defaults to True. If True, the optimizer will use XLA\ncompilation. jit_compile cannot be True when training with\ntf.distribute.experimental.ParameterServerStrategy. Additionally,\nif no GPU device is found, this flag will be ignored.", "**kwargs": "keyword arguments only used for backward compatibility."}, "Attributes": {"iterations": "The number of training steps this optimizer has run.\nBy default, iterations would be incremented by one every time\napply_gradients() is called.", "learning_rate": ""}}, "tf.keras.optimizers.experimental.Adamax": {"description": "Optimizer that implements the Adamax algorithm.", "Args": {"learning_rate": "A tf.Tensor, floating point value, a schedule that is a\ntf.keras.optimizers.schedules.LearningRateSchedule, or a callable\nthat takes no arguments and returns the actual value to use. The\nlearning rate. Defaults to 0.001.", "beta_1": "A float value or a constant float tensor. The exponential decay\nrate for the 1st moment estimates.", "beta_2": "A float value or a constant float tensor. The exponential decay\nrate for the exponentially weighted infinity norm.", "epsilon": "A small constant for numerical stability.", "name": "String. The name to use\nfor momentum accumulator weights created by\nthe optimizer.", "clipnorm": "Float. If set, the gradient of each weight is individually\nclipped so that its norm is no higher than this value.", "clipvalue": "Float. If set, the gradient of each weight is clipped to be no\nhigher than this value.", "global_clipnorm": "Float. If set, the gradient of all weights is clipped so\nthat their global norm is no higher than this value.", "use_ema": "Boolean, defaults to False. If True, exponential moving average\n(EMA) is applied. EMA consists of computing an exponential moving\naverage of the weights of the model (as the weight values change after\neach training batch), and periodically overwriting the weights with\ntheir moving average.", "ema_momentum": "Float, defaults to 0.99. Only used if use_ema=True. This is\nthe momentum to use when computing the EMA of the model's weights:\nnew_average = ema_momentum * old_average + (1 - ema_momentum) *\ncurrent_variable_value.", "ema_overwrite_frequency": "Int or None, defaults to None. Only used if\nuse_ema=True. Every ema_overwrite_frequency steps of iterations, we\noverwrite the model variable by its moving average. If None, the optimizer\n does not overwrite model variables in the middle of training, and you\nneed to explicitly overwrite the variables at the end of training\nby calling optimizer.finalize_variable_values() (which updates the model\nvariables in-place). When using the built-in fit() training loop, this\nhappens automatically after the last epoch, and you don't need to do\nanything.", "jit_compile": "Boolean, defaults to True. If True, the optimizer will use XLA\ncompilation. jit_compile cannot be True when training with\ntf.distribute.experimental.ParameterServerStrategy. Additionally,\nif no GPU device is found, this flag will be ignored.", "**kwargs": "keyword arguments only used for backward compatibility."}, "Attributes": {"iterations": "The number of training steps this optimizer has run.\nBy default, iterations would be incremented by one every time\napply_gradients() is called.", "learning_rate": ""}}, "tf.keras.optimizers.experimental.Ftrl": {"description": "Optimizer that implements the FTRL algorithm.", "Args": {"learning_rate": "A Tensor, floating point value, a schedule that is a\ntf.keras.optimizers.schedules.LearningRateSchedule, or a callable that\ntakes no arguments and returns the actual value to use. The learning rate.\nDefaults to 0.001.", "learning_rate_power": "A float value, must be less or equal to zero. Controls\nhow the learning rate decreases during training. Use zero for a fixed\nlearning rate.", "initial_accumulator_value": "The starting value for accumulators. Only zero or\npositive values are allowed.", "l1_regularization_strength": "A float value, must be greater than or equal to\nzero. Defaults to 0.0.", "l2_regularization_strength": "A float value, must be greater than or equal to\nzero. Defaults to 0.0.", "l2_shrinkage_regularization_strength": "A float value, must be greater than or\nequal to zero. This differs from L2 above in that the L2 above is a\nstabilization penalty, whereas this L2 shrinkage is a magnitude penalty.\nWhen input is sparse shrinkage will only happen on the active weights.", "beta": "A float value, representing the beta value from the paper. Defaults to\n0.0.", "name": "String. The name to use\nfor momentum accumulator weights created by\nthe optimizer.", "clipnorm": "Float. If set, the gradient of each weight is individually\nclipped so that its norm is no higher than this value.", "clipvalue": "Float. If set, the gradient of each weight is clipped to be no\nhigher than this value.", "global_clipnorm": "Float. If set, the gradient of all weights is clipped so\nthat their global norm is no higher than this value.", "use_ema": "Boolean, defaults to False. If True, exponential moving average\n(EMA) is applied. EMA consists of computing an exponential moving\naverage of the weights of the model (as the weight values change after\neach training batch), and periodically overwriting the weights with\ntheir moving average.", "ema_momentum": "Float, defaults to 0.99. Only used if use_ema=True. This is\nthe momentum to use when computing the EMA of the model's weights:\nnew_average = ema_momentum * old_average + (1 - ema_momentum) *\ncurrent_variable_value.", "ema_overwrite_frequency": "Int or None, defaults to None. Only used if\nuse_ema=True. Every ema_overwrite_frequency steps of iterations, we\noverwrite the model variable by its moving average. If None, the optimizer\n does not overwrite model variables in the middle of training, and you\nneed to explicitly overwrite the variables at the end of training\nby calling optimizer.finalize_variable_values() (which updates the model\nvariables in-place). When using the built-in fit() training loop, this\nhappens automatically after the last epoch, and you don't need to do\nanything.", "jit_compile": "Boolean, defaults to True. If True, the optimizer will use XLA\ncompilation. jit_compile cannot be True when training with\ntf.distribute.experimental.ParameterServerStrategy. Additionally,\nif no GPU device is found, this flag will be ignored.", "**kwargs": "keyword arguments only used for backward compatibility."}, "Attributes": {"iterations": "The number of training steps this optimizer has run.\nBy default, iterations would be incremented by one every time\napply_gradients() is called.", "learning_rate": ""}}, "tf.keras.optimizers.experimental.Nadam": {"description": "Optimizer that implements the Nadam algorithm.", "Args": {"learning_rate": "A tf.Tensor, floating point value, a schedule that is a\ntf.keras.optimizers.schedules.LearningRateSchedule, or a callable\nthat takes no arguments and returns the actual value to use. The\nlearning rate. Defaults to 0.001.", "beta_1": "A float value or a constant float tensor, or a callable\nthat takes no arguments and returns the actual value to use. The\nexponential decay rate for the 1st moment estimates. Defaults to 0.9.", "beta_2": "A float value or a constant float tensor, or a callable\nthat takes no arguments and returns the actual value to use. The\nexponential decay rate for the 2nd moment estimates. Defaults to 0.999.", "epsilon": "A small constant for numerical stability. This epsilon is\n\"epsilon hat\" in the Kingma and Ba paper (in the formula just before\nSection 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to\n1e-7.", "name": "String. The name to use\nfor momentum accumulator weights created by\nthe optimizer.", "clipnorm": "Float. If set, the gradient of each weight is individually\nclipped so that its norm is no higher than this value.", "clipvalue": "Float. If set, the gradient of each weight is clipped to be no\nhigher than this value.", "global_clipnorm": "Float. If set, the gradient of all weights is clipped so\nthat their global norm is no higher than this value.", "use_ema": "Boolean, defaults to False. If True, exponential moving average\n(EMA) is applied. EMA consists of computing an exponential moving\naverage of the weights of the model (as the weight values change after\neach training batch), and periodically overwriting the weights with\ntheir moving average.", "ema_momentum": "Float, defaults to 0.99. Only used if use_ema=True. This is\nthe momentum to use when computing the EMA of the model's weights:\nnew_average = ema_momentum * old_average + (1 - ema_momentum) *\ncurrent_variable_value.", "ema_overwrite_frequency": "Int or None, defaults to None. Only used if\nuse_ema=True. Every ema_overwrite_frequency steps of iterations, we\noverwrite the model variable by its moving average. If None, the optimizer\n does not overwrite model variables in the middle of training, and you\nneed to explicitly overwrite the variables at the end of training\nby calling optimizer.finalize_variable_values() (which updates the model\nvariables in-place). When using the built-in fit() training loop, this\nhappens automatically after the last epoch, and you don't need to do\nanything.", "jit_compile": "Boolean, defaults to True. If True, the optimizer will use XLA\ncompilation. jit_compile cannot be True when training with\ntf.distribute.experimental.ParameterServerStrategy. Additionally,\nif no GPU device is found, this flag will be ignored.", "**kwargs": "keyword arguments only used for backward compatibility."}, "Attributes": {"iterations": "The number of training steps this optimizer has run.\nBy default, iterations would be incremented by one every time\napply_gradients() is called.", "learning_rate": ""}}, "tf.keras.optimizers.experimental.Optimizer": {"description": "Abstract optimizer base class.", "Args": {"name": "String. The name to use\nfor momentum accumulator weights created by\nthe optimizer.", "clipnorm": "Float. If set, the gradient of each weight is individually\nclipped so that its norm is no higher than this value.", "clipvalue": "Float. If set, the gradient of each weight is clipped to be no\nhigher than this value.", "global_clipnorm": "Float. If set, the gradient of all weights is clipped so\nthat their global norm is no higher than this value.", "use_ema": "Boolean, defaults to False. If True, exponential moving average\n(EMA) is applied. EMA consists of computing an exponential moving\naverage of the weights of the model (as the weight values change after\neach training batch), and periodically overwriting the weights with\ntheir moving average.", "ema_momentum": "Float, defaults to 0.99. Only used if use_ema=True. This is\nthe momentum to use when computing the EMA of the model's weights:\nnew_average = ema_momentum * old_average + (1 - ema_momentum) *\ncurrent_variable_value.", "ema_overwrite_frequency": "Int or None, defaults to None. Only used if\nuse_ema=True. Every ema_overwrite_frequency steps of iterations, we\noverwrite the model variable by its moving average. If None, the optimizer\n does not overwrite model variables in the middle of training, and you\nneed to explicitly overwrite the variables at the end of training\nby calling optimizer.finalize_variable_values() (which updates the model\nvariables in-place). When using the built-in fit() training loop, this\nhappens automatically after the last epoch, and you don't need to do\nanything.", "jit_compile": "Boolean, defaults to True. If True, the optimizer will use XLA\ncompilation. jit_compile cannot be True when training with\ntf.distribute.experimental.ParameterServerStrategy. Additionally,\nif no GPU device is found, this flag will be ignored.", "**kwargs": "keyword arguments only used for backward compatibility."}, "Attributes": {"iterations": "The number of training steps this optimizer has run.\nBy default, iterations would be incremented by one every time\napply_gradients() is called.", "learning_rate": ""}}, "tf.keras.optimizers.experimental.RMSprop": {"description": "Optimizer that implements the RMSprop algorithm.", "Args": {"learning_rate": "Initial value for the learning rate:\neither a floating point value,\nor a tf.keras.optimizers.schedules.LearningRateSchedule instance.\nDefaults to 0.001.", "rho": "float, defaults to 0.9. Discounting factor for the old gradients.", "momentum": "float, defaults to 0.0. If not 0.0., the optimizer tracks the\nmomentum value, with a decay rate equals to 1 - momentum.", "epsilon": "A small constant for numerical stability. This epsilon is\n\"epsilon hat\" in the Kingma and Ba paper (in the formula just before\nSection 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to\n1e-7.", "centered": "Boolean. If True, gradients are normalized by the estimated\nvariance of the gradient; if False, by the uncentered second moment.\nSetting this to True may help with training, but is slightly more\nexpensive in terms of computation and memory. Defaults to False.", "name": "String. The name to use\nfor momentum accumulator weights created by\nthe optimizer.", "clipnorm": "Float. If set, the gradient of each weight is individually\nclipped so that its norm is no higher than this value.", "clipvalue": "Float. If set, the gradient of each weight is clipped to be no\nhigher than this value.", "global_clipnorm": "Float. If set, the gradient of all weights is clipped so\nthat their global norm is no higher than this value.", "use_ema": "Boolean, defaults to False. If True, exponential moving average\n(EMA) is applied. EMA consists of computing an exponential moving\naverage of the weights of the model (as the weight values change after\neach training batch), and periodically overwriting the weights with\ntheir moving average.", "ema_momentum": "Float, defaults to 0.99. Only used if use_ema=True. This is\nthe momentum to use when computing the EMA of the model's weights:\nnew_average = ema_momentum * old_average + (1 - ema_momentum) *\ncurrent_variable_value.", "ema_overwrite_frequency": "Int or None, defaults to None. Only used if\nuse_ema=True. Every ema_overwrite_frequency steps of iterations, we\noverwrite the model variable by its moving average. If None, the optimizer\n does not overwrite model variables in the middle of training, and you\nneed to explicitly overwrite the variables at the end of training\nby calling optimizer.finalize_variable_values() (which updates the model\nvariables in-place). When using the built-in fit() training loop, this\nhappens automatically after the last epoch, and you don't need to do\nanything.", "jit_compile": "Boolean, defaults to True. If True, the optimizer will use XLA\ncompilation. jit_compile cannot be True when training with\ntf.distribute.experimental.ParameterServerStrategy. Additionally,\nif no GPU device is found, this flag will be ignored.", "**kwargs": "keyword arguments only used for backward compatibility."}, "Attributes": {"iterations": "The number of training steps this optimizer has run.\nBy default, iterations would be incremented by one every time\napply_gradients() is called.", "learning_rate": ""}}, "tf.keras.optimizers.experimental.SGD": {"description": "Gradient descent (with momentum) optimizer.", "Args": {"learning_rate": "A Tensor, floating point value, or a schedule that is a\ntf.keras.optimizers.schedules.LearningRateSchedule, or a callable\nthat takes no arguments and returns the actual value to use. The\nlearning rate. Defaults to 0.001.", "momentum": "float hyperparameter >= 0 that accelerates gradient descent\nin the relevant\ndirection and dampens oscillations. Defaults to 0, i.e., vanilla gradient\ndescent.", "nesterov": "boolean. Whether to apply Nesterov momentum.\nDefaults to False.", "name": "String. The name to use\nfor momentum accumulator weights created by\nthe optimizer.", "clipnorm": "Float. If set, the gradient of each weight is individually\nclipped so that its norm is no higher than this value.", "clipvalue": "Float. If set, the gradient of each weight is clipped to be no\nhigher than this value.", "global_clipnorm": "Float. If set, the gradient of all weights is clipped so\nthat their global norm is no higher than this value.", "use_ema": "Boolean, defaults to False. If True, exponential moving average\n(EMA) is applied. EMA consists of computing an exponential moving\naverage of the weights of the model (as the weight values change after\neach training batch), and periodically overwriting the weights with\ntheir moving average.", "ema_momentum": "Float, defaults to 0.99. Only used if use_ema=True. This is\nthe momentum to use when computing the EMA of the model's weights:\nnew_average = ema_momentum * old_average + (1 - ema_momentum) *\ncurrent_variable_value.", "ema_overwrite_frequency": "Int or None, defaults to None. Only used if\nuse_ema=True. Every ema_overwrite_frequency steps of iterations, we\noverwrite the model variable by its moving average. If None, the optimizer\n does not overwrite model variables in the middle of training, and you\nneed to explicitly overwrite the variables at the end of training\nby calling optimizer.finalize_variable_values() (which updates the model\nvariables in-place). When using the built-in fit() training loop, this\nhappens automatically after the last epoch, and you don't need to do\nanything.", "jit_compile": "Boolean, defaults to True. If True, the optimizer will use XLA\ncompilation. jit_compile cannot be True when training with\ntf.distribute.experimental.ParameterServerStrategy. Additionally,\nif no GPU device is found, this flag will be ignored.", "**kwargs": "keyword arguments only used for backward compatibility."}, "Attributes": {"iterations": "The number of training steps this optimizer has run.\nBy default, iterations would be incremented by one every time\napply_gradients() is called.", "learning_rate": ""}}}, "tf.keras.utils.experimental": {"tf.keras.utils.experimental.DatasetCreator": {"description": "Object that returns a tf.data.Dataset upon invoking.", "Args": {"dataset_fn": "A callable that takes a single argument of type\ntf.distribute.InputContext, which is used for batch size calculation and\ncross-worker input pipeline sharding (if neither is needed, the\nInputContext parameter can be ignored in the dataset_fn), and returns\na tf.data.Dataset.", "input_options": "Optional tf.distribute.InputOptions, used for specific\noptions when used with distribution, for example, whether to prefetch\ndataset elements to accelerator device memory or host device memory, and\nprefetch buffer size in the replica device memory. No effect if not used\nwith distributed training. See tf.distribute.InputOptions for more\ninformation."}}}, "tf.linalg.experimental": {"tf.linalg.experimental.conjugate_gradient": {"description": "Conjugate gradient solver.", "Args": {"operator": "A LinearOperator that is self-adjoint and positive definite.", "rhs": "A possibly batched vector of shape [..., N] containing the right-hand\nsize vector.", "preconditioner": "A LinearOperator that approximates the inverse of A.\nAn efficient preconditioner could dramatically improve the rate of\nconvergence. If preconditioner represents matrix M(M approximates\nA^{-1}), the algorithm uses preconditioner.apply(x) to estimate\nA^{-1}x. For this to be useful, the cost of applying M should be\nmuch lower than computing A^{-1} directly.", "x": "A possibly batched vector of shape [..., N] containing the initial\nguess for the solution.", "tol": "A float scalar convergence tolerance.", "max_iter": "An integer giving the maximum number of iterations.", "name": "A name scope for the operation."}, "Returns": "output\n\n\nA namedtuple representing the final state with fields:\n\ni: A scalar int32 Tensor. Number of iterations executed.\nx: A rank-1 Tensor of shape [..., N] containing the computed\nsolution.\nr: A rank-1 Tensor of shape [.., M] containing the residual vector.\np: A rank-1 Tensor of shape [..., N]. A-conjugate basis vector.\ngamma: , equivalent to   when\npreconditioner=None."}}, "tf.lite.experimental": {"tf.lite.experimental.Analyzer": {"description": "Provides a collection of TFLite model analyzer tools."}, "tf.lite.experimental.OpResolverType": {"description": "Different types of op resolvers for Tensorflow Lite.", "Class Variables": {"AUTO": "<OpResolverType.AUTO: 0>", "BUILTIN": "<OpResolverType.BUILTIN: 1>", "BUILTIN_REF": "<OpResolverType.BUILTIN_REF: 2>", "BUILTIN_WITHOUT_DEFAULT_DELEGATES": "<OpResolverType.BUILTIN_WITHOUT_DEFAULT_DELEGATES: 3>"}}, "tf.lite.experimental.QuantizationDebugOptions": {"description": "Debug options to set up a given QuantizationDebugger.", "Args": {"layer_debug_metrics": "a dict to specify layer debug functions\n{function_name_str: function} where the function accepts result of\n  NumericVerify Op, which is value difference between float and\n  dequantized op results. The function returns single scalar value.", "model_debug_metrics": "a dict to specify model debug functions\n{function_name_str: function} where the function accepts outputs from\n  two models, and returns single scalar value for a metric. (e.g.\n  accuracy, IoU)", "layer_direct_compare_metrics": "a dict to specify layer debug functions\n{function_name_str: function}. The signature is different from that of\n  layer_debug_metrics, and this one gets passed (original float value,\n  original quantized value, scale, zero point). The function's\n  implementation is responsible for correctly dequantize the quantized\n  value to compare. Use this one when comparing diff is not enough.\n  (Note) quantized value is passed as int8, so cast to int32 is needed.", "denylisted_ops": "a list of op names which is expected to be removed from\nquantization.", "denylisted_nodes": "a list of op's output tensor names to be removed from\nquantization.", "fully_quantize": "Bool indicating whether to fully quantize the model.\nBesides model body, the input/output will be quantized as well.\nCorresponding to mlir_quantize's fully_quantize parameter."}, "Raises": {"ValueError": "when there are duplicate keys"}}, "tf.lite.experimental.QuantizationDebugger": {"description": "Debugger for Quantized TensorFlow Lite debug mode models.", "Args": {"quant_debug_model_path": "Path to the quantized debug TFLite model file.", "quant_debug_model_content": "Content of the quantized debug TFLite model.", "float_model_path": "Path to float TFLite model file.", "float_model_content": "Content of the float TFLite model.", "debug_dataset": "a factory function that returns dataset generator which is\nused to generate input samples (list of np.ndarray) for the model. The\ngenerated elements must have same types and shape as inputs to the\nmodel.", "debug_options": "Debug options to debug the given model.", "converter": "Optional, use converter instead of quantized model."}, "Raises": {"ValueError": "If the debugger was unable to be created."}, "Attributes": {"options": ""}}, "tf.lite.experimental.load_delegate": {"description": "Returns loaded Delegate object.", "Args": {"library": "Name of shared library containing the\nTfLiteDelegate.", "options": "Dictionary of options that are required to load the delegate. All\nkeys and values in the dictionary should be convertible to str. Consult\nthe documentation of the specific delegate for required and legal options.\n(default None)"}, "Returns": "Delegate object.", "Raises": {"ValueError": "Delegate failed to load.", "RuntimeError": "If delegate loading is used on unsupported platform."}}}, "tf.lookup.experimental": {"tf.lookup.experimental.DenseHashTable": {"description": "A mutable hash table with faster lookups and higher memory usage.", "Args": {"key_dtype": "the type of the key tensors.", "value_dtype": "the type of the value tensors.", "default_value": "The value to use if a key is missing in the table.", "empty_key": "the key to use to represent empty buckets internally. Must not\nbe used in insert, remove or lookup operations.", "deleted_key": "the key to use to represent deleted buckets internally. Must\nnot be used in insert, remove or lookup operations and be different from\nthe empty_key.", "initial_num_buckets": "the initial number of buckets (optional,\ndefault to 2^17=131072). Note that the default value is\nrelatively large (~1MB), so if you are going to create many\ntables (likely the case when experimental_is_anonymous is\nTrue), you should set initial_num_buckets to a smaller\nvalue to reduce memory usage.", "name": "A name for the operation (optional).", "checkpoint": "if True, the contents of the table are saved to and restored\nfrom checkpoints. If shared_name is empty for a checkpointed table, it\nis shared using the table node name.", "experimental_is_anonymous": "Whether to use anonymous mode for the\ntable (default is False). In anonymous mode, the table\nresource can only be accessed via a resource handle. It can't\nbe looked up by a name. When all resource handles pointing to\nthat resource are gone, the resource will be deleted\nautomatically."}, "Raises": {"ValueError": "If checkpoint is True and no name was specified."}, "Attributes": {"key_dtype": "The table key dtype.", "name": "The name of the table.", "resource_handle": "Returns the resource handle associated with this Resource.", "value_dtype": "The table value dtype."}}, "tf.lookup.experimental.MutableHashTable": {"description": "A generic mutable hash table implementation.", "Args": {"key_dtype": "the type of the key tensors.", "value_dtype": "the type of the value tensors.", "default_value": "The value to use if a key is missing in the table.", "name": "A name for the operation (optional).", "checkpoint": "if True, the contents of the table are saved to and restored\nfrom checkpoints. If shared_name is empty for a checkpointed table, it\nis shared using the table node name.", "experimental_is_anonymous": "Whether to use anonymous mode for the\ntable (default is False). In anonymous mode, the table\nresource can only be accessed via a resource handle. It can't\nbe looked up by a name. When all resource handles pointing to\nthat resource are gone, the resource will be deleted\nautomatically."}, "Raises": {"ValueError": "If checkpoint is True and no name was specified."}, "Attributes": {"key_dtype": "The table key dtype.", "name": "The name of the table.", "resource_handle": "Returns the resource handle associated with this Resource.", "value_dtype": "The table value dtype."}}}, "tf.mlir.experimental": {"tf.mlir.experimental.convert_function": {"description": "Import a ConcreteFunction and convert it to a textual MLIR module.", "Args": {"concrete_function": "An object of type ConcreteFunction.", "pass_pipeline": "A textual description of an MLIR Pass Pipeline to run on the\nmodule, see MLIR documentation for the\ntextual pass pipeline syntax.", "show_debug_info": "Whether to include locations in the emitted textual form."}, "Returns": "A textual representation of the MLIR module corresponding to the\nConcreteFunction.", "Raises": {"InvalidArgumentError": "if concrete_function is invalid or cannot be converted\nto MLIR."}}, "tf.mlir.experimental.convert_graph_def": {"description": "Import a GraphDef and convert it to a textual MLIR module.", "Args": {"graph_def": "An object of type graph_pb2.GraphDef or a textual proto\nrepresentation of a valid GraphDef.", "pass_pipeline": "A textual description of an MLIR Pass Pipeline to run on the\nmodule, see MLIR documentation for the\ntextual pass pipeline syntax.", "show_debug_info": "Whether to include locations in the emitted textual form."}, "Returns": "A textual representation of the MLIR module corresponding to the graphdef.", "Raises": {"InvalidArgumentError": "if graph_def is invalid or cannot be converted to\nMLIR."}}}, "tf.nn.experimental": {"tf.nn.experimental.stateless_dropout": {"description": "Computes dropout: randomly sets elements to zero to prevent overfitting.", "Args": {"x": "A floating point tensor.", "rate": "A scalar Tensor with the same type as x. The probability\nthat each element is dropped. For example, setting rate=0.1 would drop\n10% of input elements.", "seed": "An integer tensor of shape [2]. The seed of the random numbers.", "rng_alg": "The algorithm used to generate the random numbers\n(default to \"auto_select\"). See the alg argument of\ntf.random.stateless_uniform for the supported values.", "noise_shape": "A 1-D integer Tensor, representing the\nshape for randomly generated keep/drop flags.", "name": "A name for this operation."}, "Returns": "A Tensor of the same shape and dtype of x.", "Raises": {"ValueError": "If rate is not in [0, 1) or if x is not a floating point\ntensor. rate=1 is disallowed, because the output would be all zeros,\nwhich is likely not what was intended."}}}, "tf.profiler.experimental": {"tf.profiler.experimental.Profile": {"description": "Context-manager profile API.", "Args": {"logdir": "profile data will save to this directory.", "options": "An optional tf.profiler.experimental.ProfilerOptions can be\nprovided to fine tune the profiler's behavior."}}, "tf.profiler.experimental.ProfilerOptions": {"description": "Options for finer control over the profiler.", "Attributes": {"host_tracer_level": "A namedtuple alias for field number 0", "python_tracer_level": "A namedtuple alias for field number 1", "device_tracer_level": "A namedtuple alias for field number 2", "delay_ms": "A namedtuple alias for field number 3"}}, "tf.profiler.experimental.Trace": {"description": "Context manager that generates a trace event in the profiler.", "Args": {"name": "The name of the trace event.", "**kwargs": "Keyword arguments added to the trace event.\n          Both the key and value are of types that\n          can be converted to strings, which will be\n          interpreted by the profiler according to the\n          traceme name.\nExample usage:\n\u00a0 tf.profiler.experimental.start('logdir')\u00a0 for step in range(num_steps):\u00a0 \u00a0 # Creates a trace event for each training step with the\u00a0 \u00a0 # step number.\u00a0 \u00a0 with tf.profiler.experimental.Trace(\"Train\", step_num=step):\u00a0 \u00a0 \u00a0 train_fn()\u00a0 tf.profiler.experimental.stop()\nThe example above uses the keyword argument \"step_num\" to specify the\ntraining step being traced."}}, "tf.profiler.experimental.start": {"description": "Start profiling TensorFlow performance.", "Args": {"logdir": "Profiling results log directory.", "options": "ProfilerOptions namedtuple to specify miscellaneous profiler\noptions. See example usage below."}, "Raises": {"AlreadyExistsError": "If a profiling session is already running."}}, "tf.profiler.experimental.stop": {"description": "Stops the current profiling session.", "Args": {"save": "An optional variable to save the results to TensorBoard. Default True."}, "Raises": {"UnavailableError": "If there is no active profiling session."}}}, "tf.random.experimental": {"tf.random.experimental.index_shuffle": {"description": "Outputs the position of index in a permutation of [0, ..., max_index].", "Args": {"index": "An integer scalar tensor or vector with values in [0, max_index].\nIt can be seen as either a value v in the sequence S=[0, ...,\nmax_index] to be permutated, or as an index of an element e in a\nshuffled vector.", "seed": "A tensor of shape [2] or [n, 2] with dtype int32/uint32/int64/uint64.\nThe RNG seed. If the rank is unknown during graph building it must be 1 at\nruntime.", "max_index": "A non-negative tensor with the same shape and dtype as index.\nThe upper bound (inclusive)."}, "Returns": "If all inputs were scalar (shape [2] for seed) the output will be a scalar\nwith the same dtype as index. The output can be seen as the new position\nof v in S, or as the index of e in the vector before shuffling.\nIf one or multiple inputs were vectors (shape [n, 2] for seed) then the\noutput will be a vector of the same size which each element shuffled\nindependently. Scalar values are broadcasted in this case."}, "tf.random.experimental.stateless_fold_in": {"description": "Folds in data to an RNG seed to form a new RNG seed.", "Args": {"seed": "an RNG seed (a tensor with shape [2] and dtype int32 or\nint64). (When using XLA, only int32 is allowed.)", "data": "an int32 or int64 scalar representing data to be folded in to the\nseed.", "alg": "The RNG algorithm used to generate the random numbers. See\ntf.random.stateless_uniform for a detailed explanation."}, "Returns": "A new RNG seed that is a deterministic function of the inputs and is\nstatistically safe for producing a stream of new pseudo-random values. It\nwill have the same dtype as data (if data doesn't have an explict dtype,\nthe dtype will be determined by tf.convert_to_tensor)."}, "tf.random.experimental.stateless_shuffle": {"description": "Randomly and deterministically shuffles a tensor along its first dimension.", "Args": {"value": "A Tensor to be shuffled.", "seed": "A shape [2] Tensor. The seed to the random number generator. Must have\ndtype int32 or int64.", "alg": "The RNG algorithm used to generate the random numbers. See\ntf.random.stateless_uniform for a detailed explanation.", "name": "A name for the operation."}, "Returns": "A tensor of same shape and type as value, shuffled along its first\ndimension."}, "tf.random.experimental.stateless_split": {"description": "Splits an RNG seed into num new seeds by adding a leading axis.", "Args": {"seed": "an RNG seed (a tensor with shape [2] and dtype int32 or\nint64). (When using XLA, only int32 is allowed.)", "num": "optional, a positive integer or scalar tensor indicating the number of\nseeds to produce (default 2).", "alg": "The RNG algorithm used to generate the random numbers. See\ntf.random.stateless_uniform for a detailed explanation."}, "Returns": "A tensor with shape [num, 2] representing num new seeds. It will have the\nsame dtype as seed (if seed doesn't have an explict dtype, the dtype\nwill be determined by tf.convert_to_tensor)."}}, "tf.saved_model.experimental": {"tf.saved_model.experimental.TrackableResource": {"description": "Holds a Tensor which a tf.function can capture.", "Args": {"device": "A string indicating a required placement for this resource,\ne.g. \"CPU\" if this resource must be created on a CPU device. A blank\ndevice allows the user to place resource creation, so generally this\nshould be blank unless the resource only makes sense on one device."}, "Attributes": {"resource_handle": "Returns the resource handle associated with this Resource."}}, "tf.saved_model.experimental.VariablePolicy": {"description": "Enum defining options for variable handling when saving.", "Class Variables": {"EXPAND_DISTRIBUTED_VARIABLES": "<VariablePolicy.EXPAND_DISTRIBUTED_VARIABLES: 'expand_distributed_variables'>", "NONE": "<VariablePolicy.NONE: None>", "SAVE_VARIABLE_DEVICES": "<VariablePolicy.SAVE_VARIABLE_DEVICES: 'save_variable_devices'>"}}}, "tf.tpu.experimental": {"tf.tpu.experimental.DeviceAssignment": {"description": "Mapping from logical cores in a computation to the physical TPU topology.", "Args": {"topology": "A Topology object that describes the physical TPU topology.", "core_assignment": "A logical to physical core mapping, represented as a\nrank 3 numpy array. See the description of the core_assignment\nproperty for more details."}, "Raises": {"ValueError": "If core_assignment is not a rank 3 numpy array."}, "Attributes": {"core_assignment": "The logical to physical core mapping.", "num_cores_per_replica": "The number of cores per replica.", "num_replicas": "The number of replicas of the computation.", "topology": "A Topology that describes the TPU topology."}}, "tf.tpu.experimental.HardwareFeature": {"description": "class holds all the feature info about the TPU.", "Args": {"tpu_hardware_feature_proto": "protobuf which describe the tpu hardware\nfeature."}, "Attributes": {"embedding_feature": "TPU embedding feature."}}, "tf.tpu.experimental.HardwareFeature.EmbeddingFeature": {"description": "Embedding feature flag strings.", "Class Variables": {"UNSUPPORTED": "<EmbeddingFeature.UNSUPPORTED: 'UNSUPPORTED'>", "V1": "<EmbeddingFeature.V1: 'V1'>", "V2": "<EmbeddingFeature.V2: 'V2'>"}}, "tf.tpu.experimental.TPUSystemMetadata": {"description": "Describes some metadata about the TPU system.", "Attributes": {"num_cores": "interger. Total number of TPU cores in the TPU system.", "num_hosts": "interger. Total number of hosts (TPU workers) in the TPU system.", "num_of_cores_per_host": "interger. Number of TPU cores per host (TPU worker).", "topology": "an instance of tf.tpu.experimental.Topology, which describes the\nphysical topology of TPU system.", "devices": "a tuple of strings, which describes all the TPU devices in the\nsystem."}}, "tf.tpu.experimental.Topology": {"description": "Describes a set of TPU devices.", "Args": {"serialized": "A serialized TopologyProto, or None. If not None, the\nserialized proto is parsed to discover the topology.", "mesh_shape": "A sequence of 4 positive integers, or None. If not None,\nthe shape of the TPU topology, in number of cores. Ignored if\nserialized is not None.", "device_coordinates": "A rank 4 numpy array that describes the mapping from\nTensorFlow TPU devices to TPU fabric coordinates, or None. Ignored\nif serialized is notNone`."}, "Raises": {"ValueError": "If serialized is None and device_coordinates is not a\nrank 4 numpy int32 array that describes a valid coordinate mapping."}, "Attributes": {"device_coordinates": "Describes the mapping from TPU devices to topology coordinates.", "mesh_rank": "Returns the number of dimensions in the mesh.", "mesh_shape": "A rank 1 int32 array describing the shape of the TPU topology.", "missing_devices": "Array of indices of missing devices.", "num_tasks": "Returns the number of TensorFlow tasks in the TPU slice.", "num_tpus_per_task": "Returns the number of TPU devices per task in the TPU slice."}}, "tf.tpu.experimental.initialize_tpu_system": {"description": "Initialize the TPU devices.", "Args": {"cluster_resolver": "A tf.distribute.cluster_resolver.TPUClusterResolver,\nwhich provides information about the TPU cluster."}, "Returns": "The tf.tpu.Topology object for the topology of the TPU cluster. If called\ninside tf.function, it returns the serialized topology object instead.", "Raises": {"RuntimeError": "If running inside a tf.function.", "NotFoundError": "If no TPU devices found in eager mode."}}, "tf.tpu.experimental.shutdown_tpu_system": {"description": "Shuts down the TPU devices.", "Args": {"cluster_resolver": "A tf.distribute.cluster_resolver.TPUClusterResolver,\nwhich provides information about the TPU cluster."}, "Raises": {"RuntimeError": "If no TPU devices found for eager execution or if run in a\ntf.function."}}}, "tf.train.experimental": {"tf.train.experimental.PythonState": {"description": "A mixin for putting Python state in an object-based checkpoint."}}, "tf.types.experimental": {"tf.types.experimental.Callable": {"description": "Base class for TF callables like those created by tf.function."}, "tf.types.experimental.ConcreteFunction": {"description": "Base class for graph functions."}, "tf.types.experimental.GenericFunction": {"description": "Base class for polymorphic graph functions."}, "tf.types.experimental.SupportsTracingProtocol": {"description": "A protocol allowing custom classes to control tf.function retracing."}, "tf.types.experimental.TensorLike": {"description": "Union of all types that can be converted to a tf.Tensor by tf.convert_to_tensor."}, "tf.types.experimental.TraceType": {"description": "Represents the type of object(s) for tf.function tracing purposes."}, "tf.types.experimental.TracingContext": {"description": "Contains information scoped to the tracing of multiple objects."}}, "tf.xla.experimental": {"tf.xla.experimental.compile": {"description": "Builds an operator that compiles and runs computation with XLA. (deprecated)", "Args": {"computation": "A Python function that builds a computation to apply to the\ninput. If the function takes n inputs, 'inputs' should be a list of n\ntensors.\ncomputation may return a list of operations and tensors.  Tensors must\ncome before operations in the returned list.  The return value of\ncompile is a list of tensors corresponding to the tensors from the\noutput of computation.\nAll Operations returned from computation will be executed when\nevaluating any of the returned output tensors.", "inputs": "A list of inputs or None (equivalent to an empty list). Each input\ncan be a nested structure containing values that are convertible to\ntensors. Note that passing an N-dimension list of compatible values will\nresult in a N-dimension list of scalar tensors rather than a single Rank-N\ntensors. If you need different behavior, convert part of inputs to tensors\nwith tf.convert_to_tensor."}, "Returns": "Same data structure as if computation(*inputs) is called directly with some\nexceptions for correctness. Exceptions include:\n1) None output: a NoOp would be returned which control-depends on\n     computation.\n  2) Single value output: A tuple containing the value would be returned.\n  3) Operation-only outputs: a NoOp would be returned which\n     control-depends on computation.", "Raises": {"RuntimeError": "if called when eager execution is enabled."}}, "tf.xla.experimental.jit_scope": {"description": "Enable or disable JIT compilation of operators within the scope.", "Args": {"compile_ops": "Whether to enable or disable compilation in the scope.\nEither a Python bool, or a callable that accepts the parameter\nnode_def and returns a python bool.", "separate_compiled_gradients": "If true put each gradient subgraph into a\nseparate compilation scope. This gives fine-grained control over which\nportions of the graph will be compiled as a single unit. Compiling\ngradients separately may yield better performance for some graphs.\nThe scope is named based on the scope of the forward computation as well\nas the name of the gradients. As a result, the gradients will be compiled\nin a scope that is separate from both the forward computation, and from\nother gradients."}, "Raises": {"RuntimeError": "if called when eager execution is enabled."}}}}