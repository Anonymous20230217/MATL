
( name :  )
( job : string   optional job name  ) ( replica : int   optional replica index  ) ( task : int   optional task index  ) ( device type : optional device type string  e g  "cpu" or "gpu"  ) ( device index : int   optional device index   if leave unspecified  device represent 'any' device index  )
( persistent : boolean control whether a persistent gradient tape be create  false by default  which mean at most one call can be make to the gradient   method on this object  ) ( watch accessed variables : boolean control whether the tape will automatically watch any  trainable  variables access while the tape be active  default to true mean gradients can be request from any result compute in the tape derive from read a trainable variable  if false users must explicitly watch any variables they want to request gradients from  )
( building function : return true iff this graph represent a function  ) ( collections : return the name of the collections know to this graph  ) ( finalized : true if this graph have be finalize  ) ( graph def versions : the graphdef version information of this graph  for detail on the mean of each version  see graphdef  ) ( seed : the graph level random seed of this graph  ) ( version : return a version number that increase as ops be add to the graph  note that this be unrelated to the tf graph graph def versions  )
( dense shape : a 1 d tensor contain the shape of the correspond dense tensor  ) ( device : the name of the device on which value will be produce  or none  ) ( dtype : the dtype of elements in this tensor  ) ( graph : the graph that contain the value  indices  and shape tensors  ) ( indices : a 1 d tensor contain the indices of the slice  ) ( name : the name of this indexedslices  ) ( op : the operation that produce value as an output  ) ( shape : get the tf tensorshape represent the shape of the dense tensor  ) ( values : a tensor contain the value of the slice  )
( shape : the dense shape of the indexedslices  or none to allow any dense shape  ) ( dtype : tf dtype of value in the indexedslices  ) ( indices dtype : tf dtype of the indices in the indexedslices   one of tf int32 or tf int64  ) ( dense shape dtype : tf dtype of the dense shape in the indexedslices  one of tf int32  tf int64  or none  if the indexedslices have no dense shape tensor   ) ( indices shape : the shape of the indices component  which indicate how many slice be in the indexedslices  )
( name : return the name of this module as pass or determine in the ctor  note  this be not the same as the self name scope name which include parent module name  ) ( name scope : return a tf name scope instance for this class  ) ( non trainable variables : sequence of non trainable variables own by this module and its submodules note  this method use reflection to find variables on the current instance and submodules  for performance reason you may wish to cache the result of call this method if you don't expect the return value to change  ) ( submodules : sequence of all sub modules  submodules be modules which be properties of this module  or find as properties of modules which be properties of this module  and so on   a   tf module  b   tf module  c   tf module  a b   bb c   clist a submodules      b  c truelist b submodules      c truelist c submodules       true ) ( trainable variables : sequence of trainable variables own by this module and its submodules  note  this method use reflection to find variables on the current instance and submodules  for performance reason you may wish to cache the result of call this method if you don't expect the return value to change  ) ( variables : sequence of variables own by this module and its submodules note  this method use reflection to find variables on the current instance and submodules  for performance reason you may wish to cache the result of call this method if you don't expect the return value to change  )
( node def : node def pb2 nodedef   nodedef for the operation  use for attribute of node def pb2 nodedef  typically name  op  and device   the input attribute be irrelevant here as it will be compute when generate the model  ) ( g : graph  the parent graph  ) ( inputs : list of tensor object  the input to this operation  ) ( output types : list of dtype object   list of the type of the tensors compute by this operation   the length of this list indicate the number of output endpoints of the operation  ) ( control inputs : list of operations or tensors from which to have a control dependency  ) ( input types : list of dtype object represent the type of the tensors accept by the operation   by default use  x dtype base dtype for x in input    operations that expect reference type input must specify these explicitly  ) ( original op : optional  use to associate the new operation with an exist operation  for example  a replica with the op that be replicate   ) ( op def : optional  the op def pb2 opdef proto that describe the op type that this operation represent  )
( element spec : a  nest  structure of typespec object that represent the type specification of the optional element  ) ( value type : the python type for value that be compatible with this typespec  in particular  all value that be compatible with this typespec must be an instance of this type  )
( dtype : the dtype of value in this tensor  ) ( flat values : the innermost value tensor for this rag tensor  concretely  if rt value be a tensor  then rt flat value be rt value  otherwise  rt flat value be rt value flat value  conceptually  flat value be the tensor form by flatten the outermost dimension and all of the rag dimension into a single dimension  rt flat value shape    nvals    rt shape rt rag rank   1    where nvals be the number of items in the flatten dimension   example  rt   tf rag constant    3  1  4  1        5  9  2          6        print rt flat value tf tensor  3 1 4 1 5 9 2 6   shape  8    dtype int32  ) ( nested row splits : a tuple contain the row split for all rag dimension  rt nest row split be a tuple contain the row split tensors for all rag dimension in rt  order from outermost to innermost   in particular  rt nest row split    rt row split     value split where    `value split     ` if `rt values` be a `tensor`   `value split   rt value nest row splits` otherwise  example  rt   tf rag constant         3  1  4  1        5  9  2          6         for i  split in enumerate rt nest row split    print 'splits for dimension  d   s'    i 1  split numpy    split for dimension 1   0 3 split for dimension 2   0 3 3 5 split for dimension 3   0 4 4 7 8 8  ) ( ragged rank : the number of time the raggedtensor's flat value be partition  value   tf rag constant   1  2  3    4    5  6    7  8  9  10   value rag rank1 rt   tf raggedtensor from uniform row length value  2 rt rag rank2 ) ( row splits : the row split indices for this rag tensor's value  rt row split specify where the value for each row begin and end in rt value   in particular  the value for row rt i  be store in the slice rt value rt row split i  rt row split i 1    example  rt   tf rag constant   3  1  4  1        5  9  2    6       print rt row split     indices of row split in rt valuestf tensor  0 4 4 7 8 8   shape  6    dtype int64  ) ( shape : the statically know shape of this rag tensor  tf rag constant   0    1  2    shapetensorshape  2  none   tf rag constant    0  1      1  2    3  4     rag rank 1  shapetensorshape  2  none  2   ) ( uniform row length : the length of each row in this rag tensor  or none if row be rag  rt1   tf rag constant   1  2  3    4    5  6    7  8  9  10   print rt1 uniform row length     row be rag none rt2   tf raggedtensor from uniform row length     value rt1  uniform row length 2 print rt2 <tf raggedtensor    1  2  3    4      5  6    7  8  9  10   >print rt2 uniform row length     row be not rag  all have size 2  tf tensor 2  shape     dtype int64  a raggedtensor's row be only consider to be uniform  i e  non rag  if it can be determine statically  at graph construction time  that the row all have the same length  ) ( values : the concatenate row for this rag tensor  rt value be a potentially rag tensor form by flatten the two outermost dimension of rt into a single dimension  rt value shape    nvals    rt shape 2    where nvals be the number of items in the outer two dimension of rt   rt rag rank   self rag rank   1 example  rt   tf rag constant   3  1  4  1        5  9  2    6       print rt value tf tensor  3 1 4 1 5 9 2 6   shape  8    dtype int32  )
( shape : the shape of the raggedtensor  or none to allow any shape   if a shape be specify  then all rag dimension must have size none  ) ( dtype : tf dtype of value in the raggedtensor  ) ( ragged rank : python integer  the number of time the raggedtensor's flat value be partition   default to shape ndims   1  ) ( row splits dtype : dtype for the raggedtensor's row split tensor  one of tf int32 or tf int64  ) ( flat values spec : typespec for flat value of the raggedtensor  it shall be provide when the flat value be a compositetensor rather then tensor  if both dtype and flat value spec and  be provide  dtype must be the same as flat value spec dtype   experimental  )
( op type : the string type of an operation  this correspond to the opdef name field for the proto that define the operation  )
( shape : the dense shape of the sparsetensor  or none to allow any dense shape  ) ( dtype : tf dtype of value in the sparsetensor  )
( op : an operation  operation that compute this tensor  ) ( value index : an int  index of the operation's endpoint that produce this tensor  ) ( dtype : a dtype  type of elements store in this tensor  )
( dtype :  require  data type of the tensorarray  ) ( size :  optional  int32 scalar tensor  the size of the tensorarray  require if handle be not provide  ) ( dynamic size :  optional  python bool  if true  write to the tensorarray can grow the tensorarray past its initial size   default  false  ) ( clear after read : boolean  optional  default  true    if true  clear tensorarray value after read them   this disable read many semantics  but allow early release of memory  ) ( tensor array name :  optional  python string  the name of the tensorarray  this be use when create the tensorarray handle   if this value be set  handle should be none  ) ( handle :  optional  a tensor handle to an exist tensorarray   if this be set  tensor array name should be none  only support in graph mode  ) ( flow :  optional  a float tensor scalar come from an exist tensorarray flow  only support in graph mode  ) ( infer shape :  optional  default  true  if true  shape inference be enable   in this case  all elements must have the same shape  ) ( element shape :  optional  default  none  a tensorshape object specify the shape constraints of each of the elements of the tensorarray  need not be fully define  ) ( colocate with first write call : if true  the tensorarray will be colocated on the same device as the tensor use on its first write  write operations include write  unstack  and split    if false  the tensorarray will be place on the device determine by the device context available during its initialization  ) ( name : a name for the operation  optional   )
( element shape : the shape of each element in the tensorarray  ) ( dtype : data type of the tensorarray  ) ( dynamic size : whether the tensorarray can grow past its initial size  ) ( infer shape : whether shape inference be enable  )
( dims : a list of dimension  or none if the shape be unspecified  )
( shape : value convertible to tf tensorshape  the shape of the tensor  ) ( dtype : value convertible to tf dtype  the type of the tensor value  ) ( name : optional name for the tensor  )
( value type : the python type for value that be compatible with this typespec  in particular  all value that be compatible with this typespec must be an instance of this type  )

( initial value : a tensor  or python object convertible to a tensor  which be the initial value for the variable  the initial value must have a shape specify unless validate shape be set to false  can also be a callable with no argument that return the initial value when call  in that case  dtype must be specify   note that initializer function from init ops py must first be bind to a shape before be use here   ) ( trainable : if true  gradienttapes automatically watch use of this variable  default to true  unless synchronization be set to on read  in which case it default to false  ) ( validate shape : if false  allow the variable to be initialize with a value of unknown shape  if true  the default  the shape of initial value must be know  ) ( caching device : note  this argument be only valid when use a v1 style session  optional device string describe where the variable should be cache for read  default to the variable's device  if not none  cache on another device  typical use be to cache on the device where the ops use the variable reside  to deduplicate copy through switch and other conditional statements  ) ( name : optional name for the variable  default to 'variable' and get uniquified automatically  ) ( variable def : variabledef protocol buffer  if not none  recreate the variable object with its content  reference the variable's nod in the graph  which must already exist  the graph be not change  variable def and the other arguments be mutually exclusive  ) ( dtype : if set  initial value will be convert to the give type  if none  either the datatype will be keep  if initial value be a tensor   or convert to tensor will decide  ) ( import scope : optional string  name scope to add to the variable  only use when initialize from protocol buffer  ) ( constraint : an optional projection function to be apply to the variable after be update by an optimizer  e g  use to implement norm constraints or value constraints for layer weight   the function must take as input the unprojected tensor represent the value of the variable and return the tensor for the project value  which must have the same shape   constraints be not safe to use when do asynchronous distribute train  ) ( synchronization : indicate when a distribute a variable will be aggregate  accept value be constants define in the class tf variablesynchronization  by default the synchronization be set to auto and the current distributionstrategy choose when to synchronize  ) ( aggregation : indicate how a distribute variable will be aggregate  accept value be constants define in the class tf variableaggregation  ) ( shape :  optional  the shape of this variable  if none  the shape of initial value will be use  when set this argument to tf tensorshape none   represent an unspecified shape   the variable can be assign with value of different shape  )


( values : 1 d or higher numeric tensor  ) ( axis : the axis along which to sort  the default be  1  which sort the last axis  ) ( direction : the direction in which to sort the value  'ascending' or 'descending'   ) ( stable : if true  equal elements in the original tensor will not be re order in the return order  unstable sort be not yet implement  but will eventually be the default for performance reason  if you require a stable order  pass stable true for forward compatibility  ) ( name : optional name for the operation  )
( input : a n d tensor with shape input shape    batch    spatial shape   remain shape  where spatial shape have m dimension  ) ( block shape : a 1 d tensor with shape  m   must be one of the follow type  int32  int64  all value must be >  1  for backwards compatibility with tf 1 0  this parameter may be an int  in which case it be convert to numpy array  block shape  block shape   dtype numpy int64   ) ( crops : a  2 d tensor with shape  m  2   must be one of the follow type  int32  int64  all value must be >  0  crop i     crop start  crop end  specify the amount to crop from input dimension i   1  which correspond to spatial dimension i  it be require that crop start i    crop end i  <  block shape i    input shape i   1   this operation be equivalent to the follow step   reshape input to reshape of shape   block shape 0        block shape m 1   batch / prod block shape   input shape 1        input shape n 1   permute dimension of reshape to produce permute of shape  batch / prod block shape    input shape 1   block shape 0        input shape m   block shape m 1   input shape m 1        input shape n 1   reshape permute to produce reshape permute of shape  batch / prod block shape   input shape 1    block shape 0        input shape m    block shape m 1   input shape m 1        input shape n 1   crop the start and end of dimension  1       m  of reshape permute accord to crop to produce the output of shape   batch / prod block shape    input shape 1     block shape 0    crop 0 0    crop 0 1        input shape m     block shape m 1    crop m 1 0    crop m 1 1    input shape m 1         input shape n 1   ) ( name : a name for the operation  optional   )
( input : a tensor  must be one of the follow type  bfloat16  half  float32  float64  int64  int32  uint8  uint16  uint32  uint64  int8  int16  complex64  complex128  qint8  quint8  qint16  quint16  qint32  ) ( type : a tf dtype from  tf bfloat16  tf half  tf float32  tf float64  tf int64  tf int32  tf uint8  tf uint16  tf uint32  tf uint64  tf int8  tf int16  tf complex64  tf complex128  tf qint8  tf quint8  tf qint16  tf quint16  tf qint32  ) ( name : a name for the operation  optional   )
( tensor : n d tensor  ) ( mask : k d boolean tensor  k <  n and k must be know statically  ) ( axis : a 0 d int tensor represent the axis in tensor to mask from  by default  axis be 0 which will mask from the first dimension  otherwise k   axis <  n  ) ( name : a name for this operation  optional   )
( shape x : a rank 1 integer tensor  represent the shape of x  ) ( shape y : a rank 1 integer tensor  represent the shape of y  )
( shape x : a tensorshape ) ( shape y : a tensorshape )
( input : a tensor  a tensor to broadcast  ) ( shape : a tensor  must be one of the follow type  int32  int64  an 1 d int tensor  the shape of the desire output  ) ( name : a name for the operation  optional   )
( pred fn pairs : list of pair of a boolean scalar tensor and a callable which return a list of tensors  ) ( default : optional callable that return a list of tensors  ) ( exclusive : true iff at most one predicate be allow to evaluate to true  ) ( strict : a boolean that enables/disables 'strict' mode  see above  ) ( name : a name for this operation  optional   )
( x : a tensor or sparsetensor or indexedslices of numeric type  it could be uint8  uint16  uint32  uint64  int8  int16  int32  int64  float16  float32  float64  complex64  complex128  bfloat16  ) ( dtype : the destination type  the list of support dtypes be the same as x  ) ( name : a name for the operation  optional   )
( t list : a tuple or list of mix tensors  indexedslices  or none  ) ( clip norm : a 0 d  scalar  tensor > 0  the clip ratio  ) ( use norm : a 0 d  scalar  tensor of type float  optional   the global norm to use  if not provide  global norm   be use to compute the norm  ) ( name : a name for the operation  optional   )
( t : a tensor or indexedslices   this must be a float point type  ) ( clip norm : a 0 d  scalar  tensor > 0  a maximum clip value  also float point ) ( axes : a 1 d  vector  tensor of type int32 contain the dimension to use for compute the l2 norm  if none  the default   use all dimension  ) ( name : a name for the operation  optional   )
( t : a tensor or indexedslices  ) ( clip value min : the minimum value to clip to  a scalar tensor or one that be broadcastable to the shape of t  ) ( clip value max : the maximum value to clip to  a scalar tensor or one that be broadcastable to the shape of t  ) ( name : a name for the operation  optional   )
( values : a list of tensor object or a single tensor  ) ( axis : 0 d int32 tensor   dimension along which to concatenate  must be in the range   rank value   rank value    as in python  index for axis be 0 base  positive axis in the rage of  0  rank value   refer to axis th dimension  and negative axis refer to axis   rank value  th dimension  ) ( name : a name for the operation  optional   )
( pred : a scalar determine whether to return the result of true fn or false fn  ) ( true fn : the callable to be perform if pred be true  ) ( false fn : the callable to be perform if pred be false  ) ( name : optional name prefix for the return tensors  )
( value : a constant value  or list  of output type dtype  ) ( dtype : the type of the elements of the result tensor  ) ( shape : optional dimension of result tensor  ) ( name : optional name for the tensor  )
( value : a python scalar  list or tuple of value  or a n dimensional numpy array  all elements of the initialize variable will be set to the correspond value in the value argument  )
( control inputs : a list of operation or tensor object which must be execute or compute before run the operations define in the context  can also be none to clear the control dependencies  if eager execution be enable  any callable object in the control input list will be call  )
( value : an object whose type have a register tensor conversion function  ) ( dtype : optional element type for the return tensor  if miss  the type be infer from the type of value  ) ( dtype hint : optional element type for the return tensor  use when dtype be none  in some case  a caller may not have a dtype in mind when convert to a tensor  so dtype hint can be use as a soft preference  if the conversion to dtype hint be not possible  this argument have no effect  ) ( name : optional name to use if a new tensor be create  )
( f : function f  x  that return a tuple  y  grad fn  where   x be a sequence of  nest structure of  tensor input to the function  y be a  nest structure of  tensor output of apply tensorflow operations in f to x  grad fn be a function with the signature g  grad ys  which return a list of tensors the same size as  flatten  x   the derivatives of tensors in y with respect to the tensors in x   grad ys be a sequence of tensors the same size as  flatten  y hold the initial value gradients for each tensor in y  in a pure mathematical sense  a vector argument vector value function f's derivatives should be its jacobian matrix j  here we be express the jacobian j as a function grad fn which define how j will transform a vector grad ys when leave multiply with it  grad ys   j  the vector jacobian product  or vjp   this functional representation of a matrix be convenient to use for chain rule calculation  in e g  the back propagation algorithm   if f use variables  that be not part of the input   i e  through get variable  then grad fn should have signature g  grad ys  variables none   where variables be a list of the variables  and return a 2 tuple  grad xs  grad vars   where grad xs be the same as above  and grad vars be a list<tensor> with the derivatives of tensors in y with respect to the variables  that be  grad vars have one tensor per variable in variables   )
( device name : the device name to use in the context  )
( data : a tensor  ) ( partitions : a tensor of type int32  any shape   indices in the range  0  num partition   ) ( num partitions : an int that be >  1  the number of partition to output  ) ( name : a name for the operation  optional   )
( indices : a list of at least 1 tensor object with type int32  ) ( data : a list with the same length as indices of tensor object with the same type  ) ( name : a name for the operation  optional   )
( hypothesis : a sparsetensor contain hypothesis sequence  ) ( truth : a sparsetensor contain truth sequence  ) ( normalize : a bool  if true  normalize the levenshtein distance by length of truth  ) ( name : a name for the operation  optional   )
( equation : a str describe the contraction  in the same format as numpy einsum  ) (  inputs : the input to contract  each one a tensor   whose shape should be consistent with equation  ) (   kwargs : optimize  optimization strategy to use to find contraction path use opt einsum  must be 'greedy'  'optimal'  'branch 2'  'branch all' or 'auto'   optional  default  'greedy'   name  a name for the operation  optional   )
( x : a tensor  ) ( shape : a tensorshape represent the shape of this tensor  a tensorshapeproto  a list  a tuple  or none  ) ( name : a name for this operation  optional   default to "ensureshape"  )

( input : a tensor  ) ( axis : integer specify the dimension index at which to expand the shape of input  give an input of d dimension  axis must be in range    d 1   d   inclusive   ) ( name : optional string  the name of the output tensor  )
( input : a tensor  must be one of the follow type  float32  float64  int32  uint8  int16  int8  int64  bfloat16  uint16  half  uint32  uint64  5 d tensor with shape  batch  in plan  in row  in cols  depth   ) ( ksizes : a list of ints that have length >  5  the size of the slide window for each dimension of input  ) ( strides : a list of ints that have length >  5  1 d of length 5  how far the center of two consecutive patch be in input  must be   1  stride plan  stride row  stride cols  1   ) ( padding : a string from  "same"  "valid"  the type of pad algorithm to use  the size relate attribute be specify as follow  ksizes    1  ksize plan  ksize row  ksize cols  1 stride    1  stride plan  stride row  stride cols  1  ) ( name : a name for the operation  optional   )
( num rows : non negative int32 scalar tensor give the number of row in each batch matrix  ) ( num columns : optional non negative int32 scalar tensor give the number of columns in each batch matrix   default to num row  ) ( batch shape : a list or tuple of python integers or a 1 d int32 tensor  if provide  the return tensor will have lead batch dimension of this shape  ) ( dtype : the type of an element in the result tensor ) ( name : a name for this op   default to "eye"  )
( dims : a 1 d sequence of non negative number  represent the shape of the output tf tensor  entries should be of type  int32  int64  ) ( value : a value to fill the return tf tensor  ) ( name : optional string  the name of the output tf tensor  )
( data : a tensor  must have rank 1 or higher  ) ( method : a tensor of type tf string  fingerprint method use by this op  currently available method be farmhash64  ) ( name : a name for the operation  optional   )
( fn : the callable to be perform  ) ( elems : a tensor or  possibly nest  sequence of tensors  each of which will be unpack along their first dimension   the nest sequence of the result slice will be the first argument to fn  ) ( initializer :  optional  a tensor or  possibly nest  sequence of tensors  as the initial value for the accumulator  ) ( parallel iterations :  optional  the number of iterations allow to run in parallel  ) ( back prop :  optional  deprecate  false disable support for back propagation  prefer use tf stop gradient instead  ) ( swap memory :  optional  true enable gpu cpu memory swap  ) ( name :  optional  name prefix for the return tensors  )
( fn : the callable to be perform  ) ( elems : a tensor or  possibly nest  sequence of tensors  each of which will be unpack along their first dimension   the nest sequence of the result slice will be the first argument to fn  ) ( initializer :  optional  a tensor or  possibly nest  sequence of tensors  as the initial value for the accumulator  ) ( parallel iterations :  optional  the number of iterations allow to run in parallel  ) ( back prop :  optional  deprecate  false disable support for back propagation  prefer use tf stop gradient instead  ) ( swap memory :  optional  true enable gpu cpu memory swap  ) ( name :  optional  name prefix for the return tensors  )
( func : the function to be compile  if func be none  tf function return a decorator that can be invoke with a single argument   func  in other word  tf function input signature      func  be equivalent to tf function func  input signature       the former can be use as decorator  ) ( input signature : a possibly nest sequence of tf tensorspec object specify the shape and dtypes of the tensors that will be supply to this function  if none  a separate function be instantiate for each infer input signature   if input signature be specify  every input to func must be a tensor  and func cannot accept   kwargs  ) ( autograph : whether autograph should be apply on func before trace a graph  data dependent python control flow statements require autograph true  for more information  see the tf function and autograph guide  ) ( jit compile : if true  compile the function use xla  xla perform compiler optimizations  such as fusion  and attempt to emit more efficient code  this may drastically improve the performance  if set to true  the whole function need to be compilable by xla  or an errors invalidargumenterror be throw  if none  default   compile the function with xla when run on tpu and go through the regular function execution path when run on other devices  if false  execute the function without xla compilation   set this value to false when directly run a multi device function on tpus  e g  two tpu core  one tpu core and its host cpu   not all function be compilable  see a list of sharp corner  ) ( reduce retracing : when true  tf function attempt to reduce the amount of retrace  for example by use more generic shape  this can be control for user object by customize their associate tf type experimental tracetype  ) ( experimental implements : if provide  contain a name of a "known" function this implement  for example "mycompany my recurrent cell"  this be store as an attribute in inference function  which can then be detect when process serialize function  see standardize composite ops for detail   for an example of utilize this attribute see this example the code above automatically detect and substitute function that implement "embedded matmul" and allow tflite to substitute its own implementations  for instance  a tensorflow user can use this  attribute to mark that their function also implement embed matmul  perhaps more efficiently   by specify it use this parameter   tf function experimental implement "embedded matmul"  this can either be specify as just the string name of the function or a nameattrlist correspond to a list of key value attribute associate with the function name  the name of the function will be in the 'name' field of the nameattrlist  to define a formal tf op for this function implement  try the experimental composite tf project  ) ( experimental autograph options : optional tuple of tf autograph experimental feature value  ) ( experimental relax shapes : deprecate  use reduce retrace instead  ) ( experimental compile : deprecate alias to 'jit compile'  ) ( experimental follow type hints : when true  the function may use type annotations from func to optimize the trace performance  for example  arguments annotate with tf tensor will automatically be convert to a tensor  )
( params : the tensor from which to gather value  must be at least rank axis   1  ) ( indices : the index tensor   must be one of the follow type  int32  int64  the value must be in range  0  params shape axis    ) ( validate indices : deprecate  do nothing  indices be always validate on cpu  never validate on gpu  caution  on cpu  if an out of bind index be find  an error be raise  on gpu  if an out of bind index be find  a 0 be store in the correspond output value  ) ( axis : a tensor  must be one of the follow type  int32  int64  the axis in params to gather indices from  must be greater than or equal to batch dim   default to the first non batch dimension  support negative index  ) ( batch dims : an integer   the number of batch dimension   must be less than or equal to rank indices   ) ( name : a name for the operation  optional   )
( params : a tensor  the tensor from which to gather value  ) ( indices : a tensor  must be one of the follow type  int32  int64  index tensor  ) ( name : a name for the operation  optional   ) ( batch dims : an integer or a scalar 'tensor'  the number of batch dimension  )

( tensor : the tensor to be evaluate  ) ( partial : if true  the return numpy array be allow to have partially evaluate value  value that can't be evaluate will be none  )
( f : function f  x  that return a tensor or nest structure of tensor output  )
( ys : a tensor or list of tensors to be differentiate  ) ( xs : a tensor or list of tensors to be use for differentiation  ) ( grad ys : optional  a tensor or list of tensors the same size as ys and hold the gradients compute for each y in ys  ) ( name : optional name to use for group all the gradient ops together  default to 'gradients'  ) ( gate gradients : if true  add a tuple around the gradients return for an operations   this avoid some race condition  ) ( aggregation method : specify the method use to combine gradient term  accept value be constants define in the class aggregationmethod  ) ( stop gradients : optional  a tensor or list of tensors not to differentiate through  ) ( unconnected gradients : optional  specify the gradient value return when the give input tensors be unconnected  accept value be constants define in the class tf unconnectedgradients and the default value be none  )
(  inputs : zero or more tensors to group  ) ( name : a name for this operation  optional   )
( input : a tensor  ) ( name : a name for this operation  )
( ys : a tensor or list of tensors to be differentiate  ) ( xs : a tensor or list of tensors to be use for differentiation  ) ( gate gradients : see gradients   documentation for detail  ) ( aggregation method : see gradients   documentation for detail  ) ( name : optional name to use for group all the gradient ops together  default to 'hessians'  )
( values : numeric tensor  ) ( value range : shape  2  tensor of same dtype as value  value <  value range 0  will be map to hist 0   value >  value range 1  will be map to hist  1   ) ( nbins : scalar int32 tensor   number of histogram bin  ) ( dtype : dtype for return histogram  ) ( name : a name for this operation  default to 'histogram fix width'   )
( values : numeric tensor  ) ( value range : shape  2  tensor of same dtype as value  value <  value range 0  will be map to hist 0   value >  value range 1  will be map to hist  1   ) ( nbins : scalar int32 tensor   number of histogram bin  ) ( dtype : dtype for return histogram  ) ( name : a name for this operation  default to 'histogram fix width'   )
( input : a tensor  a variable  a compositetensor or anything that can be convert to a tensor use tf convert to tensor  ) ( name : a name for the operation  optional   )
( input : a list of tensor object  ) ( name : a name for the operation  optional   )

( x : a python object to check  )
( start : a tensor  must be one of the follow type  bfloat16  float32  float64  n d tensor  first entry in the range  ) ( stop : a tensor  must have the same type and shape as start  n d tensor  last entry in the range  ) ( num : a tensor  must be one of the follow type  int32  int64  0 d tensor  number of value to generate  ) ( name : a name for the operation  optional   ) ( axis : axis along which the operation be perform  use only when n d tensors be provide   )
( library location : path to the plugin or the folder of plugins  relative or absolute filesystem path to a dynamic library file or folder  )
( library filename : path to the plugin  relative or absolute filesystem path to a dynamic library file  )
( tensor : a tensorproto  )
( values : value to put in the tensorproto  ) ( dtype : optional tensor pb2 datatype value  ) ( shape : list of integers represent the dimension of tensor  ) ( verify shape : boolean that enable verification of a shape of value  ) ( allow broadcast : boolean that enable allow scalars and 1 length vector broadcast  cannot be true when verify shape be true  )
( fn : the callable to be perform   it accept one argument  which will have the same  possibly nest  structure as elems   its output must have the same structure as fn output signature if one be provide  otherwise it must have the same structure as elems  ) ( elems : a tensor or  possibly nest  sequence of tensors  each of which will be unstacked along their first dimension   fn will be apply to the nest sequence of the result slice   elems may include rag and sparse tensors  elems must consist of at least one tensor  ) ( dtype : deprecate  equivalent to fn output signature  ) ( parallel iterations :  optional  the number of iterations allow to run in parallel  when graph build  the default value be 10  while execute eagerly  the default value be set to 1  ) ( back prop :  optional  deprecate  prefer use tf stop gradient instead   false disable support for back propagation  ) ( swap memory :  optional  true enable gpu cpu memory swap  ) ( infer shape :  optional  false disable test for consistent output shape  ) ( name :  optional  name prefix for the return tensors  ) ( fn output signature : the output signature of fn  must be specify if fn's input and output signatures be different  i e   if their structure  dtypes  or tensor type do not match   fn output signature can be specify use any of the follow   a tf dtype or tf tensorspec  to describe a tf tensor  a tf raggedtensorspec  to describe a tf raggedtensor  a tf sparsetensorspec  to describe a tf sparse sparsetensor  a  possibly nest  tuple  list  or dict contain the above type  )
(  args : tensors with rank 1  ) (   kwargs : index  either 'xy' or 'ij'  optional  default  'xy'   name  a name for the operation  optional   )
( name : the prefix to use on all name create within the name scope  )
( op type : the string type of an operation  this correspond to the opdef name field for the proto that define the operation  )
( name : a name for the operation  optional   )
( num batch threads : number of schedule thread for process batch of work  determine the number of batch process in parallel  ) ( max batch size : batch size will never be bigger than this  ) ( batch timeout micros : maximum number of microseconds to wait before output an incomplete batch  ) ( allowed batch sizes : optional list of allow batch size  if leave empty  do nothing  otherwise  supply a list of batch size  cause the op to pad batch up to one of those size  the entries must increase monotonically  and the final entry must equal max batch size  ) ( max enqueued batches : the maximum depth of the batch queue  default to 10  ) ( autograph : whether to use autograph to compile python and eager style code for efficient graph mode execution  ) ( enable large batch splitting : the value of this option doesn't affect process output give the same input  it affect implementation detail as state below  1  improve batch efficiency by eliminate unnecessary add  2 max batch size specify the limit of input and allow batch size specify the limit of a task to be process  api user can give an input of size 128 when 'max execution batch size' be 32  > implementation can split input of 128 into 4 x 32  schedule concurrent process  and then return concatenate result correspond to 128  )
( tensor : tensor of type float32  float64  complex64  complex128 ) ( ord : order of the norm  support value be 'fro'  'euclidean'  1  2  np inf and any positive real number yield the correspond p norm  default be 'euclidean' which be equivalent to frobenius norm if tensor be a matrix and equivalent to 2 norm for vectors  some restrictions apply    a  the frobenius norm 'fro' be not define for vectors    b  if axis be a 2 tuple  matrix norm   only 'euclidean'  'fro'  1       2  np inf be support  see the description of axis on how to compute norms for a batch of vectors or matrices store in a tensor  ) ( axis : if axis be none  the default   the input be consider a vector and a single vector norm be compute over the entire set of value in the tensor  i e  norm tensor  ord ord  be equivalent to norm reshape tensor    1    ord ord   if axis be a python integer  the input be consider a batch of vectors  and axis determine the axis in tensor over which to compute vector norms  if axis be a 2 tuple of python integers it be consider a batch of matrices and axis determine the ax in tensor over which to compute a matrix norm  negative indices be support  example  if you be pass a tensor that can be either a matrix or a batch of matrices at runtime  pass axis   2  1  instead of axis none to make sure that matrix norms be compute  ) ( keepdims : if true  the axis indicate in axis be keep with size 1  otherwise  the dimension in axis be remove from the output shape  ) ( name : the name of the op  )
( func : a python function  which accept numpy ndarray object as arguments and return a list of numpy ndarray object  or a single numpy ndarray   this function must accept as many arguments as there be tensors in inp  and these argument type will match the correspond tf tensor object in inp  the return numpy ndarrays must match the number and type define tout  important note  input and output numpy ndarrays of func be not   guarantee to be copy  in some case their underlie memory will be   share with the correspond tensorflow tensors  in place modification   or store func input or return value in python datastructures   without explicit  np  copy can have non deterministic consequences  ) ( inp : a list of tf tensor object  ) ( Tout : a list or tuple of tensorflow data type or a single tensorflow data type if there be only one  indicate what func return  ) ( stateful :  boolean   set this argument to false tell the runtime to treat the function as stateless  which enable certain optimizations  a function be stateless when give the same input it will return the same output and have no side effect  its only purpose be to have a return value  the behavior for a stateful function with the stateful argument false be undefined  in particular  caution should be take when mutate the input arguments as this be a stateful operation  ) ( name :  optional  a name for the operation  )
( indices : a tensor of indices  ) ( depth : a scalar define the depth of the one hot dimension  ) ( on value : a scalar define the value to fill in output when indices j    i   default  1  ) ( off value : a scalar define the value to fill in output when indices j     i   default  0  ) ( axis : the axis to fill  default   1  a new inner most axis   ) ( dtype : the data type of the output tensor  ) ( name : a name for the operation  optional   )
( shape : a list of integers  a tuple of integers  or a 1 d tensor of type int32  ) ( dtype : optional dtype of an element in the result tensor  default be tf float32  ) ( name : optional string  a name for the operation  )

( input : a tensor  ) ( dtype : a type for the return tensor  must be float16  float32  float64  int8  uint8  int16  uint16  int32  int64  complex64  complex128  bool or string  ) ( name : a name for the operation  optional   )
( tensor : a tensor  ) ( paddings : a tensor of type int32  ) ( mode : one of "constant"  "reflect"  or "symmetric"  case insensitive  ) ( constant values : in "constant" mode  the scalar pad value to use  must be same type as tensor  ) ( name : a name for the operation  optional   )
( values : a list of tensor object with the same shape and type  ) ( name : a name for this operation  optional   )
(  inputs : positional arguments that be the input to print  input in the print output will be separate by space  input may be python primitives  tensors  data structure such as dicts and list that may contain tensors  with the data structure possibly nest in arbitrary ways   and printable python object  ) ( output stream : the output stream  log level  or file to print to  default to sys stderr  but sys stdout  tf compat v1 log info  tf compat v1 log warn  tf compat v1 log error  absl log info  absl log warn and absl log error be also support  to print to a file  pass a string start with "file //" follow by the file path  e g   "file ///tmp/foo out"  ) ( summarize : the first and last summarize elements within each dimension be recursively print per tensor  if none  then the first 3 and last 3 elements of each dimension be print for each tensor  if set to  1  it will print all elements of every tensor  ) ( sep : the string to use to separate the input  default to " "  ) ( end : end character that be append at the end the print string  default to the newline character  ) ( name : a name for the operation  optional   )
( func : a python function that accept inp as arguments  and return a value  or list of value  whose type be describe by tout  ) ( inp : input arguments for func   a list whose elements be tensors or compositetensors  such as tf raggedtensor   or a single tensor or compositetensor  ) ( Tout : the type s  of the value s  return by func   one of the follow   if func return a tensor  or a value that can be convert to a tensor   the tf dtype for that value  if func return a compositetensor  the tf typespec for that value  if func return none  the empty list       if func return a list of tensor and compositetensor value  a correspond list of tf dtypes and tf typespecs for each value  ) ( name : a name for the operation  optional   )
( mean : a python scalar or a scalar tensor  mean of the random value to generate  ) ( stddev : a python scalar or a scalar tensor  standard deviation of the random value to generate  ) ( seed : a python integer  use to create random seed  see tf random set seed for behavior  )
( minval : a python scalar or a scalar tensor  lower bind of the range of random value to generate  inclusive   ) ( maxval : a python scalar or a scalar tensor  upper bind of the range of random value to generate  exclusive   ) ( seed : a python integer  use to create random seed  see tf random set seed for behavior  )
( start : a 0 d tensor  scalar   act as first entry in the range if limit be not none  otherwise  act as range limit and first entry default to 0  ) ( limit : a 0 d tensor  scalar   upper limit of sequence  exclusive  if none  default to the value of start while the first entry of the range default to 0  ) ( delta : a 0 d tensor  scalar   number that increments start  default to 1  ) ( dtype : the type of the elements of the result tensor  ) ( name : a name for the operation  default to "range"  )
( input : a tensor or sparsetensor  ) ( name : a name for the operation  optional   )
( x : a tensor  must be one of the follow type  bfloat16  half  float32  float64  uint8  int8  uint16  int16  int32  uint32  uint64  int64  complex64  complex128  ) ( y : a tensor  must have the same type as x  ) ( name : a name for the operation  optional   )
( f : function f  x  that return a tensor or sequence of tensor output  )
( base type : the base type or tuple of base type for all object that conversion func accept  ) ( conversion func : a function that convert instance of base type to tensor  ) ( priority : optional integer that indicate the priority for apply this conversion function  conversion function with smaller priority value run earlier than conversion function with larger priority value  default to 100  )
( input : an n dimensional tensor  ) ( repeats : an 1 d int tensor  the number of repetitions for each element  repeat be broadcast to fit the shape of the give axis  len repeat  must equal input shape axis  if axis be not none  ) ( axis : an int  the axis along which to repeat value  by default  axis none   use the flatten input array  and return a flat output array  ) ( name : a name for the operation  )
( input shape : int32 tensor of shape  n   ) ( block shape : int32 tensor of shape  n   ) ( base paddings : optional int32 tensor of shape  n  2    specify the minimum amount of pad to use   all elements must be >  0   if not specify  default to 0  ) ( name : string   optional name prefix  )
( tensor : a tensor  ) ( shape : a tensor  must be one of the follow type  int32  int64  define the shape of the output tensor  ) ( name : optional string  a name for the operation  )
( tensor : a tensor  must be one of the follow type  uint8  int8  uint16  int16  int32  uint32  int64  uint64  bool  bfloat16  half  float32  float64  complex64  complex128  string  up to 8 d  ) ( axis : a tensor  must be one of the follow type  int32  int64  1 d  the indices of the dimension to reverse  must be in the range   rank tensor   rank tensor    ) ( name : a name for the operation  optional   )
( input : a tensor  the input to reverse  ) ( seq lengths : a tensor  must be one of the follow type  int32  int64  1 d with length input dim batch axis  and max seq lengths  <  input dim seq axis  ) ( seq axis : an int  the dimension which be partially reverse  ) ( batch axis : an optional int  default to 0  the dimension along which reversal be perform  ) ( name : a name for the operation  optional   )
( input : a tensor  ) ( shift : a tensor  must be one of the follow type  int32  int64  dimension must be 0 d or 1 d  shift i  specify the number of place by which elements be shift positively  towards larger indices  along the dimension specify by axis i   negative shift will roll the elements in the opposite direction  ) ( axis : a tensor  must be one of the follow type  int32  int64  dimension must be 0 d or 1 d  axis i  specify the dimension that the shift shift i  should occur  if the same axis be reference more than once  the total shift for that axis will be the sum of all the shift that belong to that axis  ) ( name : a name for the operation  optional   )
( fn : the callable to be perform   it accept two arguments   the first will have the same structure as initializer if one be provide  otherwise it will have the same structure as elems   the second will have the same  possibly nest  structure as elems   its output must have the same structure as initializer if one be provide  otherwise it must have the same structure as elems  ) ( elems : a tensor or  possibly nest  sequence of tensors  each of which will be unpack along their first dimension   the nest sequence of the result slice will be the first argument to fn  ) ( initializer :  optional  a tensor or  possibly nest  sequence of tensors  initial value for the accumulator  and the expect output type of fn  ) ( parallel iterations :  optional  the number of iterations allow to run in parallel  ) ( back prop :  optional  deprecate  false disable support for back propagation  prefer use tf stop gradient instead  ) ( swap memory :  optional  true enable gpu cpu memory swap  ) ( infer shape :  optional  false disable test for consistent output shape  ) ( reverse :  optional  true scan the tensor last to first  instead of first to last   ) ( name :  optional  name prefix for the return tensors  )
( indices : a tensor  must be one of the follow type  int16  int32  int64  tensor of indices  ) ( updates : a tensor  value to scatter into the output tensor  ) ( shape : a tensor  must have the same type as indices  1 d  the shape of the output tensor  ) ( name : a name for the operation  optional   )
( sorted sequence : n d tensor contain a sort sequence  ) ( values : n d tensor contain the search value  ) ( side : 'left' or 'right'  'left' correspond to lower bind and 'right' to upper bind  ) ( out type : the output type  int32 or int64    default be tf int32  ) ( name : optional name for the operation  )
( lengths : integer tensor  all its value <  maxlen  ) ( maxlen : scalar integer tensor  size of last dimension of return tensor  default be the maximum value in lengths  ) ( dtype : output type of the result tensor  ) ( name : name of the op  )
( input : a tensor or sparsetensor  ) ( out type :  optional  the specify output type of the operation  int32 or int64   default to tf int32  ) ( name : a name for the operation  optional   )
( input : a list of at least 1 tensor object with the same type  ) ( out type : the specify output type of the operation  int32 or int64   default to tf int32 optional   ) ( name : a name for the operation  optional   )
( input : a tensor or sparsetensor  ) ( name : a name for the operation  optional   ) ( out type :  optional  the specify non quantize numeric output type of the operation  default to tf int32  )
( input  : a tensor  ) ( begin : an int32 or int64 tensor  ) ( size : an int32 or int64 tensor  ) ( name : a name for the operation  optional   )
( values : 1 d or higher numeric tensor  ) ( axis : the axis along which to sort  the default be  1  which sort the last axis  ) ( direction : the direction in which to sort the value  'ascending' or 'descending'   ) ( name : optional name for the operation  )
( input : a tensor  n d with shape input shape    batch    spatial shape   remain shape  where spatial shape have m dimension  ) ( block shape : a tensor  must be one of the follow type  int32  int64  1 d with shape  m   all value must be >  1  ) ( paddings : a tensor  must be one of the follow type  int32  int64  2 d with shape  m  2   all value must be >  0    paddings i     pad start  pad end  specify the pad for input dimension   i   1  which correspond to spatial dimension i   it be require that   block shape i  divide input shape i   1    pad start   pad end  ) ( name : a name for the operation  optional   )
( input : a tensor  n d with shape input shape    batch    spatial shape   remain shape  where spatial shape have m dimension  ) ( block shape : a tensor  must be one of the follow type  int32  int64  1 d with shape  m   all value must be >  1  ) ( paddings : a tensor  must be one of the follow type  int32  int64  2 d with shape  m  2   all value must be >  0    paddings i     pad start  pad end  specify the pad for input dimension   i   1  which correspond to spatial dimension i   it be require that   block shape i  divide input shape i   1    pad start   pad end  ) ( name : a name for the operation  optional   )
( value : the tensor to split  ) ( num or size splits : either an int indicate the number of split along axis or a 1 d integer tensor or python list contain the size of each output tensor along axis  if an int  then it must evenly divide value shape axis   otherwise the sum of size along the split axis must match that of the value  ) ( axis : an int or scalar int32 tensor  the dimension along which to split  must be in the range   rank value   rank value    default to 0  ) ( num : optional  an int  use to specify the number of output when it cannot be infer from the shape of size split  ) ( name : a name for the operation  optional   )
( input : a tensor  the input to squeeze  ) ( axis : an optional list of ints  default to     if specify  only squeeze the dimension list  the dimension index start at 0  it be an error to squeeze a dimension that be not 1  must be in the range   rank input   rank input    must be specify if input be a raggedtensor  ) ( name : a name for the operation  optional   )
( values : a list of tensor object with the same shape and type  ) ( axis : an int  the axis to stack along  default to the first dimension  negative value wrap around  so the valid range be    r 1   r 1   ) ( name : a name for this operation  optional   )
( input : a tensor  ) ( name : a name for the operation  optional   )
( input  : a tensor  ) ( begin : an int32 or int64 tensor  ) ( end : an int32 or int64 tensor  ) ( strides : an int32 or int64 tensor  ) ( begin mask : an int32 mask  ) ( end mask : an int32 mask  ) ( ellipsis mask : an int32 mask  ) ( new axis mask : an int32 mask  ) ( shrink axis mask : an int32 mask  ) ( var : the variable correspond to input  or none ) ( name : a name for the operation  optional   )
( branch index : an int tensor specify which of branch fns should be execute  ) ( branch fns : a dict map ints to callables  or a list of  int  callable  pair  or simply a list of callables  in which case the index serve as the key   each callable must return a match structure of tensors  ) ( default : optional callable that return a structure of tensors  ) ( name : a name for this operation  optional   )
( tensor : a tensor  tensor to copy/update  ) ( indices : a tensor  must be one of the follow type  int32  int64  index tensor  ) ( updates : a tensor  must have the same type as tensor  update to scatter into output  ) ( name : a name for the operation  optional   )
( tensor : a tensor  tensor to copy/update  ) ( indices : a tensor  must be one of the follow type  int32  int64  index tensor  ) ( updates : a tensor  must have the same type as tensor  update to scatter into output  ) ( name : a name for the operation  optional   )
( tensor : tensor to copy/update  ) ( indices : indices to update  ) ( updates : update to apply at the indices  ) ( name : optional name for the operation  )
( a : tensor of type float32 or float64  ) ( b : tensor with the same type as a  ) ( axes : either a scalar n  or a list or an int32 tensor of shape  2  k   if ax be a scalar  sum over the last n ax of a and the first n ax of b in order  if ax be a list or tensor the first and second row contain the set of unique integers specify ax along which the contraction be compute  for a and b  respectively  the number of ax for a and b must be equal  if ax 0  compute the outer product between a and b  ) ( name : a name for the operation  optional   )
( input : a tensor  1 d or higher  ) ( multiples : a tensor  must be one of the follow type  int32  int64  1 d  length must be the same as the number of dimension in input ) ( name : a name for the operation  optional   )
( name : a name for the operation  optional   )
( a : a tensor  ) ( perm : a permutation of the dimension of a   this should be a vector  ) ( conjugate : optional bool  set it to true be mathematically equivalent to tf math conj tf transpose input    ) ( name : a name for the operation  optional   )
( x : a tensor  must be one of the follow type  bfloat16  half  float32  float64  uint8  int8  uint16  int16  int32  uint32  uint64  int64  complex64  complex128  ) ( y : a tensor  must have the same type as x  ) ( name : a name for the operation  optional   )
( x : a tensor  must be one of the follow type  int32  int64  bfloat16  half  float32  float64  ) ( y : a tensor  must have the same type as x  ) ( name : a name for the operation  optional   )
( tensors : a list of tensors or indexedslices  some entries can be none  ) ( control inputs : list of additional ops to finish before return  ) ( name :  optional  a name to use as a name scope for the operation  )
( x : a tensor  1 d  ) ( out idx : an optional tf dtype from  tf int32  tf int64  default to tf int32  ) ( name : a name for the operation  optional   )
( x : a tensor  1 d  ) ( out idx : an optional tf dtype from  tf int32  tf int64  default to tf int32  ) ( name : a name for the operation  optional   )
( indices : a tensor  must be one of the follow type  int32  int64  an 0 d or 1 d int tensor whose elements be indices into the flatten version of an array of dimension dim  ) ( dims : a tensor  must have the same type as indices  an 1 d int tensor  the shape of the array to use for unravel indices  ) ( name : a name for the operation  optional   )
( value : a rank r > 0 tensor to be unstacked  ) ( num : an int  the length of the dimension axis  automatically infer if none  the default   ) ( axis : an int  the axis to unstack along  default to the first dimension  negative value wrap around  so the valid range be   r  r   ) ( name : a name for the operation  optional   )
( variable creator : the pass creator )
( fn : the callable to be perform  it accept one argument  which will have the same  possibly nest  structure as elems  and return a possibly nest structure of tensors and operations  which may be different than the structure of elems  ) ( elems : a tensor or  possibly nest  sequence of tensors  each of which will be unpack along their first dimension  the nest sequence of the result slice will be map over by fn  the first dimension of all elements must broadcast to a consistent value  equivalently  each element tensor must have first dimension of either b or 1  for some common batch size b >  1  ) ( fallback to while loop : if true  on fail to vectorize an operation  the unsupported op be wrap in a tf while loop to execute the map iterations  note that this fallback only happen for unsupported ops and other part of fn be still vectorized  if false  on encounter an unsupported op  a valueerror be throw  note that the fallbacks can result in slowdowns since vectorization often yield speedup of one to two order of magnitude  )
( condition : a tf tensor of dtype bool  or any numeric dtype  condition must have dtype bool when x and y be provide  ) ( x : if provide  a tensor which be of the same type as y  and have a shape broadcastable with condition and y  ) ( y : if provide  a tensor which be of the same type as x  and have a shape broadcastable with condition and x  ) ( name : a name of the operation  optional   )
( cond : a callable that represent the termination condition of the loop  ) ( body : a callable that represent the loop body  ) ( loop vars : a  possibly nest  tuple  namedtuple or list of numpy array  tensor  and tensorarray object  ) ( shape invariants : the shape invariants for the loop variables  ) ( parallel iterations : the number of iterations allow to run in parallel  it must be a positive integer  ) ( back prop :  optional  deprecate  false disable support for back propagation  prefer use tf stop gradient instead  ) ( swap memory : whether gpu cpu memory swap be enable for this loop  ) ( maximum iterations : optional maximum number of iterations of the while loop to run   if provide  the cond output be and ed with an additional condition ensure the number of iterations execute be no greater than maximum iterations  ) ( name : optional name prefix for the return tensors  )
( shape : a list of integers  a tuple of integers  or a 1 d tensor of type int32  ) ( dtype : the dtype of an element in the result tensor  ) ( name : optional string  a name for the operation  )

( input : a tensor or array like object  ) ( dtype : a type for the return tensor  must be float16  float32  float64  int8  uint8  int16  uint16  int32  int64  complex64  complex128  bool or string  optional   ) ( name : a name for the operation  optional   )
( bytes or text : a bytearray  bytes  str  or unicode object  ) ( encoding : a string indicate the charset for encode unicode  )
( value : a object that can be convert to str  )
( bytes or text : a bytes  str  or unicode object  ) ( encoding : a string indicate the charset for decode unicode  )
( shape : a tensorshape instance  ) ( index : an integer index  )
( dimension : either a dimension instance  an integer  or none  )
( year : a year  e g   2018   must be an int  ) ( month : a month  1 <  month <  12  in year  must be an int  ) ( day : a day  1 <  day <  31  or 30  or 29  or 28  in month  must be an int  )
( year : a year  e g   2018   must be an int  ) ( month : a month  1 <  month <  12  in year  must be an int  ) ( day : a day  1 <  day <  31  or 30  or 29  or 28  in month  must be an int  )
( path : an object that can be convert to path representation  )
( dtype : datatype of the accumulate gradients  ) ( shape : shape of the accumulate gradients  ) ( shared name : optional  if non empty  this accumulator will be share under the give name across multiple sessions  ) ( name : optional name for the accumulator  ) ( reduction type : reduction type to use when take the gradient  )
( dtype : datatype of the accumulate gradients  ) ( shape : shape of the accumulate gradients  ) ( accumulator ref : a handle to the conditional accumulator  create by sub  class )
( job : string   optional job name  ) ( replica : int   optional replica index  ) ( task : int   optional task index  ) ( device type : optional device type string  e g  "cpu" or "gpu"  ) ( device index : int   optional device index   if leave unspecified  device represent 'any' device index  )
( value : the value of this dimension  or none if it be unknown  )
( record bytes : an int  ) ( header bytes : an optional int  default to 0  ) ( footer bytes : an optional int  default to 0  ) ( hop bytes : an optional int  default to 0  ) ( name : a name for the operation  optional   ) ( encoding : the type of encode for the file  default to none  )

( name : a name for the operation  optional   )
( target :  optional   the execution engine to connect to  default to use an in process engine  ) ( graph :  optional   the graph to be launch  describe above   ) ( config :  optional  configproto proto use to configure the session  )
( name : a name for the operation  optional   ) ( options : a lmdbrecordoptions object  optional   )
( input  : a tensor pass through this op  ) ( data : a list of tensors to print out when op be evaluate  ) ( message : a string  prefix of the error message  ) ( first n : only log first n number of time  negative number log always  this be the default  ) ( summarize : only print this many entries of each tensor  if none  then a maximum of 3 elements be print per input tensor  ) ( name : a name for the operation  optional   )
( reader ref : the operation that implement the reader  ) ( supports serialize : true if the reader implementation can serialize its state  )
( target :  optional   the execution engine to connect to  default to use an in process engine  see distribute tensorflow for   more examples  ) ( graph :  optional   the graph to be launch  describe above   ) ( config :  optional   a configproto   protocol buffer with configuration options for the session  )
( dtype : datatype of the accumulate gradients  ) ( shape : shape of the accumulate gradients  ) ( shared name : optional  if non empty  this accumulator will be share under the give name across multiple sessions  ) ( name : optional name for the accumulator  ) ( reduction type : reduction type to use when take the gradient  )
( name : a name for the operation  optional   ) ( options : a tfrecordoptions object  optional   )
( skip header lines : an optional int  default to 0   number of line to skip from the begin of every file  ) ( name : a name for the operation  optional   )
( initial value : a tensor  or python object convertible to a tensor  which be the initial value for the variable  the initial value must have a shape specify unless validate shape be set to false  can also be a callable with no argument that return the initial value when call  in that case  dtype must be specify   note that initializer function from init ops py must first be bind to a shape before be use here   ) ( trainable : if true  also add the variable to the graph collection graphkeys trainable variables  this collection be use as the default list of variables to use by the optimizer class  default to true  unless synchronization be set to on read  in which case it default to false  ) ( collections : list of graph collections key  the new variable be add to these collections  default to  graphkeys global variables   ) ( validate shape : if false  allow the variable to be initialize with a value of unknown shape  if true  the default  the shape of initial value must be know  ) ( caching device : optional device string describe where the variable should be cache for read   default to the variable's device  if not none  cache on another device   typical use be to cache on the device where the ops use the variable reside  to deduplicate copy through switch and other conditional statements  ) ( name : optional name for the variable  default to 'variable' and get uniquified automatically  ) ( variable def : variabledef protocol buffer  if not none  recreate the variable object with its content  reference the variable's nod in the graph  which must already exist  the graph be not change  variable def and the other arguments be mutually exclusive  ) ( dtype : if set  initial value will be convert to the give type  if none  either the datatype will be keep  if initial value be a tensor   or convert to tensor will decide  ) ( expected shape : a tensorshape  if set  initial value be expect to have this shape  ) ( import scope : optional string  name scope to add to the variable  only use when initialize from protocol buffer  ) ( constraint : an optional projection function to be apply to the variable after be update by an optimizer  e g  use to implement norm constraints or value constraints for layer weight   the function must take as input the unprojected tensor represent the value of the variable and return the tensor for the project value  which must have the same shape   constraints be not safe to use when do asynchronous distribute train  ) ( use resource : whether to use resource variables  ) ( synchronization : indicate when a distribute a variable will be aggregate  accept value be constants define in the class tf variablesynchronization  by default the synchronization be set to auto and the current distributionstrategy choose when to synchronize  ) ( aggregation : indicate how a distribute variable will be aggregate  accept value be constants define in the class tf variableaggregation  ) ( shape :  optional  the shape of this variable  if none  the shape of initial value will be use  when set this argument to tf tensorshape none   represent an unspecified shape   the variable can be assign with value of different shape  )

( name : name of the current scope  use as prefix in get variable  ) ( initializer : default initializer pass to get variable  ) ( regularizer : default regularizer pass to get variable  ) ( reuse : boolean  none  or tf compat v1 auto reuse  set the reuse in get variable  when eager execution be enable this argument be always force to be false  ) ( caching device : string  callable  or none  the cache device pass to get variable  ) ( partitioner : callable or none  the partitioner pass to get variable  ) ( custom getter : default custom getter pass to get variable  ) ( name scope : the name pass to tf name scope  ) ( dtype : default type pass to get variable  default to dt float   ) ( use resource : if false  create a normal variable  if true create an experimental resourcevariable with well define semantics  default to false  will later change to true   when eager execution be enable this argument be always force to be true  ) ( constraint : an optional projection function to be apply to the variable after be update by an optimizer  e g  use to implement norm constraints or value constraints for layer weight   the function must take as input the unprojected tensor represent the value of the variable and return the tensor for the project value  which must have the same shape   constraints be not safe to use when do asynchronous distribute train  ) ( original name scope :  )
( name : a name for the operation  optional   )

( name : the key for the collection  for example  the graphkeys class contain many standard name for collections  ) ( value : the value to add to the collection  )
( names : the key for the collections  the graphkeys class contain many standard name for collections  ) ( value : the value to add to the collections  )

( input : a tensor  must be one of the follow type  float32  float64  int32  uint8  int16  int8  complex64  int64  qint8  quint8  qint32  bfloat16  uint16  complex128  half  uint32  uint64  bool  ) ( dimension : a tensor  must be one of the follow type  int16  int32  int64  int16  int32 or int64  must be in the range   rank input   rank input    describe which dimension of the input tensor to reduce across  for vectors  use dimension   0  ) ( output type : an optional tf dtype from  tf int16  tf uint16  tf int32  tf int64  default to tf int64  ) ( name : a name for the operation  optional   )
( input : a tensor  must be one of the follow type  float32  float64  int32  uint8  int16  int8  complex64  int64  qint8  quint8  qint32  bfloat16  uint16  complex128  half  uint32  uint64  bool  ) ( dimension : a tensor  must be one of the follow type  int32  int64  int32 or int64  must be in the range   rank input   rank input    describe which dimension of the input tensor to reduce across  for vectors  use dimension   0  ) ( output type : an optional tf dtype from  tf int32  tf int64  default to tf int64  ) ( name : a name for the operation  optional   )
( input : a tensor  must be one of the follow type  float32  float64  int32  uint8  int16  int8  complex64  int64  qint8  quint8  qint32  bfloat16  uint16  complex128  half  uint32  uint64  bool  ) ( axis : a tensor  must be one of the follow type  int16  int32  int64  int16  int32 or int64  must be in the range   rank input   rank input    describe which axis of the input tensor to reduce across  for vectors  use axis   0  ) ( output type : an optional tf dtype from  tf int16  tf uint16  tf int32  tf int64  default to tf int64  ) ( name : a name for the operation  optional   )
( input : a tensor  must be one of the follow type  float32  float64  int32  uint8  int16  int8  complex64  int64  qint8  quint8  qint32  bfloat16  uint16  complex128  half  uint32  uint64  bool  ) ( axis : a tensor  must be one of the follow type  int32  int64  int32 or int64  must be in the range   rank input   rank input    describe which axis of the input tensor to reduce across  for vectors  use axis   0  ) ( output type : an optional tf dtype from  tf int32  tf int64  default to tf int64  ) ( name : a name for the operation  optional   )
( x : numeric tensor  ) ( y : numeric tensor  same dtype as and broadcastable to x  ) ( data : the tensors to print out if the condition be false   default to error message and first few entries of x  y  ) ( summarize : print this many entries of each tensor  ) ( message : a string to prefix to the default message  ) ( name : a name for this operation  optional    default to "assert equal"  )
( x : numeric tensor  ) ( y : numeric tensor  same dtype as and broadcastable to x  ) ( data : the tensors to print out if the condition be false   default to error message and first few entries of x  y  ) ( summarize : print this many entries of each tensor  ) ( message : a string to prefix to the default message  ) ( name : a name for this operation  optional    default to "assert greater"  )
( x : numeric tensor  ) ( y : numeric tensor  same dtype as and broadcastable to x  ) ( data : the tensors to print out if the condition be false   default to error message and first few entries of x  y  ) ( summarize : print this many entries of each tensor  ) ( message : a string to prefix to the default message  ) ( name : a name for this operation  optional    default to "assert greater equal"  )
( x : tensor whose basetype be integer and be not quantize  ) ( message : a string to prefix to the default message  ) ( name : a name for this operation  optional    default to "assert integer"  )
( x : numeric tensor  ) ( y : numeric tensor  same dtype as and broadcastable to x  ) ( data : the tensors to print out if the condition be false   default to error message and first few entries of x  y  ) ( summarize : print this many entries of each tensor  ) ( message : a string to prefix to the default message  ) ( name : a name for this operation  optional    default to "assert less"  )
( x : numeric tensor  ) ( y : numeric tensor  same dtype as and broadcastable to x  ) ( data : the tensors to print out if the condition be false   default to error message and first few entries of x  y  ) ( summarize : print this many entries of each tensor  ) ( message : a string to prefix to the default message  ) ( name : a name for this operation  optional    default to "assert less equal"  )
( x : float or complex tensor  ) ( y : float or complex tensor  same dtype as  and broadcastable to  x  ) ( rtol : tensor   same dtype as  and broadcastable to  x  the relative tolerance   default be 10   eps  ) ( atol : tensor   same dtype as  and broadcastable to  x  the absolute tolerance   default be 10   eps  ) ( data : the tensors to print out if the condition be false   default to error message and first few entries of x  y  ) ( summarize : print this many entries of each tensor  ) ( message : a string to prefix to the default message  ) ( name : a name for this operation  optional    default to "assert near"  )
( x : numeric tensor  ) ( data : the tensors to print out if the condition be false   default to error message and first few entries of x  ) ( summarize : print this many entries of each tensor  ) ( message : a string to prefix to the default message  ) ( name : a name for this operation  optional    default to "assert negative"  )
( x : numeric tensor  ) ( data : the tensors to print out if the condition be false   default to error message and first few entries of x  ) ( summarize : print this many entries of each tensor  ) ( message : a string to prefix to the default message  ) ( name : a name for this operation  optional    default to "assert non negative"  )
( x : numeric tensor  ) ( data : the tensors to print out if the condition be false   default to error message and first few entries of x  ) ( summarize : print this many entries of each tensor  ) ( message : a string to prefix to the default message  ) ( name : a name for this operation  optional    default to "assert non positive"  )
( x : numeric tensor  ) ( y : numeric tensor  same dtype as and broadcastable to x  ) ( data : the tensors to print out if the condition be false   default to error message and first few entries of x  y  ) ( summarize : print this many entries of each tensor  ) ( message : a string to prefix to the default message  ) ( name : a name for this operation  optional    default to "assert none equal"  )
( x : numeric tensor  ) ( data : the tensors to print out if the condition be false   default to error message and first few entries of x  ) ( summarize : print this many entries of each tensor  ) ( message : a string to prefix to the default message  ) ( name : a name for this operation  optional    default to "assert positive"  )
( x : numeric tensor  ) ( rank : scalar integer tensor  ) ( data : the tensors to print out if the condition be false   default to error message and the shape of x  ) ( summarize : print this many entries of each tensor  ) ( message : a string to prefix to the default message  ) ( name : a name for this operation  optional    default to "assert rank"  )
( x : numeric tensor  ) ( rank : scalar tensor  ) ( data : the tensors to print out if the condition be false   default to error message and first few entries of x  ) ( summarize : print this many entries of each tensor  ) ( message : a string to prefix to the default message  ) ( name : a name for this operation  optional   default to "assert rank at least"  )
( x : numeric tensor  ) ( ranks : iterable of scalar tensor object  ) ( data : the tensors to print out if the condition be false   default to error message and first few entries of x  ) ( summarize : print this many entries of each tensor  ) ( message : a string to prefix to the default message  ) ( name : a name for this operation  optional   default to "assert rank in"  )
( tensor : a tensor  ) ( name : a name for this operation  default to "assert scalar" ) ( message : a string to prefix to the default message  )
( tensor : a tensor or sparsetensor  ) ( tf type : a tensorflow type  dtypes float32  tf int64  dtypes bool  etc   ) ( message : a string to prefix to the default message  ) ( name : a name to give this op   default to "assert type" )
( var list : list of variable object to check  default to the value of global variables    )
( ref : a mutable tensor  should be from a variable node  may be uninitialized  ) ( value : a tensor  must have the same shape and dtype as ref  the value to be assign to the variable  ) ( validate shape : an optional bool  default to true  if true  the operation will validate that the shape of 'value' match the shape of the tensor be assign to   if false  'ref' will take on the shape of 'value'  ) ( use locking : an optional bool  default to true  if true  the assignment will be protect by a lock  otherwise the behavior be undefined  but may exhibit less contention  ) ( name : a name for the operation  optional   )
( ref : a mutable tensor  must be one of the follow type  float32  float64  int64  int32  uint8  uint16  int16  int8  complex64  complex128  qint8  quint8  qint32  half  should be from a variable node  ) ( value : a tensor  must have the same shape and dtype as ref  the value to be add to the variable  ) ( use locking : an optional bool  default to false  if true  the addition will be protect by a lock  otherwise the behavior be undefined  but may exhibit less contention  ) ( name : a name for the operation  optional   )
( ref : a mutable tensor  must be one of the follow type  float32  float64  int64  int32  uint8  uint16  int16  int8  complex64  complex128  qint8  quint8  qint32  half  should be from a variable node  ) ( value : a tensor  must have the same shape and dtype as ref  the value to be subtract to the variable  ) ( use locking : an optional bool  default to false  if true  the subtraction will be protect by a lock  otherwise the behavior be undefined  but may exhibit less contention  ) ( name : a name for the operation  optional   )

( ref : variable to scatter onto  ) ( indices : tensor contain indices as describe above  ) ( updates : tensor of update to apply to ref  ) ( use locking : boolean indicate whether to lock the write operation  ) ( name : optional scope name string  )
( input : a tensor  4 d tensor with shape  batch block size block size  height pad/block size  width pad/block size    depth   note that the batch size of the input tensor must be divisible by block size   block size  ) ( crops : a tensor  must be one of the follow type  int32  int64  2 d tensor of non negative integers with shape  2  2   it specify how many elements to crop from the intermediate result across the spatial dimension as follow  crop     crop top  crop bottom    crop leave  crop right   ) ( block size : an int that be >  2  ) ( name : a name for the operation  optional   )
( input : a tensor  n d with shape input shape    batch    spatial shape   remain shape  where spatial shape have m dimension  ) ( block shape : a tensor  must be one of the follow type  int32  int64  1 d with shape  m   all value must be >  1  ) ( crops : a tensor  must be one of the follow type  int32  int64  2 d with shape  m  2   all value must be >  0    crop i     crop start  crop end  specify the amount to crop from input   dimension i   1  which correspond to spatial dimension i   it be   require that   crop start i    crop end i  <  block shape i    input shape i   1   this operation be equivalent to the follow step   reshape input to reshape of shape    block shape 0        block shape m 1     batch / prod block shape     input shape 1        input shape n 1   permute dimension of reshape to produce permute of shape   batch / prod block shape   input shape 1   block shape 0            input shape m   block shape m 1   input shape m 1        input shape n 1   reshape permute to produce reshape permute of shape   batch / prod block shape   input shape 1    block shape 0            input shape m    block shape m 1   input shape m 1            input shape n 1   crop the start and end of dimension  1       m  of reshape permute accord to crop to produce the output of shape    batch / prod block shape   input shape 1    block shape 0    crop 0 0    crop 0 1            input shape m    block shape m 1    crop m 1 0    crop m 1 1   input shape m 1        input shape n 1    some examples   1  for the follow input of shape  4  1  1  1   block shape    2  2   and     crop     0  0    0  0        1        2        3        4     the output tensor have shape  1  2  2  1  and value  x       1    2      3    4      2  for the follow input of shape  4  1  1  3   block shape    2  2   and     crop     0  0    0  0        1  2  3        4  5  6        7  8  9        10  11  12     the output tensor have shape  1  2  2  3  and value  x       1  2  3    4  5  6           7  8  9    10  11  12      3  for the follow input of shape  4  2  2  1   block shape    2  2   and     crop     0  0    0  0    x       1    3      9    11            2    4      10    12            5    7      13    15            6    8      14    16     the output tensor have shape  1  4  4  1  and value  x       1      2     3     4          5      6     7     8          9     10    11     12          13    14    15     16      4  for the follow input of shape  8  1  3  1   block shape    2  2   and     crop     0  0    2  0    x       0    1    3        0    9    11            0    2    4        0    10    12            0    5    7        0    13    15            0    6    8        0    14    16     the output tensor have shape  2  2  4  1  and value  x       1      2     3     4           5      6     7     8            9     10    11     12           13    14    15     16     ) ( name : a name for the operation  optional   )
( arr : an int32 tensor of non negative value  ) ( weights : if non none  must be the same shape as arr  for each value in arr  the bin will be incremented by the correspond weight instead of 1  ) ( minlength : if give  ensure the output have length at least minlength  pad with zero at the end if necessary  ) ( maxlength : if give  skip value in arr that be equal or greater than maxlength  ensure that the output have length at most maxlength  ) ( dtype : if weight be none  determine the type of the output bin  )
( tensor : n d tensor  ) ( mask : k d boolean tensor  k <  n and k must be know statically  ) ( name : a name for this operation  optional   ) ( axis : a 0 d int tensor represent the axis in tensor to mask from  by default  axis be 0 which will mask from the first dimension  otherwise k   axis <  n  )
( pred fn pairs : dict or list of pair of a boolean scalar tensor and a callable which return a list of tensors  ) ( default : optional callable that return a list of tensors  ) ( exclusive : true iff at most one predicate be allow to evaluate to true  ) ( strict : a boolean that enables/disables 'strict' mode  see above  ) ( name : a name for this operation  optional   )
( t : a tensor  ) ( clip norm : a 0 d  scalar  tensor > 0  a maximum clip value  ) ( name : a name for the operation  optional   )

( pred : a scalar determine whether to return the result of true fn or false fn  ) ( true fn : the callable to be perform if pred be true  ) ( false fn : the callable to be perform if pred be false  ) ( strict : a boolean that enables/disables 'strict' mode  see above  ) ( name : optional name prefix for the return tensors  )
( labels : 1 d tensor of real label for the classification task  ) ( predictions : 1 d tensor of predictions for a give classification  ) ( num classes : the possible number of label the classification task can have  if this value be not provide  it will be calculate use both predictions and label array  ) ( dtype : data type of the confusion matrix  ) ( name : scope name  ) ( weights : an optional tensor whose shape match predictions  )
( value : a constant value  or list  of output type dtype  ) ( dtype : the type of the elements of the result tensor  ) ( shape : optional dimension of result tensor  ) ( name : optional name for the tensor  ) ( verify shape : boolean that enable verification of a shape of value  )
( container name : the container string to use in the context  )

( value : an object whose type have a register tensor conversion function  ) ( dtype : optional element type for the return tensor  if miss  the type be infer from the type of value  ) ( name : optional name to use if a new tensor be create  ) ( preferred dtype : optional element type for the return tensor  use when dtype be none  in some case  a caller may not have a dtype in mind when convert to a tensor  so prefer dtype can be use as a soft preference   if the conversion to prefer dtype be not possible  this argument have no effect  ) ( dtype hint : same mean as prefer dtype  and override it  )
( value : an indexedslices  sparsetensor  or an object that can be consume by convert to tensor    ) ( dtype :  optional   the require dtype of the return tensor or indexedslices  ) ( name :  optional   a name to use if a new tensor be create  )
( value : a sparsetensor  sparsetensorvalue  or an object whose type have a register tensor conversion function  ) ( dtype : optional element type for the return tensor  if miss  the type be infer from the type of value  ) ( name : optional name to use if a new tensor be create  )
( input tensor : the tensor to reduce  should be of numeric type  bool  or string  ) ( axis : the dimension to reduce  if none  the default   reduce all dimension  must be in the range   rank input tensor   rank input tensor    ) ( keepdims : if true  retain reduce dimension with length 1  ) ( dtype : the output dtype  default to tf int64  ) ( name : a name for the operation  optional   ) ( reduction indices : the old  deprecate  name for axis  ) ( keep dims : deprecate alias for keepdims  ) ( input : override input tensor  for compatibility  )
( ref : a variable  must be one of the follow type  int32  int64  should be from a scalar variable node  ) ( limit : an int  if incrementing ref would bring it above limit  instead generate an 'outofrange' error  ) ( name : a name for the operation  optional   )
( shape : list of integers   the shape of the full variable  ) ( slicing : list of integers   how to partition the variable  must be of the same length as shape   each value indicate how many slice to create in the correspond dimension   presently only one of the value can be more than 1  that be  the variable can only be slice along one dimension  for convenience  the request number of partition do not have to divide the correspond dimension evenly   if it do not  the shape of the partition be incremented by 1 start from partition 0 until all slack be absorb   the adjustment rule may change in the future  but as you can save/restore these variables with different slice specifications this should not be a problem  ) ( initializer : a tensor of shape shape or a variable initializer function   if a function  it will be call once for each slice  pass the shape and data type of the slice as parameters   the function must return a tensor with the same shape as the slice  ) ( dtype : type of the variables  ignore if initializer be a tensor  ) ( trainable : if true also add all the variables to the graph collection graphkeys trainable variables  ) ( collections : list of graph collections key to add the variables to  default to  graphkeys global variables   ) ( name : optional name for the full variable   default to "partitionedvariable" and get uniquified automatically  ) ( reuse : boolean or none  if true and name be set  it would reuse previously create variables  if false it will create new variables  if none  it would inherit the parent scope reuse  )
( records : a tensor of type string  each string be a record/row in the csv and all record should have the same format  ) ( record defaults : a list of tensor object with specific type  acceptable type be float32  float64  int32  int64  string  one tensor per column of the input record  with either a scalar default value for that column or an empty vector if the column be require  ) ( field delim : an optional string  default to " "  char delimiter to separate field in a record  ) ( use quote delim : an optional bool  default to true  if false  treat double quotation mark as regular character inside of the string field  ignore rfc 4180  section 2  bullet 5   ) ( name : a name for the operation  optional   ) ( na value : additional string to recognize as na/nan  ) ( select cols : optional sort list of column indices to select  if specify  only this subset of columns will be parse and return  )
( input bytes : each element of the input tensor be convert to an array of bytes  ) ( out type : dtype of the output  acceptable type be half  float  double  int32  uint16  uint8  int16  int8  int64  ) ( little endian : whether the input bytes data be in little endian format  data will be convert into host byte order if necessary  ) ( name : a name for the operation  optional   ) ( bytes : deprecate parameter  use input bytes instead  )
( handle : the string representation of a persistent tensor handle  ) ( name : optional name prefix for the return tensor  )
( input : a tensor  ) ( block size : an int that be >  2  the size of the spatial block  same as in space2depth  ) ( data format : an optional string from  "nhwc"  "nchw"  "nchw vect c"  default to "nhwc"  ) ( name : a name for the operation  optional   )
( device name or function : the device name or function to use in the context  )






( x : tensor numerator of real numeric type  ) ( y : tensor denominator of real numeric type  ) ( name : a name for the operation  optional   )

( config :  optional   a tf compat v1 configproto to use to configure the environment in which operations be execute  note that tf compat v1 configproto be also use to configure graph execution  via tf compat v1 session  and many options within tf compat v1 configproto be not implement  or be irrelevant  when eager execution be enable  ) ( device policy :  optional   policy control how operations require input on a specific device  e g   a gpu 0  handle input on a different device   e g  gpu 1 or cpu   when set to none  an appropriate value will be pick automatically  the value pick may change between tensorflow release  valid value   tf contrib eager device placement explicit  raise an error if the placement be not correct  tf contrib eager device placement warn  copy the tensors which be not on the right device but log a warn  tf contrib eager device placement silent  silently copy the tensors  note that this may hide performance problems as there be no notification provide when operations be block on the tensor be copy between devices  tf contrib eager device placement silent for int32  silently copy int32 tensors  raise errors on the other ones  ) ( execution mode :  optional   policy control how operations dispatch be actually execute  when set to none  an appropriate value will be pick automatically  the value pick may change between tensorflow release  valid value  tf contrib eager sync  execute each operation synchronously  tf contrib eager async  execute each operation asynchronously  these operations may return "non ready" handle  )





( input : a tensor  ) ( axis : 0 d  scalar   specify the dimension index at which to expand the shape of input  must be in the range   rank input    1  rank input    ) ( name : the name of the output tensor  optional   ) ( dim : 0 d  scalar   equivalent to axis  to be deprecate  )
( images : a tensor  must be one of the follow type  bfloat16  half  float32  float64  int8  int16  int32  int64  uint8  uint16  uint32  uint64  complex64  complex128  bool  4 d tensor with shape  batch  in row  in cols  depth   ) ( ksizes : a list of ints that have length >  4  the size of the slide window for each dimension of image  ) ( strides : a list of ints that have length >  4  how far the center of two consecutive patch be in the image  must be   1  stride row  stride cols  1   ) ( rates : a list of ints that have length >  4  must be   1  rate row  rate cols  1   this be the input stride  specify how far two consecutive patch sample be in the input  equivalent to extract patch with patch size eff   patch size    patch size   1     rat   1   follow by subsampling them spatially by a factor of rat  this be equivalent to rate in dilate  a k a  atrous  convolutions  ) ( padding : a string from  "same"  "valid"  the type of pad algorithm to use  ) ( name : a name for the operation  optional   )
( num shards : int  number of shards to partition variable  ) ( axis : int  axis to partition on  )
( x : a tensor  must be one of the follow type  bfloat16  half  float32  float64  uint8  int8  uint16  int16  int32  uint32  uint64  int64  complex64  complex128  ) ( y : a tensor  must have the same type as x  ) ( name : a name for the operation  optional   )
( fn : the callable to be perform  ) ( elems : a tensor or  possibly nest  sequence of tensors  each of which will be unpack along their first dimension   the nest sequence of the result slice will be the first argument to fn  ) ( initializer :  optional  a tensor or  possibly nest  sequence of tensors  as the initial value for the accumulator  ) ( parallel iterations :  optional  the number of iterations allow to run in parallel  ) ( back prop :  optional  true enable support for back propagation  ) ( swap memory :  optional  true enable gpu cpu memory swap  ) ( name :  optional  name prefix for the return tensors  )
( fn : the callable to be perform  ) ( elems : a tensor or  possibly nest  sequence of tensors  each of which will be unpack along their first dimension   the nest sequence of the result slice will be the first argument to fn  ) ( initializer :  optional  a tensor or  possibly nest  sequence of tensors  as the initial value for the accumulator  ) ( parallel iterations :  optional  the number of iterations allow to run in parallel  ) ( back prop :  optional  true enable support for back propagation  ) ( swap memory :  optional  true enable gpu cpu memory swap  ) ( name :  optional  name prefix for the return tensors  )
( params : the tensor from which to gather value  must be at least rank axis   1  ) ( indices : the index tensor   must be one of the follow type  int32  int64  the value must be in range  0  params shape axis    ) ( validate indices : deprecate  do nothing  indices be always validate on cpu  never validate on gpu  caution  on cpu  if an out of bind index be find  an error be raise  on gpu  if an out of bind index be find  a 0 be store in the correspond output value  ) ( axis : a tensor  must be one of the follow type  int32  int64  the axis in params to gather indices from  must be greater than or equal to batch dim   default to the first non batch dimension  support negative index  ) ( batch dims : an integer   the number of batch dimension   must be less than or equal to rank indices   ) ( name : a name for the operation  optional   )
( params : a tensor  the tensor from which to gather value  ) ( indices : a tensor  must be one of the follow type  int32  int64  index tensor  ) ( name : a name for the operation  optional   ) ( batch dims : an integer or a scalar 'tensor'  the number of batch dimension  )
( key : the key for the collection  for example  the graphkeys class contain many standard name for collections  ) ( scope :  optional   if supply  the result list be filter to include only items whose name attribute match use re match  items without a name attribute be never return if a scope be supply and the choice or re match mean that a scope without special tokens filter by prefix  )
( key : the key for the collection  for example  the graphkeys class contain many standard name for collections  )


( name : the name of the new or exist variable  ) ( shape : shape of the new or exist variable  ) ( dtype : type of the new or exist variable  default to dt float   ) ( initializer : initializer for the variable if one be create  can either be an initializer object or a tensor  if it's a tensor  its shape must be know unless validate shape be false  ) ( regularizer : a  tensor  > tensor or none  function  the result of apply it on a newly create variable will be add to the collection tf graphkeys regularization losses and can be use for regularization  ) ( collections : list of graph collections key to add the variable to  default to  graphkeys local variables   see tf variable   ) ( caching device : optional device string or function describe where the variable should be cache for read   default to the variable's device   if not none  cache on another device   typical use be to cache on the device where the ops use the variable reside  to deduplicate copy through switch and other conditional statements  ) ( partitioner : optional callable that accept a fully define tensorshape and dtype of the variable to be create  and return a list of partition for each axis  currently only one axis can be partition   ) ( validate shape : if false  allow the variable to be initialize with a value of unknown shape  if true  the default  the shape of initial value must be know  for this to be use the initializer must be a tensor and not an initializer object  ) ( use resource : if false  create a regular variable  if true  create an experimental resourcevariable instead with well define semantics  default to false  will later change to true   when eager execution be enable this argument be always force to be true  ) ( custom getter : callable that take as a first argument the true getter  and allow overwrite the internal get variable method  the signature of custom getter should match that of this method  but the most future proof version will allow for change  def custom getter getter   args    kwargs    direct access to all get variable parameters be also allow  def custom getter getter  name   args    kwargs    a simple identity custom getter that simply create variables with modify name be  def custom getter getter  name   args    kwargs    return getter name   ' suffix'   args    kwargs  ) ( constraint : an optional projection function to be apply to the variable after be update by an optimizer  e g  use to implement norm constraints or value constraints for layer weight   the function must take as input the unprojected tensor represent the value of the variable and return the tensor for the project value  which must have the same shape   constraints be not safe to use when do asynchronous distribute train  ) ( synchronization : indicate when a distribute a variable will be aggregate  accept value be constants define in the class tf variablesynchronization  by default the synchronization be set to auto and the current distributionstrategy choose when to synchronize  ) ( aggregation : indicate how a distribute variable will be aggregate  accept value be constants define in the class tf variableaggregation  )
( op seed : integer  )
( data : a tensor to be store in the session  ) ( name : optional name prefix for the return tensor  )
( handle : the string representation of a persistent tensor handle  ) ( dtype : the type of the output tensor  ) ( name : optional name prefix for the return tensor  )
( name : the name of the new or exist variable  ) ( shape : shape of the new or exist variable  ) ( dtype : type of the new or exist variable  default to dt float   ) ( initializer : initializer for the variable if one be create  can either be an initializer object or a tensor  if it's a tensor  its shape must be know unless validate shape be false  ) ( regularizer : a  tensor  > tensor or none  function  the result of apply it on a newly create variable will be add to the collection tf graphkeys regularization losses and can be use for regularization  ) ( trainable : if true also add the variable to the graph collection graphkeys trainable variables  see tf variable   ) ( collections : list of graph collections key to add the variable to  default to  graphkeys global variables   see tf variable   ) ( caching device : optional device string or function describe where the variable should be cache for read   default to the variable's device   if not none  cache on another device   typical use be to cache on the device where the ops use the variable reside  to deduplicate copy through switch and other conditional statements  ) ( partitioner : optional callable that accept a fully define tensorshape and dtype of the variable to be create  and return a list of partition for each axis  currently only one axis can be partition   ) ( validate shape : if false  allow the variable to be initialize with a value of unknown shape  if true  the default  the shape of initial value must be know  for this to be use the initializer must be a tensor and not an initializer object  ) ( use resource : if false  create a regular variable  if true  create an experimental resourcevariable instead with well define semantics  default to false  will later change to true   when eager execution be enable this argument be always force to be true  ) ( custom getter : callable that take as a first argument the true getter  and allow overwrite the internal get variable method  the signature of custom getter should match that of this method  but the most future proof version will allow for change  def custom getter getter   args    kwargs    direct access to all get variable parameters be also allow  def custom getter getter  name   args    kwargs    a simple identity custom getter that simply create variables with modify name be  def custom getter getter  name   args    kwargs    return getter name   ' suffix'   args    kwargs  ) ( constraint : an optional projection function to be apply to the variable after be update by an optimizer  e g  use to implement norm constraints or value constraints for layer weight   the function must take as input the unprojected tensor represent the value of the variable and return the tensor for the project value  which must have the same shape   constraints be not safe to use when do asynchronous distribute train  ) ( synchronization : indicate when a distribute a variable will be aggregate  accept value be constants define in the class tf variablesynchronization  by default the synchronization be set to auto and the current distributionstrategy choose when to synchronize  ) ( aggregation : indicate how a distribute variable will be aggregate  accept value be constants define in the class tf variableaggregation  )

( scope :  optional   a string  if supply  the result list be filter to include only items whose name attribute match scope use re match  items without a name attribute be never return if a scope be supply  the choice of re match mean that a scope without special tokens filter by prefix  )

( ys : a tensor or list of tensors to be differentiate  ) ( xs : a tensor or list of tensors to be use for differentiation  ) ( grad ys : optional  a tensor or list of tensors the same size as ys and hold the gradients compute for each y in ys  ) ( name : optional name to use for group all the gradient ops together  default to 'gradients'  ) ( colocate gradients with ops : if true  try colocating gradients with the correspond op  ) ( gate gradients : if true  add a tuple around the gradients return for an operations   this avoid some race condition  ) ( aggregation method : specify the method use to combine gradient term  accept value be constants define in the class aggregationmethod  ) ( stop gradients : optional  a tensor or list of tensors not to differentiate through  ) ( unconnected gradients : optional  specify the gradient value return when the give input tensors be unconnected  accept value be constants define in the class tf unconnectedgradients and the default value be none  )
( ys : a tensor or list of tensors to be differentiate  ) ( xs : a tensor or list of tensors to be use for differentiation  ) ( name : optional name to use for group all the gradient ops together  default to 'hessians'  ) ( colocate gradients with ops : see gradients   documentation for detail  ) ( gate gradients : see gradients   documentation for detail  ) ( aggregation method : see gradients   documentation for detail  )
( name : optional name for the initialization op  )



( variable : a variable  )
( library filename : path to the plugin  relative or absolute filesystem path to a dynamic library file  )
( scope :  optional   a string  if supply  the result list be filter to include only items whose name attribute match scope use re match  items without a name attribute be never return if a scope be supply  the choice of re match mean that a scope without special tokens filter by prefix  )

( name  : a name for the scope create by this template  if necessary  the name will be make unique by append  n to the name  ) ( func  : the function to wrap  ) ( create scope now  : boolean control whether the scope should be create when the template be construct or when the template be call  default be false  mean the scope be create when the template be call  ) ( unique name  : when use  it override name  and be not make unique  if a template of the same scope/unique name already exist and reuse be false  an error be raise  default to none  ) ( custom getter  : optional custom getter for variables use in func   see the tf compat v1 get variable custom getter documentation for more information  ) (   kwargs : keyword arguments to apply to func   )
( fn : the callable to be perform   it accept one argument  which will have the same  possibly nest  structure as elems   its output must have the same structure as fn output signature if one be provide  otherwise it must have the same structure as elems  ) ( elems : a tensor or  possibly nest  sequence of tensors  each of which will be unstacked along their first dimension   fn will be apply to the nest sequence of the result slice   elems may include rag and sparse tensors  elems must consist of at least one tensor  ) ( dtype : deprecate  equivalent to fn output signature  ) ( parallel iterations :  optional  the number of iterations allow to run in parallel  when graph build  the default value be 10  while execute eagerly  the default value be set to 1  ) ( back prop :  optional  false disable support for back propagation  ) ( swap memory :  optional  true enable gpu cpu memory swap  ) ( infer shape :  optional  false disable test for consistent output shape  ) ( name :  optional  name prefix for the return tensors  ) ( fn output signature : the output signature of fn  must be specify if fn's input and output signatures be different  i e   if their structure  dtypes  or tensor type do not match   fn output signature can be specify use any of the follow   a tf dtype or tf tensorspec  to describe a tf tensor  a tf raggedtensorspec  to describe a tf raggedtensor  a tf sparsetensorspec  to describe a tf sparse sparsetensor  a  possibly nest  tuple  list  or dict contain the above type  )
( max partitions : upper bind on the number of partition  default to 1  ) ( axis : axis along which to partition the variable  default to 0  ) ( min slice size : minimum size of the variable slice per partition  default to 256k  ) ( bytes per string element : if the variable be of type string  this provide an estimate of how large each scalar in the variable be  )
( scope :  optional   a string  if supply  the result list be filter to include only items whose name attribute match scope use re match  items without a name attribute be never return if a scope be supply  the choice of re match mean that a scope without special tokens filter by prefix  )
( scope :  optional   a string  if supply  the result list be filter to include only items whose name attribute match scope use re match  items without a name attribute be never return if a scope be supply  the choice of re match mean that a scope without special tokens filter by prefix  )
( logits : 2 d tensor with shape  batch size  num class    each slice  i     represent the unnormalized log probabilities for all class  ) ( num samples : 0 d   number of independent sample to draw for each row slice  ) ( seed : a python integer  use to create a random seed for the distribution  see tf random set seed for behavior  ) ( name : optional name for the operation  ) ( output dtype : the integer type of the output  int32 or int64  default to int64  )

( tensor : tensor of type float32  float64  complex64  complex128 ) ( ord : order of the norm  support value be 'fro'  'euclidean'  1  2  np inf and any positive real number yield the correspond p norm  default be 'euclidean' which be equivalent to frobenius norm if tensor be a matrix and equivalent to 2 norm for vectors  some restrictions apply    a  the frobenius norm fro be not define for vectors    b  if axis be a 2 tuple  matrix norm   only 'euclidean'  'fro'  1       2  np inf be support  see the description of axis on how to compute norms for a batch of vectors or matrices store in a tensor  ) ( axis : if axis be none  the default   the input be consider a vector and a single vector norm be compute over the entire set of value in the tensor  i e  norm tensor  ord ord  be equivalent to norm reshape tensor    1    ord ord   if axis be a python integer  the input be consider a batch of vectors  and axis determine the axis in tensor over which to compute vector norms  if axis be a 2 tuple of python integers it be consider a batch of matrices and axis determine the ax in tensor over which to compute a matrix norm  negative indices be support  example  if you be pass a tensor that can be either a matrix or a batch of matrices at runtime  pass axis   2  1  instead of axis none to make sure that matrix norms be compute  ) ( keepdims : if true  the axis indicate in axis be keep with size 1  otherwise  the dimension in axis be remove from the output shape  ) ( name : the name of the op  ) ( keep dims : deprecate alias for keepdims  )
( tensor : a tensor  ) ( dtype : a type for the return tensor  must be float32  float64  int8  uint8  int16  uint16  int32  int64  complex64  complex128 or bool  ) ( name : a name for the operation  optional   ) ( optimize : if true  attempt to statically determine the shape of 'tensor' and encode it as a constant  )

( tensor : a tensor  ) ( paddings : a tensor of type int32  ) ( mode : one of "constant"  "reflect"  or "symmetric"  case insensitive  ) ( name : a name for the operation  optional   ) ( constant values : in "constant" mode  the scalar pad value to use  must be same type as tensor  )
( serialized : a vector  1 d tensor  of string  a batch of binary serialize example protos  ) ( features : a dict map feature key to fixedlenfeature  varlenfeature  sparsefeature  and raggedfeature value  ) ( example names : a vector  1 d tensor  of string  optional   the name of the serialize protos in the batch  ) ( name : a name for this operation  optional   )
( serialized : a scalar string tensor  a single serialize example  ) ( features : a dict map feature key to fixedlenfeature or varlenfeature value  ) ( name : a name for this operation  optional   ) ( example names :  optional  a scalar string tensor  the associate name  )
( dtype : the type of elements in the tensor to be feed  ) ( shape : the shape of the tensor to be feed  optional   if the shape be not specify  you can fee a tensor of any shape  ) ( name : a name for the operation  optional   )
( input : a tensor  the default value to produce when output be not feed  ) ( shape : a tf tensorshape or list of ints  the  possibly partial  shape of the tensor  ) ( name : a name for the operation  optional   )
( func : a python function  which accept ndarray object as arguments and return a list of ndarray object  or a single ndarray   this function must accept as many arguments as there be tensors in inp  and these argument type will match the correspond tf tensor object in inp  the return ndarrays must match the number and type define tout  important note  input and output numpy ndarrays of func be not   guarantee to be copy  in some case their underlie memory will be   share with the correspond tensorflow tensors  in place modification   or store func input or return value in python datastructures   without explicit  np  copy can have non deterministic consequences  ) ( inp : a list of tensor object  ) ( Tout : a list or tuple of tensorflow data type or a single tensorflow data type if there be only one  indicate what func return  ) ( stateful :  boolean   if true  the function should be consider stateful  if a function be stateless  when give the same input it will return the same output and have no observable side effect  optimizations such as common subexpression elimination be only perform on stateless operations  ) ( name : a name for the operation  optional   )

( mean : a python scalar or a scalar tensor  mean of the random value to generate  ) ( stddev : a python scalar or a scalar tensor  standard deviation of the random value to generate  ) ( seed : a python integer  use to create random seed  see tf compat v1 set random seed for behavior  ) ( dtype : default data type  use if no dtype argument be provide when call the initializer  only float point type be support  )
( lam : a tensor or python value or n d array of type dtype  lam provide the rate parameter s  describe the poisson distribution s  to sample  ) ( shape : a 1 d integer tensor or python array  the shape of the output sample to be draw per "rate" parameterized distribution  ) ( dtype : the type of the output  float16  float32  float64  int32 or int64  ) ( seed : a python integer  use to create a random seed for the distributions  see tf random set seed for behavior  ) ( name : optional name for the operation  )
( minval : a python scalar or a scalar tensor  lower bind of the range of random value to generate  ) ( maxval : a python scalar or a scalar tensor  upper bind of the range of random value to generate   default to 1 for float type  ) ( seed : a python integer  use to create random seed  see tf compat v1 set random seed for behavior  ) ( dtype : default data type  use if no dtype argument be provide when call the initializer  )
( input tensor : the boolean tensor to reduce  ) ( axis : the dimension to reduce  if none  the default   reduce all dimension  must be in the range   rank input tensor   rank input tensor    ) ( keepdims : if true  retain reduce dimension with length 1  ) ( name : a name for the operation  optional   ) ( reduction indices : the old  deprecate  name for axis  ) ( keep dims : deprecate alias for keepdims  )
( input tensor : the boolean tensor to reduce  ) ( axis : the dimension to reduce  if none  the default   reduce all dimension  must be in the range   rank input tensor   rank input tensor    ) ( keepdims : if true  retain reduce dimension with length 1  ) ( name : a name for the operation  optional   ) ( reduction indices : the old  deprecate  name for axis  ) ( keep dims : deprecate alias for keepdims  )
( inputs : a tf string tensor  ) ( axis : which axis to join along  the default behavior be to join all elements  produce a scalar  ) ( keepdims : if true  retain reduce dimension with length 1  ) ( separator : a string add between each string be join  ) ( name : a name for the operation  optional   )
( input tensor : the tensor to reduce  should have numeric type  ) ( axis : the dimension to reduce  if none  the default   reduce all dimension  must be in the range   rank input tensor   rank input tensor    ) ( keepdims : if true  retain reduce dimension with length 1  ) ( name : a name for the operation  optional   ) ( reduction indices : the old  deprecate  name for axis  ) ( keep dims : deprecate alias for keepdims  )
( input tensor : the tensor to reduce  should have real numeric type  ) ( axis : the dimension to reduce  if none  the default   reduce all dimension  must be in the range   rank input tensor   rank input tensor    ) ( keepdims : if true  retain reduce dimension with length 1  ) ( name : a name for the operation  optional   ) ( reduction indices : the old  deprecate  name for axis  ) ( keep dims : deprecate alias for keepdims  )
( input tensor : the tensor to reduce  should have numeric type  ) ( axis : the dimension to reduce  if none  the default   reduce all dimension  must be in the range   rank input tensor   rank input tensor    ) ( keepdims : if true  retain reduce dimension with length 1  ) ( name : a name for the operation  optional   ) ( reduction indices : the old  deprecate  name for axis  ) ( keep dims : deprecate alias for keepdims  )
( input tensor : the tensor to reduce  should have real numeric type  ) ( axis : the dimension to reduce  if none  the default   reduce all dimension  must be in the range   rank input tensor   rank input tensor    ) ( keepdims : if true  retain reduce dimension with length 1  ) ( name : a name for the operation  optional   ) ( reduction indices : the old  deprecate  name for axis  ) ( keep dims : deprecate alias for keepdims  )
( input tensor : the tensor to reduce  should have numeric type  ) ( axis : the dimension to reduce  if none  the default   reduce all dimension  must be in the range   rank input tensor   rank input tensor    ) ( keepdims : if true  retain reduce dimension with length 1  ) ( name : a name for the operation  optional   ) ( reduction indices : the old  deprecate  name for axis  ) ( keep dims : deprecate alias for keepdims  )
( input tensor : the tensor to reduce  should have numeric type  ) ( axis : the dimension to reduce  if none  the default   reduce all dimension  must be in the range   rank input tensor   rank input tensor    ) ( keepdims : if true  retain reduce dimension with length 1  ) ( name : a name for the operation  optional   ) ( reduction indices : the old  deprecate  name for axis  ) ( keep dims : deprecate alias for keepdims  )
( var list : list of variable object to check  default to the value of global variables     local variables   ) ( name : optional name of the operation  )


( input : a tensor  the input to reverse  ) ( seq lengths : a tensor  must be one of the follow type  int32  int64  1 d with length input dim batch axis  and max seq lengths  <  input dim seq axis  ) ( seq axis : an int  the dimension which be partially reverse  ) ( batch axis : an optional int  default to 0  the dimension along which reversal be perform  ) ( name : a name for the operation  optional   )
( scalar : a 0 d scalar tensor  must have know shape  ) ( x : a tensor or indexedslices to be scale  ) ( name : a name for the operation  optional   )
( fn : the callable to be perform   it accept two arguments   the first will have the same structure as initializer if one be provide  otherwise it will have the same structure as elems   the second will have the same  possibly nest  structure as elems   its output must have the same structure as initializer if one be provide  otherwise it must have the same structure as elems  ) ( elems : a tensor or  possibly nest  sequence of tensors  each of which will be unpack along their first dimension   the nest sequence of the result slice will be the first argument to fn  ) ( initializer :  optional  a tensor or  possibly nest  sequence of tensors  initial value for the accumulator  and the expect output type of fn  ) ( parallel iterations :  optional  the number of iterations allow to run in parallel  ) ( back prop :  optional  true enable support for back propagation  ) ( swap memory :  optional  true enable gpu cpu memory swap  ) ( infer shape :  optional  false disable test for consistent output shape  ) ( reverse :  optional  true scan the tensor last to first  instead of first to last   ) ( name :  optional  name prefix for the return tensors  )
( ref : a variable  ) ( indices : a tensor  must be one of the follow type  int32  int64  a tensor of indices into the first dimension of ref  ) ( updates : a tensor  must have the same type as ref  a tensor of update value to store in ref  ) ( use locking : an optional bool  default to false  if true  the assignment will be protect by a lock  otherwise the behavior be undefined  but may exhibit less contention  ) ( name : a name for the operation  optional   )
( ref : a mutable tensor  must be one of the follow type  float32  float64  int32  uint8  int16  int8  complex64  int64  qint8  quint8  qint32  bfloat16  uint16  complex128  half  uint32  uint64  should be from a variable node  ) ( indices : a tensor  must be one of the follow type  int32  int64  a tensor of indices into the first dimension of ref  ) ( updates : a tensor  must have the same type as ref  a tensor of value that ref be divide by  ) ( use locking : an optional bool  default to false  if true  the operation will be protect by a lock  otherwise the behavior be undefined  but may exhibit less contention  ) ( name : a name for the operation  optional   )
( ref : a mutable tensor  must be one of the follow type  half  bfloat16  float32  float64  int32  int64  should be from a variable node  ) ( indices : a tensor  must be one of the follow type  int32  int64  a tensor of indices into the first dimension of ref  ) ( updates : a tensor  must have the same type as ref  a tensor of update value to reduce into ref  ) ( use locking : an optional bool  default to false  if true  the update will be protect by a lock  otherwise the behavior be undefined  but may exhibit less contention  ) ( name : a name for the operation  optional   )
( ref : a mutable tensor  must be one of the follow type  half  bfloat16  float32  float64  int32  int64  should be from a variable node  ) ( indices : a tensor  must be one of the follow type  int32  int64  a tensor of indices into the first dimension of ref  ) ( updates : a tensor  must have the same type as ref  a tensor of update value to reduce into ref  ) ( use locking : an optional bool  default to false  if true  the update will be protect by a lock  otherwise the behavior be undefined  but may exhibit less contention  ) ( name : a name for the operation  optional   )
( ref : a mutable tensor  must be one of the follow type  float32  float64  int32  uint8  int16  int8  complex64  int64  qint8  quint8  qint32  bfloat16  uint16  complex128  half  uint32  uint64  should be from a variable node  ) ( indices : a tensor  must be one of the follow type  int32  int64  a tensor of indices into the first dimension of ref  ) ( updates : a tensor  must have the same type as ref  a tensor of update value to multiply to ref  ) ( use locking : an optional bool  default to false  if true  the operation will be protect by a lock  otherwise the behavior be undefined  but may exhibit less contention  ) ( name : a name for the operation  optional   )
( ref : a mutable tensor  must be one of the follow type  float32  float64  int32  uint8  int16  int8  complex64  int64  qint8  quint8  qint32  bfloat16  uint16  complex128  half  uint32  uint64  a mutable tensor  should be from a variable node  ) ( indices : a tensor  must be one of the follow type  int32  int64  a tensor of indices into ref  ) ( updates : a tensor  must have the same type as ref  a tensor of update value to add to ref  ) ( use locking : an optional bool  default to false  if true  the assignment will be protect by a lock  otherwise the behavior be undefined  but may exhibit less contention  ) ( name : a name for the operation  optional   )
( ref : a mutable tensor  must be one of the follow type  float32  float64  int32  uint8  int16  int8  complex64  int64  qint8  quint8  qint32  bfloat16  uint16  complex128  half  uint32  uint64  a mutable tensor  should be from a variable node  ) ( indices : a tensor  must be one of the follow type  int32  int64  a tensor of indices into ref  ) ( updates : a tensor  must have the same type as ref  a tensor of update value to add to ref  ) ( use locking : an optional bool  default to false  an optional bool  default to true  if true  the assignment will be protect by a lock  otherwise the behavior be undefined  but may exhibit less contention  ) ( name : a name for the operation  optional   )
( ref : a variable  ) ( indices : a tensor  must be one of the follow type  int32  int64  a tensor of indices into ref  ) ( updates : a tensor  must have the same type as ref  a tensor  must have the same type as ref  a tensor of update value to add to ref  ) ( use locking : an optional bool  default to true  an optional bool  default to true  if true  the assignment will be protect by a lock  otherwise the behavior be undefined  but may exhibit less contention  ) ( name : a name for the operation  optional   )
( ref : a mutable tensor  must be one of the follow type  float32  float64  int32  uint8  int16  int8  complex64  int64  qint8  quint8  qint32  bfloat16  uint16  complex128  half  uint32  uint64  should be from a variable node  ) ( indices : a tensor  must be one of the follow type  int32  int64  a tensor of indices into the first dimension of ref  ) ( updates : a tensor  must have the same type as ref  a tensor of update value to subtract from ref  ) ( use locking : an optional bool  default to false  if true  the subtraction will be protect by a lock  otherwise the behavior be undefined  but may exhibit less contention  ) ( name : a name for the operation  optional   )
( ref : a variable  ) ( indices : a tensor  must be one of the follow type  int32  int64  a tensor of indices into the first dimension of ref  ) ( updates : a tensor  must have the same type as ref  a tensor of update value to store in ref  ) ( use locking : an optional bool  default to true  if true  the assignment will be protect by a lock  otherwise the behavior be undefined  but may exhibit less contention  ) ( name : a name for the operation  optional   )
( sp input : the input rank r sparsetensor  ) ( name : a name prefix for the return tensors  optional   ) ( out type : the dtype to use for serialization  )
( sp input : the input sparsetensor  ) ( name : a name prefix for the return tensors  optional   ) ( out type : the dtype to use for serialization  )
( seed : integer  )
( x : a tensor  1 d  value to keep  ) ( y : a tensor  must have the same type as x  1 d  value to remove  ) ( out idx : an optional tf dtype from  tf int32  tf int64  default to tf int32  ) ( name : a name for the operation  optional   )
( input : a tensor or sparsetensor  ) ( name : a name for the operation  optional   ) ( out type :  optional  the specify output type of the operation  int32 or int64   default to tf int32  )
( input : a tensor or sparsetensor  ) ( name : a name for the operation  optional   ) ( out type :  optional  the specify non quantize numeric output type of the operation  default to tf int32  )
( input : a tensor  4 d with shape  batch  height  width  depth   ) ( paddings : a tensor  must be one of the follow type  int32  int64  2 d tensor of non negative integers with shape  2  2   it specify   the pad of the input with zero across the spatial dimension as follow    paddings     pad top  pad bottom    pad leave  pad right   the effective spatial dimension of the zero pad input tensor will be    height pad   pad top   height   pad bottom  width pad   pad leave   width   pad right ) ( block size : an int that be >  2  ) ( name : a name for the operation  optional   )
( input : a tensor  ) ( block size : an int that be >  2  the size of the spatial block  ) ( data format : an optional string from  "nhwc"  "nchw"  "nchw vect c"  default to "nhwc"  ) ( name : a name for the operation  optional   )
( a : the first operand  sparsetensor or tensor  ) ( b : the second operand  sparsetensor or tensor  at least one operand must be sparse  ) ( threshold : an optional 0 d tensor  default to 0   the magnitude threshold that determine if an output value/index pair take space  its dtype should match that of the value if they be real  if the latter be complex64/complex128  then the dtype should be float32/float64  correspondingly  ) ( thresh : deprecate alias for threshold  )
( axis : dimension to concatenate along  must be in range   rank  rank   where rank be the number of dimension in each input sparsetensor  ) ( sp inputs : list of sparsetensor to concatenate  ) ( name : a name prefix for the return tensors  optional   ) ( expand nonconcat dim : whether to allow the expansion in the non concat dimension  default to false  ) ( concat dim : the old  deprecate  name for axis  ) ( expand nonconcat dims : alias for expand nonconcat dim )
( a : a tensor  must be one of the follow type  float32  bfloat16  ) ( b : a tensor  must be one of the follow type  float32  bfloat16  ) ( transpose a : an optional bool  default to false  ) ( transpose b : an optional bool  default to false  ) ( a is sparse : an optional bool  default to false  ) ( b is sparse : an optional bool  default to false  ) ( name : a name for the operation  optional   )
( sp ids : a single sparsetensor with value property of type int32 or int64 or a python list of such sparsetensors or a list thereof  ) ( sp values : a sparsetensor of any type  ) ( vocab size : a scalar int64 tensor  or python int  contain the new size of the last dimension  all 0 <  sp ids value < vocab size   or a list thereof with all 0 <  sp ids i  value < vocab size i   for all i  ) ( name : a name prefix for the return tensors  optional  ) ( already sorted : a boolean to specify whether the per batch value in sp value be already sort  if so skip sort  false by default  optional   )
( dtype : the type of value elements in the tensor to be feed  ) ( shape : the shape of the tensor to be feed  optional   if the shape be not specify  you can fee a sparse tensor of any shape  ) ( name : a name for prefix the operations  optional   )
( sp input : the sparsetensor to reduce  should have numeric type  ) ( axis : the dimension to reduce  list or scalar  if none  the default   reduce all dimension  ) ( keepdims : if true  retain reduce dimension with length 1  ) ( reduction axes : deprecate name of axis  ) ( keep dims : deprecate alias for keepdims  )
( sp input : the sparsetensor to reduce  should have numeric type  ) ( axis : the dimension to reduce  list or scalar  if none  the default   reduce all dimension  ) ( keepdims : if true  retain reduce dimension with length 1  ) ( reduction axes : deprecate name of axis  ) ( keep dims : deprecate alias for keepdims  )
( sp input : the sparsetensor to reduce  should have numeric type  ) ( axis : the dimension to reduce  list or scalar  if none  the default   reduce all dimension  ) ( keepdims : if true  retain reduce dimension with length 1  ) ( reduction axes : deprecate name of axis  ) ( keep dims : deprecate alias for keepdims  )
( sp input : the sparsetensor to reduce  should have numeric type  ) ( axis : the dimension to reduce  list or scalar  if none  the default   reduce all dimension  ) ( keepdims : if true  retain reduce dimension with length 1  ) ( reduction axes : deprecate name of axis  ) ( keep dims : deprecate alias for keepdims  )
( data : a tensor with data that will be assemble in the output  ) ( indices : a 1 d tensor with indices into data  have same rank as segment ids  ) ( segment ids : a 1 d tensor with indices into the output tensor  value should be sort and can be repeat  ) ( name : a name for the operation  optional   ) ( num segments : an optional int32 scalar  indicate the size of the output tensor  )
( data : a tensor with data that will be assemble in the output  ) ( indices : a 1 d tensor with indices into data  have same rank as segment ids  ) ( segment ids : a 1 d tensor with indices into the output tensor  value should be sort and can be repeat  ) ( name : a name for the operation  optional   ) ( num segments : an optional int32 scalar  indicate the size of the output tensor  )
( data : a tensor with data that will be assemble in the output  ) ( indices : a 1 d tensor with indices into data  have same rank as segment ids  ) ( segment ids : a 1 d tensor with indices into the output tensor  value should be sort and can be repeat  ) ( name : a name for the operation  optional   ) ( num segments : an optional int32 scalar  indicate the size of the output tensor  )
( keyword required : python 2 standin for    temporary for argument reorder  ) ( sp input : the sparsetensor to split  ) ( num split : a python integer  the number of ways to split  ) ( axis : a 0 d int32 tensor  the dimension along which to split  must be in range   rank  rank   where rank be the number of dimension in the input sparsetensor  ) ( name : a name for the operation  optional   ) ( split dim : deprecate old name for axis  )
( sparse indices : a 0 d  1 d  or 2 d tensor of type int32 or int64  sparse indices i  contain the complete index where sparse value i  will be place  ) ( output shape : a 1 d tensor of the same type as sparse indices   shape of the dense output tensor  ) ( sparse values : a 0 d or 1 d tensor   value correspond to each row of sparse indices  or a scalar value to be use for all sparse indices  ) ( default value : a 0 d tensor of the same type as sparse value   value to set for indices not specify in sparse indices   default to zero  ) ( validate indices : a boolean value   if true  indices be check to make sure they be sort in lexicographic order and that there be no repeat  ) ( name : a name for the operation  optional   )
( input : a tensor  the input to squeeze  ) ( axis : an optional list of ints  default to     if specify  only squeeze the dimension list  the dimension index start at 0  it be an error to squeeze a dimension that be not 1  must be in the range   rank input   rank input    must be specify if input be a raggedtensor  ) ( name : a name for the operation  optional   ) ( squeeze dims : deprecate keyword argument that be now axis  )
( source : 1 d string tensor  the string to split  ) ( sep : 0 d string tensor  the delimiter character  the string should be length 0 or 1  default be ' '  ) ( skip empty : a bool  if true  skip the empty string from the result  ) ( delimiter : deprecate alias for sep  ) ( result type : the tensor type for the result  one of "raggedtensor" or "sparsetensor"  ) ( name : a name for the operation  optional   )
( string tensor : a tensor of type string  ) ( num buckets : an int that be >  1  the number of bucket  ) ( name : a name for the operation  optional   )
( string tensor : a tensor of type string  ) ( out type : an optional tf dtype from  tf float32  tf float64  tf int32  tf int64  default to tf float32  the numeric type to interpret each string in string tensor as  ) ( name : a name for the operation  optional   )
( input : a tensor of type string  tensor of string ) ( pos : a tensor  must be one of the follow type  int32  int64  scalar define the position of first character in each substring ) ( len : a tensor  must have the same type as pos  scalar define the number of character to include in each substring ) ( unit : an optional string from  "byte"  "utf8 char"  default to "byte"  the unit that be use to create the substring   one of  "byte"  for define position and length by bytes  or "utf8 char"  for the utf 8 encode unicode code point    the default be "byte"  result be undefined if unit utf8 char and the input string do not contain structurally valid utf 8  ) ( name : a name for the operation  optional   )
( name : optional name for the initialization op  )
( x : a tensor or sparsetensor or indexedslices  ) ( name : a name for the operation  optional   )
( x : a tensor or sparsetensor or indexedslices  ) ( name : a name for the operation  optional   )
( x : a tensor or sparsetensor or indexedslices  ) ( name : a name for the operation  optional   )
( x : a tensor or sparsetensor or indexedslices  ) ( name : a name for the operation  optional   )
( x : a tensor or sparsetensor or indexedslices  ) ( name : a name for the operation  optional   )
( x : a tensor or sparsetensor or indexedslices  ) ( name : a name for the operation  optional   )
( x : a tensor or sparsetensor or indexedslices  ) ( name : a name for the operation  optional   )
( scope :  optional   a string  if supply  the result list be filter to include only items whose name attribute match scope use re match  items without a name attribute be never return if a scope be supply  the choice of re match mean that a scope without special tokens filter by prefix  )
( a : a tensor  ) ( perm : a permutation of the dimension of a  ) ( name : a name for the operation  optional   ) ( conjugate : optional bool  set it to true be mathematically equivalent to tf math conj tf transpose input    )
( mean : a python scalar or a scalar tensor  mean of the random value to generate  ) ( stddev : a python scalar or a scalar tensor  standard deviation of the random value to generate  ) ( seed : a python integer  use to create random seed  see tf compat v1 set random seed for behavior  ) ( dtype : default data type  use if no dtype argument be provide when call the initializer  only float point type be support  )
( tensors : a list of tensors or indexedslices  some entries can be none  ) ( name :  optional  a name to use as a name scope for the operation  ) ( control inputs : list of additional ops to finish before return  )
( factor : float   a multiplicative factor by which the value will be scale  ) ( seed : a python integer  use to create random seed  see tf compat v1 set random seed for behavior  ) ( dtype : default data type  use if no dtype argument be provide when call the initializer  only float point type be support  )
( max shard bytes : the maximum size any give shard be allow to be  ) ( axis : the axis to partition along   default  outermost axis  ) ( bytes per string element : if the variable be of type string  this provide an estimate of how large each scalar in the variable be  ) ( max shards : the maximum number of shards in int create take precedence over max shard bytes  )
( variable creator : the pass creator )

( name or scope : string or variablescope  the scope to open  ) ( default name : the default name to use if the name or scope argument be none  this name will be uniquified  if name or scope be provide it won't be use and therefore it be not require and can be none  ) ( values : the list of tensor arguments that be pass to the op function  ) ( initializer : default initializer for variables within this scope  ) ( regularizer : default regularizer for variables within this scope  ) ( caching device : default cache device for variables within this scope  ) ( partitioner : default partitioner for variables within this scope  ) ( custom getter : default custom getter for variables within this scope  ) ( reuse : true  none  or tf compat v1 auto reuse  if true  we go into reuse mode for this scope as well as all sub scopes  if tf compat v1 auto reuse  we create variables if they do not exist  and return them otherwise  if none  we inherit the parent scope's reuse flag  when eager execution be enable  new variables be always create unless an eagervariablestore or template be currently active  ) ( dtype : type of variables create in this scope  default to the type in the pass scope  or inherit from parent scope   ) ( use resource : if false  all variables will be regular variables  if true  experimental resourcevariables with well define semantics will be use instead  default to false  will later change to true   when eager execution be enable this argument be always force to be true  ) ( constraint : an optional projection function to be apply to the variable after be update by an optimizer  e g  use to implement norm constraints or value constraints for layer weight   the function must take as input the unprojected tensor represent the value of the variable and return the tensor for the project value  which must have the same shape   constraints be not safe to use when do asynchronous distribute train  ) ( auxiliary name scope : if true  we create an auxiliary name scope with the scope  if false  we don't create it  note that the argument be not inherit  and it only take effect for once when create  you should only use it for re enter a premade variable scope  )
( var list : list of variable object to initialize  ) ( name : optional name for the return operation  )
( t : tensor to check  ) ( msg : message to log on failure  ) ( name : a name for this operation  optional   ) ( x : alias for t  ) ( message : alias for msg  )
( condition : a tensor of type bool ) ( x : a tensor which may have the same shape as condition  if condition be rank 1  x may have higher rank  but its first dimension must match the size of condition  ) ( y : a tensor with the same shape and type as x  ) ( name : a name of the operation  optional  )
( cond : a callable that represent the termination condition of the loop  ) ( body : a callable that represent the loop body  ) ( loop vars : a  possibly nest  tuple  namedtuple or list of numpy array  tensor  and tensorarray object  ) ( shape invariants : the shape invariants for the loop variables  ) ( parallel iterations : the number of iterations allow to run in parallel  it must be a positive integer  ) ( back prop : whether backprop be enable for this while loop  ) ( swap memory : whether gpu cpu memory swap be enable for this loop  ) ( name : optional name prefix for the return tensors  ) ( maximum iterations : optional maximum number of iterations of the while loop to run   if provide  the cond output be and ed with an additional condition ensure the number of iterations execute be no greater than maximum iterations  ) ( return same structure : if true  output have same structure as loop vars  if eager execution be enable  this be ignore  and always treat as true   )
( fn : python function to be wrap ) ( signature : the placeholder and python arguments to be pass to the wrap function ) ( name : optional  the name of the function  )
( tensor : a tensor  ) ( dtype : a type for the return tensor  must be float16  float32  float64  int8  uint8  int16  uint16  int32  int64  complex64  complex128  bool or string   optional  ) ( name : a name for the operation  optional   ) ( optimize : if true  attempt to statically determine the shape of tensor and encode it as a constant   optional  default to true  )

( entity : python callable or class  ) ( recursive : whether to recursively convert any function that the convert function may call  ) ( arg values : deprecate  ) ( arg types : deprecate  ) ( indentation : deprecate  ) ( experimental optional features : none  a tuple of  or a single tf autograph experimental feature value  )
( entity : python callable or class to convert  ) ( recursive : whether to recursively convert any function that the convert function may call  ) ( arg values : deprecate  ) ( arg types : deprecate  ) ( experimental optional features : none  a tuple of  or a single tf autograph experimental feature value  )
( variant tensor : a dt variant tensor that represent the dataset  )
( filenames : a tf string tensor or tf data dataset contain one or more filenames  ) ( record bytes : a tf int64 scalar represent the number of bytes in each record  ) ( header bytes :  optional   a tf int64 scalar represent the number of bytes to skip at the start of a file  ) ( footer bytes :  optional   a tf int64 scalar represent the number of bytes to ignore at the end of a file  ) ( buffer size :  optional   a tf int64 scalar represent the number of bytes to buffer when read  ) ( compression type :  optional   a tf string scalar evaluate to one of ""  no compression   "zlib"  or "gzip"  ) ( num parallel reads :  optional   a tf int64 scalar represent the number of file to read in parallel  if greater than one  the record of file read in parallel be output in an interleave order  if your input pipeline be i/o bottleneck  consider set this parameter to a value greater than one to parallelize the i/o  if none  file will be read sequentially  ) ( name :  optional   a name for the tf data operation  )
( iterator resource : a tf resource scalar tf tensor represent the iterator  ) ( initializer : a tf operation that should be run to initialize this iterator  ) ( output types : a  nest  structure of tf dtype object correspond to each component of an element of this iterator  ) ( output shapes : a  nest  structure of tf tensorshape object correspond to each component of an element of this iterator  ) ( output classes : a  nest  structure of python type object correspond to each component of an element of this iterator  )
( filenames : a tf string tensor or tf data dataset contain one or more filenames  ) ( compression type :  optional   a tf string scalar evaluate to one of ""  no compression   "zlib"  or "gzip"  ) ( buffer size :  optional   a tf int64 scalar represent the number of bytes in the read buffer  if your input pipeline be i/o bottleneck  consider set this parameter to a value 1 100 mbs  if none  a sensible default for both local and remote file systems be use  ) ( num parallel reads :  optional   a tf int64 scalar represent the number of file to read in parallel  if greater than one  the record of file read in parallel be output in an interleave order  if your input pipeline be i/o bottleneck  consider set this parameter to a value greater than one to parallelize the i/o  if none  file will be read sequentially  ) ( name :  optional   a name for the tf data operation  )
( filenames : a tf data dataset whose elements be tf string scalars  a tf string tensor  or a value that can be convert to a tf string tensor  such as a list of python string   ) ( compression type :  optional   a tf string scalar evaluate to one of ""  no compression   "zlib"  or "gzip"  ) ( buffer size :  optional   a tf int64 scalar denote the number of bytes to buffer  a value of 0 result in the default buffer value choose base on the compression type  ) ( num parallel reads :  optional   a tf int64 scalar represent the number of file to read in parallel  if greater than one  the record of file read in parallel be output in an interleave order  if your input pipeline be i/o bottleneck  consider set this parameter to a value greater than one to parallelize the i/o  if none  file will be read sequentially  ) ( name :  optional   a name for the tf data operation  )
( dataset or iterator : a tf data dataset or tf data iterator  )
( dataset or iterator : a tf data dataset or tf data iterator  )
( dataset or iterator : a tf data dataset or tf data iterator  )
( dataset : a tf data dataset  ) ( shared name :  optional   if non empty  the return iterator will be share under the give name across multiple sessions that share the same devices  e g  when use a remote server   )
( dataset : a tf data dataset  )
( shapes : a list of  tensor  shape  tuples  wherein shape be the expect shape of tensor  see the example code above  the shape must be an iterable  each element of the iterable can be either a concrete integer value or a string that abstractly represent the dimension  for example    'n'  'q'  specify a 2d shape wherein the first and second dimension of shape may or may not be equal   'n'  'n'  'q'  specify a 3d shape wherein the first and second dimension be equal   1  'n'  specify a 2d shape wherein the first dimension be exactly 1 and the second dimension can be any value  note that the abstract dimension letter take effect across different tuple elements of the list  for example  tf debug assert shape   x   'n'  'a'     y   'n'  'b'    assert that both x and y be rank 2 tensors and their first dimension be equal  n   shape can also be a tf tensorshape  ) ( data : the tensors to print out if the condition be false   default to error message and first few entries of the violate tensor  ) ( summarize : print this many entries of the tensor  ) ( message : a string to prefix to the default message  ) ( name : a name for this operation  optional    default to "assert shapes"  )
( devices : a list of device string such as  '/gpu 0'  '/gpu 1'    if none  all available gpus be use  if no gpus be find  cpu be use  ) ( cross device ops : optional  a descedant of crossdeviceops  if this be not set  ncclallreduce   will be use by default   one would customize this if nccl isn't available or if a special implementation that exploit the particular hardware be available  )
( device : device string identifier for the device on which the variables should be place  see class docs for more detail on how the device be use  examples  "/cpu 0"  "/gpu 0"  "/device cpu 0"  "/device gpu 0" )
( strategy : a tf distribute strategy  ) ( replica id in sync group : an integer  a tensor or none  prefer an integer whenever possible to avoid issue with nest tf function  it accept a tensor only to be compatible with tpu replicate  )
( cluster resolver : return the cluster resolver associate with this strategy  in general  when use a multi worker tf distribute strategy such as tf distribute experimental multiworkermirroredstrategy or tf distribute tpustrategy    there be a tf distribute cluster resolver clusterresolver associate with the strategy use  and such an instance be return by this property  strategies that intend to have an associate tf distribute cluster resolver clusterresolver must set the relevant attribute  or override this property  otherwise  none be return by default  those strategies should also provide information regard what be return by this property  single worker strategies usually do not have a tf distribute cluster resolver clusterresolver  and in those case this property will return none  the tf distribute cluster resolver clusterresolver may be useful when the user need to access information such as the cluster spec  task type or task id  for example  os environ 'tf config'    json dump    'cluster'         'worker'   "localhost 12345"  "localhost 23456"        'ps'   "localhost 34567"       'task'   'type'  'worker'  'index'  0     this implicitly use tf config for the cluster and current task info strategy   tf distribute experimental multiworkermirroredstrategy     if strategy cluster resolver task type    'worker'     perform something that's only applicable on workers  since we set this    as a worker above  this block will run on this particular instance elif strategy cluster resolver task type    'ps'     perform something that's only applicable on parameter servers  since we    set this as a worker above  this block will not run on this particular    instance  for more information  please see tf distribute cluster resolver clusterresolver's api docstring  ) ( extended : tf distribute strategyextended with additional methods  ) ( num replicas in sync : return number of replicas over which gradients be aggregate  )
( experimental between graph : whether the strategy use between graph replication or not  this be expect to return a constant value that will not be change throughout its life cycle  ) ( experimental require static shapes : return true if static shape be require  false otherwise  ) ( experimental should init : whether initialization be need  ) ( parameter devices : return the tuple of all devices use to place variables  ) ( should checkpoint : whether checkpointing be need  ) ( should save summary : whether save summaries be need  ) ( worker devices : return the tuple of all devices use to for compute replica execution  )

( logits : an n d tensor represent the log odds of a 1 event  each entry in the tensor parametrizes an independent bernoulli distribution where the probability of an event be sigmoid logits   only one of logits or probs should be pass in  ) ( probs : an n d tensor represent the probability of a 1 event  each entry in the tensor parameterizes an independent bernoulli distribution  only one of logits or probs should be pass in  ) ( dtype : the type of the event sample  default  int32  ) ( validate args : python bool  default false  when true distribution parameters be check for validity despite possibly degrade runtime performance  when false invalid input may silently render incorrect output  ) ( allow nan stats : python bool  default true  when true  statistics  e g   mean  mode  variance  use the value "nan" to indicate the result be undefined  when false  an exception be raise if one or more of the statistic's batch members be undefined  ) ( name : python str name prefix to ops create by this class  )
( concentration1 : positive float point tensor indicate mean number of successes  aka "alpha"  imply self dtype and self batch shape  i e   concentration1 shape    n1  n2       nm    self batch shape  ) ( concentration0 : positive float point tensor indicate mean number of failures  aka "beta"  otherwise have same semantics as concentration1  ) ( validate args : python bool  default false  when true distribution parameters be check for validity despite possibly degrade runtime performance  when false invalid input may silently render incorrect output  ) ( allow nan stats : python bool  default true  when true  statistics  e g   mean  mode  variance  use the value "nan" to indicate the result be undefined  when false  an exception be raise if one or more of the statistic's batch members be undefined  ) ( name : python str name prefix to ops create by this class  )
( logits : an n d tensor  n >  1  represent the log probabilities of a set of categorical distributions  the first n   1 dimension index into a batch of independent distributions and the last dimension represent a vector of logits for each class  only one of logits or probs should be pass in  ) ( probs : an n d tensor  n >  1  represent the probabilities of a set of categorical distributions  the first n   1 dimension index into a batch of independent distributions and the last dimension represent a vector of probabilities for each class  only one of logits or probs should be pass in  ) ( dtype : the type of the event sample  default  int32   ) ( validate args : python bool  default false  when true distribution parameters be check for validity despite possibly degrade runtime performance  when false invalid input may silently render incorrect output  ) ( allow nan stats : python bool  default true  when true  statistics  e g   mean  mode  variance  use the value "nan" to indicate the result be undefined  when false  an exception be raise if one or more of the statistic's batch members be undefined  ) ( name : python str name prefix to ops create by this class  )
( concentration : positive float point tensor indicate mean number of class occurrences  aka "alpha"  imply self dtype  and self batch shape  self event shape  i e   if concentration shape    n1  n2       nm  k  then batch shape    n1  n2       nm  and event shape    k   ) ( validate args : python bool  default false  when true distribution parameters be check for validity despite possibly degrade runtime performance  when false invalid input may silently render incorrect output  ) ( allow nan stats : python bool  default true  when true  statistics  e g   mean  mode  variance  use the value "nan" to indicate the result be undefined  when false  an exception be raise if one or more of the statistic's batch members be undefined  ) ( name : python str name prefix to ops create by this class  )
( total count : non negative float point tensor  whose dtype be the same as concentration  the shape be broadcastable to  n1      nm  with m >  0  define this as a batch of n1 x     x nm different dirichlet multinomial distributions  its components should be equal to integer value  ) ( concentration : positive float point tensor  whose dtype be the same as n with shape broadcastable to  n1      nm  k  m >  0  define this as a batch of n1 x     x nm different k class dirichlet multinomial distributions  ) ( validate args : python bool  default false  when true distribution parameters be check for validity despite possibly degrade runtime performance  when false invalid input may silently render incorrect output  ) ( allow nan stats : python bool  default true  when true  statistics  e g   mean  mode  variance  use the value "nan" to indicate the result be undefined  when false  an exception be raise if one or more of the statistic's batch members be undefined  ) ( name : python str name prefix to ops create by this class  )
( dtype : the type of the event sample  none imply no type enforcement  ) ( reparameterization type : instance of reparameterizationtype  if distributions fully reparameterized  this distribution can be reparameterized in term of some standard distribution with a function whose jacobian be constant for the support of the standard distribution  if distributions not reparameterized  then no such reparameterization be available  ) ( validate args : python bool  default false  when true distribution parameters be check for validity despite possibly degrade runtime performance  when false invalid input may silently render incorrect output  ) ( allow nan stats : python bool  default true  when true  statistics  e g   mean  mode  variance  use the value "nan" to indicate the result be undefined  when false  an exception be raise if one or more of the statistic's batch members be undefined  ) ( parameters : python dict of parameters use to instantiate this distribution  ) ( graph parents : python list of graph prerequisites of this distribution  ) ( name : python str name prefix to ops create by this class  default  subclass name  )
( rate : float point tensor  equivalent to 1 / mean  must contain only positive value  ) ( validate args : python bool  default false  when true distribution parameters be check for validity despite possibly degrade runtime performance  when false invalid input may silently render incorrect output  ) ( allow nan stats : python bool  default true  when true  statistics  e g   mean  mode  variance  use the value "nan" to indicate the result be undefined  when false  an exception be raise if one or more of the statistic's batch members be undefined  ) ( name : python str name prefix to ops create by this class  )
( concentration : float point tensor  the concentration params of the distribution s   must contain only positive value  ) ( rate : float point tensor  the inverse scale params of the distribution s   must contain only positive value  ) ( validate args : python bool  default false  when true distribution parameters be check for validity despite possibly degrade runtime performance  when false invalid input may silently render incorrect output  ) ( allow nan stats : python bool  default true  when true  statistics  e g   mean  mode  variance  use the value "nan" to indicate the result be undefined  when false  an exception be raise if one or more of the statistic's batch members be undefined  ) ( name : python str name prefix to ops create by this class  )
( loc : float point tensor which characterize the location  center  of the distribution  ) ( scale : positive float point tensor which characterize the spread of the distribution  ) ( validate args : python bool  default false  when true distribution parameters be check for validity despite possibly degrade runtime performance  when false invalid input may silently render incorrect output  ) ( allow nan stats : python bool  default true  when true  statistics  e g   mean  mode  variance  use the value "nan" to indicate the result be undefined  when false  an exception be raise if one or more of the statistic's batch members be undefined  ) ( name : python str name prefix to ops create by this class  )
( total count : non negative float point tensor with shape broadcastable to  n1      nm  with m >  0  define this as a batch of n1 x     x nm different multinomial distributions  its components should be equal to integer value  ) ( logits : float point tensor represent unnormalized log probabilities of a positive event with shape broadcastable to  n1      nm  k  m >  0  and the same dtype as total count  define this as a batch of n1 x     x nm different k class multinomial distributions  only one of logits or probs should be pass in  ) ( probs : positive float point tensor with shape broadcastable to  n1      nm  k  m >  0 and same dtype as total count  define this as a batch of n1 x     x nm different k class multinomial distributions  probs's components in the last portion of its shape should sum to 1  only one of logits or probs should be pass in  ) ( validate args : python bool  default false  when true distribution parameters be check for validity despite possibly degrade runtime performance  when false invalid input may silently render incorrect output  ) ( allow nan stats : python bool  default true  when true  statistics  e g   mean  mode  variance  use the value "nan" to indicate the result be undefined  when false  an exception be raise if one or more of the statistic's batch members be undefined  ) ( name : python str name prefix to ops create by this class  )
( loc : float point tensor  the mean of the distribution s   ) ( scale : float point tensor  the stddevs of the distribution s   must contain only positive value  ) ( validate args : python bool  default false  when true distribution parameters be check for validity despite possibly degrade runtime performance  when false invalid input may silently render incorrect output  ) ( allow nan stats : python bool  default true  when true  statistics  e g   mean  mode  variance  use the value "nan" to indicate the result be undefined  when false  an exception be raise if one or more of the statistic's batch members be undefined  ) ( name : python str name prefix to ops create by this class  )
( dist cls a : the class of the first argument of the kl divergence  ) ( dist cls b : the class of the second argument of the kl divergence  )

( df : float point tensor  the degrees of freedom of the distribution s   df must contain only positive value  ) ( loc : float point tensor  the mean s  of the distribution s   ) ( scale : float point tensor  the scale factor s  for the distribution s   note that scale be not technically the standard deviation of this distribution but have semantics more similar to standard deviation than variance  ) ( validate args : python bool  default false  when true distribution parameters be check for validity despite possibly degrade runtime performance  when false invalid input may silently render incorrect output  ) ( allow nan stats : python bool  default true  when true  statistics  e g   mean  mode  variance  use the value "nan" to indicate the result be undefined  when false  an exception be raise if one or more of the statistic's batch members be undefined  ) ( name : python str name prefix to ops create by this class  )
( low : float point tensor  lower boundary of the output interval  must have low < high  ) ( high : float point tensor  upper boundary of the output interval  must have low < high  ) ( validate args : python bool  default false  when true distribution parameters be check for validity despite possibly degrade runtime performance  when false invalid input may silently render incorrect output  ) ( allow nan stats : python bool  default true  when true  statistics  e g   mean  mode  variance  use the value "nan" to indicate the result be undefined  when false  an exception be raise if one or more of the statistic's batch members be undefined  ) ( name : python str name prefix to ops create by this class  )
( distribution a : the first distribution  ) ( distribution b : the second distribution  ) ( allow nan stats : python bool  default true  when true  statistics  e g   mean  mode  variance  use the value "nan" to indicate the result be undefined  when false  an exception be raise if one or more of the statistic's batch members be undefined  ) ( name : python str name prefix to ops create by this class  )

( model fn : model function  follow the signature   feature    this be the first item return from the input fn pass to train  evaluate  and predict  this should be a single tf tensor or dict of same  label    this be the second item return from the input fn pass to train  evaluate  and predict  this should be a single tf tensor or dict of same  for multi head model   if mode be tf estimator modekeys predict  label none will be pass  if the model fn's signature do not accept mode  the model fn must still be able to handle label none  mode    optional  specify if this be train  evaluation or prediction  see tf estimator modekeys  params    optional dict of hyperparameters   will receive what be pass to estimator in params parameter  this allow to configure estimators from hyper parameter tune  config    optional estimator runconfig object  will receive what be pass to estimator as its config parameter  or a default value  allow set up things in your model fn base on configuration such as num ps replicas  or model dir  return    tf estimator estimatorspec ) ( model dir : directory to save model parameters  graph and etc  this can also be use to load checkpoints from the directory into an estimator to continue train a previously save model  if pathlike object  the path will be resolve  if none  the model dir in config will be use if set  if both be set  they must be same  if both be none  a temporary directory will be use  ) ( config : estimator runconfig configuration object  ) ( params : dict of hyper parameters that will be pass into model fn  key be name of parameters  value be basic python type  ) ( warm start from : optional string filepath to a checkpoint or savedmodel to warm start from  or a tf estimator warmstartsettings object to fully configure warm start   if none  only trainable variables be warm start   if the string filepath be provide instead of a tf estimator warmstartsettings  then all variables be warm start  and it be assume that vocabularies and tf tensor name be unchanged  )
( model fn : model function  follow the signature   feature    this be the first item return from the input fn pass to train  evaluate  and predict  this should be a single tf tensor or dict of same  label    this be the second item return from the input fn pass to train  evaluate  and predict  this should be a single tf tensor or dict of same  for multi head model   if mode be tf estimator modekeys predict  label none will be pass  if the model fn's signature do not accept mode  the model fn must still be able to handle label none  mode    optional  specify if this be train  evaluation or prediction  see tf estimator modekeys  params    optional dict of hyperparameters   will receive what be pass to estimator in params parameter  this allow to configure estimators from hyper parameter tune  config    optional estimator runconfig object  will receive what be pass to estimator as its config parameter  or a default value  allow set up things in your model fn base on configuration such as num ps replicas  or model dir  return    tf estimator estimatorspec ) ( model dir : directory to save model parameters  graph and etc  this can also be use to load checkpoints from the directory into an estimator to continue train a previously save model  if pathlike object  the path will be resolve  if none  the model dir in config will be use if set  if both be set  they must be same  if both be none  a temporary directory will be use  ) ( config : estimator runconfig configuration object  ) ( params : dict of hyper parameters that will be pass into model fn  key be name of parameters  value be basic python type  ) ( warm start from : optional string filepath to a checkpoint or savedmodel to warm start from  or a tf estimator warmstartsettings object to fully configure warm start   if none  only trainable variables be warm start   if the string filepath be provide instead of a tf estimator warmstartsettings  then all variables be warm start  and it be assume that vocabularies and tf tensor name be unchanged  )
( model fn : model function  follow the signature   feature    this be the first item return from the input fn pass to train  evaluate  and predict  this should be a single tf tensor or dict of same  label    this be the second item return from the input fn pass to train  evaluate  and predict  this should be a single tf tensor or dict of same  for multi head model   if mode be tf estimator modekeys predict  label none will be pass  if the model fn's signature do not accept mode  the model fn must still be able to handle label none  mode    optional  specify if this be train  evaluation or prediction  see tf estimator modekeys  params    optional dict of hyperparameters   will receive what be pass to estimator in params parameter  this allow to configure estimators from hyper parameter tune  config    optional estimator runconfig object  will receive what be pass to estimator as its config parameter  or a default value  allow set up things in your model fn base on configuration such as num ps replicas  or model dir  return    tf estimator estimatorspec ) ( model dir : directory to save model parameters  graph and etc  this can also be use to load checkpoints from the directory into an estimator to continue train a previously save model  if pathlike object  the path will be resolve  if none  the model dir in config will be use if set  if both be set  they must be same  if both be none  a temporary directory will be use  ) ( config : estimator runconfig configuration object  ) ( params : dict of hyper parameters that will be pass into model fn  key be name of parameters  value be basic python type  ) ( warm start from : optional string filepath to a checkpoint or savedmodel to warm start from  or a tf estimator warmstartsettings object to fully configure warm start   if none  only trainable variables be warm start   if the string filepath be provide instead of a tf estimator warmstartsettings  then all variables be warm start  and it be assume that vocabularies and tf tensor name be unchanged  )
( model fn : model function  follow the signature   feature    this be the first item return from the input fn pass to train  evaluate  and predict  this should be a single tf tensor or dict of same  label    this be the second item return from the input fn pass to train  evaluate  and predict  this should be a single tf tensor or dict of same  for multi head model   if mode be tf estimator modekeys predict  label none will be pass  if the model fn's signature do not accept mode  the model fn must still be able to handle label none  mode    optional  specify if this be train  evaluation or prediction  see tf estimator modekeys  params    optional dict of hyperparameters   will receive what be pass to estimator in params parameter  this allow to configure estimators from hyper parameter tune  config    optional estimator runconfig object  will receive what be pass to estimator as its config parameter  or a default value  allow set up things in your model fn base on configuration such as num ps replicas  or model dir  return    tf estimator estimatorspec ) ( model dir : directory to save model parameters  graph and etc  this can also be use to load checkpoints from the directory into an estimator to continue train a previously save model  if pathlike object  the path will be resolve  if none  the model dir in config will be use if set  if both be set  they must be same  if both be none  a temporary directory will be use  ) ( config : estimator runconfig configuration object  ) ( params : dict of hyper parameters that will be pass into model fn  key be name of parameters  value be basic python type  ) ( warm start from : optional string filepath to a checkpoint or savedmodel to warm start from  or a tf estimator warmstartsettings object to fully configure warm start   if none  only trainable variables be warm start   if the string filepath be provide instead of a tf estimator warmstartsettings  then all variables be warm start  and it be assume that vocabularies and tf tensor name be unchanged  )
( model fn : model function  follow the signature   feature    this be the first item return from the input fn pass to train  evaluate  and predict  this should be a single tf tensor or dict of same  label    this be the second item return from the input fn pass to train  evaluate  and predict  this should be a single tf tensor or dict of same  for multi head model   if mode be tf estimator modekeys predict  label none will be pass  if the model fn's signature do not accept mode  the model fn must still be able to handle label none  mode    optional  specify if this be train  evaluation or prediction  see tf estimator modekeys  params    optional dict of hyperparameters   will receive what be pass to estimator in params parameter  this allow to configure estimators from hyper parameter tune  config    optional estimator runconfig object  will receive what be pass to estimator as its config parameter  or a default value  allow set up things in your model fn base on configuration such as num ps replicas  or model dir  return    tf estimator estimatorspec ) ( model dir : directory to save model parameters  graph and etc  this can also be use to load checkpoints from the directory into an estimator to continue train a previously save model  if pathlike object  the path will be resolve  if none  the model dir in config will be use if set  if both be set  they must be same  if both be none  a temporary directory will be use  ) ( config : estimator runconfig configuration object  ) ( params : dict of hyper parameters that will be pass into model fn  key be name of parameters  value be basic python type  ) ( warm start from : optional string filepath to a checkpoint or savedmodel to warm start from  or a tf estimator warmstartsettings object to fully configure warm start   if none  only trainable variables be warm start   if the string filepath be provide instead of a tf estimator warmstartsettings  then all variables be warm start  and it be assume that vocabularies and tf tensor name be unchanged  )
( model fn : model function  follow the signature   feature    this be the first item return from the input fn pass to train  evaluate  and predict  this should be a single tf tensor or dict of same  label    this be the second item return from the input fn pass to train  evaluate  and predict  this should be a single tf tensor or dict of same  for multi head model   if mode be tf estimator modekeys predict  label none will be pass  if the model fn's signature do not accept mode  the model fn must still be able to handle label none  mode    optional  specify if this be train  evaluation or prediction  see tf estimator modekeys  params    optional dict of hyperparameters   will receive what be pass to estimator in params parameter  this allow to configure estimators from hyper parameter tune  config    optional estimator runconfig object  will receive what be pass to estimator as its config parameter  or a default value  allow set up things in your model fn base on configuration such as num ps replicas  or model dir  return    tf estimator estimatorspec ) ( model dir : directory to save model parameters  graph and etc  this can also be use to load checkpoints from the directory into an estimator to continue train a previously save model  if pathlike object  the path will be resolve  if none  the model dir in config will be use if set  if both be set  they must be same  if both be none  a temporary directory will be use  ) ( config : estimator runconfig configuration object  ) ( params : dict of hyper parameters that will be pass into model fn  key be name of parameters  value be basic python type  ) ( warm start from : optional string filepath to a checkpoint or savedmodel to warm start from  or a tf estimator warmstartsettings object to fully configure warm start   if none  only trainable variables be warm start   if the string filepath be provide instead of a tf estimator warmstartsettings  then all variables be warm start  and it be assume that vocabularies and tf tensor name be unchanged  )
( model fn : model function  follow the signature   feature    this be the first item return from the input fn pass to train  evaluate  and predict  this should be a single tf tensor or dict of same  label    this be the second item return from the input fn pass to train  evaluate  and predict  this should be a single tf tensor or dict of same  for multi head model   if mode be tf estimator modekeys predict  label none will be pass  if the model fn's signature do not accept mode  the model fn must still be able to handle label none  mode    optional  specify if this be train  evaluation or prediction  see tf estimator modekeys  params    optional dict of hyperparameters   will receive what be pass to estimator in params parameter  this allow to configure estimators from hyper parameter tune  config    optional estimator runconfig object  will receive what be pass to estimator as its config parameter  or a default value  allow set up things in your model fn base on configuration such as num ps replicas  or model dir  return    tf estimator estimatorspec ) ( model dir : directory to save model parameters  graph and etc  this can also be use to load checkpoints from the directory into an estimator to continue train a previously save model  if pathlike object  the path will be resolve  if none  the model dir in config will be use if set  if both be set  they must be same  if both be none  a temporary directory will be use  ) ( config : estimator runconfig configuration object  ) ( params : dict of hyper parameters that will be pass into model fn  key be name of parameters  value be basic python type  ) ( warm start from : optional string filepath to a checkpoint or savedmodel to warm start from  or a tf estimator warmstartsettings object to fully configure warm start   if none  only trainable variables be warm start   if the string filepath be provide instead of a tf estimator warmstartsettings  then all variables be warm start  and it be assume that vocabularies and tf tensor name be unchanged  )
( model fn : model function  follow the signature   feature    this be the first item return from the input fn pass to train  evaluate  and predict  this should be a single tf tensor or dict of same  label    this be the second item return from the input fn pass to train  evaluate  and predict  this should be a single tf tensor or dict of same  for multi head model   if mode be tf estimator modekeys predict  label none will be pass  if the model fn's signature do not accept mode  the model fn must still be able to handle label none  mode    optional  specify if this be train  evaluation or prediction  see tf estimator modekeys  params    optional dict of hyperparameters   will receive what be pass to estimator in params parameter  this allow to configure estimators from hyper parameter tune  config    optional estimator runconfig object  will receive what be pass to estimator as its config parameter  or a default value  allow set up things in your model fn base on configuration such as num ps replicas  or model dir  return    tf estimator estimatorspec ) ( model dir : directory to save model parameters  graph and etc  this can also be use to load checkpoints from the directory into an estimator to continue train a previously save model  if pathlike object  the path will be resolve  if none  the model dir in config will be use if set  if both be set  they must be same  if both be none  a temporary directory will be use  ) ( config : estimator runconfig configuration object  ) ( params : dict of hyper parameters that will be pass into model fn  key be name of parameters  value be basic python type  ) ( warm start from : optional string filepath to a checkpoint or savedmodel to warm start from  or a tf estimator warmstartsettings object to fully configure warm start   if none  only trainable variables be warm start   if the string filepath be provide instead of a tf estimator warmstartsettings  then all variables be warm start  and it be assume that vocabularies and tf tensor name be unchanged  )
( model fn : model function  follow the signature   feature    this be the first item return from the input fn pass to train  evaluate  and predict  this should be a single tf tensor or dict of same  label    this be the second item return from the input fn pass to train  evaluate  and predict  this should be a single tf tensor or dict of same  for multi head model   if mode be tf estimator modekeys predict  label none will be pass  if the model fn's signature do not accept mode  the model fn must still be able to handle label none  mode    optional  specify if this be train  evaluation or prediction  see tf estimator modekeys  params    optional dict of hyperparameters   will receive what be pass to estimator in params parameter  this allow to configure estimators from hyper parameter tune  config    optional estimator runconfig object  will receive what be pass to estimator as its config parameter  or a default value  allow set up things in your model fn base on configuration such as num ps replicas  or model dir  return    tf estimator estimatorspec ) ( model dir : directory to save model parameters  graph and etc  this can also be use to load checkpoints from the directory into an estimator to continue train a previously save model  if pathlike object  the path will be resolve  if none  the model dir in config will be use if set  if both be set  they must be same  if both be none  a temporary directory will be use  ) ( config : estimator runconfig configuration object  ) ( params : dict of hyper parameters that will be pass into model fn  key be name of parameters  value be basic python type  ) ( warm start from : optional string filepath to a checkpoint or savedmodel to warm start from  or a tf estimator warmstartsettings object to fully configure warm start   if none  only trainable variables be warm start   if the string filepath be provide instead of a tf estimator warmstartsettings  then all variables be warm start  and it be assume that vocabularies and tf tensor name be unchanged  )
( model fn : model function  follow the signature   feature    this be the first item return from the input fn pass to train  evaluate  and predict  this should be a single tf tensor or dict of same  label    this be the second item return from the input fn pass to train  evaluate  and predict  this should be a single tf tensor or dict of same  for multi head model   if mode be tf estimator modekeys predict  label none will be pass  if the model fn's signature do not accept mode  the model fn must still be able to handle label none  mode    optional  specify if this be train  evaluation or prediction  see tf estimator modekeys  params    optional dict of hyperparameters   will receive what be pass to estimator in params parameter  this allow to configure estimators from hyper parameter tune  config    optional estimator runconfig object  will receive what be pass to estimator as its config parameter  or a default value  allow set up things in your model fn base on configuration such as num ps replicas  or model dir  return    tf estimator estimatorspec ) ( model dir : directory to save model parameters  graph and etc  this can also be use to load checkpoints from the directory into an estimator to continue train a previously save model  if pathlike object  the path will be resolve  if none  the model dir in config will be use if set  if both be set  they must be same  if both be none  a temporary directory will be use  ) ( config : estimator runconfig configuration object  ) ( params : dict of hyper parameters that will be pass into model fn  key be name of parameters  value be basic python type  ) ( warm start from : optional string filepath to a checkpoint or savedmodel to warm start from  or a tf estimator warmstartsettings object to fully configure warm start   if none  only trainable variables be warm start   if the string filepath be provide instead of a tf estimator warmstartsettings  then all variables be warm start  and it be assume that vocabularies and tf tensor name be unchanged  )
( model fn : model function  follow the signature   feature    this be the first item return from the input fn pass to train  evaluate  and predict  this should be a single tf tensor or dict of same  label    this be the second item return from the input fn pass to train  evaluate  and predict  this should be a single tf tensor or dict of same  for multi head model   if mode be tf estimator modekeys predict  label none will be pass  if the model fn's signature do not accept mode  the model fn must still be able to handle label none  mode    optional  specify if this be train  evaluation or prediction  see tf estimator modekeys  params    optional dict of hyperparameters   will receive what be pass to estimator in params parameter  this allow to configure estimators from hyper parameter tune  config    optional estimator runconfig object  will receive what be pass to estimator as its config parameter  or a default value  allow set up things in your model fn base on configuration such as num ps replicas  or model dir  return    tf estimator estimatorspec ) ( model dir : directory to save model parameters  graph and etc  this can also be use to load checkpoints from the directory into an estimator to continue train a previously save model  if pathlike object  the path will be resolve  if none  the model dir in config will be use if set  if both be set  they must be same  if both be none  a temporary directory will be use  ) ( config : estimator runconfig configuration object  ) ( params : dict of hyper parameters that will be pass into model fn  key be name of parameters  value be basic python type  ) ( warm start from : optional string filepath to a checkpoint or savedmodel to warm start from  or a tf estimator warmstartsettings object to fully configure warm start   if none  only trainable variables be warm start   if the string filepath be provide instead of a tf estimator warmstartsettings  then all variables be warm start  and it be assume that vocabularies and tf tensor name be unchanged  )
( head : a  head instance construct with a method such as tf contrib estimator multi label head  ) ( feature columns : an iterable contain all the feature columns use by the model  all items in the set should be instance of class derive from featurecolumn  ) ( model dir : directory to save model parameters  graph and etc  this can also be use to load checkpoints from the directory into a estimator to continue train a previously save model  ) ( optimizer : an instance of tf optimizer use to train the model  can also be a string  one of 'adagrad'  'adam'  'ftrl'  'rmsprop'  'sgd'   or callable  default to ftrl optimizer  ) ( config : runconfig object to configure the runtime settings  ) ( partitioner : optional  partitioner for input layer  ) ( sparse combiner : a string specify how to reduce if a categorical column be multivalent   one of "mean"  "sqrtn"  and "sum"    these be effectively different ways to do example level normalization  which can be useful for bag of word feature  for more detail  see tf feature column linear model  ) ( warm start from : a string filepath to a checkpoint to warm start from  or a warmstartsettings object to fully configure warm start   if the string filepath be provide instead of a warmstartsettings  then all weight and bias be warm start  and it be assume that vocabularies and tensor name be unchanged  )
( model fn : model function  follow the signature   feature    this be the first item return from the input fn pass to train  evaluate  and predict  this should be a single tf tensor or dict of same  label    this be the second item return from the input fn pass to train  evaluate  and predict  this should be a single tf tensor or dict of same  for multi head model   if mode be tf estimator modekeys predict  label none will be pass  if the model fn's signature do not accept mode  the model fn must still be able to handle label none  mode    optional  specify if this be train  evaluation or prediction  see tf estimator modekeys  params    optional dict of hyperparameters   will receive what be pass to estimator in params parameter  this allow to configure estimators from hyper parameter tune  config    optional estimator runconfig object  will receive what be pass to estimator as its config parameter  or a default value  allow set up things in your model fn base on configuration such as num ps replicas  or model dir  return    tf estimator estimatorspec ) ( model dir : directory to save model parameters  graph and etc  this can also be use to load checkpoints from the directory into an estimator to continue train a previously save model  if pathlike object  the path will be resolve  if none  the model dir in config will be use if set  if both be set  they must be same  if both be none  a temporary directory will be use  ) ( config : estimator runconfig configuration object  ) ( params : dict of hyper parameters that will be pass into model fn  key be name of parameters  value be basic python type  ) ( warm start from : optional string filepath to a checkpoint or savedmodel to warm start from  or a tf estimator warmstartsettings object to fully configure warm start   if none  only trainable variables be warm start   if the string filepath be provide instead of a tf estimator warmstartsettings  then all variables be warm start  and it be assume that vocabularies and tf tensor name be unchanged  )
( feature columns : an iterable contain all feature columns  all items should be instance of class derive from featurecolumn  ) ( label key : a string identify the label  it mean tf example store label with this key  ) ( label dtype : a tf dtype identify the type of label  by default it be tf int64  if user define a label vocabulary  this should be set as tf string  tf float32 label be only support for binary classification  ) ( label default : use as label if label key do not exist in give tf example  an example usage  let's say label key be 'clicked' and   tf example contain click data only for positive examples in follow format key click  value 1  this mean that if there be no data with   key 'clicked' it should count as negative example by set   label deafault 0  type of this value should be compatible with   label dtype  ) ( weight column : a string or a numericcolumn create by tf feature column numeric column define feature column represent weight  it be use to down weight or boost examples during train  it will be multiply by the loss of the example  if it be a string  it be use as a key to fetch weight tensor from the feature  if it be a numericcolumn  raw tensor be fetch by key weight column key  then weight column normalizer fn be apply on it to get weight tensor  )
( feature columns : an iterable contain all feature columns  all items should be instance of class derive from  featurecolumn  ) ( label key : a string identify the label  it mean tf example store label with this key  ) ( label dtype : a tf dtype identify the type of label  by default it be tf float32  ) ( label default : use as label if label key do not exist in give tf example  by default default value be none  which mean tf parse example will error out if there be any miss label  ) ( label dimension : number of regression target per example  this be the size of the last dimension of the label and logits tensor object  typically  these have shape  batch size  label dimension    ) ( weight column : a string or a numericcolumn create by tf feature column numeric column define feature column represent weight  it be use to down weight or boost examples during train  it will be multiply by the loss of the example  if it be a string  it be use as a key to fetch weight tensor from the feature  if it be a numericcolumn  raw tensor be fetch by key weight column key  then weight column normalizer fn be apply on it to get weight tensor  )
( x : numpy array object or dict of numpy array object  if an array  the array will be treat as a single feature  ) ( y : numpy array object or dict of numpy array object  none if absent  ) ( batch size : integer  size of batch to return  ) ( num epochs : integer  number of epochs to iterate over data  if none will run forever  ) ( shuffle : boolean  if true shuffle the queue  avoid shuffle at prediction time  ) ( queue capacity : integer  size of queue to accumulate  ) ( num threads : integer  number of thread use for read and enqueueing  in order to have predict and repeatable order of read and enqueueing  such as in prediction and evaluation mode  num thread should be 1  )
( x : pandas dataframe object  ) ( y : pandas series object or dataframe  none if absent  ) ( batch size : int  size of batch to return  ) ( num epochs : int  number of epochs to iterate over data  if not none  read attempt that would exceed this value will raise outofrangeerror  ) ( shuffle : bool  whether to read the record in random order  ) ( queue capacity : int  size of the read queue  if none  it will be set roughly to the size of x  ) ( num threads : integer  number of thread use for read and enqueueing  in order to have predict and repeatable order of read and enqueueing  such as in prediction and evaluation mode  num thread should be 1  ) ( target column : str  name to give the target column y  this parameter be not use when y be a dataframe  )

( tpu config : the tpuconfig that specify tpu specific configuration  ) ( evaluation master : a string  the address of the master to use for eval  default to master if not set  ) ( master : a string  the address of the master to use for train  ) ( cluster : a clusterresolver ) (   kwargs : keyword config parameters  )
( iterations per loop : this be the number of train step run in tpu system before return to cpu host for each session run  this mean global step be increase iterations per loop time in one session run  it be recommend to be set as number of global step for next checkpoint  note that in evaluation don't use this value  instead we run total eval step on tpu for a single session run   experimental   iterations per loop can be specify as a time interval    to specify n second in one session run  one can specify it as ns   and substitute the n with the n with the number of desire second    alternatively  the unit of time can also be specify in minutes or   hours  e g  3600s or 60m or 1h  ) ( num shards :  deprecate  ignore by tpuestimator   the number of model replicas in the system  for non model parallelism case  this number equal the total number of tpu core  for model parallelism  the total number of tpu core equal num core per replica   num shards  ) ( num cores per replica : default to none  which disable model parallelism  an integer which describe the number of tpu core per model replica  this be require by model parallelism which enable partition the model to multiple core  currently num core per replica must be 1  2  4  or 8  ) ( per host input for training : if true  for per host v1  the input fn be invoke once on each host  and the number of host must be smaller or equal to the number of replicas  for per host v2  the input fn be invoke once for each host  if the number of host be less than the number of replicas  or replica  if the number of replicas be less than the number of host  with the per core input pipeline configuration  it be invoke once for each core  with a global batch size train batch size in tpuestimator constructor  the batch size for each shard be train batch size //  host in the true or per host v1 mode  in per host v2 mode  it be train batch size //  core  in broadcast mode  input fn be only invoke once on host 0 and the tensors be broadcast to all other replicas  the batch size equal to train batch size  with the per core input pipeline configuration  the shard batch size be also train batch size //  core  note  per host input for train  per shard v1 only support mode train  ) ( tpu job name : the name of the tpu job  typically  this name be auto infer within tpuestimator  however when use clusterspec propagation in more esoteric cluster configurations  you may need to specify the job name as a string  ) ( initial infeed sleep secs : the number of second the infeed thread should wait before enqueueing the first batch  this help avoid timeouts for model that require a long compilation time  ) ( input partition dims : a nest list to describe the partition dim for all the tensors from input fn    the structure of input partition dim must match the structure of feature and label from input fn    the total number of partition must match num core per replica  for example  if input fn   return two tensors    image with shape  n  h  w  c  and label  n   input partition dim       1  2  2  1   none  will split the image to 4 piece and fee into 4   tpu core  label tensor be directly broadcast to all the tpu core   since the partition dim be none  current limitations  this feature be only support with the per host v2   input mode  ) ( eval training input configuration : if slice  input fn be only invoke once on host 0 and the tensors be broadcast to all other replicas  unlike per host input for train broadcast  each replica will only get a slice of the data instead of a whole copy  if per host v1  the behaviour be determine by per host input for train  ) ( experimental host call every n steps : within a train loop  this argument set how often host call be perform during train  host call will be evaluate every n step within a train loop where n be the value of this argument  ) ( experimental allow per host v2 parallel get next : when enable  allow concurrent execution of dataset get next call when use per host v2 input  may result in a performance increase for model with a small step time  but as a consequence tpuestimator may non deterministically distribute batch to different core  rather than guarantee round robin behavior  ) ( experimental feed hook : this be a class which user can provide to the tpu estimator to override the default tpuinfeedoutfeedsessionhook implementation and add customize implementatioin to handle infeed outfeed logic  if give class be none  tpu estimator use default tpuinfeedoutfeedsessionhook implementation in tpu estimator py  if not none  tpu estimator use this customize tpu infeed outfeed session hook class rather to override the default one  )
( model fn : model function as require by estimator which return estimatorspec or tpuestimatorspec  train hook  'evaluation hooks'  and prediction hook must not capure any tpu tensor inside the model fn  ) ( model dir : directory to save model parameters  graph and etc  this can also be use to load checkpoints from the directory into a estimator to continue train a previously save model  if none  the model dir in config will be use if set  if both be set  they must be same  if both be none  a temporary directory will be use  ) ( config : an tpu config runconfig configuration object  cannot be none  ) ( params : an optional dict of hyper parameters that will be pass into input fn and model fn   key be name of parameters  value be basic python type  there be reserve key for tpuestimator  include 'batch size'  ) ( use tpu : a bool indicate whether tpu support be enable  currently    tpu train and evaluation respect this bite  but eval on tpu can override execution of eval  see below  ) ( train batch size : an int represent the global train batch size  tpuestimator transform this global batch size to a per shard batch size  as params 'batch size'   when call input fn and model fn  cannot be none if use tpu be true  must be divisible by total number of replicas  ) ( eval batch size : an int represent evaluation batch size  must be divisible by total number of replicas  ) ( predict batch size : an int represent the prediction batch size  must be divisible by total number of replicas  ) ( batch axis : a python tuple of int value describe how each tensor produce by the estimator input fn should be split across the tpu compute shards  for example  if your input fn produce  image  label  where the image tensor be in hwcn format  your shard dimension would be  3  0   where 3 correspond to the n dimension of your image tensor  and 0 correspond to the dimension along which to split the label to match up with the correspond image  if none be supply  and per host input for train be true  batch will be sharded base on the major dimension  if tpu config per host input for train be false or per host v2  batch axis be ignore  ) ( eval on tpu : if false  evaluation run on cpu or gpu  in this case  the model fn must return estimatorspec when call with mode as eval  ) ( export to tpu : if true  export save model   export a metagraph for serve on tpu  note that unsupported export modes such as eval will be ignore  for those modes  only a cpu model will be export  currently  export to tpu only support predict  ) ( export to cpu : if true  export save model   export a metagraph for serve on cpu  ) ( warm start from : optional string filepath to a checkpoint or savedmodel to warm start from  or a tf estimator warmstartsettings object to fully configure warm start   if the string filepath be provide instead of a warmstartsettings  then all variables be warm start  and it be assume that vocabularies and tensor name be unchanged  ) ( embedding config spec : optional embeddingconfigspec instance to support use tpu embed  ) ( export saved model api version : an integer  1 or 2  1 correspond to v1  2 correspond to v2   default to v1   with v1  export save model   add rewrite   and tpupartitionedcallop   for user  while in v2  user be expect to add rewrite    tpupartitionedcallop   etc in their model fn  )
( mode : a namedtuple alias for field number 0 ) ( predictions : a namedtuple alias for field number 1 ) ( loss : a namedtuple alias for field number 2 ) ( train op : a namedtuple alias for field number 3 ) ( eval metrics : a namedtuple alias for field number 4 ) ( export outputs : a namedtuple alias for field number 5 ) ( scaffold fn : a namedtuple alias for field number 6 ) ( host call : a namedtuple alias for field number 7 ) ( training hooks : a namedtuple alias for field number 8 ) ( evaluation hooks : a namedtuple alias for field number 9 ) ( prediction hooks : a namedtuple alias for field number 10 )
( key : a unique string identify the input feature  it be use as the column name and the dictionary key for feature parse configs  feature tensor object  and feature columns  ) ( vocabulary file : the vocabulary file name  ) ( vocabulary size : number of the elements in the vocabulary  this must be no greater than length of vocabulary file  if less than length  later value be ignore  if none  it be set to the length of vocabulary file  ) ( num oov buckets : non negative integer  the number of out of vocabulary bucket  all out of vocabulary input will be assign ids in the range  vocabulary size  vocabulary size num oov bucket  base on a hash of the input value  a positive num oov bucket can not be specify with default value  ) ( default value : the integer id value to return for out of vocabulary feature value  default to  1  this can not be specify with a positive num oov bucket  ) ( dtype : the type of feature  only string and integer type be support  )
( features : a map from key to tensors   featurecolumns look up via these key  for example numeric column 'price'  will look at 'price' key in this dict  value can be a sparsetensor or a tensor depend on correspond  featurecolumn  ) ( feature columns : an iterable contain the featurecolumns to use as input to your model  all items should be instance of class derive from  densecolumn such as numeric column  embed column  bucketized column  indicator column  if you have categorical feature  you can wrap them with an embed column or indicator column  ) ( weight collections : a list of collection name to which the variable will be add  note that variables will also be add to collections tf graphkeys global variables and ops graphkeys model variables  ) ( trainable : if true also add the variable to the graph collection graphkeys trainable variables  see tf variable   ) ( cols to vars : if not none  must be a dictionary that will be fill with a map from  featurecolumn to list of variables   for example  after the call  we might have cols to vars     embeddingcolumn    categorical column  hashedcategoricalcolumn      key 'sparse feature'  hash bucket size 5  dtype tf string     dimension 10     ) ( cols to output tensors : if not none  must be a dictionary that will be fill with a map from ' featurecolumn' to the associate output tensors  )
( features : a map from key to tensors   featurecolumns look up via these key  for example numeric column 'price'  will look at 'price' key in this dict  value be tensor or sparsetensor depend on correspond  featurecolumn  ) ( feature columns : an iterable contain the featurecolumns to use as input to your model  all items should be instance of class derive from  featurecolumns  ) ( units : an integer  dimensionality of the output space  default value be 1  ) ( sparse combiner : a string specify how to reduce if a categorical column be multivalent  except numeric column  almost all columns pass to linear model be consider as categorical columns   it combine each categorical column independently  currently "mean"  "sqrtn" and "sum" be support  with "sum" the default for linear model  "sqrtn" often achieve good accuracy  in particular with bag of word columns   "sum"  do not normalize feature in the column "mean"  do l1 normalization on feature in the column "sqrtn"  do l2 normalization on feature in the column ) ( weight collections : a list of collection name to which the variable will be add  note that  variables will also be add to collections tf graphkeys global variables and ops graphkeys model variables  ) ( trainable : if true also add the variable to the graph collection graphkeys trainable variables  see tf variable   ) ( cols to vars : if not none  must be a dictionary that will be fill with a map from  featurecolumn to associate list of variables   for example  after the call  we might have cols to vars      numericcolumn  key 'numeric feature1'  shape  1        'bias'       numericcolumn  key 'numeric feature2'  shape  2         if a column create no variables  its value will be an empty list  note that cols to vars will also contain a string key 'bias' that map to a list of variables  )
( feature columns : an iterable contain all feature columns  all items should be instance of class derive from  featurecolumn  )
( categorical columns : list of categorical columns create by a categorical column with   function  these columns produce the sparse ids that be input to the embed lookup  all columns must be of the same type and have the same arguments except key  e g  they can be categorical column with vocabulary file with the same vocabulary file  some or all columns could also be weight categorical column  ) ( dimension : an integer specify dimension of the embed  must be > 0  ) ( combiner : a string specify how to reduce if there be multiple entries in a single row  currently 'mean'  'sqrtn' and 'sum' be support  with 'mean' the default  'sqrtn' often achieve good accuracy  in particular with bag of word columns  each of this can be think as example level normalizations on the column  for more information  see tf embed lookup sparse  ) ( initializer : a variable initializer function to be use in embed variable initialization  if not specify  default to truncate normal initializer with mean 0 0 and standard deviation 1/sqrt dimension   ) ( shared embedding collection name : optional name of the collection where share embed weight be add  if not give  a reasonable name will be choose base on the name of categorical columns  this be also use in variable scope when create share embed weight  ) ( ckpt to load from : string represent checkpoint name/pattern from which to restore column weight  require if tensor name in ckpt be not none  ) ( tensor name in ckpt : name of the tensor in ckpt to load from from which to restore the column weight  require if ckpt to load from be not none  ) ( max norm : if not none  each embed be clip if its l2 norm be larger than this value  before combine  ) ( trainable : whether or not the embed be trainable  default be true  ) ( use safe embedding lookup : if true  use safe embed lookup sparse instead of embed lookup sparse  safe embed lookup sparse ensure there be no empty row and all weight and ids be positive at the expense of extra compute cost  this only apply to rank 2  nxm  shape input tensors  default to true  consider turn off if the above check be not need  note that have empty row will not trigger any error though the output result might be 0 or omit  )
( src : string  name of the file whose content need to be copy ) ( dst : string  name of the file to which to copy to ) ( overwrite : boolean  if false it's an error for dst to be occupy by an exist file  )
( dirname : string  a path to a directory )
( path : string  a path )
( mode : return the mode in which the file be open  ) ( name : return the file name  )
( filename : string or iterable of string  the glob pattern s   )
( dirname : string  path to a potential directory )
( dirname : string  path to a directory )
( dirname : string  name of the directory to be create )
( dirname : string  name of the directory to be create )
( filename : string  a filename )
( oldname : string  pathname for a file ) ( newname : string  pathname to which the file need to be move ) ( overwrite : boolean  if false it's an error for newname to be occupy by an exist file  )
( filename : string  path to a file )
( top : string  a directory name ) ( in order : bool  traverse in order if true  post order if false   errors that happen while list directories be ignore  )
( sess : active tensorflow session contain the variables  ) ( input graph def : graphdef object hold the network  ) ( output node names : list of name string for the result nod of the graph  ) ( variable names whitelist : the set of variable name to convert  by default  all variables be convert   ) ( variable names blacklist : the set of variable name to omit convert to constants  )
( graph def : a graph pb2 graphdef proto  ) ( dest nodes : an iterable of string specify the destination node name  )
( node : the node to be assign to a device  could be either an ops operation or nodedef  ) ( pin variables on cpu : if true  this function will return false if node def represent a variable relate op  )
( input graph : model to analyze and prune  ) ( protected nodes : an optional list of name of nod to be keep unconditionally  this be for example useful to preserve identity output nod  )


( image : a tensor  must be one of the follow type  uint8  uint16  int8  int16  int32  int64  half  float32  float64  a 4 d tensor of shape  batch  image height  image width  depth   both image height and image width need to be positive  ) ( boxes : a tensor of type float32  a 2 d tensor of shape  num box  4   the i th row of the tensor specify the coordinate of a box in the box ind i  image and be specify in normalize coordinate  y1  x1  y2  x2   a normalize coordinate value of y be map to the image coordinate at y    image height   1   so as the  0  1  interval of normalize image height be map to  0  image height   1  in image height coordinate  we do allow y1 > y2  in which case the sample crop be an up down flip version of the original image  the width dimension be treat similarly  normalize coordinate outside the  0  1  range be allow  in which case we use extrapolation value to extrapolate the input image value  ) ( box ind : a tensor of type int32  a 1 d tensor of shape  num box  with int32 value in  0  batch   the value of box ind i  specify the image that the i th box refer to  ) ( crop size : a tensor of type int32  a 1 d tensor of 2 elements  size    crop height  crop width   all crop image patch be resize to this size  the aspect ratio of the image content be not preserve  both crop height and crop width need to be positive  ) ( method : an optional string from  "bilinear"  "nearest"  default to "bilinear"  a string specify the sample method for resize  it can be either "bilinear" or "nearest" and default to "bilinear"  currently two sample methods be support  bilinear and nearest neighbor  ) ( extrapolation value : an optional float  default to 0  value use for extrapolation  when applicable  ) ( name : a name for the operation  optional   )
( images : a tensor  must be one of the follow type  float32  half  4 d with shape  batch  height  width  depth   a batch of image  ) ( boxes : a tensor of type float32  3 d with shape  batch  num bound box  4  contain bound box  ) ( name : a name for the operation  optional   ) ( colors : a tensor of type float32  2 d  a list of rgba color to cycle through for the box  )
( input : a tensor of type float32  a 4 d float tensor of shape  batch size  height  width  channel   ) ( size : a tensor of type int32  a 1 d tensor of 2 elements contain the size of the glimpse to extract   the glimpse height must be specify first  follow by the glimpse width  ) ( offsets : a tensor of type float32  a 2 d integer tensor of shape  batch size  2  contain the y  x locations of the center of each window  ) ( centered : an optional bool  default to true  indicate if the offset coordinate be center relative to the image  in which case the  0  0  offset be relative to the center of the input image  if false  the  0 0  offset correspond to the upper leave corner of the input image  ) ( normalized : an optional bool  default to true  indicate if the offset coordinate be normalize  ) ( uniform noise : an optional bool  default to true  indicate if the noise should be generate use a uniform distribution or a gaussian distribution  ) ( name : a name for the operation  optional   )
( images : 4 d tensor of shape  batch  height  width  channel  or 3 d tensor of shape  height  width  channel   ) ( size : a 1 d int32 tensor of 2 elements  new height  new width   the new size for the image  ) ( method : resizemethod   default to tf image resizemethod bilinear  ) ( align corners : bool   if true  the center of the 4 corner pixels of the input and output tensors be align  preserve the value at the corner pixels  default to false  ) ( preserve aspect ratio : whether to preserve the aspect ratio  if this be set  then image will be resize to a size that fit in size while preserve the aspect ratio of the original image  scale up the image if size be bigger than the current size of the image  default to false  ) ( name : a name for this operation  optional   )
( images : a tensor  must be one of the follow type  int8  uint8  int16  uint16  int32  int64  half  float32  float64  bfloat16  4 d with shape  batch  height  width  channel   ) ( size : a 1 d int32 tensor of 2 elements  new height  new width   the new size for the image  ) ( align corners : an optional bool  default to false  if true  the center of the 4 corner pixels of the input and output tensors be align  preserve the value at the corner pixels  default to false  ) ( name : a name for the operation  optional   )
( image : 4 d tensor of shape  batch  height  width  channel  or 3 d tensor of shape  height  width  channel   ) ( target height : target height  ) ( target width : target width  ) ( method : method to use for resize image  see resize image   ) ( align corners : bool   if true  the center of the 4 corner pixels of the input and output tensors be align  preserve the value at the corner pixels  default to false  )
( image size : a tensor  must be one of the follow type  uint8  int8  int16  int32  int64  1 d  contain  height  width  channel   ) ( bounding boxes : a tensor of type float32  3 d with shape  batch  n  4  describe the n bound box associate with the image  ) ( seed : an optional int  default to 0  if either seed or seed2 be set to non zero  the random number generator be seed by the give seed   otherwise  it be seed by a random seed  ) ( seed2 : an optional int  default to 0  a second seed to avoid seed collision  ) ( min object covered : a tensor of type float32  default to 0 1  the crop area of the image must contain at least this fraction of any bound box supply  the value of this parameter should be non negative  in the case of 0  the crop area do not need to overlap any of the bound box supply  ) ( aspect ratio range : an optional list of float  default to  0 75  1 33   the crop area of the image must have an aspect ratio   width / height within this range  ) ( area range : an optional list of float  default to  0 05  1   the crop area of the image must contain a fraction of the supply image within this range  ) ( max attempts : an optional int  default to 100  number of attempt at generate a crop region of the image of the specify constraints  after max attempt failures  return the entire image  ) ( use image if no bounding boxes : an optional bool  default to false  control behavior if no bound box supply  if true  assume an implicit bound box cover the whole input  if false  raise an error  ) ( name : a name for the operation  optional   )
( seed : a python integer  use to seed the random generator  )
( seed : a python integer  use to seed the random generator  )
( seed : a python integer  use to seed the random generator  )
( seed : a python integer  use to seed the random generator  )

( op input list : an option sequence of tensors or ops  which will be use to determine the current graph  otherwise the default graph will be use  )
( name : the name argument that be pass to the op function  ) ( default name : the default name to use if the name argument be none  ) ( values : the list of tensor arguments that be pass to the op function  )
( session : a tf session  )
( log dir : the path of the directory where to save the log file to be parse by tensorboard  ) ( histogram freq : frequency  in epochs  at which to compute activation and weight histograms for the layer of the model  if set to 0  histograms won't be compute  validation data  or split  must be specify for histogram visualizations  ) ( write graph : whether to visualize the graph in tensorboard  the log file can become quite large when write graph be set to true  ) ( write grads : whether to visualize gradient histograms in tensorboard  histogram freq must be greater than 0  ) ( batch size : size of batch of input to fee to the network for histograms computation  ) ( write images : whether to write model weight to visualize as image in tensorboard  ) ( embeddings freq : frequency  in epochs  at which select embed layer will be save  if set to 0  embeddings won't be compute  data to be visualize in tensorboard's embed tab must be pass as embeddings data  ) ( embeddings layer names : a list of name of layer to keep eye on  if none or empty list all the embed layer will be watch  ) ( embeddings metadata : a dictionary which map layer name to a file name in which metadata for this embed layer be save    here be detail     about metadata file format  in case if the same metadata file be     use for all embed layer  string can be pass  ) ( embeddings data : data to be embed at layer specify in embeddings layer name  numpy array  if the model have a single input  or list of numpy array  if the model have multiple input   learn more about embeddings in this guide  ) ( update freq : 'batch' or 'epoch' or integer  when use 'batch'  write the losses and metrics to tensorboard after each batch  the same apply for 'epoch'  if use an integer  let's say 1000  the callback will write the metrics and losses to tensorboard every 1000 sample  note that write too frequently to tensorboard can slow down your train  ) ( profile batch : profile the batch to sample compute characteristics  by default  it will profile the second batch  set profile batch 0 to disable profile  )
( keras model : a compile keras model object  this argument be mutually exclusive with keras model path  estimator's model fn use the structure of the model to clone the model  default to none  ) ( keras model path : path to a compile keras model save on disk  in hdf5 format  which can be generate with the save   method of a keras model  this argument be mutually exclusive with keras model  default to none  ) ( custom objects : dictionary for clone customize object  this be use with class that be not part of this pip package  for example  if user maintain a relu6 class that inherit from tf keras layer layer  then pass custom object  'relu6'  relu6   default to none  ) ( model dir : directory to save estimator model parameters  graph  summary file for tensorboard  etc  if unset a directory will be create with tempfile mkdtemp ) ( config : runconfig to config estimator  allow set up things in model fn base on configuration such as num ps replicas  or model dir  default to none  if both config model dir and the model dir argument  above  be specify the model dir argument take precedence  ) ( checkpoint format : set the format of the checkpoint save by the estimator when train  may be saver or checkpoint  depend on whether to save checkpoints from tf train saver or tf train checkpoint  this argument currently default to saver  when 2 0 be release  the default will be checkpoint  estimators use name base tf train saver checkpoints  while keras model use object base checkpoints from tf train checkpoint  currently  save object base checkpoints from model to estimator be only support by functional and sequential model  default to 'saver'  ) ( metric names map : optional dictionary map keras model output metric name to custom name  this can be use to override the default keras model output metrics name in a multi io model use case and provide custom name for the eval metric ops in estimator  the keras model metric name can be obtain use model metrics name exclude any loss metrics such as total loss and output losses  for example  if your keras model have two output out 1 and out 2  with mse loss and acc metric  then model metrics name will be  'loss'  'out 1 loss'  'out 2 loss'  'out 1 acc'  'out 2 acc'   the model metric name exclude the loss metrics will be  'out 1 acc'  'out 2 acc'   ) ( export outputs : optional dictionary  this can be use to override the default keras model output export in a multi io model use case and provide custom name for the export output in tf estimator estimatorspec  default be none  which be equivalent to  'serving default'  tf estimator export predictoutput   if not none  the key must match the key of model output name  a dict  name  output  where   name  an arbitrary name for this output  output  an exportoutput class such as classificationoutput  regressionoutput  or predictoutput  single head model only need to specify one entry in this dictionary  multi head model should specify one entry for each head  one of which must be name use tf save model signature constants default serve signature def key if no entry be provide  a default predictoutput map to predictions will be create  )
( value : a python scalar  list or tuple of value  or a n dimensional numpy array  all elements of the initialize variable will be set to the correspond value in the value argument  ) ( dtype : default data type  use if no dtype argument be provide when call the initializer  ) ( verify shape : boolean that enable verification of the shape of value  if true  the initializer will throw an error if the shape of value be not compatible with the shape of the initialize tensor  )
( gain : multiplicative factor to apply to the identity matrix  ) ( dtype : default data type  use if no dtype argument be provide when call the initializer  only float point type be support  )

( gain : multiplicative factor to apply to the orthogonal matrix ) ( seed : a python integer  use to create random seed  see tf compat v1 set random seed for behavior  ) ( dtype : default data type  use if no dtype argument be provide when call the initializer  only float point type be support  )
( mean : a python scalar or a scalar tensor  mean of the random value to generate  ) ( stddev : a python scalar or a scalar tensor  standard deviation of the random value to generate  ) ( seed : a python integer  use to create random seed  see tf compat v1 set random seed for behavior  ) ( dtype : default data type  use if no dtype argument be provide when call the initializer  only float point type be support  )
( minval : a python scalar or a scalar tensor  lower bind of the range of random value to generate  ) ( maxval : a python scalar or a scalar tensor  upper bind of the range of random value to generate   default to 1 for float type  ) ( seed : a python integer  use to create random seed  see tf compat v1 set random seed for behavior  ) ( dtype : default data type  use if no dtype argument be provide when call the initializer  )
( mean : a python scalar or a scalar tensor  mean of the random value to generate  ) ( stddev : a python scalar or a scalar tensor  standard deviation of the random value to generate  ) ( seed : a python integer  use to create random seed  see tf compat v1 set random seed for behavior  ) ( dtype : default data type  use if no dtype argument be provide when call the initializer  only float point type be support  )
( scale : scale factor  positive float   ) ( mode : one of "fan in"  "fan out"  "fan avg"  ) ( distribution : random distribution to use  one of "normal"  "uniform"  ) ( seed : a python integer  use to create random seed  see tf compat v1 set random seed for behavior  ) ( dtype : default data type  use if no dtype argument be provide when call the initializer  only float point type be support  )

( seed : a python integer  use to create random seed  see tf compat v1 set random seed for behavior  ) ( dtype : default data type  use if no dtype argument be provide when call the initializer  only float point type be support  )
( seed : a python integer  use to create random seed  see tf compat v1 set random seed for behavior  ) ( dtype : default data type  use if no dtype argument be provide when call the initializer  only float point type be support  )
( scale : scale factor  positive float   ) ( mode : one of "fan in"  "fan out"  "fan avg"  ) ( distribution : random distribution to use  one of "normal"  "uniform"  ) ( seed : a python integer  use to create random seed  see tf compat v1 set random seed for behavior  ) ( dtype : default data type  use if no dtype argument be provide when call the initializer  only float point type be support  )
( scale : scale factor  positive float   ) ( mode : one of "fan in"  "fan out"  "fan avg"  ) ( distribution : random distribution to use  one of "normal"  "uniform"  ) ( seed : a python integer  use to create random seed  see tf compat v1 set random seed for behavior  ) ( dtype : default data type  use if no dtype argument be provide when call the initializer  only float point type be support  )
( scale : scale factor  positive float   ) ( mode : one of "fan in"  "fan out"  "fan avg"  ) ( distribution : random distribution to use  one of "normal"  "uniform"  ) ( seed : a python integer  use to create random seed  see tf compat v1 set random seed for behavior  ) ( dtype : default data type  use if no dtype argument be provide when call the initializer  only float point type be support  )
( scale : scale factor  positive float   ) ( mode : one of "fan in"  "fan out"  "fan avg"  ) ( distribution : random distribution to use  one of "normal"  "uniform"  ) ( seed : a python integer  use to create random seed  see tf compat v1 set random seed for behavior  ) ( dtype : default data type  use if no dtype argument be provide when call the initializer  only float point type be support  )
( axis : integer or a list of integers  the axis that should be normalize  typically the feature axis   for instance  after a conv2d layer with data format "channels first"  set axis 1 in batchnormalization  ) ( momentum : momentum for the move average  ) ( epsilon : small float add to variance to avoid divide by zero  ) ( center : if true  add offset of beta to normalize tensor  if false  beta be ignore  ) ( scale : if true  multiply by gamma  if false  gamma be not use  when the next layer be linear  also e g  nn relu   this can be disable since the scale will be do by the next layer  ) ( beta initializer : initializer for the beta weight  ) ( gamma initializer : initializer for the gamma weight  ) ( moving mean initializer : initializer for the move mean  ) ( moving variance initializer : initializer for the move variance  ) ( beta regularizer : optional regularizer for the beta weight  ) ( gamma regularizer : optional regularizer for the gamma weight  ) ( beta constraint : optional constraint for the beta weight  ) ( gamma constraint : optional constraint for the gamma weight  ) ( renorm : whether to use batch renormalization  this add extra variables during   train  the inference be the same for either value of this parameter  ) ( renorm clipping : a dictionary that may map key 'rmax'  'rmin'  'dmax' to scalar tensors use to clip the renorm correction  the correction  r  d  be use as correct value   normalize value   r   d  with r clip to  rmin  rmax   and d to   dmax  dmax   miss rmax  rmin  dmax be set to inf  0  inf  respectively  ) ( renorm momentum : momentum use to update the move mean and standard deviations with renorm  unlike momentum  this affect train and should be neither too small  which would add noise  nor too large  which would give stale estimate   note that momentum be still apply to get the mean and variances for inference  ) ( fused : if true  use a faster  fuse implementation  or raise a valueerror if the fuse implementation cannot be use  if none  use the faster implementation if possible  if false  do not use the fuse implementation  note that in tensorflow 1 x  the mean of fuse true be different  if   false  the layer use the system recommend implementation  ) ( trainable : boolean  if true the variables will be mark as trainable  ) ( virtual batch size : an int  by default  virtual batch size be none  which mean batch normalization be perform across the whole batch  when virtual batch size be not none  instead perform "ghost batch normalization"  which create virtual sub batch which be each normalize separately  with share gamma  beta  and move statistics   must divide the actual batch size during execution  ) ( adjustment : a function take the tensor contain the  dynamic  shape of the input tensor and return a pair  scale  bias  to apply to the normalize value  before gamma and beta   only during train  for example  if axis  1    adjustment   lambda shape        tf random uniform shape  1    0 93  1 07       tf random uniform shape  1     0 1  0 1   will scale the normalize       value by up to 7  up or down  then shift the result by up to 0 1        with independent scale and bias for each feature but share       across all examples   and finally apply gamma and/or beta  if       none  no adjustment be apply  cannot be specify if       virtual batch size be specify  )
( units : positive integer  dimensionality of the output space  ) ( kernel initializer : initializer for the kernel weight matrix  use for the linear transformation of the input  ) ( recurrent initializer : initializer for the recurrent kernel weight matrix  use for the linear transformation of the recurrent state  ) ( bias initializer : initializer for the bias vector  ) ( kernel regularizer : regularizer function apply to the kernel weight matrix  ) ( recurrent regularizer : regularizer function apply to the recurrent kernel weight matrix  ) ( bias regularizer : regularizer function apply to the bias vector  ) ( activity regularizer : regularizer function apply to the output of the layer  its "activation"   ) ( kernel constraint : constraint function apply to the kernel weight matrix  ) ( recurrent constraint : constraint function apply to the recurrent kernel weight matrix  ) ( bias constraint : constraint function apply to the bias vector  ) ( return sequences : boolean  whether to return the last output in the output sequence  or the full sequence  ) ( return state : boolean  whether to return the last state in addition to the output  ) ( go backwards : boolean  default false   if true  process the input sequence backwards and return the reverse sequence  ) ( stateful : boolean  default false   if true  the last state for each sample at index i in a batch will be use as initial state for the sample of index i in the follow batch  )
( units : positive integer  dimensionality of the output space  ) ( kernel initializer : initializer for the kernel weight matrix  use for the linear transformation of the input  ) ( unit forget bias : boolean  if true  add 1 to the bias of the forget gate at initialization  set it to true will also force bias initializer "zeros"  this be recommend in jozefowicz et al  ) ( recurrent initializer : initializer for the recurrent kernel weight matrix  use for the linear transformation of the recurrent state  ) ( bias initializer : initializer for the bias vector  ) ( kernel regularizer : regularizer function apply to the kernel weight matrix  ) ( recurrent regularizer : regularizer function apply to the recurrent kernel weight matrix  ) ( bias regularizer : regularizer function apply to the bias vector  ) ( activity regularizer : regularizer function apply to the output of the layer  its "activation"   ) ( kernel constraint : constraint function apply to the kernel weight matrix  ) ( recurrent constraint : constraint function apply to the recurrent kernel weight matrix  ) ( bias constraint : constraint function apply to the bias vector  ) ( return sequences : boolean  whether to return the last output  in the output sequence  or the full sequence  ) ( return state : boolean  whether to return the last state in addition to the output  ) ( go backwards : boolean  default false   if true  process the input sequence backwards and return the reverse sequence  ) ( stateful : boolean  default false   if true  the last state for each sample at index i in a batch will be use as initial state for the sample of index i in the follow batch  )
( feature columns : an iterable contain the featurecolumns to use as input to your model  all items should be instance of class derive from densecolumn such as numeric column  embed column  bucketized column  indicator column  if you have categorical feature  you can wrap them with an embed column or indicator column  ) ( trainable : boolean  whether the layer's variables will be update via gradient descent during train  ) ( name : name to give to the densefeatures  ) ( partitioner : partitioner for input layer  default to none  ) (   kwargs : keyword arguments to construct a layer  )
( units : positive integer  dimensionality of the output space  ) ( activation : activation function to use  default  hyperbolic tangent  tanh   if you pass none  no activation be apply  ie  "linear" activation  a x    x   ) ( recurrent activation : activation function to use for the recurrent step  default  hard sigmoid  hard sigmoid   if you pass none  no activation be apply  ie  "linear" activation  a x    x   ) ( use bias : boolean  whether the layer use a bias vector  ) ( kernel initializer : initializer for the kernel weight matrix  use for the linear transformation of the input  ) ( recurrent initializer : initializer for the recurrent kernel weight matrix  use for the linear transformation of the recurrent state  ) ( bias initializer : initializer for the bias vector  ) ( kernel regularizer : regularizer function apply to the kernel weight matrix  ) ( recurrent regularizer : regularizer function apply to the recurrent kernel weight matrix  ) ( bias regularizer : regularizer function apply to the bias vector  ) ( activity regularizer : regularizer function apply to the output of the layer  its "activation"    ) ( kernel constraint : constraint function apply to the kernel weight matrix  ) ( recurrent constraint : constraint function apply to the recurrent kernel weight matrix  ) ( bias constraint : constraint function apply to the bias vector  ) ( dropout : float between 0 and 1  fraction of the units to drop for the linear transformation of the input  ) ( recurrent dropout : float between 0 and 1  fraction of the units to drop for the linear transformation of the recurrent state  ) ( return sequences : boolean  whether to return the last output in the output sequence  or the full sequence  ) ( return state : boolean  whether to return the last state in addition to the output  ) ( go backwards : boolean  default false   if true  process the input sequence backwards and return the reverse sequence  ) ( stateful : boolean  default false   if true  the last state for each sample at index i in a batch will be use as initial state for the sample of index i in the follow batch  ) ( unroll : boolean  default false   if true  the network will be unroll  else a symbolic loop will be use  unroll can speed up a rnn  although it tend to be more memory intensive  unroll be only suitable for short sequence  ) ( time major : the shape format of the input and output tensors  if true  the input and output will be in shape  timesteps  batch        whereas in the false case  it will be  batch  timesteps        use time major   true be a bite more efficient because it avoid transpose at the begin and end of the rnn calculation  however  most tensorflow data be batch major  so by default this function accept input and emit output in batch major form  ) ( reset after : gru convention  whether to apply reset gate after or before matrix multiplication   false   "before"  default   true   "after"  cudnn compatible   )
( units : positive integer  dimensionality of the output space  ) ( activation : activation function to use  default  hyperbolic tangent  tanh   if you pass none  no activation be apply  ie  "linear" activation  a x    x   ) ( recurrent activation : activation function to use for the recurrent step  default  hard sigmoid  hard sigmoid   if you pass none  no activation be apply  ie  "linear" activation  a x    x   ) ( use bias : boolean  whether the layer use a bias vector  ) ( kernel initializer : initializer for the kernel weight matrix  use for the linear transformation of the input  ) ( recurrent initializer : initializer for the recurrent kernel weight matrix  use for the linear transformation of the recurrent state  ) ( bias initializer : initializer for the bias vector  ) ( kernel regularizer : regularizer function apply to the kernel weight matrix  ) ( recurrent regularizer : regularizer function apply to the recurrent kernel weight matrix  ) ( bias regularizer : regularizer function apply to the bias vector  ) ( kernel constraint : constraint function apply to the kernel weight matrix  ) ( recurrent constraint : constraint function apply to the recurrent kernel weight matrix  ) ( bias constraint : constraint function apply to the bias vector  ) ( dropout : float between 0 and 1  fraction of the units to drop for the linear transformation of the input  ) ( recurrent dropout : float between 0 and 1  fraction of the units to drop for the linear transformation of the recurrent state  ) ( reset after : gru convention  whether to apply reset gate after or before matrix multiplication   false   "before"  default   true   "after"  cudnn compatible   )
( units : positive integer  dimensionality of the output space  ) ( activation : activation function to use  default  hyperbolic tangent  tanh   if you pass none  no activation be apply  ie  "linear" activation  a x    x   ) ( recurrent activation : activation function to use for the recurrent step  default  hard sigmoid  hard sigmoid   if you pass none  no activation be apply  ie  "linear" activation  a x    x   ) ( use bias : boolean  whether the layer use a bias vector  ) ( kernel initializer : initializer for the kernel weight matrix  use for the linear transformation of the input   ) ( recurrent initializer : initializer for the recurrent kernel weight matrix  use for the linear transformation of the recurrent state  ) ( bias initializer : initializer for the bias vector  ) ( unit forget bias : boolean  if true  add 1 to the bias of the forget gate at initialization  set it to true will also force bias initializer "zeros"  this be recommend in jozefowicz et al   2015  ) ( kernel regularizer : regularizer function apply to the kernel weight matrix  ) ( recurrent regularizer : regularizer function apply to the recurrent kernel weight matrix  ) ( bias regularizer : regularizer function apply to the bias vector  ) ( activity regularizer : regularizer function apply to the output of the layer  its "activation"   ) ( kernel constraint : constraint function apply to the kernel weight matrix  ) ( recurrent constraint : constraint function apply to the recurrent kernel weight matrix  ) ( bias constraint : constraint function apply to the bias vector  ) ( dropout : float between 0 and 1  fraction of the units to drop for the linear transformation of the input  ) ( recurrent dropout : float between 0 and 1  fraction of the units to drop for the linear transformation of the recurrent state  ) ( return sequences : boolean  whether to return the last output in the output sequence  or the full sequence  ) ( return state : boolean  whether to return the last state in addition to the output  ) ( go backwards : boolean  default false   if true  process the input sequence backwards and return the reverse sequence  ) ( stateful : boolean  default false   if true  the last state for each sample at index i in a batch will be use as initial state for the sample of index i in the follow batch  ) ( unroll : boolean  default false   if true  the network will be unroll  else a symbolic loop will be use  unroll can speed up a rnn  although it tend to be more memory intensive  unroll be only suitable for short sequence  ) ( time major : the shape format of the input and output tensors  if true  the input and output will be in shape  timesteps  batch        whereas in the false case  it will be  batch  timesteps        use time major   true be a bite more efficient because it avoid transpose at the begin and end of the rnn calculation  however  most tensorflow data be batch major  so by default this function accept input and emit output in batch major form  )
( units : positive integer  dimensionality of the output space  ) ( activation : activation function to use  default  hyperbolic tangent  tanh   if you pass none  no activation be apply  ie  "linear" activation  a x    x   ) ( recurrent activation : activation function to use for the recurrent step  default  hard sigmoid  hard sigmoid   if you pass none  no activation be apply  ie  "linear" activation  a x    x   ) ( use bias : boolean  whether the layer use a bias vector  ) ( kernel initializer : initializer for the kernel weight matrix  use for the linear transformation of the input  ) ( recurrent initializer : initializer for the recurrent kernel weight matrix  use for the linear transformation of the recurrent state  ) ( bias initializer : initializer for the bias vector  ) ( unit forget bias : boolean  if true  add 1 to the bias of the forget gate at initialization  set it to true will also force bias initializer "zeros"  this be recommend in jozefowicz et al   2015 ) ( kernel regularizer : regularizer function apply to the kernel weight matrix  ) ( recurrent regularizer : regularizer function apply to the recurrent kernel weight matrix  ) ( bias regularizer : regularizer function apply to the bias vector  ) ( kernel constraint : constraint function apply to the kernel weight matrix  ) ( recurrent constraint : constraint function apply to the recurrent kernel weight matrix  ) ( bias constraint : constraint function apply to the bias vector  ) ( dropout : float between 0 and 1  fraction of the units to drop for the linear transformation of the input  ) ( recurrent dropout : float between 0 and 1  fraction of the units to drop for the linear transformation of the recurrent state  )
( pool size : an integer or tuple/list of a single integer  represent the size of the pool window  ) ( strides : an integer or tuple/list of a single integer  specify the stride of the pool operation  ) ( padding : a string  the pad method  either 'valid' or 'same'  case insensitive  ) ( data format : a string  one of channel last  default  or channel first  the order of the dimension in the input  channel last correspond to input with shape  batch  length  channel  while channel first correspond to input with shape  batch  channel  length   ) ( name : a string  the name of the layer  )
( pool size : an integer or tuple/list of 2 integers   pool height  pool width  specify the size of the pool window  can be a single integer to specify the same value for all spatial dimension  ) ( strides : an integer or tuple/list of 2 integers  specify the stride of the pool operation  can be a single integer to specify the same value for all spatial dimension  ) ( padding : a string  the pad method  either 'valid' or 'same'  case insensitive  ) ( data format : a string  the order of the dimension in the input  channel last  default  and channel first be support  channel last correspond to input with shape  batch  height  width  channel  while channel first correspond to input with shape  batch  channel  height  width   ) ( name : a string  the name of the layer  )
( pool size : an integer or tuple/list of 3 integers   pool depth  pool height  pool width  specify the size of the pool window  can be a single integer to specify the same value for all spatial dimension  ) ( strides : an integer or tuple/list of 3 integers  specify the stride of the pool operation  can be a single integer to specify the same value for all spatial dimension  ) ( padding : a string  the pad method  either 'valid' or 'same'  case insensitive  ) ( data format : a string  the order of the dimension in the input  channel last  default  and channel first be support  channel last correspond to input with shape  batch  depth  height  width  channel  while channel first correspond to input with shape  batch  channel  depth  height  width   ) ( name : a string  the name of the layer  )
( axis : an int or list of int  the axis or ax that should be normalize  typically the feature axis/axes  for instance  after a conv2d layer with data format "channels first"  set axis 1  if a list of ax be provide  each axis in axis will be normalize   simultaneously  default be  1 which use the last axis  note  when     use multi axis batch norm  the beta  gamma  move mean  and     move variance variables be the same rank as the input tensor      with dimension size 1 in all reduce  non axis  dimension   ) ( momentum : momentum for the move average  ) ( epsilon : small float add to variance to avoid divide by zero  ) ( center : if true  add offset of beta to normalize tensor  if false  beta be ignore  ) ( scale : if true  multiply by gamma  if false  gamma be not use  when the next layer be linear  also e g  nn relu   this can be disable since the scale can be do by the next layer  ) ( beta initializer : initializer for the beta weight  ) ( gamma initializer : initializer for the gamma weight  ) ( moving mean initializer : initializer for the move mean  ) ( moving variance initializer : initializer for the move variance  ) ( beta regularizer : optional regularizer for the beta weight  ) ( gamma regularizer : optional regularizer for the gamma weight  ) ( beta constraint : an optional projection function to be apply to the beta weight after be update by an optimizer  e g  use to implement norm constraints or value constraints for layer weight   the function must take as input the unprojected variable and must return the project variable  which must have the same shape   constraints be not safe to use when do asynchronous distribute train  ) ( gamma constraint : an optional projection function to be apply to the gamma weight after be update by an optimizer  ) ( renorm : whether to use batch renormalization  ioffe  2017   this add extra variables during train  the inference be the same for either value of this parameter  ) ( renorm clipping : a dictionary that may map key 'rmax'  'rmin'  'dmax' to scalar tensors use to clip the renorm correction  the correction  r  d  be use as correct value   normalize value   r   d  with r clip to  rmin  rmax   and d to   dmax  dmax   miss rmax  rmin  dmax be set to inf  0  inf  respectively  ) ( renorm momentum : momentum use to update the move mean and standard deviations with renorm  unlike momentum  this affect train and should be neither too small  which would add noise  nor too large  which would give stale estimate   note that momentum be still apply to get the mean and variances for inference  ) ( fused : if none or true  use a faster  fuse implementation if possible  if false  use the system recommend implementation  ) ( trainable : boolean  if true also add variables to the graph collection graphkeys trainable variables  see tf variable   ) ( virtual batch size : an int  by default  virtual batch size be none  which mean batch normalization be perform across the whole batch  when virtual batch size be not none  instead perform "ghost batch normalization"  which create virtual sub batch which be each normalize separately  with share gamma  beta  and move statistics   must divide the actual batch size during execution  ) ( adjustment : a function take the tensor contain the  dynamic  shape of the input tensor and return a pair  scale  bias  to apply to the normalize value  before gamma and beta   only during train  for example  if axis   1    adjustment   lambda shape        tf random uniform shape  1    0 93  1 07       tf random uniform shape  1     0 1  0 1   will scale the normalize       value by up to 7  up or down  then shift the result by up to 0 1        with independent scale and bias for each feature but share       across all examples   and finally apply gamma and/or beta  if       none  no adjustment be apply  cannot be specify if       virtual batch size be specify  ) ( name : a string  the name of the layer  )
( filters : integer  the dimensionality of the output space  i e  the number of filter in the convolution   ) ( kernel size : an integer or tuple/list of a single integer  specify the length of the 1d convolution window  ) ( strides : an integer or tuple/list of a single integer  specify the stride length of the convolution  specify any stride value    1 be incompatible with specify any dilation rate value    1  ) ( padding : one of "valid" or "same"  case insensitive   "valid" mean no pad  "same" result in pad evenly to the left/right or up/down of the input such that output have the same height/width dimension as the input  ) ( data format : a string  one of channel last  default  or channel first  the order of the dimension in the input  channel last correspond to input with shape  batch  length  channel  while channel first correspond to input with shape  batch  channel  length   ) ( dilation rate : an integer or tuple/list of a single integer  specify the dilation rate to use for dilate convolution  currently  specify any dilation rate value    1 be incompatible with specify any stride value    1  ) ( activation : activation function  set it to none to maintain a linear activation  ) ( use bias : boolean  whether the layer use a bias  ) ( kernel initializer : an initializer for the convolution kernel  ) ( bias initializer : an initializer for the bias vector  if none  the default initializer will be use  ) ( kernel regularizer : optional regularizer for the convolution kernel  ) ( bias regularizer : optional regularizer for the bias vector  ) ( activity regularizer : optional regularizer function for the output  ) ( kernel constraint : optional projection function to be apply to the kernel after be update by an optimizer  e g  use to implement norm constraints or value constraints for layer weight   the function must take as input the unprojected variable and must return the project variable  which must have the same shape   constraints be not safe to use when do asynchronous distribute train  ) ( bias constraint : optional projection function to be apply to the bias after be update by an optimizer  ) ( trainable : boolean  if true also add variables to the graph collection graphkeys trainable variables  see tf variable   ) ( name : a string  the name of the layer  )
( filters : integer  the dimensionality of the output space  i e  the number of filter in the convolution   ) ( kernel size : an integer or tuple/list of 2 integers  specify the height and width of the 2d convolution window  can be a single integer to specify the same value for all spatial dimension  ) ( strides : an integer or tuple/list of 2 integers  specify the stride of the convolution along the height and width  can be a single integer to specify the same value for all spatial dimension  specify any stride value    1 be incompatible with specify any dilation rate value    1  ) ( padding : one of "valid" or "same"  case insensitive   "valid" mean no pad  "same" result in pad evenly to the left/right or up/down of the input such that output have the same height/width dimension as the input  ) ( data format : a string  one of channel last  default  or channel first  the order of the dimension in the input  channel last correspond to input with shape  batch  height  width  channel  while channel first correspond to input with shape  batch  channel  height  width   ) ( dilation rate : an integer or tuple/list of 2 integers  specify the dilation rate to use for dilate convolution  can be a single integer to specify the same value for all spatial dimension  currently  specify any dilation rate value    1 be incompatible with specify any stride value    1  ) ( activation : activation function  set it to none to maintain a linear activation  ) ( use bias : boolean  whether the layer use a bias  ) ( kernel initializer : an initializer for the convolution kernel  ) ( bias initializer : an initializer for the bias vector  if none  the default initializer will be use  ) ( kernel regularizer : optional regularizer for the convolution kernel  ) ( bias regularizer : optional regularizer for the bias vector  ) ( activity regularizer : optional regularizer function for the output  ) ( kernel constraint : optional projection function to be apply to the kernel after be update by an optimizer  e g  use to implement norm constraints or value constraints for layer weight   the function must take as input the unprojected variable and must return the project variable  which must have the same shape   constraints be not safe to use when do asynchronous distribute train  ) ( bias constraint : optional projection function to be apply to the bias after be update by an optimizer  ) ( trainable : boolean  if true also add variables to the graph collection graphkeys trainable variables  see tf variable   ) ( name : a string  the name of the layer  )
( filters : integer  the dimensionality of the output space  i e  the number of filter in the convolution   ) ( kernel size : a tuple or list of 2 positive integers specify the spatial dimension of the filter  can be a single integer to specify the same value for all spatial dimension  ) ( strides : a tuple or list of 2 positive integers specify the stride of the convolution  can be a single integer to specify the same value for all spatial dimension  ) ( padding : one of "valid" or "same"  case insensitive   "valid" mean no pad  "same" result in pad evenly to the left/right or up/down of the input such that output have the same height/width dimension as the input  ) ( data format : a string  one of channel last  default  or channel first  the order of the dimension in the input  channel last correspond to input with shape  batch  height  width  channel  while channel first correspond to input with shape  batch  channel  height  width   ) ( activation : activation function  set it to none to maintain a linear activation  ) ( use bias : boolean  whether the layer use a bias  ) ( kernel initializer : an initializer for the convolution kernel  ) ( bias initializer : an initializer for the bias vector  if none  the default initializer will be use  ) ( kernel regularizer : optional regularizer for the convolution kernel  ) ( bias regularizer : optional regularizer for the bias vector  ) ( activity regularizer : optional regularizer function for the output  ) ( kernel constraint : optional projection function to be apply to the kernel after be update by an optimizer  e g  use to implement norm constraints or value constraints for layer weight   the function must take as input the unprojected variable and must return the project variable  which must have the same shape   constraints be not safe to use when do asynchronous distribute train  ) ( bias constraint : optional projection function to be apply to the bias after be update by an optimizer  ) ( trainable : boolean  if true also add variables to the graph collection graphkeys trainable variables  see tf variable   ) ( name : a string  the name of the layer  )
( filters : integer  the dimensionality of the output space  i e  the number of filter in the convolution   ) ( kernel size : an integer or tuple/list of 3 integers  specify the depth  height and width of the 3d convolution window  can be a single integer to specify the same value for all spatial dimension  ) ( strides : an integer or tuple/list of 3 integers  specify the stride of the convolution along the depth  height and width  can be a single integer to specify the same value for all spatial dimension  specify any stride value    1 be incompatible with specify any dilation rate value    1  ) ( padding : one of "valid" or "same"  case insensitive   "valid" mean no pad  "same" result in pad evenly to the left/right or up/down of the input such that output have the same height/width dimension as the input  ) ( data format : a string  one of channel last  default  or channel first  the order of the dimension in the input  channel last correspond to input with shape  batch  depth  height  width  channel  while channel first correspond to input with shape  batch  channel  depth  height  width   ) ( dilation rate : an integer or tuple/list of 3 integers  specify the dilation rate to use for dilate convolution  can be a single integer to specify the same value for all spatial dimension  currently  specify any dilation rate value    1 be incompatible with specify any stride value    1  ) ( activation : activation function  set it to none to maintain a linear activation  ) ( use bias : boolean  whether the layer use a bias  ) ( kernel initializer : an initializer for the convolution kernel  ) ( bias initializer : an initializer for the bias vector  if none  the default initializer will be use  ) ( kernel regularizer : optional regularizer for the convolution kernel  ) ( bias regularizer : optional regularizer for the bias vector  ) ( activity regularizer : optional regularizer function for the output  ) ( kernel constraint : optional projection function to be apply to the kernel after be update by an optimizer  e g  use to implement norm constraints or value constraints for layer weight   the function must take as input the unprojected variable and must return the project variable  which must have the same shape   constraints be not safe to use when do asynchronous distribute train  ) ( bias constraint : optional projection function to be apply to the bias after be update by an optimizer  ) ( trainable : boolean  if true also add variables to the graph collection graphkeys trainable variables  see tf variable   ) ( name : a string  the name of the layer  )
( filters : integer  the dimensionality of the output space  i e  the number of filter in the convolution   ) ( kernel size : an integer or tuple/list of 3 integers  specify the depth  height and width of the 3d convolution window  can be a single integer to specify the same value for all spatial dimension  ) ( strides : an integer or tuple/list of 3 integers  specify the stride of the convolution along the depth  height and width  can be a single integer to specify the same value for all spatial dimension  ) ( padding : one of "valid" or "same"  case insensitive   "valid" mean no pad  "same" result in pad evenly to the left/right or up/down of the input such that output have the same height/width dimension as the input  ) ( data format : a string  one of channel last  default  or channel first  the order of the dimension in the input  channel last correspond to input with shape  batch  depth  height  width  channel  while channel first correspond to input with shape  batch  channel  depth  height  width   ) ( activation : activation function  set it to none to maintain a linear activation  ) ( use bias : boolean  whether the layer use a bias  ) ( kernel initializer : an initializer for the convolution kernel  ) ( bias initializer : an initializer for the bias vector  if none  the default initializer will be use  ) ( kernel regularizer : optional regularizer for the convolution kernel  ) ( bias regularizer : optional regularizer for the bias vector  ) ( activity regularizer : optional regularizer function for the output  ) ( kernel constraint : optional projection function to be apply to the kernel after be update by an optimizer  e g  use to implement norm constraints or value constraints for layer weight   the function must take as input the unprojected variable and must return the project variable  which must have the same shape   constraints be not safe to use when do asynchronous distribute train  ) ( bias constraint : optional projection function to be apply to the bias after be update by an optimizer  ) ( trainable : boolean  if true also add variables to the graph collection graphkeys trainable variables  see tf variable   ) ( name : a string  the name of the layer  )
( units : integer or long  dimensionality of the output space  ) ( activation : activation function  callable   set it to none to maintain a linear activation  ) ( use bias : boolean  whether the layer use a bias  ) ( kernel initializer : initializer function for the weight matrix  if none  default   weight be initialize use the default initializer use by tf compat v1 get variable  ) ( bias initializer : initializer function for the bias  ) ( kernel regularizer : regularizer function for the weight matrix  ) ( bias regularizer : regularizer function for the bias  ) ( activity regularizer : regularizer function for the output  ) ( kernel constraint : an optional projection function to be apply to the kernel after be update by an optimizer  e g  use to implement norm constraints or value constraints for layer weight   the function must take as input the unprojected variable and must return the project variable  which must have the same shape   constraints be not safe to use when do asynchronous distribute train  ) ( bias constraint : an optional projection function to be apply to the bias after be update by an optimizer  ) ( trainable : boolean  if true also add variables to the graph collection graphkeys trainable variables  see tf variable   ) ( name : string  the name of the layer  layer with the same name will share weight  but to avoid mistake we require reuse true in such case  ) (  reuse : boolean  whether to reuse the weight of a previous layer by the same name  )
( rate : the dropout rate  between 0 and 1  e g  rate 0 1 would drop out 10  of input units  ) ( noise shape : 1d tensor of type int32 represent the shape of the binary dropout mask that will be multiply with the input  for instance  if your input have shape  batch size  timesteps  feature   and you want the dropout mask to be the same for all timesteps  you can use noise shape  batch size  1  feature   ) ( seed : a python integer  use to create random seed  see tf compat v1 set random seed  for behavior  ) ( name : the name of the layer  string   )
( data format : a string  one of channel last  default  or channel first  the order of the dimension in the input  channel last correspond to input with shape  batch       channel  while channel first correspond to input with shape  batch  channel        )
( trainable : boolean  whether the layer's variables should be trainable  ) ( name : string name of the layer  ) ( dtype : default dtype of the layer's weight  default of none mean use the type of the first input   )
( pool size : an integer or tuple/list of a single integer  represent the size of the pool window  ) ( strides : an integer or tuple/list of a single integer  specify the stride of the pool operation  ) ( padding : a string  the pad method  either 'valid' or 'same'  case insensitive  ) ( data format : a string  one of channel last  default  or channel first  the order of the dimension in the input  channel last correspond to input with shape  batch  length  channel  while channel first correspond to input with shape  batch  channel  length   ) ( name : a string  the name of the layer  )
( pool size : an integer or tuple/list of 2 integers   pool height  pool width  specify the size of the pool window  can be a single integer to specify the same value for all spatial dimension  ) ( strides : an integer or tuple/list of 2 integers  specify the stride of the pool operation  can be a single integer to specify the same value for all spatial dimension  ) ( padding : a string  the pad method  either 'valid' or 'same'  case insensitive  ) ( data format : a string  the order of the dimension in the input  channel last  default  and channel first be support  channel last correspond to input with shape  batch  height  width  channel  while channel first correspond to input with shape  batch  channel  height  width   ) ( name : a string  the name of the layer  )
( pool size : an integer or tuple/list of 3 integers   pool depth  pool height  pool width  specify the size of the pool window  can be a single integer to specify the same value for all spatial dimension  ) ( strides : an integer or tuple/list of 3 integers  specify the stride of the pool operation  can be a single integer to specify the same value for all spatial dimension  ) ( padding : a string  the pad method  either 'valid' or 'same'  case insensitive  ) ( data format : a string  the order of the dimension in the input  channel last  default  and channel first be support  channel last correspond to input with shape  batch  depth  height  width  channel  while channel first correspond to input with shape  batch  channel  depth  height  width   ) ( name : a string  the name of the layer  )
( filters : integer  the dimensionality of the output space  i e  the number of filter in the convolution   ) ( kernel size : a single integer specify the spatial dimension of the filter  ) ( strides : a single integer specify the stride of the convolution  specify any stride value    1 be incompatible with specify any dilation rate value    1  ) ( padding : one of "valid" or "same"  case insensitive   "valid" mean no pad  "same" result in pad evenly to the left/right or up/down of the input such that output have the same height/width dimension as the input  ) ( data format : a string  one of channel last  default  or channel first  the order of the dimension in the input  channel last correspond to input with shape  batch  length  channel  while channel first correspond to input with shape  batch  channel  length   ) ( dilation rate : a single integer  specify the dilation rate to use for dilate convolution  currently  specify any dilation rate value    1 be incompatible with specify any stride value    1  ) ( depth multiplier : the number of depthwise convolution output channel for each input channel  the total number of depthwise convolution output channel will be equal to num filter in   depth multiplier  ) ( activation : activation function  set it to none to maintain a linear activation  ) ( use bias : boolean  whether the layer use a bias  ) ( depthwise initializer : an initializer for the depthwise convolution kernel  ) ( pointwise initializer : an initializer for the pointwise convolution kernel  ) ( bias initializer : an initializer for the bias vector  if none  the default initializer will be use  ) ( depthwise regularizer : optional regularizer for the depthwise convolution kernel  ) ( pointwise regularizer : optional regularizer for the pointwise convolution kernel  ) ( bias regularizer : optional regularizer for the bias vector  ) ( activity regularizer : optional regularizer function for the output  ) ( depthwise constraint : optional projection function to be apply to the depthwise kernel after be update by an optimizer  e g  use for norm constraints or value constraints for layer weight   the function must take as input the unprojected variable and must return the project variable  which must have the same shape   constraints be not safe to use when do asynchronous distribute train  ) ( pointwise constraint : optional projection function to be apply to the pointwise kernel after be update by an optimizer  ) ( bias constraint : optional projection function to be apply to the bias after be update by an optimizer  ) ( trainable : boolean  if true also add variables to the graph collection graphkeys trainable variables  see tf variable   ) ( name : a string  the name of the layer  )
( filters : integer  the dimensionality of the output space  i e  the number of filter in the convolution   ) ( kernel size : a tuple or list of 2 integers specify the spatial dimension of the filter  can be a single integer to specify the same value for all spatial dimension  ) ( strides : a tuple or list of 2 positive integers specify the stride of the convolution  can be a single integer to specify the same value for all spatial dimension  specify any stride value    1 be incompatible with specify any dilation rate value    1  ) ( padding : one of "valid" or "same"  case insensitive   "valid" mean no pad  "same" result in pad evenly to the left/right or up/down of the input such that output have the same height/width dimension as the input  ) ( data format : a string  one of channel last  default  or channel first  the order of the dimension in the input  channel last correspond to input with shape  batch  height  width  channel  while channel first correspond to input with shape  batch  channel  height  width   ) ( dilation rate : an integer or tuple/list of 2 integers  specify the dilation rate to use for dilate convolution  can be a single integer to specify the same value for all spatial dimension  currently  specify any dilation rate value    1 be incompatible with specify any stride value    1  ) ( depth multiplier : the number of depthwise convolution output channel for each input channel  the total number of depthwise convolution output channel will be equal to num filter in   depth multiplier  ) ( activation : activation function  set it to none to maintain a linear activation  ) ( use bias : boolean  whether the layer use a bias  ) ( depthwise initializer : an initializer for the depthwise convolution kernel  ) ( pointwise initializer : an initializer for the pointwise convolution kernel  ) ( bias initializer : an initializer for the bias vector  if none  the default initializer will be use  ) ( depthwise regularizer : optional regularizer for the depthwise convolution kernel  ) ( pointwise regularizer : optional regularizer for the pointwise convolution kernel  ) ( bias regularizer : optional regularizer for the bias vector  ) ( activity regularizer : optional regularizer function for the output  ) ( depthwise constraint : optional projection function to be apply to the depthwise kernel after be update by an optimizer  e g  use for norm constraints or value constraints for layer weight   the function must take as input the unprojected variable and must return the project variable  which must have the same shape   constraints be not safe to use when do asynchronous distribute train  ) ( pointwise constraint : optional projection function to be apply to the pointwise kernel after be update by an optimizer  ) ( bias constraint : optional projection function to be apply to the bias after be update by an optimizer  ) ( trainable : boolean  if true also add variables to the graph collection graphkeys trainable variables  see tf variable   ) ( name : a string  the name of the layer  )
( inputs : the tensor over which to pool  must have rank 3  ) ( pool size : an integer or tuple/list of a single integer  represent the size of the pool window  ) ( strides : an integer or tuple/list of a single integer  specify the stride of the pool operation  ) ( padding : a string  the pad method  either 'valid' or 'same'  case insensitive  ) ( data format : a string  one of channel last  default  or channel first  the order of the dimension in the input  channel last correspond to input with shape  batch  length  channel  while channel first correspond to input with shape  batch  channel  length   ) ( name : a string  the name of the layer  )
( inputs : the tensor over which to pool  must have rank 4  ) ( pool size : an integer or tuple/list of 2 integers   pool height  pool width  specify the size of the pool window  can be a single integer to specify the same value for all spatial dimension  ) ( strides : an integer or tuple/list of 2 integers  specify the stride of the pool operation  can be a single integer to specify the same value for all spatial dimension  ) ( padding : a string  the pad method  either 'valid' or 'same'  case insensitive  ) ( data format : a string  the order of the dimension in the input  channel last  default  and channel first be support  channel last correspond to input with shape  batch  height  width  channel  while channel first correspond to input with shape  batch  channel  height  width   ) ( name : a string  the name of the layer  )
( inputs : the tensor over which to pool  must have rank 5  ) ( pool size : an integer or tuple/list of 3 integers   pool depth  pool height  pool width  specify the size of the pool window  can be a single integer to specify the same value for all spatial dimension  ) ( strides : an integer or tuple/list of 3 integers  specify the stride of the pool operation  can be a single integer to specify the same value for all spatial dimension  ) ( padding : a string  the pad method  either 'valid' or 'same'  case insensitive  ) ( data format : a string  the order of the dimension in the input  channel last  default  and channel first be support  channel last correspond to input with shape  batch  depth  height  width  channel  while channel first correspond to input with shape  batch  channel  depth  height  width   ) ( name : a string  the name of the layer  )
( inputs : tensor input  ) ( axis : an int  the axis that should be normalize  typically the feature axis   for instance  after a convolution2d layer with data format "channels first"  set axis 1 in batchnormalization  ) ( momentum : momentum for the move average  ) ( epsilon : small float add to variance to avoid divide by zero  ) ( center : if true  add offset of beta to normalize tensor  if false  beta be ignore  ) ( scale : if true  multiply by gamma  if false  gamma be not use  when the next layer be linear  also e g  nn relu   this can be disable since the scale can be do by the next layer  ) ( beta initializer : initializer for the beta weight  ) ( gamma initializer : initializer for the gamma weight  ) ( moving mean initializer : initializer for the move mean  ) ( moving variance initializer : initializer for the move variance  ) ( beta regularizer : optional regularizer for the beta weight  ) ( gamma regularizer : optional regularizer for the gamma weight  ) ( beta constraint : an optional projection function to be apply to the beta weight after be update by an optimizer  e g  use to implement norm constraints or value constraints for layer weight   the function must take as input the unprojected variable and must return the project variable  which must have the same shape   constraints be not safe to use when do asynchronous distribute train  ) ( gamma constraint : an optional projection function to be apply to the gamma weight after be update by an optimizer  ) ( training : either a python boolean  or a tensorflow boolean scalar tensor  e g  a placeholder   whether to return the output in train mode  normalize with statistics of the current batch  or in inference mode  normalize with move statistics   note  make sure to set this   parameter correctly  or else your training/inference will not work   properly  ) ( trainable : boolean  if true also add variables to the graph collection graphkeys trainable variables  see tf variable   ) ( name : string  the name of the layer  ) ( reuse : boolean  whether to reuse the weight of a previous layer by the same name  ) ( renorm : whether to use batch renormalization  ioffe  2017   this add extra variables during train  the inference be the same for either value of this parameter  ) ( renorm clipping : a dictionary that may map key 'rmax'  'rmin'  'dmax' to scalar tensors use to clip the renorm correction  the correction  r  d  be use as correct value   normalize value   r   d  with r clip to  rmin  rmax   and d to   dmax  dmax   miss rmax  rmin  dmax be set to inf  0  inf  respectively  ) ( renorm momentum : momentum use to update the move mean and standard deviations with renorm  unlike momentum  this affect train and should be neither too small  which would add noise  nor too large  which would give stale estimate   note that momentum be still apply to get the mean and variances for inference  ) ( fused : if none or true  use a faster  fuse implementation if possible  if false  use the system recommend implementation  ) ( virtual batch size : an int  by default  virtual batch size be none  which mean batch normalization be perform across the whole batch  when virtual batch size be not none  instead perform "ghost batch normalization"  which create virtual sub batch which be each normalize separately  with share gamma  beta  and move statistics   must divide the actual batch size during execution  ) ( adjustment : a function take the tensor contain the  dynamic  shape of the input tensor and return a pair  scale  bias  to apply to the normalize value  before gamma and beta   only during train  for example  if axis   1    adjustment   lambda shape        tf random uniform shape  1    0 93  1 07       tf random uniform shape  1     0 1  0 1   will scale the normalize       value by up to 7  up or down  then shift the result by up to 0 1        with independent scale and bias for each feature but share       across all examples   and finally apply gamma and/or beta  if       none  no adjustment be apply  cannot be specify if       virtual batch size be specify  )
( inputs : tensor input  ) ( filters : integer  the dimensionality of the output space  i e  the number of filter in the convolution   ) ( kernel size : an integer or tuple/list of a single integer  specify the length of the 1d convolution window  ) ( strides : an integer or tuple/list of a single integer  specify the stride length of the convolution  specify any stride value    1 be incompatible with specify any dilation rate value    1  ) ( padding : one of "valid" or "same"  case insensitive   "valid" mean no pad  "same" result in pad evenly to the left/right or up/down of the input such that output have the same height/width dimension as the input  ) ( data format : a string  one of channel last  default  or channel first  the order of the dimension in the input  channel last correspond to input with shape  batch  length  channel  while channel first correspond to input with shape  batch  channel  length   ) ( dilation rate : an integer or tuple/list of a single integer  specify the dilation rate to use for dilate convolution  currently  specify any dilation rate value    1 be incompatible with specify any stride value    1  ) ( activation : activation function  set it to none to maintain a linear activation  ) ( use bias : boolean  whether the layer use a bias  ) ( kernel initializer : an initializer for the convolution kernel  ) ( bias initializer : an initializer for the bias vector  if none  the default initializer will be use  ) ( kernel regularizer : optional regularizer for the convolution kernel  ) ( bias regularizer : optional regularizer for the bias vector  ) ( activity regularizer : optional regularizer function for the output  ) ( kernel constraint : optional projection function to be apply to the kernel after be update by an optimizer  e g  use to implement norm constraints or value constraints for layer weight   the function must take as input the unprojected variable and must return the project variable  which must have the same shape   constraints be not safe to use when do asynchronous distribute train  ) ( bias constraint : optional projection function to be apply to the bias after be update by an optimizer  ) ( trainable : boolean  if true also add variables to the graph collection graphkeys trainable variables  see tf variable   ) ( name : a string  the name of the layer  ) ( reuse : boolean  whether to reuse the weight of a previous layer by the same name  )
( inputs : tensor input  ) ( filters : integer  the dimensionality of the output space  i e  the number of filter in the convolution   ) ( kernel size : an integer or tuple/list of 2 integers  specify the height and width of the 2d convolution window  can be a single integer to specify the same value for all spatial dimension  ) ( strides : an integer or tuple/list of 2 integers  specify the stride of the convolution along the height and width  can be a single integer to specify the same value for all spatial dimension  specify any stride value    1 be incompatible with specify any dilation rate value    1  ) ( padding : one of "valid" or "same"  case insensitive   "valid" mean no pad  "same" result in pad evenly to the left/right or up/down of the input such that output have the same height/width dimension as the input  ) ( data format : a string  one of channel last  default  or channel first  the order of the dimension in the input  channel last correspond to input with shape  batch  height  width  channel  while channel first correspond to input with shape  batch  channel  height  width   ) ( dilation rate : an integer or tuple/list of 2 integers  specify the dilation rate to use for dilate convolution  can be a single integer to specify the same value for all spatial dimension  currently  specify any dilation rate value    1 be incompatible with specify any stride value    1  ) ( activation : activation function  set it to none to maintain a linear activation  ) ( use bias : boolean  whether the layer use a bias  ) ( kernel initializer : an initializer for the convolution kernel  ) ( bias initializer : an initializer for the bias vector  if none  the default initializer will be use  ) ( kernel regularizer : optional regularizer for the convolution kernel  ) ( bias regularizer : optional regularizer for the bias vector  ) ( activity regularizer : optional regularizer function for the output  ) ( kernel constraint : optional projection function to be apply to the kernel after be update by an optimizer  e g  use to implement norm constraints or value constraints for layer weight   the function must take as input the unprojected variable and must return the project variable  which must have the same shape   constraints be not safe to use when do asynchronous distribute train  ) ( bias constraint : optional projection function to be apply to the bias after be update by an optimizer  ) ( trainable : boolean  if true also add variables to the graph collection graphkeys trainable variables  see tf variable   ) ( name : a string  the name of the layer  ) ( reuse : boolean  whether to reuse the weight of a previous layer by the same name  )
( inputs : input tensor  ) ( filters : integer  the dimensionality of the output space  i e  the number of filter in the convolution   ) ( kernel size : a tuple or list of 2 positive integers specify the spatial dimension of the filter  can be a single integer to specify the same value for all spatial dimension  ) ( strides : a tuple or list of 2 positive integers specify the stride of the convolution  can be a single integer to specify the same value for all spatial dimension  ) ( padding : one of "valid" or "same"  case insensitive   "valid" mean no pad  "same" result in pad evenly to the left/right or up/down of the input such that output have the same height/width dimension as the input  ) ( data format : a string  one of channel last  default  or channel first  the order of the dimension in the input  channel last correspond to input with shape  batch  height  width  channel  while channel first correspond to input with shape  batch  channel  height  width   ) ( activation : activation function  set it to none to maintain a linear activation  ) ( use bias : boolean  whether the layer use a bias  ) ( kernel initializer : an initializer for the convolution kernel  ) ( bias initializer : an initializer for the bias vector  if none  the default initializer will be use  ) ( kernel regularizer : optional regularizer for the convolution kernel  ) ( bias regularizer : optional regularizer for the bias vector  ) ( activity regularizer : optional regularizer function for the output  ) ( kernel constraint : optional projection function to be apply to the kernel after be update by an optimizer  e g  use to implement norm constraints or value constraints for layer weight   the function must take as input the unprojected variable and must return the project variable  which must have the same shape   constraints be not safe to use when do asynchronous distribute train  ) ( bias constraint : optional projection function to be apply to the bias after be update by an optimizer  ) ( trainable : boolean  if true also add variables to the graph collection graphkeys trainable variables  see tf variable   ) ( name : a string  the name of the layer  ) ( reuse : boolean  whether to reuse the weight of a previous layer by the same name  )
( inputs : tensor input  ) ( filters : integer  the dimensionality of the output space  i e  the number of filter in the convolution   ) ( kernel size : an integer or tuple/list of 3 integers  specify the depth  height and width of the 3d convolution window  can be a single integer to specify the same value for all spatial dimension  ) ( strides : an integer or tuple/list of 3 integers  specify the stride of the convolution along the depth  height and width  can be a single integer to specify the same value for all spatial dimension  specify any stride value    1 be incompatible with specify any dilation rate value    1  ) ( padding : one of "valid" or "same"  case insensitive   "valid" mean no pad  "same" result in pad evenly to the left/right or up/down of the input such that output have the same height/width dimension as the input  ) ( data format : a string  one of channel last  default  or channel first  the order of the dimension in the input  channel last correspond to input with shape  batch  depth  height  width  channel  while channel first correspond to input with shape  batch  channel  depth  height  width   ) ( dilation rate : an integer or tuple/list of 3 integers  specify the dilation rate to use for dilate convolution  can be a single integer to specify the same value for all spatial dimension  currently  specify any dilation rate value    1 be incompatible with specify any stride value    1  ) ( activation : activation function  set it to none to maintain a linear activation  ) ( use bias : boolean  whether the layer use a bias  ) ( kernel initializer : an initializer for the convolution kernel  ) ( bias initializer : an initializer for the bias vector  if none  the default initializer will be use  ) ( kernel regularizer : optional regularizer for the convolution kernel  ) ( bias regularizer : optional regularizer for the bias vector  ) ( activity regularizer : optional regularizer function for the output  ) ( kernel constraint : optional projection function to be apply to the kernel after be update by an optimizer  e g  use to implement norm constraints or value constraints for layer weight   the function must take as input the unprojected variable and must return the project variable  which must have the same shape   constraints be not safe to use when do asynchronous distribute train  ) ( bias constraint : optional projection function to be apply to the bias after be update by an optimizer  ) ( trainable : boolean  if true also add variables to the graph collection graphkeys trainable variables  see tf variable   ) ( name : a string  the name of the layer  ) ( reuse : boolean  whether to reuse the weight of a previous layer by the same name  )
( inputs : input tensor  ) ( filters : integer  the dimensionality of the output space  i e  the number of filter in the convolution   ) ( kernel size : a tuple or list of 3 positive integers specify the spatial dimension of the filter  can be a single integer to specify the same value for all spatial dimension  ) ( strides : a tuple or list of 3 positive integers specify the stride of the convolution  can be a single integer to specify the same value for all spatial dimension  ) ( padding : one of "valid" or "same"  case insensitive   "valid" mean no pad  "same" result in pad evenly to the left/right or up/down of the input such that output have the same height/width dimension as the input  ) ( data format : a string  one of channel last  default  or channel first  the order of the dimension in the input  channel last correspond to input with shape  batch  depth  height  width  channel  while channel first correspond to input with shape  batch  channel  depth  height  width   ) ( activation : activation function  set it to none to maintain a linear activation  ) ( use bias : boolean  whether the layer use a bias  ) ( kernel initializer : an initializer for the convolution kernel  ) ( bias initializer : an initializer for the bias vector  if none  the default initializer will be use  ) ( kernel regularizer : optional regularizer for the convolution kernel  ) ( bias regularizer : optional regularizer for the bias vector  ) ( activity regularizer : optional regularizer function for the output  ) ( kernel constraint : optional projection function to be apply to the kernel after be update by an optimizer  e g  use to implement norm constraints or value constraints for layer weight   the function must take as input the unprojected variable and must return the project variable  which must have the same shape   constraints be not safe to use when do asynchronous distribute train  ) ( bias constraint : optional projection function to be apply to the bias after be update by an optimizer  ) ( trainable : boolean  if true also add variables to the graph collection graphkeys trainable variables  see tf variable   ) ( name : a string  the name of the layer  ) ( reuse : boolean  whether to reuse the weight of a previous layer by the same name  )
( inputs : tensor input  ) ( units : integer or long  dimensionality of the output space  ) ( activation : activation function  callable   set it to none to maintain a linear activation  ) ( use bias : boolean  whether the layer use a bias  ) ( kernel initializer : initializer function for the weight matrix  if none  default   weight be initialize use the default initializer use by tf compat v1 get variable  ) ( bias initializer : initializer function for the bias  ) ( kernel regularizer : regularizer function for the weight matrix  ) ( bias regularizer : regularizer function for the bias  ) ( activity regularizer : regularizer function for the output  ) ( kernel constraint : an optional projection function to be apply to the kernel after be update by an optimizer  e g  use to implement norm constraints or value constraints for layer weight   the function must take as input the unprojected variable and must return the project variable  which must have the same shape   constraints be not safe to use when do asynchronous distribute train  ) ( bias constraint : an optional projection function to be apply to the bias after be update by an optimizer  ) ( trainable : boolean  if true also add variables to the graph collection graphkeys trainable variables  see tf variable   ) ( name : string  the name of the layer  ) ( reuse : boolean  whether to reuse the weight of a previous layer by the same name  )
( inputs : tensor input  ) ( rate : the dropout rate  between 0 and 1  e g  "rate 0 1" would drop out 10  of input units  ) ( noise shape : 1d tensor of type int32 represent the shape of the binary dropout mask that will be multiply with the input  for instance  if your input have shape  batch size  timesteps  feature   and you want the dropout mask to be the same for all timesteps  you can use noise shape  batch size  1  feature   ) ( seed : a python integer  use to create random seed  see tf compat v1 set random seed for behavior  ) ( training : either a python boolean  or a tensorflow boolean scalar tensor  e g  a placeholder   whether to return the output in train mode  apply dropout  or in inference mode  return the input untouched   ) ( name : the name of the layer  string   )
( inputs : tensor input  ) ( name : the name of the layer  string   ) ( data format : a string  one of channel last  default  or channel first  the order of the dimension in the input  channel last correspond to input with shape  batch  height  width  channel  while channel first correspond to input with shape  batch  channel  height  width   )
( inputs : the tensor over which to pool  must have rank 3  ) ( pool size : an integer or tuple/list of a single integer  represent the size of the pool window  ) ( strides : an integer or tuple/list of a single integer  specify the stride of the pool operation  ) ( padding : a string  the pad method  either 'valid' or 'same'  case insensitive  ) ( data format : a string  one of channel last  default  or channel first  the order of the dimension in the input  channel last correspond to input with shape  batch  length  channel  while channel first correspond to input with shape  batch  channel  length   ) ( name : a string  the name of the layer  )
( inputs : the tensor over which to pool  must have rank 4  ) ( pool size : an integer or tuple/list of 2 integers   pool height  pool width  specify the size of the pool window  can be a single integer to specify the same value for all spatial dimension  ) ( strides : an integer or tuple/list of 2 integers  specify the stride of the pool operation  can be a single integer to specify the same value for all spatial dimension  ) ( padding : a string  the pad method  either 'valid' or 'same'  case insensitive  ) ( data format : a string  the order of the dimension in the input  channel last  default  and channel first be support  channel last correspond to input with shape  batch  height  width  channel  while channel first correspond to input with shape  batch  channel  height  width   ) ( name : a string  the name of the layer  )
( inputs : the tensor over which to pool  must have rank 5  ) ( pool size : an integer or tuple/list of 3 integers   pool depth  pool height  pool width  specify the size of the pool window  can be a single integer to specify the same value for all spatial dimension  ) ( strides : an integer or tuple/list of 3 integers  specify the stride of the pool operation  can be a single integer to specify the same value for all spatial dimension  ) ( padding : a string  the pad method  either 'valid' or 'same'  case insensitive  ) ( data format : a string  the order of the dimension in the input  channel last  default  and channel first be support  channel last correspond to input with shape  batch  depth  height  width  channel  while channel first correspond to input with shape  batch  channel  depth  height  width   ) ( name : a string  the name of the layer  )
( inputs : input tensor  ) ( filters : integer  the dimensionality of the output space  i e  the number of filter in the convolution   ) ( kernel size : a single integer specify the spatial dimension of the filter  ) ( strides : a single integer specify the stride of the convolution  specify any stride value    1 be incompatible with specify any dilation rate value    1  ) ( padding : one of "valid" or "same"  case insensitive   "valid" mean no pad  "same" result in pad evenly to the left/right or up/down of the input such that output have the same height/width dimension as the input  ) ( data format : a string  one of channel last  default  or channel first  the order of the dimension in the input  channel last correspond to input with shape  batch  length  channel  while channel first correspond to input with shape  batch  channel  length   ) ( dilation rate : a single integer  specify the dilation rate to use for dilate convolution  currently  specify any dilation rate value    1 be incompatible with specify any stride value    1  ) ( depth multiplier : the number of depthwise convolution output channel for each input channel  the total number of depthwise convolution output channel will be equal to num filter in   depth multiplier  ) ( activation : activation function  set it to none to maintain a linear activation  ) ( use bias : boolean  whether the layer use a bias  ) ( depthwise initializer : an initializer for the depthwise convolution kernel  ) ( pointwise initializer : an initializer for the pointwise convolution kernel  ) ( bias initializer : an initializer for the bias vector  if none  the default initializer will be use  ) ( depthwise regularizer : optional regularizer for the depthwise convolution kernel  ) ( pointwise regularizer : optional regularizer for the pointwise convolution kernel  ) ( bias regularizer : optional regularizer for the bias vector  ) ( activity regularizer : optional regularizer function for the output  ) ( depthwise constraint : optional projection function to be apply to the depthwise kernel after be update by an optimizer  e g  use for norm constraints or value constraints for layer weight   the function must take as input the unprojected variable and must return the project variable  which must have the same shape   constraints be not safe to use when do asynchronous distribute train  ) ( pointwise constraint : optional projection function to be apply to the pointwise kernel after be update by an optimizer  ) ( bias constraint : optional projection function to be apply to the bias after be update by an optimizer  ) ( trainable : boolean  if true also add variables to the graph collection graphkeys trainable variables  see tf variable   ) ( name : a string  the name of the layer  ) ( reuse : boolean  whether to reuse the weight of a previous layer by the same name  )
( inputs : input tensor  ) ( filters : integer  the dimensionality of the output space  i e  the number of filter in the convolution   ) ( kernel size : a tuple or list of 2 integers specify the spatial dimension of the filter  can be a single integer to specify the same value for all spatial dimension  ) ( strides : a tuple or list of 2 positive integers specify the stride of the convolution  can be a single integer to specify the same value for all spatial dimension  specify any stride value    1 be incompatible with specify any dilation rate value    1  ) ( padding : one of "valid" or "same"  case insensitive   "valid" mean no pad  "same" result in pad evenly to the left/right or up/down of the input such that output have the same height/width dimension as the input  ) ( data format : a string  one of channel last  default  or channel first  the order of the dimension in the input  channel last correspond to input with shape  batch  height  width  channel  while channel first correspond to input with shape  batch  channel  height  width   ) ( dilation rate : an integer or tuple/list of 2 integers  specify the dilation rate to use for dilate convolution  can be a single integer to specify the same value for all spatial dimension  currently  specify any dilation rate value    1 be incompatible with specify any stride value    1  ) ( depth multiplier : the number of depthwise convolution output channel for each input channel  the total number of depthwise convolution output channel will be equal to num filter in   depth multiplier  ) ( activation : activation function  set it to none to maintain a linear activation  ) ( use bias : boolean  whether the layer use a bias  ) ( depthwise initializer : an initializer for the depthwise convolution kernel  ) ( pointwise initializer : an initializer for the pointwise convolution kernel  ) ( bias initializer : an initializer for the bias vector  if none  the default initializer will be use  ) ( depthwise regularizer : optional regularizer for the depthwise convolution kernel  ) ( pointwise regularizer : optional regularizer for the pointwise convolution kernel  ) ( bias regularizer : optional regularizer for the bias vector  ) ( activity regularizer : optional regularizer function for the output  ) ( depthwise constraint : optional projection function to be apply to the depthwise kernel after be update by an optimizer  e g  use for norm constraints or value constraints for layer weight   the function must take as input the unprojected variable and must return the project variable  which must have the same shape   constraints be not safe to use when do asynchronous distribute train  ) ( pointwise constraint : optional projection function to be apply to the pointwise kernel after be update by an optimizer  ) ( bias constraint : optional projection function to be apply to the bias after be update by an optimizer  ) ( trainable : boolean  if true also add variables to the graph collection graphkeys trainable variables  see tf variable   ) ( name : a string  the name of the layer  ) ( reuse : boolean  whether to reuse the weight of a previous layer by the same name  )
( function name : name of the function  the custom op name in tflite  ) ( level : ophint level  ) ( children inputs mappings : children ophint inputs/outputs map  children input mappings should like below  "parent first child input"        "parent input index"  num  "child input index"  num        "parent last child output"        "parent output index"  num  "child output index"  num        "internal children input output"        "child input index"  num  "child output index"  num        ) (   kwargs : keyword arguments of any constant attribute for the function  )
( graph def : freeze tensorflow graphdef  ) ( input tensors : list of input tensors  type and shape be compute use foo shape and foo dtype  ) ( output tensors : list of output tensors  only  name be use from this   ) ( input arrays with shape : tuple of string represent input tensor name and list of integers represent input shape  e g     "foo"    1  16  16  3      use only when graph cannot be load   into tensorflow and when input tensors and output tensors be   none   default none  ) ( output arrays : list of output tensors to freeze graph with  use only when graph cannot be load into tensorflow and when input tensors and output tensors be none   default none  ) ( experimental debug info func : an experimental function to retrieve the graph debug info for a set of nod from the graph def  )



( level : the level at which to log  ) ( msg : the message to be log  ) ( n : the number of time this should be call before it be log  ) (  args : the args to be substitute into the msg  )
( level : the level at which to log  ) ( msg : the message to be log  ) ( n : the number of time this should be call before it be log  ) (  args : the args to be substitute into the msg  )


( initializer : the table initializer to use  see hashtable kernel for support key and value type  ) ( default value : the value to use if a key be miss in the table  ) ( name : a name for the operation  optional   ) ( experimental is anonymous : whether to use anonymous mode for the table  default be false   in anonymous mode  the table resource can only be access via a resource handle  it can't be look up by a name  when all resource handle point to that resource be go  the resource will be delete automatically  )
( initializer : a tableinitializerbase object that contain the data use to initialize the table  if none  then we only use out of vocab bucket  ) ( num oov buckets : number of bucket to use for out of vocabulary key  must be greater than zero  if out of vocab bucket be not require  use statichashtable instead  ) ( lookup key dtype : data type of key pass to lookup  default to initializer key dtype if initializer be specify  otherwise tf string  must be string or integer  and must be castable to initializer key dtype  ) ( name : a name for the operation  optional   ) ( experimental is anonymous : whether to use anonymous mode for the table  default be false   in anonymous mode  the table resource can only be access via a resource handle  it can't be look up by a name  when all resource handle point to that resource be go  the resource will be delete automatically  )

( labels : the grind truth output tensor  same dimension as 'predictions'  ) ( predictions : the predict output  ) ( weights : optional tensor whose rank be either 0  or the same rank as label  and must be broadcastable to label  i e   all dimension must be either 1  or the same as the correspond losses dimension   ) ( scope : the scope for the operations perform in compute the loss  ) ( loss collection : collection to which this loss will be add  ) ( reduction : type of reduction to apply to loss  )
( loss : a loss tensor  ) ( loss collection : optional collection to add the loss to  )
( losses : tensor of shape  batch size  d1      dn   ) ( weights : optional tensor whose rank be either 0  or the same rank as losses  and must be broadcastable to losses  i e   all dimension must be either 1  or the same as the correspond losses dimension   ) ( scope : the scope for the operations perform in compute the loss  ) ( loss collection : the loss will be add to these collections  ) ( reduction : type of reduction to apply to loss  )
( labels : tensor whose shape match 'predictions' ) ( predictions : an arbitrary matrix  ) ( axis : the dimension along which the cosine distance be compute  ) ( weights : optional tensor whose rank be either 0  or the same rank as label  and must be broadcastable to label  i e   all dimension must be either 1  or the same as the correspond losses dimension   ) ( scope : the scope for the operations perform in compute the loss  ) ( loss collection : collection to which this loss will be add  ) ( reduction : type of reduction to apply to loss  ) ( dim : the old  deprecate  name for axis  )
( scope : an optional scope name for filter the losses to return  ) ( loss collection : optional losses collection  )
( scope : an optional scope name for filter the losses to return  ) ( name : the name of the return tensor  )
( scope : an optional scope name for filter the losses to return  )
( add regularization losses : a boolean indicate whether or not to use the regularization losses in the sum  ) ( name : the name of the return tensor  ) ( scope : an optional scope name for filter the losses to return  note that this filter the losses add with tf add loss   as well as the regularization losses to that scope  )
( labels : the grind truth output tensor  its shape should match the shape of logits  the value of the tensor be expect to be 0 0 or 1 0  internally the  0 1  label be convert to   1 1  when calculate the hinge loss  ) ( logits : the logits  a float tensor  note that logits be assume to be unbounded and 0 center  a value > 0  resp  < 0  be consider a positive  resp  negative  binary prediction  ) ( weights : optional tensor whose rank be either 0  or the same rank as label  and must be broadcastable to label  i e   all dimension must be either 1  or the same as the correspond losses dimension   ) ( scope : the scope for the operations perform in compute the loss  ) ( loss collection : collection to which the loss will be add  ) ( reduction : type of reduction to apply to loss  )
( labels : the grind truth output tensor  same dimension as 'predictions'  ) ( predictions : the predict output  ) ( weights : optional tensor whose rank be either 0  or the same rank as label  and must be broadcastable to label  i e   all dimension must be either 1  or the same as the correspond losses dimension   ) ( delta : float  the point where the huber loss function change from a quadratic to linear  ) ( scope : the scope for the operations perform in compute the loss  ) ( loss collection : collection to which the loss will be add  ) ( reduction : type of reduction to apply to loss  )
( labels : the grind truth output tensor  same dimension as 'predictions'  ) ( predictions : the predict output  ) ( weights : optional tensor whose rank be either 0  or the same rank as label  and must be broadcastable to label  i e   all dimension must be either 1  or the same as the correspond losses dimension   ) ( epsilon : a small increment to add to avoid take a log of zero  ) ( scope : the scope for the operations perform in compute the loss  ) ( loss collection : collection to which the loss will be add  ) ( reduction : type of reduction to apply to loss  )
( labels : the grind truth output tensor  whose shape must match the shape of predictions  ) ( predictions : the predict output  a tensor of size  batch size  d0     dn  where n 1 be the total number of dimension in predictions  ) ( weights : coefficients for the loss a scalar  a tensor of shape  batch size  or a tensor whose shape match predictions  ) ( scope : the scope for the operations perform in compute the loss  ) ( loss collection : collection to which the loss will be add  )
( labels : the grind truth output tensor  same dimension as 'predictions'  ) ( predictions : the predict output  ) ( weights : optional tensor whose rank be either 0  or the same rank as label  and must be broadcastable to label  i e   all dimension must be either 1  or the same as the correspond losses dimension   ) ( scope : the scope for the operations perform in compute the loss  ) ( loss collection : collection to which the loss will be add  ) ( reduction : type of reduction to apply to loss  )
( multi class labels :  batch size  num class  target integer label in  0  1   ) ( logits : float  batch size  num class  logits output of the network  ) ( weights : optional tensor whose rank be either 0  or the same rank as multi class label  and must be broadcastable to multi class label  i e   all dimension must be either 1  or the same as the correspond losses dimension   ) ( label smoothing : if greater than 0 then smooth the label  ) ( scope : the scope for the operations perform in compute the loss  ) ( loss collection : collection to which the loss will be add  ) ( reduction : type of reduction to apply to loss  )
( onehot labels : one hot encode label  ) ( logits : logits output of the network  ) ( weights : optional tensor that be broadcastable to loss  ) ( label smoothing : if greater than 0 then smooth the label  ) ( scope : the scope for the operations perform in compute the loss  ) ( loss collection : collection to which the loss will be add  ) ( reduction : type of reduction to apply to loss  )
( labels : tensor of shape  d 0  d 1       d  r 1    where r be rank of label and result  and dtype int32 or int64  each entry in label must be an index in  0  num class   other value will raise an exception when this op be run on cpu  and return nan for correspond loss and gradient row on gpu  ) ( logits : unscaled log probabilities of shape  d 0  d 1       d  r 1   num class  and dtype float16  float32 or float64  ) ( weights : coefficients for the loss  this must be scalar or broadcastable to label  i e  same rank and each dimension be either 1 or the same   ) ( scope : the scope for the operations perform in compute the loss  ) ( loss collection : collection to which the loss will be add  ) ( reduction : type of reduction to apply to loss  )
( predictions : a tensor of type float32  a batch size x class tensor  ) ( targets : a tensor  must be one of the follow type  int32  int64  a batch size vector of class ids  ) ( k : an int  number of top elements to look at for compute precision  ) ( name : a name for the operation  optional   )
( logits : a non empty tensor  must be one of the follow type  half  float32  float64  ) ( axis : the dimension softmax would be perform on  the default be  1 which indicate the last dimension  ) ( name : a name for the operation  optional   ) ( dim : deprecate alias for axis  )
( logits : a non empty tensor  must be one of the follow type  half  float32  float64  ) ( axis : the dimension softmax would be perform on  the default be  1 which indicate the last dimension  ) ( name : a name for the operation  optional   )
( labels : the grind truth value  a tensor whose shape match predictions  ) ( predictions : the predict value  a tensor of any shape  ) ( weights : optional tensor whose rank be either 0  or the same rank as label  and must be broadcastable to label  i e   all dimension must be either 1  or the same as the correspond label dimension   ) ( metrics collections : an optional list of collections that accuracy should be add to  ) ( updates collections : an optional list of collections that update op should be add to  ) ( name : an optional variable scope name  )
( labels : a tensor whose shape match predictions  will be cast to bool  ) ( predictions : a float point tensor of arbitrary shape and whose value be in the range  0  1   ) ( weights : optional tensor whose rank be either 0  or the same rank as label  and must be broadcastable to label  i e   all dimension must be either 1  or the same as the correspond label dimension   ) ( num thresholds : the number of thresholds to use when discretizing the roc curve  ) ( metrics collections : an optional list of collections that auc should be add to  ) ( updates collections : an optional list of collections that update op should be add to  ) ( curve : specify the name of the curve to be compute  'roc'  default  or 'pr' for the precision recall curve  ) ( name : an optional variable scope name  ) ( summation method : specify the riemann summation method use  https //en wikipedia org/wiki/riemann sum   'trapezoidal'  default  that apply the trapezoidal rule  'careful interpolation'  a variant of it differ only by a more correct interpolation scheme for pr auc   interpolate  true/false  positives but not the ratio that be precision  'minoring' that apply leave summation for increase intervals and right summation for decrease intervals  'majoring' that do the opposite  note that 'careful interpolation' be strictly prefer to 'trapezoidal'  to be deprecate soon  as it apply the same method for roc  and a better one  see davis   goadrich 2006 for detail  for the pr curve  ) ( thresholds : an optional list of float point value to use as the thresholds for discretizing the curve  if set  the num thresholds parameter be ignore  value should be in  0  1   endpoint thresholds equal to   epsilon  1 epsilon  for a small positive epsilon value will be automatically include with these to correctly handle predictions equal to  exactly 0 or 1  )
( labels : int64 tensor or sparsetensor with shape  d1      dn  num label  or  d1      dn   where the latter imply num label 1  n >  1 and num label be the number of target class for the associate prediction  commonly  n 1 and label have shape  batch size  num label    d1      dn  must match predictions  value should be in range  0  num class   where num class be the last dimension of predictions  value outside this range be ignore  ) ( predictions : float tensor with shape  d1      dn  num class  where n >  1  commonly  n 1 and predictions have shape  batch size  num class   the final dimension contain the logit value for each class   d1      dn  must match label  ) ( k : integer  k for  k metric  this will calculate an average precision for range  1 k   as document above  ) ( weights : tensor whose rank be either 0  or n 1  where n be the rank of label  if the latter  it must be broadcastable to label  i e   all dimension must be either 1  or the same as the correspond label dimension   ) ( metrics collections : an optional list of collections that value should be add to  ) ( updates collections : an optional list of collections that update should be add to  ) ( name : name of new update operation  and namespace for other dependent ops  )
( labels : the grind truth value  a tensor whose dimension must match predictions  will be cast to bool  ) ( predictions : the predict value  a tensor of arbitrary dimension  will be cast to bool  ) ( weights : optional tensor whose rank be either 0  or the same rank as label  and must be broadcastable to label  i e   all dimension must be either 1  or the same as the correspond label dimension   ) ( metrics collections : an optional list of collections that the metric value variable should be add to  ) ( updates collections : an optional list of collections that the metric update ops should be add to  ) ( name : an optional variable scope name  )
( labels : a tensor whose shape match predictions  will be cast to bool  ) ( predictions : a float point tensor of arbitrary shape and whose value be in the range  0  1   ) ( thresholds : a python list or tuple of float thresholds in  0  1   ) ( weights : optional tensor whose rank be either 0  or the same rank as label  and must be broadcastable to label  i e   all dimension must be either 1  or the same as the correspond label dimension   ) ( metrics collections : an optional list of collections that false negative should be add to  ) ( updates collections : an optional list of collections that update op should be add to  ) ( name : an optional variable scope name  )
( labels : the grind truth value  a tensor whose dimension must match predictions  will be cast to bool  ) ( predictions : the predict value  a tensor of arbitrary dimension  will be cast to bool  ) ( weights : optional tensor whose rank be either 0  or the same rank as label  and must be broadcastable to label  i e   all dimension must be either 1  or the same as the correspond label dimension   ) ( metrics collections : an optional list of collections that the metric value variable should be add to  ) ( updates collections : an optional list of collections that the metric update ops should be add to  ) ( name : an optional variable scope name  )
( labels : a tensor whose shape match predictions  will be cast to bool  ) ( predictions : a float point tensor of arbitrary shape and whose value be in the range  0  1   ) ( thresholds : a python list or tuple of float thresholds in  0  1   ) ( weights : optional tensor whose rank be either 0  or the same rank as label  and must be broadcastable to label  i e   all dimension must be either 1  or the same as the correspond label dimension   ) ( metrics collections : an optional list of collections that false positives should be add to  ) ( updates collections : an optional list of collections that update op should be add to  ) ( name : an optional variable scope name  )
( values : a tensor of arbitrary dimension  ) ( weights : optional tensor whose rank be either 0  or the same rank as value  and must be broadcastable to value  i e   all dimension must be either 1  or the same as the correspond value dimension   ) ( metrics collections : an optional list of collections that mean should be add to  ) ( updates collections : an optional list of collections that update op should be add to  ) ( name : an optional variable scope name  )
( labels : a tensor of the same shape as predictions  ) ( predictions : a tensor of arbitrary shape  ) ( weights : optional tensor whose rank be either 0  or the same rank as label  and must be broadcastable to label  i e   all dimension must be either 1  or the same as the correspond label dimension   ) ( metrics collections : an optional list of collections that mean absolute error should be add to  ) ( updates collections : an optional list of collections that update op should be add to  ) ( name : an optional variable scope name  )
( labels : a tensor of arbitrary shape  ) ( predictions : a tensor of the same shape as label  ) ( dim : the dimension along which the cosine distance be compute  ) ( weights : optional tensor whose rank be either 0  or the same rank as label  and must be broadcastable to label  i e   all dimension must be either 1  or the same as the correspond label dimension   also  dimension dim must be 1  ) ( metrics collections : an optional list of collections that the metric value variable should be add to  ) ( updates collections : an optional list of collections that the metric update ops should be add to  ) ( name : an optional variable scope name  )
( labels : a tensor of grind truth label with shape  batch size  and of type int32 or int64  the tensor will be flatten if its rank > 1  ) ( predictions : a tensor of prediction result for semantic label  whose shape be  batch size  and type int32 or int64  the tensor will be flatten if its rank > 1  ) ( num classes : the possible number of label the prediction task can have  this value must be provide  since a confusion matrix of dimension    num class  num class  will be allocate  ) ( weights : optional tensor whose rank be either 0  or the same rank as label  and must be broadcastable to label  i e   all dimension must be either 1  or the same as the correspond label dimension   ) ( metrics collections : an optional list of collections that mean iou should be add to  ) ( updates collections : an optional list of collections update op should be add to  ) ( name : an optional variable scope name  )
( labels : a tensor of grind truth label with shape  batch size  and of type int32 or int64  the tensor will be flatten if its rank > 1  ) ( predictions : a tensor of prediction result for semantic label  whose shape be  batch size  and type int32 or int64  the tensor will be flatten if its rank > 1  ) ( num classes : the possible number of label the prediction task can have  this value must be provide  since two variables with shape    num class  will be allocate  ) ( weights : optional tensor whose rank be either 0  or the same rank as label  and must be broadcastable to label  i e   all dimension must be either 1  or the same as the correspond label dimension   )
( labels : a tensor of the same shape as predictions  ) ( predictions : a tensor of arbitrary shape  ) ( normalizer : a tensor of the same shape as predictions  ) ( weights : optional tensor whose rank be either 0  or the same rank as label  and must be broadcastable to label  i e   all dimension must be either 1  or the same as the correspond label dimension   ) ( metrics collections : an optional list of collections that mean relative error should be add to  ) ( updates collections : an optional list of collections that update op should be add to  ) ( name : an optional variable scope name  )
( labels : a tensor of the same shape as predictions  ) ( predictions : a tensor of arbitrary shape  ) ( weights : optional tensor whose rank be either 0  or the same rank as label  and must be broadcastable to label  i e   all dimension must be either 1  or the same as the correspond label dimension   ) ( metrics collections : an optional list of collections that mean square error should be add to  ) ( updates collections : an optional list of collections that update op should be add to  ) ( name : an optional variable scope name  )
( values : a tensor of arbitrary dimension  ) ( weights : optional tensor whose rank be either 0  or the same rank as value  and must be broadcastable to value  i e   all dimension must be either 1  or the same as the correspond value dimension   ) ( metrics collections : an optional list of collections that mean should be add to  ) ( updates collections : an optional list of collections that update op should be add to  ) ( name : an optional variable scope name  )
( values : a numeric tensor of arbitrary size  ) ( threshold : a scalar threshold  ) ( weights : optional tensor whose rank be either 0  or the same rank as value  and must be broadcastable to value  i e   all dimension must be either 1  or the same as the correspond value dimension   ) ( metrics collections : an optional list of collections that the metric value variable should be add to  ) ( updates collections : an optional list of collections that the metric update ops should be add to  ) ( name : an optional variable scope name  )
( labels : the grind truth value  a tensor whose dimension must match predictions  will be cast to bool  ) ( predictions : the predict value  a tensor of arbitrary dimension  will be cast to bool  ) ( weights : optional tensor whose rank be either 0  or the same rank as label  and must be broadcastable to label  i e   all dimension must be either 1  or the same as the correspond label dimension   ) ( metrics collections : an optional list of collections that precision should be add to  ) ( updates collections : an optional list of collections that update op should be add to  ) ( name : an optional variable scope name  )
( labels : int64 tensor or sparsetensor with shape  d1      dn  num label  or  d1      dn   where the latter imply num label 1  n >  1 and num label be the number of target class for the associate prediction  commonly  n 1 and label have shape  batch size  num label    d1      dn  must match predictions  value should be in range  0  num class   where num class be the last dimension of predictions  value outside this range be ignore  ) ( predictions : float tensor with shape  d1      dn  num class  where n >  1  commonly  n 1 and predictions have shape  batch size  num class   the final dimension contain the logit value for each class   d1      dn  must match label  ) ( k : integer  k for  k metric  ) ( class id : integer class id for which we want binary metrics  this should be in range  0  num class   where num class be the last dimension of predictions  if class id be outside this range  the method return nan  ) ( weights : tensor whose rank be either 0  or n 1  where n be the rank of label  if the latter  it must be broadcastable to label  i e   all dimension must be either 1  or the same as the correspond label dimension   ) ( metrics collections : an optional list of collections that value should be add to  ) ( updates collections : an optional list of collections that update should be add to  ) ( name : name of new update operation  and namespace for other dependent ops  )
( labels : the grind truth value  a tensor whose dimension must match predictions  will be cast to bool  ) ( predictions : a float point tensor of arbitrary shape and whose value be in the range  0  1   ) ( thresholds : a python list or tuple of float thresholds in  0  1   ) ( weights : optional tensor whose rank be either 0  or the same rank as label  and must be broadcastable to label  i e   all dimension must be either 1  or the same as the correspond label dimension   ) ( metrics collections : an optional list of collections that auc should be add to  ) ( updates collections : an optional list of collections that update op should be add to  ) ( name : an optional variable scope name  )
( labels : int64 tensor or sparsetensor with shape  d1      dn  num label  or  d1      dn   where the latter imply num label 1  n >  1 and num label be the number of target class for the associate prediction  commonly  n 1 and label have shape  batch size  num label    d1      dn  must match predictions  value should be in range  0  num class   where num class be the last dimension of predictions  value outside this range be ignore  ) ( predictions idx : integer tensor with shape  d1      dn  k  where n >  1  commonly  n 1 and predictions have shape  batch size  k   the final dimension contain the top k predict class indices   d1      dn  must match label  ) ( k : integer  k for  k metric  only use for the default op name  ) ( class id : integer class id for which we want binary metrics  this should be in range  0  num class   where num class be the last dimension of predictions  if class id be outside this range  the method return nan  ) ( weights : tensor whose rank be either 0  or n 1  where n be the rank of label  if the latter  it must be broadcastable to label  i e   all dimension must be either 1  or the same as the correspond label dimension   ) ( metrics collections : an optional list of collections that value should be add to  ) ( updates collections : an optional list of collections that update should be add to  ) ( name : name of new update operation  and namespace for other dependent ops  )
( labels : the grind truth value  a tensor whose dimension must match predictions  will be cast to bool  ) ( predictions : the predict value  a tensor of arbitrary dimension  will be cast to bool  ) ( weights : optional tensor whose rank be either 0  or the same rank as label  and must be broadcastable to label  i e   all dimension must be either 1  or the same as the correspond label dimension   ) ( metrics collections : an optional list of collections that recall should be add to  ) ( updates collections : an optional list of collections that update op should be add to  ) ( name : an optional variable scope name  )
( labels : int64 tensor or sparsetensor with shape  d1      dn  num label  or  d1      dn   where the latter imply num label 1  n >  1 and num label be the number of target class for the associate prediction  commonly  n 1 and label have shape  batch size  num label    d1      dn  must match predictions  value should be in range  0  num class   where num class be the last dimension of predictions  value outside this range always count towards false negative at <k>  ) ( predictions : float tensor with shape  d1      dn  num class  where n >  1  commonly  n 1 and predictions have shape  batch size  num class   the final dimension contain the logit value for each class   d1      dn  must match label  ) ( k : integer  k for  k metric  ) ( class id : integer class id for which we want binary metrics  this should be in range  0  num class   where num class be the last dimension of predictions  if class id be outside this range  the method return nan  ) ( weights : tensor whose rank be either 0  or n 1  where n be the rank of label  if the latter  it must be broadcastable to label  i e   all dimension must be either 1  or the same as the correspond label dimension   ) ( metrics collections : an optional list of collections that value should be add to  ) ( updates collections : an optional list of collections that update should be add to  ) ( name : name of new update operation  and namespace for other dependent ops  )
( labels : the grind truth value  a tensor whose dimension must match predictions  will be cast to bool  ) ( predictions : a float point tensor of arbitrary shape and whose value be in the range  0  1   ) ( thresholds : a python list or tuple of float thresholds in  0  1   ) ( weights : optional tensor whose rank be either 0  or the same rank as label  and must be broadcastable to label  i e   all dimension must be either 1  or the same as the correspond label dimension   ) ( metrics collections : an optional list of collections that recall should be add to  ) ( updates collections : an optional list of collections that update op should be add to  ) ( name : an optional variable scope name  )
( labels : int64 tensor or sparsetensor with shape  d1      dn  num label  or  d1      dn   where the latter imply num label 1  n >  1 and num label be the number of target class for the associate prediction  commonly  n 1 and label have shape  batch size  num label    d1      dn  must match predictions  value should be in range  0  num class   where num class be the last dimension of predictions  value outside this range always count towards false negative at <k>  ) ( predictions idx : integer tensor with shape  d1      dn  k  where n >  1  commonly  n 1 and predictions have shape  batch size  k   the final dimension contain the top k predict class indices   d1      dn  must match label  ) ( k : integer  k for  k metric  only use for the default op name  ) ( class id : integer class id for which we want binary metrics  this should be in range  0  num class   where num class be the last dimension of predictions  if class id be outside this range  the method return nan  ) ( weights : tensor whose rank be either 0  or n 1  where n be the rank of label  if the latter  it must be broadcastable to label  i e   all dimension must be either 1  or the same as the correspond label dimension   ) ( metrics collections : an optional list of collections that value should be add to  ) ( updates collections : an optional list of collections that update should be add to  ) ( name : name of new update operation  and namespace for other dependent ops  )
( labels : a tensor of the same shape as predictions  ) ( predictions : a tensor of arbitrary shape  ) ( weights : optional tensor whose rank be either 0  or the same rank as label  and must be broadcastable to label  i e   all dimension must be either 1  or the same as the correspond label dimension   ) ( metrics collections : an optional list of collections that root mean square error should be add to  ) ( updates collections : an optional list of collections that update op should be add to  ) ( name : an optional variable scope name  )
( labels : the grind truth value  a tensor whose dimension must match predictions  will be cast to bool  ) ( predictions : a float point tensor of arbitrary shape and whose value be in the range  0  1   ) ( specificity : a scalar value in range  0  1   ) ( weights : optional tensor whose rank be either 0  or the same rank as label  and must be broadcastable to label  i e   all dimension must be either 1  or the same as the correspond label dimension   ) ( num thresholds : the number of thresholds to use for match the give specificity  ) ( metrics collections : an optional list of collections that sensitivity should be add to  ) ( updates collections : an optional list of collections that update op should be add to  ) ( name : an optional variable scope name  )


( labels : the grind truth value  a tensor whose dimension must match predictions  will be cast to bool  ) ( predictions : a float point tensor of arbitrary shape and whose value be in the range  0  1   ) ( sensitivity : a scalar value in range  0  1   ) ( weights : optional tensor whose rank be either 0  or the same rank as label  and must be broadcastable to label  i e   all dimension must be either 1  or the same as the correspond label dimension   ) ( num thresholds : the number of thresholds to use for match the give sensitivity  ) ( metrics collections : an optional list of collections that specificity should be add to  ) ( updates collections : an optional list of collections that update op should be add to  ) ( name : an optional variable scope name  )
( labels : the grind truth value  a tensor whose dimension must match predictions  will be cast to bool  ) ( predictions : the predict value  a tensor of arbitrary dimension  will be cast to bool  ) ( weights : optional tensor whose rank be either 0  or the same rank as label  and must be broadcastable to label  i e   all dimension must be either 1  or the same as the correspond label dimension   ) ( metrics collections : an optional list of collections that the metric value variable should be add to  ) ( updates collections : an optional list of collections that the metric update ops should be add to  ) ( name : an optional variable scope name  )
( labels : a tensor whose shape match predictions  will be cast to bool  ) ( predictions : a float point tensor of arbitrary shape and whose value be in the range  0  1   ) ( thresholds : a python list or tuple of float thresholds in  0  1   ) ( weights : optional tensor whose rank be either 0  or the same rank as label  and must be broadcastable to label  i e   all dimension must be either 1  or the same as the correspond label dimension   ) ( metrics collections : an optional list of collections that true negative should be add to  ) ( updates collections : an optional list of collections that update op should be add to  ) ( name : an optional variable scope name  )
( labels : the grind truth value  a tensor whose dimension must match predictions  will be cast to bool  ) ( predictions : the predict value  a tensor of arbitrary dimension  will be cast to bool  ) ( weights : optional tensor whose rank be either 0  or the same rank as label  and must be broadcastable to label  i e   all dimension must be either 1  or the same as the correspond label dimension   ) ( metrics collections : an optional list of collections that the metric value variable should be add to  ) ( updates collections : an optional list of collections that the metric update ops should be add to  ) ( name : an optional variable scope name  )
( labels : a tensor whose shape match predictions  will be cast to bool  ) ( predictions : a float point tensor of arbitrary shape and whose value be in the range  0  1   ) ( thresholds : a python list or tuple of float thresholds in  0  1   ) ( weights : optional tensor whose rank be either 0  or the same rank as label  and must be broadcastable to label  i e   all dimension must be either 1  or the same as the correspond label dimension   ) ( metrics collections : an optional list of collections that true positives should be add to  ) ( updates collections : an optional list of collections that update op should be add to  ) ( name : an optional variable scope name  )
( value : a 4 d tensor of shape  batch  height  width  channel  and type float32  float64  qint8  quint8  or qint32  ) ( ksize : an int or list of ints that have length 1  2 or 4  the size of the window for each dimension of the input tensor  ) ( strides : an int or list of ints that have length 1  2 or 4  the stride of the slide window for each dimension of the input tensor  ) ( padding : a string  either 'valid' or 'same'  the pad algorithm  see the "returns" section of tf nn convolution for detail  ) ( data format : a string  'nhwc' and 'nchw' be support  ) ( name : optional name for the operation  ) ( input : alias for value  )
( t : a 4d input tensor  ) ( m : a 1d mean tensor with size match the last dimension of t  this be the first output from tf nn moments  or a save move average thereof  ) ( v : a 1d variance tensor with size match the last dimension of t  this be the second output from tf nn moments  or a save move average thereof  ) ( beta : a 1d beta tensor with size match the last dimension of t  an offset to be add to the normalize tensor  ) ( gamma : a 1d gamma tensor with size match the last dimension of t  if "scale after normalization" be true  this tensor will be multiply with the normalize tensor  ) ( variance epsilon : a small float number to avoid divide by 0  ) ( scale after normalization : a bool indicate whether the result tensor need to be multiply with gamma  ) ( name : a name for this operation  optional   ) ( input : alias for t  ) ( mean : alias for m  ) ( variance : alias for v  )
( cell fw : an instance of rnncell  to be use for forward direction  ) ( cell bw : an instance of rnncell  to be use for backward direction  ) ( inputs : the rnn input  if time major    false  default   this must be a tensor of shape     batch size  max time        or a nest tuple of such elements  if time major    true  this must be a tensor of shape   max time    batch size        or a nest tuple of such elements  ) ( sequence length :  optional  an int32/int64 vector  size  batch size   contain the actual lengths for each of the sequence in the batch  if not provide  all batch entries be assume to be full sequence  and time reversal be apply from time 0 to max time for each sequence  ) ( initial state fw :  optional  an initial state for the forward rnn  this must be a tensor of appropriate type and shape  batch size  cell fw state size   if cell fw state size be a tuple  this should be a tuple of tensors have shape  batch size  s  for s in cell fw state size  ) ( initial state bw :  optional  same as for initial state fw  but use the correspond properties of cell bw  ) ( dtype :  optional  the data type for the initial state and expect output  require if initial state be not provide or rnn state have a heterogeneous dtype  ) ( parallel iterations :  default  32    the number of iterations to run in parallel   those operations which do not have any temporal dependency and can be run in parallel  will be   this parameter trade off time for space   value >> 1 use more memory but take less time  while smaller value use less memory but computations take longer  ) ( swap memory : transparently swap the tensors produce in forward inference but need for back prop from gpu to cpu   this allow train rnns which would typically not fit on a single gpu  with very minimal  or no  performance penalty  ) ( time major : the shape format of the input and output tensors  if true  these tensors must be shape  max time  batch size  depth   if false  these tensors must be shape  batch size  max time  depth   use time major   true be a bite more efficient because it avoid transpose at the begin and end of the rnn calculation   however  most tensorflow data be batch major  so by default this function accept input and emit output in batch major form  ) ( scope : variablescope for the create subgraph  default to "bidirectional rnn" )
( value : a tensor of rank at least 3  must be of type float16  float32  or float64  ) ( filters : a tensor of rank at least 3   must have the same type as value  ) ( stride : an int or list of ints that have length 1 or 3   the number of entries by which the filter be move right at each step  ) ( padding : 'same' or 'valid' ) ( use cudnn on gpu : an optional bool   default to true  ) ( data format : an optional string from "nwc"  "ncw"   default to "nwc"  the data be store in the order of batch shape    in width  in channel    the "ncw" format store data as batch shape    in channel  in width   ) ( name : a name for the operation  optional   ) ( input : alias for value  ) ( dilations : an int or list of ints that have length 1 or 3 which default to 1  the dilation factor for each dimension of input  if set to k > 1  there will be k 1 skip cells between each filter element on that dimension  dilations in the batch and depth dimension must be 1  )
( input : a tensor  must be one of the follow type  half  bfloat16  float32  float64  a 4 d tensor  the dimension order be interpret accord to the value of data format  see below for detail  ) ( filter : a tensor  must have the same type as input  a 4 d tensor of shape  filter height  filter width  in channel  out channel  ) ( strides : an int or list of ints that have length 1  2 or 4   the stride of the slide window for each dimension of input  if a single value be give it be replicate in the h and w dimension  by default the n and c dimension be set to 1  the dimension order be determine by the value of data format  see below for detail  ) ( padding : either the string "same" or "valid" indicate the type of pad algorithm to use  or a list indicate the explicit paddings at the start and end of each dimension  when explicit pad be use and data format be "nhwc"  this should be in the form   0  0    pad top  pad bottom    pad leave  pad right    0  0    when explicit pad use and data format be "nchw"  this should be in the form   0  0    0  0    pad top  pad bottom    pad leave  pad right    ) ( use cudnn on gpu : an optional bool  default to true  ) ( data format : an optional string from  "nhwc"  "nchw"  default to "nhwc"  specify the data format of the input and output data  with the default format "nhwc"  the data be store in the order of       batch  height  width  channel   alternatively  the format could be "nchw"  the data storage order of       batch  channel  height  width   ) ( dilations : an int or list of ints that have length 1  2 or 4  default to 1  the dilation factor for each dimension ofinput  if a single value be give it be replicate in the h and w dimension  by default the n and c dimension be set to 1  if set to k > 1  there will be k 1 skip cells between each filter element on that dimension  the dimension order be determine by the value of data format  see above for detail  dilations in the batch and depth dimension if a 4 d tensor must be 1  ) ( name : a name for the operation  optional   ) ( filters : alias for filter  )
( input : a tensor  must be one of the follow type  half  bfloat16  float32  float64  4 d with shape  batch  in height  in width  in channel   ) ( filter sizes : a tensor of type int32  an integer vector represent the tensor shape of filter  where filter be a 4 d  filter height  filter width  in channel  out channel  tensor  ) ( out backprop : a tensor  must have the same type as input  4 d with shape  batch  out height  out width  out channel   gradients w r t  the output of the convolution  ) ( strides : a list of ints  the stride of the slide window for each dimension of the input of the convolution  must be in the same order as the dimension specify with format  ) ( padding : either the string "same" or "valid" indicate the type of pad algorithm to use  or a list indicate the explicit paddings at the start and end of each dimension  when explicit pad be use and data format be "nhwc"  this should be in the form   0  0    pad top  pad bottom    pad leave  pad right    0  0    when explicit pad use and data format be "nchw"  this should be in the form   0  0    0  0    pad top  pad bottom    pad leave  pad right    ) ( use cudnn on gpu : an optional bool  default to true  ) ( data format : an optional string from  "nhwc"  "nchw"  default to "nhwc"  specify the data format of the input and output data  with the default format "nhwc"  the data be store in the order of       batch  in height  in width  in channel   alternatively  the format could be "nchw"  the data storage order of       batch  in channel  in height  in width   ) ( dilations : an optional list of ints  default to  1  1  1  1   1 d tensor of length 4   the dilation factor for each dimension of input  if set to k > 1  there will be k 1 skip cells between each filter element on that dimension  the dimension order be determine by the value of data format  see above for detail  dilations in the batch and depth dimension must be 1  ) ( name : a name for the operation  optional   )
( input sizes : a tensor of type int32  an integer vector represent the shape of input  where input be a 4 d  batch  height  width  channel  tensor  ) ( filter : a tensor  must be one of the follow type  half  bfloat16  float32  float64  4 d with shape  filter height  filter width  in channel  out channel   ) ( out backprop : a tensor  must have the same type as filter  4 d with shape  batch  out height  out width  out channel   gradients w r t  the output of the convolution  ) ( strides : a list of ints  the stride of the slide window for each dimension of the input of the convolution  must be in the same order as the dimension specify with format  ) ( padding : either the string "same" or "valid" indicate the type of pad algorithm to use  or a list indicate the explicit paddings at the start and end of each dimension  when explicit pad be use and data format be "nhwc"  this should be in the form   0  0    pad top  pad bottom    pad leave  pad right    0  0    when explicit pad use and data format be "nchw"  this should be in the form   0  0    0  0    pad top  pad bottom    pad leave  pad right    ) ( use cudnn on gpu : an optional bool  default to true  ) ( data format : an optional string from  "nhwc"  "nchw"  default to "nhwc"  specify the data format of the input and output data  with the default format "nhwc"  the data be store in the order of       batch  in height  in width  in channel   alternatively  the format could be "nchw"  the data storage order of       batch  in channel  in height  in width   ) ( dilations : an optional list of ints  default to  1  1  1  1   1 d tensor of length 4   the dilation factor for each dimension of input  if set to k > 1  there will be k 1 skip cells between each filter element on that dimension  the dimension order be determine by the value of data format  see above for detail  dilations in the batch and depth dimension must be 1  ) ( name : a name for the operation  optional   ) ( filters : alias for filter  )
( value : a 4 d tensor of type float and shape  batch  height  width  in channel  for nhwc data format or  batch  in channel  height  width  for nchw data format  ) ( filter : a 4 d tensor with the same type as value and shape  height  width  output channel  in channel    filter's in channel dimension must match that of value  ) ( output shape : a 1 d tensor represent the output shape of the deconvolution op  ) ( strides : an int or list of ints that have length 1  2 or 4   the stride of the slide window for each dimension of input  if a single value be give it be replicate in the h and w dimension  by default the n and c dimension be set to 0  the dimension order be determine by the value of data format  see below for detail  ) ( padding : a string  either 'valid' or 'same'  the pad algorithm  see the "returns" section of tf nn convolution for detail  ) ( data format : a string  'nhwc' and 'nchw' be support  ) ( name : optional name for the return tensor  ) ( input : alias for value  ) ( filters : alias for filter  ) ( dilations : an int or list of ints that have length 1  2 or 4  default to 1  the dilation factor for each dimension ofinput  if a single value be give it be replicate in the h and w dimension  by default the n and c dimension be set to 1  if set to k > 1  there will be k 1 skip cells between each filter element on that dimension  the dimension order be determine by the value of data format  see above for detail  dilations in the batch and depth dimension if a 4 d tensor must be 1  )
( input : a tensor  must be one of the follow type  half  bfloat16  float32  float64  shape  batch  in depth  in height  in width  in channel   ) ( filter : a tensor  must have the same type as input  shape  filter depth  filter height  filter width  in channel  out channel   in channel must match between input and filter  ) ( strides : a list of ints that have length >  5  1 d tensor of length 5  the stride of the slide window for each dimension of input  must have stride 0    stride 4    1  ) ( padding : a string from  "same"  "valid"  the type of pad algorithm to use  ) ( data format : an optional string from  "ndhwc"  "ncdhw"  default to "ndhwc"  the data format of the input and output data  with the default format "ndhwc"  the data be store in the order of       batch  in depth  in height  in width  in channel   alternatively  the format could be "ncdhw"  the data storage order be       batch  in channel  in depth  in height  in width   ) ( dilations : an optional list of ints  default to  1  1  1  1  1   1 d tensor of length 5   the dilation factor for each dimension of input  if set to k > 1  there will be k 1 skip cells between each filter element on that dimension  the dimension order be determine by the value of data format  see above for detail  dilations in the batch and depth dimension must be 1  ) ( name : a name for the operation  optional   )
( input : a tensor  must be one of the follow type  half  bfloat16  float32  float64  shape  batch  depth  row  cols  in channel   ) ( filter sizes : a tensor of type int32  an integer vector represent the tensor shape of filter  where filter be a 5 d  filter depth  filter height  filter width  in channel  out channel  tensor  ) ( out backprop : a tensor  must have the same type as input  backprop signal of shape  batch  out depth  out row  out cols  out channel   ) ( strides : a list of ints that have length >  5  1 d tensor of length 5  the stride of the slide window for each dimension of input  must have stride 0    stride 4    1  ) ( padding : a string from  "same"  "valid"  the type of pad algorithm to use  ) ( data format : an optional string from  "ndhwc"  "ncdhw"  default to "ndhwc"  the data format of the input and output data  with the default format "ndhwc"  the data be store in the order of       batch  in depth  in height  in width  in channel   alternatively  the format could be "ncdhw"  the data storage order be       batch  in channel  in depth  in height  in width   ) ( dilations : an optional list of ints  default to  1  1  1  1  1   1 d tensor of length 5   the dilation factor for each dimension of input  if set to k > 1  there will be k 1 skip cells between each filter element on that dimension  the dimension order be determine by the value of data format  see above for detail  dilations in the batch and depth dimension must be 1  ) ( name : a name for the operation  optional   )
( value : a 5 d tensor of type float and shape  batch  depth  height  width  in channel   ) ( filter : a 5 d tensor with the same type as value and shape  depth  height  width  output channel  in channel    filter's in channel dimension must match that of value  ) ( output shape : a 1 d tensor represent the output shape of the deconvolution op  ) ( strides : a list of ints  the stride of the slide window for each dimension of the input tensor  ) ( padding : a string  either 'valid' or 'same'  the pad algorithm  see the "returns" section of tf nn convolution for detail  ) ( data format : a string  either 'ndhwc' or 'ncdhw' specify the layout of the input and output tensors  default to 'ndhwc'  ) ( name : optional name for the return tensor  ) ( input : alias of value  ) ( filters : alias of filter  ) ( dilations : an int or list of ints that have length 1  3 or 5  default to 1  the dilation factor for each dimension ofinput  if a single value be give it be replicate in the d  h and w dimension  by default the n and c dimension be set to 1  if set to k > 1  there will be k 1 skip cells between each filter element on that dimension  the dimension order be determine by the value of data format  see above for detail  dilations in the batch and depth dimension if a 5 d tensor must be 1  )
( input : an  n 2  d tensor of type t  of shape  batch size    input spatial shape    in channel  if data format do not start with "nc"  default   or  batch size  in channel    input spatial shape if data format start with "nc"  ) ( filter : an  n 2  d tensor with the same type as input and shape spatial filter shape    in channel  out channel   ) ( padding : a string  either "valid" or "same"  the pad algorithm  "valid" mean no pad  "same" result in pad evenly to the left/right or up/down of the input such that output have the same height/width dimension as the input when the stride be 1  see here for more information  ) ( strides : optional   sequence of n ints >  1   specify the output stride  default to  1  n   if any value of stride be > 1  then all value of dilation rate must be 1  ) ( dilation rate : optional   sequence of n ints >  1   specify the filter upsampling/input downsampling rate   in the literature  the same parameter be sometimes call input stride or dilation   the effective filter size use for the convolution will be spatial filter shape    spatial filter shape   1     rate   1   obtain by insert  dilation rate i  1  zero between consecutive elements of the original filter in each spatial dimension i   if any value of dilation rate be > 1  then all value of stride must be 1  ) ( name : optional name for the return tensor  ) ( data format : a string or none   specify whether the channel dimension of the input and output be the last dimension  default  or if data format do not start with "nc"   or the second dimension  if data format start with "nc"    for n 1  the valid value be "nwc"  default  and "ncw"   for n 2  the valid value be "nhwc"  default  and "nchw"  for n 3  the valid value be "ndhwc"  default  and "ncdhw"  )
( features : a tensor with type float  double  int32  int64  uint8  int16  or int8  ) ( name : a name for the operation  optional   ) ( axis : the axis that the output value be concatenate along  default be  1  )
( inputs : 3 d float tensor  size  max time x batch size x num class   the logits  ) ( sequence length : 1 d int32 vector contain sequence lengths  have size  batch size   ) ( beam width : an int scalar >  0  beam search beam width   ) ( top paths : an int scalar >  0  <  beam width  control output size   ) ( merge repeated : boolean   default  true  )
( labels : an int32 sparsetensor  label indices i         b  t  mean label value i  store the id   for  batch b  time t   label value i  must take on value in  0    num label   see core/ops/ctc ops cc for more detail  ) ( inputs : 3 d float tensor  if time major    false  this will be a tensor shape   batch size    max time  num class   if time major    true  default   this will be a tensor shape     max time  batch size  num class   the logits  ) ( sequence length : 1 d int32 vector  size  batch size   the sequence lengths  ) ( preprocess collapse repeated : boolean   default  false  if true  repeat label be collapse prior to the ctc calculation  ) ( ctc merge repeated : boolean   default  true  ) ( ignore longer outputs than inputs : boolean  default  false  if true  sequence with longer output than input will be ignore  ) ( time major : the shape format of the input tensors  if true  these tensors must be shape  max time  batch size  num class   if false  these tensors must be shape  batch size  max time  num class   use time major   true  default  be a bite more efficient because it avoid transpose at the begin of the ctc loss calculation   however  most tensorflow data be batch major  so by this function also accept input in batch major form  ) ( logits : alias for input  )
( labels : tensor of shape  batch size  max label seq length  or sparsetensor ) ( logits : tensor of shape  frame  batch size  num label   if logits time major    false  shape be  batch size  frame  num label   ) ( label length : tensor of shape  batch size   none if label be sparsetensor length of reference label sequence in label  ) ( logit length : tensor of shape  batch size  length of input sequence in logits  ) ( logits time major :  optional  if true  default   logits be shape  time  batch  logits   if false  shape be  batch  time  logits  ) ( unique :  optional  unique label indices as compute by ctc unique label label    if supply  enable a faster  memory efficient implementation on tpu  ) ( blank index :  optional  set the class index to use for the blank label  negative value will start from num class  ie   1 will reproduce the ctc loss behavior of use num class   1 for the blank symbol  there be some memory/performance overhead to switch from the default of 0 as an additional shift copy of the logits may be create  ) ( name : a name for this op  default to "ctc loss dense"  )
( input : 4 d with shape accord to data format  ) ( filter : 4 d with shape  filter height  filter width  in channel  channel multiplier   ) ( strides : 1 d of size 4   the stride of the slide window for each dimension of input  ) ( padding : control how to pad the image before apply the convolution  can be the string "same" or "valid" indicate the type of pad algorithm to use  or a list indicate the explicit paddings at the start and end of each dimension  when explicit pad be use and data format be "nhwc"  this should be in the form   0  0    pad top  pad bottom    pad leave  pad right    0  0    when explicit pad use and data format be "nchw"  this should be in the form   0  0    0  0    pad top  pad bottom    pad leave  pad right    ) ( rate : 1 d of size 2  the dilation rate in which we sample input value across the height and width dimension in atrous convolution  if it be greater than 1  then all value of stride must be 1  ) ( name : a name for this operation  optional   ) ( data format : the data format for input  either "nhwc"  default  or "nchw"  ) ( dilations : alias of rate  )
( input : a tensor  must be one of the follow type  half  bfloat16  float32  float64  ) ( filter : a tensor  must have the same type as input  ) ( strides : a list of ints  1 d of length 4   the stride of the slide window for each dimension of input  ) ( padding : control how to pad the image before apply the convolution  can be the string "same" or "valid" indicate the type of pad algorithm to use  or a list indicate the explicit paddings at the start and end of each dimension  when explicit pad be use and data format be "nhwc"  this should be in the form   0  0    pad top  pad bottom    pad leave  pad right    0  0    when explicit pad use and data format be "nchw"  this should be in the form   0  0    0  0    pad top  pad bottom    pad leave  pad right    ) ( data format : an optional string from  "nhwc"  "nchw"  default to "nhwc"  specify the data format of the input and output data  with the default format "nhwc"  the data be store in the order of   batch  height    width  channel   alternatively  the format could be "nchw"  the data storage order of     batch  channel  height  width   ) ( dilations : an optional list of ints  default to  1  1  1  1   1 d tensor of length 4   the dilation factor for each dimension of input  if set to k > 1  there will be k 1 skip cells between each filter element on that dimension  the dimension order be determine by the value of data format  see above for detail  dilations in the batch and depth dimension must be 1  ) ( name : a name for the operation  optional   )
( input : a tensor  must be one of the follow type  float32  float64  int32  uint8  int16  int8  int64  bfloat16  uint16  half  uint32  uint64  4 d with shape  batch  in height  in width  depth   ) ( filter : a tensor  must have the same type as input  3 d with shape  filter height  filter width  depth   ) ( strides : a list of ints that have length >  4  the stride of the slide window for each dimension of the input tensor  must be   1  stride height  stride width  1   ) ( rates : a list of ints that have length >  4  the input stride for atrous morphological dilation  must be   1  rate height  rate width  1   ) ( padding : a string from  "same"  "valid"  the type of pad algorithm to use  ) ( name : a name for the operation  optional   )
( x : a float point tensor  ) ( keep prob :  deprecate  a deprecate alias for  1 rate   ) ( noise shape : a 1 d integer tensor  represent the shape for randomly generate keep/drop flag  ) ( seed : a python integer  use to create random seed  see tf random set seed for behavior  ) ( name : a name for this operation  optional   ) ( rate : a scalar tensor with the same type as x  the probability that each element of x be discard  )
( cell : an instance of rnncell  ) ( inputs : the rnn input  if time major    false  default   this must be a tensor of shape     batch size  max time        or a nest tuple of such elements  if time major    true  this must be a tensor of shape   max time    batch size        or a nest tuple of such elements  this may also be   a  possibly nest  tuple of tensors satisfy this property   the   first two dimension must match across all the input  but otherwise the   rank and other shape components may differ  in this case  input to   cell at each time step will replicate the structure of these tuples    except for the time dimension  from which the time be take   the input   to cell at each time step will be a tensor or  possibly nest    tuple of tensors each with dimension  batch size        ) ( sequence length :  optional  an int32/int64 vector size  batch size   use to copy through state and zero out output when past a batch element's sequence length   this parameter enable users to extract the last valid state and properly pad output  so it be provide for correctness  ) ( initial state :  optional  an initial state for the rnn  if cell state size be an integer  this must be a tensor of appropriate type and shape  batch size  cell state size   if cell state size be a tuple  this should be a tuple of tensors have shape  batch size  s  for s in cell state size  ) ( dtype :  optional  the data type for the initial state and expect output  require if initial state be not provide or rnn state have a heterogeneous dtype  ) ( parallel iterations :  default  32    the number of iterations to run in parallel   those operations which do not have any temporal dependency and can be run in parallel  will be   this parameter trade off time for space   value >> 1 use more memory but take less time  while smaller value use less memory but computations take longer  ) ( swap memory : transparently swap the tensors produce in forward inference but need for back prop from gpu to cpu   this allow train rnns which would typically not fit on a single gpu  with very minimal  or no  performance penalty  ) ( time major : the shape format of the input and output tensors  if true  these tensors must be shape  max time  batch size  depth   if false  these tensors must be shape  batch size  max time  depth   use time major   true be a bite more efficient because it avoid transpose at the begin and end of the rnn calculation   however  most tensorflow data be batch major  so by default this function accept input and emit output in batch major form  ) ( scope : variablescope for the create subgraph  default to "rnn"  )
( params : a single tensor represent the complete embed tensor  or a list of p tensors all of same shape except for the first dimension  represent sharded embed tensors   alternatively  a partitionedvariable  create by partition along dimension 0  each element must be appropriately size for the give partition strategy  ) ( ids : a tensor or a 'raggedtensor' with type int32 or int64 contain the ids to be look up in params  ) ( partition strategy : a string specify the partition strategy  relevant if len params  > 1  currently "div" and "mod" be support  default be "mod"  ) ( name : a name for the operation  optional   ) ( validate indices : deprecate  if this operation be assign to cpu  value in indices be always validate to be within range   if assign to gpu  out of bind indices result in safe but unspecified behavior  which may include raise an error  ) ( max norm : if not none  each embed be clip if its l2 norm be larger than this value  )
( params : a single tensor represent the complete embed tensor  or a list tensors all of same shape except for the first dimension  represent sharded embed tensors  alternatively  a partitionedvariable  create by partition along dimension 0  each element must be appropriately size for the give partition strategy  ) ( sp ids : n x m sparsetensor of int64 ids where n be typically batch size and m be arbitrary  ) ( sp weights : either a sparsetensor of float / double weight  or none to indicate all weight should be take to be 1  if specify  sp weight must have exactly the same shape and indices as sp ids  ) ( partition strategy : a string specify the partition strategy  relevant if len params  > 1  currently "div" and "mod" be support  default be "mod"  see tf nn embed lookup for more detail  ) ( name : optional name for the op  ) ( combiner : a string specify the reduction op  currently "mean"  "sqrtn" and "sum" be support  "sum" compute the weight sum of the embed result for each row  "mean" be the weight sum divide by the total weight  "sqrtn" be the weight sum divide by the square root of the sum of the square of the weight  default to mean  ) ( max norm : if not none  each embed be clip if its l2 norm be larger than this value  before combine  )
( value : a tensor  4 d with shape  batch  in height  in width  depth   ) ( kernel : a tensor  must have the same type as value  3 d with shape  kernel height  kernel width  depth   ) ( strides : a list of ints that have length >  4  1 d of length 4  the stride of the slide window for each dimension of the input tensor  must be   1  stride height  stride width  1   ) ( rates : a list of ints that have length >  4  1 d of length 4  the input stride for atrous morphological dilation  must be   1  rate height  rate width  1   ) ( padding : a string from  "same"  "valid"  the type of pad algorithm to use  ) ( name : a name for the operation  optional   if not specify "erosion2d" be use  )
( value : a tensor  4 d with shape  batch  height  width  channel   ) ( pooling ratio : a list of float that have length >  4   pool ratio for each dimension of value  currently only support row and col dimension and should be >  1 0  for example  a valid pool ratio look like  1 0  1 44  1 73  1 0   the first and last elements must be 1 0 because we don't allow pool on batch and channel dimension   1 44 and 1 73 be pool ratio on height and width dimension respectively  ) ( pseudo random : an optional bool   default to false  when set to true  generate the pool sequence in a pseudorandom fashion  otherwise  in a random fashion  check paper  graham  2015  for difference between pseudorandom and random  ) ( overlapping : an optional bool   default to false   when set to true  it mean when pool  the value at the boundary of adjacent pool cells be use by both cells  for example  index  0  1  2  3  4 value  20 5  16 3  7 if the pool sequence be  0  2  4   then 16  at index 2 will be use twice   the result would be  20  16  for fractional avg pool  ) ( deterministic : an optional bool   deprecate  use fractional avg pool v2 instead  ) ( seed : an optional int   default to 0   if set to be non zero  the random number generator be seed by the give seed   otherwise it be seed by a random seed  ) ( seed2 : an optional int   deprecate  use fractional avg pool v2 instead  ) ( name : a name for the operation  optional   )
( value : a tensor  4 d with shape  batch  height  width  channel   ) ( pooling ratio : a list of float that have length >  4   pool ratio for each dimension of value  currently only support row and col dimension and should be >  1 0  for example  a valid pool ratio look like  1 0  1 44  1 73  1 0   the first and last elements must be 1 0 because we don't allow pool on batch and channel dimension   1 44 and 1 73 be pool ratio on height and width dimension respectively  ) ( pseudo random : an optional bool   default to false  when set to true  generate the pool sequence in a pseudorandom fashion  otherwise  in a random fashion  check  graham  2015  for difference between pseudorandom and random  ) ( overlapping : an optional bool   default to false   when set to true  it mean when pool  the value at the boundary of adjacent pool cells be use by both cells  for example  index  0  1  2  3  4 value  20 5  16 3  7 if the pool sequence be  0  2  4   then 16  at index 2 will be use twice   the result would be  20  16  for fractional max pool  ) ( deterministic : an optional bool   deprecate  use fractional max pool v2 instead  ) ( seed : an optional int   default to 0   if set to be non zero  the random number generator be seed by the give seed   otherwise it be seed by a random seed  ) ( seed2 : an optional int   deprecate  use fractional max pool v2 instead  ) ( name : a name for the operation  optional   )
( x : input tensor of 4 or 5 dimension  ) ( scale : a tensor of 1 dimension for scale  ) ( offset : a tensor of 1 dimension for bias  ) ( mean : a tensor of 1 dimension for population mean  the shape and mean of this argument depend on the value of be train and exponential avg factor as follow  be trainingfalse  inference     mean must be a tensor of the same shape as scale contain the   estimate population mean compute during train  be trainingtrue and exponential avg factor    1 0    mean must be none  be trainingtrue and exponential avg factor    1 0    mean must be a tensor of the same shape as scale contain the   exponential run mean  ) ( variance : a tensor of 1 dimension for population variance  the shape and mean of this argument depend on the value of be train and exponential avg factor as follow  be trainingfalse  inference     variance must be a tensor of the same shape as scale contain   the estimate population variance compute during train  be train  true and exponential avg factor    1 0    variance must be none  be train  true and exponential avg factor    1 0    variance must be a tensor of the same shape as scale contain   the exponential run variance  ) ( epsilon : a small float number add to the variance of x  ) ( data format : the data format for x  support "nhwc"  default  or "nchw" for 4d tenors and "ndhwc" or "ncdhw" for 5d tensors  ) ( is training : a bool value to specify if the operation be use for train or inference  ) ( name : a name for this operation  optional   ) ( exponential avg factor : a float number  usually between 0 and 1  use for control the decay of the run population average of mean and variance  if set to 1 0  the current batch average be return  )
( value : a 4 d tensor of the format specify by data format  ) ( ksize : an int or list of ints that have length 1  2 or 4  the size of the window for each dimension of the input tensor  ) ( strides : an int or list of ints that have length 1  2 or 4  the stride of the slide window for each dimension of the input tensor  ) ( padding : either the string "same" or "valid" indicate the type of pad algorithm to use  or a list indicate the explicit paddings at the start and end of each dimension  when explicit pad be use and data format be "nhwc"  this should be in the form   0  0    pad top  pad bottom    pad leave  pad right    0  0    when explicit pad use and data format be "nchw"  this should be in the form   0  0    0  0    pad top  pad bottom    pad leave  pad right    when use explicit pad  the size of the paddings cannot be greater than the slide window size  ) ( data format : a string  'nhwc'  'nchw' and 'nchw vect c' be support  ) ( name : optional name for the operation  ) ( input : alias for value  )
( input : a tensor  must be one of the follow type  float32  float64  int32  uint8  int16  int8  int64  bfloat16  uint16  half  uint32  uint64  4 d with shape  batch  height  width  channel    input to pool over  ) ( ksize : a list of ints that have length >  4  the size of the window for each dimension of the input tensor  ) ( strides : a list of ints that have length >  4  the stride of the slide window for each dimension of the input tensor  ) ( padding : a string from  "same"  "valid"  the type of pad algorithm to use  ) ( Targmax : an optional tf dtype from  tf int32  tf int64  default to tf int64  ) ( include batch in index : an optional bool  default to false  whether to include batch dimension in flatten index of argmax  ) ( name : a name for the operation  optional   )
( x : a tensor  ) ( axes : array of ints   ax along which to compute mean and variance  ) ( shift : not use in the current implementation ) ( name : name use to scope the operations that compute the moments  ) ( keep dims : produce moments with the same dimensionality as the input  ) ( keepdims : alias to keep dim  )
( weights : a tensor of shape  num class  dim   or a list of tensor object whose concatenation along dimension 0 have shape  num class  dim    the  possibly partition  class embeddings  ) ( biases : a tensor of shape  num class    the class bias  ) ( labels : a tensor of type int64 and shape  batch size  num true   the target class  ) ( inputs : a tensor of shape  batch size  dim    the forward activations of the input network  ) ( num sampled : an int   the number of negative class to randomly sample per batch  this single sample of negative class be evaluate for each element in the batch  ) ( num classes : an int  the number of possible class  ) ( num true : an int   the number of target class per train example  ) ( sampled values : a tuple of  sample candidates  true expect count  sample expect count  return by a   candidate sampler function   if none  we default to log uniform candidate sampler  ) ( remove accidental hits : a bool   whether to remove "accidental hits" where a sample class equal one of the target class   if set to true  this be a "sampled logistic" loss instead of nce  and we be learn to generate log odds instead of log probabilities  see our candidate sample algorithms reference  pdf   default be false  ) ( partition strategy : a string specify the partition strategy  relevant if len weight  > 1  currently "div" and "mod" be support  default be "mod"  see tf nn embed lookup for more detail  ) ( name : a name for the operation  optional   )
( input : tensor of rank n 2  of shape  batch size    input spatial shape    num channel  if data format do not start with "nc"  default   or  batch size  num channel    input spatial shape if data format start with "nc"   pool happen over the spatial dimension only  ) ( window shape : sequence of n ints >  1  ) ( pooling type : specify pool operation  must be "avg" or "max"  ) ( padding : the pad algorithm  must be "same" or "valid"  see the "returns" section of tf nn convolution for detail  ) ( dilation rate : optional   dilation rate   list of n ints >  1  default to  1  n   if any value of dilation rate be > 1  then all value of stride must be 1  ) ( strides : optional   sequence of n ints >  1   default to  1  n  if any value of stride be > 1  then all value of dilation rate must be 1  ) ( name : optional  name of the op  ) ( data format : a string or none   specify whether the channel dimension of the input and output be the last dimension  default  or if data format do not start with "nc"   or the second dimension  if data format start with "nc"    for n 1  the valid value be "nwc"  default  and "ncw"   for n 2  the valid value be "nhwc"  default  and "nchw"  for n 3  the valid value be "ndhwc"  default  and "ncdhw"  ) ( dilations : alias for dilation rate )
( input : a tensor  must be one of the follow type  qint8  quint8  qint32  qint16  quint16  4 d with shape  batch  height  width  channel   ) ( min input : a tensor of type float32  the float value that the lowest quantize input value represent  ) ( max input : a tensor of type float32  the float value that the highest quantize input value represent  ) ( ksize : a list of ints  the size of the window for each dimension of the input tensor  the length must be 4 to match the number of dimension of the input  ) ( strides : a list of ints  the stride of the slide window for each dimension of the input tensor   the length must be 4 to match the number of dimension of the input  ) ( padding : a string from  "same"  "valid"  the type of pad algorithm to use  ) ( name : a name for the operation  optional   )
( input : a tensor  must be one of the follow type  qint8  quint8  qint32  qint16  quint16  ) ( filter : a tensor  must be one of the follow type  qint8  quint8  qint32  qint16  quint16  filter's input depth dimension must match input's depth dimension  ) ( min input : a tensor of type float32  the float value that the lowest quantize input value represent  ) ( max input : a tensor of type float32  the float value that the highest quantize input value represent  ) ( min filter : a tensor of type float32  the float value that the lowest quantize filter value represent  ) ( max filter : a tensor of type float32  the float value that the highest quantize filter value represent  ) ( strides : a list of ints  the stride of the slide window for each dimension of the input tensor  ) ( padding : a string from  "same"  "valid"  the type of pad algorithm to use  ) ( out type : an optional tf dtype from  tf qint8  tf quint8  tf qint32  tf qint16  tf quint16  default to tf qint32  ) ( dilations : an optional list of ints  default to  1  1  1  1   1 d tensor of length 4   the dilation factor for each dimension of input  if set to k > 1  there will be k 1 skip cells between each filter element on that dimension  the dimension order be determine by the value of data format  see above for detail  dilations in the batch and depth dimension must be 1  ) ( name : a name for the operation  optional   )
( input : a tensor  must be one of the follow type  qint8  quint8  qint32  qint16  quint16  the 4d  batch x row x cols x depth  tensor to maxreduce over  ) ( min input : a tensor of type float32  the float value that the lowest quantize input value represent  ) ( max input : a tensor of type float32  the float value that the highest quantize input value represent  ) ( ksize : a list of ints  the size of the window for each dimension of the input tensor  the length must be 4 to match the number of dimension of the input  ) ( strides : a list of ints  the stride of the slide window for each dimension of the input tensor  the length must be 4 to match the number of dimension of the input  ) ( padding : a string from  "same"  "valid"  the type of pad algorithm to use  ) ( name : a name for the operation  optional   )
( features : a tensor  must be one of the follow type  qint8  quint8  qint32  qint16  quint16  ) ( max value : a tensor of type float32  ) ( min features : a tensor of type float32  the float value that the lowest quantize value represent  ) ( max features : a tensor of type float32  the float value that the highest quantize value represent  ) ( out type : an optional tf dtype from  tf qint8  tf quint8  tf qint32  tf qint16  tf quint16  default to tf quint8  ) ( name : a name for the operation  optional   )
( cell : an instance of rnncell  ) ( loop fn : a callable that take input  time  cell output  cell state  loop state  and return the tuple  finish  next input  next cell state  emit output  next loop state   here time be an int32 scalar tensor  cell output be a tensor or  possibly nest  tuple of tensors as determine by cell output size  and cell state be a tensor or  possibly nest  tuple of tensors  as determine by the loop fn on its first call  and should match cell state size   the output be  finish  a boolean tensor of shape  batch size   next input  the next input to fee to cell  next cell state  the next state to fee to cell  and emit output  the output to store for this iteration   note that   emit output should be a tensor or  possibly nest  tuple of tensors   which be aggregate in the emit ta inside the while loop  for the   first call to loop fn  the emit output correspond to the   emit structure which be then use to determine the size of the   zero tensor for the emit ta  default to cell output size   for   the subsequent call to the loop fn  the emit output correspond to   the actual output tensor that be to be aggregate in the emit ta  the   parameter cell state and output next cell state may be either a   single or  possibly nest  tuple of tensors   the parameter   loop state and output next loop state may be either a single or    possibly nest  tuple of tensor and tensorarray object   this   last parameter may be ignore by loop fn and the return value may be   none   if it be not none  then the loop state will be propagate   through the rnn loop  for use purely by loop fn to keep track of its   own state  the next loop state parameter return may be none   the   first call to loop fn will be time   0  cell output   none  cell state   none  and loop state   none   for this call  the   next cell state value should be the value with which to initialize the   cell's state   it may be a final state from a previous rnn or it may be   the output of cell zero state     it should be a  possibly nest    tuple structure of tensors  if cell state size be an integer  this   must be a tensor of appropriate type and shape  batch size    cell state size   if cell state size be a tensorshape  this must be   a tensor of appropriate type and shape  batch size      cell state size  if cell state size be a  possibly nest  tuple of   ints or tensorshape  this will be a tuple have the correspond   shape  the emit output value may be either none or a  possibly   nest  tuple structure of tensors  e g    tf zero shape 0    dtype dtype 0   tf zero shape 1  dtype dtype 1    if this first   emit output return value be none  then the emit ta result of   raw rnn will have the same structure and dtypes as cell output size    otherwise emit ta will have the same structure  shape  prepended with   a batch size dimension   and dtypes as emit output   the actual   value return for emit output at this initialize call be ignore    note  this emit structure must be consistent across all time step  ) ( parallel iterations :  default  32    the number of iterations to run in parallel   those operations which do not have any temporal dependency and can be run in parallel  will be   this parameter trade off time for space   value >> 1 use more memory but take less time  while smaller value use less memory but computations take longer  ) ( swap memory : transparently swap the tensors produce in forward inference but need for back prop from gpu to cpu   this allow train rnns which would typically not fit on a single gpu  with very minimal  or no  performance penalty  ) ( scope : variablescope for the create subgraph  default to "rnn"  )
( x : a 2d tensor   dimension typically  batch  in units ) ( weights : a 2d tensor   dimension typically  in units  out units ) ( biases : a 1d tensor   dimension  out units ) ( name : a name for the operation  optional    if not specify "nn relu layer" be use  )
( embedding weights : a single tensor represent the complete embed tensor  or a list tensors all of same shape except for the first dimension  represent sharded embed tensors  alternatively  a partitionedvariable  create by partition along dimension 0  each element must be appropriately size for the give partition strategy  ) ( sparse ids : sparsetensor of shape  d 0  d 1       d n  contain the ids  d 0 be typically batch size  ) ( sparse weights : sparsetensor of same shape as sparse ids  contain float weight correspond to sparse ids  or none if all weight be be assume to be 1 0  ) ( combiner : a string specify how to combine embed result for each entry  currently "mean"  "sqrtn" and "sum" be support  with "mean" the default  ) ( default id : the id to use for an entry with no feature  ) ( name : a name for this operation  optional   ) ( partition strategy : a string specify the partition strategy  currently "div" and "mod" be support  default be "div"  ) ( max norm : if not none  all embeddings be l2 normalize to max norm before combine  )
( weights : a tensor of shape  num class  dim   or a list of tensor object whose concatenation along dimension 0 have shape  num class  dim    the  possibly sharded  class embeddings  ) ( biases : a tensor of shape  num class    the class bias  ) ( labels : a tensor of type int64 and shape  batch size  num true   the target class   note that this format differ from the label argument of nn softmax cross entropy with logits  ) ( inputs : a tensor of shape  batch size  dim    the forward activations of the input network  ) ( num sampled : an int   the number of class to randomly sample per batch  ) ( num classes : an int  the number of possible class  ) ( num true : an int   the number of target class per train example  ) ( sampled values : a tuple of  sample candidates  true expect count  sample expect count  return by a   candidate sampler function   if none  we default to log uniform candidate sampler  ) ( remove accidental hits : a bool   whether to remove "accidental hits" where a sample class equal one of the target class   default be true  ) ( partition strategy : a string specify the partition strategy  relevant if len weight  > 1  currently "div" and "mod" be support  default be "mod"  see tf nn embed lookup for more detail  ) ( name : a name for the operation  optional   ) ( seed : random seed for candidate sample  default to none  which doesn't set the op level random seed for candidate sample  )
( input : 4 d tensor with shape accord to data format  ) ( depthwise filter : 4 d tensor with shape  filter height  filter width  in channel  channel multiplier   contain in channel convolutional filter of depth 1  ) ( pointwise filter : 4 d tensor with shape  1  1  channel multiplier   in channel  out channel    pointwise filter to mix channel after depthwise filter have convolve spatially  ) ( strides : 1 d of size 4   the stride for the depthwise convolution for each dimension of input  ) ( padding : control how to pad the image before apply the depthwise convolution  can be the string "same" or "valid" indicate the type of pad algorithm to use  or a python list indicate the explicit paddings at the start and end of each dimension  when explicit pad be use and data format be "nhwc"  this should be in the form   0  0    pad top  pad bottom    pad leave  pad right    0  0    when explicit pad use and data format be "nchw"  this should be in the form   0  0    0  0    pad top  pad bottom    pad leave  pad right    ) ( rate : 1 d of size 2  the dilation rate in which we sample input value across the height and width dimension in atrous convolution  if it be greater than 1  then all value of stride must be 1  ) ( name : a name for this operation  optional   ) ( data format : the data format for input  either "nhwc"  default  or "nchw"  ) ( dilations : alias of rate  )
( labels : a tensor of the same type and shape as logits  between 0 and 1  inclusive  ) ( logits : a tensor of type float32 or float64  any real number  ) ( name : a name for the operation  optional   )
(  sentinel : use to prevent positional parameters  internal  do not use  ) ( labels : each vector along the class dimension should hold a valid probability distribution e g  for the case in which label be of shape  batch size  num class   each row of label i  must be a valid probability distribution  ) ( logits : per label activations  typically a linear output  these activation energies be interpret as unnormalized log probabilities  ) ( dim : the class dimension  default to  1 which be the last dimension  ) ( name : a name for the operation  optional   ) ( axis : alias for dim  )
( labels : each vector along the class dimension should hold a valid probability distribution e g  for the case in which label be of shape  batch size  num class   each row of label i  must be a valid probability distribution  ) ( logits : unscaled log probabilities  ) ( axis : the class dimension  default to  1 which be the last dimension  ) ( name : a name for the operation  optional   ) ( dim : deprecate alias for axis  )
(  sentinel : use to prevent positional parameters  internal  do not use  ) ( labels : tensor of shape  d 0  d 1       d  r 1    where r be rank of label and result  and dtype int32 or int64  each entry in label must be an index in  0  num class   other value will raise an exception when this op be run on cpu  and return nan for correspond loss and gradient row on gpu  ) ( logits : per label activations  typically a linear output  of shape  d 0  d 1       d  r 1   num class  and dtype float16  float32  or float64  these activation energies be interpret as unnormalized log probabilities  ) ( name : a name for the operation  optional   )
( cell fw : an instance of rnncell  to be use for forward direction  ) ( cell bw : an instance of rnncell  to be use for backward direction  ) ( inputs : a length t list of input  each a tensor of shape  batch size  input size   or a nest tuple of such elements  ) ( initial state fw :  optional  an initial state for the forward rnn  this must be a tensor of appropriate type and shape  batch size  cell fw state size   if cell fw state size be a tuple  this should be a tuple of tensors have shape  batch size  s  for s in cell fw state size  ) ( initial state bw :  optional  same as for initial state fw  but use the correspond properties of cell bw  ) ( dtype :  optional  the data type for the initial state   require if either of the initial state be not provide  ) ( sequence length :  optional  an int32/int64 vector  size  batch size   contain the actual lengths for each of the sequence  ) ( scope : variablescope for the create subgraph  default to "bidirectional rnn" )
( cell : an instance of rnncell  ) ( inputs : a length t list of input  each a tensor of shape  batch size  input size   or a nest tuple of such elements  ) ( initial state :  optional  an initial state for the rnn  if cell state size be an integer  this must be a tensor of appropriate type and shape  batch size  cell state size   if cell state size be a tuple  this should be a tuple of tensors have shape  batch size  s  for s in cell state size  ) ( dtype :  optional  the data type for the initial state and expect output  require if initial state be not provide or rnn state have a heterogeneous dtype  ) ( sequence length : specify the length of each sequence in input  an int32 or int64 vector  tensor  size  batch size   value in  0  t   ) ( scope : variablescope for the create subgraph  default to "rnn"  )
( cell : an instance of rnncell  ) ( inputs : a length t list of input  each a tensor of shape  batch size  input size   ) ( state saver : a state saver object with methods state and save state  ) ( state name : python string or tuple of string   the name to use with the state saver  if the cell return tuples of state  i e   cell state size be a tuple  then state name should be a tuple of string have the same length as cell state size   otherwise it should be a single string  ) ( sequence length :  optional  an int32/int64 vector size  batch size   see the documentation for rnn   for more detail about sequence length  ) ( scope : variablescope for the create subgraph  default to "rnn"  )
( x : a tensor  ) ( axes : array of ints  ax along which to compute mean and variance  as in python  the ax can also be negative number  a negative axis be interpret as count from the end of the rank  i e   axis   rank value  th dimension  ) ( shift : a tensor contain the value by which to shift the data for numerical stability  or none if no shift be to be perform  a shift close to the true mean provide the most numerically stable result  ) ( keep dims : produce statistics with the same dimensionality as the input  ) ( name : name use to scope the operations that compute the sufficient stats  ) ( keepdims : alias for keep dim  )
( labels : a tensor of the same type and shape as logits  ) ( logits : a tensor of type float32 or float64  ) ( pos weight : a coefficient to use on the positive examples  ) ( name : a name for the operation  optional   ) ( targets : deprecate alias for label  )
( x : a tensor  ) ( axes : 1 d tensor of int32 value  these be the ax along which to compute mean and variance  ) ( frequency weights : a tensor of positive weight which can be broadcast with x  ) ( name : name use to scope the operation  ) ( keep dims : produce moments with the same dimensionality as the input  ) ( keepdims : alias of keep dim  )
( x : a 2d tensor   dimension typically  batch  in units ) ( weights : a 2d tensor   dimension typically  in units  out units ) ( biases : a 1d tensor   dimension  out units ) ( name : a name for the operation  optional    if not specify "xw plus b" be use  )
( num units : int  the number of units in the lstm cell  ) ( forget bias : float  the bias add to forget gate  see above   must set to 0 0 manually when restore from cudnnlstm train checkpoints  ) ( state is tuple : if true  accept and return state be 2 tuples of the c state and m state   if false  they be concatenate along the column axis   the latter behavior will soon be deprecate  ) ( activation : activation function of the inner state   default  tanh  it could also be string that be within keras activation function name  ) ( reuse :  optional  python boolean describe whether to reuse variables in an exist scope   if not true  and the exist scope already have the give variables  an error be raise  ) ( name : string  the name of the layer  layer with the same name will share weight  but to avoid mistake we require reuse true in such case  ) ( dtype : default dtype of the layer  default of none mean use the type of the first input   require when build be call before call  ) (   kwargs : dict  keyword name properties for common layer attribute  like trainable etc when construct the cell from configs of get config    when restore from cudnnlstm train checkpoints  must use cudnncompatiblelstmcell instead  )
( num units : int  the number of units in the rnn cell  ) ( activation : nonlinearity to use   default  tanh  it could also be string that be within keras activation function name  ) ( reuse :  optional  python boolean describe whether to reuse variables in an exist scope   if not true  and the exist scope already have the give variables  an error be raise  ) ( name : string  the name of the layer  layer with the same name will share weight  but to avoid mistake we require reuse true in such case  ) ( dtype : default dtype of the layer  default of none mean use the type of the first input   require when build be call before call  ) (   kwargs : dict  keyword name properties for common layer attribute  like trainable etc when construct the cell from configs of get config    )
( cell : an instance of rnncell  ) ( device : a device string or function  for pass to tf device  ) (   kwargs : dict of keyword arguments for base layer  )
( cell : an rnncell  a projection to output size be add to it  ) ( input keep prob : unit tensor or float between 0 and 1  input keep probability  if it be constant and 1  no input dropout will be add  ) ( output keep prob : unit tensor or float between 0 and 1  output keep probability  if it be constant and 1  no output dropout will be add  ) ( state keep prob : unit tensor or float between 0 and 1  output keep probability  if it be constant and 1  no output dropout will be add  state dropout be perform on the outgo state of the cell  note the state components to which dropout be apply when state keep prob be in  0  1  be also determine by the argument dropout state filter visitor  e g  by default dropout be never apply to the c component of an lstmstatetuple   ) ( variational recurrent : python bool   if true  then the same dropout pattern be apply across all time step per run call  if this parameter be set  input size must be provide  ) ( input size :  optional   possibly nest tuple of  tensorshape object contain the depth s  of the input tensors expect to be pass in to the dropoutwrapper   require and use iff variational recurrent   true and input keep prob < 1  ) ( dtype :  optional  the dtype of the input  state  and output tensors  require and use iff variational recurrent   true  ) ( seed :  optional  integer  the randomness seed  ) ( dropout state filter visitor :  optional   default   see below    function that take any hierarchical level of the state and return a scalar or depth 1 structure of python booleans describe which term in the state should be drop out   in addition  if the function return true  dropout be apply across this sublevel   if the function return false  dropout be not apply across this entire sublevel  default behavior  perform dropout on all term except the memory  c    state of lstmcellstate object  and don't try to apply dropout to tensorarray object  def dropout state filter visitor s     if isinstance s  lstmcellstate     never perform dropout on the c     state  return lstmcellstate c false  h true    elif isinstance s  tensorarray   return false return true ) (   kwargs : dict of keyword arguments for base layer  )
( num units : int  the number of units in the gru cell  ) ( activation : nonlinearity to use   default  tanh  ) ( reuse :  optional  python boolean describe whether to reuse variables in an exist scope   if not true  and the exist scope already have the give variables  an error be raise  ) ( kernel initializer :  optional  the initializer to use for the weight and projection matrices  ) ( bias initializer :  optional  the initializer to use for the bias  ) ( name : string  the name of the layer  layer with the same name will share weight  but to avoid mistake we require reuse true in such case  ) ( dtype : default dtype of the layer  default of none mean use the type of the first input   require when build be call before call  ) (   kwargs : dict  keyword name properties for common layer attribute  like   trainable etc when construct the cell from configs of get config    reference  learn phrase representations use rnn encoder decoder for statistical machine translation    cho et al   2014    pdf  )
( num units : int  the number of units in the lstm cell  ) ( use peepholes : bool  set true to enable diagonal/peephole connections  ) ( cell clip :  optional  a float value  if provide the cell state be clip by this value prior to the cell output activation  ) ( initializer :  optional  the initializer to use for the weight and projection matrices  ) ( num proj :  optional  int  the output dimensionality for the projection matrices   if none  no projection be perform  ) ( proj clip :  optional  a float value   if num proj > 0 and proj clip be provide  then the project value be clip elementwise to within   proj clip  proj clip   ) ( num unit shards : deprecate  will be remove by jan  2017  use a variable scope partitioner instead  ) ( num proj shards : deprecate  will be remove by jan  2017  use a variable scope partitioner instead  ) ( forget bias : bias of the forget gate be initialize by default to 1 in order to reduce the scale of forget at the begin of the train  must set it manually to 0 0 when restore from cudnnlstm train checkpoints  ) ( state is tuple : if true  accept and return state be 2 tuples of the c state and m state   if false  they be concatenate along the column axis   this latter behavior will soon be deprecate  ) ( activation : activation function of the inner state   default  tanh  it could also be string that be within keras activation function name  ) ( reuse :  optional  python boolean describe whether to reuse variables in an exist scope   if not true  and the exist scope already have the give variables  an error be raise  ) ( name : string  the name of the layer  layer with the same name will share weight  but to avoid mistake we require reuse true in such case  ) ( dtype : default dtype of the layer  default of none mean use the type of the first input   require when build be call before call  ) (   kwargs : dict  keyword name properties for common layer attribute  like trainable etc when construct the cell from configs of get config    when restore from cudnnlstm train checkpoints  use cudnncompatiblelstmcell instead  )
( c : a namedtuple alias for field number 0 ) ( h : a namedtuple alias for field number 1 ) ( dtype :  )
( cells : list of rnncells that will be compose in this order  ) ( state is tuple : if true  accept and return state be n tuples  where n   len cells    if false  the state be all concatenate along the column axis   this latter behavior will soon be deprecate  )
( graph :  ) ( output size : integer or tensorshape  size of output produce by this cell  ) ( scope name :  ) ( state size : size s  of state s  use by this cell  it can be represent by an integer  a tensorshape or a tuple of integers or tensorshapes  )
( cell : an instance of rnncell  ) ( residual fn :  optional  the function to map raw cell input and raw cell output to the actual cell output of the residual network  default to call nest map structure on  lambda i  o  i   o   input   and output  ) (   kwargs : dict of keyword arguments for base layer  )
( options : optional initial option dict to start with  )
( graph : tf graph  if none and eager execution be not enable  use default graph  ) ( op log : optional  tensorflow  tfprof  oplogproto proto  use to define extra op type  )
( graph : tf graph  if none and eager execution be not enable  use default graph  ) ( run meta : optional tensorflow runmetadata proto  it be necessary to to support run time information profile  such as time and memory  ) ( options : see all advice example above  default check everything  )
( graph : tf graph  if none and eager execution be not enable  use default graph  ) ( run meta : optional tensorflow runmetadata proto  it be necessary to to support run time information profile  such as time and memory  ) ( op log : tensorflow tfprof oplogproto proto  user can assign "types" to graph nod with op log  "types" allow user to flexibly group and account profile use options 'accounted type regexes'   ) ( cmd : string  either 'op'  'scope'  'graph' or 'code'  'op' view organize profile use operation type   e g  matmul  'scope' view organize profile use graph node name scope  'graph' view organize profile use graph node inputs/outputs  'code' view organize profile use python call stack  ) ( options : a dict of options  see core/profiler/g3doc/options md  )
( graph : tf graph  if none and eager execution be not enable  use default graph  ) ( log dir : directory to write the log file  ) ( op log :  optional  oplogproto proto to be write  if not provide  an new one be create  ) ( run meta :  optional  runmetadata proto that help flop computation use run time shape information  ) ( add trace : whether to add python code trace information  use to support "code" view  )
( values : a numpy array of any type and shape  or a raggedtensorvalue  ) ( row splits : a 1 d int32 or int64 numpy array  )
( pylist : a nest list  tuple or np ndarray   any nest element that be not a list or tuple must be a scalar value compatible with dtype  ) ( dtype : numpy dtype   the type of elements for the return raggedtensor  if not specify  then a default be choose base on the scalar value in pylist  ) ( ragged rank : an integer specify the rag rank of the return raggedtensorvalue   must be nonnegative and less than k  default to max 0  k   1  if inner shape be not specify   default to `max 0  k  1   len inner shape  ifinner shapeis specify  </td> </tr><tr> <td>inner shape</td> <td> a tuple of integers specify the shape for individual inner value in the returnedraggedtensorvalue   default to  ifragged rankis not specify   ifragged rankis specify  then a default be choose base on the content ofpylist  </td> </tr><tr> <td>row split dtype</td> <td> data type for the constructedraggedtensorvalue's row split   one ofnumpy int32ornumpy int64`  )
( dtype : the data type for the raggedtensor  ) ( ragged rank : the rag rank for the raggedtensor ) ( value shape : the shape for individual flat value in the raggedtensor  ) ( name : a name for the operation  optional   )
( logits : 2 d tensor with shape  batch size  num class    each slice  i     represent the unnormalized log probabilities for all class  ) ( num samples : 0 d   number of independent sample to draw for each row slice  ) ( seed : a shape  2  tensor  the seed to the random number generator  must have dtype int32 or int64   when use xla  only int32 be allow   ) ( output dtype : the integer type of the output  int32 or int64  default to int64  ) ( name : optional name for the operation  )

( path : a string resource path relative to tensorflow/ )

( path : a string resource path relative to tensorflow/  )


( inputs : input of the signaturedef define as a proto map of string to tensor info  ) ( outputs : output of the signaturedef define as a proto map of string to tensor info  ) ( method name : method name of the signaturedef as a string  )
( tensor : tensor or sparsetensor whose name  dtype and shape be use to build the tensorinfo  for sparsetensors  the name of the three constituent tensors be use  )
( examples : a string tensor  expect to accept serialize tf examples  ) ( classes : a string tensor   note that the classificationresponse message require that class label be string  not integers or anything else  ) ( scores : a float tensor  )
( export dir : absolute string path to possible export location  for example  '/my/foo/model'  )
( tensor info : a tensorinfo proto describe a tensor or sparsetensor or compositetensor  ) ( graph : the tf graph in which tensors be look up  if none  the current default graph be use  ) ( import scope : if not none  name in tensor info be prefix with this string before lookup  )

( sess : the tensorflow session to restore the variables  ) ( tags : set of string tag to identify the require metagraphdef  these should correspond to the tag use when save the variables use the savedmodel save   api  ) ( export dir : directory in which the savedmodel protocol buffer and variables to be load be locate  ) ( import scope : optional string    if specify  prepend this string follow by '/' to all load tensor name  this scope be apply to tensor instance load into the pass session  but it be not write through to the static metagraphdef protocol buffer that be return  ) (   saver kwargs : optional keyword arguments pass through to saver  )
( restore op name : name of the op to use to restore the graph  )
( inputs : dict of string to tensor  ) ( outputs : dict of string to tensor  )
( examples : a string tensor  expect to accept serialize tf examples  ) ( predictions : a float tensor  )
( session : the tensorflow session from which to save the meta graph and variables  ) ( export dir : the path to which the savedmodel will be store  ) ( inputs : dict map string input name to tensors  these be add to the signaturedef as the input  ) ( outputs : dict map string output name to tensors  these be add to the signaturedef as the output  ) ( legacy init op : legacy support for op or group of ops to execute after the restore op upon a load  )

( input : a tensor of type string  the string for which to compute the length for each element  ) ( name : a name for the operation  optional   ) ( unit : an optional string from  "byte"  "utf8 char"  default to "byte"  the unit that be count to compute string length   one of    "byte"  for the number of bytes in each string  or "utf8 char"  for   the number of utf 8 encode unicode code point in each string   result   be undefined if unit utf8 char and the input string do not contain   structurally valid utf 8  )
( input : a string tensor of rank n  the string to split   if rank input  be not know statically  then it be assume to be 1  ) ( sep : 0 d string tensor  the delimiter character  ) ( maxsplit : an int  if maxsplit > 0  limit of the split of the result  ) ( result type : the tensor type for the result  one of "raggedtensor" or "sparsetensor"  ) ( source : alias for "input" argument  ) ( name : a name for the operation  optional   )
( input : a tensor of type string  tensor of string ) ( pos : a tensor  must be one of the follow type  int32  int64  scalar define the position of first character in each substring ) ( len : a tensor  must have the same type as pos  scalar define the number of character to include in each substring ) ( unit : an optional string from  "byte"  "utf8 char"  default to "byte"  the unit that be use to create the substring   one of  "byte"  for define position and length by bytes  or "utf8 char"  for the utf 8 encode unicode code point    the default be "byte"  result be undefined if unit utf8 char and the input string do not contain structurally valid utf 8  ) ( name : a name for the operation  optional   )
( logdir : a string  directory where event file will be write  ) ( graph : a graph object  such as sess graph  ) ( max queue : integer  size of the queue for pending events and summaries  ) ( flush secs : number  how often  in second  to flush the pending events and summaries to disk  ) ( graph def : deprecate  use the graph argument instead  ) ( filename suffix : a string  every event file's name be suffix with suffix  ) ( session : a tf compat v1 session object  see detail above  )


( name : a name for the generate node  will also serve as a series name in tensorboard  ) ( tensor : a 3 d float32 tensor of shape  batch size  frame  channel  or a 2 d float32 tensor of shape  batch size  frame   ) ( sample rate : a scalar float32 tensor indicate the sample rate of the signal in hertz  ) ( max outputs : max number of batch elements to generate audio for  ) ( collections : optional list of ops graphkeys   the collections to add the summary to   default to   ops graphkeys summaries  ) ( family : optional  if provide  use as the prefix of the summary tag name  which control the tab name use for display on tensorboard  )
( node def : the node def pb2 nodedef of a tensorsummary op )
( name : a name for the generate node  will also serve as a series name in tensorboard  ) ( values : a real numeric tensor  any shape  value to use to build the histogram  ) ( collections : optional list of graph collections key  the new summary op be add to these collections  default to  graphkeys summaries   ) ( family : optional  if provide  use as the prefix of the summary tag name  which control the tab name use for display on tensorboard  )
( name : a name for the generate node  will also serve as a series name in tensorboard  ) ( tensor : a 4 d uint8 or float32 tensor of shape  batch size  height  width  channel  where channel be 1  3  or 4  ) ( max outputs : max number of batch elements to generate image for  ) ( collections : optional list of ops graphkeys   the collections to add the summary to   default to   ops graphkeys summaries  ) ( family : optional  if provide  use as the prefix of the summary tag name  which control the tab name use for display on tensorboard  )
( graph : a tf graph or tf compat v1 graphdef to output to the writer  this function will not write the default graph by default  when write to an event log file  the associate step will be zero  ) ( session : so this method can call tf session run  this default to tf compat v1 get default session  )
( inputs : a list of string tensor object contain serialize summary protocol buffer  ) ( collections : optional list of graph collections key  the new summary op be add to these collections  default to     ) ( name : a name for the operation  optional   )
( key : graphkey use to collect the summaries   default to graphkeys summaries  ) ( scope : optional scope use to filter the summary ops  use re match  ) ( name : a name for the operation  optional   )
( name : a name for the generate node  will also serve as the series name in tensorboard  ) ( tensor : a real numeric tensor contain a single value  ) ( collections : optional list of graph collections key  the new summary op be add to these collections  default to  graphkeys summaries   ) ( family : optional  if provide  use as the prefix of the summary tag name  which control the tab name use for display on tensorboard  )
( name : a name for the generate node  if display name be not set  it will also serve as the tag name in tensorboard   in that case  the tag name will inherit tf name scopes   ) ( tensor : a tensor of any type and shape to serialize  ) ( summary description : a long description of the summary sequence  markdown be support  ) ( collections : optional list of graph collections key  the new summary op be add to these collections  default to  graphkeys summaries   ) ( summary metadata : optional summarymetadata proto  which describe which plugins may use the summary value   ) ( family : optional  if provide  use as the prefix of the summary tag  which control the name use for display on tensorboard when display name be not set  ) ( display name : a string use to name this data in tensorboard  if this be not set  then the node name will be use instead  )
( name : a name for the generate node  will also serve as a series name in tensorboard  ) ( tensor : a string type tensor to summarize  ) ( collections : optional list of ops graphkeys   the collections to add the summary to   default to   ops graphkeys summaries  )

( actual : the graphdef we have  ) ( expected : the graphdef we expect  ) ( checkpoint v2 : boolean determine whether to ignore randomize attribute value that appear in v2 checkpoints  ) ( hash table shared name : boolean determine whether to ignore randomize share name that appear in hashtablev2 op defs  )
( x : a tensor or list of tensors ) ( x shape : the dimension of x as a tuple or an array of ints  if x be a list  then this be the list of shape  ) ( y : a tensor ) ( y shape : the dimension of y as a tuple or an array of ints  ) ( x init value :  optional  a numpy array of the same shape as "x" represent the initial value of x  if x be a list  this should be a list of numpy array   if this be none  the function will pick a random tensor as the initial value  ) ( delta :  optional  the amount of perturbation  ) ( init targets : list of target to run to initialize model params  ) ( extra feed dict : dict that allow fix specify tensor value during the jacobian calculation  )
( x : a tensor or list of tensors ) ( x shape : the dimension of x as a tuple or an array of ints  if x be a list  then this be the list of shape  ) ( y : a tensor ) ( y shape : the dimension of y as a tuple or an array of ints  ) ( x init value :  optional  a numpy array of the same shape as "x" represent the initial value of x  if x be a list  this should be a list of numpy array   if this be none  the function will pick a random tensor as the initial value  ) ( delta :  optional  the amount of perturbation  ) ( init targets : list of target to run to initialize model params  ) ( extra feed dict : dict that allow fix specify tensor value during the jacobian calculation  )

( relative path : a path relative to tensorflow root  e g  "core/platform"  )
( opt : an exist optimizer to encapsulate  ) ( reduction : the reduction to apply to the shard losses  ) ( name : optional name prefix for the operations create when apply gradients  default to "crossshardoptimizer"  ) ( group assignment : optional 2d int32 list with shape  num group  num replicas per group  which describles how to apply optimizer to subgroups  )
( computation : a python function that build a computation to apply to each shard of the input  ) ( inputs : a list of input tensors or none  equivalent to an empty list   the 0 th dimension of each tensor must have size divisible by num shards  ) ( num shards : the number of shards  ) ( infeed queue : if not none  the infeedqueue from which to append a tuple of arguments as input to computation  ) ( device assignment : if not none  a deviceassignment describe the map between logical core in the computation with physical core in the tpu topology  use a default device assignment if none  the deviceassignment may be omit if each shard of the computation use only one core  and there be either only one shard  or the number of shards be equal to the number of core in the tpu system  ) ( name :  deprecate  do nothing  ) ( xla options : an instance of tpu xlaoptions which indicate the options pass to xla compiler  use none for default options  )

( num : the virtual core number within each replica to which operators should be assign  )
( x : the local tensor to the sum  ) ( group assignment : optional 2d int32 list with shape  num group  num replicas per group   group assignment i  represent the replica ids in the ith subgroup  ) ( name : optional op name  )
( embedding config : if not none  a tpuembeddingconfiguration proto describe the desire configuration of the hardware embed lookup table  if embed config be none  no hardware embeddings can be use  ) ( job : the job  the xxx in tensorflow device specification /job xxx  that contain the tpu devices that will be initialize  if job none it be assume there be only one job in the tensorflow flock  and an error will be return if this assumption do not hold  ) ( compilation failure closes chips : set the configuration whether we want to close tpu chip when there be a compilation failure  ) ( tpu cancellation closes chips : set the configuration whether we want to close tpu chip when a tpu execution be cancel  if the value be none  the behavior will be determine by the command line flag tpu cancellation close chip for the tpu worker  warn  this argument only apply to tfrt tpu runtime  )
( computation : a python function that build the computation to place on the host  ) (  args : the positional arguments for the computation  ) (   kwargs : the keyword arguments for the computation  )
( computation : a python function that build the computation to replicate  ) ( inputs : a list of list of input tensors or none  equivalent to        index by  replica num  input num   all replicas must have the same number of input  each input can be a nest structure contain value that be convertible to tensors  note that pass an n dimension list of compatible value will result in a n dimension list of scalar tensors rather than a single rank n tensors  if you need different behavior  convert part of input to tensors with tf convert to tensor  ) ( infeed queue : if not none  the infeedqueue from which to append a tuple of arguments as input to computation  ) ( device assignment : if not none  a deviceassignment describe the map between logical core in the computation with physical core in the tpu topology  use a default device assignment if none  the deviceassignment may be omit if each replica of the computation use only one core  and there be either only one replica  or the number of replicas be equal to the number of core in the tpu system  ) ( name :  deprecate  do nothing  ) ( maximum shapes : a nest structure of tf tensorshape represent the shape to which the respective component of each input element in each replica should be pad  any unknown dimension  e g  tf compat v1 dimension none  in a tf tensorshape or  1 in a tensor like object  will be pad to the maximum size of that dimension over all replicas  the structure of maximum shape need to be the same as input 0   ) ( padding spec : an enum specify by tpu paddingspec  this describe the pad policy when the input to tpu replicate be dynamic  one usage be to enable automatic bucketizing on the input by set the value to tpu paddingspec power of two  which can help to reduce the recompilation in the xla side  ) ( xla options : an instance of tpu xlaoptions which indicate the options pass to xla compiler  use none for default options  )
( computation : a python function that build a computation to apply to the input  if the function take n input  'inputs' should be a list of n tensors  computation may return a list of operations and tensors  tensors must come before operations in the return list   the return value of rewrite be a list of tensors correspond to the tensors from the output of computation  all operations construct during computation will be execute when evaluate any of the return output tensors  not just the ones return  ) ( inputs : a list of input tensors or none  equivalent to an empty list   each input can be a nest structure contain value that be convertible to tensors  note that pass an n dimension list of compatible value will result in a n dimension list of scalar tensors rather than a single rank n tensors  if you need different behavior  convert part of input to tensors with tf convert to tensor  ) ( infeed queue : if not none  the infeedqueue from which to append a tuple of arguments as input to computation  ) ( device assignment : if not none  a deviceassignment describe the map between logical core in the computation with physical core in the tpu topology  may be omit for a single core computation  in which case the core attach to task 0  tpu device 0 be use  ) ( name :  deprecate  do nothing  ) ( xla options : an instance of tpu xlaoptions which indicate the options pass to xla compiler  use none for default options  )
( computation : a python function that build a computation to apply to each shard of the input  ) ( inputs : a list of input tensors or none  equivalent to an empty list   each input tensor have a correspond shard ax  give by input shard ax  which must have size divisible by num shards  ) ( num shards : the number of shards  ) ( input shard axes : a list of dimension along which to shard input  or none  none mean "shard all input along dimension 0"  if not none  there must be one dimension per input  ) ( outputs from all shards : boolean or list of boolean  for each output  if true  output from all shards be concatenate along the correspond output shard ax entry  otherwise  each output be take from an arbitrary shard  if the argument be a boolean  the argument's value be use for each output  ) ( output shard axes : a list of dimension along which to concatenate the output of computation  or none  none mean "concatenate all output along dimension 0"  if not none  there must be one dimension per output  ignore if output from all shards be false  ) ( infeed queue : if not none  the infeedqueue to use to augment the input of computation  ) ( device assignment : if not none  a deviceassignment describe the map between logical core in the computation with physical core in the tpu topology  use a default device assignment if none  the deviceassignment may be omit if each shard of the computation use only one core  and there be either only one shard  or the number of shards be equal to the number of core in the tpu system  ) ( name :  deprecate  do nothing  ) ( xla options : an instance of tpu xlaoptions which indicate the options pass to xla compiler  use none for default options  )
( job : the job  the xxx in tensorflow device specification /job xxx  that contain the tpu devices that will be shutdown  if job none it be assume there be only one job in the tensorflow flock  and an error will be return if this assumption do not hold  )
( learning rate : a tensor or a float point value  the learn rate  to match the exact form in the original paper use 1 0  ) ( rho : a tensor or a float point value  the decay rate  ) ( epsilon : a tensor or a float point value   a constant epsilon use to better condition the grad update  ) ( use locking : if true use lock for update operations  ) ( name : optional name prefix for the operations create when apply gradients   default to "adadelta"  )
( learning rate : a tensor or a float point value   the learn rate  ) ( global step : a tensor contain the current train step number  ) ( initial gradient squared accumulator value : a float point value  start value for the accumulators  must be positive  ) ( l1 regularization strength : a float value  must be greater than or equal to zero  ) ( l2 regularization strength : a float value  must be greater than or equal to zero  ) ( use locking : if true use lock for update operations  ) ( name : optional name prefix for the operations create when apply gradients   default to "adagradda"  )
( learning rate : a tensor or a float point value   the learn rate  ) ( initial accumulator value : a float point value  start value for the accumulators  must be positive  ) ( use locking : if true use lock for update operations  ) ( name : optional name prefix for the operations create when apply gradients   default to "adagrad"  )
( learning rate : a tensor or a float point value   the learn rate  ) ( beta1 : a float value or a constant float tensor  the exponential decay rate for the 1st moment estimate  ) ( beta2 : a float value or a constant float tensor  the exponential decay rate for the 2nd moment estimate  ) ( epsilon : a small constant for numerical stability  this epsilon be "epsilon hat" in the kingma and ba paper  in the formula just before section 2 1   not the epsilon in algorithm 1 of the paper  ) ( use locking : if true use lock for update operations  ) ( name : optional name for the operations create when apply gradients  default to "adam"  )
(   kwargs : keyword arguments be set as attribute of this object  and be save with the checkpoint  value must be trackable object  )
( scaffold : a scaffold use for gather or build supportive ops  if not specify a default one be create  it's use to finalize the graph  ) ( master : string representation of the tensorflow master to use  ) ( config : configproto proto use to configure the session  ) ( checkpoint dir : a string   optional path to a directory where to restore variables  ) ( checkpoint filename with path : full file name path to the checkpoint file  )
( learning rate : a float value or a constant float tensor  ) ( learning rate power : a float value  must be less or equal to zero  control how the learn rate decrease during train  use zero for a fix learn rate  see section 3 1 in  mcmahan et al   2013   ) ( initial accumulator value : the start value for accumulators  only zero or positive value be allow  ) ( l1 regularization strength : a float value  must be greater than or equal to zero  ) ( l2 regularization strength : a float value  must be greater than or equal to zero  ) ( use locking : if true use lock for update operations  ) ( name : optional name prefix for the operations create when apply gradients   default to "ftrl"  ) ( accum name : the suffix for the variable that keep the gradient square accumulator   if not present  default to name  ) ( linear name : the suffix for the variable that keep the linear gradient accumulator   if not present  default to name   "1"  ) ( l2 shrinkage regularization strength : a float value  must be greater than or equal to zero  this differ from l2 above in that the l2 above be a stabilization penalty  whereas this l2 shrinkage be a magnitude penalty  the ftrl formulation can be write as  w t 1    argminw \hat g  1 t w   l1  w   1   l2  w   2 2   where \hat g    g    2l2 shrinkagew   and g be the gradient of the loss function w r t  the weight w  specifically  in the absence of l1 regularization  it be equivalent to the follow update rule  w  t 1    w t   lr t /  beta   2l2lr t    g t             2l2 shrinkagelr t /  beta   2l2lr t    w t where lr t be the learn rate at t  when input be sparse shrinkage will only happen on the active weight  ) ( beta : a float value  correspond to the beta parameter in the paper  )
( learning rate : a tensor or a float point value   the learn rate to use  ) ( use locking : if true use lock for update operations  ) ( name : optional name prefix for the operations create when apply gradients  default to "gradientdescent"  )
( coord : a coordinator  ) ( timer interval secs : time boundaries at which to call run    or none if it should be call back to back  ) ( target : optional callable object that will be execute in the thread  ) ( args : optional arguments to pass to target when call it  ) ( kwargs : optional keyword arguments to pass to target when call it  )
( learning rate : a tensor or a float point value   the learn rate  ) ( momentum : a tensor or a float point value   the momentum  ) ( use locking : if true use lock for update operations  ) ( name : optional name prefix for the operations create when apply gradients   default to "momentum"  ) ( use nesterov : if true use nesterov momentum  see  sutskever et al   2013   this implementation always compute gradients at the value of the variable s  pass to the optimizer  use nesterov momentum make the variable s  track the value call theta t   mu v t in the paper  this implementation be an approximation of the original formula  valid for high value of momentum  it will compute the "adjusted gradient" in nag by assume that the new gradient will be estimate by the current average gradient plus the product of momentum and the change in the average gradient  )
( session creator : a factory object to create session  typically a chiefsessioncreator which be the default one  ) ( hooks : an iterable of `sessionrunhook' object  )
( master : string the tensorflow master to use  ) ( is chief : if true  it will take care of initialization and recovery the underlie tensorflow session  if false  it will wait on a chief to initialize or recover the tensorflow session  ) ( checkpoint dir : a string   optional path to a directory where to restore variables  ) ( scaffold : a scaffold use for gather or build supportive ops  if not specify  a default one be create  it's use to finalize the graph  ) ( hooks : optional list of sessionrunhook object  ) ( chief only hooks : list of sessionrunhook object  activate these hook if be chief  true  ignore otherwise  ) ( save checkpoint secs : the frequency  in second  that a checkpoint be save use a default checkpoint saver  if both save checkpoint step and save checkpoint secs be set to none  then the default checkpoint saver isn't use  if both be provide  then only save checkpoint secs be use  default 600  ) ( save summaries steps : the frequency  in number of global step  that the summaries be write to disk use a default summary saver  if both save summaries step and save summaries secs be set to none  then the default summary saver isn't use  default 100  ) ( save summaries secs : the frequency  in secs  that the summaries be write to disk use a default summary saver   if both save summaries step and save summaries secs be set to none  then the default summary saver isn't use  default not enable  ) ( config : an instance of tf compat v1 configproto proto use to configure the session  it's the config argument of constructor of tf compat v1 session  ) ( stop grace period secs : number of second give to thread to stop after close   have be call  ) ( log step count steps : the frequency  in number of global step  that the global step/sec be log  ) ( max wait secs : maximum time workers should wait for the session to become available  this should be keep relatively short to help detect incorrect code  but sometimes may need to be increase if the chief take a while to start up  ) ( save checkpoint steps : the frequency  in number of global step  that a checkpoint be save use a default checkpoint saver  if both save checkpoint step and save checkpoint secs be set to none  then the default checkpoint saver isn't use  if both be provide  then only save checkpoint secs be use  default not enable  ) ( summary dir : a string   optional path to a directory where to save summaries  if none  checkpoint dir be use instead  ) ( save graph def : whether to save the graphdef and metagraphdef to checkpoint dir  the graphdef be save after the session be create as graph pbtxt  metagraphdefs be save out for every checkpoint as model ckpt   meta  )
( filepattern : the filename  )
( use locking : bool  if true apply use lock to prevent concurrent update to variables  ) ( name : a non empty string   the name to use for accumulators create for the optimizer  )
( learning rate : a tensor or a float point value   the learn rate  ) ( initial accumulator value : a float point value  start value for the accumulators  must be positive  ) ( l1 regularization strength : a float value  must be greater than or equal to zero  ) ( l2 regularization strength : a float value  must be greater than or equal to zero  ) ( use locking : if true use lock for update operations  ) ( name : optional name prefix for the operations create when apply gradients   default to "adagrad"  )
( learning rate : a tensor or a float point value   the learn rate to use  ) ( l1 regularization strength : a float value  must be greater than or equal to zero  ) ( l2 regularization strength : a float value  must be greater than or equal to zero  ) ( use locking : if true use lock for update operations  ) ( name : optional name prefix for the operations create when apply gradients  default to "gradientdescent"  )
( queue : a queue  ) ( enqueue ops : list of enqueue ops to run in thread later  ) ( close op : op to close the queue  pending enqueue ops be preserve  ) ( cancel op : op to close the queue and cancel pending enqueue ops  ) ( queue closed exception types : optional tuple of exception type that indicate that the queue have be close when raise during an enqueue operation   default to  tf errors outofrangeerror     another common case include  tf errors outofrangeerror  tf errors cancellederror   when some of the enqueue ops may dequeue from other queue  ) ( queue runner def : optional queuerunnerdef protocol buffer  if specify  recreate the queuerunner from its content  queue runner def and the other arguments be mutually exclusive  ) ( import scope : optional string  name scope to add  only use when initialize from protocol buffer  )
( learning rate : a tensor or a float point value   the learn rate  ) ( decay : discount factor for the history/coming gradient ) ( momentum : a scalar tensor  ) ( epsilon : small value to avoid zero denominator  ) ( use locking : if true use lock for update operation  ) ( centered : if true  gradients be normalize by the estimate variance of the gradient  if false  by the uncentered second moment  set this to true may help with train  but be slightly more expensive in term of computation and memory  default to false  ) ( name : optional name prefix for the operations create when apply gradients  default to "rmsprop"  )
( var list : a list of variable/saveableobject  or a dictionary map name to saveableobjects  if none  default to the list of all saveable object  ) ( reshape : if true  allow restore parameters from a checkpoint where the variables have a different shape  ) ( sharded : if true  shard the checkpoints  one per device  ) ( max to keep : maximum number of recent checkpoints to keep  default to 5  ) ( keep checkpoint every n hours : how often to keep checkpoints  default to 10 000 hours  ) ( name : string   optional name to use as a prefix when add operations  ) ( restore sequentially : a bool  which if true  cause restore of different variables to happen sequentially within each device   this can lower memory usage when restore very large model  ) ( saver def : optional saverdef proto to use instead of run the builder  this be only useful for specialty code that want to recreate a saver object for a previously build graph that have a saver  the saver def proto should be the one return by the as saver def   call of the saver that be create for that graph  ) ( builder : optional saverbuilder to use if a saver def be not provide  default to bulksaverbuilder    ) ( defer build : if true  defer add the save and restore ops to the build   call  in that case build   should be call before finalize the graph or use the saver  ) ( allow empty : if false  default  raise an error if there be no variables in the graph  otherwise  construct the saver anyway and make it a no op  ) ( write version : control what format to use when save checkpoints   it also affect certain filepath match logic   the v2 format be the recommend choice  it be much more optimize than v1 in term of memory   require and latency incur during restore   regardless of this   flag  the saver be able to restore from both v2 and v1 checkpoints  ) ( pad step number : if true  pad the global step number in the checkpoint filepaths to some fix width  8 by default    this be turn off by default  ) ( save relative paths : if true  will write relative paths to the checkpoint state file  this be need if the user want to copy the checkpoint directory and reload from the copy directory  ) ( filename : if know at graph construction time  filename use for variable loading/saving  )
( init op : optional op for initialize variables  ) ( init feed dict : optional session fee dictionary to use when run the init op  ) ( init fn : optional function to use to initialize the model after run the init op   will be call as init fn scaffold  session   ) ( ready op : optional op to verify that the variables be initialize   must return an empty 1d string tensor when the variables be initialize  or a non empty 1d string tensor list the name of the non initialize variables  ) ( ready for local init op : optional op to verify that the global variables be initialize and local init op can be run  must return an empty 1d string tensor when the global variables be initialize  or a non empty 1d string tensor list the name of the non initialize global variables  ) ( local init op : optional op to initialize local variables  ) ( summary op : optional op to gather all summaries   must return a scalar string tensor contain a serialize summary proto  ) ( saver : optional tf compat v1 train saver object to use to save and restore variables   may also be a tf train checkpoint object  in which case object base checkpoints be save  this will also load some object base checkpoints save from elsewhere  but that load may be fragile since it use fix key rather than perform a full graph base match  for example if a variable have two paths from the checkpoint object because two model object share the layer object that own it  remove one model may change the key and break checkpoint load through this api  whereas a graph base match would match the variable through the other model  ) ( copy from scaffold : optional scaffold object to copy field from  its field will be overwrite by the provide field in this function  ) ( local init feed dict : optional session fee dictionary to use when run the local init op  )

( local init op : an operation run immediately after session creation  usually use to initialize table and local variables  ) ( ready op : an operation to check if the model be initialize  ) ( ready for local init op : an operation to check if the model be ready to run local init op  ) ( graph : the graph that the model will use  ) ( recovery wait secs : second between check for the model to be ready  ) ( local init run options : runoptions to be pass to session run when execute the local init op  ) ( local init feed dict : optional session fee dictionary to use when run the local init op  )

( graph : a graph   the graph that the model will use   default to the default graph   the supervisor may add operations to the graph before create a session  but the graph should not be modify by the caller after pass it to the supervisor  ) ( ready op : 1 d string tensor   this tensor be evaluate by supervisors in prepare or wait for session   to check if the model be ready to use  the model be consider ready if it return an empty array   default to the tensor return from tf compat v1 report uninitialized variables   if none  the model be not check for readiness  ) ( ready for local init op : 1 d string tensor   this tensor be evaluate by supervisors in prepare or wait for session   to check if the model be ready to run the local init op  the model be consider ready if it return an empty array  default to none  if none  the model be not check for readiness before run local init op  ) ( is chief : if true  create a chief supervisor in charge of initialize and restore the model   if false  create a supervisor that rely on a chief supervisor for inits and restore  ) ( init op : operation   use by chief supervisors to initialize the model when it can not be recover   default to an operation that initialize all global variables   if none  no initialization be do automatically unless you pass a value for init fn  see below  ) ( init feed dict : a dictionary that map tensor object to fee value  this fee dictionary will be use when init op be evaluate  ) ( local init op : operation  use by all supervisors to run initializations that should run for every new supervisor instance  by default these be table initializers and initializers for local variables  if none  no further per supervisor instance initialization be do automatically  ) ( logdir : a string   optional path to a directory where to checkpoint the model and log events for the visualizer   use by chief supervisors  the directory will be create if it do not exist  ) ( summary op : an operation that return a summary for the event log  use by chief supervisors if a logdir be specify   default to the operation return from summary merge all     if none  summaries be not compute automatically  ) ( saver : a saver object   use by chief supervisors if a logdir be specify   default to the save return by saver    if none  the model be not save automatically  ) ( global step : an integer tensor of size 1 that count step   the value from 'global step' be use in summaries and checkpoint filenames  default to the op name 'global step' in the graph if it exist  be of rank 1  size 1  and of type tf int32 or tf int64   if none the global step be not record in summaries and checkpoint file   use by chief supervisors if a logdir be specify  ) ( save summaries secs : number of second between the computation of summaries for the event log   default to 120 second   pass 0 to disable summaries  ) ( save model secs : number of second between the creation of model checkpoints   default to 600 second   pass 0 to disable checkpoints  ) ( recovery wait secs : number of second between check that the model be ready   use by supervisors when wait for a chief supervisor to initialize or restore the model   default to 30 second  ) ( stop grace secs : grace period  in second  give to run thread to stop when stop   be call   default to 120 second  ) ( checkpoint basename : the basename for checkpoint save  ) ( session manager : sessionmanager  which manage session creation and recovery  if it be none  a default sessionmanager will be create with the set of arguments pass in for backwards compatibility  ) ( summary writer : summarywriter to use or use default   can be none to indicate that no summaries should be write  ) ( init fn : optional callable use to initialize the model  call after the optional init op be call   the callable must accept one argument  the session be initialize  ) ( local init run options : runoptions to be pass as the sessionmanager local init run options parameter  )
( opt : the actual optimizer that will be use to compute and apply the gradients  must be one of the optimizer class  ) ( replicas to aggregate : number of replicas to aggregate for each variable update  ) ( total num replicas : total number of tasks/workers/replicas  could be different from replicas to aggregate  if total num replicas > replicas to aggregate  it be backup replicas   replicas to aggregate  if total num replicas < replicas to aggregate  replicas compute multiple batch per update to variables  ) ( variable averages : optional exponentialmovingaverage object  use to maintain move average for the variables pass in variables to average  ) ( variables to average : a list of variables that need to be average  only need if variable average be pass in  ) ( use locking : if true use lock for update operation  ) ( name : string  optional name of the return operation  )
( scaffold : a scaffold use for gather or build supportive ops  if not specify a default one be create  it's use to finalize the graph  ) ( master : string representation of the tensorflow master to use  ) ( config : configproto proto use to configure the session  ) ( max wait secs : maximum time to wait for the session to become available  )
( qr : a queuerunner  ) ( collection : a graphkey specify the graph collection to add the queue runner to   default to graphkeys queue runners  )
( global step tensor : tensor to test  )
( supervisor : tf compat v1 train supervisor to run the train service  ) ( train step fn : callable to execute one train step   call repeatedly as train step fn session   args   kwargs   ) ( args : optional positional arguments pass to train step fn  ) ( kwargs : optional keyword arguments pass to train step fn  ) ( master : master to use to create the train session   default to "" which cause the session to be create in the local process  )
( tensors : the list or dictionary of tensors to enqueue  ) ( batch size : the new batch size pull from the queue  ) ( num threads : the number of thread enqueuing tensors   the batch will be nondeterministic if num thread > 1  ) ( capacity : an integer  the maximum number of elements in the queue  ) ( enqueue many : whether each tensor in tensors be a single example  ) ( shapes :  optional  the shape for each example   default to the infer shape for tensors  ) ( dynamic pad : boolean   allow variable dimension in input shape  the give dimension be pad upon dequeue so that tensors within a batch have the same shape  ) ( allow smaller final batch :  optional  boolean  if true  allow the final batch to be smaller if there be insufficient items leave in the queue  ) ( shared name :  optional   if set  this queue will be share under the give name across multiple sessions  ) ( name :  optional  a name for the operations  )
( tensors list : a list of tuples or dictionaries of tensors to enqueue  ) ( batch size : an integer  the new batch size pull from the queue  ) ( capacity : an integer  the maximum number of elements in the queue  ) ( enqueue many : whether each tensor in tensor list list be a single example  ) ( shapes :  optional  the shape for each example   default to the infer shape for tensor list list i   ) ( dynamic pad : boolean   allow variable dimension in input shape  the give dimension be pad upon dequeue so that tensors within a batch have the same shape  ) ( allow smaller final batch :  optional  boolean  if true  allow the final batch to be smaller if there be insufficient items leave in the queue  ) ( shared name :  optional  if set  this queue will be share under the give name across multiple sessions  ) ( name :  optional  a name for the operations  )
( checkpoint prefix : the prefix of a v1 or v2 checkpoint  with v2 take priority   typically the result of saver save   or that of tf train latest checkpoint    regardless of sharded/non sharded or v1/v2  )
( learning rate : a scalar float32 or float64 tensor or a python number  the initial learn rate  ) ( global step : a scalar int32 or int64 tensor or a python number  global step to use for the decay computation  ) ( decay steps : a scalar int32 or int64 tensor or a python number  number of step to decay over  ) ( alpha : a scalar float32 or float64 tensor or a python number  minimum learn rate value as a fraction of learn rate  ) ( name : string  optional name of the operation   default to 'cosinedecay'  )
( learning rate : a scalar float32 or float64 tensor or a python number  the initial learn rate  ) ( global step : a scalar int32 or int64 tensor or a python number  global step to use for the decay computation  ) ( first decay steps : a scalar int32 or int64 tensor or a python number  number of step to decay over  ) ( t mul : a scalar float32 or float64 tensor or a python number  use to derive the number of iterations in the i th period ) ( m mul : a scalar float32 or float64 tensor or a python number  use to derive the initial learn rate of the i th period  ) ( alpha : a scalar float32 or float64 tensor or a python number  minimum learn rate value as a fraction of the learn rate  ) ( name : string  optional name of the operation   default to 'sgdrdecay'  )
( graph : the graph in which to create the global step tensor  if miss  use default graph  )
( input graph : a graphdef  ) ( num bits : the number of bits for quantize train  )
( learning rate : a scalar float32 or float64 tensor or a python number  the initial learn rate  ) ( global step : a scalar int32 or int64 tensor or a python number  global step to use for the decay computation   must not be negative  ) ( decay steps : a scalar int32 or int64 tensor or a python number  must be positive   see the decay computation above  ) ( decay rate : a scalar float32 or float64 tensor or a python number  the decay rate  ) ( staircase : boolean   if true decay the learn rate at discrete intervals ) ( name : string   optional name of the operation   default to 'exponentialdecay'  )
( filename : optional filename include the path for write the generate metagraphdef protocol buffer  ) ( meta info def : metainfodef protocol buffer  ) ( graph def : graphdef protocol buffer  ) ( saver def : saverdef protocol buffer  ) ( collection list : list of string key to collect  ) ( as text : if true  write the metagraphdef as an ascii proto  ) ( graph : the graph to export  if none  use the default graph  ) ( export scope : optional string  name scope under which to extract the subgraph  the scope name will be strip from the node definitions for easy import later into new name scopes  if none  the whole graph be export  graph def and export scope cannot both be specify  ) ( clear devices : whether or not to clear the device field for an operation or tensor during export  ) ( clear extraneous savers : remove any saver relate information from the graph  both save/restore ops and saverdefs  that be not associate with the provide saverdef  ) ( strip default attrs : boolean  if true  default value attribute will be remove from the nodedefs  for a detail guide  see strip default value attribute  ) ( save debug info : if true  save the graphdebuginfo to a separate file  which in the same directory of filename and with  debug add before the file extend  ) (   kwargs : optional key arguments  )
( save dir : directory where the model be save  ) ( model checkpoint path : the checkpoint file  ) ( all model checkpoint paths : list of string   paths to all not yet delete checkpoints  sort from oldest to newest   if this be a non empty list  the last element must be equal to model checkpoint path   these paths be also save in the checkpointstate proto  ) ( all model checkpoint timestamps : a list of float  indicate the number of second since the epoch when each checkpoint be generate  ) ( last preserved timestamp : a float  indicate the number of second since the epoch when the last preserve checkpoint be write  e g  due to a keep checkpoint every n hours parameter  see tf train checkpointmanager for an implementation   )
( checkpoint prefixes : a list of checkpoint paths  typically the result of saver save   or those of tf train latest checkpoint    regardless of sharded/non sharded or v1/v2  )
( graph : the graph to find the global step in  if miss  use default graph  )
( graph : the graph in which to create the global step tensor  if miss  use default graph  )
( sess : a tensorflow session object  ) ( global step tensor : tensor or the name of the operation that contain the global step  )
( meta graph or file : metagraphdef protocol buffer or filename  include the path  contain a metagraphdef  ) ( clear devices : whether or not to clear the device field for an operation or tensor during import  ) ( import scope : optional string  name scope to add  only use when initialize from protocol buffer  ) (   kwargs : optional key arguments  )
( ckpt dir or file : directory with checkpoints file or path to checkpoint  ) ( assignment map : dict  or a list of key value pair  where key be name of the variables in the checkpoint and value be current variables or name of current variables  in default graph   )
( input tensor : a tensor with the row to produce  must be at least one dimensional  must either have a fully define shape  or element shape must be define  ) ( element shape :  optional   a tensorshape represent the shape of a row of input tensor  if it cannot be infer  ) ( num epochs :  optional   an integer  if specify input producer produce each row of input tensor num epochs time before generate an outofrange error  if not specify  input producer can cycle through the row of input tensor an unlimited number of time  ) ( shuffle :  optional   a boolean  if true  the row be randomly shuffle within each epoch  ) ( seed :  optional   an integer  the seed to use if shuffle be true  ) ( capacity :  optional   the capacity of the queue to be use for buffer the input  ) ( shared name :  optional   if set  this queue will be share under the give name across multiple sessions  ) ( summary name :  optional   if set  a scalar summary for the current queue size will be generate  use this name as part of the tag  ) ( name :  optional   a name for queue  ) ( cancel op :  optional   cancel op for the queue )
( learning rate : a scalar float32 or float64 tensor or a python number  the initial learn rate  ) ( global step : a python number  global step to use for the decay computation  must not be negative  ) ( decay steps : how often to apply decay  ) ( decay rate : a python number   the decay rate  ) ( staircase : whether to apply decay in a discrete staircase  as oppose to continuous  fashion  ) ( name : string   optional name of the operation   default to 'inversetimedecay'  )
( tensor : any tensor  ) ( num epochs : a positive integer  optional    if specify  limit the number of step the output tensor may be evaluate  ) ( name : a name for the operations  optional   )
( learning rate : a scalar float32 or float64 tensor or a python number  the initial learn rate  ) ( global step : a scalar int32 or int64 tensor or a python number  global step to use for the decay computation  ) ( decay steps : a scalar int32 or int64 tensor or a python number  number of step to decay over  ) ( num periods : number of periods in the cosine part of the decay  see computation above  ) ( alpha : see computation above  ) ( beta : see computation above  ) ( name : string   optional name of the operation   default to 'linearcosinedecay'  )
( tensors : the list or dictionary of tensors to enqueue  ) ( keep input : a bool tensor   this tensor control whether the input be add to the queue or not   if it be a scalar and evaluate true  then tensors be all add to the queue  if it be a vector and enqueue many be true  then each example be add to the queue only if the correspond value in keep input be true  this tensor essentially act as a filter mechanism  ) ( batch size : the new batch size pull from the queue  ) ( num threads : the number of thread enqueuing tensors   the batch will be nondeterministic if num thread > 1  ) ( capacity : an integer  the maximum number of elements in the queue  ) ( enqueue many : whether each tensor in tensors be a single example  ) ( shapes :  optional  the shape for each example   default to the infer shape for tensors  ) ( dynamic pad : boolean   allow variable dimension in input shape  the give dimension be pad upon dequeue so that tensors within a batch have the same shape  ) ( allow smaller final batch :  optional  boolean  if true  allow the final batch to be smaller if there be insufficient items leave in the queue  ) ( shared name :  optional   if set  this queue will be share under the give name across multiple sessions  ) ( name :  optional  a name for the operations  )
( tensors list : a list of tuples or dictionaries of tensors to enqueue  ) ( keep input : a bool tensor   this tensor control whether the input be add to the queue or not   if it be a scalar and evaluate true  then tensors be all add to the queue  if it be a vector and enqueue many be true  then each example be add to the queue only if the correspond value in keep input be true  this tensor essentially act as a filter mechanism  ) ( batch size : an integer  the new batch size pull from the queue  ) ( capacity : an integer  the maximum number of elements in the queue  ) ( enqueue many : whether each tensor in tensor list list be a single example  ) ( shapes :  optional  the shape for each example   default to the infer shape for tensor list list i   ) ( dynamic pad : boolean   allow variable dimension in input shape  the give dimension be pad upon dequeue so that tensors within a batch have the same shape  ) ( allow smaller final batch :  optional  boolean  if true  allow the final batch to be smaller if there be insufficient items leave in the queue  ) ( shared name :  optional  if set  this queue will be share under the give name across multiple sessions  ) ( name :  optional  a name for the operations  )
( tensors : the list or dictionary of tensors to enqueue  ) ( batch size : the new batch size pull from the queue  ) ( capacity : an integer  the maximum number of elements in the queue  ) ( min after dequeue : minimum number elements in the queue after a dequeue  use to ensure a level of mix of elements  ) ( keep input : a bool tensor   this tensor control whether the input be add to the queue or not   if it be a scalar and evaluate true  then tensors be all add to the queue  if it be a vector and enqueue many be true  then each example be add to the queue only if the correspond value in keep input be true  this tensor essentially act as a filter mechanism  ) ( num threads : the number of thread enqueuing tensor list  ) ( seed : seed for the random shuffle within the queue  ) ( enqueue many : whether each tensor in tensor list be a single example  ) ( shapes :  optional  the shape for each example   default to the infer shape for tensor list  ) ( allow smaller final batch :  optional  boolean  if true  allow the final batch to be smaller if there be insufficient items leave in the queue  ) ( shared name :  optional  if set  this queue will be share under the give name across multiple sessions  ) ( name :  optional  a name for the operations  )
( tensors list : a list of tuples or dictionaries of tensors to enqueue  ) ( batch size : an integer  the new batch size pull from the queue  ) ( capacity : an integer  the maximum number of elements in the queue  ) ( min after dequeue : minimum number elements in the queue after a dequeue  use to ensure a level of mix of elements  ) ( keep input : a bool tensor   this tensor control whether the input be add to the queue or not   if it be a scalar and evaluate true  then tensors be all add to the queue  if it be a vector and enqueue many be true  then each example be add to the queue only if the correspond value in keep input be true  this tensor essentially act as a filter mechanism  ) ( seed : seed for the random shuffle within the queue  ) ( enqueue many : whether each tensor in tensor list list be a single example  ) ( shapes :  optional  the shape for each example   default to the infer shape for tensors list i   ) ( allow smaller final batch :  optional  boolean  if true  allow the final batch to be smaller if there be insufficient items leave in the queue  ) ( shared name :  optional   if set  this queue will be share under the give name across multiple sessions  ) ( name :  optional  a name for the operations  )
( learning rate : a scalar float32 or float64 tensor or a python number  the initial learn rate  ) ( global step : a python number  global step to use for the decay computation  must not be negative  ) ( decay steps : how often to apply decay  ) ( decay rate : a python number   the decay rate  ) ( staircase : whether to apply decay in a discrete staircase  as oppose to continuous  fashion  ) ( name : string   optional name of the operation   default to 'exponentialtimedecay'  )
( learning rate : a scalar float32 or float64 tensor or a python number  the initial learn rate  ) ( global step : a scalar int32 or int64 tensor or a python number  global step to use for the decay computation  ) ( decay steps : a scalar int32 or int64 tensor or a python number  number of step to decay over  ) ( initial variance : initial variance for the noise  see computation above  ) ( variance decay : decay for the noise's variance  see computation above  ) ( num periods : number of periods in the cosine part of the decay  see computation above  ) ( alpha : see computation above  ) ( beta : see computation above  ) ( name : string   optional name of the operation   default to 'noisylinearcosinedecay'  )
( x : a 0 d scalar tensor  must be one of the follow type  float32  float64  uint8  int8  int16  int32  int64  ) ( boundaries : a list of tensors or ints or float with strictly increase entries  and with all elements have the same type as x  ) ( values : a list of tensors or float or ints that specify the value for the intervals define by boundaries  it should have one more element than boundaries  and all elements should have the same type  ) ( name : a string  optional name of the operation  default to 'piecewiseconstant'  )
( learning rate : a scalar float32 or float64 tensor or a python number  the initial learn rate  ) ( global step : a scalar int32 or int64 tensor or a python number  global step to use for the decay computation   must not be negative  ) ( decay steps : a scalar int32 or int64 tensor or a python number  must be positive   see the decay computation above  ) ( end learning rate : a scalar float32 or float64 tensor or a python number   the minimal end learn rate  ) ( power : a scalar float32 or float64 tensor or a python number   the power of the polynomial  default to linear  1 0  ) ( cycle : a boolean  whether or not it should cycle beyond decay step  ) ( name : string   optional name of the operation  default to 'polynomialdecay'  )
( limit : an int32 scalar tensor  ) ( num epochs : an integer  optional   if specify  range input producer produce each integer num epochs time before generate an outofrange error  if not specify  range input producer can cycle through the integers an unlimited number of time  ) ( shuffle : boolean  if true  the integers be randomly shuffle within each epoch  ) ( seed : an integer  optional   seed use if shuffle    true  ) ( capacity : an integer  set the queue capacity  ) ( shared name :  optional   if set  this queue will be share under the give name across multiple sessions  ) ( name : a name for the operations  optional   )
( checkpoint prefix : the prefix of a v1 or v2 checkpoint  typically the result of saver save   or that of tf train latest checkpoint    regardless of sharded/non sharded or v1/v2  ) ( checkpoint format version : saverdef checkpointformatversion  default to saverdef v2  ) ( meta graph suffix : suffix for metagraphdef file  default to 'meta'  )
( ps tasks : number of task in the ps job   ignore if cluster be provide  ) ( ps device : string   device of the ps job   if empty no ps job be use  default to ps  ) ( worker device : string   device of the worker job   if empty no worker job be use  ) ( merge devices : boolean  if true  merge or only set a device if the device constraint be completely unset  merge device specification rather than override them  ) ( cluster : clusterdef proto or clusterspec  ) ( ps ops : list of string represent operation type that need to be place on ps devices   if none  default to standard ps ops  ) ( ps strategy : a callable invoke for every ps operation  i e  match by ps ops   that take the operation and return the ps task index to use   if none  default to a round robin strategy across all ps devices  )
( input : a tensor of type string  vector of string to compute fingerprint on  ) ( name : a name for the operation  optional   )
( sparse example indices : a list of tensor object with type int64  a list of vectors which contain example indices  ) ( sparse feature indices : a list with the same length as sparse example indices of tensor object with type int64  a list of vectors which contain feature indices  ) ( sparse feature values : a list of tensor object with type float32  a list of vectors which contain feature value associate with each feature group  ) ( dense features : a list of tensor object with type float32  a list of matrices which contain the dense feature value  ) ( example weights : a tensor of type float32  a vector which contain the weight associate with each example  ) ( example labels : a tensor of type float32  a vector which contain the label/target associate with each example  ) ( sparse indices : a list with the same length as sparse example indices of tensor object with type int64  a list of vectors where each value be the indices which have correspond weight in sparse weight  this field maybe omit for the dense approach  ) ( sparse weights : a list with the same length as sparse example indices of tensor object with type float32  a list of vectors where each value be the weight associate with a sparse feature group  ) ( dense weights : a list with the same length as dense feature of tensor object with type float32  a list of vectors where the value be the weight associate with a dense feature group  ) ( example state data : a tensor of type float32  a list of vectors contain the example state data  ) ( loss type : a string from  "logistic loss"  "squared loss"  "hinge loss"  "smooth hinge loss"  "poisson loss"  type of the primal loss  currently sdcasolver support logistic  square and hinge losses  ) ( l1 : a float  symmetric l1 regularization strength  ) ( l2 : a float  symmetric l2 regularization strength  ) ( num loss partitions : an int that be >  1  number of partition of the global loss function  ) ( num inner iterations : an int that be >  1  number of iterations per mini batch  ) ( adaptative : an optional bool  default to true  whether to use adaptive sdca for the inner loop  ) ( name : a name for the operation  optional   )
( weights : a list of tensor object with type mutable float32  a list of vectors where each value be the weight associate with a feature group  ) ( l1 : a float  symmetric l1 regularization strength  ) ( l2 : a float  symmetric l2 regularization strength  should be a positive float  ) ( name : a name for the operation  optional   )
( tensors : the list or dictionary of tensors to enqueue  ) ( batch size : the new batch size pull from the queue  ) ( capacity : an integer  the maximum number of elements in the queue  ) ( min after dequeue : minimum number elements in the queue after a dequeue  use to ensure a level of mix of elements  ) ( num threads : the number of thread enqueuing tensor list  ) ( seed : seed for the random shuffle within the queue  ) ( enqueue many : whether each tensor in tensor list be a single example  ) ( shapes :  optional  the shape for each example   default to the infer shape for tensor list  ) ( allow smaller final batch :  optional  boolean  if true  allow the final batch to be smaller if there be insufficient items leave in the queue  ) ( shared name :  optional  if set  this queue will be share under the give name across multiple sessions  ) ( name :  optional  a name for the operations  )
( tensors list : a list of tuples or dictionaries of tensors to enqueue  ) ( batch size : an integer  the new batch size pull from the queue  ) ( capacity : an integer  the maximum number of elements in the queue  ) ( min after dequeue : minimum number elements in the queue after a dequeue  use to ensure a level of mix of elements  ) ( seed : seed for the random shuffle within the queue  ) ( enqueue many : whether each tensor in tensor list list be a single example  ) ( shapes :  optional  the shape for each example   default to the infer shape for tensors list i   ) ( allow smaller final batch :  optional  boolean  if true  allow the final batch to be smaller if there be insufficient items leave in the queue  ) ( shared name :  optional   if set  this queue will be share under the give name across multiple sessions  ) ( name :  optional  a name for the operations  )
( tensor list : a list of tensor object  every tensor in tensor list must have the same size in the first dimension  ) ( num epochs : an integer  optional   if specify  slice input producer produce each slice num epochs time before generate an outofrange error  if not specify  slice input producer can cycle through the slice an unlimited number of time  ) ( shuffle : boolean  if true  the integers be randomly shuffle within each epoch  ) ( seed : an integer  optional   seed use if shuffle    true  ) ( capacity : an integer  set the queue capacity  ) ( shared name :  optional   if set  this queue will be share under the give name across multiple sessions  ) ( name : a name for the operations  optional   )
( sess : session use to run the queue ops   default to the default session  ) ( coord : optional coordinator for coordinate the start thread  ) ( daemon : whether the thread should be mark as daemons  mean they don't block program exit  ) ( start : set to false to only create the thread  not start them  ) ( collection : a graphkey specify the graph collection to get the queue runners from   default to graphkeys queue runners  )
( string tensor : a 1 d string tensor with the string to produce  ) ( num epochs : an integer  optional   if specify  string input producer produce each string from string tensor num epochs time before generate an outofrange error  if not specify  string input producer can cycle through the string in string tensor an unlimited number of time  ) ( shuffle : boolean  if true  the string be randomly shuffle within each epoch  ) ( seed : an integer  optional   seed use if shuffle    true  ) ( capacity : an integer  set the queue capacity  ) ( shared name :  optional   if set  this queue will be share under the give name across multiple sessions  all sessions open to the device which have this queue will be able to access it via the share name  use this in a distribute set mean each name will only be see by one of the sessions which have access to this operation  ) ( name : a name for the operations  optional   ) ( cancel op : cancel op for the queue  optional   )
( path : the path to an event file create by a summarywriter  )
( save dir : directory where the model be save  ) ( model checkpoint path : the checkpoint file  ) ( all model checkpoint paths : list of string   paths to all not yet delete checkpoints  sort from oldest to newest   if this be a non empty list  the last element must be equal to model checkpoint path   these paths be also save in the checkpointstate proto  ) ( latest filename : optional name of the checkpoint file   default to 'checkpoint'  ) ( all model checkpoint timestamps : optional list of timestamps  float  second since the epoch  indicate when the checkpoints in all model checkpoint paths be create  ) ( last preserved timestamp : a float  indicate the number of second since the epoch when the last preserve checkpoint be write  e g  due to a keep checkpoint every n hours parameter  see tf train checkpointmanager for an implementation   )
( ckpt to initialize from :  require  a string specify the directory with checkpoint file s  or path to checkpoint from which to warm start the model parameters  ) ( vars to warm start :  optional  one of the follow   a regular expression  string  that capture which variables to warm start  see tf compat v1 get collection    this expression will only consider variables in the trainable variables collection    if you need to warm start non trainable vars  such as optimizer accumulators or batch norm statistics   please use the below option  a list of string  each a regex scope provide to tf compat v1 get collection with global variables  please see tf compat v1 get collection    for backwards compatibility reason  this be separate from the single string argument type  a list of variables to warm start   if you do not have access to the variable object at the call site  please use the above option  none  in which case only trainable variables specify in var name to vocab info will be warm start   default to '  '  which warm start all variables in the trainable variables collection   note that this exclude variables such as accumulators and move statistics from batch norm  ) ( var name to vocab info :  optional  dict of variable name  string  to tf estimator vocabinfo  the variable name should be "full" variables  not the name of the partition   if not explicitly provide  the variable be assume to have no  change to  vocabulary  ) ( var name to prev var name :  optional  dict of variable name  string  to name of the previously train variable in ckpt to initialize from  if not explicitly provide  the name of the variable be assume to be same between previous checkpoint and current model   note that this have no effect on the set of variables that be warm start  and only control name map  use vars to warm start for control what variables to warm start   )

( scores : a float tensor give score  sometimes but not always interpretable as probabilities  for each class   may be none  but only if class be set   interpretation vary   see class doc  ) ( classes : a string tensor give predict class label   may be none  but only if score be set   interpretation vary   see class doc  )

( outputs : a tensor or a dict of string to tensor represent the predictions  )
( value : a float tensor give the predict value   require  )
( features : a tensor  sparsetensor  or dict of string or int to tensor or sparsetensor  specify the feature to be pass to the model  note  if feature pass be not a dict  it will be wrap in a dict   with a single entry  use 'feature' as the key   consequently  the   model must accept a feature dict of the form  'feature'  tensor    you may use   tensorservinginputreceiver if you want the tensor to be pass as be  ) ( receiver tensors : a tensor  sparsetensor  or dict of string to tensor or sparsetensor  specify input nod where this receiver expect to be feed by default   typically  this be a single placeholder expect serialize tf example protos  ) ( receiver tensors alternatives : a dict of string to additional group of receiver tensors  each of which may be a tensor  sparsetensor  or dict of string to tensor orsparsetensor  these name receiver tensor alternatives generate additional serve signatures  which may be use to fee input at different point within the input receiver subgraph   a typical usage be to allow feed raw feature tensors downstream of the tf parse example   op  default to none  )
( features : a single tensor or sparsetensor  represent the feature to be pass to the model  ) ( receiver tensors : a tensor  sparsetensor  or dict of string to tensor or sparsetensor  specify input nod where this receiver expect to be feed by default   typically  this be a single placeholder expect serialize tf example protos  ) ( receiver tensors alternatives : a dict of string to additional group of receiver tensors  each of which may be a tensor  sparsetensor  or dict of string to tensor orsparsetensor  these name receiver tensor alternatives generate additional serve signatures  which may be use to fee input at different point within the input receiver subgraph   a typical usage be to allow feed raw feature tensors downstream of the tf parse example   op  default to none  )
( feature spec : a dict of string to varlenfeature/fixedlenfeature  ) ( default batch size : the number of query examples expect per batch  leave unset for variable batch size  recommend   )
( features : a dict of string to tensor  ) ( default batch size : the number of query examples expect per batch  leave unset for variable batch size  recommend   )
( shape : a shape tuple  integers   not include the batch size  for instance  shape  32   indicate that the expect input will be batch of 32 dimensional vectors  elements of this tuple can be none  'none' elements represent dimension where the shape be not know  ) ( batch size : optional static batch size  integer   ) ( name : an optional name string for the layer  should be unique in a model  do not reuse the same name twice   it will be autogenerated if it isn't provide  ) ( dtype : the data type expect by the input  as a string  float32  float64  int32     ) ( sparse : a boolean specify whether the placeholder to be create be sparse  only one of 'ragged' and 'sparse' can be true  note that  if sparse be false  sparse tensors can still be pass into the input   they will be densified with a default value of 0  ) ( tensor : optional exist tensor to wrap into the input layer  if set  the layer will use the tf typespec of this tensor rather than create a new placeholder tensor  ) ( ragged : a boolean specify whether the placeholder to be create be rag  only one of 'ragged' and 'sparse' can be true  in this case  value of 'none' in the 'shape' argument represent rag dimension  for more information about raggedtensors  see this guide  ) ( type spec : a tf typespec object to create the input placeholder from  when provide  all other args except name must be none  ) (   kwargs : deprecate arguments support  support batch shape and batch input shape  )
( inputs : the input s  of the model  a keras input object or list of keras input object  ) ( outputs : the output s  of the model  see functional api example below  ) ( name : string  the name of the model  )
( layers : optional list of layer to add to the model  ) ( name : optional name for the model  )
( name : the name of the activation function  ) ( custom objects : optional  function name  function obj  dictionary list user provide activation function  )
( x : input tensor  ) ( alpha : a scalar  slope of negative section  alpha control the value to which an elu saturate for negative net input  )
( x : input tensor  )
( identifier : function or string )
( x : input tensor  )
( x : input tensor  )
( x : input tensor or variable  ) ( alpha : a float that govern the slope for value lower than the threshold  ) ( max value : a float that set the saturation threshold  the largest value the function will return   ) ( threshold : a float give the threshold value of the activation function below which value will be damp or set to zero  )
( x : a tensor or variable to compute the activation function for  )
( activation : function object  )
( x : input tensor  )
( x : input tensor  ) ( axis : integer  axis along which the softmax normalization be apply  )
( x : input tensor  )
( x : input tensor  )
( x : input tensor  )
( include top : whether to include the fully connect layer at the top of the network  ) ( weights : one of none  random initialization   'imagenet'  pre train on imagenet   or the path to the weight file to be load  ) ( input tensor : optional keras tensor  i e  output of layer input    to use as image input for the model  ) ( input shape : optional shape tuple  only to be specify if include top be false  otherwise the input shape have to be  224  224  3   with 'channels last' data format  or  3  224  224   with 'channels first' data format   it should have exactly 3 input channel  and width and height should be no smaller than 32  e g   200  200  3  would be one valid value  ) ( pooling : optional pool mode for feature extraction when include top be false   none mean that the output of the model will be the 4d tensor output of the last convolutional block  avg mean that global average pool will be apply to the output of the last convolutional block  and thus the output of the model will be a 2d tensor  max mean that global max pool will be apply  ) ( classes : optional number of class to classify image into  only to be specify if include top be true  and if no weight argument be specify  ) ( classifier activation : a str or callable  the activation function to use on the "top" layer  ignore unless include top true  set classifier activation none to return the logits of the "top" layer  when load pretrained weight  classifier activation can only be none or "softmax"  )
( include top : whether to include the fully connect layer at the top of the network  ) ( weights : one of none  random initialization   'imagenet'  pre train on imagenet   or the path to the weight file to be load  ) ( input tensor : optional keras tensor  i e  output of layer input    to use as image input for the model  ) ( input shape : optional shape tuple  only to be specify if include top be false  otherwise the input shape have to be  224  224  3   with 'channels last' data format  or  3  224  224   with 'channels first' data format   it should have exactly 3 input channel  and width and height should be no smaller than 32  e g   200  200  3  would be one valid value  ) ( pooling : optional pool mode for feature extraction when include top be false   none mean that the output of the model will be the 4d tensor output of the last convolutional block  avg mean that global average pool will be apply to the output of the last convolutional block  and thus the output of the model will be a 2d tensor  max mean that global max pool will be apply  ) ( classes : optional number of class to classify image into  only to be specify if include top be true  and if no weight argument be specify  ) ( classifier activation : a str or callable  the activation function to use on the "top" layer  ignore unless include top true  set classifier activation none to return the logits of the "top" layer  when load pretrained weight  classifier activation can only be none or "softmax"  )
( include top : whether to include the fully connect layer at the top of the network  ) ( weights : one of none  random initialization   'imagenet'  pre train on imagenet   or the path to the weight file to be load  ) ( input tensor : optional keras tensor  i e  output of layer input    to use as image input for the model  ) ( input shape : optional shape tuple  only to be specify if include top be false  otherwise the input shape have to be  224  224  3   with 'channels last' data format  or  3  224  224   with 'channels first' data format   it should have exactly 3 input channel  and width and height should be no smaller than 32  e g   200  200  3  would be one valid value  ) ( pooling : optional pool mode for feature extraction when include top be false   none mean that the output of the model will be the 4d tensor output of the last convolutional block  avg mean that global average pool will be apply to the output of the last convolutional block  and thus the output of the model will be a 2d tensor  max mean that global max pool will be apply  ) ( classes : optional number of class to classify image into  only to be specify if include top be true  and if no weight argument be specify  ) ( classifier activation : a str or callable  the activation function to use on the "top" layer  ignore unless include top true  set classifier activation none to return the logits of the "top" layer  when load pretrained weight  classifier activation can only be none or "softmax"  )
( preds : numpy array encode a batch of predictions  ) ( top : integer  how many top guess to return  default to 5  )
( x : a float point numpy array or a tf tensor  3d or 4d with 3 color channel  with value in the range  0  255   the preprocessed data be write over the input data if the data type be compatible  to avoid this behaviour  numpy copy x  can be use  ) ( data format : optional data format of the image tensor/array  default to none  in which case the global set tf keras backend image data format   be use  unless you change it  it default to "channels last"   )
( preds : numpy array encode a batch of predictions  ) ( top : integer  how many top guess to return  default to 5  )
( x : a float point numpy array or a tf tensor  3d or 4d with 3 color channel  with value in the range  0  255   the preprocessed data be write over the input data if the data type be compatible  to avoid this behaviour  numpy copy x  can be use  ) ( data format : optional data format of the image tensor/array  default to none  in which case the global set tf keras backend image data format   be use  unless you change it  it default to "channels last"   ) ( mode : one of "caffe"  "tf" or "torch"  default to "caffe"   caffe  will convert the image from rgb to bgr  then will zero center each color channel with respect to the imagenet dataset  without scale  tf  will scale pixels between  1 and 1  sample wise  torch  will scale pixels between 0 and 1 and then will normalize each channel with respect to the imagenet dataset  )
( include top : whether to include the fully connect layer at the top of the network  ) ( weights : one of none  random initialization   'imagenet'  pre train on imagenet   or the path to the weight file to be load  ) ( input tensor : optional keras tensor  i e  output of layer input    to use as image input for the model  ) ( input shape : optional shape tuple  only to be specify if include top be false  otherwise the input shape have to be  299  299  3   with 'channels last' data format  or  3  299  299   with 'channels first' data format   it should have exactly 3 input channel  and width and height should be no smaller than 75  e g   150  150  3  would be one valid value  ) ( pooling : optional pool mode for feature extraction when include top be false   none mean that the output of the model will be the 4d tensor output of the last convolutional block  'avg' mean that global average pool will be apply to the output of the last convolutional block  and thus the output of the model will be a 2d tensor  'max' mean that global max pool will be apply  ) ( classes : optional number of class to classify image into  only to be specify if include top be true  and if no weight argument be specify  ) ( classifier activation : a str or callable  the activation function to use on the "top" layer  ignore unless include top true  set classifier activation none to return the logits of the "top" layer  when load pretrained weight  classifier activation can only be none or "softmax"  ) (   kwargs : for backwards compatibility only  )
( preds : numpy array encode a batch of predictions  ) ( top : integer  how many top guess to return  default to 5  )
( x : a float point numpy array or a tf tensor  3d or 4d with 3 color channel  with value in the range  0  255   the preprocessed data be write over the input data if the data type be compatible  to avoid this behaviour  numpy copy x  can be use  ) ( data format : optional data format of the image tensor/array  default to none  in which case the global set tf keras backend image data format   be use  unless you change it  it default to "channels last"   )
( include top : boolean  whether to include the fully connect layer at the top  as the last layer of the network  default to true  ) ( weights : one of none  random initialization   imagenet  pre train on imagenet   or the path to the weight file to be load  default to imagenet  ) ( input tensor : optional keras tensor  i e  output of layer input    to use as image input for the model  input tensor be useful for share input between multiple different network  default to none  ) ( input shape : optional shape tuple  only to be specify if include top be false  otherwise the input shape have to be  299  299  3   with channel last data format  or  3  299  299   with channel first data format   it should have exactly 3 input channel  and width and height should be no smaller than 75  e g   150  150  3  would be one valid value  input shape will be ignore if the input tensor be provide  ) ( pooling : optional pool mode for feature extraction when include top be false   none  default  mean that the output of the model will be the 4d tensor output of the last convolutional block  avg mean that global average pool will be apply to the output of the last convolutional block  and thus the output of the model will be a 2d tensor  max mean that global max pool will be apply  ) ( classes : optional number of class to classify image into  only to be specify if include top be true  and if no weight argument be specify  default to 1000  ) ( classifier activation : a str or callable  the activation function to use on the "top" layer  ignore unless include top true  set classifier activation none to return the logits of the "top" layer  when load pretrained weight  classifier activation can only be none or "softmax"  )
( preds : numpy array encode a batch of predictions  ) ( top : integer  how many top guess to return  default to 5  )
( x : a float point numpy array or a tf tensor  3d or 4d with 3 color channel  with value in the range  0  255   the preprocessed data be write over the input data if the data type be compatible  to avoid this behaviour  numpy copy x  can be use  ) ( data format : optional data format of the image tensor/array  default to none  in which case the global set tf keras backend image data format   be use  unless you change it  it default to "channels last"   )
( input shape : optional shape tuple  only to be specify if include top be false  otherwise the input shape have to be  224  224  3   with channel last data format  or  3  224  224   with channel first data format   it should have exactly 3 input channel  and width and height should be no smaller than 32  e g   200  200  3  would be one valid value  default to none  input shape will be ignore if the input tensor be provide  ) ( alpha : control the width of the network  this be know as the width multiplier in the mobilenet paper    if alpha < 1 0  proportionally decrease the number of filter in each layer    if alpha > 1 0  proportionally increase the number of filter in each layer    if alpha   1  default number of filter from the paper be use at each layer  default to 1 0  ) ( depth multiplier : depth multiplier for depthwise convolution  this be call the resolution multiplier in the mobilenet paper  default to 1 0  ) ( dropout : dropout rate  default to 0 001  ) ( include top : boolean  whether to include the fully connect layer at the top of the network  default to true  ) ( weights : one of none  random initialization   'imagenet'  pre train on imagenet   or the path to the weight file to be load  default to imagenet  ) ( input tensor : optional keras tensor  i e  output of layer input    to use as image input for the model  input tensor be useful for share input between multiple different network  default to none  ) ( pooling : optional pool mode for feature extraction when include top be false   none  default  mean that the output of the model will be the 4d tensor output of the last convolutional block  avg mean that global average pool will be apply to the output of the last convolutional block  and thus the output of the model will be a 2d tensor  max mean that global max pool will be apply  ) ( classes : optional number of class to classify image into  only to be specify if include top be true  and if no weight argument be specify  default to 1000  ) ( classifier activation : a str or callable  the activation function to use on the "top" layer  ignore unless include top true  set classifier activation none to return the logits of the "top" layer  when load pretrained weight  classifier activation can only be none or "softmax"  ) (   kwargs : for backwards compatibility only  )
( preds : numpy array encode a batch of predictions  ) ( top : integer  how many top guess to return  default to 5  )
( x : a float point numpy array or a tf tensor  3d or 4d with 3 color channel  with value in the range  0  255   the preprocessed data be write over the input data if the data type be compatible  to avoid this behaviour  numpy copy x  can be use  ) ( data format : optional data format of the image tensor/array  default to none  in which case the global set tf keras backend image data format   be use  unless you change it  it default to "channels last"   )
( input shape : optional shape tuple  to be specify if you would like to use a model with an input image resolution that be not  224  224  3   it should have exactly 3 input channel  224  224  3   you can also omit this option if you would like to infer input shape from an input tensor  if you choose to include both input tensor and input shape then input shape will be use if they match  if the shape do not match then we will throw an error  e g   160  160  3  would be one valid value  ) ( alpha : float  larger than zero  control the width of the network  this be know as the width multiplier in the mobilenetv2 paper  but the name be keep for consistency with applications mobilenetv1 model in keras   if alpha < 1 0  proportionally decrease the number of filter in each layer  if alpha > 1 0  proportionally increase the number of filter in each layer  if alpha   1 0  default number of filter from the paper be use at each layer  ) ( include top : boolean  whether to include the fully connect layer at the top of the network  default to true  ) ( weights : string  one of none  random initialization   'imagenet'  pre train on imagenet   or the path to the weight file to be load  ) ( input tensor : optional keras tensor  i e  output of layer input    to use as image input for the model  ) ( pooling : string  optional pool mode for feature extraction when include top be false  none mean that the output of the model will be the 4d tensor output of the last convolutional block  avg mean that global average pool will be apply to the output of the last convolutional block  and thus the output of the model will be a 2d tensor  max mean that global max pool will be apply  ) ( classes : optional integer number of class to classify image into  only to be specify if include top be true  and if no weight argument be specify  ) ( classifier activation : a str or callable  the activation function to use on the "top" layer  ignore unless include top true  set classifier activation none to return the logits of the "top" layer  when load pretrained weight  classifier activation can only be none or "softmax"  ) (   kwargs : for backwards compatibility only  )
( preds : numpy array encode a batch of predictions  ) ( top : integer  how many top guess to return  default to 5  )
( x : a float point numpy array or a tf tensor  3d or 4d with 3 color channel  with value in the range  0  255   the preprocessed data be write over the input data if the data type be compatible  to avoid this behaviour  numpy copy x  can be use  ) ( data format : optional data format of the image tensor/array  default to none  in which case the global set tf keras backend image data format   be use  unless you change it  it default to "channels last"   )
( input shape : optional shape tuple  only to be specify if include top be false  otherwise the input shape have to be  331  331  3  for nasnetlarge  it should have exactly 3 input channel  and width and height should be no smaller than 32  e g   224  224  3  would be one valid value  ) ( include top : whether to include the fully connect layer at the top of the network  ) ( weights : none  random initialization  or imagenet  imagenet weight  for load imagenet weight  input shape should be  331  331  3  ) ( input tensor : optional keras tensor  i e  output of layer input    to use as image input for the model  ) ( pooling : optional pool mode for feature extraction when include top be false   none mean that the output of the model will be the 4d tensor output of the last convolutional layer  avg mean that global average pool will be apply to the output of the last convolutional layer  and thus the output of the model will be a 2d tensor  max mean that global max pool will be apply  ) ( classes : optional number of class to classify image into  only to be specify if include top be true  and if no weight argument be specify  ) ( classifier activation : a str or callable  the activation function to use on the "top" layer  ignore unless include top true  set classifier activation none to return the logits of the "top" layer  when load pretrained weight  classifier activation can only be none or "softmax"  )
( input shape : optional shape tuple  only to be specify if include top be false  otherwise the input shape have to be  224  224  3  for nasnetmobile it should have exactly 3 input channel  and width and height should be no smaller than 32  e g   224  224  3  would be one valid value  ) ( include top : whether to include the fully connect layer at the top of the network  ) ( weights : none  random initialization  or imagenet  imagenet weight  for load imagenet weight  input shape should be  224  224  3  ) ( input tensor : optional keras tensor  i e  output of layer input    to use as image input for the model  ) ( pooling : optional pool mode for feature extraction when include top be false   none mean that the output of the model will be the 4d tensor output of the last convolutional layer  avg mean that global average pool will be apply to the output of the last convolutional layer  and thus the output of the model will be a 2d tensor  max mean that global max pool will be apply  ) ( classes : optional number of class to classify image into  only to be specify if include top be true  and if no weight argument be specify  ) ( classifier activation : a str or callable  the activation function to use on the "top" layer  ignore unless include top true  set classifier activation none to return the logits of the "top" layer  when load pretrained weight  classifier activation can only be none or "softmax"  )
( preds : numpy array encode a batch of predictions  ) ( top : integer  how many top guess to return  default to 5  )
( x : a float point numpy array or a tf tensor  3d or 4d with 3 color channel  with value in the range  0  255   the preprocessed data be write over the input data if the data type be compatible  to avoid this behaviour  numpy copy x  can be use  ) ( data format : optional data format of the image tensor/array  default to none  in which case the global set tf keras backend image data format   be use  unless you change it  it default to "channels last"   )
( include top : whether to include the fully connect layer at the top of the network  ) ( weights : one of none  random initialization   'imagenet'  pre train on imagenet   or the path to the weight file to be load  ) ( input tensor : optional keras tensor  i e  output of layer input    to use as image input for the model  ) ( input shape : optional shape tuple  only to be specify if include top be false  otherwise the input shape have to be  224  224  3   with 'channels last' data format  or  3  224  224   with 'channels first' data format   it should have exactly 3 input channel  and width and height should be no smaller than 32  e g   200  200  3  would be one valid value  ) ( pooling : optional pool mode for feature extraction when include top be false   none mean that the output of the model will be the 4d tensor output of the last convolutional block  avg mean that global average pool will be apply to the output of the last convolutional block  and thus the output of the model will be a 2d tensor  max mean that global max pool will be apply  ) ( classes : optional number of class to classify image into  only to be specify if include top be true  and if no weight argument be specify  ) ( classifier activation : a str or callable  the activation function to use on the "top" layer  ignore unless include top true  set classifier activation none to return the logits of the "top" layer  when load pretrained weight  classifier activation can only be none or "softmax"  )
( include top : whether to include the fully connect layer at the top of the network  ) ( weights : one of none  random initialization   'imagenet'  pre train on imagenet   or the path to the weight file to be load  ) ( input tensor : optional keras tensor  i e  output of layer input    to use as image input for the model  ) ( input shape : optional shape tuple  only to be specify if include top be false  otherwise the input shape have to be  224  224  3   with 'channels last' data format  or  3  224  224   with 'channels first' data format   it should have exactly 3 input channel  and width and height should be no smaller than 32  e g   200  200  3  would be one valid value  ) ( pooling : optional pool mode for feature extraction when include top be false   none mean that the output of the model will be the 4d tensor output of the last convolutional block  avg mean that global average pool will be apply to the output of the last convolutional block  and thus the output of the model will be a 2d tensor  max mean that global max pool will be apply  ) ( classes : optional number of class to classify image into  only to be specify if include top be true  and if no weight argument be specify  ) ( classifier activation : a str or callable  the activation function to use on the "top" layer  ignore unless include top true  set classifier activation none to return the logits of the "top" layer  when load pretrained weight  classifier activation can only be none or "softmax"  )
( include top : whether to include the fully connect layer at the top of the network  ) ( weights : one of none  random initialization   'imagenet'  pre train on imagenet   or the path to the weight file to be load  ) ( input tensor : optional keras tensor  i e  output of layer input    to use as image input for the model  ) ( input shape : optional shape tuple  only to be specify if include top be false  otherwise the input shape have to be  224  224  3   with 'channels last' data format  or  3  224  224   with 'channels first' data format   it should have exactly 3 input channel  and width and height should be no smaller than 32  e g   200  200  3  would be one valid value  ) ( pooling : optional pool mode for feature extraction when include top be false   none mean that the output of the model will be the 4d tensor output of the last convolutional block  avg mean that global average pool will be apply to the output of the last convolutional block  and thus the output of the model will be a 2d tensor  max mean that global max pool will be apply  ) ( classes : optional number of class to classify image into  only to be specify if include top be true  and if no weight argument be specify  ) ( classifier activation : a str or callable  the activation function to use on the "top" layer  ignore unless include top true  set classifier activation none to return the logits of the "top" layer  when load pretrained weight  classifier activation can only be none or "softmax"  )
( preds : numpy array encode a batch of predictions  ) ( top : integer  how many top guess to return  default to 5  )
( x : a float point numpy array or a tf tensor  3d or 4d with 3 color channel  with value in the range  0  255   the preprocessed data be write over the input data if the data type be compatible  to avoid this behaviour  numpy copy x  can be use  ) ( data format : optional data format of the image tensor/array  default to none  in which case the global set tf keras backend image data format   be use  unless you change it  it default to "channels last"   )
( include top : whether to include the fully connect layer at the top of the network  ) ( weights : one of none  random initialization   'imagenet'  pre train on imagenet   or the path to the weight file to be load  ) ( input tensor : optional keras tensor  i e  output of layer input    to use as image input for the model  ) ( input shape : optional shape tuple  only to be specify if include top be false  otherwise the input shape have to be  224  224  3   with 'channels last' data format  or  3  224  224   with 'channels first' data format   it should have exactly 3 input channel  and width and height should be no smaller than 32  e g   200  200  3  would be one valid value  ) ( pooling : optional pool mode for feature extraction when include top be false   none mean that the output of the model will be the 4d tensor output of the last convolutional block  avg mean that global average pool will be apply to the output of the last convolutional block  and thus the output of the model will be a 2d tensor  max mean that global max pool will be apply  ) ( classes : optional number of class to classify image into  only to be specify if include top be true  and if no weight argument be specify  ) ( classifier activation : a str or callable  the activation function to use on the "top" layer  ignore unless include top true  set classifier activation none to return the logits of the "top" layer  when load pretrained weight  classifier activation can only be none or "softmax"  )
( include top : whether to include the fully connect layer at the top of the network  ) ( weights : one of none  random initialization   'imagenet'  pre train on imagenet   or the path to the weight file to be load  ) ( input tensor : optional keras tensor  i e  output of layer input    to use as image input for the model  ) ( input shape : optional shape tuple  only to be specify if include top be false  otherwise the input shape have to be  224  224  3   with 'channels last' data format  or  3  224  224   with 'channels first' data format   it should have exactly 3 input channel  and width and height should be no smaller than 32  e g   200  200  3  would be one valid value  ) ( pooling : optional pool mode for feature extraction when include top be false   none mean that the output of the model will be the 4d tensor output of the last convolutional block  avg mean that global average pool will be apply to the output of the last convolutional block  and thus the output of the model will be a 2d tensor  max mean that global max pool will be apply  ) ( classes : optional number of class to classify image into  only to be specify if include top be true  and if no weight argument be specify  ) ( classifier activation : a str or callable  the activation function to use on the "top" layer  ignore unless include top true  set classifier activation none to return the logits of the "top" layer  when load pretrained weight  classifier activation can only be none or "softmax"  )
( include top : whether to include the fully connect layer at the top of the network  ) ( weights : one of none  random initialization   'imagenet'  pre train on imagenet   or the path to the weight file to be load  ) ( input tensor : optional keras tensor  i e  output of layer input    to use as image input for the model  ) ( input shape : optional shape tuple  only to be specify if include top be false  otherwise the input shape have to be  224  224  3   with 'channels last' data format  or  3  224  224   with 'channels first' data format   it should have exactly 3 input channel  and width and height should be no smaller than 32  e g   200  200  3  would be one valid value  ) ( pooling : optional pool mode for feature extraction when include top be false   none mean that the output of the model will be the 4d tensor output of the last convolutional block  avg mean that global average pool will be apply to the output of the last convolutional block  and thus the output of the model will be a 2d tensor  max mean that global max pool will be apply  ) ( classes : optional number of class to classify image into  only to be specify if include top be true  and if no weight argument be specify  ) ( classifier activation : a str or callable  the activation function to use on the "top" layer  ignore unless include top true  set classifier activation none to return the logits of the "top" layer  when load pretrained weight  classifier activation can only be none or "softmax"  )
( preds : numpy array encode a batch of predictions  ) ( top : integer  how many top guess to return  default to 5  )
( x : a float point numpy array or a tf tensor  3d or 4d with 3 color channel  with value in the range  0  255   the preprocessed data be write over the input data if the data type be compatible  to avoid this behaviour  numpy copy x  can be use  ) ( data format : optional data format of the image tensor/array  default to none  in which case the global set tf keras backend image data format   be use  unless you change it  it default to "channels last"   )
( include top : whether to include the 3 fully connect layer at the top of the network  ) ( weights : one of none  random initialization   'imagenet'  pre train on imagenet   or the path to the weight file to be load  ) ( input tensor : optional keras tensor  i e  output of layer input    to use as image input for the model  ) ( input shape : optional shape tuple  only to be specify if include top be false  otherwise the input shape have to be  224  224  3   with channel last data format  or  3  224  224   with channel first data format   it should have exactly 3 input channel  and width and height should be no smaller than 32  e g   200  200  3  would be one valid value  ) ( pooling : optional pool mode for feature extraction when include top be false   none mean that the output of the model will be the 4d tensor output of the last convolutional block  avg mean that global average pool will be apply to the output of the last convolutional block  and thus the output of the model will be a 2d tensor  max mean that global max pool will be apply  ) ( classes : optional number of class to classify image into  only to be specify if include top be true  and if no weight argument be specify  ) ( classifier activation : a str or callable  the activation function to use on the "top" layer  ignore unless include top true  set classifier activation none to return the logits of the "top" layer  when load pretrained weight  classifier activation can only be none or "softmax"  )
( preds : numpy array encode a batch of predictions  ) ( top : integer  how many top guess to return  default to 5  )
( x : a float point numpy array or a tf tensor  3d or 4d with 3 color channel  with value in the range  0  255   the preprocessed data be write over the input data if the data type be compatible  to avoid this behaviour  numpy copy x  can be use  ) ( data format : optional data format of the image tensor/array  default to none  in which case the global set tf keras backend image data format   be use  unless you change it  it default to "channels last"   )
( include top : whether to include the 3 fully connect layer at the top of the network  ) ( weights : one of none  random initialization   'imagenet'  pre train on imagenet   or the path to the weight file to be load  ) ( input tensor : optional keras tensor  i e  output of layer input    to use as image input for the model  ) ( input shape : optional shape tuple  only to be specify if include top be false  otherwise the input shape have to be  224  224  3   with channel last data format  or  3  224  224   with channel first data format   it should have exactly 3 input channel  and width and height should be no smaller than 32  e g   200  200  3  would be one valid value  ) ( pooling : optional pool mode for feature extraction when include top be false   none mean that the output of the model will be the 4d tensor output of the last convolutional block  avg mean that global average pool will be apply to the output of the last convolutional block  and thus the output of the model will be a 2d tensor  max mean that global max pool will be apply  ) ( classes : optional number of class to classify image into  only to be specify if include top be true  and if no weight argument be specify  ) ( classifier activation : a str or callable  the activation function to use on the "top" layer  ignore unless include top true  set classifier activation none to return the logits of the "top" layer  when load pretrained weight  classifier activation can only be none or "softmax"  )
( preds : numpy array encode a batch of predictions  ) ( top : integer  how many top guess to return  default to 5  )
( x : a float point numpy array or a tf tensor  3d or 4d with 3 color channel  with value in the range  0  255   the preprocessed data be write over the input data if the data type be compatible  to avoid this behaviour  numpy copy x  can be use  ) ( data format : optional data format of the image tensor/array  default to none  in which case the global set tf keras backend image data format   be use  unless you change it  it default to "channels last"   )
( include top : whether to include the fully connect layer at the top of the network  ) ( weights : one of none  random initialization   'imagenet'  pre train on imagenet   or the path to the weight file to be load  ) ( input tensor : optional keras tensor  i e  output of layer input    to use as image input for the model  ) ( input shape : optional shape tuple  only to be specify if include top be false  otherwise the input shape have to be  299  299  3   it should have exactly 3 input channel  and width and height should be no smaller than 71  e g   150  150  3  would be one valid value  ) ( pooling : optional pool mode for feature extraction when include top be false   none mean that the output of the model will be the 4d tensor output of the last convolutional block  avg mean that global average pool will be apply to the output of the last convolutional block  and thus the output of the model will be a 2d tensor  max mean that global max pool will be apply  ) ( classes : optional number of class to classify image into  only to be specify if include top be true  and if no weight argument be specify  ) ( classifier activation : a str or callable  the activation function to use on the "top" layer  ignore unless include top true  set classifier activation none to return the logits of the "top" layer  when load pretrained weight  classifier activation can only be none or "softmax"  )
( preds : numpy array encode a batch of predictions  ) ( top : integer  how many top guess to return  default to 5  )
( x : a float point numpy array or a tf tensor  3d or 4d with 3 color channel  with value in the range  0  255   the preprocessed data be write over the input data if the data type be compatible  to avoid this behaviour  numpy copy x  can be use  ) ( data format : optional data format of the image tensor/array  default to none  in which case the global set tf keras backend image data format   be use  unless you change it  it default to "channels last"   )



( prefix : string prefix to index  )

( x : a candidate tensor  )

( step function : rnn step function  args      input  tensor with shape  sample        no time dimension           represent input for the batch of sample at a certain         time step      state  list of tensors  return      output  tensor with shape  sample  output dim           no time dimension       new state  list of tensors  same length and shape         as 'states'  the first state in the list must be the         output tensor at the previous timestep  ) ( inputs : tensor of temporal data of shape  sample  time        at least 3d   or nest tensors  and each of which have shape  sample  time        ) ( initial states : tensor with shape  sample  state size   no time dimension   contain the initial value for the state use in the step function  in the case that state size be in a nest shape  the shape of initial state will also follow the nest structure  ) ( go backwards : boolean  if true  do the iteration over the time dimension in reverse order and return the reverse sequence  ) ( mask : binary tensor with shape  sample  time  1   with a zero for every element that be mask  ) ( constants : list of constant value pass at each step  ) ( unroll : whether to unroll the rnn or to use a symbolic while loop  ) ( input length : an integer or a 1 d tensor  depend on whether the time dimension be fix length or not  in case of variable length input  it be use for mask in case there's no mask specify  ) ( time major : boolean  if true  the input and output will be in shape  timesteps  batch        whereas in the false case  it will be  batch  timesteps        use time major   true be a bite more efficient because it avoid transpose at the begin and end of the rnn calculation  however  most tensorflow data be batch major  so by default this function accept input and emit output in batch major form  ) ( zero output for mask : boolean  if true  the output for mask timestep will be zero  whereas in the false case  output from previous timestep be return  ) ( return all outputs : boolean  if true  return the recurrent output for all timesteps in the sequence  if false  only return the output for the last timestep  which consume less memory   )
( value : float  new value of epsilon  )
( value : string  'float16'  'float32'  or 'float64'  )
( data format : string  'channels first' or 'channels last'  )
( stateful metrics : iterable of string name of metrics that should not be average over an epoch  metrics in this list will be log as be in on epoch end  all others will be average in on epoch end  )
( filename : filename of the csv file  e g  'run/log csv'  ) ( separator : string use to separate elements in the csv file  ) ( append : boolean  true  append if file exist  useful for continue train   false  overwrite exist file  )
( params : dict  train parameters  eg  verbosity  batch size  number of epochs      ) ( model : instance of keras model model  reference of the model be train  )
( monitor : quantity to be monitor  ) ( min delta : minimum change in the monitor quantity to qualify as an improvement  i e  an absolute change of less than min delta  will count as no improvement  ) ( patience : number of epochs with no improvement after which train will be stop  ) ( verbose : verbosity mode  0 or 1  mode 0 be silent  and mode 1 display message when the callback take an action  ) ( mode : one of  "auto"  "min"  "max"   in min mode  train will stop when the quantity monitor have stop decrease  in "max" mode it will stop when the quantity monitor have stop increase  in "auto" mode  the direction be automatically infer from the name of the monitor quantity  ) ( baseline : baseline value for the monitor quantity  train will stop if the model doesn't show improvement over the baseline  ) ( restore best weights : whether to restore model weight from the epoch with the best value of the monitor quantity  if false  the model weight obtain at the last step of train be use  an epoch will be restore regardless of the performance relative to the baseline  if no epoch improve on baseline  train will run for patience epochs and restore weight from the best epoch in that set  )

( on epoch begin : call at the begin of every epoch  ) ( on epoch end : call at the end of every epoch  ) ( on batch begin : call at the begin of every batch  ) ( on batch end : call at the end of every batch  ) ( on train begin : call at the begin of model train  ) ( on train end : call at the end of model train  )
( schedule : a function that take an epoch index  integer  index from 0  and current learn rate  float  as input and return a new learn rate as output  float   ) ( verbose : int  0  quiet  1  update message  )
( filepath : string or pathlike  path to save the model file  e g  filepath   os path join work dir  'ckpt'  file name   filepath can contain name format options  which will be fill the value of epoch and key in log  pass in on epoch end   for example  if filepath be weight  epoch 02d   val loss  2f  hdf5  then the model checkpoints will be save with the epoch number and the validation loss in the filename  the directory of the filepath should not be reuse by any other callbacks to avoid conflict  ) ( monitor : the metric name to monitor  typically the metrics be set by the model compile method  note   prefix the name with "val " to monitor validation metrics  use "loss" or "val loss" to monitor the model's total loss  if you specify metrics as string  like "accuracy"  pass the same string  with or without the "val " prefix   if you pass metrics metric object  monitor should be set to metric name if you're not sure about the metric name you can check the content of the history history dictionary return by history   model fit   multi output model set additional prefix on the metric name  ) ( verbose : verbosity mode  0 or 1  mode 0 be silent  and mode 1 display message when the callback take an action  ) ( save best only : if save best only true  it only save when the model be consider the "best" and the latest best model accord to the quantity monitor will not be overwrite  if filepath doesn't contain format options like  epoch  then filepath will be overwrite by each new better model  ) ( mode : one of  'auto'  'min'  'max'   if save best only true  the decision to overwrite the current save file be make base on either the maximization or the minimization of the monitor quantity  for val acc  this should be max  for val loss this should be min  etc  in auto mode  the mode be set to max if the quantities monitor be 'acc' or start with 'fmeasure' and be set to min for the rest of the quantities  ) ( save weights only : if true  then only the model's weight will be save  model save weight filepath    else the full model be save  model save filepath    ) ( save freq : 'epoch' or integer  when use 'epoch'  the callback save the model after each epoch  when use integer  the callback save the model at end of this many batch  if the model be compile with step per execution n  then the save criteria will be check every nth batch  note that if the save isn't align to epochs  the monitor metric may potentially be less reliable  it could reflect as little as 1 batch  since the metrics get reset every epoch   default to 'epoch'  ) ( options : optional tf train checkpointoptions object if save weight only be true or optional tf save model saveoptions object if save weight only be false  ) ( initial value threshold : float point initial "best" value of the metric to be monitor  only apply if save best value true  only overwrite the model weight already save if the performance of current model be better than this value  ) (   kwargs : additional arguments for backwards compatibility  possible key be period  )
( count mode : one of "steps" or "samples"  whether the progress bar should count sample see or step  batch  see  ) ( stateful metrics : iterable of string name of metrics that should not be average over an epoch  metrics in this list will be log as be  all others will be average over time  e g  loss  etc   if not provide  default to the model's metrics  )
( monitor : quantity to be monitor  ) ( factor : factor by which the learn rate will be reduce  new lr   lr   factor  ) ( patience : number of epochs with no improvement after which learn rate will be reduce  ) ( verbose : int  0  quiet  1  update message  ) ( mode : one of  'auto'  'min'  'max'   in 'min' mode  the learn rate will be reduce when the quantity monitor have stop decrease  in 'max' mode it will be reduce when the quantity monitor have stop increase  in 'auto' mode  the direction be automatically infer from the name of the monitor quantity  ) ( min delta : threshold for measure the new optimum  to only focus on significant change  ) ( cooldown : number of epochs to wait before resume normal operation after lr have be reduce  ) ( min lr : lower bind on the learn rate  )
( root : string  root url of the target server  ) ( path : string  path relative to root to which the events will be send  ) ( field : string  json field under which the data will be store  the field be use only if the payload be send within a form  i e  send as json be set to false   ) ( headers : dictionary  optional custom http headers  ) ( send as json : boolean  whether the request should be send as "application/json"  )
( log dir : the path of the directory where to save the log file to be parse by tensorboard  e g  log dir   os path join work dir  'logs'  this directory should not be reuse by any other callbacks  ) ( histogram freq : frequency  in epochs  at which to compute weight histograms for the layer of the model  if set to 0  histograms won't be compute  validation data  or split  must be specify for histogram visualizations  ) ( write graph : whether to visualize the graph in tensorboard  the log file can become quite large when write graph be set to true  ) ( write images : whether to write model weight to visualize as image in tensorboard  ) ( write steps per second : whether to log the train step per second into tensorboard  this support both epoch and batch frequency log  ) ( update freq : 'batch' or 'epoch' or integer  when use 'batch'  write the losses and metrics to tensorboard after each batch  the same apply for 'epoch'  if use an integer  let's say 1000  the callback will write the metrics and losses to tensorboard every 1000 batch  note that write too frequently to tensorboard can slow down your train  ) ( profile batch : profile the batch es  to sample compute characteristics  profile batch must be a non negative integer or a tuple of integers  a pair of positive integers signify a range of batch to profile  by default  profile be disable  ) ( embeddings freq : frequency  in epochs  at which embed layer will be visualize  if set to 0  embeddings won't be visualize  ) ( embeddings metadata : dictionary which map embed layer name to the filename of a file in which to save metadata for the embed layer  in case the same metadata file be to be use for all embed layer  a single filename can be pass  )


( max value : the maximum norm value for the incoming weight  ) ( axis : integer  axis along which to calculate weight norms  for instance  in a dense layer the weight matrix have shape  input dim  output dim   set axis to 0 to constrain each weight vector of length  input dim    in a conv2d layer with data format "channels last"  the weight tensor have shape  row  cols  input depth  output depth   set axis to  0  1  2  to constrain the weight of each filter tensor of size  row  cols  input depth   )
( min value : the minimum norm for the incoming weight  ) ( max value : the maximum norm for the incoming weight  ) ( rate : rate for enforce the constraint  weight will be rescale to yield  1   rate    norm   rate   norm clip min value  max value   effectively  this mean that rate 1 0 stand for strict enforcement of the constraint  while rate<1 0 mean that weight will be rescale at each step to slowly move towards a value inside the desire interval  ) ( axis : integer  axis along which to calculate weight norms  for instance  in a dense layer the weight matrix have shape  input dim  output dim   set axis to 0 to constrain each weight vector of length  input dim    in a conv2d layer with data format "channels last"  the weight tensor have shape  row  cols  input depth  output depth   set axis to  0  1  2  to constrain the weight of each filter tensor of size  row  cols  input depth   )


( axis : integer  axis along which to calculate weight norms  for instance  in a dense layer the weight matrix have shape  input dim  output dim   set axis to 0 to constrain each weight vector of length  input dim    in a conv2d layer with data format "channels last"  the weight tensor have shape  row  cols  input depth  output depth   set axis to  0  1  2  to constrain the weight of each filter tensor of size  row  cols  input depth   )

( path : path where to cache the dataset locally  relative to ~/ keras/datasets   ) ( test split : fraction of the data to reserve as test set  ) ( seed : random seed for shuffle the data before compute the test split  )

( label mode : one of "fine"  "coarse"  if it be "fine" the category label be the fine grain label  if it be "coarse" the output label be the coarse grain superclasses  )

( path : where to cache the data  relative to ~/ keras/dataset   )
( path : where to cache the data  relative to ~/ keras/dataset   ) ( num words : integer or none  word be rank by how often they occur  in the train set  and only the num word most frequent word be keep  any less frequent word will appear as oov char value in the sequence data  if none  all word be keep  default to none  so all word be keep  ) ( skip top : skip the top n most frequently occur word  which may not be informative   these word will appear as oov char value in the dataset  default to 0  so no word be skip  ) ( maxlen : int or none  maximum sequence length  any longer sequence will be truncate  default to none  which mean no truncation  ) ( seed : int  seed for reproducible data shuffle  ) ( start char : int  the start of a sequence will be mark with this character  default to 1 because 0 be usually the pad character  ) ( oov char : int  the out of vocabulary character  word that be cut out because of the num word or skip top limit will be replace with this character  ) ( index from : int  index actual word with this index and higher  ) (   kwargs : use for backwards compatibility  )
( path : path where to cache the dataset locally  relative to ~/ keras/datasets   )
( path : where to cache the data  relative to ~/ keras/dataset   )
( path : where to cache the data  relative to ~/ keras/dataset   ) ( num words : integer or none  word be rank by how often they occur  in the train set  and only the num word most frequent word be keep  any less frequent word will appear as oov char value in the sequence data  if none  all word be keep  default to none  so all word be keep  ) ( skip top : skip the top n most frequently occur word  which may not be informative   these word will appear as oov char value in the dataset  default to 0  so no word be skip  ) ( maxlen : int or none  maximum sequence length  any longer sequence will be truncate  default to none  which mean no truncation  ) ( test split : float between 0 and 1  fraction of the dataset to be use as test data  default to 0 2  mean 20  of the dataset be use as test data  ) ( seed : int  seed for reproducible data shuffle  ) ( start char : int  the start of a sequence will be mark with this character  default to 1 because 0 be usually the pad character  ) ( oov char : int  the out of vocabulary character  word that be cut out because of the num word or skip top limit will be replace with this character  ) ( index from : int  index actual word with this index and higher  ) (   kwargs : use for backwards compatibility  )
( keras model : a compile keras model object  this argument be mutually exclusive with keras model path  estimator's model fn use the structure of the model to clone the model  default to none  ) ( keras model path : path to a compile keras model save on disk  in hdf5 format  which can be generate with the save   method of a keras model  this argument be mutually exclusive with keras model  default to none  ) ( custom objects : dictionary for clone customize object  this be use with class that be not part of this pip package  for example  if user maintain a relu6 class that inherit from tf keras layer layer  then pass custom object  'relu6'  relu6   default to none  ) ( model dir : directory to save estimator model parameters  graph  summary file for tensorboard  etc  if unset a directory will be create with tempfile mkdtemp ) ( config : runconfig to config estimator  allow set up things in model fn base on configuration such as num ps replicas  or model dir  default to none  if both config model dir and the model dir argument  above  be specify the model dir argument take precedence  ) ( checkpoint format : set the format of the checkpoint save by the estimator when train  may be saver or checkpoint  depend on whether to save checkpoints from tf compat v1 train saver or tf train checkpoint  the default be checkpoint  estimators use name base tf train saver checkpoints  while keras model use object base checkpoints from tf train checkpoint  currently  save object base checkpoints from model to estimator be only support by functional and sequential model  default to 'checkpoint'  ) ( metric names map : optional dictionary map keras model output metric name to custom name  this can be use to override the default keras model output metrics name in a multi io model use case and provide custom name for the eval metric ops in estimator  the keras model metric name can be obtain use model metrics name exclude any loss metrics such as total loss and output losses  for example  if your keras model have two output out 1 and out 2  with mse loss and acc metric  then model metrics name will be  'loss'  'out 1 loss'  'out 2 loss'  'out 1 acc'  'out 2 acc'   the model metric name exclude the loss metrics will be  'out 1 acc'  'out 2 acc'   ) ( export outputs : optional dictionary  this can be use to override the default keras model output export in a multi io model use case and provide custom name for the export output in tf estimator estimatorspec  default be none  which be equivalent to  'serving default'  tf estimator export predictoutput   if not none  the key must match the key of model output name  a dict  name  output  where   name  an arbitrary name for this output  output  an exportoutput class such as classificationoutput  regressionoutput  or predictoutput  single head model only need to specify one entry in this dictionary  multi head model should specify one entry for each head  one of which must be name use tf save model signature constants default serve signature def key if no entry be provide  a default predictoutput map to predictions will be create  )
( value : a python scalar  )
( seed : a python integer  use to make the behavior of the initializer deterministic  note that a seed initializer will not produce the same random value across multiple call  but multiple initializers will produce the same sequence when construct with the same seed value  )
( seed : a python integer  use to make the behavior of the initializer deterministic  note that a seed initializer will not produce the same random value across multiple call  but multiple initializers will produce the same sequence when construct with the same seed value  )
( gain : multiplicative factor to apply to the identity matrix  )


( gain : multiplicative factor to apply to the orthogonal matrix ) ( seed : a python integer  use to make the behavior of the initializer deterministic  note that a seed initializer will not produce the same random value across multiple call  but multiple initializers will produce the same sequence when construct with the same seed value  )
( mean : a python scalar or a scalar tensor  mean of the random value to generate  ) ( stddev : a python scalar or a scalar tensor  standard deviation of the random value to generate  ) ( seed : a python integer  use to make the behavior of the initializer deterministic  note that a seed initializer will not produce the same random value across multiple call  but multiple initializers will produce the same sequence when construct with the same seed value  )
( minval : a python scalar or a scalar tensor  lower bind of the range of random value to generate  inclusive   ) ( maxval : a python scalar or a scalar tensor  upper bind of the range of random value to generate  exclusive   ) ( seed : a python integer  use to make the behavior of the initializer deterministic  note that a seed initializer will not produce the same random value across multiple call  but multiple initializers will produce the same sequence when construct with the same seed value  )
( mean : a python scalar or a scalar tensor  mean of the random value to generate  ) ( stddev : a python scalar or a scalar tensor  standard deviation of the random value to generate before truncation  ) ( seed : a python integer  use to make the behavior of the initializer deterministic  note that a seed initializer will not produce the same random value across multiple call  but multiple initializers will produce the same sequence when construct with the same seed value  )
( scale : scale factor  positive float   ) ( mode : one of "fan in"  "fan out"  "fan avg"  ) ( distribution : random distribution to use  one of "truncated normal"  "untruncated normal" and  "uniform"  ) ( seed : a python integer  use to make the behavior of the initializer deterministic  note that a seed initializer will not produce the same random value across multiple call  but multiple initializers will produce the same sequence when construct with the same seed value  )


( identifier : string or dict that contain the initializer name or configurations  )
( output size : integer or tensorshape  size of output produce by this cell  ) ( state size : size s  of state s  use by this cell  it can be represent by an integer  a tensorshape or a tuple of integers or tensorshapes  )
( activation : activation function  such as tf nn relu  or string name of build in activation function  such as "relu"  )
( l1 : l1 regularization factor  positive float   ) ( l2 : l2 regularization factor  positive float   )
(   kwargs : standard layer keyword arguments  )
( use scale : if true  will create a variable to scale the attention score  ) ( causal : boolean  set to true for decoder self attention  add a mask such that position i cannot attend to position j > i  this prevent the flow of information from the future towards the past  default to false  ) ( dropout : float between 0 and 1  fraction of the units to drop for the attention score  default to 0 0  )
( rate : float  drop probability  as with dropout   the multiplicative noise will have standard deviation sqrt rate /  1   rate    ) ( seed : integer  optional random seed to enable deterministic behavior  )
( use scale : if true  will create a scalar variable to scale the attention score  ) ( causal : boolean  set to true for decoder self attention  add a mask such that position i cannot attend to position j > i  this prevent the flow of information from the future towards the past  default to false  ) ( dropout : float between 0 and 1  fraction of the units to drop for the attention score  default to 0 0  ) ( score mode : function to use to compute attention score  one of  "dot"  "concat"   "dot" refer to the dot product between the query and key vectors  "concat" refer to the hyperbolic tangent of the concatenation of the query and key vectors  )
(   kwargs : standard layer keyword arguments  )
( pool size : integer  size of the average pool windows  ) ( strides : integer  or none  factor by which to downscale  e g  2 will halve the input  if none  it will default to pool size  ) ( padding : one of "valid" or "same"  case insensitive   "valid" mean no pad  "same" result in pad evenly to the left/right or up/down of the input such that output have the same height/width dimension as the input  ) ( data format : a string  one of channel last  default  or channel first  the order of the dimension in the input  channel last correspond to input with shape  batch  step  feature  while channel first correspond to input with shape  batch  feature  step   )
( pool size : integer or tuple of 2 integers  factor by which to downscale  vertical  horizontal    2  2  will halve the input in both spatial dimension  if only one integer be specify  the same window length will be use for both dimension  ) ( strides : integer  tuple of 2 integers  or none  stride value  if none  it will default to pool size  ) ( padding : one of "valid" or "same"  case insensitive   "valid" mean no pad  "same" result in pad evenly to the left/right or up/down of the input such that output have the same height/width dimension as the input  ) ( data format : a string  one of channel last  default  or channel first  the order of the dimension in the input  channel last correspond to input with shape  batch  height  width  channel  while channel first correspond to input with shape  batch  channel  height  width   it default to the image data format value find in your keras config file at ~/ keras/keras json  if you never set it  then it will be "channels last"  )
( pool size : tuple of 3 integers  factor by which to downscale  dim1  dim2  dim3    2  2  2  will halve the size of the 3d input in each dimension  ) ( strides : tuple of 3 integers  or none  stride value  ) ( padding : one of "valid" or "same"  case insensitive   "valid" mean no pad  "same" result in pad evenly to the left/right or up/down of the input such that output have the same height/width dimension as the input  ) ( data format : a string  one of channel last  default  or channel first  the order of the dimension in the input  channel last correspond to input with shape  batch  spatial dim1  spatial dim2  spatial dim3  channel  while channel first correspond to input with shape  batch  channel  spatial dim1  spatial dim2  spatial dim3   it default to the image data format value find in your keras config file at ~/ keras/keras json  if you never set it  then it will be "channels last"  )
( axis : integer  the axis that should be normalize  typically the feature axis   for instance  after a conv2d layer with data format "channels first"  set axis 1 in batchnormalization  ) ( momentum : momentum for the move average  ) ( epsilon : small float add to variance to avoid divide by zero  ) ( center : if true  add offset of beta to normalize tensor  if false  beta be ignore  ) ( scale : if true  multiply by gamma  if false  gamma be not use  when the next layer be linear  also e g  nn relu   this can be disable since the scale will be do by the next layer  ) ( beta initializer : initializer for the beta weight  ) ( gamma initializer : initializer for the gamma weight  ) ( moving mean initializer : initializer for the move mean  ) ( moving variance initializer : initializer for the move variance  ) ( beta regularizer : optional regularizer for the beta weight  ) ( gamma regularizer : optional regularizer for the gamma weight  ) ( beta constraint : optional constraint for the beta weight  ) ( gamma constraint : optional constraint for the gamma weight  )
( layer : keras layer rnn instance  such as keras layer lstm or keras layer gru  it could also be a keras layer layer instance that meet the follow criteria   be a sequence process layer  accept 3d  input   have a go backwards  return sequence and return state attribute  with the same semantics as for the rnn class   have an input spec attribute  implement serialization via get config   and from config    note that the recommend way to create new rnn layer be to write a custom rnn cell and use it with keras layer rnn  instead of subclassing keras layer layer directly  when the return sequence be true  the output of the mask timestep will be zero regardless of the layer's original zero output for mask value  ) ( merge mode : mode by which output of the forward and backward rnns will be combine  one of  'sum'  'mul'  'concat'  'ave'  none   if none  the output will not be combine  they will be return as a list  default value be 'concat'  ) ( backward layer : optional keras layer rnn  or keras layer layer instance to be use to handle backwards input process  if backward layer be not provide  the layer instance pass as the layer argument will be use to generate the backward layer automatically  note that the provide backward layer layer should have properties match those of the layer argument  in particular it should have the same value for stateful  return state  return sequence  etc  in addition  backward layer and layer should have different go backwards argument value  a valueerror will be raise if these requirements be not meet  )
( axis : axis along which to concatenate  ) (   kwargs : standard layer keyword arguments  )
( filters : integer  the dimensionality of the output space  i e  the number of output filter in the convolution   ) ( kernel size : an integer or tuple/list of a single integer  specify the length of the 1d convolution window  ) ( strides : an integer or tuple/list of a single integer  specify the stride length of the convolution  specify any stride value    1 be incompatible with specify any dilation rate value    1  ) ( padding : one of "valid"  "same" or "causal"  case insensitive   "valid" mean no pad  "same" result in pad with zero evenly to the left/right or up/down of the input such that output have the same height/width dimension as the input  "causal" result in causal  dilate  convolutions  e g  output t  do not depend on input t 1    useful when model temporal data where the model should not violate the temporal order  see wavenet  a generative model for raw audio  section   2 1  ) ( data format : a string  one of channel last  default  or channel first  ) ( dilation rate : an integer or tuple/list of a single integer  specify the dilation rate to use for dilate convolution  currently  specify any dilation rate value    1 be incompatible with specify any stride value    1  ) ( groups : a positive integer specify the number of group in which the input be split along the channel axis  each group be convolve separately with filter / group filter  the output be the concatenation of all the group result along the channel axis  input channel and filter must both be divisible by group  ) ( activation : activation function to use  if you don't specify anything  no activation be apply  see keras activations   ) ( use bias : boolean  whether the layer use a bias vector  ) ( kernel initializer : initializer for the kernel weight matrix  see keras initializers   default to 'glorot uniform'  ) ( bias initializer : initializer for the bias vector  see keras initializers   default to 'zeros'  ) ( kernel regularizer : regularizer function apply to the kernel weight matrix  see keras regularizers   ) ( bias regularizer : regularizer function apply to the bias vector  see keras regularizers   ) ( activity regularizer : regularizer function apply to the output of the layer  its "activation"   see keras regularizers   ) ( kernel constraint : constraint function apply to the kernel matrix  see keras constraints   ) ( bias constraint : constraint function apply to the bias vector  see keras constraints   )
( filters : integer  the dimensionality of the output space  i e  the number of output filter in the convolution   ) ( kernel size : an integer or tuple/list of 2 integers  specify the height and width of the 2d convolution window  can be a single integer to specify the same value for all spatial dimension  ) ( strides : an integer or tuple/list of 2 integers  specify the stride of the convolution along the height and width  can be a single integer to specify the same value for all spatial dimension  specify any stride value    1 be incompatible with specify any dilation rate value    1  ) ( padding : one of "valid" or "same"  case insensitive   "valid" mean no pad  "same" result in pad with zero evenly to the left/right or up/down of the input  when pad "same" and stride 1  the output have the same size as the input  ) ( data format : a string  one of channel last  default  or channel first  the order of the dimension in the input  channel last correspond to input with shape  batch size  height  width  channel  while channel first correspond to input with shape  batch size  channel  height  width   it default to the image data format value find in your keras config file at ~/ keras/keras json  if you never set it  then it will be channel last  ) ( dilation rate : an integer or tuple/list of 2 integers  specify the dilation rate to use for dilate convolution  can be a single integer to specify the same value for all spatial dimension  currently  specify any dilation rate value    1 be incompatible with specify any stride value    1  ) ( groups : a positive integer specify the number of group in which the input be split along the channel axis  each group be convolve separately with filter / group filter  the output be the concatenation of all the group result along the channel axis  input channel and filter must both be divisible by group  ) ( activation : activation function to use  if you don't specify anything  no activation be apply  see keras activations   ) ( use bias : boolean  whether the layer use a bias vector  ) ( kernel initializer : initializer for the kernel weight matrix  see keras initializers   default to 'glorot uniform'  ) ( bias initializer : initializer for the bias vector  see keras initializers   default to 'zeros'  ) ( kernel regularizer : regularizer function apply to the kernel weight matrix  see keras regularizers   ) ( bias regularizer : regularizer function apply to the bias vector  see keras regularizers   ) ( activity regularizer : regularizer function apply to the output of the layer  its "activation"   see keras regularizers   ) ( kernel constraint : constraint function apply to the kernel matrix  see keras constraints   ) ( bias constraint : constraint function apply to the bias vector  see keras constraints   )
( filters : integer  the dimensionality of the output space  i e  the number of output filter in the convolution   ) ( kernel size : an integer or tuple/list of 2 integers  specify the height and width of the 2d convolution window  can be a single integer to specify the same value for all spatial dimension  ) ( strides : an integer or tuple/list of 2 integers  specify the stride of the convolution along the height and width  can be a single integer to specify the same value for all spatial dimension  specify any stride value    1 be incompatible with specify any dilation rate value    1  ) ( padding : one of "valid" or "same"  case insensitive   "valid" mean no pad  "same" result in pad with zero evenly to the left/right or up/down of the input such that output have the same height/width dimension as the input  ) ( output padding : an integer or tuple/list of 2 integers  specify the amount of pad along the height and width of the output tensor  can be a single integer to specify the same value for all spatial dimension  the amount of output pad along a give dimension must be lower than the stride along that same dimension  if set to none  default   the output shape be infer  ) ( data format : a string  one of channel last  default  or channel first  the order of the dimension in the input  channel last correspond to input with shape  batch size  height  width  channel  while channel first correspond to input with shape  batch size  channel  height  width   it default to the image data format value find in your keras config file at ~/ keras/keras json  if you never set it  then it will be "channels last"  ) ( dilation rate : an integer  specify the dilation rate for all spatial dimension for dilate convolution  specify different dilation rat for different dimension be not support  currently  specify any dilation rate value    1 be incompatible with specify any stride value    1  ) ( activation : activation function to use  if you don't specify anything  no activation be apply  see keras activations   ) ( use bias : boolean  whether the layer use a bias vector  ) ( kernel initializer : initializer for the kernel weight matrix  see keras initializers   default to 'glorot uniform'  ) ( bias initializer : initializer for the bias vector  see keras initializers   default to 'zeros'  ) ( kernel regularizer : regularizer function apply to the kernel weight matrix  see keras regularizers   ) ( bias regularizer : regularizer function apply to the bias vector  see keras regularizers   ) ( activity regularizer : regularizer function apply to the output of the layer  its "activation"   see keras regularizers   ) ( kernel constraint : constraint function apply to the kernel matrix  see keras constraints   ) ( bias constraint : constraint function apply to the bias vector  see keras constraints   )
( filters : integer  the dimensionality of the output space  i e  the number of output filter in the convolution   ) ( kernel size : an integer or tuple/list of 3 integers  specify the depth  height and width of the 3d convolution window  can be a single integer to specify the same value for all spatial dimension  ) ( strides : an integer or tuple/list of 3 integers  specify the stride of the convolution along each spatial dimension  can be a single integer to specify the same value for all spatial dimension  specify any stride value    1 be incompatible with specify any dilation rate value    1  ) ( padding : one of "valid" or "same"  case insensitive   "valid" mean no pad  "same" result in pad with zero evenly to the left/right or up/down of the input such that output have the same height/width dimension as the input  ) ( data format : a string  one of channel last  default  or channel first  the order of the dimension in the input  channel last correspond to input with shape batch shape    spatial dim1  spatial dim2  spatial dim3  channel  while channel first correspond to input with shape batch shape    channel  spatial dim1  spatial dim2  spatial dim3   it default to the image data format value find in your keras config file at ~/ keras/keras json  if you never set it  then it will be "channels last"  ) ( dilation rate : an integer or tuple/list of 3 integers  specify the dilation rate to use for dilate convolution  can be a single integer to specify the same value for all spatial dimension  currently  specify any dilation rate value    1 be incompatible with specify any stride value    1  ) ( groups : a positive integer specify the number of group in which the input be split along the channel axis  each group be convolve separately with filter / group filter  the output be the concatenation of all the group result along the channel axis  input channel and filter must both be divisible by group  ) ( activation : activation function to use  if you don't specify anything  no activation be apply  see keras activations   ) ( use bias : boolean  whether the layer use a bias vector  ) ( kernel initializer : initializer for the kernel weight matrix  see keras initializers   default to 'glorot uniform'  ) ( bias initializer : initializer for the bias vector  see keras initializers   default to 'zeros'  ) ( kernel regularizer : regularizer function apply to the kernel weight matrix  see keras regularizers   ) ( bias regularizer : regularizer function apply to the bias vector  see keras regularizers   ) ( activity regularizer : regularizer function apply to the output of the layer  its "activation"   see keras regularizers   ) ( kernel constraint : constraint function apply to the kernel matrix  see keras constraints   ) ( bias constraint : constraint function apply to the bias vector  see keras constraints   )
( filters : integer  the dimensionality of the output space  i e  the number of output filter in the convolution   ) ( kernel size : an integer or tuple/list of 3 integers  specify the depth  height and width of the 3d convolution window  can be a single integer to specify the same value for all spatial dimension  ) ( strides : an integer or tuple/list of 3 integers  specify the stride of the convolution along the depth  height   and width  can be a single integer to specify the same value for all spatial dimension  specify any stride value    1 be incompatible with specify any dilation rate value    1  ) ( padding : one of "valid" or "same"  case insensitive   "valid" mean no pad  "same" result in pad with zero evenly to the left/right or up/down of the input such that output have the same height/width dimension as the input  ) ( output padding : an integer or tuple/list of 3 integers  specify the amount of pad along the depth  height  and width  can be a single integer to specify the same value for all spatial dimension  the amount of output pad along a give dimension must be lower than the stride along that same dimension  if set to none  default   the output shape be infer  ) ( data format : a string  one of channel last  default  or channel first  the order of the dimension in the input  channel last correspond to input with shape  batch size  depth  height  width  channel  while channel first correspond to input with shape  batch size  channel  depth  height  width   it default to the image data format value find in your keras config file at ~/ keras/keras json  if you never set it  then it will be "channels last"  ) ( dilation rate : an integer or tuple/list of 3 integers  specify the dilation rate to use for dilate convolution  can be a single integer to specify the same value for all spatial dimension  currently  specify any dilation rate value    1 be incompatible with specify any stride value    1  ) ( activation : activation function to use  if you don't specify anything  no activation be apply  see keras activations   ) ( use bias : boolean  whether the layer use a bias vector  ) ( kernel initializer : initializer for the kernel weight matrix  see keras initializers   default to 'glorot uniform'  ) ( bias initializer : initializer for the bias vector  see keras initializers   default to 'zeros'  ) ( kernel regularizer : regularizer function apply to the kernel weight matrix  see keras regularizers   ) ( bias regularizer : regularizer function apply to the bias vector  see keras regularizers   ) ( activity regularizer : regularizer function apply to the output of the layer  its "activation"   see keras regularizers   ) ( kernel constraint : constraint function apply to the kernel matrix  see keras constraints   ) ( bias constraint : constraint function apply to the bias vector  see keras constraints   )
( filters : integer  the dimensionality of the output space  i e  the number of output filter in the convolution   ) ( kernel size : an integer or tuple/list of n integers  specify the dimension of the convolution window  ) ( strides : an integer or tuple/list of n integers  specify the stride of the convolution  specify any stride value    1 be incompatible with specify any dilation rate value    1  ) ( padding : one of "valid" or "same"  case insensitive   "valid" mean no pad  "same" result in pad evenly to the left/right or up/down of the input such that output have the same height/width dimension as the input  ) ( data format : a string  one of channel last  default  or channel first  the order of the dimension in the input  channel last correspond to input with shape  batch  time       channel  while channel first correspond to input with shape  batch  time  channel        it default to the image data format value find in your keras config file at ~/ keras/keras json  if you never set it  then it will be "channels last"  ) ( dilation rate : an integer or tuple/list of n integers  specify the dilation rate to use for dilate convolution  currently  specify any dilation rate value    1 be incompatible with specify any stride value    1  ) ( activation : activation function to use  by default hyperbolic tangent activation function be apply  tanh x    ) ( recurrent activation : activation function to use for the recurrent step  ) ( use bias : boolean  whether the layer use a bias vector  ) ( kernel initializer : initializer for the kernel weight matrix  use for the linear transformation of the input  ) ( recurrent initializer : initializer for the recurrent kernel weight matrix  use for the linear transformation of the recurrent state  ) ( bias initializer : initializer for the bias vector  ) ( unit forget bias : boolean  if true  add 1 to the bias of the forget gate at initialization  use in combination with bias initializer "zeros"  this be recommend in jozefowicz et al   2015 ) ( kernel regularizer : regularizer function apply to the kernel weight matrix  ) ( recurrent regularizer : regularizer function apply to the recurrent kernel weight matrix  ) ( bias regularizer : regularizer function apply to the bias vector  ) ( activity regularizer : regularizer function apply to  ) ( kernel constraint : constraint function apply to the kernel weight matrix  ) ( recurrent constraint : constraint function apply to the recurrent kernel weight matrix  ) ( bias constraint : constraint function apply to the bias vector  ) ( return sequences : boolean  whether to return the last output in the output sequence  or the full sequence   default false  ) ( return state : boolean whether to return the last state in addition to the output   default false  ) ( go backwards : boolean  default false   if true  process the input sequence backwards  ) ( stateful : boolean  default false   if true  the last state for each sample at index i in a batch will be use as initial state for the sample of index i in the follow batch  ) ( dropout : float between 0 and 1  fraction of the units to drop for the linear transformation of the input  ) ( recurrent dropout : float between 0 and 1  fraction of the units to drop for the linear transformation of the recurrent state  )
( cropping : int or tuple of int  length 2  how many units should be trim off at the begin and end of the crop dimension  axis 1   if a single int be provide  the same value will be use for both  )
( cropping : int  or tuple of 2 ints  or tuple of 2 tuples of 2 ints   if int  the same symmetric crop be apply to height and width  if tuple of 2 ints  interpret as two different symmetric crop value for height and width   symmetric height crop  symmetric width crop   if tuple of 2 tuples of 2 ints  interpret as   top crop  bottom crop    leave crop  right crop   ) ( data format : a string  one of channel last  default  or channel first  the order of the dimension in the input  channel last correspond to input with shape  batch size  height  width  channel  while channel first correspond to input with shape  batch size  channel  height  width   it default to the image data format value find in your keras config file at ~/ keras/keras json  if you never set it  then it will be "channels last"  )
( cropping : int  or tuple of 3 ints  or tuple of 3 tuples of 2 ints   if int  the same symmetric crop be apply to depth  height  and width  if tuple of 3 ints  interpret as two different symmetric crop value for depth  height  and width   symmetric dim1 crop  symmetric dim2 crop  symmetric dim3 crop   if tuple of 3 tuples of 2 ints  interpret as   leave dim1 crop  right dim1 crop    leave dim2 crop  right dim2 crop    leave dim3 crop  right dim3 crop   ) ( data format : a string  one of channel last  default  or channel first  the order of the dimension in the input  channel last correspond to input with shape  batch size  spatial dim1  spatial dim2  spatial dim3  channel  while channel first correspond to input with shape  batch size  channel  spatial dim1  spatial dim2  spatial dim3   it default to the image data format value find in your keras config file at ~/ keras/keras json  if you never set it  then it will be "channels last"  )
( units : positive integer  dimensionality of the output space  ) ( activation : activation function to use  if you don't specify anything  no activation be apply  ie  "linear" activation  a x    x   ) ( use bias : boolean  whether the layer use a bias vector  ) ( kernel initializer : initializer for the kernel weight matrix  ) ( bias initializer : initializer for the bias vector  ) ( kernel regularizer : regularizer function apply to the kernel weight matrix  ) ( bias regularizer : regularizer function apply to the bias vector  ) ( activity regularizer : regularizer function apply to the output of the layer  its "activation"   ) ( kernel constraint : constraint function apply to the kernel weight matrix  ) ( bias constraint : constraint function apply to the bias vector  )
( feature columns : an iterable contain the featurecolumns to use as input to your model  all items should be instance of class derive from densecolumn such as numeric column  embed column  bucketized column  indicator column  if you have categorical feature  you can wrap them with an embed column or indicator column  ) ( trainable : boolean  whether the layer's variables will be update via gradient descent during train  ) ( name : name to give to the densefeatures  ) (   kwargs : keyword arguments to construct a layer  )
( kernel size : an integer or tuple/list of 2 integers  specify the height and width of the 2d convolution window  can be a single integer to specify the same value for all spatial dimension  ) ( strides : an integer or tuple/list of 2 integers  specify the stride of the convolution along the height and width  can be a single integer to specify the same value for all spatial dimension  specify any stride value    1 be incompatible with specify any dilation rate value    1  ) ( padding : one of 'valid' or 'same'  case insensitive   "valid" mean no pad  "same" result in pad with zero evenly to the left/right or up/down of the input such that output have the same height/width dimension as the input  ) ( depth multiplier : the number of depthwise convolution output channel for each input channel  the total number of depthwise convolution output channel will be equal to filter in   depth multiplier  ) ( data format : a string  one of channel last  default  or channel first  the order of the dimension in the input  channel last correspond to input with shape  batch size  height  width  channel  while channel first correspond to input with shape  batch size  channel  height  width   it default to the image data format value find in your keras config file at ~/ keras/keras json  if you never set it  then it will be 'channels last'  ) ( dilation rate : an integer or tuple/list of 2 integers  specify the dilation rate to use for dilate convolution  currently  specify any dilation rate value    1 be incompatible with specify any stride value    1  ) ( activation : activation function to use  if you don't specify anything  no activation be apply  see keras activations   ) ( use bias : boolean  whether the layer use a bias vector  ) ( depthwise initializer : initializer for the depthwise kernel matrix  see keras initializers   if none  the default initializer  'glorot uniform'  will be use  ) ( bias initializer : initializer for the bias vector  see keras initializers   if none  the default initializer  'zeros'  will be use  ) ( depthwise regularizer : regularizer function apply to the depthwise kernel matrix  see keras regularizers   ) ( bias regularizer : regularizer function apply to the bias vector  see keras regularizers   ) ( activity regularizer : regularizer function apply to the output of the layer  its 'activation'   see keras regularizers   ) ( depthwise constraint : constraint function apply to the depthwise kernel matrix  see keras constraints   ) ( bias constraint : constraint function apply to the bias vector  see keras constraints   )
( axes : integer or tuple of integers  axis or ax along which to take the dot product  if a tuple  should be two integers correspond to the desire axis from the first input and the desire axis from the second input  respectively  note that the size of the two select ax must match  ) ( normalize : whether to l2 normalize sample along the dot product axis before take the dot product  if set to true  then the output of the dot product be the cosine proximity between the two sample  ) (   kwargs : standard layer keyword arguments  )
( rate : float between 0 and 1  fraction of the input units to drop  ) ( noise shape : 1d integer tensor represent the shape of the binary dropout mask that will be multiply with the input  for instance  if your input have shape  batch size  timesteps  feature  and you want the dropout mask to be the same for all timesteps  you can use noise shape  batch size  1  feature   ) ( seed : a python integer to use as random seed  )
( alpha : scale for the negative factor  )
( input dim : integer  size of the vocabulary  i e  maximum integer index   1  ) ( output dim : integer  dimension of the dense embed  ) ( embeddings initializer : initializer for the embeddings matrix  see keras initializers   ) ( embeddings regularizer : regularizer function apply to the embeddings matrix  see keras regularizers   ) ( embeddings constraint : constraint function apply to the embeddings matrix  see keras constraints   ) ( mask zero : boolean  whether or not the input value 0 be a special "padding" value that should be mask out  this be useful when use recurrent layer which may take variable length input  if this be true  then all subsequent layer in the model need to support mask or an exception will be raise  if mask zero be set to true  as a consequence  index 0 cannot be use in the vocabulary  input dim should equal size of vocabulary   1   ) ( input length : length of input sequence  when it be constant  this argument be require if you be go to connect flatten then dense layer upstream  without it  the shape of the dense output cannot be compute   )
( data format : a string  one of channel last  default  or channel first  the order of the dimension in the input  channel last correspond to input with shape  batch       channel  while channel first correspond to input with shape  batch  channel        it default to the image data format value find in your keras config file at ~/ keras/keras json  if you never set it  then it will be "channels last"  )
( units : positive integer  dimensionality of the output space  ) ( activation : activation function to use  default  hyperbolic tangent  tanh   if you pass none  no activation be apply  ie  "linear" activation  a x    x   ) ( recurrent activation : activation function to use for the recurrent step  default  sigmoid  sigmoid   if you pass none  no activation be apply  ie  "linear" activation  a x    x   ) ( use bias : boolean   default true   whether the layer use a bias vector  ) ( kernel initializer : initializer for the kernel weight matrix  use for the linear transformation of the input  default  glorot uniform  ) ( recurrent initializer : initializer for the recurrent kernel weight matrix  use for the linear transformation of the recurrent state  default  orthogonal  ) ( bias initializer : initializer for the bias vector  default  zero  ) ( kernel regularizer : regularizer function apply to the kernel weight matrix  default  none  ) ( recurrent regularizer : regularizer function apply to the recurrent kernel weight matrix  default  none  ) ( bias regularizer : regularizer function apply to the bias vector  default  none  ) ( activity regularizer : regularizer function apply to the output of the layer  its "activation"   default  none  ) ( kernel constraint : constraint function apply to the kernel weight matrix  default  none  ) ( recurrent constraint : constraint function apply to the recurrent kernel weight matrix  default  none  ) ( bias constraint : constraint function apply to the bias vector  default  none  ) ( dropout : float between 0 and 1  fraction of the units to drop for the linear transformation of the input  default  0  ) ( recurrent dropout : float between 0 and 1  fraction of the units to drop for the linear transformation of the recurrent state  default  0  ) ( return sequences : boolean  whether to return the last output in the output sequence  or the full sequence  default  false  ) ( return state : boolean  whether to return the last state in addition to the output  default  false  ) ( go backwards : boolean  default false   if true  process the input sequence backwards and return the reverse sequence  ) ( stateful : boolean  default false   if true  the last state for each sample at index i in a batch will be use as initial state for the sample of index i in the follow batch  ) ( unroll : boolean  default false   if true  the network will be unroll  else a symbolic loop will be use  unroll can speed up a rnn  although it tend to be more memory intensive  unroll be only suitable for short sequence  ) ( time major : the shape format of the input and output tensors  if true  the input and output will be in shape  timesteps  batch  feature   whereas in the false case  it will be  batch  timesteps  feature   use time major   true be a bite more efficient because it avoid transpose at the begin and end of the rnn calculation  however  most tensorflow data be batch major  so by default this function accept input and emit output in batch major form  ) ( reset after : gru convention  whether to apply reset gate after or before matrix multiplication   false   "before"  true   "after"  default and cudnn compatible   )
( units : positive integer  dimensionality of the output space  ) ( activation : activation function to use  default  hyperbolic tangent  tanh   if you pass none  no activation be apply  ie  "linear" activation  a x    x   ) ( recurrent activation : activation function to use for the recurrent step  default  sigmoid  sigmoid   if you pass none  no activation be apply  ie  "linear" activation  a x    x   ) ( use bias : boolean   default true   whether the layer use a bias vector  ) ( kernel initializer : initializer for the kernel weight matrix  use for the linear transformation of the input  default  glorot uniform  ) ( recurrent initializer : initializer for the recurrent kernel weight matrix  use for the linear transformation of the recurrent state  default  orthogonal  ) ( bias initializer : initializer for the bias vector  default  zero  ) ( kernel regularizer : regularizer function apply to the kernel weight matrix  default  none  ) ( recurrent regularizer : regularizer function apply to the recurrent kernel weight matrix  default  none  ) ( bias regularizer : regularizer function apply to the bias vector  default  none  ) ( kernel constraint : constraint function apply to the kernel weight matrix  default  none  ) ( recurrent constraint : constraint function apply to the recurrent kernel weight matrix  default  none  ) ( bias constraint : constraint function apply to the bias vector  default  none  ) ( dropout : float between 0 and 1  fraction of the units to drop for the linear transformation of the input  default  0  ) ( recurrent dropout : float between 0 and 1  fraction of the units to drop for the linear transformation of the recurrent state  default  0  ) ( reset after : gru convention  whether to apply reset gate after or before matrix multiplication   false   "before"  true   "after"  default and cudnn compatible   )
( rate : float  drop probability  as with dropout   the multiplicative noise will have standard deviation sqrt rate /  1   rate    ) ( seed : integer  optional random seed to enable deterministic behavior  )
( stddev : float  standard deviation of the noise distribution  ) ( seed : integer  optional random seed to enable deterministic behavior  )
( data format : a string  one of channel last  default  or channel first  the order of the dimension in the input  channel last correspond to input with shape  batch  step  feature  while channel first correspond to input with shape  batch  feature  step   ) ( keepdims : a boolean  whether to keep the temporal dimension or not  if keepdims be false  default   the rank of the tensor be reduce for spatial dimension  if keepdims be true  the temporal dimension be retain with length 1  the behavior be the same as for tf reduce mean or np mean  )
( data format : a string  one of channel last  default  or channel first  the order of the dimension in the input  channel last correspond to input with shape  batch  height  width  channel  while channel first correspond to input with shape  batch  channel  height  width   it default to the image data format value find in your keras config file at ~/ keras/keras json  if you never set it  then it will be "channels last"  ) ( keepdims : a boolean  whether to keep the spatial dimension or not  if keepdims be false  default   the rank of the tensor be reduce for spatial dimension  if keepdims be true  the spatial dimension be retain with length 1  the behavior be the same as for tf reduce mean or np mean  )
( data format : a string  one of channel last  default  or channel first  the order of the dimension in the input  channel last correspond to input with shape  batch  spatial dim1  spatial dim2  spatial dim3  channel  while channel first correspond to input with shape  batch  channel  spatial dim1  spatial dim2  spatial dim3   it default to the image data format value find in your keras config file at ~/ keras/keras json  if you never set it  then it will be "channels last"  ) ( keepdims : a boolean  whether to keep the spatial dimension or not  if keepdims be false  default   the rank of the tensor be reduce for spatial dimension  if keepdims be true  the spatial dimension be retain with length 1  the behavior be the same as for tf reduce mean or np mean  )
( data format : a string  one of channel last  default  or channel first  the order of the dimension in the input  channel last correspond to input with shape  batch  step  feature  while channel first correspond to input with shape  batch  feature  step   ) ( keepdims : a boolean  whether to keep the temporal dimension or not  if keepdims be false  default   the rank of the tensor be reduce for spatial dimension  if keepdims be true  the temporal dimension be retain with length 1  the behavior be the same as for tf reduce max or np max  )
( data format : a string  one of channel last  default  or channel first  the order of the dimension in the input  channel last correspond to input with shape  batch  height  width  channel  while channel first correspond to input with shape  batch  channel  height  width   it default to the image data format value find in your keras config file at ~/ keras/keras json  if you never set it  then it will be "channels last"  ) ( keepdims : a boolean  whether to keep the spatial dimension or not  if keepdims be false  default   the rank of the tensor be reduce for spatial dimension  if keepdims be true  the spatial dimension be retain with length 1  the behavior be the same as for tf reduce max or np max  )
( input shape : shape tuple  not include the batch axis   or tensorshape instance  not include the batch axis   ) ( batch size : optional input batch size  integer or none   ) ( dtype : optional datatype of the input  when not provide  the keras default float type will be use  ) ( input tensor : optional tensor to use as layer input  if set  the layer will use the tf typespec of this tensor rather than create a new placeholder tensor  ) ( sparse : boolean  whether the placeholder create be mean to be sparse  default to false  ) ( ragged : boolean  whether the placeholder create be mean to be rag  in this case  value of none in the shape argument represent rag dimension  for more information about tf raggedtensor  see this guide  default to false  ) ( type spec : a tf typespec object to create input from  this tf typespec represent the entire batch  when provide  all other args except name must be none  ) ( name : optional name of the layer  string   )
( dtype : expect datatype of the input  ) ( shape : shape tuple  expect shape of the input  may include none for unchecked ax   include the batch size  ) ( ndim : integer  expect rank of the input  ) ( max ndim : integer  maximum rank of the input  ) ( min ndim : integer  minimum rank of the input  ) ( axes : dictionary map integer ax to a specific dimension value  ) ( allow last axis squeeze : if true  then allow input of rank n 1 as long as the last axis of the input be 1  as well as input of rank n 1 as long as the last axis of the spec be 1  ) ( name : expect key correspond to this input when pass data as a dictionary  )
( units : positive integer  dimensionality of the output space  ) ( activation : activation function to use  default  hyperbolic tangent  tanh   if you pass none  no activation be apply  ie  "linear" activation  a x    x   ) ( recurrent activation : activation function to use for the recurrent step  default  sigmoid  sigmoid   if you pass none  no activation be apply  ie  "linear" activation  a x    x   ) ( use bias : boolean  default true   whether the layer use a bias vector  ) ( kernel initializer : initializer for the kernel weight matrix  use for the linear transformation of the input  default  glorot uniform  ) ( recurrent initializer : initializer for the recurrent kernel weight matrix  use for the linear transformation of the recurrent state  default  orthogonal  ) ( bias initializer : initializer for the bias vector  default  zero  ) ( unit forget bias : boolean  default true   if true  add 1 to the bias of the forget gate at initialization  set it to true will also force bias initializer "zeros"  this be recommend in jozefowicz et     al   ) ( kernel regularizer : regularizer function apply to the kernel weight matrix  default  none  ) ( recurrent regularizer : regularizer function apply to the recurrent kernel weight matrix  default  none  ) ( bias regularizer : regularizer function apply to the bias vector  default  none  ) ( activity regularizer : regularizer function apply to the output of the layer  its "activation"   default  none  ) ( kernel constraint : constraint function apply to the kernel weight matrix  default  none  ) ( recurrent constraint : constraint function apply to the recurrent kernel weight matrix  default  none  ) ( bias constraint : constraint function apply to the bias vector  default  none  ) ( dropout : float between 0 and 1  fraction of the units to drop for the linear transformation of the input  default  0  ) ( recurrent dropout : float between 0 and 1  fraction of the units to drop for the linear transformation of the recurrent state  default  0  ) ( return sequences : boolean  whether to return the last output  in the output sequence  or the full sequence  default  false  ) ( return state : boolean  whether to return the last state in addition to the output  default  false  ) ( go backwards : boolean  default false   if true  process the input sequence backwards and return the reverse sequence  ) ( stateful : boolean  default false   if true  the last state for each sample at index i in a batch will be use as initial state for the sample of index i in the follow batch  ) ( time major : the shape format of the input and output tensors  if true  the input and output will be in shape  timesteps  batch  feature   whereas in the false case  it will be  batch  timesteps  feature   use time major   true be a bite more efficient because it avoid transpose at the begin and end of the rnn calculation  however  most tensorflow data be batch major  so by default this function accept input and emit output in batch major form  ) ( unroll : boolean  default false   if true  the network will be unroll  else a symbolic loop will be use  unroll can speed up a rnn  although it tend to be more memory intensive  unroll be only suitable for short sequence  )
( units : positive integer  dimensionality of the output space  ) ( activation : activation function to use  default  hyperbolic tangent  tanh   if you pass none  no activation be apply  ie  "linear" activation  a x    x   ) ( recurrent activation : activation function to use for the recurrent step  default  sigmoid  sigmoid   if you pass none  no activation be apply  ie  "linear" activation  a x    x   ) ( use bias : boolean   default true   whether the layer use a bias vector  ) ( kernel initializer : initializer for the kernel weight matrix  use for the linear transformation of the input  default  glorot uniform  ) ( recurrent initializer : initializer for the recurrent kernel weight matrix  use for the linear transformation of the recurrent state  default  orthogonal  ) ( bias initializer : initializer for the bias vector  default  zero  ) ( unit forget bias : boolean  default true   if true  add 1 to the bias of the forget gate at initialization  set it to true will also force bias initializer "zeros"  this be recommend in jozefowicz et   al  ) ( kernel regularizer : regularizer function apply to the kernel weight matrix  default  none  ) ( recurrent regularizer : regularizer function apply to the recurrent kernel weight matrix  default  none  ) ( bias regularizer : regularizer function apply to the bias vector  default  none  ) ( kernel constraint : constraint function apply to the kernel weight matrix  default  none  ) ( recurrent constraint : constraint function apply to the recurrent kernel weight matrix  default  none  ) ( bias constraint : constraint function apply to the bias vector  default  none  ) ( dropout : float between 0 and 1  fraction of the units to drop for the linear transformation of the input  default  0  ) ( recurrent dropout : float between 0 and 1  fraction of the units to drop for the linear transformation of the recurrent state  default  0  )
( function : the function to be evaluate  take input tensor as first argument  ) ( output shape : expect output shape from function  this argument can be infer if not explicitly provide  can be a tuple or function  if a tuple  it only specify the first dimension onward  sample dimension be assume either the same as the input  output shape      input shape 0       output shape or  the input be none and the sample dimension be also none  output shape    none        output shape if a function  it specify the entire shape as a function   of the input shape  output shape   f input shape  ) ( mask : either none  indicate no mask  or a callable with the same signature as the compute mask layer method  or a tensor that will be return as output mask regardless of what the input be  ) ( arguments : optional dictionary of keyword arguments to be pass to the function  )
( trainable : boolean  whether the layer's variables should be trainable  ) ( name : string name of the layer  ) ( dtype : the dtype of the layer's computations and weight  can also be a tf keras mix precision policy  which allow the computation and weight dtype to differ  default of none mean to use tf keras mix precision global policy    which be a float32 policy unless set to different value  ) ( dynamic : set this to true if your layer should only be run eagerly  and should not be use to generate a static computation graph  this would be the case for a tree rnn or a recursive network  for example  or generally for any layer that manipulate tensors use python control flow  if false  we assume that the layer can safely be use to generate a static computation graph  )
( axis : integer or list/tuple  the axis or ax to normalize across  typically this be the feature axis/axes  the leave out ax be typically the batch axis/axes  this argument default to  1  the last dimension in the input  ) ( epsilon : small float add to variance to avoid divide by zero  default to 1e 3 ) ( center : if true  add offset of beta to normalize tensor  if false  beta be ignore  default to true  ) ( scale : if true  multiply by gamma  if false  gamma be not use  default to true  when the next layer be linear  also e g  nn relu   this can be disable since the scale will be do by the next layer  ) ( beta initializer : initializer for the beta weight  default to zero  ) ( gamma initializer : initializer for the gamma weight  default to ones  ) ( beta regularizer : optional regularizer for the beta weight  none by default  ) ( gamma regularizer : optional regularizer for the gamma weight  none by default  ) ( beta constraint : optional constraint for the beta weight  none by default  ) ( gamma constraint : optional constraint for the gamma weight  none by default  )
( alpha : float >  0  negative slope coefficient  default to 0 3  )
( filters : integer  the dimensionality of the output space  i e  the number of output filter in the convolution   ) ( kernel size : an integer or tuple/list of a single integer  specify the length of the 1d convolution window  ) ( strides : an integer or tuple/list of a single integer  specify the stride length of the convolution  ) ( padding : currently only support "valid"  case insensitive   "same" may be support in the future  "valid" mean no pad  ) ( data format : a string  one of channel last  default  or channel first  the order of the dimension in the input  channel last correspond to input with shape  batch  length  channel  while channel first correspond to input with shape  batch  channel  length   it default to the image data format value find in your keras config file at ~/ keras/keras json  if you never set it  then it will be "channels last"  ) ( activation : activation function to use  if you don't specify anything  no activation be apply    ie  "linear" activation  a x    x   ) ( use bias : boolean  whether the layer use a bias vector  ) ( kernel initializer : initializer for the kernel weight matrix  ) ( bias initializer : initializer for the bias vector  ) ( kernel regularizer : regularizer function apply to the kernel weight matrix  ) ( bias regularizer : regularizer function apply to the bias vector  ) ( activity regularizer : regularizer function apply to the output of the layer  its "activation"    ) ( kernel constraint : constraint function apply to the kernel matrix  ) ( bias constraint : constraint function apply to the bias vector  ) ( implementation : implementation mode  either 1  2  or 3  1 loop over input spatial locations to perform the forward pass  it be memory efficient but perform a lot of  small  ops   2 store layer weight in a dense but sparsely populate 2d matrix and implement the forward pass as a single matrix multiply  it use a lot of ram but perform few  large  ops   3 store layer weight in a sparse tensor and implement the forward pass as a single sparse matrix multiply    how to choose    1  large  dense model    2  small model    3  large  sparse model   where "large" stand for large     input/output activations  i e  many filter  input filter      large input size  output size   and "sparse" stand for few     connections between input and output  i e  small ratio filter       input filter   kernel size /  input size   stride   where input     to and output of the layer be assume to have shape  input size      input filter    output size  filter  respectively   it be     recommend to benchmark each in the set of interest to pick the     most efficient one  in term of speed and memory usage   correct     choice of implementation can lead to dramatic speed improvements      e g  50x   potentially at the expense of ram   also  only     pad "valid" be support by implementation 1  )
( filters : integer  the dimensionality of the output space  i e  the number of output filter in the convolution   ) ( kernel size : an integer or tuple/list of 2 integers  specify the width and height of the 2d convolution window  can be a single integer to specify the same value for all spatial dimension  ) ( strides : an integer or tuple/list of 2 integers  specify the stride of the convolution along the width and height  can be a single integer to specify the same value for all spatial dimension  ) ( padding : currently only support "valid"  case insensitive   "same" will be support in future  "valid" mean no pad  ) ( data format : a string  one of channel last  default  or channel first  the order of the dimension in the input  channel last correspond to input with shape  batch  height  width  channel  while channel first correspond to input with shape  batch  channel  height  width   it default to the image data format value find in your keras config file at ~/ keras/keras json  if you never set it  then it will be "channels last"  ) ( activation : activation function to use  if you don't specify anything  no activation be apply    ie  "linear" activation  a x    x   ) ( use bias : boolean  whether the layer use a bias vector  ) ( kernel initializer : initializer for the kernel weight matrix  ) ( bias initializer : initializer for the bias vector  ) ( kernel regularizer : regularizer function apply to the kernel weight matrix  ) ( bias regularizer : regularizer function apply to the bias vector  ) ( activity regularizer : regularizer function apply to the output of the layer  its "activation"   ) ( kernel constraint : constraint function apply to the kernel matrix  ) ( bias constraint : constraint function apply to the bias vector  ) ( implementation : implementation mode  either 1  2  or 3  1 loop over input spatial locations to perform the forward pass  it be memory efficient but perform a lot of  small  ops   2 store layer weight in a dense but sparsely populate 2d matrix and implement the forward pass as a single matrix multiply  it use a lot of ram but perform few  large  ops   3 store layer weight in a sparse tensor and implement the forward pass as a single sparse matrix multiply    how to choose    1  large  dense model    2  small model    3  large  sparse model   where "large" stand for large     input/output activations  i e  many filter  input filter      large np prod input size   np prod output size    and "sparse"     stand for few connections between input and output  i e  small     ratio filter   input filter   np prod kernel size  /      np prod input size    np prod stride    where input to and     output of the layer be assume to have shape input size        input filter    output size    filter   respectively   it be     recommend to benchmark each in the set of interest to pick the     most efficient one  in term of speed and memory usage   correct     choice of implementation can lead to dramatic speed improvements      e g  50x   potentially at the expense of ram   also  only     pad "valid" be support by implementation 1  )

( pool size : integer  size of the max pool window  ) ( strides : integer  or none  specify how much the pool window move for each pool step  if none  it will default to pool size  ) ( padding : one of "valid" or "same"  case insensitive   "valid" mean no pad  "same" result in pad evenly to the left/right or up/down of the input such that output have the same height/width dimension as the input  ) ( data format : a string  one of channel last  default  or channel first  the order of the dimension in the input  channel last correspond to input with shape  batch  step  feature  while channel first correspond to input with shape  batch  feature  step   )
( pool size : integer or tuple of 2 integers  window size over which to take the maximum   2  2  will take the max value over a 2x2 pool window  if only one integer be specify  the same window length will be use for both dimension  ) ( strides : integer  tuple of 2 integers  or none  stride value   specify how far the pool window move for each pool step  if none  it will default to pool size  ) ( padding : one of "valid" or "same"  case insensitive   "valid" mean no pad  "same" result in pad evenly to the left/right or up/down of the input such that output have the same height/width dimension as the input  ) ( data format : a string  one of channel last  default  or channel first  the order of the dimension in the input  channel last correspond to input with shape  batch  height  width  channel  while channel first correspond to input with shape  batch  channel  height  width   it default to the image data format value find in your keras config file at ~/ keras/keras json  if you never set it  then it will be "channels last"  )
( pool size : tuple of 3 integers  factor by which to downscale  dim1  dim2  dim3    2  2  2  will halve the size of the 3d input in each dimension  ) ( strides : tuple of 3 integers  or none  stride value  ) ( padding : one of "valid" or "same"  case insensitive   "valid" mean no pad  "same" result in pad evenly to the left/right or up/down of the input such that output have the same height/width dimension as the input  ) ( data format : a string  one of channel last  default  or channel first  the order of the dimension in the input  channel last correspond to input with shape  batch  spatial dim1  spatial dim2  spatial dim3  channel  while channel first correspond to input with shape  batch  channel  spatial dim1  spatial dim2  spatial dim3   it default to the image data format value find in your keras config file at ~/ keras/keras json  if you never set it  then it will be "channels last"  )
(   kwargs : standard layer keyword arguments  )
(   kwargs : standard layer keyword arguments  )
(   kwargs : standard layer keyword arguments  )
( alpha initializer : initializer function for the weight  ) ( alpha regularizer : regularizer for the weight  ) ( alpha constraint : constraint for the weight  ) ( shared axes : the ax along which to share learnable parameters for the activation function  for example  if the incoming feature map be from a 2d convolution with output shape  batch  height  width  channel   and you wish to share parameters across space so that each filter only have one set of parameters  set share ax  1  2   )
( dims : tuple of integers  permutation pattern do not include the sample dimension  index start at 1  for instance   2  1  permute the first and second dimension of the input  )
( cell : a rnn cell instance or a list of rnn cell instance  a rnn cell be a class that have   a call input at t  state at t  method  return  output at t  state at t plus 1   the call method of the cell can also take the optional argument constants  see section "note on pass external constants" below  a state size attribute  this can be a single integer  single state  in which case it be the size of the recurrent state  this can also be a list/tuple of integers  one size per state   the state size can also be tensorshape or tuple/list of tensorshape  to represent high dimension state  a output size attribute  this can be a single integer or a tensorshape  which represent the shape of the output  for backward compatible reason  if this attribute be not available for the cell  the value will be infer by the first element of the state size  a get initial state input none  batch size none  dtype none  method that create a tensor mean to be feed to call   as the initial state  if the user didn't specify any initial state via other mean  the return initial state should have a shape of  batch size  cell state size   the cell might choose to create a tensor full of zero  or full of other value base on the cell's implementation  input be the input tensor to the rnn layer  which should contain the batch size as its shape 0   and also dtype  note that the shape 0  might be none during the graph construction  either the input or the pair of batch size and dtype be provide  batch size be a scalar tensor that represent the batch size of the input  dtype be tf dtype that represent the dtype of the input  for backward compatibility  if this method be not implement by the cell  the rnn layer will create a zero fill tensor with the size of  batch size  cell state size   in the case that cell be a list of rnn cell instance  the cells will be stack on top of each other in the rnn  result in an efficient stack rnn  ) ( return sequences : boolean  default false   whether to return the last output in the output sequence  or the full sequence  ) ( return state : boolean  default false   whether to return the last state in addition to the output  ) ( go backwards : boolean  default false   if true  process the input sequence backwards and return the reverse sequence  ) ( stateful : boolean  default false   if true  the last state for each sample at index i in a batch will be use as initial state for the sample of index i in the follow batch  ) ( unroll : boolean  default false   if true  the network will be unroll  else a symbolic loop will be use  unroll can speed up a rnn  although it tend to be more memory intensive  unroll be only suitable for short sequence  ) ( time major : the shape format of the input and output tensors  if true  the input and output will be in shape  timesteps  batch        whereas in the false case  it will be  batch  timesteps        use time major   true be a bite more efficient because it avoid transpose at the begin and end of the rnn calculation  however  most tensorflow data be batch major  so by default this function accept input and emit output in batch major form  ) ( zero output for mask : boolean  default false   whether the output should use zero for the mask timesteps  note that this field be only use when return sequence be true and mask be provide  it can useful if you want to reuse the raw output sequence of the rnn without interference from the mask timesteps  eg  merge bidirectional rnns  )
( max value : float >  0  maximum activation value  default to none  which mean unlimited  ) ( negative slope : float >  0  negative slope coefficient  default to 0  ) ( threshold : float >  0  threshold value for thresholded activation  default to 0  )
( n : integer  repetition factor  )
( target shape : target shape  tuple of integers  do not include the sample dimension  batch size   ) (   kwargs : any additional layer keyword arguments  )
( filters : integer  the dimensionality of the output space  i e  the number of filter in the convolution   ) ( kernel size : a single integer specify the spatial dimension of the filter  ) ( strides : a single integer specify the stride of the convolution  specify any stride value    1 be incompatible with specify any dilation rate value    1  ) ( padding : one of "valid"  "same"  or "causal"  case insensitive   "valid" mean no pad  "same" result in pad with zero evenly to the left/right or up/down of the input such that output have the same height/width dimension as the input  "causal" result in causal  dilate  convolutions  e g  output t  do not depend on input t 1    ) ( data format : a string  one of channel last  default  or channel first  the order of the dimension in the input  channel last correspond to input with shape  batch size  length  channel  while channel first correspond to input with shape  batch size  channel  length   ) ( dilation rate : a single integer  specify the dilation rate to use for dilate convolution  ) ( depth multiplier : the number of depthwise convolution output channel for each input channel  the total number of depthwise convolution output channel will be equal to num filter in   depth multiplier  ) ( activation : activation function to use  if you don't specify anything  no activation be apply  see keras activations   ) ( use bias : boolean  whether the layer use a bias  ) ( depthwise initializer : an initializer for the depthwise convolution kernel  see keras initializers   if none  then the default initializer  'glorot uniform'  will be use  ) ( pointwise initializer : an initializer for the pointwise convolution kernel  see keras initializers   if none  then the default initializer  'glorot uniform'  will be use  ) ( bias initializer : an initializer for the bias vector  if none  the default initializer  'zeros'  will be use  see keras initializers   ) ( depthwise regularizer : optional regularizer for the depthwise convolution kernel  see keras regularizers   ) ( pointwise regularizer : optional regularizer for the pointwise convolution kernel  see keras regularizers   ) ( bias regularizer : optional regularizer for the bias vector  see keras regularizers   ) ( activity regularizer : optional regularizer function for the output  see keras regularizers   ) ( depthwise constraint : optional projection function to be apply to the depthwise kernel after be update by an optimizer  e g  use for norm constraints or value constraints for layer weight   the function must take as input the unprojected variable and must return the project variable  which must have the same shape   constraints be not safe to use when do asynchronous distribute train  see keras constraints   ) ( pointwise constraint : optional projection function to be apply to the pointwise kernel after be update by an optimizer  see keras constraints   ) ( bias constraint : optional projection function to be apply to the bias after be update by an optimizer  see keras constraints   ) ( trainable : boolean  if true the weight of this layer will be mark as trainable  and list in layer trainable weight   )
( filters : integer  the dimensionality of the output space  i e  the number of output filter in the convolution   ) ( kernel size : an integer or tuple/list of 2 integers  specify the height and width of the 2d convolution window  can be a single integer to specify the same value for all spatial dimension  ) ( strides : an integer or tuple/list of 2 integers  specify the stride of the convolution along the height and width  can be a single integer to specify the same value for all spatial dimension  current implementation only support equal length stride in the row and column dimension  specify any stride value    1 be incompatible with specify any dilation rate value    1  ) ( padding : one of "valid" or "same"  case insensitive   "valid" mean no pad  "same" result in pad with zero evenly to the left/right or up/down of the input such that output have the same height/width dimension as the input  ) ( data format : a string  one of channel last  default  or channel first  the order of the dimension in the input  channel last correspond to input with shape  batch size  height  width  channel  while channel first correspond to input with shape  batch size  channel  height  width   it default to the image data format value find in your keras config file at ~/ keras/keras json  if you never set it  then it will be "channels last"  ) ( dilation rate : an integer or tuple/list of 2 integers  specify the dilation rate to use for dilate convolution  ) ( depth multiplier : the number of depthwise convolution output channel for each input channel  the total number of depthwise convolution output channel will be equal to filter in   depth multiplier  ) ( activation : activation function to use  if you don't specify anything  no activation be apply  see keras activations   ) ( use bias : boolean  whether the layer use a bias vector  ) ( depthwise initializer : an initializer for the depthwise convolution kernel  see keras initializers   if none  then the default initializer  'glorot uniform'  will be use  ) ( pointwise initializer : an initializer for the pointwise convolution kernel  see keras initializers   if none  then the default initializer  'glorot uniform'  will be use  ) ( bias initializer : an initializer for the bias vector  if none  the default initializer  'zeros'  will be use  see keras initializers   ) ( depthwise regularizer : regularizer function apply to the depthwise kernel matrix  see keras regularizers   ) ( pointwise regularizer : regularizer function apply to the pointwise kernel matrix  see keras regularizers   ) ( bias regularizer : regularizer function apply to the bias vector  see keras regularizers   ) ( activity regularizer : regularizer function apply to the output of the layer  its "activation"   see keras regularizers   ) ( depthwise constraint : constraint function apply to the depthwise kernel matrix  see keras constraints   ) ( pointwise constraint : constraint function apply to the pointwise kernel matrix  see keras constraints   ) ( bias constraint : constraint function apply to the bias vector  see keras constraints   )
( units : positive integer  dimensionality of the output space  ) ( activation : activation function to use  default  hyperbolic tangent  tanh   if you pass none  no activation be apply  ie  "linear" activation  a x    x   ) ( use bias : boolean   default true   whether the layer use a bias vector  ) ( kernel initializer : initializer for the kernel weight matrix  use for the linear transformation of the input  default  glorot uniform  ) ( recurrent initializer : initializer for the recurrent kernel weight matrix  use for the linear transformation of the recurrent state  default  orthogonal  ) ( bias initializer : initializer for the bias vector  default  zero  ) ( kernel regularizer : regularizer function apply to the kernel weight matrix  default  none  ) ( recurrent regularizer : regularizer function apply to the recurrent kernel weight matrix  default  none  ) ( bias regularizer : regularizer function apply to the bias vector  default  none  ) ( activity regularizer : regularizer function apply to the output of the layer  its "activation"   default  none  ) ( kernel constraint : constraint function apply to the kernel weight matrix  default  none  ) ( recurrent constraint : constraint function apply to the recurrent kernel weight matrix   default  none  ) ( bias constraint : constraint function apply to the bias vector  default  none  ) ( dropout : float between 0 and 1  fraction of the units to drop for the linear transformation of the input  default  0  ) ( recurrent dropout : float between 0 and 1  fraction of the units to drop for the linear transformation of the recurrent state  default  0  ) ( return sequences : boolean  whether to return the last output in the output sequence  or the full sequence  default  false  ) ( return state : boolean  whether to return the last state in addition to the output  default  false ) ( go backwards : boolean  default false   if true  process the input sequence backwards and return the reverse sequence  ) ( stateful : boolean  default false   if true  the last state for each sample at index i in a batch will be use as initial state for the sample of index i in the follow batch  ) ( unroll : boolean  default false   if true  the network will be unroll  else a symbolic loop will be use  unroll can speed up a rnn  although it tend to be more memory intensive  unroll be only suitable for short sequence  )
( units : positive integer  dimensionality of the output space  ) ( activation : activation function to use  default  hyperbolic tangent  tanh   if you pass none  no activation be apply  ie  "linear" activation  a x    x   ) ( use bias : boolean   default true   whether the layer use a bias vector  ) ( kernel initializer : initializer for the kernel weight matrix  use for the linear transformation of the input  default  glorot uniform  ) ( recurrent initializer : initializer for the recurrent kernel weight matrix  use for the linear transformation of the recurrent state  default  orthogonal  ) ( bias initializer : initializer for the bias vector  default  zero  ) ( kernel regularizer : regularizer function apply to the kernel weight matrix  default  none  ) ( recurrent regularizer : regularizer function apply to the recurrent kernel weight matrix  default  none  ) ( bias regularizer : regularizer function apply to the bias vector  default  none  ) ( kernel constraint : constraint function apply to the kernel weight matrix  default  none  ) ( recurrent constraint : constraint function apply to the recurrent kernel weight matrix  default  none  ) ( bias constraint : constraint function apply to the bias vector  default  none  ) ( dropout : float between 0 and 1  fraction of the units to drop for the linear transformation of the input  default  0  ) ( recurrent dropout : float between 0 and 1  fraction of the units to drop for the linear transformation of the recurrent state  default  0  )
( axis : integer  or list of integers  axis along which the softmax normalization be apply  )
( rate : float between 0 and 1  fraction of the input units to drop  )
( rate : float between 0 and 1  fraction of the input units to drop  ) ( data format : 'channels first' or 'channels last'  in 'channels first' mode  the channel dimension  the depth  be at index 1  in 'channels last' mode be it at index 3  it default to the image data format value find in your keras config file at ~/ keras/keras json  if you never set it  then it will be "channels last"  )
( rate : float between 0 and 1  fraction of the input units to drop  ) ( data format : 'channels first' or 'channels last'  in 'channels first' mode  the channel dimension  the depth  be at index 1  in 'channels last' mode be it at index 4  it default to the image data format value find in your keras config file at ~/ keras/keras json  if you never set it  then it will be "channels last"  )
( cells : list of rnn cell instance  )
(   kwargs : standard layer keyword arguments  )
( theta : float >  0  threshold location of activation  )
( layer : a tf keras layer layer instance  )
( size : integer  upsampling factor  )
( size : int  or tuple of 2 integers  the upsampling factor for row and columns  ) ( data format : a string  one of channel last  default  or channel first  the order of the dimension in the input  channel last correspond to input with shape  batch size  height  width  channel  while channel first correspond to input with shape  batch size  channel  height  width   it default to the image data format value find in your keras config file at ~/ keras/keras json  if you never set it  then it will be "channels last"  ) ( interpolation : a string  one of "area"  "bicubic"  "bilinear"  "gaussian"  "lanczos3"  "lanczos5"  "mitchellcubic"  "nearest"  )
( size : int  or tuple of 3 integers  the upsampling factor for dim1  dim2 and dim3  ) ( data format : a string  one of channel last  default  or channel first  the order of the dimension in the input  channel last correspond to input with shape  batch size  spatial dim1  spatial dim2  spatial dim3  channel  while channel first correspond to input with shape  batch size  channel  spatial dim1  spatial dim2  spatial dim3   it default to the image data format value find in your keras config file at ~/ keras/keras json  if you never set it  then it will be "channels last"  )
( layer : the layer to be wrap  )
( padding : int  or tuple of int  length 2   or dictionary   if int  how many zero to add at the begin and end of the pad dimension  axis 1   if tuple of int  length 2   how many zero to add at the begin and the end of the pad dimension   leave pad  right pad    )
( padding : int  or tuple of 2 ints  or tuple of 2 tuples of 2 ints   if int  the same symmetric pad be apply to height and width  if tuple of 2 ints  interpret as two different symmetric pad value for height and width   symmetric height pad  symmetric width pad   if tuple of 2 tuples of 2 ints  interpret as   top pad  bottom pad    leave pad  right pad   ) ( data format : a string  one of channel last  default  or channel first  the order of the dimension in the input  channel last correspond to input with shape  batch size  height  width  channel  while channel first correspond to input with shape  batch size  channel  height  width   it default to the image data format value find in your keras config file at ~/ keras/keras json  if you never set it  then it will be "channels last"  )
( padding : int  or tuple of 3 ints  or tuple of 3 tuples of 2 ints   if int  the same symmetric pad be apply to height and width  if tuple of 3 ints  interpret as two different symmetric pad value for height and width   symmetric dim1 pad  symmetric dim2 pad  symmetric dim3 pad   if tuple of 3 tuples of 2 ints  interpret as   leave dim1 pad  right dim1 pad    leave dim2 pad  right dim2 pad    leave dim3 pad  right dim3 pad   ) ( data format : a string  one of channel last  default  or channel first  the order of the dimension in the input  channel last correspond to input with shape  batch size  spatial dim1  spatial dim2  spatial dim3  channel  while channel first correspond to input with shape  batch size  channel  spatial dim1  spatial dim2  spatial dim3   it default to the image data format value find in your keras config file at ~/ keras/keras json  if you never set it  then it will be "channels last"  )
( inputs : a list of input tensors with the same shape  ) (   kwargs : standard layer keyword arguments  )
( inputs : a list of input tensors  ) (   kwargs : standard layer keyword arguments  )
( inputs : a list of input tensors  ) ( axis : concatenation axis  ) (   kwargs : standard layer keyword arguments  )
( config : dict of the form  'class name'  str  'config'  dict  ) ( custom objects : dict map class name  or function name  of custom  non keras  object to class/functions )
( inputs : a list of input tensors  at least 2   ) ( axes : integer or tuple of integers  axis or ax along which to take the dot product  ) ( normalize : whether to l2 normalize sample along the dot product axis before take the dot product  if set to true  then the output of the dot product be the cosine proximity between the two sample  ) (   kwargs : standard layer keyword arguments  )
( inputs : a list of input tensors of same shape  ) (   kwargs : standard layer keyword arguments  )
( inputs : a list of input tensors  ) (   kwargs : standard layer keyword arguments  )
( inputs : a list of input tensors  ) (   kwargs : standard layer keyword arguments  )
( layer : the layer object to serialize  )
( inputs : a list of input tensors  exactly 2   ) (   kwargs : standard layer keyword arguments  )
( is adapted : whether the layer have be fit to data already  )
( from logits : whether to interpret y pred as a tensor of logit value  by default  we   assume that y pred contain probabilities  i e   value in  0  1    ) ( label smoothing : float in  0  1   when 0  no smooth occur  when > 0  we compute the loss between the predict label and a smooth version of the true label  where the smooth squeeze the label towards 0 5  larger value of label smooth correspond to heavier smooth  ) ( axis : the axis along which to compute crossentropy  the feature axis   default to  1  ) ( reduction : type of tf keras losses reduction to apply to loss  default value be auto  auto indicate that the reduction option will be determine by the usage context  for almost all case this default to sum over batch size  when use with tf distribute strategy  outside of build in train loop such as tf keras compile and fit  use auto or sum over batch size will raise an error  please see this custom train tutorial for     more detail  ) ( name : name for the op  default to 'binary crossentropy'  )
( from logits : whether y pred be expect to be a logits tensor  by default  we assume that y pred encode a probability distribution  ) ( label smoothing : float in  0  1   when > 0  label value be smooth  mean the confidence on label value be relax  for example  if 0 1  use 0 1 / num class for non target label and 0 9   0 1 / num class for target label  ) ( axis : the axis along which to compute crossentropy  the feature axis   default to  1  ) ( reduction : type of tf keras losses reduction to apply to loss  default value be auto  auto indicate that the reduction option will be determine by the usage context  for almost all case this default to sum over batch size  when use with tf distribute strategy  outside of build in train loop such as tf keras compile and fit  use auto or sum over batch size will raise an error  please see this custom train tutorial for     more detail  ) ( name : optional name for the instance  default to 'categorical crossentropy'  )
( reduction : type of tf keras losses reduction to apply to loss  default value be auto  auto indicate that the reduction option will be determine by the usage context  for almost all case this default to sum over batch size  when use with tf distribute strategy  outside of build in train loop such as tf keras compile and fit  use auto or sum over batch size will raise an error  please see this custom train tutorial for     more detail  ) ( name : optional name for the instance  default to 'categorical hinge'  )
( axis : the axis along which the cosine similarity be compute  the feature axis   default to  1  ) ( reduction : type of tf keras losses reduction to apply to loss  default value be auto  auto indicate that the reduction option will be determine by the usage context  for almost all case this default to sum over batch size  when use with tf distribute strategy  outside of build in train loop such as tf keras compile and fit  use auto or sum over batch size will raise an error  please see this custom train tutorial for more   detail  ) ( name : optional name for the instance  )
( reduction : type of tf keras losses reduction to apply to loss  default value be auto  auto indicate that the reduction option will be determine by the usage context  for almost all case this default to sum over batch size  when use with tf distribute strategy  outside of build in train loop such as tf keras compile and fit  use auto or sum over batch size will raise an error  please see this custom train tutorial for     more detail  ) ( name : optional name for the instance  default to 'hinge'  )
( delta : a float  the point where the huber loss function change from a quadratic to linear  ) ( reduction : type of tf keras losses reduction to apply to loss  default value be auto  auto indicate that the reduction option will be determine by the usage context  for almost all case this default to sum over batch size  when use with tf distribute strategy  outside of build in train loop such as tf keras compile and fit  use auto or sum over batch size will raise an error  please see this custom train tutorial for     more detail  ) ( name : optional name for the instance  default to 'huber loss'  )
( reduction : type of tf keras losses reduction to apply to loss  default value be auto  auto indicate that the reduction option will be determine by the usage context  for almost all case this default to sum over batch size  when use with tf distribute strategy  outside of build in train loop such as tf keras compile and fit  use auto or sum over batch size will raise an error  please see this custom train tutorial for     more detail  ) ( name : optional name for the instance  default to 'kl divergence'  )
( reduction : type of tf keras losses reduction to apply to loss  default value be auto  auto indicate that the reduction option will be determine by the usage context  for almost all case this default to sum over batch size  when use with tf distribute strategy  outside of build in train loop such as tf keras compile and fit  use auto or sum over batch size will raise an error  please see this custom train tutorial for     more detail  ) ( name : optional name for the instance  default to 'log cosh'  )
( reduction : type of tf keras losses reduction to apply to loss  default value be auto  auto indicate that the reduction option will be determine by the usage context  for almost all case this default to sum over batch size  when use with tf distribute strategy  outside of build in train loop such as tf keras compile and fit  use auto or sum over batch size will raise an error  please see this custom train tutorial for     more detail  ) ( name : optional name for the instance  )
( reduction : type of tf keras losses reduction to apply to loss  default value be auto  auto indicate that the reduction option will be determine by the usage context  for almost all case this default to sum over batch size  when use with tf distribute strategy  outside of build in train loop such as tf keras compile and fit  use auto or sum over batch size will raise an error  please see this custom train tutorial for     more detail  ) ( name : optional name for the instance  default to 'mean absolute error'  )
( reduction : type of tf keras losses reduction to apply to loss  default value be auto  auto indicate that the reduction option will be determine by the usage context  for almost all case this default to sum over batch size  when use with tf distribute strategy  outside of build in train loop such as tf keras compile and fit  use auto or sum over batch size will raise an error  please see this custom train tutorial for     more detail  ) ( name : optional name for the instance  default to 'mean absolute percentage error'  )
( reduction : type of tf keras losses reduction to apply to loss  default value be auto  auto indicate that the reduction option will be determine by the usage context  for almost all case this default to sum over batch size  when use with tf distribute strategy  outside of build in train loop such as tf keras compile and fit  use auto or sum over batch size will raise an error  please see this custom train tutorial for     more detail  ) ( name : optional name for the instance  default to 'mean square error'  )
( reduction : type of tf keras losses reduction to apply to loss  default value be auto  auto indicate that the reduction option will be determine by the usage context  for almost all case this default to sum over batch size  when use with tf distribute strategy  outside of build in train loop such as tf keras compile and fit  use auto or sum over batch size will raise an error  please see this custom train tutorial for     more detail  ) ( name : optional name for the instance  default to 'mean square logarithmic error'  )
( reduction : type of tf keras losses reduction to apply to loss  default value be auto  auto indicate that the reduction option will be determine by the usage context  for almost all case this default to sum over batch size  when use with tf distribute strategy  outside of build in train loop such as tf keras compile and fit  use auto or sum over batch size will raise an error  please see this custom train tutorial for     more detail  ) ( name : optional name for the instance  default to 'poisson'  )

( from logits : whether y pred be expect to be a logits tensor  by default  we assume that y pred encode a probability distribution  ) ( reduction : type of tf keras losses reduction to apply to loss  default value be auto  auto indicate that the reduction option will be determine by the usage context  for almost all case this default to sum over batch size  when use with tf distribute strategy  outside of build in train loop such as tf keras compile and fit  use auto or sum over batch size will raise an error  please see this custom train tutorial for     more detail  ) ( name : optional name for the instance  default to 'sparse categorical crossentropy'  )
( reduction : type of tf keras losses reduction to apply to loss  default value be auto  auto indicate that the reduction option will be determine by the usage context  for almost all case this default to sum over batch size  when use with tf distribute strategy  outside of build in train loop such as tf keras compile and fit  use auto or sum over batch size will raise an error  please see this custom train tutorial for     more detail  ) ( name : optional name for the instance  default to 'squared hinge'  )
( y true : the grind truth value  y true value be expect to be either   1   1  or  0  1   i e  a one hot encode tensor   ) ( y pred : the predict value  )
( y true : tensor of true target  ) ( y pred : tensor of predict target  ) ( axis : axis along which to determine similarity  )
( name : loss configuration  ) ( custom objects : optional dictionary map name  string  to custom object  class and function  to be consider during deserialization  )
( identifier : a loss identifier  one of none or string name of a loss function/class or loss configuration dictionary or a loss function or a loss class instance  )
( loss : a keras loss instance or a loss function  )
( num thresholds :  optional  default to 200  the number of thresholds to use when discretizing the roc curve  value must be > 1  ) ( curve :  optional  specify the name of the curve to be compute  'roc'  default  or 'pr' for the precision recall curve  ) ( summation method :  optional  specify the riemann summation method use  'interpolation'  default  apply mid point summation scheme for roc  for pr auc  interpolate  true/false  positives but not the ratio that be precision  see davis   goadrich 2006 for detail   'minoring' apply leave summation for increase intervals and right summation for decrease intervals  'majoring' do the opposite  ) ( name :  optional  string name of the metric instance  ) ( dtype :  optional  data type of the metric result  ) ( thresholds :  optional  a list of float point value to use as the thresholds for discretizing the curve  if set  the num thresholds parameter be ignore  value should be in  0  1   endpoint thresholds equal to   epsilon  1 epsilon  for a small positive epsilon value will be automatically include with these to correctly handle predictions equal to exactly 0 or 1  ) ( multi label : boolean indicate whether multilabel data should be treat as such  wherein auc be compute separately for each label and then average across label  or  when false  if the data should be flatten into a single label before auc computation  in the latter case  when multilabel data be pass to auc  each label prediction pair be treat as an individual data point  should be set to false for multi class data  ) ( num labels :  optional  the number of label  use when multi label be true  if num label be not specify  then state variables get create on the first call to update state  ) ( label weights :  optional  list  array  or tensor of non negative weight use to compute aucs for multilabel data  when multi label be true  the weight be apply to the individual label aucs when they be average to produce the multi label auc  when it's false  they be use to weight the individual label predictions in compute the confusion matrix on the flatten data  note that this be unlike class weight in that class weight weight the example depend on the value of its label  whereas label weight depend only on the index of that label before flatten  therefore label weight should not be use for multi class data  ) ( from logits : boolean indicate whether the predictions  y pred in update state  be probabilities or sigmoid logits  as a rule of thumb  when use a keras loss  the from logits constructor argument of the loss should match the auc from logits constructor argument  )
( name :  optional  string name of the metric instance  ) ( dtype :  optional  data type of the metric result  )
( name :  optional  string name of the metric instance  ) ( dtype :  optional  data type of the metric result  ) ( threshold :  optional  float represent the threshold for decide whether prediction value be 1 or 0  )
( name :  optional  string name of the metric instance  ) ( dtype :  optional  data type of the metric result  ) ( from logits :  optional  whether output be expect to be a logits tensor  by default  we consider that output encode a probability distribution  ) ( label smoothing :  optional  float in  0  1   when > 0  label value be smooth  mean the confidence on label value be relax  e g  label smooth 0 2 mean that we will use a value of 0 1 for label 0 and 0 9 for label 1"  )
( name :  optional  string name of the metric instance  ) ( dtype :  optional  data type of the metric result  )
( name :  optional  string name of the metric instance  ) ( dtype :  optional  data type of the metric result  ) ( from logits :  optional  whether output be expect to be a logits tensor  by default  we consider that output encode a probability distribution  ) ( label smoothing :  optional  float in  0  1   when > 0  label value be smooth  mean the confidence on label value be relax  e g  label smooth 0 2 mean that we will use a value of 0 1 for label 0 and 0 9 for label 1" )
( name :  optional  string name of the metric instance  ) ( dtype :  optional  data type of the metric result  )
( name :  optional  string name of the metric instance  ) ( dtype :  optional  data type of the metric result  ) ( axis :  optional  default to  1  the dimension along which the cosine similarity be compute  )
( thresholds :  optional  default to 0 5  a float value or a python list/tuple of float threshold value in  0  1   a threshold be compare with prediction value to determine the truth value of predictions  i e   above the threshold be true  below be false   one metric value be generate for each threshold value  ) ( name :  optional  string name of the metric instance  ) ( dtype :  optional  data type of the metric result  )
( thresholds :  optional  default to 0 5  a float value or a python list/tuple of float threshold value in  0  1   a threshold be compare with prediction value to determine the truth value of predictions  i e   above the threshold be true  below be false   one metric value be generate for each threshold value  ) ( name :  optional  string name of the metric instance  ) ( dtype :  optional  data type of the metric result  )
( name :  optional  string name of the metric instance  ) ( dtype :  optional  data type of the metric result  )
( name :  optional  string name of the metric instance  ) ( dtype :  optional  data type of the metric result  )
( name :  optional  string name of the metric instance  ) ( dtype :  optional  data type of the metric result  )
( name :  optional  string name of the metric instance  ) ( dtype :  optional  data type of the metric result  )
( name :  optional  string name of the metric instance  ) ( dtype :  optional  data type of the metric result  )
( name :  optional  string name of the metric instance  ) ( dtype :  optional  data type of the metric result  )
( num classes : the possible number of label the prediction task can have  this value must be provide  since a confusion matrix of dimension    num class  num class  will be allocate  ) ( name :  optional  string name of the metric instance  ) ( dtype :  optional  data type of the metric result  )
( normalizer : the normalizer value with same shape as predictions  ) ( name :  optional  string name of the metric instance  ) ( dtype :  optional  data type of the metric result  )
( name :  optional  string name of the metric instance  ) ( dtype :  optional  data type of the metric result  )
( name :  optional  string name of the metric instance  ) ( dtype :  optional  data type of the metric result  )
( name :  optional  string name of the metric instance  ) ( dtype :  optional  data type of the metric result  ) ( shape :  optional  a list of integers  a tuple of integers  or a 1 d tensor of type int32  if not specify  the shape be infer from the value at the first call of update state  )
( name :  optional  string name of the metric instance  ) ( dtype :  optional  data type of the metric result  ) (   kwargs : additional layer keywords arguments  )
( name :  optional  string name of the metric instance  ) ( dtype :  optional  data type of the metric result  )
( thresholds :  optional  a float value or a python list/tuple of float threshold value in  0  1   a threshold be compare with prediction value to determine the truth value of predictions  i e   above the threshold be true  below be false   one metric value be generate for each threshold value  if neither thresholds nor top k be set  the default be to calculate precision with thresholds 0 5  ) ( top k :  optional  unset by default  an int value specify the top k predictions to consider when calculate precision  ) ( class id :  optional  integer class id for which we want binary metrics  this must be in the half open interval  0  num class   where num class be the last dimension of predictions  ) ( name :  optional  string name of the metric instance  ) ( dtype :  optional  data type of the metric result  )
( recall : a scalar value in range  0  1   ) ( num thresholds :  optional  default to 200  the number of thresholds to use for match the give recall  ) ( class id :  optional  integer class id for which we want binary metrics  this must be in the half open interval  0  num class   where num class be the last dimension of predictions  ) ( name :  optional  string name of the metric instance  ) ( dtype :  optional  data type of the metric result  )
( thresholds :  optional  a float value or a python list/tuple of float threshold value in  0  1   a threshold be compare with prediction value to determine the truth value of predictions  i e   above the threshold be true  below be false   one metric value be generate for each threshold value  if neither thresholds nor top k be set  the default be to calculate recall with thresholds 0 5  ) ( top k :  optional  unset by default  an int value specify the top k predictions to consider when calculate recall  ) ( class id :  optional  integer class id for which we want binary metrics  this must be in the half open interval  0  num class   where num class be the last dimension of predictions  ) ( name :  optional  string name of the metric instance  ) ( dtype :  optional  data type of the metric result  )

( specificity : a scalar value in range  0  1   ) ( num thresholds :  optional  default to 200  the number of thresholds to use for match the give specificity  ) ( class id :  optional  integer class id for which we want binary metrics  this must be in the half open interval  0  num class   where num class be the last dimension of predictions  ) ( name :  optional  string name of the metric instance  ) ( dtype :  optional  data type of the metric result  )
( name :  optional  string name of the metric instance  ) ( dtype :  optional  data type of the metric result  )
( name :  optional  string name of the metric instance  ) ( dtype :  optional  data type of the metric result  ) ( from logits :  optional  whether output be expect to be a logits tensor  by default  we consider that output encode a probability distribution  ) ( axis :  optional  default to  1  the dimension along which the metric be compute  )
( k :  optional  number of top elements to look at for compute accuracy  default to 5  ) ( name :  optional  string name of the metric instance  ) ( dtype :  optional  data type of the metric result  )
( sensitivity : a scalar value in range  0  1   ) ( num thresholds :  optional  default to 200  the number of thresholds to use for match the give sensitivity  ) ( class id :  optional  integer class id for which we want binary metrics  this must be in the half open interval  0  num class   where num class be the last dimension of predictions  ) ( name :  optional  string name of the metric instance  ) ( dtype :  optional  data type of the metric result  )
( name :  optional  string name of the metric instance  ) ( dtype :  optional  data type of the metric result  )
( name :  optional  string name of the metric instance  ) ( dtype :  optional  data type of the metric result  )
( k :  optional  number of top elements to look at for compute accuracy  default to 5  ) ( name :  optional  string name of the metric instance  ) ( dtype :  optional  data type of the metric result  )
( thresholds :  optional  default to 0 5  a float value or a python list/tuple of float threshold value in  0  1   a threshold be compare with prediction value to determine the truth value of predictions  i e   above the threshold be true  below be false   one metric value be generate for each threshold value  ) ( name :  optional  string name of the metric instance  ) ( dtype :  optional  data type of the metric result  )
( thresholds :  optional  default to 0 5  a float value or a python list/tuple of float threshold value in  0  1   a threshold be compare with prediction value to determine the truth value of predictions  i e   above the threshold be true  below be false   one metric value be generate for each threshold value  ) ( name :  optional  string name of the metric instance  ) ( dtype :  optional  data type of the metric result  )
( y true : grind truth value  shape    batch size  d0     dn   ) ( y pred : the predict value  shape    batch size  d0     dn   ) ( threshold :  optional  float represent the threshold for decide whether prediction value be 1 or 0  )
( y true : grind truth value  shape    batch size  d0     dn   ) ( y pred : the predict value  shape    batch size  d0     dn   ) ( from logits : whether y pred be expect to be a logits tensor  by default  we assume that y pred encode a probability distribution  ) ( label smoothing : float in  0  1   if > 0 then smooth the label by squeeze them towards 0 5 that be  use 1    0 5   label smooth for the target class and 0 5   label smooth for the non target class  ) ( axis : the axis along which the mean be compute  default to  1  )
( y true : one hot grind truth value  ) ( y pred : the prediction value  )
( y true : tensor of one hot true target  ) ( y pred : tensor of predict target  ) ( from logits : whether y pred be expect to be a logits tensor  by default  we assume that y pred encode a probability distribution  ) ( label smoothing : float in  0  1   if > 0 then smooth the label  for example  if 0 1  use 0 1 / num class for non target label and 0 9   0 1 / num class for target label  ) ( axis : default to  1  the dimension along which the entropy be compute  )
( config : metric configuration  ) ( custom objects : optional dictionary map name  string  to custom object  class and function  to be consider during deserialization  )
( identifier : a metric identifier  one of none or string name of a metric function/class or metric configuration dictionary or a metric function or a metric class instance )
( y true : the grind truth value  y true value be expect to be  1 or 1  if binary  0 or 1  label be provide they will be convert to  1 or 1  shape    batch size  d0     dn   ) ( y pred : the predict value  shape    batch size  d0     dn   )
( y true : grind truth value  shape    batch size  d0     dn   ) ( y pred : the predict value  shape    batch size  d0     dn   )
( y true : grind truth value  shape    batch size  d0     dn   ) ( y pred : the predict value  shape    batch size  d0     dn   )
( y true : grind truth value  shape    batch size  d0     dn   ) ( y pred : the predict value  shape    batch size  d0     dn   )
( y true : grind truth value  shape    batch size  d0     dn   ) ( y pred : the predict value  shape    batch size  d0     dn   )
( y true : grind truth value  shape    batch size  d0     dn   ) ( y pred : the predict value  shape    batch size  d0     dn   )
( metric : a keras metric instance or a metric function  )
( y true : integer grind truth value  ) ( y pred : the prediction value  )
( y true : grind truth value  ) ( y pred : the predict value  ) ( from logits : whether y pred be expect to be a logits tensor  by default  we assume that y pred encode a probability distribution  ) ( axis : default to  1  the dimension along which the entropy be compute  )
( y true : tensor of true target  ) ( y pred : tensor of predict target  ) ( k :  optional  number of top elements to look at for compute accuracy  default to 5  )
( y true : the grind truth value  y true value be expect to be  1 or 1  if binary  0 or 1  label be provide we will convert them to  1 or 1  shape    batch size  d0     dn   ) ( y pred : the predict value  shape    batch size  d0     dn   )
( y true : the grind truth value  ) ( y pred : the prediction value  ) ( k :  optional  number of top elements to look at for compute accuracy  default to 5  )
( model : instance of model  could be a functional model or a sequential model   ) ( input tensors : optional list of input tensors or inputlayer object to build the model upon  if not provide  new input object will be create  ) ( clone function : callable to be use to clone each layer in the target model  except inputlayer instance   it take as argument the layer instance to be clone  and return the correspond layer instance to be use in the model copy  if unspecified  this callable default to the follow serialization/deserialization function  lambda layer  layer   class   from config layer get config     by pass a custom callable  you can customize your copy of the model  e g  by wrap certain layer of interest  you might want to replace all lstm instance with equivalent bidirectional lstm       instance  for example   )
( filepath : one of the follow   string or pathlib path object  path to the save model h5py file object from which to load the model ) ( custom objects : optional dictionary map name  string  to custom class or function to be consider during deserialization  ) ( compile : boolean  whether to compile the model after load  ) ( options : optional tf save model loadoptions object that specify options for load from savedmodel  )
( config : configuration dictionary  ) ( custom objects : optional dictionary map name  string  to custom class or function to be consider during deserialization  )
( json string : json string encode a model configuration  ) ( custom objects : optional dictionary map name  string  to custom class or function to be consider during deserialization  )
( yaml string : yaml string or open file encode a model configuration  ) ( custom objects : optional dictionary map name  string  to custom class or function to be consider during deserialization  )
( model : keras model instance to be save  ) ( filepath : one of the follow   string or pathlib path object  path where to save the model h5py file object where to save the model ) ( overwrite : whether we should overwrite any exist model at the target location  or instead ask the user with a manual prompt  ) ( include optimizer : if true  save optimizer's state together  ) ( save format : either 'tf' or 'h5'  indicate whether to save the model to tensorflow savedmodel or hdf5  default to 'tf' in tf 2 x  and 'h5' in tf 1 x  ) ( signatures : signatures to save with the savedmodel  applicable to the 'tf' format only  please see the signatures argument in tf save model save for detail  ) ( options :  only apply to savedmodel format  tf save model saveoptions object that specify options for save to savedmodel  ) ( save traces :  only apply to savedmodel format  when enable  the savedmodel will store the function trace for each layer  this can be disable  so that only the configs of each layer be store  default to true  disable this will decrease serialization time and reduce file size  but it require that all custom layers/models implement a get config   method  )
( learning rate : initial value for the learn rate  either a float point value  or a tf keras optimizers schedule learningrateschedule instance  default to 0 001  note that adadelta tend to benefit from higher initial learn rate value compare to other optimizers  to match the exact form in the original paper  use 1 0  ) ( rho : a tensor or a float point value  the decay rate  ) ( epsilon : small float point value use to maintain numerical stability  ) ( name : optional name prefix for the operations create when apply gradients   default to "adadelta"  ) (   kwargs : keyword arguments  allow arguments be clipvalue  clipnorm  global clipnorm  if clipvalue  float  be set  the gradient of each weight be clip to be no higher than this value  if clipnorm  float  be set  the gradient of each weight be individually clip so that its norm be no higher than this value  if global clipnorm  float  be set the gradient of all weight be clip so that their global norm be no higher than this value  )
( learning rate : initial value for the learn rate  either a float point value  or a tf keras optimizers schedule learningrateschedule instance  default to 0 001  note that adagrad tend to benefit from higher initial learn rate value compare to other optimizers  to match the exact form in the original paper  use 1 0  ) ( initial accumulator value : float point value  start value for the accumulators  per parameter momentum value   must be non negative  ) ( epsilon : small float point value use to maintain numerical stability  ) ( name : optional name prefix for the operations create when apply gradients   default to "adagrad"  ) (   kwargs : keyword arguments  allow arguments be clipvalue  clipnorm  global clipnorm  if clipvalue  float  be set  the gradient of each weight be clip to be no higher than this value  if clipnorm  float  be set  the gradient of each weight be individually clip so that its norm be no higher than this value  if global clipnorm  float  be set the gradient of all weight be clip so that their global norm be no higher than this value   )
( learning rate : a tensor  float point value  or a schedule that be a tf keras optimizers schedule learningrateschedule  or a callable that take no arguments and return the actual value to use  the learn rate  default to 0 001  ) ( beta 1 : a float value or a constant float tensor  or a callable that take no arguments and return the actual value to use  the exponential decay rate for the 1st moment estimate  default to 0 9  ) ( beta 2 : a float value or a constant float tensor  or a callable that take no arguments and return the actual value to use  the exponential decay rate for the 2nd moment estimate  default to 0 999  ) ( epsilon : a small constant for numerical stability  this epsilon be "epsilon hat" in the kingma and ba paper  in the formula just before section 2 1   not the epsilon in algorithm 1 of the paper  default to 1e 7  ) ( amsgrad : boolean  whether to apply amsgrad variant of this algorithm from the paper "on the convergence of adam and beyond"  default to false  ) ( name : optional name for the operations create when apply gradients  default to "adam"  ) (   kwargs : keyword arguments  allow arguments be clipvalue  clipnorm  global clipnorm  if clipvalue  float  be set  the gradient of each weight be clip to be no higher than this value  if clipnorm  float  be set  the gradient of each weight be individually clip so that its norm be no higher than this value  if global clipnorm  float  be set the gradient of all weight be clip so that their global norm be no higher than this value  )
( learning rate : a tensor  float point value  or a schedule that be a tf keras optimizers schedule learningrateschedule  the learn rate  ) ( beta 1 : a float value or a constant float tensor  the exponential decay rate for the 1st moment estimate  ) ( beta 2 : a float value or a constant float tensor  the exponential decay rate for the exponentially weight infinity norm  ) ( epsilon : a small constant for numerical stability  ) ( name : optional name for the operations create when apply gradients  default to "adamax"  ) (   kwargs : keyword arguments  allow arguments be clipvalue  clipnorm  global clipnorm  if clipvalue  float  be set  the gradient of each weight be clip to be no higher than this value  if clipnorm  float  be set  the gradient of each weight be individually clip so that its norm be no higher than this value  if global clipnorm  float  be set the gradient of all weight be clip so that their global norm be no higher than this value  )
( learning rate : a tensor  float point value  or a schedule that be a tf keras optimizers schedule learningrateschedule  the learn rate  ) ( learning rate power : a float value  must be less or equal to zero  control how the learn rate decrease during train  use zero for a fix learn rate  ) ( initial accumulator value : the start value for accumulators  only zero or positive value be allow  ) ( l1 regularization strength : a float value  must be greater than or equal to zero  default to 0 0  ) ( l2 regularization strength : a float value  must be greater than or equal to zero  default to 0 0  ) ( name : optional name prefix for the operations create when apply gradients   default to "ftrl"  ) ( l2 shrinkage regularization strength : a float value  must be greater than or equal to zero  this differ from l2 above in that the l2 above be a stabilization penalty  whereas this l2 shrinkage be a magnitude penalty  when input be sparse shrinkage will only happen on the active weight  ) ( beta : a float value  represent the beta value from the paper  default to 0 0  ) (   kwargs : keyword arguments  allow arguments be clipvalue  clipnorm  global clipnorm  if clipvalue  float  be set  the gradient of each weight be clip to be no higher than this value  if clipnorm  float  be set  the gradient of each weight be individually clip so that its norm be no higher than this value  if global clipnorm  float  be set the gradient of all weight be clip so that their global norm be no higher than this value  )
( learning rate : a tensor or a float point value   the learn rate  ) ( beta 1 : a float value or a constant float tensor  the exponential decay rate for the 1st moment estimate  ) ( beta 2 : a float value or a constant float tensor  the exponential decay rate for the exponentially weight infinity norm  ) ( epsilon : a small constant for numerical stability  ) ( name : optional name for the operations create when apply gradients  default to "nadam"  ) (   kwargs : keyword arguments  allow arguments be clipvalue  clipnorm  global clipnorm  if clipvalue  float  be set  the gradient of each weight be clip to be no higher than this value  if clipnorm  float  be set  the gradient of each weight be individually clip so that its norm be no higher than this value  if global clipnorm  float  be set the gradient of all weight be clip so that their global norm be no higher than this value  )
( name : string  the name to use for momentum accumulator weight create by the optimizer  ) ( gradient aggregator : the function to use to aggregate gradients across devices  when use tf distribute strategy   if none  default to sum the gradients across devices  the function should accept and return a list of  gradient  variable  tuples  ) ( gradient transformers : optional  list of function to use to transform gradients before apply update to variables  the function be apply after gradient aggregator  the function should accept and return a list of  gradient  variable  tuples  ) (   kwargs : keyword arguments  allow arguments be clipvalue  clipnorm  global clipnorm  if clipvalue  float  be set  the gradient of each weight be clip to be no higher than this value  if clipnorm  float  be set  the gradient of each weight be individually clip so that its norm be no higher than this value  if global clipnorm  float  be set the gradient of all weight be clip so that their global norm be no higher than this value  )
( learning rate : a tensor  float point value  or a schedule that be a tf keras optimizers schedule learningrateschedule  or a callable that take no arguments and return the actual value to use  the learn rate  default to 0 001  ) ( rho : discount factor for the history/coming gradient  default to 0 9  ) ( momentum : a scalar or a scalar tensor  default to 0 0  ) ( epsilon : a small constant for numerical stability  this epsilon be "epsilon hat" in the kingma and ba paper  in the formula just before section 2 1   not the epsilon in algorithm 1 of the paper  default to 1e 7  ) ( centered : boolean  if true  gradients be normalize by the estimate variance of the gradient  if false  by the uncentered second moment  set this to true may help with train  but be slightly more expensive in term of computation and memory  default to false  ) ( name : optional name prefix for the operations create when apply gradients  default to "rmsprop"  ) (   kwargs : keyword arguments  allow arguments be clipvalue  clipnorm  global clipnorm  if clipvalue  float  be set  the gradient of each weight be clip to be no higher than this value  if clipnorm  float  be set  the gradient of each weight be individually clip so that its norm be no higher than this value  if global clipnorm  float  be set the gradient of all weight be clip so that their global norm be no higher than this value  )
( learning rate : a tensor  float point value  or a schedule that be a tf keras optimizers schedule learningrateschedule  or a callable that take no arguments and return the actual value to use  the learn rate  default to 0 01  ) ( momentum : float hyperparameter >  0 that accelerate gradient descent in the relevant direction and dampen oscillations  default to 0  i e   vanilla gradient descent  ) ( nesterov : boolean  whether to apply nesterov momentum  default to false  ) ( name : optional name prefix for the operations create when apply gradients   default to "sgd"  ) (   kwargs : keyword arguments  allow arguments be clipvalue  clipnorm  global clipnorm  if clipvalue  float  be set  the gradient of each weight be clip to be no higher than this value  if clipnorm  float  be set  the gradient of each weight be individually clip so that its norm be no higher than this value  if global clipnorm  float  be set the gradient of all weight be clip so that their global norm be no higher than this value  )
( config : optimizer configuration dictionary  ) ( custom objects : optional dictionary map name  string  to custom object  class and function  to be consider during deserialization  )
( identifier : optimizer identifier  one of  string  name of an optimizer dictionary  configuration dictionary    keras optimizer instance  it will be return unchanged     tensorflow optimizer instance  it will be wrap as a keras optimizer   )
( optimizer : an optimizer instance to serialize  )
( initial learning rate : a scalar float32 or float64 tensor or a python number   the initial learn rate  ) ( decay steps : a scalar int32 or int64 tensor or a python number  must be positive   see the decay computation above  ) ( decay rate : a scalar float32 or float64 tensor or a python number   the decay rate  ) ( staircase : boolean   if true decay the learn rate at discrete intervals ) ( name : string   optional name of the operation   default to 'exponentialdecay'  )
( initial learning rate : a scalar float32 or float64 tensor or a python number   the initial learn rate  ) ( decay steps : how often to apply decay  ) ( decay rate : a python number   the decay rate  ) ( staircase : whether to apply decay in a discrete staircase  as oppose to continuous  fashion  ) ( name : string   optional name of the operation   default to 'inversetimedecay'  )

( boundaries : a list of tensors or ints or float with strictly increase entries  and with all elements have the same type as the optimizer step  ) ( values : a list of tensors or float or ints that specify the value for the intervals define by boundaries  it should have one more element than boundaries  and all elements should have the same type  ) ( name : a string  optional name of the operation  default to 'piecewiseconstant'  )
( initial learning rate : a scalar float32 or float64 tensor or a python number   the initial learn rate  ) ( decay steps : a scalar int32 or int64 tensor or a python number  must be positive   see the decay computation above  ) ( end learning rate : a scalar float32 or float64 tensor or a python number   the minimal end learn rate  ) ( power : a scalar float32 or float64 tensor or a python number   the power of the polynomial  default to linear  1 0  ) ( cycle : a boolean  whether or not it should cycle beyond decay step  ) ( name : string   optional name of the operation  default to 'polynomialdecay'  )
( config : the serialize form of the learningrateschedule  dictionary of the form  'class name'  str  'config'  dict   ) ( custom objects : a dictionary map class name  or function name  of custom  non keras  object to class/functions  )
( learning rate schedule : the learningrateschedule object to serialize  )
( directory : path to the directory to read image from  each subdirectory in this directory will be consider to contain image from one class  or alternatively you could specify class subdirectories via the class argument  ) ( image data generator : instance of imagedatagenerator to use for random transformations and normalization  ) ( target size : tuple of integers  dimension to resize input image to  ) ( color mode : one of "rgb"  "rgba"  "grayscale"  color mode to read image  ) ( classes : optional list of string  name of subdirectories contain image from each class  e g   "dogs"  "cats"    it will be compute automatically if not set  ) ( class mode : mode for yield the target   "binary"  binary target  if there be only two class   "categorical"  categorical target  "sparse"  integer target  "input"  target be image identical to input image  mainly use to work with autoencoders   none  no target get yield  only input image be yield   ) ( batch size : integer  size of a batch  ) ( shuffle : boolean  whether to shuffle the data between epochs  ) ( seed : random seed for data shuffle  ) ( data format : string  one of channel first  channel last  ) ( save to dir : optional directory where to save the picture be yield  in a viewable format  this be useful for visualize the random transformations be apply  for debug purpose  ) ( save prefix : string prefix to use for save sample image  if save to dir be set   ) ( save format : format to use for save sample image  if save to dir be set   ) ( subset : subset of data  "training" or "validation"  if validation split be set in imagedatagenerator  ) ( interpolation : interpolation method use to resample the image if the target size be different from that of the load image  support methods be "nearest"  "bilinear"  and "bicubic"  if pil version 1 1 3 or newer be instal  "lanczos" be also support  if pil version 3 4 0 or newer be instal  "box" and "hamming" be also support  by default  "nearest" be use  ) ( keep aspect ratio : boolean  whether to resize image to a target size without aspect ratio distortion  the image be crop in the center with target aspect ratio before resize  ) ( dtype : dtype to use for generate array  )
( featurewise center : boolean  set input mean to 0 over the dataset  feature wise  ) ( samplewise center : boolean  set each sample mean to 0  ) ( featurewise std normalization : boolean  divide input by std of the dataset  feature wise  ) ( samplewise std normalization : boolean  divide each input by its std  ) ( zca epsilon : epsilon for zca whiten  default be 1e 6  ) ( zca whitening : boolean  apply zca whiten  ) ( rotation range : int  degree range for random rotations  ) ( width shift range : float  1 d array like or int  float  fraction of total width  if < 1  or pixels if >  1  1 d array like  random elements from the array  int  integer number of pixels from interval   width shift range   width shift range    with width shift range 2 possible value be integers   1  0   1   same as with width shift range   1  0   1   while with width shift range 1 0 possible value be float in the interval   1 0   1 0   ) ( height shift range : float  1 d array like or int float  fraction of total height  if < 1  or pixels if >  1  1 d array like  random elements from the array  int  integer number of pixels from interval   height shift range   height shift range    with height shift range 2 possible value be integers   1  0   1   same as with height shift range   1  0   1   while with height shift range 1 0 possible value be float in the interval   1 0   1 0   ) ( brightness range : tuple or list of two float  range for pick a brightness shift value from  ) ( shear range : float  shear intensity  shear angle in counter clockwise direction in degrees  ) ( zoom range : float or  lower  upper   range for random zoom  if a float   lower  upper     1 zoom range  1 zoom range   ) ( channel shift range : float  range for random channel shift  ) ( fill mode : one of  "constant"  "nearest"  "reflect" or "wrap"   default be 'nearest'  point outside the boundaries of the input be fill accord to the give mode   'constant'  kkkkkkkk abcd kkkkkkkk  cval k  'nearest'   aaaaaaaa abcd dddddddd 'reflect'   abcddcba abcd dcbaabcd 'wrap'   abcdabcd abcd abcdabcd ) ( cval : float or int  value use for point outside the boundaries when fill mode   "constant"  ) ( horizontal flip : boolean  randomly flip input horizontally  ) ( vertical flip : boolean  randomly flip input vertically  ) ( rescale : rescale factor  default to none  if none or 0  no rescale be apply  otherwise we multiply the data by the value provide  after apply all other transformations   ) ( preprocessing function : function that will be apply on each input  the function will run after the image be resize and augment  the function should take one argument  one image  numpy tensor with rank 3   and should output a numpy tensor with the same shape  ) ( data format : image data format  either "channels first" or "channels last"  "channels last" mode mean that the image should have shape  sample  height  width  channel   "channels first" mode mean that the image should have shape  sample  channel  height  width   it default to the image data format value find in your keras config file at ~/ keras/keras json  if you never set it  then it will be "channels last"  ) ( validation split : float  fraction of image reserve for validation  strictly between 0 and 1   ) ( dtype : dtype to use for the generate array  )
( n : integer  total number of sample in the dataset to loop over  ) ( batch size : integer  size of a batch  ) ( shuffle : boolean  whether to shuffle the data between epochs  ) ( seed : random seed for data shuffle  )
( x : numpy array of input data or tuple  if tuple  the second elements be either another numpy array or a list of numpy array  each of which get pass through as an output without any modifications  ) ( y : numpy array of target data  ) ( image data generator : instance of imagedatagenerator to use for random transformations and normalization  ) ( batch size : integer  size of a batch  ) ( shuffle : boolean  whether to shuffle the data between epochs  ) ( sample weight : numpy array of sample weight  ) ( seed : random seed for data shuffle  ) ( data format : string  one of channel first  channel last  ) ( save to dir : optional directory where to save the picture be yield  in a viewable format  this be useful for visualize the random transformations be apply  for debug purpose  ) ( save prefix : string prefix to use for save sample image  if save to dir be set   ) ( save format : format to use for save sample image  if save to dir be set   ) ( subset : subset of data  "training" or "validation"  if validation split be set in imagedatagenerator  ) ( ignore class split : boolean  default  false   ignore difference in number of class in label across train and validation split  useful for non classification task  ) ( dtype : dtype to use for the generate array  )
( x : 3d numpy array   a 2d image with one or more channel  ) ( theta : rotation angle in degrees  ) ( tx : width shift  ) ( ty : heigh shift  ) ( shear : shear angle in degrees  ) ( zx : zoom in x direction  ) ( zy : zoom in y direction ) ( row axis : index of axis for row  aka y axis  in the input image  direction  leave to right  ) ( col axis : index of axis for columns  aka x axis  in the input image  direction  top to bottom  ) ( channel axis : index of axis for channel in the input image  ) ( fill mode : point outside the boundaries of the input be fill accord to the give mode  one of  'constant'  'nearest'  'reflect'  'wrap'    ) ( cval : value use for point outside the boundaries of the input if mode 'constant'  ) ( order : int  order of interpolation )
( x : input tensor  must be 3d  ) ( brightness : float  the new brightness value  ) ( scale : whether to rescale the image such that minimum and maximum value be 0 and 255 respectively  default  true  )
( x : input tensor  must be 3d  ) ( intensity : transformation intensity  ) ( channel axis : index of axis for channel in the input tensor  )
( x : input tensor  must be 3d  ) ( brightness range : tuple of float  brightness range  ) ( scale : whether to rescale the image such that minimum and maximum value be 0 and 255 respectively  default  true  )
( x : input tensor  must be 3d  ) ( intensity range : transformation intensity  ) ( channel axis : index of axis for channel in the input tensor  )
( x : input tensor  must be 3d  ) ( rg : rotation range  in degrees  ) ( row axis : index of axis for row in the input tensor  ) ( col axis : index of axis for columns in the input tensor  ) ( channel axis : index of axis for channel in the input tensor  ) ( fill mode : point outside the boundaries of the input be fill accord to the give mode  one of  'constant'  'nearest'  'reflect'  'wrap'    ) ( cval : value use for point outside the boundaries of the input if mode 'constant'  ) ( interpolation order : int  order of spline interpolation  see ndimage interpolation affine transform )
( x : input tensor  must be 3d  ) ( intensity : transformation intensity in degrees  ) ( row axis : index of axis for row in the input tensor  ) ( col axis : index of axis for columns in the input tensor  ) ( channel axis : index of axis for channel in the input tensor  ) ( fill mode : point outside the boundaries of the input be fill accord to the give mode  one of  'constant'  'nearest'  'reflect'  'wrap'    ) ( cval : value use for point outside the boundaries of the input if mode 'constant'  ) ( interpolation order : int  order of spline interpolation  see ndimage interpolation affine transform )
( x : input tensor  must be 3d  ) ( wrg : width shift range  as a float fraction of the width  ) ( hrg : height shift range  as a float fraction of the height  ) ( row axis : index of axis for row in the input tensor  ) ( col axis : index of axis for columns in the input tensor  ) ( channel axis : index of axis for channel in the input tensor  ) ( fill mode : point outside the boundaries of the input be fill accord to the give mode  one of  'constant'  'nearest'  'reflect'  'wrap'    ) ( cval : value use for point outside the boundaries of the input if mode 'constant'  ) ( interpolation order : int  order of spline interpolation  see ndimage interpolation affine transform )
( x : input tensor  must be 3d  ) ( zoom range : tuple of float  zoom range for width and height  ) ( row axis : index of axis for row in the input tensor  ) ( col axis : index of axis for columns in the input tensor  ) ( channel axis : index of axis for channel in the input tensor  ) ( fill mode : point outside the boundaries of the input be fill accord to the give mode  one of  'constant'  'nearest'  'reflect'  'wrap'    ) ( cval : value use for point outside the boundaries of the input if mode 'constant'  ) ( interpolation order : int  order of spline interpolation  see ndimage interpolation affine transform )

( size : int  number of possible word to sample  ) ( sampling factor : the sample factor in the word2vec formula  )
( sequence : a word sequence  sentence   encode as a list of word indices  integers   if use a sample table  word indices be expect to match the rank of the word in a reference dataset  e g  10 would encode the 10 th most frequently occur token   note that index 0 be expect to be a non word and will be skip  ) ( vocabulary size : int  maximum possible word index   1 ) ( window size : int  size of sample windows  technically half window   the window of a word w i will be  i   window size  i   window size 1   ) ( negative samples : float >  0  0 for no negative  i e  random  sample  1 for same number as positive sample  ) ( shuffle : whether to shuffle the word couple before return them  ) ( categorical : bool  if false  label will be integers  eg   0  1  1        if true  label will be categorical  e g    1 0   0 1   0 1        ) ( sampling table : 1d array of size vocabulary size where the entry i encode the probability to sample a word of rank i  ) ( seed : random seed  )
( num words : the maximum number of word to keep  base on word frequency  only the most common num word 1 word will be keep  ) ( filters : a string where each element be a character that will be filter from the texts  the default be all punctuation  plus tabs and line break  minus the ' character  ) ( lower : boolean  whether to convert the texts to lowercase  ) ( split : str  separator for word split  ) ( char level : if true  every character will be treat as a token  ) ( oov token : if give  it will be add to word index and use to replace out of vocabulary word during text to sequence call ) ( analyzer : function  custom analyzer to split the text  the default analyzer be text to word sequence )
( text : input text  string   ) ( n : dimension of the hash space  ) ( hash function : default to python hash function  can be 'md5' or any function that take in input a string and return a int  note that 'hash' be not a stable hash function  so it be not consistent across different run  while 'md5' be a stable hash function  ) ( filters : list  or concatenation  of character to filter out  such as punctuation  default   "           /  < >?  \\   `   ~\\t\\n  include basic punctuation  tabs  and newlines  ) ( lower : boolean  whether to set the text to lowercase  ) ( split : str  separator for word split  ) ( analyzer : function  custom analyzer to split the text )
( input text : input text  string   ) ( n : int  size of vocabulary  ) ( filters : list  or concatenation  of character to filter out  such as punctuation  default  ' "           /  < >?  \   `   ~\t\n  include basic punctuation  tabs  and newlines  ) ( lower : boolean  whether to set the text to lowercase  ) ( split : str  separator for word split  ) ( analyzer : function  custom analyzer to split the text )
( input text : input text  string   ) ( filters : list  or concatenation  of character to filter out  such as punctuation  default  ' "           /  < >?  \\   `   ~\\t\\n'    include basic punctuation  tabs  and newlines  ) ( lower : boolean  whether to convert the input to lowercase  ) ( split : str  separator for word split  )
( json string : json string encode a tokenizer configuration  )



( l1 : float  l1 regularization factor  ) ( l2 : float  l2 regularization factor  )
( generator : a generator function which yield data ) ( use multiprocessing : use multiprocessing if true  otherwise thread ) ( random seed : initial seed for workers  will be incremented by one for each worker  )
( sequence : a tf keras utils data utils sequence object  ) ( use multiprocessing : use multiprocessing if true  otherwise thread ) ( shuffle : whether to shuffle the data at the begin of each epoch )
( target : total number of step expect  none if unknown  ) ( width : progress bar width on screen  ) ( verbose : verbosity mode  0  silent   1  verbose   2  semi verbose  ) ( stateful metrics : iterable of string name of metrics that should not be average over time  metrics in this list will be display as be  all others will be average by the progbar before display  ) ( interval : minimum visual progress update interval  in second   ) ( unit name : display name for step count  usually "step" or "sample"   )


(  args : dictionary or dictionaries of  name  object  pair  )
( identifier : the serialize form of the object  ) ( module objects : a dictionary of build in object to look the name up in  generally  module object be provide by midlevel library implementers  ) ( custom objects : a dictionary of custom object to look the name up in  generally  custom object be provide by the end user  ) ( printable module name : a human readable string represent the type of the object  print in case of exception  )

( fname : name of the file  if an absolute path /path/to/file txt be specify the file will be save at that location  if none  the name of the file at origin will be use  ) ( origin : original url of the file  ) ( untar : deprecate in favor of extract argument  boolean  whether the file should be decompress ) ( md5 hash : deprecate in favor of file hash argument  md5 hash of the file for verification ) ( file hash : the expect hash string of the file after download  the sha256 and md5 hash algorithms be both support  ) ( cache subdir : subdirectory under the keras cache dir where the file be save  if an absolute path /path/to/folder be specify the file will be save at that location  ) ( hash algorithm : select the hash algorithm to verify the file  options be 'md5'  'sha256'  and 'auto'  the default 'auto' detect the hash algorithm in use  ) ( extract : true try extract the file as an archive  like tar or zip  ) ( archive format : archive format to try for extract the file  options be 'auto'  'tar'  'zip'  and none  'tar' include tar  tar gz  and tar bz file  the default 'auto' correspond to  'tar'  'zip'   none or an empty list will return no match find  ) ( cache dir : location to store cache file  when none it default to the default directory ~/ keras/  )
( tensor : the tensor to start from  ) ( layer : origin layer of the tensor  will be determine via tensor  keras history if not provide  ) ( node index : origin node index of the tensor  )
( model : a keras model instance  ) ( show shapes : whether to display shape information  ) ( show dtype : whether to display layer dtypes  ) ( show layer names : whether to display layer name  ) ( rankdir : rankdir argument pass to pydot  a string specify the format of the plot  'tb' create a vertical plot  'lr' create a horizontal plot  ) ( expand nested : whether to expand nest model into cluster  ) ( dpi : dot per inch  ) ( subgraph : whether to return a pydot cluster instance  ) ( layer range : input of list contain two str items  which be the start layer name and end layer name  both inclusive  indicate the range of layer for which the pydot dot will be generate  it also accept regex pattern instead of exact name  in such case  start predicate will be the first element it match to layer range 0  and the end predicate will be the last element it match to layer range 1   by default none which consider all layer of model  note that you must pass range such that the resultant subgraph must be complete  ) ( show layer activations : display layer activations  only for layer that have an activation property   )
( x : numpy array to normalize  ) ( axis : axis along which to normalize  ) ( order : normalization order  e g  order 2 for l2 norm   )
( model : a keras model instance ) ( to file : file name of the plot image  ) ( show shapes : whether to display shape information  ) ( show dtype : whether to display layer dtypes  ) ( show layer names : whether to display layer name  ) ( rankdir : rankdir argument pass to pydot  a string specify the format of the plot  'tb' create a vertical   plot  'lr' create a horizontal plot  ) ( expand nested : whether to expand nest model into cluster  ) ( dpi : dot per inch  ) ( layer range : input of list contain two str items  which be the start layer name and end layer name  both inclusive  indicate the range of layer for which the plot will be generate  it also accept regex pattern instead of exact name  in such case  start predicate will be the first element it match to layer range 0  and the end predicate will be the last element it match to layer range 1   by default none which consider all layer of model  note that you must pass range such that the resultant subgraph must be complete  ) ( show layer activations : display layer activations  only for layer that have an activation property   )
( package : the package that this class belong to  ) ( name : the name to serialize this class under in this package  if none  the class' name will be use  )
( instance : the object to serialize  )
( y : array like with class value to be convert into a matrix  integers from 0 to num class   1   ) ( num classes : total number of class  if none  this would be infer as max y    1  ) ( dtype : the data type expect by the input  default  'float32'  )


( logdir : a string specify the directory in which to write an event file  ) ( max queue : the largest number of summaries to keep in a queue  will flush once the queue get bigger than this  default to 10  ) ( flush millis : the largest interval between flush  default to 120 000  ) ( filename suffix : optional suffix for the event file name  default to  v2  ) ( name : a name for the op that create the writer  ) ( experimental trackable : a boolean that control whether the return writer will be a trackableresource  which make it compatible with savedmodel when use as a tf module property  )

( writer : the tf summary summarywriter to flush  if none  the current default writer will be use instead  if there be no current writer  this return tf no op  ) ( name : ignore legacy argument for a name for the operation  )


( condition : can be true  false  a bool tensor  or a callable provide such  )


( name : a name for the summary to be write  ) ( step : explicit int64 castable monotonic step value for this summary  if omit  this default to tf summary experimental get step    which must not be none  ) ( profiler outdir : output directory for profiler  it be require when profiler be enable when trace be start  otherwise  it be ignore  )

( graph : if true  enable collection of execute graph  it include ones from tf function invocation and ones from the legacy graph mode  the default be true  ) ( profiler : if true  enable the advance profiler  enable profiler implicitly enable the graph collection  the profiler may incur a high memory overhead  the default be false  )
( tag : string tag use to identify the summary  e g  in tensorboard   usually generate with tf summary summary scope ) ( tensor : the tensor hold the summary data to write or a callable that return this tensor  if a callable be pass  it will only be call when a default summarywriter exist and the record condition specify by record if   be meet  ) ( step : explicit int64 castable monotonic step value for this summary  if omit  this default to tf summary experimental get step    which must not be none  ) ( metadata : optional summarymetadata  as a proto or serialize bytes ) ( name : optional string name for this op  )
( start :  optional   the start value for the counter  default to 0  ) ( step :  optional   the step size for the counter  default to 1  ) ( dtype :  optional   the data type for counter elements  default to tf int64  )
( filenames : a tf string tensor contain one or more filenames  ) ( record defaults : a list of default value for the csv field  each item in the list be either a valid csv dtype  float32  float64  int32  int64  string   or a tensor object with one of the above type  one per column of csv data  with either a scalar tensor default value for the column if it be optional  or dtype or empty tensor if require  if both this and select columns be specify  these must have the same lengths  and column default be assume to be sort in order of increase column index  if both this and 'exclude cols' be specify  the sum of lengths of record default and exclude cols should equal the total number of columns in the csv file  ) ( compression type :  optional   a tf string scalar evaluate to one of ""  no compression   "zlib"  or "gzip"  default to no compression  ) ( buffer size :  optional   a tf int64 scalar denote the number of bytes to buffer while read file  default to 4mb  ) ( header :  optional   a tf bool scalar indicate whether the csv file s  have header line s  that should be skip when parse  default to false  ) ( field delim :  optional   a tf string scalar contain the delimiter character that separate field in a record  default to " "  ) ( use quote delim :  optional   a tf bool scalar  if false  treat double quotation mark as regular character inside of string field  ignore rfc 4180  section 2  bullet 5   default to true  ) ( na value :  optional   a tf string scalar indicate a value that will be treat as na/nan  ) ( select cols :  optional   a sort list of column indices to select from the input data  if specify  only this subset of columns will be parse  default to parse all columns  at most one of select cols and exclude cols can be specify  )

( element spec : the type specification of an element of this dataset  dataset   tf data dataset from tensor slice  1  2  3  dataset element spectensorspec shape     dtype tf int32  name none  for more information  read this guide  ) ( output classes : return the class of each component of an element of this dataset   deprecate  deprecate  this function be deprecate  it will be remove in a future version  instructions for update  use tf compat v1 data get output class dataset   ) ( output shapes : return the shape of each component of an element of this dataset   deprecate deprecate  this function be deprecate  it will be remove in a future version  instructions for update  use tf compat v1 data get output shape dataset   ) ( output types : return the type of each component of an element of this dataset   deprecate deprecate  this function be deprecate  it will be remove in a future version  instructions for update  use tf compat v1 data get output type dataset   )

( driver name : a 0 d tf string tensor contain the database type  currently  the only support value be 'sqlite'  ) ( data source name : a 0 d tf string tensor contain a connection string to connect to the database  ) ( query : a 0 d tf string tensor contain the sql query to execute  ) ( output types : a tuple of tf dtype object represent the type of the columns return by query  )


( datasets : a non empty list of tf data dataset object with compatible structure  ) ( choice dataset : a tf data dataset of scalar tf int64 tensors between 0 and len datasets    1  ) ( stop on empty dataset : if true  selection stop if it encounter an empty dataset  if false  it skip empty datasets  it be recommend to set it to true  otherwise  the select elements start off as the user intend  but may change as input datasets become empty  this can be difficult to detect since the dataset start off look correct  default to false for backward compatibility  )
( file pattern : list of file or pattern of file paths contain example record  see tf io gfile glob for pattern rule  ) ( batch size : an int represent the number of record to combine in a single batch  ) ( features : a dict map feature key to fixedlenfeature or varlenfeature value  see tf io parse example  ) ( reader : a function or class that can be call with a filenames tensor and  optional  reader args and return a dataset of example tensors  default to tf data tfrecorddataset  ) ( label key :  optional  a string correspond to the key label be store in tf examples  if provide  it must be one of the feature key  otherwise result in valueerror  ) ( reader args : additional arguments to pass to the reader class  ) ( num epochs : integer specify the number of time to read through the dataset  if none  cycle through the dataset forever  default to none  ) ( shuffle : a boolean  indicate whether the input should be shuffle  default to true  ) ( shuffle buffer size : buffer size of the shuffledataset  a large capacity ensure better shuffle but would increase memory usage and startup time  ) ( shuffle seed : randomization seed to use for shuffle  ) ( prefetch buffer size : number of feature batch to prefetch in order to improve performance  recommend value be the number of batch consume per train step  default to auto tune  ) ( reader num threads : number of thread use to read example record  if >1  the result will be interleave  default to 1  ) ( parser num threads : number of thread to use for parse example tensors into a dictionary of feature tensors  default to 2  ) ( sloppy ordering : if true  read performance will be improve at the cost of non deterministic order  if false  the order of elements produce be deterministic prior to shuffle  elements be still randomize if shuffle true  note that if the seed be set  then order of elements after shuffle be deterministic   default to false  ) ( drop final batch : if true  and the batch size do not evenly divide the input dataset size  the final smaller batch will be drop  default to false  )
( file pattern : list of file or pattern of file paths contain csv record  see tf io gfile glob for pattern rule  ) ( batch size : an int represent the number of record to combine in a single batch  ) ( column names : an optional list of string that correspond to the csv columns  in order  one per column of the input record  if this be not provide  infer the column name from the first row of the record  these name will be the key of the feature dict of each dataset element  ) ( column defaults : a optional list of default value for the csv field  one item per select column of the input record  each item in the list be either a valid csv dtype  float32  float64  int32  int64  or string   or a tensor with one of the aforementioned type  the tensor can either be a scalar default value  if the column be optional   or an empty tensor  if the column be require   if a dtype be provide instead of a tensor  the column be also treat as require  if this list be not provide  try to infer type base on read the first num row for inference row of file specify  and assume all columns be optional  default to 0 for numeric value and "" for string value  if both this and select columns be specify  these must have the same lengths  and column default be assume to be sort in order of increase column index  ) ( label name : a optional string correspond to the label column  if provide  the data for this column be return as a separate tensor from the feature dictionary  so that the dataset comply with the format expect by a tf estimator train or tf estimator evaluate input function  ) ( select columns : an optional list of integer indices or string column name  that specify a subset of columns of csv data to select  if column name be provide  these must correspond to name provide in column name or infer from the file header line  when this argument be specify  only a subset of csv columns will be parse and return  correspond to the columns specify  use this result in faster parse and lower memory usage  if both this and column default be specify  these must have the same lengths  and column default be assume to be sort in order of increase column index  ) ( field delim : an optional string  default to " "  char delimiter to separate field in a record  ) ( use quote delim : an optional bool  default to true  if false  treat double quotation mark as regular character inside of the string field  ) ( na value : additional string to recognize as na/nan  ) ( header : a bool that indicate whether the first row of provide csv file correspond to header line with column name  and should not be include in the data  ) ( num epochs : an int specify the number of time this dataset be repeat  if none  cycle through the dataset forever  ) ( shuffle : a bool that indicate whether the input should be shuffle  ) ( shuffle buffer size : buffer size to use for shuffle  a large buffer size ensure better shuffle  but increase memory usage and startup time  ) ( shuffle seed : randomization seed to use for shuffle  ) ( prefetch buffer size : an int specify the number of feature batch to prefetch for performance improvement  recommend value be the number of batch consume per train step  default to auto tune  ) ( num parallel reads : number of thread use to read csv record from file  if >1  the result will be interleave  default to 1  ) ( sloppy : if true  read performance will be improve at the cost of non deterministic order  if false  the order of elements produce be deterministic prior to shuffle  elements be still randomize if shuffle true  note that if the seed be set  then order of elements after shuffle be deterministic   default to false  ) ( num rows for inference : number of row of a file to use for type inference if record default be not provide  if none  read all the row of all the file  default to 100  ) ( compression type :  optional   a tf string scalar evaluate to one of ""  no compression   "zlib"  or "gzip"  default to no compression  ) ( ignore errors :  optional   if true  ignore errors with csv file parse  such as malformed data or empty line  and move on to the next valid csv record  otherwise  the dataset raise an error and stop process when encounter any invalid record  default to false  )
( map func : a function map a nest structure of tensors to another nest structure of tensors  ) ( batch size : a tf int64 scalar tf tensor  represent the number of consecutive elements of this dataset to combine in a single batch  ) ( num parallel batches :  optional   a tf int64 scalar tf tensor  represent the number of batch to create in parallel  on one hand  higher value can help mitigate the effect of stragglers  on the other hand  higher value can increase contention if cpu be scarce  ) ( drop remainder :  optional   a tf bool scalar tf tensor  represent whether the last batch should be drop in case its size be smaller than desire  the default behavior be not to drop the smaller batch  ) ( num parallel calls :  optional   a tf int32 scalar tf tensor  represent the number of elements to process in parallel  if not specify  batch size   num parallel batch elements will be process in parallel  if the value tf data autotune be use  then the number of parallel call be set dynamically base on available cpu  )
( datasets : a non empty list of tf data dataset object with compatible structure  ) ( weights :  optional   a list or tensor of len datasets  float point value where weight i  represent the probability to sample from datasets i   or a tf data dataset object where each element be such a list  default to a uniform distribution across datasets  ) ( seed :  optional   a tf int64 scalar tf tensor  represent the random seed that will be use to create the distribution  see tf random set seed for behavior  ) ( stop on empty dataset : if true  sample stop if it encounter an empty dataset  if false  it skip empty datasets  it be recommend to set it to true  otherwise  the distribution of sample start off as the user intend  but may change as input datasets become empty  this can be difficult to detect since the dataset start off look correct  default to false for backward compatibility  )
( cluster resolver : return the cluster resolver associate with this strategy  in general  when use a multi worker tf distribute strategy such as tf distribute experimental multiworkermirroredstrategy or tf distribute tpustrategy    there be a tf distribute cluster resolver clusterresolver associate with the strategy use  and such an instance be return by this property  strategies that intend to have an associate tf distribute cluster resolver clusterresolver must set the relevant attribute  or override this property  otherwise  none be return by default  those strategies should also provide information regard what be return by this property  single worker strategies usually do not have a tf distribute cluster resolver clusterresolver  and in those case this property will return none  the tf distribute cluster resolver clusterresolver may be useful when the user need to access information such as the cluster spec  task type or task id  for example  os environ 'tf config'    json dump    'cluster'         'worker'   "localhost 12345"  "localhost 23456"        'ps'   "localhost 34567"       'task'   'type'  'worker'  'index'  0     this implicitly use tf config for the cluster and current task info strategy   tf distribute experimental multiworkermirroredstrategy     if strategy cluster resolver task type    'worker'     perform something that's only applicable on workers  since we set this    as a worker above  this block will run on this particular instance elif strategy cluster resolver task type    'ps'     perform something that's only applicable on parameter servers  since we    set this as a worker above  this block will not run on this particular    instance  for more information  please see tf distribute cluster resolver clusterresolver's api docstring  ) ( extended : tf distribute strategyextended with additional methods  ) ( num replicas in sync : return number of replicas over which gradients be aggregate  )
( cluster resolver : return the cluster resolver associate with this strategy  in general  when use a multi worker tf distribute strategy such as tf distribute experimental multiworkermirroredstrategy or tf distribute tpustrategy    there be a tf distribute cluster resolver clusterresolver associate with the strategy use  and such an instance be return by this property  strategies that intend to have an associate tf distribute cluster resolver clusterresolver must set the relevant attribute  or override this property  otherwise  none be return by default  those strategies should also provide information regard what be return by this property  single worker strategies usually do not have a tf distribute cluster resolver clusterresolver  and in those case this property will return none  the tf distribute cluster resolver clusterresolver may be useful when the user need to access information such as the cluster spec  task type or task id  for example  os environ 'tf config'    json dump    'cluster'         'worker'   "localhost 12345"  "localhost 23456"        'ps'   "localhost 34567"       'task'   'type'  'worker'  'index'  0     this implicitly use tf config for the cluster and current task info strategy   tf distribute experimental multiworkermirroredstrategy     if strategy cluster resolver task type    'worker'     perform something that's only applicable on workers  since we set this    as a worker above  this block will run on this particular instance elif strategy cluster resolver task type    'ps'     perform something that's only applicable on parameter servers  since we    set this as a worker above  this block will not run on this particular    instance  for more information  please see tf distribute cluster resolver clusterresolver's api docstring  ) ( extended : tf distribute strategyextended with additional methods  ) ( num replicas in sync : return number of replicas over which gradients be aggregate  )
( cluster resolver : optional tf distribute cluster resolver clusterresolver object  default to a tf distribute cluster resolver tfconfigclusterresolver  )
( tpu cluster resolver : a tf distribute cluster resolver tpuclusterresolver  which provide information about the tpu cluster  ) ( steps per run : number of step to run on device before return to the host  note that this can have side effect on performance  hook  metrics  summaries etc  this parameter be only use when distribution strategy be use with estimator or keras  ) ( device assignment : optional tf tpu experimental deviceassignment to specify the placement of replicas on the tpu cluster  currently only support the usecase of use a single core within a tpu cluster  )
( num clusters : an integer tensor specify the number of cluster  this argument be ignore if initial cluster be a tensor or numpy array  ) ( model dir : the directory to save the model result and log file  ) ( initial clusters : specify how the initial cluster center be choose  one of the follow    a tensor or numpy array with the initial cluster   center    a callable f input  k  that select and return up to   k center from an input batch  f be free to return any number of   center from 0 to k  it will be invoke on successive input   batch as necessary until all num cluster center be choose   kmeansclustering random init  choose center randomly from an input batch  if the batch size be less than num cluster then the entire batch be choose to be initial cluster center and the remain center be choose from successive input batch  kmeansclustering kmeans plus plus init  use kmeans   to choose center from the first input batch  if the batch size be less than num cluster  a tensorflow runtime error occur  ) ( distance metric : the distance metric use for cluster  one of  kmeansclustering square euclidean distance  euclidean distance between vectors u and v be define as   u−v  2 which be the square root of the sum of the absolute square of the elements' difference  kmeansclustering cosine distance  cosine distance between vectors u and v be define as 1− u v /   u  2  v  2   ) ( seed : python integer  seed for prng use to initialize center  ) ( use mini batch : a boolean specify whether to use the mini batch k mean algorithm  see explanation above  ) ( mini batch steps per iteration : the number of step after which the update cluster center be sync back to a master copy  use only if use mini batch true  see explanation above  ) ( kmeans plus plus num retries : for each point that be sample during kmeans   initialization  this parameter specify the number of additional point to draw from the current distribution before select the best  if a negative value be specify  a heuristic be use to sample o log num to sample   additional point  use only if initial cluster kmeansclustering kmeans plus plus init  ) ( relative tolerance : a relative tolerance of change in the loss between iterations  stop learn if the loss change less than this amount  this may not work correctly if use mini batch true  ) ( config : see tf estimator estimator  ) ( feature columns : an optionable iterable contain all the feature columns use by the model  all items in the set should be feature column instance that can be pass to tf feature column input layer  if this be none  all feature will be use  )
( units : an int indicate the dimension of the logit layer   in the multihead case  this should be the sum of all component heads' logit dimension  ) ( hidden units : iterable of integer number of hide units per layer  ) ( feature columns : iterable of feature column  featurecolumn model input  ) ( activation fn : activation function apply to each layer  ) ( dropout : when not none  the probability we will drop out a give coordinate  ) ( input layer partitioner : partitioner for input layer  ) ( batch norm : whether to use batch normalization after each hide layer  )
( units : an int indicate the dimension of the logit layer  ) ( feature columns : an iterable contain all the feature columns use by the model  ) ( sparse combiner : a string specify how to reduce if a categorical column be multivalent   one of "mean"  "sqrtn"  and "sum"  )
( feature columns : all embed featurecolumns use by model  ) ( optimization parameters : an instance of adagradparameters  adamparameters or stochasticgradientdescentparameters  this optimizer will be apply to all embed variables specify by feature columns  ) ( clipping limit :  optional  clip limit  absolute value   ) ( pipeline execution with tensor core : set this to true make train faster  but train model will be different if step n and step n 1 involve the same set of embed ids  please see tpu embed configuration proto for detail  ) ( experimental gradient multiplier fn :  optional  a fn take global step as input return the current multiplier for all embed gradients  ) ( feature to config dict : a dictionary map feature name to instance of the class featureconfig  either feature columns or the pair of feature to config dict and table to config dict must be specify  ) ( table to config dict : a dictionary map feature name to instance of the class tableconfig  either feature columns or the pair of feature to config dict and table to config dict must be specify  )
( state : true  false or none  none restore the default behavior  )
( model : a tf keras model to be save  if the model be subclassed  the flag serve only must be set to true  ) ( saved model path : a string specify the path to the savedmodel directory  ) ( custom objects : optional dictionary map string name to custom class or function  e g  custom loss function   ) ( as text : bool  false by default  whether to write the savedmodel proto in text format  currently unavailable in serve only mode  ) ( input signature : a possibly nest sequence of tf tensorspec object  use to specify the expect model input  see tf function for more detail  ) ( serving only : bool  false by default  when this be true  only the prediction graph be save  )
( saved model path : a string specify the path to an exist savedmodel  ) ( custom objects : optional dictionary map name  string  to custom class or function to be consider during deserialization  )


( session : a tensorflow session that contain the graph to convert  ) ( graph def : a graph def that we should convert  ) ( write callback : a function pointer that can be use to write intermediate step of graph transformation  optional   )
( learning rate : use for update embed table  ) ( initial accumulator : initial accumulator for adagrad  ) ( use gradient accumulation : set this to false make embed gradients calculation less accurate but faster  please see optimization parameters proto for detail  ) ( clip weight min : the minimum value to clip by  none mean  infinity  ) ( clip weight max : the maximum value to clip by  none mean  infinity  ) ( weight decay factor : amount of weight decay to apply  none mean that the weight be not decay  ) ( multiply weight decay factor by learning rate : if true  weight decay factor be multiply by the current learn rate  ) ( clip gradient min : the minimum value to clip by  none mean  infinity  gradient accumulation must be set to true if this be set  ) ( clip gradient max : the maximum value to clip by  none mean  infinity  gradient accumulation must be set to true if this be set  )
( learning rate : a float point value  the learn rate  ) ( beta1 : a float value  the exponential decay rate for the 1st moment estimate  ) ( beta2 : a float value  the exponential decay rate for the 2nd moment estimate  ) ( epsilon : a small constant for numerical stability  ) ( lazy adam : use lazy adam instead of adam  lazy adam train faster  see optimization parameters proto for detail  ) ( sum inside sqrt : this improve train speed  please see optimization parameters proto for detail  ) ( use gradient accumulation : set this to false make embed gradients calculation less accurate but faster  please see optimization parameters proto for detail  ) ( clip weight min : the minimum value to clip by  none mean  infinity  ) ( clip weight max : the maximum value to clip by  none mean  infinity  ) ( weight decay factor : amount of weight decay to apply  none mean that the weight be not decay  ) ( multiply weight decay factor by learning rate : if true  weight decay factor be multiply by the current learn rate  ) ( clip gradient min : the minimum value to clip by  none mean  infinity  gradient accumulation must be set to true if this be set  ) ( clip gradient max : the maximum value to clip by  none mean  infinity  gradient accumulation must be set to true if this be set  )
( learning rate : a float point value  the learn rate  ) ( use gradient accumulation : set this to false make embed gradients calculation less accurate but faster  please see optimization parameters proto for detail  ) ( clip weight min : the minimum value to clip by  none mean  infinity  ) ( clip weight max : the maximum value to clip by  none mean  infinity  ) ( weight decay factor : amount of weight decay to apply  none mean that the weight be not decay  ) ( multiply weight decay factor by learning rate : if true  weight decay factor be multiply by the current learn rate  ) ( clip gradient min : the minimum value to clip by  none mean  infinity  ) ( clip gradient max : the maximum value to clip by  none mean  infinity  )
( categorical column : a categorical column return from categorical column with identity  weight categorical column  categorical column with vocabulary file  categorical column with vocabulary list  sequence categorical column with identity  sequence categorical column with vocabulary file  sequence categorical column with vocabulary list ) ( dimension : an integer specify dimension of the embed  must be > 0  ) ( combiner : a string specify how to reduce if there be multiple entries in a single row for a non sequence column  for more information  see tf feature column embed column  ) ( initializer : a variable initializer function to be use in embed variable initialization  if not specify  default to tf compat v1 truncate normal initializer with mean 0 0 and standard deviation 1/sqrt dimension   ) ( max sequence length : an non negative integer specify the max sequence length  any sequence shorter then this will be pad with 0 embeddings and any sequence longer will be truncate  this must be positive for sequence feature and 0 for non sequence feature  ) ( learning rate fn : a function that take global step and return learn rate for the embed table  if you intend to use the same learn rate for multiple embed table  please ensure that you pass the exact same python function to all call of embed column  otherwise performence may suffer  ) ( embedding lookup device : the device on which to run the embed lookup  valid options be "cpu"  "tpu tensor core"  and "tpu embed core"  if specify "tpu tensor core"  a tensor core shape must be supply  if not specify  the default behavior be embed lookup on "tpu embed core" for train and "cpu" for inference  valid options for train    "tpu embed core"  "tpu tensor core"  valid options for serve     "cpu"  "tpu tensor core"  for train  tpu embed core be good for large embed vocab  >1m   otherwise  tpu tensor core be often sufficient  for serve  do embed lookup on tpu tensor core during serve be a way to reduce host cpu usage in case where that be a bottleneck  ) ( tensor core shape : if supply  a list of integers which specify the intend dense shape to run embed lookup for this feature on tensorcore  the batch dimension can be leave none or  1 to indicate a dynamic shape  only rank 2 shape currently support  ) ( use safe embedding lookup : if true  use safe embed lookup sparse instead of embed lookup sparse  safe embed lookup sparse ensure there be no empty row and all weight and ids be positive at the expense of extra compute cost  this only apply to rank 2  nxm  shape input tensors  default to true  consider turn off if the above check be not need  note that have empty row will not trigger any error though the output result might be 0 or omit  )
( categorical columns : a list of categorical columns return from categorical column with identity  weight categorical column  categorical column with vocabulary file  categorical column with vocabulary list  sequence categorical column with identity  sequence categorical column with vocabulary file  sequence categorical column with vocabulary list ) ( dimension : an integer specify dimension of the embed  must be > 0  ) ( combiner : a string specify how to reduce if there be multiple entries in a single row for a non sequence column  for more information  see tf feature column embed column  ) ( initializer : a variable initializer function to be use in embed variable initialization  if not specify  default to tf truncate normal initializer with mean 0 0 and standard deviation 1/sqrt dimension   ) ( shared embedding collection name : optional name of the collection where share embed weight be add  if not give  a reasonable name will be choose base on the name of categorical columns  this be also use in variable scope when create share embed weight  ) ( max sequence lengths : an list of non negative integers  either none or empty or the same length as the argument categorical columns  entries correspond to non sequence columns must be 0 and entries correspond to sequence columns specify the max sequence length for the column  any sequence shorter then this will be pad with 0 embeddings and any sequence longer will be truncate  ) ( learning rate fn : a function that take global step and return learn rate for the embed table  if you intend to use the same learn rate for multiple embed table  please ensure that you pass the exact same python function to all call of share embed columns  otherwise performence may suffer  ) ( embedding lookup device : the device on which to run the embed lookup  valid options be "cpu"  "tpu tensor core"  and "tpu embed core"  if specify "tpu tensor core"  a tensor core shape must be supply  default to "cpu"  if not specify  the default behavior be embed lookup on "tpu embed core" for train and "cpu" for inference  valid options for train    "tpu embed core"  "tpu tensor core"  valid options for serve     "cpu"  "tpu tensor core"  for train  tpu embed core be good for large embed vocab  >1m   otherwise  tpu tensor core be often sufficient  for serve  do embed lookup on tpu tensor core during serve be a way to reduce host cpu usage in case where that be a bottleneck  ) ( tensor core shape : if supply  a list of integers which specify the intend dense shape to run embed lookup for this feature on tensorcore  the batch dimension can be leave none or  1 to indicate a dynamic shape  only rank 2 shape currently support  ) ( use safe embedding lookup : if true  use safe embed lookup sparse instead of embed lookup sparse  safe embed lookup sparse ensure there be no empty row and all weight and ids be positive at the expense of extra compute cost  this only apply to rank 2  nxm  shape input tensors  default to true  consider turn off if the above check be not need  note that have empty row will not trigger any error though the output result might be 0 or omit  )
( estimator : a tf estimator estimator instance to call evaluate  ) ( input fn : equivalent to the input fn arg to estimator evaluate  a function that construct the input data for evaluation  see create input function   for more information  the function should construct and return one of the follow   a 'tf data dataset' object  output of dataset object must be a tuple  feature  label  with same constraints as below  a tuple  feature  label   where feature be a tensor or a dictionary of string feature name to tensor and label be a tensor or a dictionary of string label name to tensor  both feature and label be consume by model fn  they should satisfy the expectation of model fn from input  ) ( steps : equivalent to the step arg to estimator evaluate   number of step for which to evaluate model  if none  evaluate until input fn raise an end of input exception  ) ( hooks : equivalent to the hook arg to estimator evaluate  list of sessionrunhook subclass instance  use for callbacks inside the evaluation call  ) ( name : equivalent to the name arg to estimator evaluate  name of the evaluation if user need to run multiple evaluations on different data set  such as on train data vs test data  metrics for different evaluations be save in separate folders  and appear separately in tensorboard  ) ( every n iter : int  run the evaluator once every n train iteration  )
( example id column : the column name contain the example ids  ) ( num loss partitions : number of workers  ) ( num table shards : number of shards of the internal state table  typically set to match the number of parameter servers  ) ( symmetric l1 regularization : a float value  must be greater than or equal to zero  ) ( symmetric l2 regularization : a float value  must be greater than zero and should typically be greater than 1  ) ( adaptive : a boolean indicate whether to use adaptive sample  )
( sequence feature columns : an iterable contain the featurecolumns that represent sequential input  all items in the set should either be sequence columns  e g  sequence numeric column  or construct from one  e g  embed column with sequence categorical column   as input   ) ( context feature columns : an iterable contain the featurecolumns for contextual input  the data represent by these columns will be replicate and give to the rnn at each timestep  these columns must be instance of class derive from densecolumn such as numeric column  not the sequential variants  ) ( units : iterable of integer number of hide units per rnn layer  if set  cell type must also be specify and rnn cell fn must be none  ) ( cell type : a class produce a rnn cell or a string specify the cell type  support string be  'simple rnn'  'lstm'  and 'gru'  if   set  units must also be specify and rnn cell fn must be none  ) ( rnn cell fn : a function that return a rnn cell instance that will be use to construct the rnn  if set  units and cell type cannot be set  this be for advance users who need additional customization beyond units and cell type  note that tf keras layer stackedrnncells be need for stack rnns  ) ( return sequences : a boolean indicate whether to return the last output in the output sequence  or the full sequence  note that if true  weight column must be none or a string  ) ( model dir : directory to save model parameters  graph and etc  this can also be use to load checkpoints from the directory into a estimator to continue train a previously save model  ) ( n classes : number of label class  default to 2  namely binary classification  must be > 1  ) ( weight column : a string or a numericcolumn create by tf feature column numeric column define feature column represent weight  it be use to down weight or boost examples during train  it will be multiply by the loss of the example  if it be a string  it be use as a key to fetch weight tensor from the feature  if it be a numericcolumn  raw tensor be fetch by key weight column key  then weight column normalizer fn be apply on it to get weight tensor  ) ( label vocabulary : a list of string represent possible label value  if give  label must be string type and have any value in label vocabulary  if it be not give  that mean label be already encode as integer or float within  0  1  for n class 2 and encode as integer value in  0  1      n class 1  for n classes>2   also there will be errors if vocabulary be not provide and label be string  ) ( optimizer : an instance of tf optimizer or string specify optimizer type  default to adagrad optimizer  ) ( loss reduction : one of tf losses reduction except none  describe how to reduce train loss over batch  default to sum over batch size  ) ( sequence mask : a string with the name of the sequence mask tensor  if sequence mask be in the feature dictionary  the provide tensor be use  otherwise the sequence mask be compute from the length of sequential feature  the sequence mask be use in evaluation and train mode to aggregate loss and metrics computation while exclude pad step  it be also add to the predictions dictionary in prediction mode to indicate which step be pad  ) ( config : runconfig object to configure the runtime settings  )
( head : a head instance  this specify the model's output and loss function to be optimize  ) ( sequence feature columns : an iterable contain the featurecolumns that represent sequential input  all items in the set should either be sequence columns  e g  sequence numeric column  or construct from one  e g  embed column with sequence categorical column   as input   ) ( context feature columns : an iterable contain the featurecolumns for contextual input  the data represent by these columns will be replicate and give to the rnn at each timestep  these columns must be instance of class derive from densecolumn such as numeric column  not the sequential variants  ) ( units : iterable of integer number of hide units per rnn layer  if set  cell type must also be specify and rnn cell fn must be none  ) ( cell type : a class produce a rnn cell or a string specify the cell type  support string be  'simple rnn'  'lstm'  and 'gru'  if   set  units must also be specify and rnn cell fn must be none  ) ( rnn cell fn : a function that return a rnn cell instance that will be use to construct the rnn  if set  units and cell type cannot be set  this be for advance users who need additional customization beyond units and cell type  note that tf keras layer stackedrnncells be need for stack rnns  ) ( return sequences : a boolean indicate whether to return the last output in the output sequence  or the full sequence  ) ( model dir : directory to save model parameters  graph and etc  this can also be use to load checkpoints from the directory into a estimator to continue train a previously save model  ) ( optimizer : an instance of tf optimizer or string specify optimizer type  default to adagrad optimizer  ) ( config : runconfig object to configure the runtime settings  )
( features : a dict of string to tensor or tensor  ) ( labels : a dict of string to tensor or tensor  ) ( default batch size : the number of query examples expect per batch  leave unset for variable batch size  recommend   )
( logit fn : a logit fn as define above  ) ( features : the feature dict  ) ( mode : train / eval / predict modekeys  ) ( params : the hyperparameter dict  ) ( config : the configuration object  )
( estimator : a tf estimator estimator instance  ) ( should stop fn : callable  function that take no arguments and return a bool  if the function return true  stop will be initiate by the chief  ) ( run every secs : if specify  call should stop fn at an interval of run every secs second  default to 60 second  either this or run every step must be set  ) ( run every steps : if specify  call should stop fn every run every step step  either this or run every secs must be set  )

( estimator : a tf estimator estimator instance  ) ( metric name : str  metric to track  "loss"  "accuracy"  etc  ) ( threshold : numeric threshold for the give metric  ) ( eval dir : if set  directory contain summary file with eval metrics  by default  estimator eval dir   will be use  ) ( min steps : int  stop be never request if global step be less than this value  default to 0  ) ( run every secs : if specify  call should stop fn at an interval of run every secs second  default to 60 second  either this or run every step must be set  ) ( run every steps : if specify  call should stop fn every run every step step  either this or run every secs must be set  )
( estimator : a tf estimator estimator instance  ) ( metric name : str  metric to track  "loss"  "accuracy"  etc  ) ( threshold : numeric threshold for the give metric  ) ( eval dir : if set  directory contain summary file with eval metrics  by default  estimator eval dir   will be use  ) ( min steps : int  stop be never request if global step be less than this value  default to 0  ) ( run every secs : if specify  call should stop fn at an interval of run every secs second  default to 60 second  either this or run every step must be set  ) ( run every steps : if specify  call should stop fn every run every step step  either this or run every secs must be set  )
( estimator : a tf estimator estimator instance  ) ( metric name : str  metric to track  "loss"  "accuracy"  etc  ) ( max steps without decrease : int  maximum number of train step with no decrease in the give metric  ) ( eval dir : if set  directory contain summary file with eval metrics  by default  estimator eval dir   will be use  ) ( min steps : int  stop be never request if global step be less than this value  default to 0  ) ( run every secs : if specify  call should stop fn at an interval of run every secs second  default to 60 second  either this or run every step must be set  ) ( run every steps : if specify  call should stop fn every run every step step  either this or run every secs must be set  )
( estimator : a tf estimator estimator instance  ) ( metric name : str  metric to track  "loss"  "accuracy"  etc  ) ( max steps without increase : int  maximum number of train step with no increase in the give metric  ) ( eval dir : if set  directory contain summary file with eval metrics  by default  estimator eval dir   will be use  ) ( min steps : int  stop be never request if global step be less than this value  default to 0  ) ( run every secs : if specify  call should stop fn at an interval of run every secs second  default to 60 second  either this or run every step must be set  ) ( run every steps : if specify  call should stop fn every run every step step  either this or run every secs must be set  )
( units : positive integer  output dimension without the batch size  ) ( activation : activation function to use  if you don't specify anything  no activation be apply  ) ( use bias : whether to calculate the bias/intercept for this model  if set to false  no bias/intercept will be use in calculations  e g   the data be already center  ) ( kernel initializer : initializer for the kernel weight matrices  ) ( bias initializer : initializer for the bias vector  ) ( kernel regularizer : regularizer for kernel vectors  ) ( bias regularizer : regularizer for bias vector  ) (   kwargs : the keyword arguments that be pass on to baselayer init  )
( feature columns : an iterable of dense sequence columns  valid columns be  embed column that wrap a sequence categorical column with   sequence numeric column  ) ( trainable : boolean  whether the layer's variables will be update via gradient descent during train  ) ( name : name to give to the sequencefeatures  ) (   kwargs : keyword arguments to construct a layer  )
( linear model : a premade linearmodel  its output must match the output of the dnn model  ) ( dnn model : a tf keras model  its output must match the output of the linear model  ) ( activation : activation function  set it to none to maintain a linear activation  ) (   kwargs : the keyword arguments that be pass on to baselayer init  allow keyword arguments include name  )