class function with metaclass functionmeta   c  functionbase   contextmethodmixin   hookmixin          r   record operation history and define formulas for differentiate ops       see the note on extend the autograd engine for more detail on how to use     this class  https //pytorch org/docs/stable/notes/extending html extending-torch-autograd      every operation perform on  class `tensor` s create a new function     object  that perform the computation  and record that it happen      the history be retain in the form of a dag of function  with edge     denote data dependencies  ``input <- output``   then  when backward be     call  the graph be process in the topological order  by call      func `backward` methods of each  class `function` object  and pass     return gradients on to next  class `function` s       normally  the only way users interact with function be by create     subclasses and define new operations  this be a recommend way of     extend torch autograd       examples            >>> class exp function           >>>         >>>      staticmethod         >>>     def forward ctx  i           >>>         result = i exp           >>>         ctx save for backward result          >>>         return result         >>>         >>>      staticmethod         >>>     def backward ctx  grad output           >>>         result  = ctx save tensors         >>>         return grad output   result         >>>         >>>  use it by call the apply method          >>> output = exp apply input              def   init   self   args    kwargs           cls = self   class           warn warn f  cls  should not be instantiate  methods on autograd function                        string                       string                       string  deprecationwarning       def   call   self   args    kwargs           raise runtimeerror              string             string             string            be traceable = false       staticmethod     def forward ctx  any   args  any    kwargs  any  -> any          r   perform the operation           this function be to be override by all subclasses           it must accept a context ctx as the first argument  follow by any         number of arguments  tensors or other type            the context can be use to store arbitrary data that can be then         retrieve during the backward pass                      raise notimplementederror string                                   string        staticmethod     def backward ctx  any   grad output  any  -> any          r   define a formula for differentiate the operation           this function be to be override by all subclasses           it must accept a context  attr `ctx` as the first argument  follow by         as many output as the  func `forward` return  none will be pass in         for non tensor output of the forward function           and it should return as many tensors  as there be input to          func `forward`  each argument be the gradient w r t the give output          and each return value should be the gradient w r t  the         correspond input  if an input be not a tensor or be a tensor not         require grads  you can just pass none as a gradient for that input           the context can be use to retrieve tensors save during the forward         pass  it also have an attribute  attr `ctx need input grad` as a tuple         of booleans represent whether each input need gradient  e g            func `backward` will have ``ctx need input grad 0  = true`` if the         first input to  func `forward` need gradient computated w r t  the         output                      raise notimplementederror string                                   string  
class profile object       string     def   init                self              enabled=true                             use cuda=false              record shapes=false              with flops=false              profile memory=false              with stack=false              use kineto=false              use cpu=true           self enable  bool = enable         if not self enable              return         self use cuda = use cuda         self function events = none         self enter = false         self record shape = record shape         self with flop = with flop         self record shape  = self with flop         self profile memory = profile memory         self with stack = with stack         self use cpu = use cpu         self kineto result = none         if not self use cpu              assert use kineto  \                 string         if self use cuda and not torch cuda be available                warn string              self use cuda = false          self profiler kind = none         self kineto activities = set           if use kineto              if torch autograd kineto available                    if self use cpu                      self kineto activities add profileractivity cpu                  use gpu fallback = false                 if self use cuda                      if  profileractivity cuda not in                             torch autograd  support kineto activities                             warn string                          use gpu fallback = true                     else                          self kineto activities add profileractivity cuda                  self profiler kind = profilerstate kineto if not use gpu fallback else \                     profilerstate kineto gpu fallback                  assert len self kineto activities  > 0  \                     string             else                  warn string           if not self kineto activities              if self use cuda                                   self profiler kind = profilerstate cuda             else                  self profiler kind = profilerstate cpu      def config self           assert self profiler kind be not none         return torch autograd profilerconfig              self profiler kind              self record shape              self profile memory              self with stack              self with flop       def   enter   self           if not self enable              return         if self enter              raise runtimeerror string          self  prepare trace           self  start trace           return self      def  prepare trace self           self enter = true         if self kineto activities              torch autograd  prepare profiler self config    self kineto activities          else                           pass      def  start trace self           self enter = true         if self kineto activities              torch autograd  enable profiler self config    self kineto activities          else              torch autograd  enable profiler legacy self config         def   exit   self  exc type  exc val  exc tb           if not self enable              return         if self use cuda              torch cuda synchronize           if self kineto activities              self kineto result = torch autograd  disable profiler               parse result = parse kineto result self kineto result          else              record = torch autograd  disable profiler legacy               parse result = parse legacy record record          self function events = eventlist              parse result              use cuda=self use cuda              profile memory=self profile memory              with flops=self with flop          self function events  build tree           return false      def   repr   self           if self function events be none              return string         return repr self function events       def   str   self           if self function events be none              return string         return str self function events       def  check finish self           if self function events be none              raise runtimeerror string       def table self  sort by=none  row limit=100  max src column width=75  header=none  top level events only=false           self  check finish           assert self function events be not none         return self function events table              sort by=sort by  row limit=row limit  max src column width=max src column width  header=header              top level events only=top level events only               table   doc   = eventlist table   doc        def export chrome trace self  path           self  check finish           if self kineto result be not none              self kineto result save path          else              assert self function events be not none             return self function events export chrome trace path      export chrome trace   doc   = eventlist export chrome trace   doc        def export stack self  path  str  metric  str = string           self  check finish           assert self function events be not none  string         assert self with stack  string         return self function events export stack path  metric       def key average self  group by input shape=false  group by stack n=0           self  check finish           assert self function events be not none  string         return self function events key average group by input shape  group by stack n      key average   doc   = eventlist key average   doc        def total average self           self  check finish           assert self function events be not none  string         return self function events total average       total average   doc   = eventlist total average   doc         property     def self cpu time total self           string         self  check finish           assert self function events be not none         return self function events self cpu time total 
class emit nvtx object       string     def   init   self  enabled=true  record shapes=false           self enable = enable         self enter = false         self record shape = record shape      def   enter   self           if not self enable              return         if self enter              raise runtimeerror string          self enter = true         torch cuda synchronize           torch autograd  enable profiler legacy              torch autograd profilerconfig                  profilerstate nvtx                  self record shape                  false                  false                  false                    return self      def   exit   self  exc type  exc val  exc tb           if not self enable              return         torch cuda synchronize           torch autograd  disable profiler legacy           return false 
class detect anomaly object       r   context-manager that enable anomaly detection for the autograd engine       this do two things       - run the forward pass with detection enable will allow the backward       pass to print the traceback of the forward operation that create the fail       backward function      - any backward computation that generate  nan  value will raise an error          warn           this mode should be enable only for debug as the different test         will slow down your program execution       example           >>> import torch         >>> from torch import autograd         >>> class myfunc autograd function                    staticmethod                 def forward ctx  inp                       return inp clone                    staticmethod                 def backward ctx  go                         error during the backward pass                     raise runtimeerror  some error in backward                       return go clone           >>> def run fn a                   out = myfunc apply a                  return out sum           >>> inp = torch rand 10  10  require grad=true          >>> out = run fn inp          >>> out backward               traceback  most recent call last                 file  <stdin>   line 1  in <module>               file  /your/pytorch/install/torch/ tensor py   line 93  in backward                 torch autograd backward self  gradient  retain graph  create graph                file  /your/pytorch/install/torch/autograd/  init   py   line 90  in backward                 allow unreachable=true     allow unreachable flag               file  /your/pytorch/install/torch/autograd/function py   line 76  in apply                 return self  forward cls backward self   args                file  <stdin>   line 8  in backward             runtimeerror  some error in backward         >>> with autograd detect anomaly                    inp = torch rand 10  10  require grad=true                  out = run fn inp                  out backward               traceback of forward call that cause the error                file  tmp py   line 53  in <module>                 out = run fn inp                file  tmp py   line 44  in run fn                 out = myfunc apply a              traceback  most recent call last                 file  <stdin>   line 4  in <module>               file  /your/pytorch/install/torch/ tensor py   line 93  in backward                 torch autograd backward self  gradient  retain graph  create graph                file  /your/pytorch/install/torch/autograd/  init   py   line 90  in backward                 allow unreachable=true     allow unreachable flag               file  /your/pytorch/install/torch/autograd/function py   line 76  in apply                 return self  forward cls backward self   args                file  <stdin>   line 8  in backward             runtimeerror  some error in backward               def   init   self  -> none          self prev = torch be anomaly enable           warn warn string                       string                       string  stacklevel=2       def   enter   self  -> none          torch set anomaly enable true       def   exit   self   args  any  -> none          torch set anomaly enable self prev  
class set detect anomaly object       r   context-manager that set the anomaly detection for the autograd engine on or off       ``set detect anomaly`` will enable or disable the autograd anomaly detection     base on its argument  attr `mode`      it can be use as a context-manager or as a function       see ``detect anomaly`` above for detail of the anomaly detection behaviour       args          mode  bool   flag whether to enable anomaly detection  ``true``                        or disable  ``false``                 def   init   self  mode  bool  -> none          self prev = torch be anomaly enable           torch set anomaly enable mode       def   enter   self  -> none          pass      def   exit   self   args  any  -> none          torch set anomaly enable self prev  
class autocast object       r        instance of  class `autocast` serve as context managers or decorators that     allow regions of your script to run in mix precision       in these regions  cuda ops run in an op-specific dtype choose by autocast     to improve performance while maintain accuracy      see the  ref `autocast op reference<autocast-op-reference>` for detail       when enter an autocast-enabled region  tensors may be any type      you should not call `` half  `` on your model s  or input when use autocasting        class `autocast` should wrap only the forward pass es  of your network  include the loss     computation s    backward pass under autocast be not recommend      backward ops run in the same type that autocast use for correspond forward ops       example              create model and optimizer in default precision         model = net   cuda           optimizer = optim sgd model parameters                  for input  target in data              optimizer zero grad                  enable autocasting for the forward pass  model   loss              with autocast                    output = model input                  loss = loss fn output  target                 exit the context manager before backward               loss backward               optimizer step        see the  ref `automatic mix precision examples<amp-examples>` for usage  along with gradient scale      in more complex scenarios  e g   gradient penalty  multiple models/losses  custom autograd function         class `autocast` can also be use as a decorator  e g   on the ``forward`` method of your model            class autocastmodel nn module                                autocast               def forward self  input                            floating-point tensors produce in an autocast-enabled region may be ``float16``      after return to an autocast-disabled region  use them with floating-point     tensors of different dtypes may cause type mismatch errors   if so  cast the tensor s      produce in the autocast region back to ``float32``  or other dtype if desire       if a tensor from the autocast region be already ``float32``  the cast be a no-op      and incur no additional overhead   example              create some tensors in default dtype  here assume to be float32          a float32 = torch rand  8  8   device= cuda           b float32 = torch rand  8  8   device= cuda           c float32 = torch rand  8  8   device= cuda           d float32 = torch rand  8  8   device= cuda            with autocast                  torch mm be on autocast s list of ops that should run in float16                input be float32  but the op run in float16 and produce float16 output                no manual cast be require              e float16 = torch mm a float32  b float32                also handle mix input type             f float16 = torch mm d float32  e float16             after exit autocast  call f float16 float   to use with d float32         g float32 = torch mm d float32  f float16 float         type mismatch errors  in  an autocast-enabled region be a bug  if this be what you observe      please file an issue       ``autocast enabled=false `` subregions can be nest in autocast-enabled regions      locally disable autocast can be useful  for example  if you want to force a subregion     to run in a particular ``dtype``   disable autocast give you explicit control over     the execution type   in the subregion  input from the surround region     should be cast to ``dtype`` before use              create some tensors in default dtype  here assume to be float32          a float32 = torch rand  8  8   device= cuda           b float32 = torch rand  8  8   device= cuda           c float32 = torch rand  8  8   device= cuda           d float32 = torch rand  8  8   device= cuda            with autocast                e float16 = torch mm a float32  b float32               with autocast enabled=false                     call e float16 float   to ensure float32 execution                    necessary because e float16 be create in an autocasted region                  f float32 = torch mm c float32  e float16 float                   no manual cast be require when re-enter the autocast-enabled region                torch mm again run in float16 and produce float16 output  regardless of input type              g float16 = torch mm d float32  f float32       the autocast state be thread-local   if you want it enable in a new thread  the context manager or decorator     must be invoke in that thread   this affect  class `torch nn dataparallel` and      class `torch nn parallel distributeddataparallel` when use with more than one gpu per process      see  ref `working with multiple gpus<amp-multigpu>`        args          enable bool  optional  default=true    whether autocasting should be enable in the region              def   init   self  enabled=true           if enable and amp definitely not available                warn warn string              self  enable = false         else              self  enable = enable      def   enter   self           self prev = torch be autocast enable           torch set autocast enable self  enable          torch autocast increment nest        def   exit   self   args                    if torch autocast decrement nest   == 0              torch clear autocast cache           torch set autocast enable self prev          return false      def   call   self  func            functools wrap func          def decorate autocast  args    kwargs               with self                  return func  args    kwargs          return decorate autocast 
class gradscaler object        scale  optional torch tensor       grow tracker  optional torch tensor       per optimizer state  dict int  dict str  any       string     def   init   self                   init scale=2   16                   growth factor=2 0                   backoff factor=0 5                   growth interval=2000                   enabled=true           if enable and amp definitely not available                warn warn string              self  enable = false         else              self  enable = enable          if self  enable              assert growth factor > 1 0  string             assert backoff factor < 1 0  string              self  init scale = init scale                          self  scale = none             self  growth factor = growth factor             self  backoff factor = backoff factor             self  growth interval = growth interval             self  init growth tracker = 0                          self  growth tracker = none             self  per optimizer state = defaultdict  refresh per optimizer state       def  check scale growth tracker self  funcname  -> tuple torch tensor  torch tensor           fix = string         assert self  scale be not none  string format funcname    fix         assert self  growth tracker be not none  string format funcname    fix         return  self  scale  self  growth tracker       def  lazy init scale growth tracker self  dev           assert self  growth tracker be none  string         self  scale = torch full  1    self  init scale  dtype=torch float32  device=dev          self  growth tracker = torch full  1    self  init growth tracker  dtype=torch int32  device=dev       def scale self  output           string         if not self  enable              return output                   if isinstance output  torch tensor               assert output be cuda or output device type == string             if self  scale be none                  self  lazy init scale growth tracker output device              assert self  scale be not none             return output   self  scale to device=outputs device  non blocking=true                    stash  list  multidevicereplicator  =               def apply scale val               if isinstance val  torch tensor                   assert val be cuda or val device type == string                 if len stash  == 0                      if self  scale be none                          self  lazy init scale growth tracker val device                      assert self  scale be not none                     stash append  multidevicereplicator self  scale                   return val   stash 0  get val device              elif isinstance val  abc iterable                   iterable = map apply scale  val                  if isinstance val  list  or isinstance val  tuple                       return type val  iterable                  else                      return iterable             else                  raise valueerror string           return apply scale output       def  unscale grads  self  optimizer  inv scale  find inf  allow fp16           per device inv scale =  multidevicereplicator inv scale          per device find inf =  multidevicereplicator find inf                                                         per device and dtype grads = defaultdict lambda  defaultdict list             with torch no grad                for group in optimizer param group                  for param in group string                       if param grad be none                          continue                     if  not allow fp16  and param grad dtype == torch float16                          raise valueerror string                      if param grad be sparse                                                                                                                              if param grad dtype be torch float16                              param grad = param grad coalesce                           to unscale = param grad  value                       else                          to unscale = param grad                                           per device and dtype grads to unscale device  to unscale dtype  append to unscale               for device  per dtype grads in per device and dtype grads items                    for grads in per dtype grads value                        torch  amp foreach non finite check and unscale  grads                                                                       per device find inf get device                                                                        per device inv scale get device            return per device find inf  per device tensors      def unscale  self  optimizer           string         if not self  enable              return          self  check scale growth tracker string           optimizer state = self  per optimizer state id optimizer            if optimizer state string  be optstate unscaled              raise runtimeerror string          elif optimizer state string  be optstate step              raise runtimeerror string                    assert self  scale be not none         inv scale = self  scale double   reciprocal   float           find inf = torch full  1    0 0  dtype=torch float32  device=self  scale device           optimizer state string  = self  unscale grads  optimizer  inv scale  find inf  false          optimizer state string  = optstate unscaled      def  maybe opt step self  optimizer  optimizer state   args    kwargs           retval = none         if not sum v item   for v in optimizer state string  value                 retval = optimizer step  args    kwargs          return retval      def step self  optimizer   args    kwargs           string         if  not self  enable               return optimizer step  args    kwargs           if string in kwargs              raise runtimeerror string           self  check scale growth tracker string           optimizer state = self  per optimizer state id optimizer            if optimizer state string  be optstate step              raise runtimeerror string           retval = none          if  hasattr optimizer  string  and optimizer  step support amp scale                                                                   retval = optimizer step  args    dict kwargs  grad scaler=self               optimizer state string  = optstate step             return retval          if optimizer state string  be optstate ready              self unscale  optimizer           assert len optimizer state string   > 0  string          retval = self  maybe opt step optimizer  optimizer state   args    kwargs           optimizer state string  = optstate step          return retval      def update self  new scale=none           string         if not self  enable              return           scale   growth tracker = self  check scale growth tracker string           if new scale be not none                           if isinstance new scale  float                   self  scale fill  new scale                else                  reason = string                 assert isinstance new scale  torch cuda floattensor   reason                   assert new scale numel   == 1  reason                 assert new scale require grad be false  reason                 self  scale copy  new scale            else                                        find infs =  find inf to device= scale device  non blocking=true                            for state in self  per optimizer state value                             for find inf in state string  value                 assert len find infs  > 0  string              find inf combine = find infs 0              if len find infs  > 1                  for i in range 1  len find infs                        find inf combine  = find infs i               torch  amp update scale   scale                                        growth tracker                                       find inf combine                                       self  growth factor                                       self  backoff factor                                       self  growth interval                    self  per optimizer state = defaultdict  refresh per optimizer state       def  get scale async self           return self  scale      def get scale self           string         if self  enable              return self  init scale if self  scale be none else self  get scale async   item           else              return 1 0      def get growth factor self           r            return a python float contain the scale growth factor                      return self  growth factor      def set growth factor self  new factor           r            args              new scale  float    value to use as the new scale growth factor                      self  growth factor = new factor      def get backoff factor self           r            return a python float contain the scale backoff factor                      return self  backoff factor      def set backoff factor self  new factor           r            args              new scale  float    value to use as the new scale backoff factor                      self  backoff factor = new factor      def get growth interval self           r            return a python int contain the growth interval                      return self  growth interval      def set growth interval self  new interval           r            args              new interval  int    value to use as the new growth interval                      self  growth interval = new interval      def  get growth tracker self           if self  enable              return self  init growth tracker if self  growth tracker be none else self  growth tracker item           else              return 0      def be enable self           r            return a bool indicate whether this instance be enable                      return self  enable      def state dict self           r            return the state of the scaler as a  class `dict`   it contain five entries             `` scale `` - a python float contain the current scale           `` growth factor `` - a python float contain the current growth factor           `` backoff factor `` - a python float contain the current backoff factor           `` growth interval `` - a python int contain the current growth interval           ``  growth tracker `` - a python int contain the number of recent consecutive unskipped step           if this instance be not enable  return an empty dict              note              if you wish to checkpoint the scaler s state after a particular iteration   meth `state dict`            should be call after  meth `update`                      return  string  self get scale                    string  self  growth factor                  string  self  backoff factor                  string  self  growth interval                  string  self  get growth tracker    if self  enable else         def load state dict self  state dict           r            load the scaler state   if this instance be disable   meth `load state dict` be a no-op           args             state dict dict   scaler state   should be an object return from a call to  meth `state dict`                      if not self  enable              return          if len state dict  == 0              raise runtimeerror string                                string           self  init scale = state dict string          if self  scale be not none              self  scale fill  state dict string           self  growth factor = state dict string          self  backoff factor = state dict string          self  growth interval = state dict string          self  init growth tracker = state dict string          if self  growth tracker be not none              self  growth tracker fill  state dict string        def   getstate   self           state = self   dict   copy           if self  enable              assert len self  per optimizer state  == 0  string\                                                          string                                                    state string  = self get scale               state string  = self  get growth tracker               state string  = none             state string  = none         return state      def   setstate   self  state           self   dict   update state       def  check inf per device self  optimizer            scale    = self  check scale growth tracker string           dummy inv scale = torch full  1    1 0  dtype=torch float32  device= scale device          find inf = torch full  1    0 0  dtype=torch float32  device= scale device           self  per optimizer state id optimizer   string  = \             self  unscale grads  optimizer  dummy inv scale  find inf  true           return self  per optimizer state id optimizer   string       def  find inf per device self  optimizer           return self  per optimizer state id optimizer   string  
def get backoff factor self       r        return a python float contain the scale backoff factor              return self  backoff factor 
def get growth factor self       r        return a python float contain the scale growth factor              return self  growth factor 
def get growth interval self       r        return a python int contain the growth interval              return self  growth interval 
def get scale self       string     if self  enable          return self  init scale if self  scale be none else self  get scale async   item       else          return 1 0 
def be enable self       r        return a bool indicate whether this instance be enable              return self  enable 
def load state dict self  state dict       r        load the scaler state   if this instance be disable   meth `load state dict` be a no-op       args         state dict dict   scaler state   should be an object return from a call to  meth `state dict`              if not self  enable          return      if len state dict  == 0          raise runtimeerror string                            string       self  init scale = state dict string      if self  scale be not none          self  scale fill  state dict string       self  growth factor = state dict string      self  backoff factor = state dict string      self  growth interval = state dict string      self  init growth tracker = state dict string      if self  growth tracker be not none          self  growth tracker fill  state dict string   
def scale self  output       string     if not self  enable          return output           if isinstance output  torch tensor           assert output be cuda or output device type == string         if self  scale be none              self  lazy init scale growth tracker output device          assert self  scale be not none         return output   self  scale to device=outputs device  non blocking=true            stash  list  multidevicereplicator  =           def apply scale val           if isinstance val  torch tensor               assert val be cuda or val device type == string             if len stash  == 0                  if self  scale be none                      self  lazy init scale growth tracker val device                  assert self  scale be not none                 stash append  multidevicereplicator self  scale               return val   stash 0  get val device          elif isinstance val  abc iterable               iterable = map apply scale  val              if isinstance val  list  or isinstance val  tuple                   return type val  iterable              else                  return iterable         else              raise valueerror string       return apply scale output  
def set backoff factor self  new factor       r        args          new scale  float    value to use as the new scale backoff factor              self  backoff factor = new factor 
def set growth factor self  new factor       r        args          new scale  float    value to use as the new scale growth factor              self  growth factor = new factor 
def set growth interval self  new interval       r        args          new interval  int    value to use as the new growth interval              self  growth interval = new interval 
def state dict self       r        return the state of the scaler as a  class `dict`   it contain five entries         `` scale `` - a python float contain the current scale       `` growth factor `` - a python float contain the current growth factor       `` backoff factor `` - a python float contain the current backoff factor       `` growth interval `` - a python int contain the current growth interval       ``  growth tracker `` - a python int contain the number of recent consecutive unskipped step       if this instance be not enable  return an empty dict          note          if you wish to checkpoint the scaler s state after a particular iteration   meth `state dict`        should be call after  meth `update`              return  string  self get scale                string  self  growth factor              string  self  backoff factor              string  self  growth interval              string  self  get growth tracker    if self  enable else    
def step self  optimizer   args    kwargs       string     if  not self  enable           return optimizer step  args    kwargs       if string in kwargs          raise runtimeerror string       self  check scale growth tracker string       optimizer state = self  per optimizer state id optimizer        if optimizer state string  be optstate step          raise runtimeerror string       retval = none      if  hasattr optimizer  string  and optimizer  step support amp scale                                               retval = optimizer step  args    dict kwargs  grad scaler=self           optimizer state string  = optstate step         return retval      if optimizer state string  be optstate ready          self unscale  optimizer       assert len optimizer state string   > 0  string      retval = self  maybe opt step optimizer  optimizer state   args    kwargs       optimizer state string  = optstate step      return retval 
def unscale  self  optimizer       string     if not self  enable          return      self  check scale growth tracker string       optimizer state = self  per optimizer state id optimizer        if optimizer state string  be optstate unscaled          raise runtimeerror string      elif optimizer state string  be optstate step          raise runtimeerror string            assert self  scale be not none     inv scale = self  scale double   reciprocal   float       find inf = torch full  1    0 0  dtype=torch float32  device=self  scale device       optimizer state string  = self  unscale grads  optimizer  inv scale  find inf  false      optimizer state string  = optstate unscaled 
def update self  new scale=none       string     if not self  enable          return       scale   growth tracker = self  check scale growth tracker string       if new scale be not none                   if isinstance new scale  float               self  scale fill  new scale            else              reason = string             assert isinstance new scale  torch cuda floattensor   reason               assert new scale numel   == 1  reason             assert new scale require grad be false  reason             self  scale copy  new scale        else                            find infs =  find inf to device= scale device  non blocking=true                        for state in self  per optimizer state value                         for find inf in state string  value             assert len find infs  > 0  string          find inf combine = find infs 0          if len find infs  > 1              for i in range 1  len find infs                    find inf combine  = find infs i           torch  amp update scale   scale                                    growth tracker                                   find inf combine                                   self  growth factor                                   self  backoff factor                                   self  growth interval            self  per optimizer state = defaultdict  refresh per optimizer state  
def custom fwd fwd=none    kwargs       string     if fwd be none          if len kwargs  == 0              cast input = none         else              assert len kwargs  == 1             cast input = kwargs string          return functools partial custom fwd  cast inputs=cast input       if len kwargs  == 0          cast input = none     else          assert len kwargs  == 1         cast input = kwargs string        functools wrap fwd      def decorate fwd  args    kwargs           if cast input be none              args 0   fwd use autocast = torch be autocast enable               return fwd  args    kwargs          else              autocast context = torch be autocast enable               args 0   fwd use autocast = false             if autocast context                  with autocast enabled=false                       return fwd   cast args  cast input      cast kwargs  cast input               else                  return fwd  args    kwargs      return decorate fwd 
def custom bwd bwd       string      functools wrap bwd      def decorate bwd  args    kwargs           with autocast args 0   fwd use autocast               return bwd  args    kwargs      return decorate bwd 
def be available        r   return whether pytorch be build with mkl support         return torch  c have mkl 
def be available        r   return whether pytorch be build with openmp support         return torch  c have openmp 
class backend object       string     undefined = string     gloo = string     nccl = string     mpi = string     tcp = string      def   new   cls  name  str           if not isinstance name  string class               raise valueerror string format name           value = getattr backend  name upper    backend undefined           if value == backend tcp              raise valueerror string                              string                              string          elif value == backend undefined              raise valueerror string format name           elif value  = backend gloo and value  = backend nccl and value  = backend mpi              value = name         return value       classmethod     def register backend cls  name  func           string         setattr backend  name upper    func  
def be available   -> bool      string     return hasattr torch  c  string  
def init process group backend                         init method=none                         timeout=default pg timeout                         world size=-1                         rank=-1                         store=none                         group name=string                         pg options=none       string     global  pg group rank     global  backend     global  default pg init method      if not isinstance timeout  timedelta           raise runtimeerror string                            string       if groupmember world be not none          raise runtimeerror string                            string       assert  store be none  or  init method be none   \         string      if store be not none          assert world size > 0  string         assert rank >= 0  string     elif init method be none          init method = string      backend = backend backend       if backend == backend mpi          if world size  = -1 or rank  = -1              warn warn                  string                 string                 string format world size  rank            default pg =  new process group helper              -1              -1                              backend mpi              none              group name=group name              timeout=timeout           update default pg default pg      else                   if store be none              rendezvous iterator = rendezvous                  init method  rank  world size  timeout=timeout                           store  rank  world size = next rendezvous iterator              store set timeout timeout           default pg =  new process group helper              world size              rank                              backend              store              pg options=pg options              group name=group name              timeout=timeout           update default pg default pg        pg group rank groupmember world  =  i  i for i in range groupmember world size            backend =  pg map groupmember world  0         default pg init method = init method                     if backend == backend mpi                   barrier       else                             store base barrier rank  store  timeout                   if get backend default pg  in  backend gloo  backend nccl               default pg  set sequence number for group   
def be initialize        string     return groupmember world be not none 
def be mpi available        string     return  mpi available 
def be nccl available        string     return  nccl available 
def get backend group=none       string     if group be none          pg =  get default group       else          pg = group     if  rank not in group pg           raise runtimeerror string      pg store =  pg map get pg  none      assert pg store be not none     return pg store 0  
def get rank group=none       string     if  rank not in group group           return -1      default pg =  get default group       if group be none or group be groupmember world          return default pg rank        return  get group rank group  default pg rank    
def get world size group=none       string     if  rank not in group group           return -1      return  get group size group  
def new group ranks=none  timeout=default pg timeout  backend=none  pg options=none       string       global  pg group rank      default pg =  get default group       default backend  default store =  pg map default pg      global rank = default pg rank       global world size = default pg size                  if not backend          backend = default backend           if rank be not none          rank = sort rank          group world size = len rank          if group world size > global world size              raise runtimeerror string                                string                                string                   for rank in rank              if rank < 0 or rank >= global world size                  raise runtimeerror string                                    string          if global rank in rank              group rank = rank index global rank          else              group rank = none     else          rank = list range global world size           group world size = global world size         group rank = global rank      backend = backend backend      pg =  new process group helper group world size                                     group rank                                     rank                                     backend                                     default store                                     pg options=pg options                                     timeout=timeout             pg group rank pg  =           global rank  group rank         for group rank  global rank in enumerate rank                            if backend == backend mpi                   barrier       else                             store base barrier global rank  default store  timeout                   if pg  = groupmember non group member and get backend pg  in               backend gloo              backend nccl                         pg  set sequence number for group        return pg 
def send tensor           dst           group=none           tag=0       string      check single tensor tensor  string      if  rank not in group group           return      if group be none or group be groupmember world          default pg =  get default group           default pg send  tensor   dst  tag  wait       else          group dst rank =  get group rank group  dst          group send  tensor   group dst rank  tag  wait   
def recv tensor           src=none           group=none           tag=0       string      check single tensor tensor  string      if  rank not in group group           return -1      if group be none          pg =  get default group       else          pg = group      if src be none          work = pg recv anysource  tensor   tag          work wait           src rank = work  source rank           if group be none or group be groupmember world              return src rank         else              return  get global rank pg  src rank      else          if group be none or group be groupmember world              pg recv  tensor   src  tag  wait           else              group src rank =  get group rank pg  src              pg recv  tensor   group src rank  tag  wait           return src 
def isend tensor            dst            group=none            tag=0       string      check single tensor tensor  string      if  rank not in group group           return      if group be none or group be groupmember world          default pg =  get default group           return default pg send  tensor   dst  tag      else          group dst rank =  get group rank group  dst          return group send  tensor   group dst rank  tag  
def irecv tensor            src=none            group=none            tag=0       string      check single tensor tensor  string      if  rank not in group group           return      if group be none or group be groupmember world          pg =  get default group       else          pg = group      if src be none          return pg recv anysource  tensor   tag      else          if pg be groupmember world              return pg recv  tensor   src  tag          else              group src rank =  get group rank pg  src              return pg recv  tensor   group src rank  tag  
def broadcast tensor                src                group=none                async op=false       string      check single tensor tensor  string      if  rank not in group group           return      opt = broadcastoptions       opt rootrank = src     opt roottensor = 0      if group be none or group be groupmember world          default pg =  get default group           work = default pg broadcast  tensor   opt      else          group src rank =  get group rank group  src          opt rootrank = group src rank         work = group broadcast  tensor   opt      if async op          return work     else          work wait   
def broadcast object list object list  src=0  group=none       string     if  rank not in group group           return      my rank = get rank            if my rank == src          tensor list  size list = zip    object to tensor obj  for obj in object list           object size tensor = torch cat size list      else          object size tensor = torch empty len object list   dtype=torch long       group backend = get backend group      be nccl backend = group backend == backend nccl     current device = torch device string      if be nccl backend                                     current device = torch device string  torch cuda current device            object size tensor = object size tensor to current device          object size tensor = object size tensor to current device            broadcast object size tensor  src=src  group=group            if my rank == src          object tensor = torch cat tensor list      else          object tensor = torch empty              torch sum object size tensor  int   item                  dtype=torch uint8                if be nccl backend          object tensor = object tensor to current device      broadcast object tensor  src=src  group=group           offset = 0     if my rank  = src          for i  obj size in enumerate object size tensor               obj view = object tensor offset   offset   obj size              obj view = obj view type torch uint8                if obj view device  = torch device string                   obj view = obj view cpu               offset  = obj size             object list i  =  tensor to object obj view  obj size  
def all reduce tensor                 op=reduceop sum                 group=none                 async op=false       string      check single tensor tensor  string      if  rank not in group group           return      if tensor be complex            if not support complex op               raise runtimeerror f all reduce do not support  op  on complex tensors           tensor = torch view as real tensor       opt = allreduceoptions       opt reduceop = op     if group be none          default pg =  get default group           work = default pg allreduce  tensor   opt      else          work = group allreduce  tensor   opt       if async op          return work     else          work wait   
def reduce tensor             dst             op=reduceop sum             group=none             async op=false       string      check single tensor tensor  string      if  rank not in group group           return      opt = reduceoptions       opt reduceop = op     opt rootrank = dst      if group be none or group be groupmember world          default pg =  get default group           work = default pg reduce  tensor   opt      else          group dst rank =  get group rank group  dst          opt rootrank = group dst rank         work = group reduce  tensor   opt       if async op          return work     else          work wait   
def all gather tensor list                 tensor                 group=none                 async op=false       string      check tensor list tensor list  string       check single tensor tensor  string      if  rank not in group group           return      tensor list =  t if not t be complex   else torch view as real t  for t in tensor list      tensor = tensor if not tensor be complex   else torch view as real tensor       if group be none          default pg =  get default group           work = default pg allgather  tensor list    tensor       else          work = group allgather  tensor list    tensor        if async op          return work     else          work wait   
def all gather object object list  obj  group=none       string     if  rank not in group group           return      input tensor  local size =  object to tensor obj      group backend = get backend group      be nccl backend = group backend == backend nccl     current device = torch device string      if be nccl backend                                     current device = torch device string  torch cuda current device            input tensor = input tensor to current device          local size = local size to current device                group size = get world size group=group      object size tensor = torch zero group size  dtype=torch long  device=current device      object size list =           object size tensor i  unsqueeze dim=0  for i in range group size                 all gather object size list  local size  group=group      max object size = int max object size list  item               input tensor resize  max object size      coalesce output tensor = torch empty          max object size   group size  dtype=torch uint8  device=current device                output tensors =           coalesce output tensor max object size   i   max object size    i   1           for i in range group size            all gather output tensors  input tensor  group=group           for i  tensor in enumerate output tensors           tensor = tensor type torch uint8            if tensor device  = torch device string               tensor = tensor cpu           tensor size = object size list i          object list i  =  tensor to object tensor  tensor size  
def gather tensor             gather list=none             dst=0             group=none             async op=false       string      check single tensor tensor  string            if gather list           check tensor list gather list  string      else          gather list =         if  rank not in group group           return      my rank = get rank        validate output list for rank my rank  dst  gather list      output tensors =  gather list  if dst == my rank else        input tensors =  tensor       opt = gatheroptions       opt rootrank = dst      if group be none or group be groupmember world          default pg =  get default group           work = default pg gather output tensors  input tensors  opt      else          group dst rank =  get group rank group  dst          opt rootrank = group dst rank         work = group gather output tensors  input tensors  opt       if async op          return work     else          work wait   
def gather object obj  object gather list=none  dst=0  group=none       string     if  rank not in group group           return           my rank = get rank        validate output list for rank my rank  dst  object gather list      input tensor  local size =  object to tensor obj      group backend = get backend group      current device = torch device string      be nccl backend = group backend == backend nccl     if be nccl backend          current device = torch device string  torch cuda current device            input tensor = input tensor to current device          local size = local size to current device                group size = get world size group=group      object size tensor = torch zero group size  dtype=torch long  device=current device      object size list =           object size tensor i  unsqueeze dim=0  for i in range group size                           all gather object size list  local size  group=group      max object size = int max object size list  item               input tensor resize  max object size           if my rank == dst          coalesce output tensor = torch empty              max object size   group size  dtype=torch uint8  device=current device                            output tensors =               coalesce output tensor max object size   i   max object size    i   1               for i in range group size                     gather          input tensor          gather list=output tensors if my rank == dst else none          dst=dst          group=group            if my rank  = dst          return     for i  tensor in enumerate output tensors           tensor = tensor type torch uint8            tensor size = object size list i          object gather list i  =  tensor to object tensor  tensor size  
def scatter tensor              scatter list=none              src=0              group=none              async op=false       string      check single tensor tensor  string            if scatter list           check tensor list scatter list  string      else          scatter list =         if  rank not in group group           return      my rank = get rank       if src == my rank          if not scatter list              raise valueerror string                              string          input tensors =  scatter list          output tensors =  tensor      else          if scatter list              raise valueerror string                              string          input tensors =            output tensors =  tensor       opt = scatteroptions       opt rootrank = src      if group be none or group be groupmember world          default pg =  get default group           work = default pg scatter output tensors  input tensors  opt      else          group src rank =  get group rank group  src          opt rootrank = group src rank         work = group scatter output tensors  input tensors  opt       if async op          return work     else          work wait   
def scatter object list      scatter object output list  scatter object input list  src=0  group=none        string     if  rank not in group group           return      if           not isinstance scatter object output list  list          or len scatter object output list  < 1                raise runtimeerror              string                my rank = get rank group      if my rank == src          tensor list  tensor size = zip                 object to tensor obj  for obj in scatter object input list                    tensor list  tensor size = list tensor list   list tensor size                 if my rank == src          max tensor size = max tensor size          for tensor in tensor list              tensor resize  max tensor size      else          max tensor size = torch tensor  0   dtype=torch long      broadcast max tensor size  src=src  group=group            output tensor = torch empty max tensor size item    dtype=torch uint8      scatter          output tensor          scatter list=none if my rank  = src else tensor list          src=src          group=group                  obj tensor size = torch tensor  0   dtype=torch long      scatter          obj tensor size          scatter list=none if my rank  = src else tensor size          src=src          group=group                  scatter object output list 0  =  tensor to object output tensor  obj tensor size  
def reduce scatter output                     input list                     op=reduceop sum                     group=none                     async op=false       string      check single tensor output  string       check tensor list input list  string      if  rank not in group group           return      opt = reducescatteroptions       opt reduceop = op      if group be none          default pg =  get default group           work = default pg reduce scatter  output    input list   opt      else          work = group reduce scatter  output    input list   opt       if async op          return work     else          work wait   
def all to all output tensor list                 input tensor list                 group=none                 async op=false       string     if  rank not in group group           return      opt = alltoalloptions        check tensor list output tensor list  string       check tensor list input tensor list  string       if group be none          default pg =  get default group           work = default pg alltoall output tensor list  input tensor list  opt      else          work = group alltoall output tensor list  input tensor list  opt       if async op          return work     else          work wait   
def barrier group=groupmember world              async op=false              device ids=none        string     if  rank not in group group           return      opt = barrieroptions       if device ids be not none          if get backend group   = backend nccl              raise runtimeerror string                                string format get backend group            if isinstance device ids  list               opt device ids = device ids         else              raise runtimeerror string                                string       if group be none          default pg =  get default group           work = default pg barrier opts=opts      else          work = group barrier opts=opts       if async op          return work     else          work wait   
def monitor barrier group=groupmember world  timeout=none  wait all ranks=false       string                if  rank not in group group           return      if get backend group   = backend gloo          raise runtimeerror              string                if timeout be none          timeout = default pg timeout      group to use =  get default group   if group be none else group     return group to use monitor barrier timeout  wait all ranks=wait all rank  
def broadcast multigpu tensor list                         src                         group=none                         async op=false                         src tensor=0       string     if  rank not in group group           return      opt = broadcastoptions       opt rootrank = src     opt roottensor = src tensor      if group be none or group be groupmember world          default pg =  get default group           work = default pg broadcast tensor list  opt      else          group src rank =  get group rank group  src          opt rootrank = group src rank         work = group broadcast tensor list  opt      if async op          return work     else          work wait   
def all reduce multigpu tensor list                          op=reduceop sum                          group=none                          async op=false       r        reduce the tensor data across all machine in such a way that all get     the final result  this function reduce a number of tensors on every node      while each tensor reside on different gpus      therefore  the input tensor in the tensor list need to be gpu tensors      also  each tensor in the tensor list need to reside on a different gpu       after the call  all ``tensor`` in ``tensor list`` be go to be bitwise     identical in all process       complex tensors be support       only nccl and gloo backend be currently support     tensors should only be gpu tensors      args          tensor list  list tensor    list of input and output tensors of             the collective  the function operate in-place and require that             each tensor to be a gpu tensor on different gpus              you also need to make sure that ``len tensor list `` be the same for             all the distribute process call this function          op  optional   one of the value from             ``torch distribute reduceop``             enum   specify an operation use for element-wise reductions          group  processgroup  optional   the process group to work on  if none              the default process group will be use          async op  bool  optional   whether this op should be an async op      return          async work handle  if async op be set to true          none  if not async op or if not part of the group              if  rank not in group group           return      tensor list =  t if not t be complex   else torch view as real t  for t in tensor list       opt = allreduceoptions       opt reduceop = op     if group be none          default pg =  get default group           work = default pg allreduce tensor list  opt      else          work = group allreduce tensor list  opt       if async op          return work     else          work wait   
def reduce multigpu tensor list                      dst                      op=reduceop sum                      group=none                      async op=false                      dst tensor=0       string     if  rank not in group group           return      opt = reduceoptions       opt reduceop = op     opt rootrank = dst     opt roottensor = dst tensor      if group be none or group be groupmember world          default pg =  get default group           work = default pg reduce tensor list  opt      else          group dst rank =  get group rank group  dst          opt rootrank = group dst rank         work = group reduce tensor list  opt       if async op          return work     else          work wait   
def all gather multigpu output tensor list                          input tensor list                          group=none                          async op=false       string     if  rank not in group group           return      output tensor list =   t if not t be complex   else torch view as real t  for t in l  for l in output tensor list      input tensor list =  t if not t be complex   else torch view as real t  for t in input tensor list       if group be none          default pg =  get default group           work = default pg allgather output tensor list  input tensor list      else          work = group allgather output tensor list  input tensor list       if async op          return work     else          work wait   
def reduce scatter multigpu output tensor list                              input tensor list                              op=reduceop sum                              group=none                              async op=false       string     if  rank not in group group           return      opt = reducescatteroptions       opt reduceop = op      if group be none          default pg =  get default group           work = default pg reduce scatter              output tensor list              input tensor list              opt               else          work = group reduce scatter              output tensor list              input tensor list              opt                if async op          return work     else          work wait   
class distribution object       r        distribution be the abstract base class for probability distributions               have rsample = false     have enumerate support = false      validate args =   debug         staticmethod     def set default validate args value           string         if value not in  true  false               raise valueerror         distribution  validate args = value      def   init   self  batch shape=torch size    event shape=torch size    validate args=none           self  batch shape = batch shape         self  event shape = event shape         if validate args be not none              self  validate args = validate args         if self  validate args              try                  arg constraints = self arg constraints             except notimplementederror                  arg constraints =                    warn warn f  self   class    do not define `arg constraints`                                    string                                 string              for param  constraint in arg constraints items                    if constraints be dependent constraint                       continue                   if param not in self   dict   and isinstance getattr type self   param   lazy property                       continue                   if not constraint check getattr self  param   all                        raise valueerror string format param           super distribution  self    init          def expand self  batch shape   instance=none           string         raise notimplementederror       property     def batch shape self           string         return self  batch shape       property     def event shape self           string         return self  event shape       property     def arg constraints self  -> dict str  constraints constraint           string         raise notimplementederror       property     def support self  -> optional any           string         raise notimplementederror       property     def mean self           string         raise notimplementederror       property     def variance self           string         raise notimplementederror       property     def stddev self           string         return self variance sqrt        def sample self  sample shape=torch size             string         with torch no grad                return self rsample sample shape       def rsample self  sample shape=torch size             string         raise notimplementederror      def sample n self  n           string         warn warn string  userwarning          return self sample torch size  n          def log prob self  value           string         raise notimplementederror      def cdf self  value           string         raise notimplementederror      def icdf self  value           string         raise notimplementederror      def enumerate support self  expand=true           string         raise notimplementederror      def entropy self           string         raise notimplementederror      def perplexity self           string         return torch exp self entropy         def  extend shape self  sample shape=torch size             string         if not isinstance sample shape  torch size               sample shape = torch size sample shape          return sample shape   self  batch shape   self  event shape      def  validate sample self  value           string         if not isinstance value  torch tensor               raise valueerror string           event dim start = len value size    - len self  event shape          if value size   event dim start    = self  event shape              raise valueerror string                               format value size    self  event shape            actual shape = value size           expect shape = self  batch shape   self  event shape         for i  j in zip reverse actual shape   reverse expect shape                if i  = 1 and j  = 1 and i  = j                  raise valueerror string                                   format actual shape  expect shape           try              support = self support         except notimplementederror              warn warn f  self   class    do not define `support` to enable                               string                             string              return         assert support be not none         if not support check value  all                raise valueerror string       def  get check instance self  cls   instance=none           if  instance be none and type self    init    = cls   init                raise notimplementederror string                                       string                                        format self   class     name    cls   name             return self   new   type self   if  instance be none else  instance      def   repr   self           param name =  k for k    in self arg constraints items   if k in self   dict            args string = string join  string format p  self   dict   p                                  if self   dict   p  numel   == 1                                 else self   dict   p  size    for p in param name           return self   class     name     string   args string   string 
class exponentialfamily distribution       r        exponentialfamily be the abstract base class for probability distributions belong to an     exponential family  whose probability mass/density function have the form be define below         math            p  f  x  \theta  = \exp \langle t x   \theta\rangle - f \theta    k x        where  math `\theta` denote the natural parameters   math `t x ` denote the sufficient statistic       math `f \theta ` be the log normalizer function for a give family and  math `k x ` be the carrier     measure       note          this class be an intermediary between the `distribution` class and distributions which belong         to an exponential family mainly to check the correctness of the ` entropy  ` and analytic kl         divergence methods  we use this class to compute the entropy and kl divergence use the ad         framework and bregman divergences  courtesy of  frank nielsen and richard nock  entropies and         cross-entropies of exponential families                 property     def  natural params self           string         raise notimplementederror      def  log normalizer self   natural params           string         raise notimplementederror       property     def  mean carrier measure self           string         raise notimplementederror      def entropy self           string         result = -self  mean carrier measure         nparams =  p detach   require grad    for p in self  natural params          lg normal = self  log normalizer  nparams          gradients = torch autograd grad lg normal sum    nparams  create graph=true          result  = lg normal         for np  g in zip nparams  gradients               result -= np   g         return result 
class bernoulli exponentialfamily       r        create a bernoulli distribution parameterized by  attr `probs`     or  attr `logits`  but not both        sample be binary  0 or 1   they take the value `1` with probability `p`     and `0` with probability `1 - p`       example            >>> m = bernoulli torch tensor  0 3            >>> m sample      30  chance 1  70  chance 0         tensor   0         args          probs  number  tensor   the probability of sample `1`         logits  number  tensor   the log-odds of sample `1`             arg constraints =  string  constraints unit interval                         string  constraints real      support = constraints boolean     have enumerate support = true      mean carrier measure = 0      def   init   self  probs=none  logits=none  validate args=none           if  probs be none  ==  logits be none               raise valueerror string          if probs be not none              be scalar = isinstance probs  number              self probs  = broadcast all probs          else              be scalar = isinstance logits  number              self logits  = broadcast all logits          self  param = self probs if probs be not none else self logits         if be scalar              batch shape = torch size           else              batch shape = self  param size           super bernoulli  self    init   batch shape  validate args=validate args       def expand self  batch shape   instance=none           new = self  get check instance bernoulli   instance          batch shape = torch size batch shape          if string in self   dict                new probs = self probs expand batch shape              new  param = new probs         if string in self   dict                new logits = self logits expand batch shape              new  param = new logits         super bernoulli  new    init   batch shape  validate args=false          new  validate args = self  validate args         return new      def  new self   args    kwargs           return self  param new  args    kwargs        property     def mean self           return self probs       property     def variance self           return self probs    1 - self probs        lazy property     def logits self           return probs to logits self probs  be binary=true        lazy property     def probs self           return logits to probs self logits  be binary=true        property     def param shape self           return self  param size        def sample self  sample shape=torch size             shape = self  extend shape sample shape          with torch no grad                return torch bernoulli self probs expand shape        def log prob self  value           if self  validate args              self  validate sample value          logits  value = broadcast all self logits  value          return -binary cross entropy with logits logits  value  reduction=string       def entropy self           return binary cross entropy with logits self logits  self probs  reduction=string       def enumerate support self  expand=true           value = torch arange 2  dtype=self  param dtype  device=self  param device          value = value view  -1      1     len self  batch shape           if expand              value = value expand  -1     self  batch shape          return value       property     def  natural params self           return  torch log self probs /  1 - self probs           def  log normalizer self  x           return torch log 1   torch exp x   
class beta exponentialfamily       r        beta distribution parameterized by  attr `concentration1` and  attr `concentration0`       example            >>> m = beta torch tensor  0 5    torch tensor  0 5            >>> m sample      beta distribute with concentration concentration1 and concentration0         tensor   0 1046        args          concentration1  float or tensor   1st concentration parameter of the distribution              often refer to as alpha          concentration0  float or tensor   2nd concentration parameter of the distribution              often refer to as beta              arg constraints =  string  constraints positive  string  constraints positive      support = constraints unit interval     have rsample = true      def   init   self  concentration1  concentration0  validate args=none           if isinstance concentration1  real  and isinstance concentration0  real               concentration1 concentration0 = torch tensor  float concentration1   float concentration0            else              concentration1  concentration0 = broadcast all concentration1  concentration0              concentration1 concentration0 = torch stack  concentration1  concentration0   -1          self  dirichlet = dirichlet concentration1 concentration0  validate args=validate args          super beta  self    init   self  dirichlet  batch shape  validate args=validate args       def expand self  batch shape   instance=none           new = self  get check instance beta   instance          batch shape = torch size batch shape          new  dirichlet = self  dirichlet expand batch shape          super beta  new    init   batch shape  validate args=false          new  validate args = self  validate args         return new       property     def mean self           return self concentration1 /  self concentration1   self concentration0        property     def variance self           total = self concentration1   self concentration0         return  self concentration1   self concentration0 /                  total pow 2     total   1         def rsample self  sample shape=             return self  dirichlet rsample sample shape  select -1  0       def log prob self  value           if self  validate args              self  validate sample value          head tail = torch stack  value  1 0 - value   -1          return self  dirichlet log prob head tail       def entropy self           return self  dirichlet entropy         property     def concentration1 self           result = self  dirichlet concentration      0          if isinstance result  number               return torch tensor  result           else              return result       property     def concentration0 self           result = self  dirichlet concentration      1          if isinstance result  number               return torch tensor  result           else              return result       property     def  natural params self           return  self concentration1  self concentration0       def  log normalizer self  x  y           return torch lgamma x    torch lgamma y  - torch lgamma x   y  
class binomial distribution       r        create a binomial distribution parameterized by  attr `total count` and     either  attr `probs` or  attr `logits`  but not both    attr `total count` must be     broadcastable with  attr `probs`/ attr `logits`       example            >>> m = binomial 100  torch tensor  0    2   8  1            >>> x = m sample           tensor     0     22     71    100             >>> m = binomial torch tensor   5     10      torch tensor  0 5  0 8            >>> x = m sample           tensor    4    5                      7    6          args          total count  int or tensor   number of bernoulli trials         probs  tensor   event probabilities         logits  tensor   event log-odds             arg constraints =  string  constraints nonnegative integer                         string  constraints unit interval                         string  constraints real      have enumerate support = true      def   init   self  total count=1  probs=none  logits=none  validate args=none           if  probs be none  ==  logits be none               raise valueerror string          if probs be not none              self total count  self probs  = broadcast all total count  probs              self total count = self total count type as self probs          else              self total count  self logits  = broadcast all total count  logits              self total count = self total count type as self logits           self  param = self probs if probs be not none else self logits         batch shape = self  param size           super binomial  self    init   batch shape  validate args=validate args       def expand self  batch shape   instance=none           new = self  get check instance binomial   instance          batch shape = torch size batch shape          new total count = self total count expand batch shape          if string in self   dict                new probs = self probs expand batch shape              new  param = new probs         if string in self   dict                new logits = self logits expand batch shape              new  param = new logits         super binomial  new    init   batch shape  validate args=false          new  validate args = self  validate args         return new      def  new self   args    kwargs           return self  param new  args    kwargs        constraints dependent property be discrete=true  event dim=0      def support self           return constraints integer interval 0  self total count        property     def mean self           return self total count   self probs       property     def variance self           return self total count   self probs    1 - self probs        lazy property     def logits self           return probs to logits self probs  be binary=true        lazy property     def probs self           return logits to probs self logits  be binary=true        property     def param shape self           return self  param size        def sample self  sample shape=torch size             shape = self  extend shape sample shape          with torch no grad                return torch binomial self total count expand shape   self probs expand shape        def log prob self  value           if self  validate args              self  validate sample value          log factorial n = torch lgamma self total count   1          log factorial k = torch lgamma value   1          log factorial nmk = torch lgamma self total count - value   1                                                       normalize term =  self total count    clamp by zero self logits                              self total count   torch log1p torch exp -torch abs self logits                              - log factorial n          return value   self logits - log factorial k - log factorial nmk - normalize term      def enumerate support self  expand=true           total count = int self total count max            if not self total count min   == total count              raise notimplementederror string          value = torch arange 1   total count  dtype=self  param dtype  device=self  param device          value = value view  -1      1     len self  batch shape           if expand              value = value expand  -1     self  batch shape          return value 
class categorical distribution       r        create a categorical distribution parameterized by either  attr `probs` or      attr `logits`  but not both           note           it be equivalent to the distribution that  func `torch multinomial`         sample from       sample be integers from  math `\ 0  \ldots  k-1\ ` where `k` be ``probs size -1 ``       if `probs` be 1-dimensional with length-`k`  each element be the relative probability     of sample the class at that index       if `probs` be n-dimensional  the first n-1 dimension be treat as a batch of     relative probability vectors          note   the `probs` argument must be non-negative  finite and have a non-zero sum                and it will be normalize to sum to 1 along the last dimension  attr `probs`               will return this normalize value                the `logits` argument will be interpret as unnormalized log probabilities               and can therefore be any real number  it will likewise be normalize so that               the result probabilities sum to 1 along the last dimension  attr `logits`               will return this normalize value       see also   func `torch multinomial`      example            >>> m = categorical torch tensor   0 25  0 25  0 25  0 25             >>> m sample      equal probability of 0  1  2  3         tensor 3       args          probs  tensor   event probabilities         logits  tensor   event log probabilities  unnormalized              arg constraints =  string  constraints simplex                         string  constraints real vector      have enumerate support = true      def   init   self  probs=none  logits=none  validate args=none           if  probs be none  ==  logits be none               raise valueerror string          if probs be not none              if probs dim   < 1                  raise valueerror string              self probs = probs / probs sum -1  keepdim=true          else              if logits dim   < 1                  raise valueerror string                           self logits = logits - logits logsumexp dim=-1  keepdim=true          self  param = self probs if probs be not none else self logits         self  num events = self  param size   -1          batch shape = self  param size    -1  if self  param ndimension   > 1 else torch size           super categorical  self    init   batch shape  validate args=validate args       def expand self  batch shape   instance=none           new = self  get check instance categorical   instance          batch shape = torch size batch shape          param shape = batch shape   torch size  self  num events            if string in self   dict                new probs = self probs expand param shape              new  param = new probs         if string in self   dict                new logits = self logits expand param shape              new  param = new logits         new  num events = self  num events         super categorical  new    init   batch shape  validate args=false          new  validate args = self  validate args         return new      def  new self   args    kwargs           return self  param new  args    kwargs        constraints dependent property be discrete=true  event dim=0      def support self           return constraints integer interval 0  self  num events - 1        lazy property     def logits self           return probs to logits self probs        lazy property     def probs self           return logits to probs self logits        property     def param shape self           return self  param size         property     def mean self           return torch full self  extend shape    nan  dtype=self probs dtype  device=self probs device        property     def variance self           return torch full self  extend shape    nan  dtype=self probs dtype  device=self probs device       def sample self  sample shape=torch size             if not isinstance sample shape  torch size               sample shape = torch size sample shape          probs 2d = self probs reshape -1  self  num events          sample 2d = torch multinomial probs 2d  sample shape numel    true  t         return sample 2d reshape self  extend shape sample shape        def log prob self  value           if self  validate args              self  validate sample value          value = value long   unsqueeze -1          value  log pmf = torch broadcast tensors value  self logits          value = value       1          return log pmf gather -1  value  squeeze -1       def entropy self           min real = torch finfo self logits dtype  min         logits = torch clamp self logits  min=min real          p log p = logits   self probs         return -p log p sum -1       def enumerate support self  expand=true           num events = self  num events         value = torch arange num events  dtype=torch long  device=self  param device          value = value view  -1      1     len self  batch shape           if expand              value = value expand  -1     self  batch shape          return value 
class cauchy distribution       r        sample from a cauchy  lorentz  distribution  the distribution of the ratio of     independent normally distribute random variables with mean `0` follow a     cauchy distribution       example            >>> m = cauchy torch tensor  0 0    torch tensor  1 0            >>> m sample      sample from a cauchy distribution with loc=0 and scale=1         tensor   2 3214        args          loc  float or tensor   mode or median of the distribution          scale  float or tensor   half width at half maximum              arg constraints =  string  constraints real  string  constraints positive      support = constraints real     have rsample = true      def   init   self  loc  scale  validate args=none           self loc  self scale = broadcast all loc  scale          if isinstance loc  number  and isinstance scale  number               batch shape = torch size           else              batch shape = self loc size           super cauchy  self    init   batch shape  validate args=validate args       def expand self  batch shape   instance=none           new = self  get check instance cauchy   instance          batch shape = torch size batch shape          new loc = self loc expand batch shape          new scale = self scale expand batch shape          super cauchy  new    init   batch shape  validate args=false          new  validate args = self  validate args         return new       property     def mean self           return torch full self  extend shape    nan  dtype=self loc dtype  device=self loc device        property     def variance self           return torch full self  extend shape    inf  dtype=self loc dtype  device=self loc device       def rsample self  sample shape=torch size             shape = self  extend shape sample shape          eps = self loc new shape  cauchy            return self loc   eps   self scale      def log prob self  value           if self  validate args              self  validate sample value          return -math log math pi  - self scale log   -  1     value - self loc  / self scale   2  log        def cdf self  value           if self  validate args              self  validate sample value          return torch atan  value - self loc  / self scale  / math pi   0 5      def icdf self  value           return torch tan math pi    value - 0 5     self scale   self loc      def entropy self           return math log 4   math pi    self scale log   
class chi2 gamma       r        create a chi2 distribution parameterized by shape parameter  attr `df`      this be exactly equivalent to ``gamma alpha=0 5 df  beta=0 5 ``      example            >>> m = chi2 torch tensor  1 0            >>> m sample      chi2 distribute with shape df=1         tensor   0 1046        args          df  float or tensor   shape parameter of the distribution             arg constraints =  string  constraints positive       def   init   self  df  validate args=none           super chi2  self    init   0 5   df  0 5  validate args=validate args       def expand self  batch shape   instance=none           new = self  get check instance chi2   instance          return super chi2  self  expand batch shape  new        property     def df self           return self concentration   2 
class continuousbernoulli exponentialfamily       r        create a continuous bernoulli distribution parameterized by  attr `probs`     or  attr `logits`  but not both        the distribution be support in  0  1  and parameterized by  probs   in      0 1   or  logits   real-valued   note that  unlike the bernoulli   probs      do not correspond to a probability and  logits  do not correspond to     log-odds  but the same name be use due to the similarity with the     bernoulli  see  1  for more detail       example            >>> m = continuousbernoulli torch tensor  0 3            >>> m sample           tensor   0 2538        args          probs  number  tensor    0 1  value parameters         logits  number  tensor   real value parameters whose sigmoid match  probs        1  the continuous bernoulli  fix a pervasive error in variational     autoencoders  loaiza-ganem g and cunningham jp  neurips 2019      https //arxiv org/abs/1907 06845             arg constraints =  string  constraints unit interval                         string  constraints real      support = constraints unit interval      mean carrier measure = 0     have rsample = true      def   init   self  probs=none  logits=none  lims= 0 499  0 501   validate args=none           if  probs be none  ==  logits be none               raise valueerror string          if probs be not none              be scalar = isinstance probs  number              self probs  = broadcast all probs                                        if validate args be not none                  if not self arg constraints string  check getattr self  string   all                        raise valueerror string format string               self probs = clamp probs self probs          else              be scalar = isinstance logits  number              self logits  = broadcast all logits          self  param = self probs if probs be not none else self logits         if be scalar              batch shape = torch size           else              batch shape = self  param size           self  lims = lims         super continuousbernoulli  self    init   batch shape  validate args=validate args       def expand self  batch shape   instance=none           new = self  get check instance continuousbernoulli   instance          new  lims = self  lims         batch shape = torch size batch shape          if string in self   dict                new probs = self probs expand batch shape              new  param = new probs         if string in self   dict                new logits = self logits expand batch shape              new  param = new logits         super continuousbernoulli  new    init   batch shape  validate args=false          new  validate args = self  validate args         return new      def  new self   args    kwargs           return self  param new  args    kwargs       def  outside unstable region self           return torch max torch le self probs  self  lims 0                             torch gt self probs  self  lims 1         def  cut probs self           return torch where self  outside unstable region                               self probs                             self  lims 0    torch ones like self probs        def  cont bern log norm self           string         cut probs = self  cut probs           cut probs below half = torch where torch le cut probs  0 5                                              cut probs                                             torch zero like cut probs           cut probs above half = torch where torch ge cut probs  0 5                                              cut probs                                             torch ones like cut probs           log norm = torch log torch abs torch log1p -cut probs  - torch log cut probs    - torch where              torch le cut probs  0 5               torch log1p -2 0   cut probs below half               torch log 2 0   cut probs above half - 1 0           x = torch pow self probs - 0 5  2          taylor = math log 2 0     4 0 / 3 0   104 0 / 45 0   x    x         return torch where self  outside unstable region    log norm  taylor        property     def mean self           cut probs = self  cut probs           mus = cut probs /  2 0   cut probs - 1 0    1 0 /  torch log1p -cut probs  - torch log cut probs           x = self probs - 0 5         taylor = 0 5    1 0 / 3 0   16 0 / 45 0   torch pow x  2     x         return torch where self  outside unstable region    mus  taylor        property     def stddev self           return torch sqrt self variance        property     def variance self           cut probs = self  cut probs           vars = cut probs    cut probs - 1 0  / torch pow 1 0 - 2 0   cut probs  2    1 0 / torch pow              torch log1p -cut probs  - torch log cut probs   2          x = torch pow self probs - 0 5  2          taylor = 1 0 / 12 0 -  1 0 / 15 0 - 128  / 945 0   x    x         return torch where self  outside unstable region    vars  taylor        lazy property     def logits self           return probs to logits self probs  be binary=true        lazy property     def probs self           return clamp probs logits to probs self logits  be binary=true         property     def param shape self           return self  param size        def sample self  sample shape=torch size             shape = self  extend shape sample shape          u = torch rand shape  dtype=self probs dtype  device=self probs device          with torch no grad                return self icdf u       def rsample self  sample shape=torch size             shape = self  extend shape sample shape          u = torch rand shape  dtype=self probs dtype  device=self probs device          return self icdf u       def log prob self  value           if self  validate args              self  validate sample value          logits  value = broadcast all self logits  value          return -binary cross entropy with logits logits  value  reduction=string    self  cont bern log norm        def cdf self  value           if self  validate args              self  validate sample value          cut probs = self  cut probs           cdfs =  torch pow cut probs  value    torch pow 1 0 - cut probs  1 0 - value                    cut probs - 1 0  /  2 0   cut probs - 1 0          unbounded cdfs = torch where self  outside unstable region    cdfs  value          return torch where              torch le value  0 0               torch zero like value               torch where torch ge value  1 0   torch ones like value   unbounded cdfs        def icdf self  value           cut probs = self  cut probs           return torch where              self  outside unstable region                 torch log1p -cut probs   value    2 0   cut probs - 1 0                - torch log1p -cut probs   /  torch log cut probs  - torch log1p -cut probs                value       def entropy self           log probs0 = torch log1p -self probs          log probs1 = torch log self probs          return self mean    log probs0 - log probs1  - self  cont bern log norm   - log probs0       property     def  natural params self           return  self logits         def  log normalizer self  x           string         out unst reg = torch max torch le x  self  lims 0  - 0 5                                    torch gt x  self  lims 1  - 0 5           cut nat params = torch where out unst reg                                       x                                        self  lims 0  - 0 5    torch ones like x           log norm = torch log torch abs torch exp cut nat params  - 1 0   - torch log torch abs cut nat params           taylor = 0 5   x   torch pow x  2  / 24 0 - torch pow x  4  / 2880 0         return torch where out unst reg  log norm  taylor  
class dirichlet exponentialfamily       r        create a dirichlet distribution parameterized by concentration  attr `concentration`       example            >>> m = dirichlet torch tensor  0 5  0 5            >>> m sample      dirichlet distribute with concentrarion concentration         tensor   0 1046   0 8954        args          concentration  tensor   concentration parameter of the distribution              often refer to as alpha              arg constraints =  string  constraints independent constraints positive  1       support = constraints simplex     have rsample = true      def   init   self  concentration  validate args=none           if concentration dim   < 1              raise valueerror string          self concentration = concentration         batch shape  event shape = concentration shape  -1   concentration shape -1           super dirichlet  self    init   batch shape  event shape  validate args=validate args       def expand self  batch shape   instance=none           new = self  get check instance dirichlet   instance          batch shape = torch size batch shape          new concentration = self concentration expand batch shape   self event shape          super dirichlet  new    init   batch shape  self event shape  validate args=false          new  validate args = self  validate args         return new      def rsample self  sample shape=             shape = self  extend shape sample shape          concentration = self concentration expand shape          return  dirichlet apply concentration       def log prob self  value           if self  validate args              self  validate sample value          return   torch log value     self concentration - 1 0   sum -1                    torch lgamma self concentration sum -1   -                 torch lgamma self concentration  sum -1         property     def mean self           return self concentration / self concentration sum -1  true        property     def variance self           con0 = self concentration sum -1  true          return self concentration    con0 - self concentration  /  con0 pow 2     con0   1        def entropy self           k = self concentration size -1          a0 = self concentration sum -1          return  torch lgamma self concentration  sum -1  - torch lgamma a0  -                  k - a0    torch digamma a0  -                   self concentration - 1 0    torch digamma self concentration   sum -1         property     def  natural params self           return  self concentration         def  log normalizer self  x           return x lgamma   sum -1  - torch lgamma x sum -1   
class exponential exponentialfamily       r        create a exponential distribution parameterized by  attr `rate`       example            >>> m = exponential torch tensor  1 0            >>> m sample      exponential distribute with rate=1         tensor   0 1046        args          rate  float or tensor   rate = 1 / scale of the distribution             arg constraints =  string  constraints positive      support = constraints positive     have rsample = true      mean carrier measure = 0       property     def mean self           return self rate reciprocal         property     def stddev self           return self rate reciprocal         property     def variance self           return self rate pow -2       def   init   self  rate  validate args=none           self rate  = broadcast all rate          batch shape = torch size   if isinstance rate  number  else self rate size           super exponential  self    init   batch shape  validate args=validate args       def expand self  batch shape   instance=none           new = self  get check instance exponential   instance          batch shape = torch size batch shape          new rate = self rate expand batch shape          super exponential  new    init   batch shape  validate args=false          new  validate args = self  validate args         return new      def rsample self  sample shape=torch size             shape = self  extend shape sample shape          if torch  c  get trace state                             u = torch rand shape  dtype=self rate dtype  device=self rate device              return - -u  log1p   / self rate         return self rate new shape  exponential    / self rate      def log prob self  value           if self  validate args              self  validate sample value          return self rate log   - self rate   value      def cdf self  value           if self  validate args              self  validate sample value          return 1 - torch exp -self rate   value       def icdf self  value           return -torch log 1 - value  / self rate      def entropy self           return 1 0 - torch log self rate        property     def  natural params self           return  -self rate         def  log normalizer self  x           return -torch log -x  
class fishersnedecor distribution       r        create a fisher-snedecor distribution parameterized by  attr `df1` and  attr `df2`       example            >>> m = fishersnedecor torch tensor  1 0    torch tensor  2 0            >>> m sample      fisher-snedecor-distributed with df1=1 and df2=2         tensor   0 2453        args          df1  float or tensor   degrees of freedom parameter 1         df2  float or tensor   degrees of freedom parameter 2             arg constraints =  string  constraints positive  string  constraints positive      support = constraints positive     have rsample = true      def   init   self  df1  df2  validate args=none           self df1  self df2 = broadcast all df1  df2          self  gamma1 = gamma self df1   0 5  self df1          self  gamma2 = gamma self df2   0 5  self df2           if isinstance df1  number  and isinstance df2  number               batch shape = torch size           else              batch shape = self df1 size           super fishersnedecor  self    init   batch shape  validate args=validate args       def expand self  batch shape   instance=none           new = self  get check instance fishersnedecor   instance          batch shape = torch size batch shape          new df1 = self df1 expand batch shape          new df2 = self df2 expand batch shape          new  gamma1 = self  gamma1 expand batch shape          new  gamma2 = self  gamma2 expand batch shape          super fishersnedecor  new    init   batch shape  validate args=false          new  validate args = self  validate args         return new       property     def mean self           df2 = self df2 clone memory format=torch contiguous format          df2 df2 <= 2  = nan         return df2 /  df2 - 2        property     def variance self           df2 = self df2 clone memory format=torch contiguous format          df2 df2 <= 4  = nan         return 2   df2 pow 2     self df1   df2 - 2  /  self df1    df2 - 2  pow 2     df2 - 4        def rsample self  sample shape=torch size               shape = self  extend shape sample shape                            x1 = self  gamma1 rsample sample shape  view shape          x2 = self  gamma2 rsample sample shape  view shape          tiny = torch finfo x2 dtype  tiny         x2 clamp  min=tiny          y = x1 / x2         y clamp  min=tiny          return y      def log prob self  value           if self  validate args              self  validate sample value          ct1 = self df1   0 5         ct2 = self df2   0 5         ct3 = self df1 / self df2         t1 =  ct1   ct2  lgamma   - ct1 lgamma   - ct2 lgamma           t2 = ct1   ct3 log      ct1 - 1    torch log value          t3 =  ct1   ct2    torch log1p ct3   value          return t1   t2 - t3 
class gamma exponentialfamily       r        create a gamma distribution parameterized by shape  attr `concentration` and  attr `rate`       example            >>> m = gamma torch tensor  1 0    torch tensor  1 0            >>> m sample      gamma distribute with concentration=1 and rate=1         tensor   0 1046        args          concentration  float or tensor   shape parameter of the distribution              often refer to as alpha          rate  float or tensor   rate = 1 / scale of the distribution              often refer to as beta              arg constraints =  string  constraints positive  string  constraints positive      support = constraints positive     have rsample = true      mean carrier measure = 0       property     def mean self           return self concentration / self rate       property     def variance self           return self concentration / self rate pow 2       def   init   self  concentration  rate  validate args=none           self concentration  self rate = broadcast all concentration  rate          if isinstance concentration  number  and isinstance rate  number               batch shape = torch size           else              batch shape = self concentration size           super gamma  self    init   batch shape  validate args=validate args       def expand self  batch shape   instance=none           new = self  get check instance gamma   instance          batch shape = torch size batch shape          new concentration = self concentration expand batch shape          new rate = self rate expand batch shape          super gamma  new    init   batch shape  validate args=false          new  validate args = self  validate args         return new      def rsample self  sample shape=torch size             shape = self  extend shape sample shape          value =  standard gamma self concentration expand shape   / self rate expand shape          value detach   clamp  min=torch finfo value dtype  tiny            return value      def log prob self  value           value = torch as tensor value  dtype=self rate dtype  device=self rate device          if self  validate args              self  validate sample value          return  self concentration   torch log self rate                     self concentration - 1    torch log value  -                 self rate   value - torch lgamma self concentration        def entropy self           return  self concentration - torch log self rate    torch lgamma self concentration                     1 0 - self concentration    torch digamma self concentration         property     def  natural params self           return  self concentration - 1  -self rate       def  log normalizer self  x  y           return torch lgamma x   1     x   1    torch log -y reciprocal    
class geometric distribution       r        create a geometric distribution parameterized by  attr `probs`      where  attr `probs` be the probability of success of bernoulli trials      it represent the probability that in  math `k   1` bernoulli trials  the     first  math `k` trials fail  before see a success       sample be non-negative integers  0   math `\inf`        example            >>> m = geometric torch tensor  0 3            >>> m sample      underlie bernoulli have 30  chance 1  70  chance 0         tensor   2         args          probs  number  tensor   the probability of sample `1`  must be in range  0  1          logits  number  tensor   the log-odds of sample `1`              arg constraints =  string  constraints unit interval                         string  constraints real      support = constraints nonnegative integer      def   init   self  probs=none  logits=none  validate args=none           if  probs be none  ==  logits be none               raise valueerror string          if probs be not none              self probs  = broadcast all probs              if not self probs gt 0  all                    raise valueerror string          else              self logits  = broadcast all logits          probs or logits = probs if probs be not none else logits         if isinstance probs or logits  number               batch shape = torch size           else              batch shape = probs or logits size           super geometric  self    init   batch shape  validate args=validate args       def expand self  batch shape   instance=none           new = self  get check instance geometric   instance          batch shape = torch size batch shape          if string in self   dict                new probs = self probs expand batch shape          if string in self   dict                new logits = self logits expand batch shape          super geometric  new    init   batch shape  validate args=false          new  validate args = self  validate args         return new       property     def mean self           return 1  / self probs - 1        property     def variance self           return  1  / self probs - 1   / self probs       lazy property     def logits self           return probs to logits self probs  be binary=true        lazy property     def probs self           return logits to probs self logits  be binary=true       def sample self  sample shape=torch size             shape = self  extend shape sample shape          tiny = torch finfo self probs dtype  tiny         with torch no grad                if torch  c  get trace state                                     u = torch rand shape  dtype=self probs dtype  device=self probs device                  u = u clamp min=tiny              else                  u = self probs new shape  uniform  tiny  1              return  u log   /  -self probs  log1p    floor        def log prob self  value           if self  validate args              self  validate sample value          value  probs = broadcast all value  self probs          probs = probs clone memory format=torch contiguous format          probs  probs == 1     value == 0   = 0         return value    -probs  log1p     self probs log        def entropy self           return binary cross entropy with logits self logits  self probs  reduction=string  / self probs 
class gumbel transformeddistribution       r        sample from a gumbel distribution       examples            >>> m = gumbel torch tensor  1 0    torch tensor  2 0            >>> m sample      sample from gumbel distribution with loc=1  scale=2         tensor   1 0124        args          loc  float or tensor   location parameter of the distribution         scale  float or tensor   scale parameter of the distribution             arg constraints =  string  constraints real  string  constraints positive      support = constraints real      def   init   self  loc  scale  validate args=none           self loc  self scale = broadcast all loc  scale          finfo = torch finfo self loc dtype          if isinstance loc  number  and isinstance scale  number               base dist = uniform finfo tiny  1 - finfo eps          else              base dist = uniform torch full like self loc  finfo tiny                                   torch full like self loc  1 - finfo eps           transform =  exptransform   inv  affinetransform loc=0  scale=-torch ones like self scale                          exptransform   inv  affinetransform loc=loc  scale=-self scale           super gumbel  self    init   base dist  transform  validate args=validate args       def expand self  batch shape   instance=none           new = self  get check instance gumbel   instance          new loc = self loc expand batch shape          new scale = self scale expand batch shape          return super gumbel  self  expand batch shape   instance=new            def log prob self  value           if self  validate args              self  validate sample value          y =  self loc - value  / self scale         return  y - y exp    - self scale log         property     def mean self           return self loc   self scale   euler constant       property     def stddev self           return  math pi / math sqrt 6     self scale       property     def variance self           return self stddev pow 2       def entropy self           return self scale log      1   euler constant  
class halfcauchy transformeddistribution       r        create a half-cauchy distribution parameterized by `scale` where            x ~ cauchy 0  scale          y =  x  ~ halfcauchy scale       example            >>> m = halfcauchy torch tensor  1 0            >>> m sample      half-cauchy distribute with scale=1         tensor   2 3214        args          scale  float or tensor   scale of the full cauchy distribution             arg constraints =  string  constraints positive      support = constraints positive     have rsample = true      def   init   self  scale  validate args=none           base dist = cauchy 0  scale  validate args=false          super halfcauchy  self    init   base dist  abstransform                                             validate args=validate args       def expand self  batch shape   instance=none           new = self  get check instance halfcauchy   instance          return super halfcauchy  self  expand batch shape   instance=new        property     def scale self           return self base dist scale       property     def mean self           return torch full self  extend shape    math inf  dtype=self scale dtype  device=self scale device        property     def variance self           return self base dist variance      def log prob self  value           if self  validate args              self  validate sample value          value = torch as tensor value  dtype=self base dist scale dtype                                  device=self base dist scale device          log prob = self base dist log prob value    math log 2          log prob value expand log prob shape  < 0  = -inf         return log prob      def cdf self  value           if self  validate args              self  validate sample value          return 2   self base dist cdf value  - 1      def icdf self  prob           return self base dist icdf  prob   1  / 2       def entropy self           return self base dist entropy   - math log 2  
class halfnormal transformeddistribution       r        create a half-normal distribution parameterized by `scale` where            x ~ normal 0  scale          y =  x  ~ halfnormal scale       example            >>> m = halfnormal torch tensor  1 0            >>> m sample      half-normal distribute with scale=1         tensor   0 1046        args          scale  float or tensor   scale of the full normal distribution             arg constraints =  string  constraints positive      support = constraints positive     have rsample = true      def   init   self  scale  validate args=none           base dist = normal 0  scale  validate args=false          super halfnormal  self    init   base dist  abstransform                                             validate args=validate args       def expand self  batch shape   instance=none           new = self  get check instance halfnormal   instance          return super halfnormal  self  expand batch shape   instance=new        property     def scale self           return self base dist scale       property     def mean self           return self scale   math sqrt 2 / math pi        property     def variance self           return self scale pow 2     1 - 2 / math pi       def log prob self  value           if self  validate args              self  validate sample value          log prob = self base dist log prob value    math log 2          log prob value expand log prob shape  < 0  = -inf         return log prob      def cdf self  value           if self  validate args              self  validate sample value          return 2   self base dist cdf value  - 1      def icdf self  prob           return self base dist icdf  prob   1  / 2       def entropy self           return self base dist entropy   - math log 2  
class independent distribution       r        reinterpret some of the batch dim of a distribution as event dim       this be mainly useful for change the shape of the result of      meth `log prob`  for example to create a diagonal normal distribution with     the same shape as a multivariate normal distribution  so they be     interchangeable   you can            >>> loc = torch zero 3          >>> scale = torch ones 3          >>> mvn = multivariatenormal loc  scale tril=torch diag scale           >>>  mvn batch shape  mvn event shape           torch size      torch size  3             >>> normal = normal loc  scale          >>>  normal batch shape  normal event shape           torch size  3     torch size              >>> diagn = independent normal  1          >>>  diagn batch shape  diagn event shape           torch size      torch size  3          args          base distribution  torch distributions distribution distribution   a             base distribution         reinterpret batch ndims  int   the number of batch dim to             reinterpret as event dim             arg constraints  dict str  constraints constraint  =         def   init   self  base distribution  reinterpret batch ndims  validate args=none           if reinterpret batch ndims > len base distribution batch shape               raise valueerror string                              string format reinterpret batch ndims                                                        len base distribution batch shape            shape = base distribution batch shape   base distribution event shape         event dim = reinterpret batch ndims   len base distribution event shape          batch shape = shape  len shape  - event dim          event shape = shape len shape  - event dim           self base dist = base distribution         self reinterpret batch ndims = reinterpret batch ndims         super independent  self    init   batch shape  event shape  validate args=validate args       def expand self  batch shape   instance=none           new = self  get check instance independent   instance          batch shape = torch size batch shape          new base dist = self base dist expand batch shape                                                 self event shape  self reinterpret batch ndims           new reinterpret batch ndims = self reinterpret batch ndims         super independent  new    init   batch shape  self event shape  validate args=false          new  validate args = self  validate args         return new       property     def have rsample self           return self base dist have rsample       property     def have enumerate support self           if self reinterpret batch ndims > 0              return false         return self base dist have enumerate support       constraints dependent property     def support self           result = self base dist support         if self reinterpret batch ndims              result = constraints independent result  self reinterpret batch ndims          return result       property     def mean self           return self base dist mean       property     def variance self           return self base dist variance      def sample self  sample shape=torch size             return self base dist sample sample shape       def rsample self  sample shape=torch size             return self base dist rsample sample shape       def log prob self  value           log prob = self base dist log prob value          return  sum rightmost log prob  self reinterpret batch ndims       def entropy self           entropy = self base dist entropy           return  sum rightmost entropy  self reinterpret batch ndims       def enumerate support self  expand=true           if self reinterpret batch ndims > 0              raise notimplementederror string          return self base dist enumerate support expand=expand       def   repr   self           return self   class     name     string format self base dist  self reinterpret batch ndims  
class kumaraswamy transformeddistribution       r        sample from a kumaraswamy distribution       example            >>> m = kumaraswamy torch tensor  1 0    torch tensor  1 0            >>> m sample      sample from a kumaraswamy distribution with concentration alpha=1 and beta=1         tensor   0 1729        args          concentration1  float or tensor   1st concentration parameter of the distribution              often refer to as alpha          concentration0  float or tensor   2nd concentration parameter of the distribution              often refer to as beta              arg constraints =  string  constraints positive  string  constraints positive      support = constraints unit interval     have rsample = true      def   init   self  concentration1  concentration0  validate args=none           self concentration1  self concentration0 = broadcast all concentration1  concentration0          finfo = torch finfo self concentration0 dtype          base dist = uniform torch full like self concentration0  0                               torch full like self concentration0  1                               validate args=validate args          transform =  powertransform exponent=self concentration0 reciprocal                           affinetransform loc=1   scale=-1                          powertransform exponent=self concentration1 reciprocal             super kumaraswamy  self    init   base dist  transform  validate args=validate args       def expand self  batch shape   instance=none           new = self  get check instance kumaraswamy   instance          new concentration1 = self concentration1 expand batch shape          new concentration0 = self concentration0 expand batch shape          return super kumaraswamy  self  expand batch shape   instance=new        property     def mean self           return  moments self concentration1  self concentration0  1        property     def variance self           return  moments self concentration1  self concentration0  2  - torch pow self mean  2       def entropy self           t1 =  1 - self concentration1 reciprocal            t0 =  1 - self concentration0 reciprocal            h0 = torch digamma self concentration0   1    euler constant         return t0   t1   h0 - torch log self concentration1  - torch log self concentration0  
class lkjcholesky distribution       r        lkj distribution for lower cholesky factor of correlation matrices      the distribution be control by ``concentration`` parameter  math `\eta`     to make the probability of the correlation matrix  math `m` generate from     a cholesky factor propotional to  math `\det m   \eta - 1 `  because of that      when ``concentration == 1``  we have a uniform distribution over cholesky     factor of correlation matrices  note that this distribution sample the     cholesky factor of correlation matrices and not the correlation matrices     themselves and thereby differ slightly from the derivations in  1  for     the `lkjcorr` distribution  for sample  this use the onion method from      1  section 3           l ~ lkjcholesky dim  concentration          x = l   l  ~ lkjcorr dim  concentration       example            >>> l = lkjcholesky 3  0 5          >>> l sample      l   l t be a sample of a correlation 3x3 matrix         tensor    1 0000   0 0000   0 0000                     0 3516   0 9361   0 0000                    -0 1899   0 4748   0 8593         args          dimension  dim   dimension of the matrices         concentration  float or tensor   concentration/shape parameter of the             distribution  often refer to as eta         reference         1  `generating random correlation matrices base on vines and extend onion method`      daniel lewandowski  dorota kurowicka  harry joe              arg constraints =  string  constraints positive      support = constraints corr cholesky      def   init   self  dim  concentration=1   validate args=none           if dim < 2              raise valueerror f expect dim to be an integer greater than or equal to 2  find dim= dim             self dim = dim         self concentration  = broadcast all concentration          batch shape = self concentration size           event shape = torch size  dim  dim                    marginal conc = self concentration   0 5    self dim - 2          offset = torch arange self dim - 1  dtype=self concentration dtype  device=self concentration device          offset = torch cat  offset new zero  1     offset           beta conc1 = offset   0 5         beta conc0 = marginal conc unsqueeze -1  - 0 5   offset         self  beta = beta beta conc1  beta conc0          super lkjcholesky  self    init   batch shape  event shape  validate args       def expand self  batch shape   instance=none           new = self  get check instance lkjcholesky   instance          batch shape = torch size batch shape          new dim = self dim         new concentration = self concentration expand batch shape          new  beta = self  beta expand batch shape    self dim            super lkjcholesky  new    init   batch shape  self event shape  validate args=false          new  validate args = self  validate args         return new      def sample self  sample shape=torch size                                                                   y = self  beta sample sample shape  unsqueeze -1          u normal = torch randn self  extend shape sample shape                                  dtype=y dtype                                 device=y device  tril -1          u hypersphere = u normal / u normal norm dim=-1  keepdim=true                   u hypersphere      0     fill  0           w = torch sqrt y    u hypersphere                  eps = torch finfo w dtype  tiny         diag elems = torch clamp 1 - torch sum w  2  dim=-1   min=eps  sqrt           w  = torch diag embed diag elems          return w      def log prob self  value                                                                                            if self  validate args              self  validate sample value          diag elems = value diagonal dim1=-1  dim2=-2       1           order = torch arange 2  self dim   1          order = 2    self concentration - 1  unsqueeze -1    self dim - order         unnormalized log pdf = torch sum order   diag elems log    dim=-1                   dm1 = self dim - 1         alpha = self concentration   0 5   dm1         denominator = torch lgamma alpha    dm1         numerator = torch mvlgamma alpha - 0 5  dm1                                     pi constant = 0 5   dm1   math log math pi          normalize term = pi constant   numerator - denominator         return unnormalized log pdf - normalize term 
class laplace distribution       r        create a laplace distribution parameterized by  attr `loc` and  attr `scale`       example            >>> m = laplace torch tensor  0 0    torch tensor  1 0            >>> m sample      laplace distribute with loc=0  scale=1         tensor   0 1046        args          loc  float or tensor   mean of the distribution         scale  float or tensor   scale of the distribution             arg constraints =  string  constraints real  string  constraints positive      support = constraints real     have rsample = true       property     def mean self           return self loc       property     def variance self           return 2   self scale pow 2        property     def stddev self           return  2    0 5    self scale      def   init   self  loc  scale  validate args=none           self loc  self scale = broadcast all loc  scale          if isinstance loc  number  and isinstance scale  number               batch shape = torch size           else              batch shape = self loc size           super laplace  self    init   batch shape  validate args=validate args       def expand self  batch shape   instance=none           new = self  get check instance laplace   instance          batch shape = torch size batch shape          new loc = self loc expand batch shape          new scale = self scale expand batch shape          super laplace  new    init   batch shape  validate args=false          new  validate args = self  validate args         return new      def rsample self  sample shape=torch size             shape = self  extend shape sample shape          finfo = torch finfo self loc dtype          if torch  c  get trace state                             u = torch rand shape  dtype=self loc dtype  device=self loc device    2 - 1             return self loc - self scale   u sign     torch log1p -u abs   clamp min=finfo tiny           u = self loc new shape  uniform  finfo eps - 1  1                            return self loc - self scale   u sign     torch log1p -u abs         def log prob self  value           if self  validate args              self  validate sample value          return -torch log 2   self scale  - torch abs value - self loc  / self scale      def cdf self  value           if self  validate args              self  validate sample value          return 0 5 - 0 5    value - self loc  sign     torch expm1 - value - self loc  abs   / self scale       def icdf self  value           term = value - 0 5         return self loc - self scale    term  sign     torch log1p -2   term abs         def entropy self           return 1   torch log 2   self scale  
class lognormal transformeddistribution       r        create a log-normal distribution parameterized by      attr `loc` and  attr `scale` where            x ~ normal loc  scale          y = exp x  ~ lognormal loc  scale       example            >>> m = lognormal torch tensor  0 0    torch tensor  1 0            >>> m sample      log-normal distribute with mean=0 and stddev=1         tensor   0 1046        args          loc  float or tensor   mean of log of distribution         scale  float or tensor   standard deviation of log of the distribution             arg constraints =  string  constraints real  string  constraints positive      support = constraints positive     have rsample = true      def   init   self  loc  scale  validate args=none           base dist = normal loc  scale  validate args=validate args          super lognormal  self    init   base dist  exptransform    validate args=validate args       def expand self  batch shape   instance=none           new = self  get check instance lognormal   instance          return super lognormal  self  expand batch shape   instance=new        property     def loc self           return self base dist loc       property     def scale self           return self base dist scale       property     def mean self           return  self loc   self scale pow 2  / 2  exp         property     def variance self           return  self scale pow 2  exp   - 1     2   self loc   self scale pow 2   exp        def entropy self           return self base dist entropy     self loc 
class lowrankmultivariatenormal distribution       r        create a multivariate normal distribution with covariance matrix have a low-rank form     parameterized by  attr `cov factor` and  attr `cov diag`            covariance matrix = cov factor   cov factor t   cov diag      example           >>> m = lowrankmultivariatenormal torch zero 2   torch tensor   1     0      torch ones 2           >>> m sample      normally distribute with mean=` 0 0 `  cov factor=`  1   0  `  cov diag=` 1 1 `         tensor  -0 2102  -0 5429        args          loc  tensor   mean of the distribution with shape `batch shape   event shape`         cov factor  tensor   factor part of low-rank form of covariance matrix with shape             `batch shape   event shape    rank  `         cov diag  tensor   diagonal part of low-rank form of covariance matrix with shape             `batch shape   event shape`      note          the computation for determinant and inverse of covariance matrix be avoid when         `cov factor shape 1  << cov factor shape 0 ` thank to `woodbury matrix identity         <https //en wikipedia org/wiki/woodbury matrix identity>`  and         `matrix determinant lemma <https //en wikipedia org/wiki/matrix determinant lemma>`           thank to these formulas  we just need to compute the determinant and inverse of         the small size  capacitance  matrix                capacitance = i   cov factor t   inv cov diag    cov factor             arg constraints =  string  constraints real vector                         string  constraints independent constraints real  2                          string  constraints independent constraints positive  1       support = constraints real vector     have rsample = true      def   init   self  loc  cov factor  cov diag  validate args=none           if loc dim   < 1              raise valueerror string          event shape = loc shape -1           if cov factor dim   < 2              raise valueerror string                              string          if cov factor shape -2 -1   = event shape              raise valueerror string                               format event shape 0            if cov diag shape -1    = event shape              raise valueerror string format event shape            loc  = loc unsqueeze -1          cov diag  = cov diag unsqueeze -1          try              loc   self cov factor  cov diag  = torch broadcast tensors loc   cov factor  cov diag           except runtimeerror as e              raise valueerror string                               format loc shape  cov factor shape  cov diag shape   from e         self loc = loc       0          self cov diag = cov diag       0          batch shape = self loc shape  -1           self  unbroadcasted cov factor = cov factor         self  unbroadcasted cov diag = cov diag         self  capacitance tril =  batch capacitance tril cov factor  cov diag          super lowrankmultivariatenormal  self    init   batch shape  event shape                                                          validate args=validate args       def expand self  batch shape   instance=none           new = self  get check instance lowrankmultivariatenormal   instance          batch shape = torch size batch shape          loc shape = batch shape   self event shape         new loc = self loc expand loc shape          new cov diag = self cov diag expand loc shape          new cov factor = self cov factor expand loc shape   self cov factor shape -1            new  unbroadcasted cov factor = self  unbroadcasted cov factor         new  unbroadcasted cov diag = self  unbroadcasted cov diag         new  capacitance tril = self  capacitance tril         super lowrankmultivariatenormal  new    init   batch shape                                                         self event shape                                                         validate args=false          new  validate args = self  validate args         return new       property     def mean self           return self loc       lazy property     def variance self           return  self  unbroadcasted cov factor pow 2  sum -1                    self  unbroadcasted cov diag  expand self  batch shape   self  event shape        lazy property     def scale tril self                                                        n = self  event shape 0          cov diag sqrt unsqueeze = self  unbroadcasted cov diag sqrt   unsqueeze -1          dinvsqrt w = self  unbroadcasted cov factor / cov diag sqrt unsqueeze         k = torch matmul dinvsqrt w  dinvsqrt w transpose -1  -2   contiguous           k view -1  n   n       n   1   = 1           scale tril = cov diag sqrt unsqueeze   torch linalg cholesky k          return scale tril expand self  batch shape   self  event shape   self  event shape        lazy property     def covariance matrix self           covariance matrix =  torch matmul self  unbroadcasted cov factor                                            self  unbroadcasted cov factor transpose -1  -2                                  torch diag embed self  unbroadcasted cov diag           return covariance matrix expand self  batch shape   self  event shape                                           self  event shape        lazy property     def precision matrix self                                      wt dinv =  self  unbroadcasted cov factor transpose -1  -2                     / self  unbroadcasted cov diag unsqueeze -2           a = torch triangular solve wt dinv  self  capacitance tril  upper=false  0          precision matrix =  torch diag embed self  unbroadcasted cov diag reciprocal                                - torch matmul a transpose -1  -2   a           return precision matrix expand self  batch shape   self  event shape                                          self  event shape       def rsample self  sample shape=torch size             shape = self  extend shape sample shape          w shape = shape  -1    self cov factor shape -1           eps w =  standard normal w shape  dtype=self loc dtype  device=self loc device          eps d =  standard normal shape  dtype=self loc dtype  device=self loc device          return  self loc    batch mv self  unbroadcasted cov factor  eps w                    self  unbroadcasted cov diag sqrt     eps d       def log prob self  value           if self  validate args              self  validate sample value          diff = value - self loc         m =  batch lowrank mahalanobis self  unbroadcasted cov factor                                         self  unbroadcasted cov diag                                         diff                                         self  capacitance tril          log det =  batch lowrank logdet self  unbroadcasted cov factor                                          self  unbroadcasted cov diag                                          self  capacitance tril          return -0 5    self  event shape 0    math log 2   math pi    log det   m       def entropy self           log det =  batch lowrank logdet self  unbroadcasted cov factor                                          self  unbroadcasted cov diag                                          self  capacitance tril          h = 0 5    self  event shape 0     1 0   math log 2   math pi     log det          if len self  batch shape  == 0              return h         else              return h expand self  batch shape  
class mixturesamefamily distribution       r        the `mixturesamefamily` distribution implement a  batch of  mixture     distribution where all component be from different parameterizations of     the same distribution type  it be parameterized by a `categorical`      select distribution   over `k` component  and a component     distribution  i e   a `distribution` with a rightmost batch shape      equal to ` k `  which index each  batch of  component       examples              construct gaussian mixture model in 1d consist of 5 equally           weight normal distributions         >>> mix = d categorical torch ones 5            >>> comp = d normal torch randn 5    torch rand 5            >>> gmm = mixturesamefamily mix  comp             construct gaussian mixture modle in 2d consist of 5 equally           weight bivariate normal distributions         >>> mix = d categorical torch ones 5            >>> comp = d independent d normal                       torch randn 5 2   torch rand 5 2    1          >>> gmm = mixturesamefamily mix  comp             construct a batch of 3 gaussian mixture model in 2d each           consist of 5 random weight bivariate normal distributions         >>> mix = d categorical torch rand 3 5           >>> comp = d independent d normal                      torch randn 3 5 2   torch rand 3 5 2    1          >>> gmm = mixturesamefamily mix  comp       args          mixture distribution  `torch distributions categorical`-like             instance  manage the probability of select component              the number of categories must match the rightmost batch             dimension of the `component distribution`  must have either             scalar `batch shape` or `batch shape` match             `component distribution batch shape  -1 `         component distribution  `torch distributions distribution`-like             instance  right-most batch dimension index component              arg constraints  dict str  constraints constraint  =        have rsample = false      def   init   self                   mixture distribution                   component distribution                   validate args=none           self  mixture distribution = mixture distribution         self  component distribution = component distribution          if not isinstance self  mixture distribution  categorical               raise valueerror string                              string           if not isinstance self  component distribution  distribution               raise valueerror string                              string                    mdbs = self  mixture distribution batch shape         cdbs = self  component distribution batch shape  -1          for size1  size2 in zip reverse mdbs   reverse cdbs                if size1  = 1 and size2  = 1 and size1  = size2                  raise valueerror string                                  string                                  string format mdbs  cdbs                     km = self  mixture distribution logits shape -1          kc = self  component distribution batch shape -1          if km be not none and kc be not none and km  = kc              raise valueerror string                              string                              string format km  kc           self  num component = km          event shape = self  component distribution event shape         self  event ndims = len event shape          super mixturesamefamily  self    init   batch shape=cdbs                                                  event shape=event shape                                                  validate args=validate args       def expand self  batch shape   instance=none           batch shape = torch size batch shape          batch shape comp = batch shape    self  num component           new = self  get check instance mixturesamefamily   instance          new  component distribution = \             self  component distribution expand batch shape comp          new  mixture distribution = \             self  mixture distribution expand batch shape          new  num component = self  num component         new  event ndims = self  event ndims         event shape = new  component distribution event shape         super mixturesamefamily  new    init   batch shape=batch shape                                                 event shape=event shape                                                 validate args=false          new  validate args = self  validate args         return new       constraints dependent property     def support self                             return self  component distribution support       property     def mixture distribution self           return self  mixture distribution       property     def component distribution self           return self  component distribution       property     def mean self           probs = self  pad mixture dimension self mixture distribution probs          return torch sum probs   self component distribution mean                           dim=-1 - self  event ndims          property     def variance self                    probs = self  pad mixture dimension self mixture distribution probs          mean cond var = torch sum probs   self component distribution variance                                    dim=-1 - self  event ndims          var cond mean = torch sum probs    self component distribution mean -                                            self  pad self mean   pow 2 0                                     dim=-1 - self  event ndims          return mean cond var   var cond mean      def cdf self  x           x = self  pad x          cdf x = self component distribution cdf x          mix prob = self mixture distribution probs          return torch sum cdf x   mix prob  dim=-1       def log prob self  x           if self  validate args              self  validate sample x          x = self  pad x          log prob x = self component distribution log prob x            log mix prob = torch log softmax self mixture distribution logits                                           dim=-1            return torch logsumexp log prob x   log mix prob  dim=-1         def sample self  sample shape=torch size             with torch no grad                sample len = len sample shape              batch len = len self batch shape              gather dim = sample len   batch len             es = self event shape                           mix sample = self mixture distribution sample sample shape              mix shape = mix sample shape                           comp sample = self component distribution sample sample shape                            mix sample r = mix sample reshape                  mix shape   torch size  1     len es    1                mix sample r = mix sample r repeat                  torch size  1    len mix shape     torch size  1     es               sample = torch gather comp sample  gather dim  mix sample r              return sample squeeze gather dim       def  pad self  x           return x unsqueeze -1 - self  event ndims       def  pad mixture dimension self  x           dist batch ndims = self batch shape numel           cat batch ndims = self mixture distribution batch shape numel           pad ndims = 0 if cat batch ndims == 1 else \             dist batch ndims - cat batch ndims         xs = x shape         x = x reshape xs  -1    torch size pad ndims    1                           xs -1     torch size self  event ndims    1            return x      def   repr   self           args string = string format self mixture distribution                                               self component distribution          return string   string   args string   string 
class multinomial distribution       r        create a multinomial distribution parameterized by  attr `total count` and     either  attr `probs` or  attr `logits`  but not both   the innermost dimension of      attr `probs` index over categories  all other dimension index over batch       note that  attr `total count` need not be specify if only  meth `log prob` be     call  see example below          note   the `probs` argument must be non-negative  finite and have a non-zero sum                and it will be normalize to sum to 1 along the last dimension  attr `probs`               will return this normalize value                the `logits` argument will be interpret as unnormalized log probabilities               and can therefore be any real number  it will likewise be normalize so that               the result probabilities sum to 1 along the last dimension  attr `logits`               will return this normalize value       -    meth `sample` require a single share `total count` for all         parameters and sample      -    meth `log prob` allow different `total count` for each parameter and         sample       example            >>> m = multinomial 100  torch tensor   1   1   1   1             >>> x = m sample      equal probability of 0  1  2  3         tensor   21    24    30    25             >>> multinomial probs=torch tensor  1   1   1   1     log prob x          tensor  -4 1338        args          total count  int   number of trials         probs  tensor   event probabilities         logits  tensor   event log probabilities  unnormalized              arg constraints =  string  constraints simplex                         string  constraints real vector      total count  int       property     def mean self           return self probs   self total count       property     def variance self           return self total count   self probs    1 - self probs       def   init   self  total count=1  probs=none  logits=none  validate args=none           if not isinstance total count  int               raise notimplementederror string          self total count = total count         self  categorical = categorical probs=probs  logits=logits          batch shape = self  categorical batch shape         event shape = self  categorical param shape -1           super multinomial  self    init   batch shape  event shape  validate args=validate args       def expand self  batch shape   instance=none           new = self  get check instance multinomial   instance          batch shape = torch size batch shape          new total count = self total count         new  categorical = self  categorical expand batch shape          super multinomial  new    init   batch shape  self event shape  validate args=false          new  validate args = self  validate args         return new      def  new self   args    kwargs           return self  categorical  new  args    kwargs        constraints dependent property be discrete=true  event dim=1      def support self           return constraints multinomial self total count        property     def logits self           return self  categorical logits       property     def probs self           return self  categorical probs       property     def param shape self           return self  categorical param shape      def sample self  sample shape=torch size             sample shape = torch size sample shape          sample = self  categorical sample torch size  self total count      sample shape                            shift idx = list range sample dim             shift idx append shift idx pop 0           sample = sample permute  shift idx          count = sample new self  extend shape sample shape   zero            count scatter add  -1  sample  torch ones like sample           return count type as self probs       def log prob self  value           if self  validate args              self  validate sample value          logits  value = broadcast all self logits  value          logits = logits clone memory format=torch contiguous format          log factorial n = torch lgamma value sum -1    1          log factorial xs = torch lgamma value   1  sum -1          logits  value == 0     logits == -inf   = 0         log power =  logits   value  sum -1          return log factorial n - log factorial xs   log power 
class multivariatenormal distribution       r        create a multivariate normal  also call gaussian  distribution     parameterized by a mean vector and a covariance matrix       the multivariate normal distribution can be parameterized either     in term of a positive definite covariance matrix  math `\mathbf \sigma `     or a positive definite precision matrix  math `\mathbf \sigma   -1 `     or a lower-triangular matrix  math `\mathbf l ` with positive-valued     diagonal entries  such that      math `\mathbf \sigma  = \mathbf l \mathbf l  \top`  this triangular matrix     can be obtain via e g  cholesky decomposition of the covariance       example           >>> m = multivariatenormal torch zero 2   torch eye 2           >>> m sample      normally distribute with mean=` 0 0 ` and covariance matrix=`i`         tensor  -0 2102  -0 5429        args          loc  tensor   mean of the distribution         covariance matrix  tensor   positive-definite covariance matrix         precision matrix  tensor   positive-definite precision matrix         scale tril  tensor   lower-triangular factor of covariance  with positive-valued diagonal      note          only one of  attr `covariance matrix` or  attr `precision matrix` or          attr `scale tril` can be specify           use  attr `scale tril` will be more efficient  all computations internally         be base on  attr `scale tril`  if  attr `covariance matrix` or          attr `precision matrix` be pass instead  it be only use to compute         the correspond lower triangular matrices use a cholesky decomposition              arg constraints =  string  constraints real vector                         string  constraints positive definite                         string  constraints positive definite                         string  constraints lower cholesky      support = constraints real vector     have rsample = true      def   init   self  loc  covariance matrix=none  precision matrix=none  scale tril=none  validate args=none           if loc dim   < 1              raise valueerror string          if  covariance matrix be not none     scale tril be not none     precision matrix be not none   = 1              raise valueerror string           if scale tril be not none              if scale tril dim   < 2                  raise valueerror string                                  string              batch shape = torch broadcast shape scale tril shape  -2   loc shape  -1               self scale tril = scale tril expand batch shape    -1  -1           elif covariance matrix be not none              if covariance matrix dim   < 2                  raise valueerror string                                  string              batch shape = torch broadcast shape covariance matrix shape  -2   loc shape  -1               self covariance matrix = covariance matrix expand batch shape    -1  -1           else              if precision matrix dim   < 2                  raise valueerror string                                  string              batch shape = torch broadcast shape precision matrix shape  -2   loc shape  -1               self precision matrix = precision matrix expand batch shape    -1  -1           self loc = loc expand batch shape    -1             event shape = self loc shape -1           super multivariatenormal  self    init   batch shape  event shape  validate args=validate args           if scale tril be not none              self  unbroadcasted scale tril = scale tril         elif covariance matrix be not none              self  unbroadcasted scale tril = torch linalg cholesky covariance matrix          else                self  unbroadcasted scale tril =  precision to scale tril precision matrix       def expand self  batch shape   instance=none           new = self  get check instance multivariatenormal   instance          batch shape = torch size batch shape          loc shape = batch shape   self event shape         cov shape = batch shape   self event shape   self event shape         new loc = self loc expand loc shape          new  unbroadcasted scale tril = self  unbroadcasted scale tril         if string in self   dict                new covariance matrix = self covariance matrix expand cov shape          if string in self   dict                new scale tril = self scale tril expand cov shape          if string in self   dict                new precision matrix = self precision matrix expand cov shape          super multivariatenormal  new    init   batch shape                                                  self event shape                                                  validate args=false          new  validate args = self  validate args         return new       lazy property     def scale tril self           return self  unbroadcasted scale tril expand              self  batch shape   self  event shape   self  event shape        lazy property     def covariance matrix self           return  torch matmul self  unbroadcasted scale tril                               self  unbroadcasted scale tril transpose -1  -2                    expand self  batch shape   self  event shape   self  event shape         lazy property     def precision matrix self           identity = torch eye self loc size -1   device=self loc device  dtype=self loc dtype                   return torch cholesky solve identity  self  unbroadcasted scale tril  expand              self  batch shape   self  event shape   self  event shape        property     def mean self           return self loc       property     def variance self           return self  unbroadcasted scale tril pow 2  sum -1  expand              self  batch shape   self  event shape       def rsample self  sample shape=torch size             shape = self  extend shape sample shape          eps =  standard normal shape  dtype=self loc dtype  device=self loc device          return self loc    batch mv self  unbroadcasted scale tril  eps       def log prob self  value           if self  validate args              self  validate sample value          diff = value - self loc         m =  batch mahalanobis self  unbroadcasted scale tril  diff          half log det = self  unbroadcasted scale tril diagonal dim1=-2  dim2=-1  log   sum -1          return -0 5    self  event shape 0    math log 2   math pi    m  - half log det      def entropy self           half log det = self  unbroadcasted scale tril diagonal dim1=-2  dim2=-1  log   sum -1          h = 0 5   self  event shape 0     1 0   math log 2   math pi     half log det         if len self  batch shape  == 0              return h         else              return h expand self  batch shape  
class negativebinomial distribution       r        create a negative binomial distribution  i e  distribution     of the number of successful independent and identical bernoulli trials     before  attr `total count` failures be achieve  the probability     of failure of each bernoulli trial be  attr `probs`       args          total count  float or tensor   non-negative number of negative bernoulli             trials to stop  although the distribution be still valid for real             value count         probs  tensor   event probabilities of failure in the half open interval  0  1          logits  tensor   event log-odds for probabilities of failure             arg constraints =  string  constraints greater than eq 0                          string  constraints half open interval 0   1                           string  constraints real      support = constraints nonnegative integer      def   init   self  total count  probs=none  logits=none  validate args=none           if  probs be none  ==  logits be none               raise valueerror string          if probs be not none              self total count  self probs  = broadcast all total count  probs              self total count = self total count type as self probs          else              self total count  self logits  = broadcast all total count  logits              self total count = self total count type as self logits           self  param = self probs if probs be not none else self logits         batch shape = self  param size           super negativebinomial  self    init   batch shape  validate args=validate args       def expand self  batch shape   instance=none           new = self  get check instance negativebinomial   instance          batch shape = torch size batch shape          new total count = self total count expand batch shape          if string in self   dict                new probs = self probs expand batch shape              new  param = new probs         if string in self   dict                new logits = self logits expand batch shape              new  param = new logits         super negativebinomial  new    init   batch shape  validate args=false          new  validate args = self  validate args         return new      def  new self   args    kwargs           return self  param new  args    kwargs        property     def mean self           return self total count   torch exp self logits        property     def variance self           return self mean / torch sigmoid -self logits        lazy property     def logits self           return probs to logits self probs  be binary=true        lazy property     def probs self           return logits to probs self logits  be binary=true        property     def param shape self           return self  param size         lazy property     def  gamma self                    return torch distributions gamma concentration=self total count                                           rate=torch exp -self logits                                            validate args=false       def sample self  sample shape=torch size             with torch no grad                rate = self  gamma sample sample shape=sample shape              return torch poisson rate       def log prob self  value           if self  validate args              self  validate sample value           log unnormalized prob =  self total count   f logsigmoid -self logits                                     value   f logsigmoid self logits            log normalization =  -torch lgamma self total count   value    torch lgamma 1    value                                 torch lgamma self total count            return log unnormalized prob - log normalization 
class normal exponentialfamily       r        create a normal  also call gaussian  distribution parameterized by      attr `loc` and  attr `scale`       example            >>> m = normal torch tensor  0 0    torch tensor  1 0            >>> m sample      normally distribute with loc=0 and scale=1         tensor   0 1046        args          loc  float or tensor   mean of the distribution  often refer to as mu          scale  float or tensor   standard deviation of the distribution              often refer to as sigma              arg constraints =  string  constraints real  string  constraints positive      support = constraints real     have rsample = true      mean carrier measure = 0       property     def mean self           return self loc       property     def stddev self           return self scale       property     def variance self           return self stddev pow 2       def   init   self  loc  scale  validate args=none           self loc  self scale = broadcast all loc  scale          if isinstance loc  number  and isinstance scale  number               batch shape = torch size           else              batch shape = self loc size           super normal  self    init   batch shape  validate args=validate args       def expand self  batch shape   instance=none           new = self  get check instance normal   instance          batch shape = torch size batch shape          new loc = self loc expand batch shape          new scale = self scale expand batch shape          super normal  new    init   batch shape  validate args=false          new  validate args = self  validate args         return new      def sample self  sample shape=torch size             shape = self  extend shape sample shape          with torch no grad                return torch normal self loc expand shape   self scale expand shape        def rsample self  sample shape=torch size             shape = self  extend shape sample shape          eps =  standard normal shape  dtype=self loc dtype  device=self loc device          return self loc   eps   self scale      def log prob self  value           if self  validate args              self  validate sample value                   var =  self scale    2          log scale = math log self scale  if isinstance self scale  real  else self scale log           return -  value - self loc     2  /  2   var  - log scale - math log math sqrt 2   math pi        def cdf self  value           if self  validate args              self  validate sample value          return 0 5    1   torch erf  value - self loc    self scale reciprocal   / math sqrt 2         def icdf self  value           return self loc   self scale   torch erfinv 2   value - 1    math sqrt 2       def entropy self           return 0 5   0 5   math log 2   math pi    torch log self scale        property     def  natural params self           return  self loc / self scale pow 2   -0 5   self scale pow 2  reciprocal         def  log normalizer self  x  y           return -0 25   x pow 2  / y   0 5   torch log -math pi / y  
class onehotcategorical distribution       r        create a one-hot categorical distribution parameterized by  attr `probs` or      attr `logits`       sample be one-hot cod vectors of size ``probs size -1 ``          note   the `probs` argument must be non-negative  finite and have a non-zero sum                and it will be normalize to sum to 1 along the last dimension  attr `probs`               will return this normalize value                the `logits` argument will be interpret as unnormalized log probabilities               and can therefore be any real number  it will likewise be normalize so that               the result probabilities sum to 1 along the last dimension  attr `logits`               will return this normalize value       see also   func `torch distributions categorical` for specifications of      attr `probs` and  attr `logits`       example            >>> m = onehotcategorical torch tensor   0 25  0 25  0 25  0 25             >>> m sample      equal probability of 0  1  2  3         tensor   0    0    0    1         args          probs  tensor   event probabilities         logits  tensor   event log probabilities  unnormalized              arg constraints =  string  constraints simplex                         string  constraints real vector      support = constraints one hot     have enumerate support = true      def   init   self  probs=none  logits=none  validate args=none           self  categorical = categorical probs  logits          batch shape = self  categorical batch shape         event shape = self  categorical param shape -1           super onehotcategorical  self    init   batch shape  event shape  validate args=validate args       def expand self  batch shape   instance=none           new = self  get check instance onehotcategorical   instance          batch shape = torch size batch shape          new  categorical = self  categorical expand batch shape          super onehotcategorical  new    init   batch shape  self event shape  validate args=false          new  validate args = self  validate args         return new      def  new self   args    kwargs           return self  categorical  new  args    kwargs        property     def  param self           return self  categorical  param       property     def probs self           return self  categorical probs       property     def logits self           return self  categorical logits       property     def mean self           return self  categorical probs       property     def variance self           return self  categorical probs    1 - self  categorical probs        property     def param shape self           return self  categorical param shape      def sample self  sample shape=torch size             sample shape = torch size sample shape          probs = self  categorical probs         num events = self  categorical  num events         indices = self  categorical sample sample shape          return torch nn functional one hot indices  num events  to probs       def log prob self  value           if self  validate args              self  validate sample value          indices = value max -1  1          return self  categorical log prob indices       def entropy self           return self  categorical entropy        def enumerate support self  expand=true           n = self event shape 0          value = torch eye n  dtype=self  param dtype  device=self  param device          value = value view  n      1     len self batch shape     n            if expand              value = value expand  n     self batch shape    n            return value 
class pareto transformeddistribution       r        sample from a pareto type 1 distribution       example            >>> m = pareto torch tensor  1 0    torch tensor  1 0            >>> m sample      sample from a pareto distribution with scale=1 and alpha=1         tensor   1 5623        args          scale  float or tensor   scale parameter of the distribution         alpha  float or tensor   shape parameter of the distribution             arg constraints =  string  constraints positive  string  constraints positive       def   init   self  scale  alpha  validate args=none           self scale  self alpha = broadcast all scale  alpha          base dist = exponential self alpha  validate args=validate args          transform =  exptransform    affinetransform loc=0  scale=self scale           super pareto  self    init   base dist  transform  validate args=validate args       def expand self  batch shape   instance=none           new = self  get check instance pareto   instance          new scale = self scale expand batch shape          new alpha = self alpha expand batch shape          return super pareto  self  expand batch shape   instance=new        property     def mean self                    a = self alpha clamp min=1          return a   self scale /  a - 1        property     def variance self                    a = self alpha clamp min=2          return self scale pow 2    a /   a - 1  pow 2     a - 2         constraints dependent property be discrete=false  event dim=0      def support self           return constraints greater than self scale       def entropy self           return   self scale / self alpha  log      1   self alpha reciprocal     
class poisson exponentialfamily       r        create a poisson distribution parameterized by  attr `rate`  the rate parameter       sample be nonnegative integers  with a pmf give by         math         \mathrm rate  k \frac e  -\mathrm rate    k        example            >>> m = poisson torch tensor  4            >>> m sample           tensor   3         args          rate  number  tensor   the rate parameter             arg constraints =  string  constraints positive      support = constraints nonnegative integer       property     def mean self           return self rate       property     def variance self           return self rate      def   init   self  rate  validate args=none           self rate  = broadcast all rate          if isinstance rate  number               batch shape = torch size           else              batch shape = self rate size           super poisson  self    init   batch shape  validate args=validate args       def expand self  batch shape   instance=none           new = self  get check instance poisson   instance          batch shape = torch size batch shape          new rate = self rate expand batch shape          super poisson  new    init   batch shape  validate args=false          new  validate args = self  validate args         return new      def sample self  sample shape=torch size             shape = self  extend shape sample shape          with torch no grad                return torch poisson self rate expand shape        def log prob self  value           if self  validate args              self  validate sample value          rate  value = broadcast all self rate  value          return  rate log     value  - rate -  value   1  lgamma         property     def  natural params self           return  torch log self rate          def  log normalizer self  x           return torch exp x  
class relaxedbernoulli transformeddistribution       r        create a relaxedbernoulli distribution  parametrized by      attr `temperature`  and either  attr `probs` or  attr `logits`      but not both   this be a relax version of the `bernoulli` distribution      so the value be in  0  1   and have reparametrizable sample       example            >>> m = relaxedbernoulli torch tensor  2 2                                     torch tensor  0 1  0 2  0 3  0 99            >>> m sample           tensor   0 2951   0 3442   0 8918   0 9021        args          temperature  tensor   relaxation temperature         probs  number  tensor   the probability of sample `1`         logits  number  tensor   the log-odds of sample `1`             arg constraints =  string  constraints unit interval                         string  constraints real      support = constraints unit interval     have rsample = true      def   init   self  temperature  probs=none  logits=none  validate args=none           base dist = logitrelaxedbernoulli temperature  probs  logits          super relaxedbernoulli  self    init   base dist                                                 sigmoidtransform                                                   validate args=validate args       def expand self  batch shape   instance=none           new = self  get check instance relaxedbernoulli   instance          return super relaxedbernoulli  self  expand batch shape   instance=new        property     def temperature self           return self base dist temperature       property     def logits self           return self base dist logits       property     def probs self           return self base dist probs 
class logitrelaxedbernoulli distribution       r        create a logitrelaxedbernoulli distribution parameterized by  attr `probs`     or  attr `logits`  but not both   which be the logit of a relaxedbernoulli     distribution       sample be logits of value in  0  1   see  1  for more detail       args          temperature  tensor   relaxation temperature         probs  number  tensor   the probability of sample `1`         logits  number  tensor   the log-odds of sample `1`       1  the concrete distribution  a continuous relaxation of discrete random     variables  maddison et al  2017        2  categorical reparametrization with gumbel-softmax      jang et al  2017              arg constraints =  string  constraints unit interval                         string  constraints real      support = constraints real      def   init   self  temperature  probs=none  logits=none  validate args=none           self temperature = temperature         if  probs be none  ==  logits be none               raise valueerror string          if probs be not none              be scalar = isinstance probs  number              self probs  = broadcast all probs          else              be scalar = isinstance logits  number              self logits  = broadcast all logits          self  param = self probs if probs be not none else self logits         if be scalar              batch shape = torch size           else              batch shape = self  param size           super logitrelaxedbernoulli  self    init   batch shape  validate args=validate args       def expand self  batch shape   instance=none           new = self  get check instance logitrelaxedbernoulli   instance          batch shape = torch size batch shape          new temperature = self temperature         if string in self   dict                new probs = self probs expand batch shape              new  param = new probs         if string in self   dict                new logits = self logits expand batch shape              new  param = new logits         super logitrelaxedbernoulli  new    init   batch shape  validate args=false          new  validate args = self  validate args         return new      def  new self   args    kwargs           return self  param new  args    kwargs        lazy property     def logits self           return probs to logits self probs  be binary=true        lazy property     def probs self           return logits to probs self logits  be binary=true        property     def param shape self           return self  param size        def rsample self  sample shape=torch size             shape = self  extend shape sample shape          probs = clamp probs self probs expand shape           uniform = clamp probs torch rand shape  dtype=probs dtype  device=probs device           return  uniform log   -  -uniforms  log1p     probs log   -  -probs  log1p    / self temperature      def log prob self  value           if self  validate args              self  validate sample value          logits  value = broadcast all self logits  value          diff = logits - value mul self temperature          return self temperature log     diff - 2   diff exp   log1p   
class relaxedonehotcategorical transformeddistribution       r        create a relaxedonehotcategorical distribution parametrized by      attr `temperature`  and either  attr `probs` or  attr `logits`      this be a relax version of the  class `onehotcategorical` distribution  so     its sample be on simplex  and be reparametrizable       example            >>> m = relaxedonehotcategorical torch tensor  2 2                                             torch tensor  0 1  0 2  0 3  0 4            >>> m sample           tensor   0 1294   0 2324   0 3859   0 2523        args          temperature  tensor   relaxation temperature         probs  tensor   event probabilities         logits  tensor   unnormalized log probability for each event             arg constraints =  string  constraints simplex                         string  constraints real vector      support = constraints simplex     have rsample = true      def   init   self  temperature  probs=none  logits=none  validate args=none           base dist = exprelaxedcategorical temperature  probs  logits  validate args=validate args          super relaxedonehotcategorical  self    init   base dist                                                         exptransform                                                           validate args=validate args       def expand self  batch shape   instance=none           new = self  get check instance relaxedonehotcategorical   instance          return super relaxedonehotcategorical  self  expand batch shape   instance=new        property     def temperature self           return self base dist temperature       property     def logits self           return self base dist logits       property     def probs self           return self base dist probs 
class studentt distribution       r        create a student s t-distribution parameterized by degree of     freedom  attr `df`  mean  attr `loc` and scale  attr `scale`       example            >>> m = studentt torch tensor  2 0            >>> m sample      student s t-distributed with degrees of freedom=2         tensor   0 1046        args          df  float or tensor   degrees of freedom         loc  float or tensor   mean of the distribution         scale  float or tensor   scale of the distribution             arg constraints =  string  constraints positive  string  constraints real  string  constraints positive      support = constraints real     have rsample = true       property     def mean self           m = self loc clone memory format=torch contiguous format          m self df <= 1  = nan         return m       property     def variance self           m = self df clone memory format=torch contiguous format          m self df > 2  = self scale self df > 2  pow 2    self df self df > 2  /  self df self df > 2  - 2          m  self df <= 2     self df > 1   = inf         m self df <= 1  = nan         return m      def   init   self  df  loc=0   scale=1   validate args=none           self df  self loc  self scale = broadcast all df  loc  scale          self  chi2 = chi2 self df          batch shape = self df size           super studentt  self    init   batch shape  validate args=validate args       def expand self  batch shape   instance=none           new = self  get check instance studentt   instance          batch shape = torch size batch shape          new df = self df expand batch shape          new loc = self loc expand batch shape          new scale = self scale expand batch shape          new  chi2 = self  chi2 expand batch shape          super studentt  new    init   batch shape  validate args=false          new  validate args = self  validate args         return new      def rsample self  sample shape=torch size                                                                    shape = self  extend shape sample shape          x =  standard normal shape  dtype=self df dtype  device=self df device          z = self  chi2 rsample sample shape          y = x   torch rsqrt z / self df          return self loc   self scale   y      def log prob self  value           if self  validate args              self  validate sample value          y =  value - self loc  / self scale         z =  self scale log                  0 5   self df log                  0 5   math log math pi                 torch lgamma 0 5   self df  -              torch lgamma 0 5    self df   1             return -0 5    self df   1     torch log1p y  2  / self df  - z      def entropy self           lbeta = torch lgamma 0 5   self df    math lgamma 0 5  - torch lgamma 0 5    self df   1           return  self scale log                     0 5    self df   1                     torch digamma 0 5    self df   1   - torch digamma 0 5   self df                     0 5   self df log     lbeta  
class transformeddistribution distribution       r        extension of the distribution class  which apply a sequence of transform     to a base distribution   let f be the composition of transform apply            x ~ basedistribution         y = f x  ~ transformeddistribution basedistribution  f          log p y  = log p x    log  det  dx/dy        note that the `` event shape`` of a  class `transformeddistribution` be the     maximum shape of its base distribution and its transform  since transform     can introduce correlations among events       an example for the usage of  class `transformeddistribution` would be              build a logistic distribution           x ~ uniform 0  1            f = a   b   logit x            y ~ f x  ~ logistic a  b          base distribution = uniform 0  1          transform =  sigmoidtransform   inv  affinetransform loc=a  scale=b           logistic = transformeddistribution base distribution  transform       for more examples  please look at the implementations of      class `~torch distributions gumbel gumbel`       class `~torch distributions half cauchy halfcauchy`       class `~torch distributions half normal halfnormal`       class `~torch distributions log normal lognormal`       class `~torch distributions pareto pareto`       class `~torch distributions weibull weibull`       class `~torch distributions relax bernoulli relaxedbernoulli` and      class `~torch distributions relax categorical relaxedonehotcategorical`             arg constraints  dict str  constraints constraint  =         def   init   self  base distribution  transform  validate args=none           if isinstance transform  transform               self transform =  transform            elif isinstance transform  list               if not all isinstance t  transform  for t in transform                   raise valueerror string              self transform = transform         else              raise valueerror string format transform                     base shape = base distribution batch shape   base distribution event shape         base event dim = len base distribution event shape          transform = composetransform self transform          domain event dim = transform domain event dim         if len base shape  < domain event dim              raise valueerror string                               format domain event dim  base shape           shape = transform forward shape base shape          expand base shape = transform inverse shape shape          if base shape  = expand base shape              base batch shape = expand base shape  len expand base shape  - base event dim              base distribution = base distribution expand base batch shape          reinterpret batch ndims = domain event dim - base event dim         if reinterpret batch ndims > 0              base distribution = independent base distribution  reinterpret batch ndims          self base dist = base distribution                   event dim = transform codomain event dim   max base event dim - domain event dim  0          assert len shape  >= event dim         cut = len shape  - event dim         batch shape = shape  cut          event shape = shape cut           super transformeddistribution  self    init   batch shape  event shape  validate args=validate args       def expand self  batch shape   instance=none           new = self  get check instance transformeddistribution   instance          batch shape = torch size batch shape          shape = batch shape   self event shape         for t in reverse self transform               shape = t inverse shape shape          base batch shape = shape  len shape  - len self base dist event shape           new base dist = self base dist expand base batch shape          new transform = self transform         super transformeddistribution  new    init   batch shape  self event shape  validate args=false          new  validate args = self  validate args         return new       constraints dependent property be discrete=false      def support self           if not self transform              return self base dist support         support = self transform -1  codomain         if len self event shape  > support event dim              support = constraints independent support  len self event shape  - support event dim          return support       property     def have rsample self           return self base dist have rsample      def sample self  sample shape=torch size             string         with torch no grad                x = self base dist sample sample shape              for transform in self transform                  x = transform x              return x      def rsample self  sample shape=torch size             string         x = self base dist rsample sample shape          for transform in self transform              x = transform x          return x      def log prob self  value           string         if self  validate args              self  validate sample value          event dim = len self event shape          log prob = 0 0         y = value         for transform in reverse self transform               x = transform inv y              event dim  = transform domain event dim - transform codomain event dim             log prob = log prob -  sum rightmost transform log abs det jacobian x  y                                                    event dim - transform domain event dim              y = x          log prob = log prob    sum rightmost self base dist log prob y                                                event dim - len self base dist event shape           return log prob      def  monotonize cdf self  value           string         sign = 1         for transform in self transform              sign = sign   transform sign         if isinstance sign  int  and sign == 1              return value         return sign    value - 0 5    0 5      def cdf self  value           string         for transform in self transform   -1               value = transform inv value          if self  validate args              self base dist  validate sample value          value = self base dist cdf value          value = self  monotonize cdf value          return value      def icdf self  value           string         value = self  monotonize cdf value          if self  validate args              self base dist  validate sample value          value = self base dist icdf value          for transform in self transform              value = transform value          return value 
class uniform distribution       r        generate uniformly distribute random sample from the half-open interval     `` low  high ``       example            >>> m = uniform torch tensor  0 0    torch tensor  5 0            >>> m sample      uniformly distribute in the range  0 0  5 0          tensor   2 3418        args          low  float or tensor   lower range  inclusive           high  float or tensor   upper range  exclusive                    arg constraints =  string  constraints dependent be discrete=false  event dim=0                          string  constraints dependent be discrete=false  event dim=0       have rsample = true       property     def mean self           return  self high   self low  / 2       property     def stddev self           return  self high - self low  / 12  0 5       property     def variance self           return  self high - self low  pow 2  / 12      def   init   self  low  high  validate args=none           self low  self high = broadcast all low  high           if isinstance low  number  and isinstance high  number               batch shape = torch size           else              batch shape = self low size           super uniform  self    init   batch shape  validate args=validate args           if self  validate args and not torch lt self low  self high  all                raise valueerror string       def expand self  batch shape   instance=none           new = self  get check instance uniform   instance          batch shape = torch size batch shape          new low = self low expand batch shape          new high = self high expand batch shape          super uniform  new    init   batch shape  validate args=false          new  validate args = self  validate args         return new       constraints dependent property be discrete=false  event dim=0      def support self           return constraints interval self low  self high       def rsample self  sample shape=torch size             shape = self  extend shape sample shape          rand = torch rand shape  dtype=self low dtype  device=self low device          return self low   rand    self high - self low       def log prob self  value           if self  validate args              self  validate sample value          lb = self low le value  type as self low          ub = self high gt value  type as self low          return torch log lb mul ub   - torch log self high - self low       def cdf self  value           if self  validate args              self  validate sample value          result =  value - self low  /  self high - self low          return result clamp min=0  max=1       def icdf self  value           result = value    self high - self low    self low         return result      def entropy self           return torch log self high - self low  
class vonmises distribution       string     arg constraints =  string  constraints real  string  constraints positive      support = constraints real     have rsample = false      def   init   self  loc  concentration  validate args=none           self loc  self concentration = broadcast all loc  concentration          batch shape = self loc shape         event shape = torch size                     tau = 1    1   4   self concentration    2  sqrt           rho =  tau -  2   tau  sqrt    /  2   self concentration          self  proposal r =  1   rho    2  /  2   rho           super vonmises  self    init   batch shape  event shape  validate args       def log prob self  value           log prob = self concentration   torch cos value - self loc          log prob = log prob - math log 2   math pi  -  log modify bessel fn self concentration  order=0          return log prob       torch no grad       def sample self  sample shape=torch size             string         shape = self  extend shape sample shape          x = torch empty shape  dtype=self loc dtype  device=self loc device          return  rejection sample self loc  self concentration  self  proposal r  x       def expand self  batch shape           try              return super vonmises  self  expand batch shape          except notimplementederror              validate args = self   dict   get string              loc = self loc expand batch shape              concentration = self concentration expand batch shape              return type self  loc  concentration  validate args=validate args        property     def mean self           string         return self loc       lazy property     def variance self           string         return 1 -   log modify bessel fn self concentration  order=1  -                      log modify bessel fn self concentration  order=0   exp   
class weibull transformeddistribution       r        sample from a two-parameter weibull distribution       example           >>> m = weibull torch tensor  1 0    torch tensor  1 0            >>> m sample      sample from a weibull distribution with scale=1  concentration=1         tensor   0 4784        args          scale  float or tensor   scale parameter of distribution  lambda           concentration  float or tensor   concentration parameter of distribution  k/shape               arg constraints =  string  constraints positive  string  constraints positive      support = constraints positive      def   init   self  scale  concentration  validate args=none           self scale  self concentration = broadcast all scale  concentration          self concentration reciprocal = self concentration reciprocal           base dist = exponential torch ones like self scale   validate args=validate args          transform =  powertransform exponent=self concentration reciprocal                         affinetransform loc=0  scale=self scale           super weibull  self    init   base dist                                        transform                                        validate args=validate args       def expand self  batch shape   instance=none           new = self  get check instance weibull   instance          new scale = self scale expand batch shape          new concentration = self concentration expand batch shape          new concentration reciprocal = new concentration reciprocal           base dist = self base dist expand batch shape          transform =  powertransform exponent=new concentration reciprocal                         affinetransform loc=0  scale=new scale           super weibull  new    init   base dist                                       transform                                       validate args=false          new  validate args = self  validate args         return new       property     def mean self           return self scale   torch exp torch lgamma 1   self concentration reciprocal         property     def variance self           return self scale pow 2     torch exp torch lgamma 1   2   self concentration reciprocal   -                                     torch exp 2   torch lgamma 1   self concentration reciprocal         def entropy self           return euler constant    1 - self concentration reciprocal    \             torch log self scale   self concentration reciprocal    1 
class abstransform transform       r        transform via the map  math `y =  x `              domain = constraints real     codomain = constraints positive      def   eq   self  other           return isinstance other  abstransform       def  call self  x           return x abs        def  inverse self  y           return y 
class affinetransform transform       r        transform via the pointwise affine map  math `y = \text loc    \text scale  \times x`       args          loc  tensor or float   location parameter          scale  tensor or float   scale parameter          event dim  int   optional size of `event shape`  this should be zero             for univariate random variables  1 for distributions over vectors              2 for distributions over matrices  etc              bijective = true      def   init   self  loc  scale  event dim=0  cache size=0           super affinetransform  self    init   cache size=cache size          self loc = loc         self scale = scale         self  event dim = event dim       property     def event dim self           return self  event dim       constraints dependent property be discrete=false      def domain self           if self event dim == 0              return constraints real         return constraints independent constraints real  self event dim        constraints dependent property be discrete=false      def codomain self           if self event dim == 0              return constraints real         return constraints independent constraints real  self event dim       def with cache self  cache size=1           if self  cache size == cache size              return self         return affinetransform self loc  self scale  self event dim  cache size=cache size       def   eq   self  other           if not isinstance other  affinetransform               return false          if isinstance self loc  number number  and isinstance other loc  number number               if self loc  = other loc                  return false         else              if not  self loc == other loc  all   item                    return false          if isinstance self scale  number number  and isinstance other scale  number number               if self scale  = other scale                  return false         else              if not  self scale == other scale  all   item                    return false          return true       property     def sign self           if isinstance self scale  number real               return 1 if float self scale  > 0 else -1 if float self scale  < 0 else 0         return self scale sign        def  call self  x           return self loc   self scale   x      def  inverse self  y           return  y - self loc  / self scale      def log abs det jacobian self  x  y           shape = x shape         scale = self scale         if isinstance scale  number real               result = torch full like x  math log abs scale            else              result = torch abs scale  log           if self event dim              result size = result size    -self event dim     -1               result = result view result size  sum -1              shape = shape  -self event dim          return result expand shape       def forward shape self  shape           return torch broadcast shape shape                                        getattr self loc  string                                             getattr self scale  string            def inverse shape self  shape           return torch broadcast shape shape                                        getattr self loc  string                                             getattr self scale  string       
class composetransform transform       string     def   init   self  part  list transform   cache size=0           if cache size              part =  part with cache cache size  for part in part          super composetransform  self    init   cache size=cache size          self part = part      def   eq   self  other           if not isinstance other  composetransform               return false         return self part == other part       constraints dependent property be discrete=false      def domain self           if not self part              return constraints real         domain = self part 0  domain                  event dim = self part -1  codomain event dim         for part in reverse self part               event dim  = part domain event dim - part codomain event dim             event dim = max event dim  part domain event dim          assert event dim >= domain event dim         if event dim > domain event dim              domain = constraints independent domain  event dim - domain event dim          return domain       constraints dependent property be discrete=false      def codomain self           if not self part              return constraints real         codomain = self part -1  codomain                  event dim = self part 0  domain event dim         for part in self part              event dim  = part codomain event dim - part domain event dim             event dim = max event dim  part codomain event dim          assert event dim >= codomain event dim         if event dim > codomain event dim              codomain = constraints independent codomain  event dim - codomain event dim          return codomain       lazy property     def bijective self           return all p bijective for p in self part        lazy property     def sign self           sign = 1         for p in self part              sign = sign   p sign         return sign       property     def inv self           inv = none         if self  inv be not none              inv = self  inv           if inv be none              inv = composetransform  p inv for p in reverse self part                self  inv = weakref ref inv              inv  inv = weakref ref self          return inv      def with cache self  cache size=1           if self  cache size == cache size              return self         return composetransform self part  cache size=cache size       def   call   self  x           for part in self part              x = part x          return x      def log abs det jacobian self  x  y           if not self part              return torch zero like x                    xs =  x          for part in self part  -1               xs append part xs -1            xs append y           term =            event dim = self domain event dim         for part  x  y in zip self part  xs  -1   xs 1                 term append  sum rightmost part log abs det jacobian x  y                                           event dim - part domain event dim               event dim  = part codomain event dim - part domain event dim         return functools reduce operator add  term       def forward shape self  shape           for part in self part              shape = part forward shape shape          return shape      def inverse shape self  shape           for part in reverse self part               shape = part inverse shape shape          return shape      def   repr   self           fmt string = self   class     name     string         fmt string  = string join  p   repr     for p in self part           fmt string  = string         return fmt string 
class corrcholeskytransform transform       r        transform an uncontrained real vector  math `x` with length  math `d  d-1 /2` into the     cholesky factor of a d-dimension correlation matrix  this cholesky factor be a lower     triangular matrix with positive diagonals and unit euclidean norm for each row      the transform be process as follow           1  first we convert x into a lower triangular matrix in row order          2  for each row  math `x i` of the lower triangular part  we apply a  sign  version of            class  class `stickbreakingtransform` to transform  math `x i` into a            unit euclidean length vector use the follow step             - scale into the interval  math ` -1  1 ` domain   math `r i = \tanh x i `             - transform into an unsigned domain   math `z i = r i 2`             - apply  math `s i = stickbreakingtransform z i `             - transform back into sign domain   math `y i = sign r i    \sqrt s i `              domain = constraints real vector     codomain = constraints corr cholesky     bijective = true      def  call self  x           x = torch tanh x          eps = torch finfo x dtype  eps         x = x clamp min=-1   eps  max=1 - eps          r = vec to tril matrix x  diag=-1                                     z = r    2         z1m cumprod sqrt =  1 - z  sqrt   cumprod -1                   r = r   torch eye r shape -1   dtype=r dtype  device=r device          y = r   pad z1m cumprod sqrt       -1    1  0   value=1          return y      def  inverse self  y                             y cumsum = 1 - torch cumsum y   y  dim=-1          y cumsum shift = pad y cumsum       -1    1  0   value=1          y vec = tril matrix to vec y  diag=-1          y cumsum vec = tril matrix to vec y cumsum shift  diag=-1          t = y vec /  y cumsum vec  sqrt                    x =   1   t  /  1 - t   log   / 2         return x      def log abs det jacobian self  x  y  intermediates=none                                                y1m cumsum = 1 -  y   y  cumsum dim=-1                            y1m cumsum tril = tril matrix to vec y1m cumsum  diag=-2          stick break logdet = 0 5    y1m cumsum tril  log   sum -1          tanh logdet = -2    x   softplus -2   x  - math log 2    sum dim=-1          return stick break logdet   tanh logdet      def forward shape self  shape                    if len shape  < 1              raise valueerror string          n = shape -1          d = round  0 25   2   n     0 5   0 5          if d    d - 1  // 2  = n              raise valueerror string          return shape  -1     d  d       def inverse shape self  shape                    if len shape  < 2              raise valueerror string          if shape -2   = shape -1               raise valueerror string          d = shape -1          n = d    d - 1  // 2         return shape  -2     n   
class exptransform transform       r        transform via the map  math `y = \exp x `              domain = constraints real     codomain = constraints positive     bijective = true     sign =  1      def   eq   self  other           return isinstance other  exptransform       def  call self  x           return x exp        def  inverse self  y           return y log        def log abs det jacobian self  x  y           return x 
class independenttransform transform       string     def   init   self  base transform  reinterpret batch ndims  cache size=0           super     init   cache size=cache size          self base transform = base transform with cache cache size          self reinterpret batch ndims = reinterpret batch ndims      def with cache self  cache size=1           if self  cache size == cache size              return self         return independenttransform self base transform                                      self reinterpret batch ndims                                      cache size=cache size        constraints dependent property be discrete=false      def domain self           return constraints independent self base transform domain                                         self reinterpret batch ndims        constraints dependent property be discrete=false      def codomain self           return constraints independent self base transform codomain                                         self reinterpret batch ndims        property     def bijective self           return self base transform bijective       property     def sign self           return self base transform sign      def  call self  x           if x dim   < self domain event dim              raise valueerror string          return self base transform x       def  inverse self  y           if y dim   < self codomain event dim              raise valueerror string          return self base transform inv y       def log abs det jacobian self  x  y           result = self base transform log abs det jacobian x  y          result =  sum rightmost result  self reinterpret batch ndims          return result      def   repr   self           return f  self   class     name     repr self base transform     self reinterpret batch ndims         def forward shape self  shape           return self base transform forward shape shape       def inverse shape self  shape           return self base transform inverse shape shape  
class lowercholeskytransform transform       string     domain = constraints independent constraints real  2      codomain = constraints lower cholesky      def   eq   self  other           return isinstance other  lowercholeskytransform       def  call self  x           return x tril -1    x diagonal dim1=-2  dim2=-1  exp   diag embed        def  inverse self  y           return y tril -1    y diagonal dim1=-2  dim2=-1  log   diag embed   
class powertransform transform       r        transform via the map  math `y = x  \text exponent  `              domain = constraints positive     codomain = constraints positive     bijective = true     sign =  1      def   init   self  exponent  cache size=0           super powertransform  self    init   cache size=cache size          self exponent  = broadcast all exponent       def with cache self  cache size=1           if self  cache size == cache size              return self         return powertransform self exponent  cache size=cache size       def   eq   self  other           if not isinstance other  powertransform               return false         return self exponent eq other exponent  all   item        def  call self  x           return x pow self exponent       def  inverse self  y           return y pow 1 / self exponent       def log abs det jacobian self  x  y           return  self exponent   y / x  abs   log        def forward shape self  shape           return torch broadcast shape shape  getattr self exponent  string            def inverse shape self  shape           return torch broadcast shape shape  getattr self exponent  string       
class reshapetransform transform       string     bijective = true      def   init   self  in shape  out shape  cache size=0           self in shape = torch size in shape          self out shape = torch size out shape          if self in shape numel    = self out shape numel                raise valueerror string          super     init   cache size=cache size        constraints dependent property     def domain self           return constraints independent constraints real  len self in shape         constraints dependent property     def codomain self           return constraints independent constraints real  len self out shape        def with cache self  cache size=1           if self  cache size == cache size              return self         return reshapetransform self in shape  self out shape  cache size=cache size       def  call self  x           batch shape = x shape  x dim   - len self in shape           return x reshape batch shape   self out shape       def  inverse self  y           batch shape = y shape  y dim   - len self out shape           return y reshape batch shape   self in shape       def log abs det jacobian self  x  y           batch shape = x shape  x dim   - len self in shape           return x new zero batch shape       def forward shape self  shape           if len shape  < len self in shape               raise valueerror string          cut = len shape  - len self in shape          if shape cut    = self in shape              raise valueerror string format shape cut    self in shape           return shape  cut    self out shape      def inverse shape self  shape           if len shape  < len self out shape               raise valueerror string          cut = len shape  - len self out shape          if shape cut    = self out shape              raise valueerror string format shape cut    self out shape           return shape  cut    self in shape 
class sigmoidtransform transform       r        transform via the map  math `y = \frac 1  1   \exp -x  ` and  math `x = \text logit  y `              domain = constraints real     codomain = constraints unit interval     bijective = true     sign =  1      def   eq   self  other           return isinstance other  sigmoidtransform       def  call self  x           return  clip sigmoid x       def  inverse self  y           finfo = torch finfo y dtype          y = y clamp min=finfo tiny  max=1  - finfo eps          return y log   -  -y  log1p        def log abs det jacobian self  x  y           return -f softplus -x  - f softplus x  
class tanhtransform transform       r        transform via the map  math `y = \tanh x `       it be equivalent to     ```     composetransform  affinetransform 0   2    sigmoidtransform    affinetransform -1   2         ```     however this might not be numerically stable  thus it be recommend to use `tanhtransform`     instead       note that one should use `cache size=1` when it come to `nan/inf` value               domain = constraints real     codomain = constraints interval -1 0  1 0      bijective = true     sign =  1      def   eq   self  other           return isinstance other  tanhtransform       def  call self  x           return x tanh        def  inverse self  y                             return torch atanh y       def log abs det jacobian self  x  y                             return 2     math log 2   - x - softplus -2    x   
class softmaxtransform transform       r        transform from unconstrained space to the simplex via  math `y = \exp x ` then     normalize       this be not bijective and cannot be use for hmc  however this act mostly     coordinate-wise  except for the final normalization   and thus be     appropriate for coordinate-wise optimization algorithms              domain = constraints real vector     codomain = constraints simplex      def   eq   self  other           return isinstance other  softmaxtransform       def  call self  x           logprobs = x         probs =  logprobs - logprobs max -1  true  0   exp           return probs / probs sum -1  true       def  inverse self  y           probs = y         return probs log        def forward shape self  shape           if len shape  < 1              raise valueerror string          return shape      def inverse shape self  shape           if len shape  < 1              raise valueerror string          return shape 
class stacktransform transform       string     def   init   self  tseq  dim=0  cache size=0           assert all isinstance t  transform  for t in tseq          if cache size              tseq =  t with cache cache size  for t in tseq          super stacktransform  self    init   cache size=cache size          self transform = list tseq          self dim = dim      def with cache self  cache size=1           if self  cache size == cache size              return self         return stacktransform self transform  self dim  cache size       def  slice self  z           return  z select self dim  i  for i in range z size self dim         def  call self  x           assert -x dim   <= self dim < x dim           assert x size self dim  == len self transform          yslices =            for xslice  trans in zip self  slice x   self transform               yslices append trans xslice           return torch stack yslices  dim=self dim       def  inverse self  y           assert -y dim   <= self dim < y dim           assert y size self dim  == len self transform          xslices =            for yslice  trans in zip self  slice y   self transform               xslices append trans inv yslice           return torch stack xslices  dim=self dim       def log abs det jacobian self  x  y           assert -x dim   <= self dim < x dim           assert x size self dim  == len self transform          assert -y dim   <= self dim < y dim           assert y size self dim  == len self transform          logdetjacs =            yslices = self  slice y          xslices = self  slice x          for xslice  yslice  trans in zip xslices  yslices  self transform               logdetjacs append trans log abs det jacobian xslice  yslice           return torch stack logdetjacs  dim=self dim        property     def bijective self           return all t bijective for t in self transform        constraints dependent property     def domain self           return constraints stack  t domain for t in self transform   self dim        constraints dependent property     def codomain self           return constraints stack  t codomain for t in self transform   self dim  
class stickbreakingtransform transform       string     domain = constraints real vector     codomain = constraints simplex     bijective = true      def   eq   self  other           return isinstance other  stickbreakingtransform       def  call self  x           offset = x shape -1    1 - x new ones x shape -1   cumsum -1          z =  clip sigmoid x - offset log            z cumprod =  1 - z  cumprod -1          y = pad z   0  1   value=1    pad z cumprod   1  0   value=1          return y      def  inverse self  y           y crop = y       -1          offset = y shape -1  - y new ones y crop shape -1   cumsum -1          sf = 1 - y crop cumsum -1                            sf = torch clamp sf  min=torch finfo y dtype  tiny          x = y crop log   - sf log     offset log           return x      def log abs det jacobian self  x  y           offset = x shape -1    1 - x new ones x shape -1   cumsum -1          x = x - offset log                    detj =  -x   f logsigmoid x    y       -1  log    sum -1          return detj      def forward shape self  shape           if len shape  < 1              raise valueerror string          return shape  -1     shape -1    1        def inverse shape self  shape           if len shape  < 1              raise valueerror string          return shape  -1     shape -1  - 1   
class transform object       string     bijective = false     domain  constraints constraint     codomain  constraints constraint      def   init   self  cache size=0           self  cache size = cache size         self  inv = none         if cache size == 0              pass           elif cache size == 1              self  cache x y = none  none         else              raise valueerror string          super transform  self    init           property     def event dim self           if self domain event dim == self codomain event dim              return self domain event dim         raise valueerror string        property     def inv self           string         inv = none         if self  inv be not none              inv = self  inv           if inv be none              inv =  inversetransform self              self  inv = weakref ref inv          return inv       property     def sign self           string         raise notimplementederror      def with cache self  cache size=1           if self  cache size == cache size              return self         if type self    init   be transform   init                return type self  cache size=cache size          raise notimplementederror string format type self         def   eq   self  other           return self be other      def   ne   self  other                    return not self   eq   other       def   call   self  x           string         if self  cache size == 0              return self  call x          x old  y old = self  cache x y         if x be x old              return y old         y = self  call x          self  cache x y = x  y         return y      def  inv call self  y           string         if self  cache size == 0              return self  inverse y          x old  y old = self  cache x y         if y be y old              return x old         x = self  inverse y          self  cache x y = x  y         return x      def  call self  x           string         raise notimplementederror      def  inverse self  y           string         raise notimplementederror      def log abs det jacobian self  x  y           string         raise notimplementederror      def   repr   self           return self   class     name     string      def forward shape self  shape           string         return shape      def inverse shape self  shape           string         return shape 
class constraint object       string     be discrete = false       event dim = 0        def check self  value           string         raise notimplementederror      def   repr   self           return self   class     name   1     string 
class constraintregistry object       string     def   init   self           self  registry =            super constraintregistry  self    init          def register self  constraint  factory=none           string                  if factory be none              return lambda factory  self register constraint  factory                    if isinstance constraint  constraints constraint               constraint = type constraint           if not isinstance constraint  type  or not issubclass constraint  constraints constraint               raise typeerror string                             string format constraint            self  registry constraint  = factory         return factory      def   call   self  constraint           string                  try              factory = self  registry type constraint           except keyerror              raise notimplementederror                  f cannot transform  type constraint    name    constraints   from none         return factory constraint  
def cdf self  value       string     raise notimplementederror 
def entropy self       string     raise notimplementederror 
def enumerate support self  expand=true       string     raise notimplementederror 
def expand self  batch shape   instance=none       string     raise notimplementederror 
def icdf self  value       string     raise notimplementederror 
def log prob self  value       string     raise notimplementederror 
def perplexity self       string     return torch exp self entropy    
def rsample self  sample shape=torch size         string     raise notimplementederror 
def sample self  sample shape=torch size         string     with torch no grad            return self rsample sample shape  
def sample n self  n       string     warn warn string  userwarning      return self sample torch size  n     
 staticmethod def set default validate args value       string     if value not in  true  false           raise valueerror     distribution  validate args = value 
def entropy self       string     result = -self  mean carrier measure     nparams =  p detach   require grad    for p in self  natural params      lg normal = self  log normalizer  nparams      gradients = torch autograd grad lg normal sum    nparams  create graph=true      result  = lg normal     for np  g in zip nparams  gradients           result -= np   g     return result 
def entropy self       return binary cross entropy with logits self logits  self probs  reduction=string  
def enumerate support self  expand=true       value = torch arange 2  dtype=self  param dtype  device=self  param device      value = value view  -1      1     len self  batch shape       if expand          value = value expand  -1     self  batch shape      return value 
def expand self  batch shape   instance=none       new = self  get check instance bernoulli   instance      batch shape = torch size batch shape      if string in self   dict            new probs = self probs expand batch shape          new  param = new probs     if string in self   dict            new logits = self logits expand batch shape          new  param = new logits     super bernoulli  new    init   batch shape  validate args=false      new  validate args = self  validate args     return new 
def log prob self  value       if self  validate args          self  validate sample value      logits  value = broadcast all self logits  value      return -binary cross entropy with logits logits  value  reduction=string  
def sample self  sample shape=torch size         shape = self  extend shape sample shape      with torch no grad            return torch bernoulli self probs expand shape   
def entropy self       return self  dirichlet entropy   
def expand self  batch shape   instance=none       new = self  get check instance beta   instance      batch shape = torch size batch shape      new  dirichlet = self  dirichlet expand batch shape      super beta  new    init   batch shape  validate args=false      new  validate args = self  validate args     return new 
def log prob self  value       if self  validate args          self  validate sample value      head tail = torch stack  value  1 0 - value   -1      return self  dirichlet log prob head tail  
def rsample self  sample shape=         return self  dirichlet rsample sample shape  select -1  0  
def entropy self       string     raise notimplementederror 
def enumerate support self  expand=true       total count = int self total count max        if not self total count min   == total count          raise notimplementederror string      value = torch arange 1   total count  dtype=self  param dtype  device=self  param device      value = value view  -1      1     len self  batch shape       if expand          value = value expand  -1     self  batch shape      return value 
def expand self  batch shape   instance=none       new = self  get check instance binomial   instance      batch shape = torch size batch shape      new total count = self total count expand batch shape      if string in self   dict            new probs = self probs expand batch shape          new  param = new probs     if string in self   dict            new logits = self logits expand batch shape          new  param = new logits     super binomial  new    init   batch shape  validate args=false      new  validate args = self  validate args     return new 
def log prob self  value       if self  validate args          self  validate sample value      log factorial n = torch lgamma self total count   1      log factorial k = torch lgamma value   1      log factorial nmk = torch lgamma self total count - value   1                               normalize term =  self total count    clamp by zero self logits                          self total count   torch log1p torch exp -torch abs self logits                          - log factorial n      return value   self logits - log factorial k - log factorial nmk - normalize term 
def sample self  sample shape=torch size         shape = self  extend shape sample shape      with torch no grad            return torch binomial self total count expand shape   self probs expand shape   
def entropy self       min real = torch finfo self logits dtype  min     logits = torch clamp self logits  min=min real      p log p = logits   self probs     return -p log p sum -1  
def enumerate support self  expand=true       num events = self  num events     value = torch arange num events  dtype=torch long  device=self  param device      value = value view  -1      1     len self  batch shape       if expand          value = value expand  -1     self  batch shape      return value 
def expand self  batch shape   instance=none       new = self  get check instance categorical   instance      batch shape = torch size batch shape      param shape = batch shape   torch size  self  num events        if string in self   dict            new probs = self probs expand param shape          new  param = new probs     if string in self   dict            new logits = self logits expand param shape          new  param = new logits     new  num events = self  num events     super categorical  new    init   batch shape  validate args=false      new  validate args = self  validate args     return new 
def log prob self  value       if self  validate args          self  validate sample value      value = value long   unsqueeze -1      value  log pmf = torch broadcast tensors value  self logits      value = value       1      return log pmf gather -1  value  squeeze -1  
def sample self  sample shape=torch size         if not isinstance sample shape  torch size           sample shape = torch size sample shape      probs 2d = self probs reshape -1  self  num events      sample 2d = torch multinomial probs 2d  sample shape numel    true  t     return sample 2d reshape self  extend shape sample shape   
def cdf self  value       if self  validate args          self  validate sample value      return torch atan  value - self loc  / self scale  / math pi   0 5 
def entropy self       return math log 4   math pi    self scale log   
def expand self  batch shape   instance=none       new = self  get check instance cauchy   instance      batch shape = torch size batch shape      new loc = self loc expand batch shape      new scale = self scale expand batch shape      super cauchy  new    init   batch shape  validate args=false      new  validate args = self  validate args     return new 
def icdf self  value       return torch tan math pi    value - 0 5     self scale   self loc 
def log prob self  value       if self  validate args          self  validate sample value      return -math log math pi  - self scale log   -  1     value - self loc  / self scale   2  log   
def rsample self  sample shape=torch size         shape = self  extend shape sample shape      eps = self loc new shape  cauchy        return self loc   eps   self scale 
def expand self  batch shape   instance=none       new = self  get check instance chi2   instance      return super chi2  self  expand batch shape  new  
def cdf self  value       if self  validate args          self  validate sample value      cut probs = self  cut probs       cdfs =  torch pow cut probs  value    torch pow 1 0 - cut probs  1 0 - value                cut probs - 1 0  /  2 0   cut probs - 1 0      unbounded cdfs = torch where self  outside unstable region    cdfs  value      return torch where          torch le value  0 0           torch zero like value           torch where torch ge value  1 0   torch ones like value   unbounded cdfs   
def entropy self       log probs0 = torch log1p -self probs      log probs1 = torch log self probs      return self mean    log probs0 - log probs1  - self  cont bern log norm   - log probs0 
def expand self  batch shape   instance=none       new = self  get check instance continuousbernoulli   instance      new  lims = self  lims     batch shape = torch size batch shape      if string in self   dict            new probs = self probs expand batch shape          new  param = new probs     if string in self   dict            new logits = self logits expand batch shape          new  param = new logits     super continuousbernoulli  new    init   batch shape  validate args=false      new  validate args = self  validate args     return new 
def icdf self  value       cut probs = self  cut probs       return torch where          self  outside unstable region             torch log1p -cut probs   value    2 0   cut probs - 1 0            - torch log1p -cut probs   /  torch log cut probs  - torch log1p -cut probs            value  
def log prob self  value       if self  validate args          self  validate sample value      logits  value = broadcast all self logits  value      return -binary cross entropy with logits logits  value  reduction=string    self  cont bern log norm   
def rsample self  sample shape=torch size         shape = self  extend shape sample shape      u = torch rand shape  dtype=self probs dtype  device=self probs device      return self icdf u  
def sample self  sample shape=torch size         shape = self  extend shape sample shape      u = torch rand shape  dtype=self probs dtype  device=self probs device      with torch no grad            return self icdf u  
def entropy self       k = self concentration size -1      a0 = self concentration sum -1      return  torch lgamma self concentration  sum -1  - torch lgamma a0  -              k - a0    torch digamma a0  -               self concentration - 1 0    torch digamma self concentration   sum -1   
def expand self  batch shape   instance=none       new = self  get check instance dirichlet   instance      batch shape = torch size batch shape      new concentration = self concentration expand batch shape   self event shape      super dirichlet  new    init   batch shape  self event shape  validate args=false      new  validate args = self  validate args     return new 
def log prob self  value       if self  validate args          self  validate sample value      return   torch log value     self concentration - 1 0   sum -1                torch lgamma self concentration sum -1   -             torch lgamma self concentration  sum -1   
def rsample self  sample shape=         shape = self  extend shape sample shape      concentration = self concentration expand shape      return  dirichlet apply concentration  
def cdf self  value       if self  validate args          self  validate sample value      return 1 - torch exp -self rate   value  
def entropy self       return 1 0 - torch log self rate  
def expand self  batch shape   instance=none       new = self  get check instance exponential   instance      batch shape = torch size batch shape      new rate = self rate expand batch shape      super exponential  new    init   batch shape  validate args=false      new  validate args = self  validate args     return new 
def icdf self  value       return -torch log 1 - value  / self rate 
def log prob self  value       if self  validate args          self  validate sample value      return self rate log   - self rate   value 
def rsample self  sample shape=torch size         shape = self  extend shape sample shape      if torch  c  get trace state                     u = torch rand shape  dtype=self rate dtype  device=self rate device          return - -u  log1p   / self rate     return self rate new shape  exponential    / self rate 
def expand self  batch shape   instance=none       new = self  get check instance fishersnedecor   instance      batch shape = torch size batch shape      new df1 = self df1 expand batch shape      new df2 = self df2 expand batch shape      new  gamma1 = self  gamma1 expand batch shape      new  gamma2 = self  gamma2 expand batch shape      super fishersnedecor  new    init   batch shape  validate args=false      new  validate args = self  validate args     return new 
def log prob self  value       if self  validate args          self  validate sample value      ct1 = self df1   0 5     ct2 = self df2   0 5     ct3 = self df1 / self df2     t1 =  ct1   ct2  lgamma   - ct1 lgamma   - ct2 lgamma       t2 = ct1   ct3 log      ct1 - 1    torch log value      t3 =  ct1   ct2    torch log1p ct3   value      return t1   t2 - t3 
def rsample self  sample shape=torch size           shape = self  extend shape sample shape                x1 = self  gamma1 rsample sample shape  view shape      x2 = self  gamma2 rsample sample shape  view shape      tiny = torch finfo x2 dtype  tiny     x2 clamp  min=tiny      y = x1 / x2     y clamp  min=tiny      return y 
def entropy self       return  self concentration - torch log self rate    torch lgamma self concentration                 1 0 - self concentration    torch digamma self concentration   
def expand self  batch shape   instance=none       new = self  get check instance gamma   instance      batch shape = torch size batch shape      new concentration = self concentration expand batch shape      new rate = self rate expand batch shape      super gamma  new    init   batch shape  validate args=false      new  validate args = self  validate args     return new 
def log prob self  value       value = torch as tensor value  dtype=self rate dtype  device=self rate device      if self  validate args          self  validate sample value      return  self concentration   torch log self rate                 self concentration - 1    torch log value  -             self rate   value - torch lgamma self concentration   
def rsample self  sample shape=torch size         shape = self  extend shape sample shape      value =  standard gamma self concentration expand shape   / self rate expand shape      value detach   clamp  min=torch finfo value dtype  tiny        return value 
def entropy self       return binary cross entropy with logits self logits  self probs  reduction=string  / self probs 
def expand self  batch shape   instance=none       new = self  get check instance geometric   instance      batch shape = torch size batch shape      if string in self   dict            new probs = self probs expand batch shape      if string in self   dict            new logits = self logits expand batch shape      super geometric  new    init   batch shape  validate args=false      new  validate args = self  validate args     return new 
def log prob self  value       if self  validate args          self  validate sample value      value  probs = broadcast all value  self probs      probs = probs clone memory format=torch contiguous format      probs  probs == 1     value == 0   = 0     return value    -probs  log1p     self probs log   
def sample self  sample shape=torch size         shape = self  extend shape sample shape      tiny = torch finfo self probs dtype  tiny     with torch no grad            if torch  c  get trace state                             u = torch rand shape  dtype=self probs dtype  device=self probs device              u = u clamp min=tiny          else              u = self probs new shape  uniform  tiny  1          return  u log   /  -self probs  log1p    floor   
def entropy self       return self scale log      1   euler constant  
def expand self  batch shape   instance=none       new = self  get check instance gumbel   instance      new loc = self loc expand batch shape      new scale = self scale expand batch shape      return super gumbel  self  expand batch shape   instance=new  
def log prob self  value       if self  validate args          self  validate sample value      y =  self loc - value  / self scale     return  y - y exp    - self scale log   
def cdf self  value       if self  validate args          self  validate sample value      return 2   self base dist cdf value  - 1 
def entropy self       return self base dist entropy   - math log 2  
def expand self  batch shape   instance=none       new = self  get check instance halfcauchy   instance      return super halfcauchy  self  expand batch shape   instance=new  
def icdf self  prob       return self base dist icdf  prob   1  / 2  
def log prob self  value       if self  validate args          self  validate sample value      value = torch as tensor value  dtype=self base dist scale dtype                              device=self base dist scale device      log prob = self base dist log prob value    math log 2      log prob value expand log prob shape  < 0  = -inf     return log prob 
def cdf self  value       if self  validate args          self  validate sample value      return 2   self base dist cdf value  - 1 
def entropy self       return self base dist entropy   - math log 2  
def expand self  batch shape   instance=none       new = self  get check instance halfnormal   instance      return super halfnormal  self  expand batch shape   instance=new  
def icdf self  prob       return self base dist icdf  prob   1  / 2  
def log prob self  value       if self  validate args          self  validate sample value      log prob = self base dist log prob value    math log 2      log prob value expand log prob shape  < 0  = -inf     return log prob 
def entropy self       entropy = self base dist entropy       return  sum rightmost entropy  self reinterpret batch ndims  
def enumerate support self  expand=true       if self reinterpret batch ndims > 0          raise notimplementederror string      return self base dist enumerate support expand=expand  
def expand self  batch shape   instance=none       new = self  get check instance independent   instance      batch shape = torch size batch shape      new base dist = self base dist expand batch shape                                             self event shape  self reinterpret batch ndims       new reinterpret batch ndims = self reinterpret batch ndims     super independent  new    init   batch shape  self event shape  validate args=false      new  validate args = self  validate args     return new 
def log prob self  value       log prob = self base dist log prob value      return  sum rightmost log prob  self reinterpret batch ndims  
def rsample self  sample shape=torch size         return self base dist rsample sample shape  
def sample self  sample shape=torch size         return self base dist sample sample shape  
def entropy self       t1 =  1 - self concentration1 reciprocal        t0 =  1 - self concentration0 reciprocal        h0 = torch digamma self concentration0   1    euler constant     return t0   t1   h0 - torch log self concentration1  - torch log self concentration0  
def expand self  batch shape   instance=none       new = self  get check instance kumaraswamy   instance      new concentration1 = self concentration1 expand batch shape      new concentration0 = self concentration0 expand batch shape      return super kumaraswamy  self  expand batch shape   instance=new  
def expand self  batch shape   instance=none       new = self  get check instance lkjcholesky   instance      batch shape = torch size batch shape      new dim = self dim     new concentration = self concentration expand batch shape      new  beta = self  beta expand batch shape    self dim        super lkjcholesky  new    init   batch shape  self event shape  validate args=false      new  validate args = self  validate args     return new 
def log prob self  value                                                    if self  validate args          self  validate sample value      diag elems = value diagonal dim1=-1  dim2=-2       1       order = torch arange 2  self dim   1      order = 2    self concentration - 1  unsqueeze -1    self dim - order     unnormalized log pdf = torch sum order   diag elems log    dim=-1           dm1 = self dim - 1     alpha = self concentration   0 5   dm1     denominator = torch lgamma alpha    dm1     numerator = torch mvlgamma alpha - 0 5  dm1                     pi constant = 0 5   dm1   math log math pi      normalize term = pi constant   numerator - denominator     return unnormalized log pdf - normalize term 
def sample self  sample shape=torch size                                       y = self  beta sample sample shape  unsqueeze -1      u normal = torch randn self  extend shape sample shape                              dtype=y dtype                             device=y device  tril -1      u hypersphere = u normal / u normal norm dim=-1  keepdim=true           u hypersphere      0     fill  0       w = torch sqrt y    u hypersphere          eps = torch finfo w dtype  tiny     diag elems = torch clamp 1 - torch sum w  2  dim=-1   min=eps  sqrt       w  = torch diag embed diag elems      return w 
def cdf self  value       if self  validate args          self  validate sample value      return 0 5 - 0 5    value - self loc  sign     torch expm1 - value - self loc  abs   / self scale  
def entropy self       return 1   torch log 2   self scale  
def expand self  batch shape   instance=none       new = self  get check instance laplace   instance      batch shape = torch size batch shape      new loc = self loc expand batch shape      new scale = self scale expand batch shape      super laplace  new    init   batch shape  validate args=false      new  validate args = self  validate args     return new 
def icdf self  value       term = value - 0 5     return self loc - self scale    term  sign     torch log1p -2   term abs    
def log prob self  value       if self  validate args          self  validate sample value      return -torch log 2   self scale  - torch abs value - self loc  / self scale 
def rsample self  sample shape=torch size         shape = self  extend shape sample shape      finfo = torch finfo self loc dtype      if torch  c  get trace state                     u = torch rand shape  dtype=self loc dtype  device=self loc device    2 - 1         return self loc - self scale   u sign     torch log1p -u abs   clamp min=finfo tiny       u = self loc new shape  uniform  finfo eps - 1  1                return self loc - self scale   u sign     torch log1p -u abs    
def entropy self       return self base dist entropy     self loc 
def expand self  batch shape   instance=none       new = self  get check instance lognormal   instance      return super lognormal  self  expand batch shape   instance=new  
def entropy self       log det =  batch lowrank logdet self  unbroadcasted cov factor                                      self  unbroadcasted cov diag                                      self  capacitance tril      h = 0 5    self  event shape 0     1 0   math log 2   math pi     log det      if len self  batch shape  == 0          return h     else          return h expand self  batch shape  
def expand self  batch shape   instance=none       new = self  get check instance lowrankmultivariatenormal   instance      batch shape = torch size batch shape      loc shape = batch shape   self event shape     new loc = self loc expand loc shape      new cov diag = self cov diag expand loc shape      new cov factor = self cov factor expand loc shape   self cov factor shape -1        new  unbroadcasted cov factor = self  unbroadcasted cov factor     new  unbroadcasted cov diag = self  unbroadcasted cov diag     new  capacitance tril = self  capacitance tril     super lowrankmultivariatenormal  new    init   batch shape                                                     self event shape                                                     validate args=false      new  validate args = self  validate args     return new 
def log prob self  value       if self  validate args          self  validate sample value      diff = value - self loc     m =  batch lowrank mahalanobis self  unbroadcasted cov factor                                     self  unbroadcasted cov diag                                     diff                                     self  capacitance tril      log det =  batch lowrank logdet self  unbroadcasted cov factor                                      self  unbroadcasted cov diag                                      self  capacitance tril      return -0 5    self  event shape 0    math log 2   math pi    log det   m  
def rsample self  sample shape=torch size         shape = self  extend shape sample shape      w shape = shape  -1    self cov factor shape -1       eps w =  standard normal w shape  dtype=self loc dtype  device=self loc device      eps d =  standard normal shape  dtype=self loc dtype  device=self loc device      return  self loc    batch mv self  unbroadcasted cov factor  eps w                self  unbroadcasted cov diag sqrt     eps d  
def cdf self  x       x = self  pad x      cdf x = self component distribution cdf x      mix prob = self mixture distribution probs      return torch sum cdf x   mix prob  dim=-1  
def expand self  batch shape   instance=none       batch shape = torch size batch shape      batch shape comp = batch shape    self  num component       new = self  get check instance mixturesamefamily   instance      new  component distribution = \         self  component distribution expand batch shape comp      new  mixture distribution = \         self  mixture distribution expand batch shape      new  num component = self  num component     new  event ndims = self  event ndims     event shape = new  component distribution event shape     super mixturesamefamily  new    init   batch shape=batch shape                                             event shape=event shape                                             validate args=false      new  validate args = self  validate args     return new 
def log prob self  x       if self  validate args          self  validate sample x      x = self  pad x      log prob x = self component distribution log prob x        log mix prob = torch log softmax self mixture distribution logits                                       dim=-1        return torch logsumexp log prob x   log mix prob  dim=-1    
def sample self  sample shape=torch size         with torch no grad            sample len = len sample shape          batch len = len self batch shape          gather dim = sample len   batch len         es = self event shape                   mix sample = self mixture distribution sample sample shape          mix shape = mix sample shape                   comp sample = self component distribution sample sample shape                    mix sample r = mix sample reshape              mix shape   torch size  1     len es    1            mix sample r = mix sample r repeat              torch size  1    len mix shape     torch size  1     es           sample = torch gather comp sample  gather dim  mix sample r          return sample squeeze gather dim  
def entropy self       string     raise notimplementederror 
def expand self  batch shape   instance=none       new = self  get check instance multinomial   instance      batch shape = torch size batch shape      new total count = self total count     new  categorical = self  categorical expand batch shape      super multinomial  new    init   batch shape  self event shape  validate args=false      new  validate args = self  validate args     return new 
def log prob self  value       if self  validate args          self  validate sample value      logits  value = broadcast all self logits  value      logits = logits clone memory format=torch contiguous format      log factorial n = torch lgamma value sum -1    1      log factorial xs = torch lgamma value   1  sum -1      logits  value == 0     logits == -inf   = 0     log power =  logits   value  sum -1      return log factorial n - log factorial xs   log power 
def sample self  sample shape=torch size         sample shape = torch size sample shape      sample = self  categorical sample torch size  self total count      sample shape                shift idx = list range sample dim         shift idx append shift idx pop 0       sample = sample permute  shift idx      count = sample new self  extend shape sample shape   zero        count scatter add  -1  sample  torch ones like sample       return count type as self probs  
def entropy self       half log det = self  unbroadcasted scale tril diagonal dim1=-2  dim2=-1  log   sum -1      h = 0 5   self  event shape 0     1 0   math log 2   math pi     half log det     if len self  batch shape  == 0          return h     else          return h expand self  batch shape  
def expand self  batch shape   instance=none       new = self  get check instance multivariatenormal   instance      batch shape = torch size batch shape      loc shape = batch shape   self event shape     cov shape = batch shape   self event shape   self event shape     new loc = self loc expand loc shape      new  unbroadcasted scale tril = self  unbroadcasted scale tril     if string in self   dict            new covariance matrix = self covariance matrix expand cov shape      if string in self   dict            new scale tril = self scale tril expand cov shape      if string in self   dict            new precision matrix = self precision matrix expand cov shape      super multivariatenormal  new    init   batch shape                                              self event shape                                              validate args=false      new  validate args = self  validate args     return new 
def log prob self  value       if self  validate args          self  validate sample value      diff = value - self loc     m =  batch mahalanobis self  unbroadcasted scale tril  diff      half log det = self  unbroadcasted scale tril diagonal dim1=-2  dim2=-1  log   sum -1      return -0 5    self  event shape 0    math log 2   math pi    m  - half log det 
def rsample self  sample shape=torch size         shape = self  extend shape sample shape      eps =  standard normal shape  dtype=self loc dtype  device=self loc device      return self loc    batch mv self  unbroadcasted scale tril  eps  
def expand self  batch shape   instance=none       new = self  get check instance negativebinomial   instance      batch shape = torch size batch shape      new total count = self total count expand batch shape      if string in self   dict            new probs = self probs expand batch shape          new  param = new probs     if string in self   dict            new logits = self logits expand batch shape          new  param = new logits     super negativebinomial  new    init   batch shape  validate args=false      new  validate args = self  validate args     return new 
def log prob self  value       if self  validate args          self  validate sample value       log unnormalized prob =  self total count   f logsigmoid -self logits                                 value   f logsigmoid self logits        log normalization =  -torch lgamma self total count   value    torch lgamma 1    value                             torch lgamma self total count        return log unnormalized prob - log normalization 
def sample self  sample shape=torch size         with torch no grad            rate = self  gamma sample sample shape=sample shape          return torch poisson rate  
def cdf self  value       if self  validate args          self  validate sample value      return 0 5    1   torch erf  value - self loc    self scale reciprocal   / math sqrt 2    
def entropy self       return 0 5   0 5   math log 2   math pi    torch log self scale  
def expand self  batch shape   instance=none       new = self  get check instance normal   instance      batch shape = torch size batch shape      new loc = self loc expand batch shape      new scale = self scale expand batch shape      super normal  new    init   batch shape  validate args=false      new  validate args = self  validate args     return new 
def icdf self  value       return self loc   self scale   torch erfinv 2   value - 1    math sqrt 2  
def log prob self  value       if self  validate args          self  validate sample value           var =  self scale    2      log scale = math log self scale  if isinstance self scale  real  else self scale log       return -  value - self loc     2  /  2   var  - log scale - math log math sqrt 2   math pi   
def rsample self  sample shape=torch size         shape = self  extend shape sample shape      eps =  standard normal shape  dtype=self loc dtype  device=self loc device      return self loc   eps   self scale 
def sample self  sample shape=torch size         shape = self  extend shape sample shape      with torch no grad            return torch normal self loc expand shape   self scale expand shape   
def entropy self       return self  categorical entropy   
def enumerate support self  expand=true       n = self event shape 0      value = torch eye n  dtype=self  param dtype  device=self  param device      value = value view  n      1     len self batch shape     n        if expand          value = value expand  n     self batch shape    n        return value 
def expand self  batch shape   instance=none       new = self  get check instance onehotcategorical   instance      batch shape = torch size batch shape      new  categorical = self  categorical expand batch shape      super onehotcategorical  new    init   batch shape  self event shape  validate args=false      new  validate args = self  validate args     return new 
def log prob self  value       if self  validate args          self  validate sample value      indices = value max -1  1      return self  categorical log prob indices  
def sample self  sample shape=torch size         sample shape = torch size sample shape      probs = self  categorical probs     num events = self  categorical  num events     indices = self  categorical sample sample shape      return torch nn functional one hot indices  num events  to probs  
def entropy self       return   self scale / self alpha  log      1   self alpha reciprocal     
def expand self  batch shape   instance=none       new = self  get check instance pareto   instance      new scale = self scale expand batch shape      new alpha = self alpha expand batch shape      return super pareto  self  expand batch shape   instance=new  
def expand self  batch shape   instance=none       new = self  get check instance poisson   instance      batch shape = torch size batch shape      new rate = self rate expand batch shape      super poisson  new    init   batch shape  validate args=false      new  validate args = self  validate args     return new 
def log prob self  value       if self  validate args          self  validate sample value      rate  value = broadcast all self rate  value      return  rate log     value  - rate -  value   1  lgamma   
def sample self  sample shape=torch size         shape = self  extend shape sample shape      with torch no grad            return torch poisson self rate expand shape   
def expand self  batch shape   instance=none       new = self  get check instance relaxedbernoulli   instance      return super relaxedbernoulli  self  expand batch shape   instance=new  
def expand self  batch shape   instance=none       new = self  get check instance logitrelaxedbernoulli   instance      batch shape = torch size batch shape      new temperature = self temperature     if string in self   dict            new probs = self probs expand batch shape          new  param = new probs     if string in self   dict            new logits = self logits expand batch shape          new  param = new logits     super logitrelaxedbernoulli  new    init   batch shape  validate args=false      new  validate args = self  validate args     return new 
def log prob self  value       if self  validate args          self  validate sample value      logits  value = broadcast all self logits  value      diff = logits - value mul self temperature      return self temperature log     diff - 2   diff exp   log1p   
def rsample self  sample shape=torch size         shape = self  extend shape sample shape      probs = clamp probs self probs expand shape       uniform = clamp probs torch rand shape  dtype=probs dtype  device=probs device       return  uniform log   -  -uniforms  log1p     probs log   -  -probs  log1p    / self temperature 
def expand self  batch shape   instance=none       new = self  get check instance relaxedonehotcategorical   instance      return super relaxedonehotcategorical  self  expand batch shape   instance=new  
def entropy self       lbeta = torch lgamma 0 5   self df    math lgamma 0 5  - torch lgamma 0 5    self df   1       return  self scale log                 0 5    self df   1                 torch digamma 0 5    self df   1   - torch digamma 0 5   self df                 0 5   self df log     lbeta  
def expand self  batch shape   instance=none       new = self  get check instance studentt   instance      batch shape = torch size batch shape      new df = self df expand batch shape      new loc = self loc expand batch shape      new scale = self scale expand batch shape      new  chi2 = self  chi2 expand batch shape      super studentt  new    init   batch shape  validate args=false      new  validate args = self  validate args     return new 
def log prob self  value       if self  validate args          self  validate sample value      y =  value - self loc  / self scale     z =  self scale log              0 5   self df log              0 5   math log math pi             torch lgamma 0 5   self df  -          torch lgamma 0 5    self df   1         return -0 5    self df   1     torch log1p y  2  / self df  - z 
def rsample self  sample shape=torch size                                        shape = self  extend shape sample shape      x =  standard normal shape  dtype=self df dtype  device=self df device      z = self  chi2 rsample sample shape      y = x   torch rsqrt z / self df      return self loc   self scale   y 
def cdf self  value       string     for transform in self transform   -1           value = transform inv value      if self  validate args          self base dist  validate sample value      value = self base dist cdf value      value = self  monotonize cdf value      return value 
def expand self  batch shape   instance=none       new = self  get check instance transformeddistribution   instance      batch shape = torch size batch shape      shape = batch shape   self event shape     for t in reverse self transform           shape = t inverse shape shape      base batch shape = shape  len shape  - len self base dist event shape       new base dist = self base dist expand base batch shape      new transform = self transform     super transformeddistribution  new    init   batch shape  self event shape  validate args=false      new  validate args = self  validate args     return new 
def icdf self  value       string     value = self  monotonize cdf value      if self  validate args          self base dist  validate sample value      value = self base dist icdf value      for transform in self transform          value = transform value      return value 
def log prob self  value       string     if self  validate args          self  validate sample value      event dim = len self event shape      log prob = 0 0     y = value     for transform in reverse self transform           x = transform inv y          event dim  = transform domain event dim - transform codomain event dim         log prob = log prob -  sum rightmost transform log abs det jacobian x  y                                                event dim - transform domain event dim          y = x      log prob = log prob    sum rightmost self base dist log prob y                                            event dim - len self base dist event shape       return log prob 
def rsample self  sample shape=torch size         string     x = self base dist rsample sample shape      for transform in self transform          x = transform x      return x 
def sample self  sample shape=torch size         string     with torch no grad            x = self base dist sample sample shape          for transform in self transform              x = transform x          return x 
def cdf self  value       if self  validate args          self  validate sample value      result =  value - self low  /  self high - self low      return result clamp min=0  max=1  
def entropy self       return torch log self high - self low  
def expand self  batch shape   instance=none       new = self  get check instance uniform   instance      batch shape = torch size batch shape      new low = self low expand batch shape      new high = self high expand batch shape      super uniform  new    init   batch shape  validate args=false      new  validate args = self  validate args     return new 
def icdf self  value       result = value    self high - self low    self low     return result 
def log prob self  value       if self  validate args          self  validate sample value      lb = self low le value  type as self low      ub = self high gt value  type as self low      return torch log lb mul ub   - torch log self high - self low  
def rsample self  sample shape=torch size         shape = self  extend shape sample shape      rand = torch rand shape  dtype=self low dtype  device=self low device      return self low   rand    self high - self low  
def expand self  batch shape       try          return super vonmises  self  expand batch shape      except notimplementederror          validate args = self   dict   get string          loc = self loc expand batch shape          concentration = self concentration expand batch shape          return type self  loc  concentration  validate args=validate args  
def log prob self  value       log prob = self concentration   torch cos value - self loc      log prob = log prob - math log 2   math pi  -  log modify bessel fn self concentration  order=0      return log prob 
 torch no grad   def sample self  sample shape=torch size         string     shape = self  extend shape sample shape      x = torch empty shape  dtype=self loc dtype  device=self loc device      return  rejection sample self loc  self concentration  self  proposal r  x  
def entropy self       return euler constant    1 - self concentration reciprocal    \         torch log self scale   self concentration reciprocal    1 
def expand self  batch shape   instance=none       new = self  get check instance weibull   instance      new scale = self scale expand batch shape      new concentration = self concentration expand batch shape      new concentration reciprocal = new concentration reciprocal       base dist = self base dist expand batch shape      transform =  powertransform exponent=new concentration reciprocal                     affinetransform loc=0  scale=new scale       super weibull  new    init   base dist                                   transform                                   validate args=false      new  validate args = self  validate args     return new 
def log abs det jacobian self  x  y       string     raise notimplementederror 
def forward shape self  shape       string     return shape 
def inverse shape self  shape       string     return shape 
def check self  value       string     raise notimplementederror 
def register self  constraint  factory=none       string          if factory be none          return lambda factory  self register constraint  factory            if isinstance constraint  constraints constraint           constraint = type constraint       if not isinstance constraint  type  or not issubclass constraint  constraints constraint           raise typeerror string                         string format constraint        self  registry constraint  = factory     return factory 
def kl divergence p  q       r        compute kullback-leibler divergence  math `kl p \  q ` between two distributions          math            kl p \  q  = \int p x  \log\frac  p x    q x   \ dx      args          p  distribution   a  class `~torch distributions distribution` object          q  distribution   a  class `~torch distributions distribution` object       return          tensor  a batch of kl divergences of shape `batch shape`       raise          notimplementederror  if the distribution type have not be register via              meth `register kl`              try          fun =  kl memoize type p   type q       except keyerror          fun =  dispatch kl type p   type q            kl memoize type p   type q   = fun     if fun be notimplemented          raise notimplementederror     return fun p  q  
def register kl type p  type q       string     if not isinstance type p  type  and issubclass type p  distribution           raise typeerror string format type p       if not isinstance type q  type  and issubclass type q  distribution           raise typeerror string format type q        def decorator fun            kl registry type p  type q  = fun          kl memoize clear             return fun      return decorator 
class  dependentproperty property   dependent       string     def   init   self  fn=none     be discrete=notimplemented  event dim=notimplemented           super     init   fn          self  be discrete = be discrete         self  event dim = event dim      def   call   self  fn           string         return  dependentproperty fn  be discrete=self  be discrete  event dim=self  event dim  
class  greaterthan constraint       string     def   init   self  lower bind           self lower bind = lower bind         super     init          def check self  value           return self lower bind < value      def   repr   self           fmt string = self   class     name   1           fmt string  = string format self lower bind          return fmt string 
class  greaterthaneq constraint       string     def   init   self  lower bind           self lower bind = lower bind         super     init          def check self  value           return self lower bind <= value      def   repr   self           fmt string = self   class     name   1           fmt string  = string format self lower bind          return fmt string 
class  independentconstraint constraint       string     def   init   self  base constraint  reinterpret batch ndims           assert isinstance base constraint  constraint          assert isinstance reinterpret batch ndims  int          assert reinterpret batch ndims >= 0         self base constraint = base constraint         self reinterpret batch ndims = reinterpret batch ndims         super     init           property     def be discrete self           return self base constraint be discrete       property     def event dim self           return self base constraint event dim   self reinterpret batch ndims      def check self  value           result = self base constraint check value          if result dim   < self reinterpret batch ndims              expect = self base constraint event dim   self reinterpret batch ndims             raise valueerror f expect value dim   >=  expect  but get  value dim              result = result reshape result shape  result dim   - self reinterpret batch ndims     -1            result = result all -1          return result      def   repr   self           return string format self   class     name   1    repr self base constraint                                      self reinterpret batch ndims  
class  integerinterval constraint       string     be discrete = true      def   init   self  lower bind  upper bind           self lower bind = lower bind         self upper bind = upper bind         super     init          def check self  value           return  value   1 == 0     self lower bind <= value     value <= self upper bind       def   repr   self           fmt string = self   class     name   1           fmt string  = string format self lower bind  self upper bind          return fmt string 
class  interval constraint       string     def   init   self  lower bind  upper bind           self lower bind = lower bind         self upper bind = upper bind         super     init          def check self  value           return  self lower bind <= value     value <= self upper bind       def   repr   self           fmt string = self   class     name   1           fmt string  = string format self lower bind  self upper bind          return fmt string 
class  halfopeninterval constraint       string     def   init   self  lower bind  upper bind           self lower bind = lower bind         self upper bind = upper bind         super     init          def check self  value           return  self lower bind <= value     value < self upper bind       def   repr   self           fmt string = self   class     name   1           fmt string  = string format self lower bind  self upper bind          return fmt string 
class  lessthan constraint       string     def   init   self  upper bind           self upper bind = upper bind         super     init          def check self  value           return value < self upper bind      def   repr   self           fmt string = self   class     name   1           fmt string  = string format self upper bind          return fmt string 
class  multinomial constraint       string     be discrete = true     event dim = 1      def   init   self  upper bind           self upper bind = upper bind      def check self  x           return  x >= 0  all dim=-1     x sum dim=-1  <= self upper bind  
class  stack constraint       string     def   init   self  cseq  dim=0           assert all isinstance c  constraint  for c in cseq          self cseq = list cseq          self dim = dim         super     init           property     def be discrete self           return any c be discrete for c in self cseq        property     def event dim self           dim = max c event dim for c in self cseq          if self dim   dim < 0              dim  = 1         return dim      def check self  value           assert -value dim   <= self dim < value dim           vs =  value select self dim  i  for i in range value size self dim            return torch stack  constr check v                              for v  constr in zip vs  self cseq    self dim  
def add do callback self  callback         r        append the give callback function to this ``future``  which will be run     when the ``future`` be complete   multiple callbacks can be add to     the same ``future``  but the order in which they will be execute cannot     be guarantee  the callback must take one argument  which be the     reference to this ``future``  the callback function can use the      meth `value` method to get the value  note that if this ``future`` be     already complete  the give callback will be run inline       we recommend that you use the  meth `then` method as it provide a way     to synchronize after your callback have complete  ``add do callback``     can be cheaper if your callback do not return anything  but both      meth `then` and ``add do callback`` use the same callback     registration api under the hood       with respect to gpu tensors  this method behave in the same way as      meth `then`       args          callback ``future``   a ``callable`` that take in one argument          which be the reference to this ``future``          note   note that if the callback function throw  either         through the original future be complete with an exception and         call ``fut wait  ``  or through other code in the callback          error handle must be carefully take care of  for example  if         this callback later complete additional futures  those futures be         not mark as complete with an error and the user be responsible         for handle completion/waiting on those futures independently       example           >>> import torch         >>>         >>> def callback fut           >>>     print f this will run after the future have finish            >>>     print fut wait            >>>         >>> fut = torch futures future           >>> fut add do callback callback          >>> fut set result 5          >>>         >>>   output be          >>> this will run after the future have finish          >>> 5             super   add do callback callback  
def do self  -> bool      r        return ``true`` if this ``future`` be do  a ``future`` be do if it     have a result or an exception       if the value contain tensors that reside on gpus  ``future do  ``     will return ``true`` even if the asynchronous kernels that be     populate those tensors haven t yet complete run on the device      because at such stage the result be already usable  provide one     perform the appropriate synchronizations  see  meth `wait`               return super   do   
def set exception self  result  t  -> none      r        set an exception for this ``future``  which will mark this ``future`` as     complete with an error and trigger all attach callbacks  note that     when call wait  /value   on this ``future``  the exception set here     will be raise inline       args          result  baseexception   the exception for this ``future``       example           >>> import torch         >>>         >>> fut = torch futures future           >>> fut set exception valueerror  foo            >>> fut wait           >>>         >>>   output          >>>   this will run after the future have finish          >>> valueerror  foo             assert isinstance result  exception   f  result  be of type  type result    not an exception        def raise error fut result           raise fut result      super    set unwrap func raise error      self set result result    
def set result self  result  t  -> none      r        set the result for this ``future``  which will mark this ``future`` as     complete and trigger all attach callbacks  note that a ``future``     cannot be mark complete twice       if the result contain tensors that reside on gpus  this method can be     call even if the asynchronous kernels that be populate those     tensors haven t yet complete run on the device  provide that the     stream on which those kernels be enqueued be set as the current ones     when this method be call  put simply  it s safe to call this method     immediately after launch those kernels  without any additional     synchronization  as long as one doesn t change stream in between  this     method will record events on all the relevant current stream and will     use them to ensure proper schedule for all the consumers of this     ``future``       args          result  object   the result object of this ``future``       example           >>> import thread         >>> import time         >>> import torch         >>>         >>> def slow set future fut  value           >>>     time sleep 0 5          >>>     fut set result value          >>>         >>> fut = torch futures future           >>> t = thread thread          >>>     target=slow set future          >>>     args= fut  torch ones 2    3          >>>           >>> t start           >>>         >>> print fut wait       tensor  3   3            >>> t join               super   set result result  
def then self  callback         r        append the give callback function to this ``future``  which will be run     when the ``future`` be complete   multiple callbacks can be add to     the same ``future``  but the order in which they will be execute cannot     be guarantee  to enforce a certain order consider chain      ``fut then cb1  then cb2 ``   the callback must take one argument  which     be the reference to this ``future``  the callback function can use the      meth `value` method to get the value  note that if this ``future`` be     already complete  the give callback will be run immediately inline       if the ``future`` s value contain tensors that reside on gpus  the     callback might be invoke while the async kernels that be populate     those tensors haven t yet finish execute on the device  however  the     callback will be invoke with some dedicate stream set as current      fetch from a global pool  which will be synchronize with those     kernels  hence any operation perform by the callback on these tensors     will be schedule on the device after the kernels complete  in other     word  as long as the callback doesn t switch stream  it can safely     manipulate the result without any additional synchronization  this be     similar to the non-blocking behavior of  meth `wait`       similarly  if the callback return a value that contain tensors that     reside on a gpu  it can do so even if the kernels that be produce     these tensors be still run on the device  as long as the callback     didn t change stream during its execution  if one want to change     stream  one must be careful to re-synchronize them with the original     stream  that be  those that be current when the callback be invoke       args          callback ``callable``   a ``callable`` that take this ``future`` as                                 the only argument       return          a new ``future`` object that hold the return value of the         ``callback`` and will be mark as complete when the give         ``callback`` finish          note   note that if the callback function throw  either         through the original future be complete with an exception and         call ``fut wait  ``  or through other code in the callback  the         future return by ``then`` will be mark appropriately with the         encounter error  however  if this callback later complete         additional futures  those futures be not mark as complete with         an error and the user be responsible for handle completion/waiting         on those futures independently       example           >>> import torch         >>>         >>> def callback fut           >>>     print f rpc return value be  fut wait               >>>         >>> fut = torch futures future           >>>   the insert callback will print the return value when         >>>   receive the response from  worker1          >>> cb fut = fut then callback          >>> chain cb fut = cb fut then          >>>     lambda x   print f chain cb do   x wait              >>>           >>> fut set result 5          >>>         >>>   output be          >>>   rpc return value be 5          >>>   chain cb do  none             return cast future s   super   then callback   
def value self  -> t      r        obtain the value of an already-completed future       this method should only be call after a call to  meth `wait` have     complete  or inside a callback function pass to  meth `then`  in     other case this ``future`` may not yet hold a value and call     ``value  `` could fail       if the value contain tensors that reside on gpus  then this method will      not  perform any additional synchronization  this should be do     beforehand  separately  through a call to  meth `wait`  except within     callbacks  for which it s already be take care of by  meth `then`        return          the value hold by this ``future``  if the function  callback or rpc          create the value have throw an error  this ``value  `` method will         also throw an error              return super   value   
def wait self  -> t      r        block until the value of this ``future`` be ready       if the value contain tensors that reside on gpus  then an additional     synchronization be perform with the kernels  execute on the device      which may be asynchronously populate those tensors  such sync be     non-blocking  which mean that ``wait  `` will insert the necessary     instructions in the current stream to ensure that further operations     enqueued on those stream will be properly schedule after the async     kernels but  once that be do  ``wait  `` will return  even if those     kernels be still run  no further synchronization be require when     access and use the value  as long as one doesn t change stream       return          the value hold by this ``future``  if the function  callback or rpc          create the value have throw an error  this ``wait`` method will         also throw an error              return super   wait   
def collect all futures  list future   -> future list future        r        collect the provide  class `~torch futures future` object into a single     combine  class `~torch futures future` that be complete when all of the     sub-futures be complete       args          futures  list   a list of  class `~torch futures future` object       return          return a  class `~torch futures future` object to a list of the pass         in futures       example           >>> import torch         >>>         >>> fut0 = torch futures future           >>> fut1 = torch futures future           >>>         >>> fut = torch futures collect all  fut0  fut1           >>>         >>> fut0 set result 0          >>> fut1 set result 1          >>>         >>> fut list = fut wait           >>> print f fut0 result =  fut list 0  wait              >>> print f fut1 result =  fut list 1  wait              >>>   output          >>>   fut0 result = 0         >>>   fut1 result = 1             return cast future list future    torch  c  collect all cast list torch  c future   futures    
def wait all futures  list future   -> list      r        wait for all provide futures to be complete  and return     the list of complete value  if any of the futures encounter an error      the method will exit early and report the error not wait for other     futures to complete       args          futures  list   a list of  class `~torch futures future` object       return          a list of the complete  class `~torch futures future` result  this         method will throw an error if ``wait`` on any          class `~torch futures future` throw              return  fut wait   for fut in torch  c  collect all cast list torch  c future   futures   wait    
def list github  force reload=false       r        list all entrypoints available in `github` hubconf       args          github  string   a string with format  repo owner/repo name  tag name   with an optional             tag/branch  the default branch be `master` if not specify              example   pytorch/vision  hub           force reload  bool  optional   whether to discard the exist cache and force a fresh download              default be `false`      return          entrypoints  a list of available entrypoint name      example          >>> entrypoints = torch hub list  pytorch/vision   force reload=true              repo dir =  get cache or reload github  force reload  true       sys path insert 0  repo dir       hub module = import module module hubconf  repo dir   string   module hubconf       sys path remove repo dir            entrypoints =  f for f in dir hub module  if callable getattr hub module  f   and not f startswith string        return entrypoints 
def help github  model  force reload=false       r        show the docstring of entrypoint `model`       args          github  string   a string with format <repo owner/repo name  tag name > with an optional             tag/branch  the default branch be `master` if not specify              example   pytorch/vision  hub           model  string   a string of entrypoint name define in repo s hubconf py         force reload  bool  optional   whether to discard the exist cache and force a fresh download              default be `false`      example          >>> print torch hub help  pytorch/vision    resnet18   force reload=true               repo dir =  get cache or reload github  force reload  true       sys path insert 0  repo dir       hub module = import module module hubconf  repo dir   string   module hubconf       sys path remove repo dir       entry =  load entry from hubconf hub module  model       return entry   doc   
def load repo or dir  model   args    kwargs       r        load a model from a github repo or a local directory       note  load a model be the typical use case  but this can also be use to     for load other object such as tokenizers  loss function  etc       if  attr `source` be `` github ``   attr `repo or dir` be expect to be     of the form ``repo owner/repo name  tag name `` with an optional     tag/branch       if  attr `source` be `` local ``   attr `repo or dir` be expect to be a     path to a local directory       args          repo or dir  string   repo name  ``repo owner/repo name  tag name ``               if ``source =  github ``  or a path to a local directory  if             ``source =  local ``          model  string   the name of a callable  entrypoint  define in the             repo/dir s ``hubconf py``           args  optional   the correspond args for callable  attr `model`          source  string  optional   `` github ``   `` local ``  specify how             ``repo or dir`` be to be interpret  default be `` github ``          force reload  bool  optional   whether to force a fresh download of             the github repo unconditionally  do not have any effect if             ``source =  local ``  default be ``false``          verbose  bool  optional   if ``false``  mute message about hit             local cache  note that the message about first download cannot be             mute  do not have any effect if ``source =  local ``              default be ``true``            kwargs  optional   the correspond kwargs for callable              attr `model`       return          the output of the  attr `model` callable when call with the give         `` args`` and ``  kwargs``       example          >>>   from a github repo         >>> repo =  pytorch/vision          >>> model = torch hub load repo   resnet50   pretrained=true          >>>   from a local directory         >>> path =  /some/local/path/pytorch/vision          >>> model = torch hub load path   resnet50   pretrained=true              source = kwargs pop string  string  lower       force reload = kwargs pop string  false      verbose = kwargs pop string  true       if source not in  string  string           raise valueerror              f unknown source    source    allow value   github     local          if source == string          repo or dir =  get cache or reload repo or dir  force reload  verbose       model =  load local repo or dir  model   args    kwargs      return model 
def download url to file url  dst  hash prefix=none  progress=true       r   download object at the give url to a local path       args          url  string   url of the object to download         dst  string   full path where object will be save  e g  `/tmp/temporary file`         hash prefix  string  optional   if not none  the sha256 download file should start with `hash prefix`              default  none         progress  bool  optional   whether or not to display a progress bar to stderr             default  true      example          >>> torch hub download url to file  https //s3 amazonaws com/pytorch/models/resnet18-5c106cde pth    /tmp/temporary file                file size = none               req = request url  headers= string  string       u = urlopen req      meta = u info       if hasattr meta  string           content length = meta getheaders string      else          content length = meta get all string      if content length be not none and len content length  > 0          file size = int content length 0                       dst = os path expanduser dst      dst dir = os path dirname dst      f = tempfile namedtemporaryfile delete=false  dir=dst dir       try          if hash prefix be not none              sha256 = hashlib sha256           with tqdm total=file size  disable=not progress                    unit=string  unit scale=true  unit divisor=1024  as pbar              while true                  buffer = u read 8192                  if len buffer  == 0                      break                 f write buffer                  if hash prefix be not none                      sha256 update buffer                  pbar update len buffer            f close           if hash prefix be not none              digest = sha256 hexdigest               if digest  len hash prefix    = hash prefix                  raise runtimeerror string                                     format hash prefix  digest           shutil move f name  dst      finally          f close           if os path exist f name               os remove f name  
def load state dict from url url  model dir=none  map location=none  progress=true  check hash=false  file name=none       r   load the torch serialize object at the give url       if download file be a zip file  it will be automatically     decompress       if the object be already present in `model dir`  it s deserialized and     return      the default value of `model dir` be ``<hub dir>/checkpoints`` where     `hub dir` be the directory return by  func `~torch hub get dir`       args          url  string   url of the object to download         model dir  string  optional   directory in which to save the object         map location  optional   a function or a dict specify how to remap storage locations  see torch load          progress  bool  optional   whether or not to display a progress bar to stderr              default  true         check hash bool  optional   if true  the filename part of the url should follow the name convention             ``filename-<sha256> ext`` where ``<sha256>`` be the first eight or more             digits of the sha256 hash of the content of the file  the hash be use to             ensure unique name and to verify the content of the file              default  false         file name  string  optional   name for the download file  filename from `url` will be use if not set       example          >>> state dict = torch hub load state dict from url  https //s3 amazonaws com/pytorch/models/resnet18-5c106cde pth                     if os getenv string           warn warn string       if model dir be none          hub dir = get dir           model dir = os path join hub dir  string       try          os makedirs model dir      except oserror as e          if e errno == errno eexist                           pass         else                           raise      part = urlparse url      filename = os path basename part path      if file name be not none          filename = file name     cache file = os path join model dir  filename      if not os path exist cache file           sys stderr write string format url  cache file           hash prefix = none         if check hash              r = hash regex search filename                hash prefix = r group 1  if r else none         download url to file url  cache file  hash prefix  progress=progress       if  be legacy zip format cache file           return  legacy zip load cache file  model dir  map location      return torch load cache file  map location=map location  
def get dir        r        get the torch hub cache directory use for store download model   weight       if  func `~torch hub set dir` be not call  default path be `` torch home/hub`` where     environment variable `` torch home`` default to `` xdg cache home/torch``      `` xdg cache home`` follow the x design group specification of the linux     filesystem layout  with a default value ``~/ cache`` if the environment     variable be not set                   if os getenv string           warn warn string       if  hub dir be not none          return  hub dir     return os path join  get torch home    string  
def set dir d       r        optionally set the torch hub directory use to save download model   weight       args          d  string   path to a local folder to save download model   weight              global  hub dir      hub dir = d 
def handle torch function          public api  callable  relevant args  iterable any    args    kwargs  -> any      string          overload args =  get overload args relevant args           type = tuple map type  overload args             for overload arg in overload args                            result = overload arg   torch function   public api  type  args  kwargs           if result be not notimplemented              return result      func name = string format public api   module    public api   name        raise typeerror string                     string                      format func name   type arg  for arg in overload args    
def be tensor like inp       string     return type inp  be torch tensor or hasattr type inp   string  
def be tensor method or property func  callable  -> bool      string     return func in  get tensor methods   or func   name   == string 
def wrap torch function dispatcher  callable       string     def inner func            functools wrap func          def wrap  args    kwargs               relevant args = dispatcher  args    kwargs              if have torch function relevant args                   return handle torch function func  relevant args   args    kwargs               return func  args    kwargs           return wrap      return inner 
class packagingerror exception       string      def   init   self  dependency graph  digraph                    break  dict packagingerrorreason  list str   = defaultdict list          for module name  attrs in dependency graph nod items                error = attrs get string              if error be none                  continue             if error == packagingerrorreason no action                  assert string not in attrs             break error  append module name           message = io stringio           message write string           for reason  module name in break items                message write f    reason value \n               for module name in module name                  message write f      module name \n                                     error context = dependency graph nod module name  get string                  if error context be not none                      message write f       context   error context \n                     self dependency graph = dependency graph         super     init   message getvalue    
class emptymatcherror exception       string      pass 
class packageexporter         exporters allow you to write package of code  pickle python data  and     arbitrary binary and text resources into a self-contained package       import can load this code in a hermetic way  such that code be load     from the package rather than the normal python import system  this allow     for the package of pytorch model code and data so that it can be run     on a server or use in the future for transfer learn       the code contain in package be copy file-by-file from the original     source when it be create  and the file format be a specially organize     zip file  future users of the package can unzip the package  and edit the code     in order to perform custom modifications to it       the importer for package ensure that code in the module can only be load from     within the package  except for modules explicitly list as external use  meth `extern`      the file ``extern modules`` in the zip archive list all the modules that a package externally depend on      this prevent  implicit  dependencies where the package run locally because it be import     a locally-installed package  but then fail when the package be copy to another machine       when source code be add to the package  the exporter can optionally scan it     for further code dependencies  ``dependencies=true``   it look for import statements      resolve relative reference to qualify module name  and perform an action specify by the user      see   meth `extern`   meth `mock`  and  meth `intern`                   a importer that will be search in order to find the modules reference by other modules or by     pickle object  the default module environment just use sys importer  which search the python environment              importer  importer      def   init            self          f  union str  path  binaryio           importer  union importer  sequence importer   = sys importer          verbose  bool = true                             create an exporter           args              f  the location to export to  can be a  ``string``/``path`` object contain a filename                 or a binary i/o object              importer  if a single importer be pass  use that to search for modules                  if a sequence of importers be pass  an ``orderedimporter`` will be construct out of them              verbose  print information about dependency resolution to stdout                  useful for track down why certain file get include                      if isinstance f   path  str                f = str f              self buffer  optional binaryio  = none         else     be a byte buffer             self buffer = f          self zip file = torch  c pytorchfilewriter f          self zip file set min version 6          self serialize reduce  dict int  any  =            self serialize storages  set str  = set              a graph track all the modules and pickle object add to this           package and the dependencies between them            - each node be a module name  or a pickle name that look like  <foo obj pkl>             - each direct edge  u  v  mean u depend on v            - nod may contain metadata that describe how to write the thing to the zipfile          self dependency graph = digraph           self verbose = verbose         self script module serializer = torch  c scriptmoduleserializer self zip file             these be ordereddicts for compatibility with removablehandle            generic ordereddict type annotations be not present until 3 7            the real type signature be ordereddict int  callable  packageexporter  str   none           self  extern hook  ordereddict = ordereddict           self  mock hook  ordereddict = ordereddict           self  intern hook  ordereddict = ordereddict            if isinstance importer  importer               self importer = importer         else              if not isinstance importer  collections abc sequence                   raise typeerror                       importer arg should be an importer or a sequence of importers                        f get  type importer   instead                                 self importer = orderedimporter  importer           self pattern  dict globgroup   patterninfo  =            self  unique id = 0      def get unique id self  -> str             get an id  this id be guarantee to only be hand out once for this package             ret = str self  unique id          self  unique id  = 1         return ret      def  get dependencies          self  src  str  module name  str  be package  bool       -> list str              return all modules that this source code depend on           dependencies be find by scan the source code for import-like statements           arguments              src  the python source code to analyze for dependencies              module name  the name of the module that ``src`` correspond to              be package  whether this module should be treat as a package                  see  py meth `save source string` for more info           return              a list contain modules detect as direct dependencies in             ``src``   the items in the list be guarantee to be unique                      package name =               module name if be package else module name rsplit      maxsplit=1  0                    try              dep pair = find file source depend on src  package name          except exception as e              self dependency graph add node                  module name                  error=packagingerrorreason dependency resolution fail                  error context=str e                             return               use a dict to get uniquing but also deterministic order         dependencies =            for dep module name  dep module obj in dep pair                handle the case where someone do something like `from pack import sub`               where `sub` be a submodule  in this case we don t have to save pack  just sub                this ensure we don t pick up additional dependencies on pack                however  in the case where `sub` be not a submodule but an object  then we do have               to save pack              if dep module obj be not none                  possible submodule = f  dep module name   dep module obj                   if self  module exist possible submodule                       dependencies possible submodule  = true                       we don t need to save `pack`                     continue             if self  module exist dep module name                   dependencies dep module name  = true          if self verbose              dep str =    join f    dep \n  for dep in dependencies              print f  module name  depend on \n dep str \n            return list dependencies key         def save source string          self          module name  str          src  str          be package  bool = false          dependencies  bool = true                    add ``src`` as the source code for ``module name`` in the export package           args              module name  str   e g  ``my package my subpackage``  code will be save to provide code for this package              src  str   the python source code to save for this package              be package  bool  optional   if ``true``  this module be treat as a package  package be allow to have submodules                  e g  ``my package my subpackage my subsubpackage``   and resources can be save inside them  default to ``false``              dependencies  bool  optional   if ``true``  we scan the source for dependencies                      self dependency graph add node              module name              source=src              be package=is package              provided=true              action= moduleprovideraction intern                     if dependencies              deps = self  get dependencies src  module name  be package               for dep in deps                  self dependency graph add edge module name  dep                  self require module if not provide dep       def  write source string          self          module name  str          src  str          be package  bool = false                    write ``src`` as the source code for ``module name`` in the zip archive           arguments be otherwise the same as for  meth `save source string`                      extension =  /  init   py  if be package else   py          filename = module name replace       /     extension          self  write filename  src       def  import module self  module name  str           try              return self importer import module module name          except modulenotfounderror as e              if not be mangle module name                   raise             msg =                   f module not find    module name    modules import                    from a torch package cannot be re-exported directly                             raise modulenotfounderror msg  from none      def  module exist self  module name  str  -> bool          try              self  import module module name              return true         except exception              return false      def  write dep graph self  fail module=none           edge =  \n  join f   f   ->   t     for f  t in self dependency graph edge          fail =    if fail module be none else f   fail module    color=red            template = f   \ digraph g    rankdir = lr  node  shape=box    fail   edge                 arg = quote template  safe=            return f https //dreampuf github io/graphvizonline/  arg        def  get source of module self  module  type moduletype  -> optional str           filename = getattr module     file     none          result =               none             if filename be none or not filename endswith   py               else linecache getlines filename  module   dict                       if result be none              return none          return    join result       def require module if not provide self  module name  str  dependencies=true           if               module name in self dependency graph             and self dependency graph nod module name  get  provide   be true                        return          if self  can implicitly extern module name               if self verbose                  print                      f implicitly add  module name  to external modules                       f since it be part of the standard library and be a dependency                                 self dependency graph add node                  module name  action= moduleprovideraction extern  provided=true                           return          for pattern  pattern info in self pattern items                if pattern match module name                   pattern info be match = true                 self dependency graph add node                      module name  action=pattern info action  provided=true                                    if pattern info action ==  moduleprovideraction deny                        require a deny module just add an error to the graph                      self dependency graph add node                          module name  error=packagingerrorreason deny                                          if we be intern this module  we need to retrieve its                   dependencies and package those as well                  if pattern info action ==  moduleprovideraction intern                      self  add module to dependency graph module name  dependencies                  return            no pattern have match  explicitly add this as an error          self dependency graph add node              module name  error=packagingerrorreason no action                def save module self  module name  str  dependencies=true              save the code for ``module`` into the package  code for the module be resolve use the ``importers`` path to find the         module object  and then use its ``  file  `` attribute to find the source code           args              module name  str   e g  ``my package my subpackage``  code will be save to provide code                 for this package              dependencies  bool  optional   if ``true``  we scan the source for dependencies                      if not isinstance module name  str               raise typeerror                   save module   expect a string input  do you perhaps mean to pass `  name  `?                         self dependency graph add node              module name  provided=true  action= moduleprovideraction intern                   self  add module to dependency graph module name  dependencies       def  add module to dependency graph          self          module name  str          dependencies  bool                 module obj = self  import module module name             find dependencies of this module and require them as well          be package = hasattr module obj     path             source = self  get source of module module obj          if source be none                couldn t find a source   add it to our dependency graph as break               and continue              filename = getattr module obj     file     none              error context = none             if filename be none                  package error = packagingerrorreason no dunder file             elif filename endswith tuple importlib machinery extension suffix                    package error = packagingerrorreason be extension module             else                  package error = packagingerrorreason source file not find                 error context = f filename   filename               self dependency graph add node                  module name                  be package=is package                  error=packaging error                  error context=error context                            return          self dependency graph add node              module name  be package=is package  source=source  provided=true                    if dependencies              deps = self  get dependencies source  module name  be package              for dep in deps                  self dependency graph add edge module name  dep                  self require module if not provide dep       def save pickle          self  package  str  resource  str  obj  any  dependencies  bool = true                   save a python object to the archive use pickle  equivalent to  func `torch save` but save into         the archive rather than a stand-alone file  stanard pickle do not save the code  only the object          if ``dependencies`` be true  this method will also scan the pickle object for which modules be require         to reconstruct them and save the relevant code           to be able to save an object where ``type obj    name  `` be ``my module myobject``          ``my module myobject`` must resolve to the class of the object accord to the ``importer`` order  when save object that         have previously be package  the importer s ``import module`` method will need to be present in the ``importer`` list         for this to work           args              package  str   the name of module package this resource should go in  e g  `` my package my subpackage ``               resource  str   a unique name for the resource  use to identify it to load              obj  any   the object to save  must be picklable              dependencies  bool  optional   if ``true``  we scan the source for dependencies                      filename = self  filename package  resource            write the pickle data for `obj`         data buf = io bytesio           pickler = create pickler data buf  self importer          pickler persistent id = self  persistent id         pickler dump obj          data value = data buf getvalue            name in dependency graph = f < package   resource >          self dependency graph add node              name in dependency graph              action= moduleprovideraction intern              provided=true              be pickle=true                     if dependencies              all dependencies =                for opcode  arg  pos in pickletools genops data value                   if opcode name ==  global      a global reference                     assert isinstance arg  str                      module  field = arg split                          if module not in all dependencies                          all dependencies append module               if self verbose                  dep string =    join f    dep \n  for dep in all dependencies                  print f  resource  depend on \n dep string \n                for module name in all dependencies                  self dependency graph add edge name in dependency graph  module name                  self require module if not provide module name           self  write filename  data value       def save text self  package  str  resource  str  text  str              save text data to the package           args              package  str   the name of module package this resource should go it  e g  `` my package my subpackage ``               resource  str   a unique name for the resource  use to identify it to load              text  str   the content to save                      return self save binary package  resource  text encode  utf-8         def save binary self  package  resource  binary  bytes              save raw bytes to the package           args              package  str   the name of module package this resource should go it  e g  `` my package my subpackage ``               resource  str   a unique name for the resource  use to identify it to load              binary  str   the data to save                      filename = self  filename package  resource          self  write filename  binary       def register extern hook self  hook  actionhook  -> removablehandle             register an extern hook on the exporter           the hook will be call each time a module match against an  meth `extern` pattern          it should have the follow signature                hook exporter  packageexporter  module name  str  -> none          hook will be call in order of registration           return               class `torch utils hook removablehandle`                  a handle that can be use to remove the add hook by call                 ``handle remove  ``                      handle = removablehandle self  extern hook          self  extern hook handle id  = hook         return handle      def register mock hook self  hook  actionhook  -> removablehandle             register a mock hook on the exporter           the hook will be call each time a module match against a  meth `mock` pattern          it should have the follow signature                hook exporter  packageexporter  module name  str  -> none          hook will be call in order of registration           return               class `torch utils hook removablehandle`                  a handle that can be use to remove the add hook by call                 ``handle remove  ``                      handle = removablehandle self  mock hook          self  mock hook handle id  = hook         return handle      def register intern hook self  hook  actionhook  -> removablehandle             register an intern hook on the exporter           the hook will be call each time a module match against an  meth `intern` pattern          it should have the follow signature                hook exporter  packageexporter  module name  str  -> none          hook will be call in order of registration           return               class `torch utils hook removablehandle`                  a handle that can be use to remove the add hook by call                 ``handle remove  ``                      handle = removablehandle self  intern hook          self  intern hook handle id  = hook         return handle      def intern          self          include   globpattern                      exclude   globpattern  =             allow empty  bool = true                    specify modules that should be package  a module must match some ``intern`` pattern in order to be         include in the package and have its dependencies process recursively           args              include  union list str   str    a string e g   my package my subpackage   or list of string                 for the name of the modules to be externed  this can also be a glob-style pattern  as describe in  meth `mock`               exclude  union list str   str    an optional pattern that exclude some pattern that match the include string               allow empty  bool   an optional flag that specify whether the intern modules specify by this call                 to the ``intern`` method must be match to some module during package  if an ``intern`` module glob                 pattern be add with ``allow empty=false``  and  meth `close` be call  either explicitly or via ``  exit  ``                  before any modules match that pattern  an exception be throw  if ``allow empty=true``  no such exception be throw                       self pattern globgroup include  exclude=exclude   =  patterninfo               moduleprovideraction intern  allow empty                def mock          self          include   globpattern                      exclude   globpattern  =             allow empty  bool = true                    replace some require modules with a mock implementation   mock modules will return a fake         object for any attribute access from it  because we copy file-by-file  the dependency resolution will sometimes         find file that be import by model file but whose functionality be never use          e g  custom serialization code or train helpers           use this function to mock this functionality out without have to modify the original code           args              include  union list str   str    a string e g  `` my package my subpackage ``  or list of string                 for the name of the modules to be mock out  string can also be a glob-style pattern                 string that may match multiple modules  any require dependencies that match this pattern                 string will be mock out automatically                   examples                       `` torch    `` -- match ``torch`` and all submodules of torch  e g  `` torch nn ``                     and `` torch nn functional ``                      `` torch   `` -- match `` torch nn `` or `` torch functional ``  but not                     `` torch nn functional ``              exclude  union list str   str    an optional pattern that exclude some pattern that match the include string                  e g  ``include= torch      exclude= torch foo `` will mock all torch package except `` torch foo ``                  default  be ``  ``               allow empty  bool   an optional flag that specify whether the mock implementation s  specify by this call                 to the  meth `mock` method must be match to some module during package  if a mock be add with                 ``allow empty=false``  and  meth `close` be call  either explicitly or via ``  exit  ``  and the mock have                 not be match to a module use by the package be export  an exception be throw                  if ``allow empty=true``  no such exception be throw                       self pattern globgroup include  exclude=exclude   =  patterninfo               moduleprovideraction mock  allow empty                def extern          self          include   globpattern                      exclude   globpattern  =             allow empty  bool = true                    include ``module`` in the list of external modules the package can import          this will prevent dependency discovery from save         it in the package  the importer will load an external module directly from the standard import system          code for extern modules must also exist in the process load the package           args              include  union list str   str    a string e g  `` my package my subpackage ``  or list of string                 for the name of the modules to be externed  this can also be a glob-style pattern  as                 describe in  meth `mock`               exclude  union list str   str    an optional pattern that exclude some pattern that match the                 include string               allow empty  bool   an optional flag that specify whether the extern modules specify by this call                 to the ``extern`` method must be match to some module during package  if an extern module glob                 pattern be add with ``allow empty=false``  and  meth `close` be call  either explicitly or via                 ``  exit  ``  before any modules match that pattern  an exception be throw  if ``allow empty=true``                  no such exception be throw                       self pattern globgroup include  exclude=exclude   =  patterninfo               moduleprovideraction extern  allow empty                def deny self  include   globpattern      exclude   globpattern  =                 blocklist modules who name match the give glob pattern from the list of modules the package can import          if a dependency on any match package be find  a  class `packagingerror` be raise           args              include  union list str   str    a string e g  `` my package my subpackage ``  or list of string                 for the name of the modules to be externed  this can also be a glob-style pattern  as describe in  meth `mock`               exclude  union list str   str    an optional pattern that exclude some pattern that match the include string                      self pattern globgroup include  exclude=exclude   =  patterninfo               moduleprovideraction deny  allow empty=true                def  persistent id self  obj           if torch be storage obj               storage type = normalize storage type type obj               obj key = str obj  cdata              location = location tag obj              name = f  data/ obj key  storage               if name not in self serialize storages                    check to see if storage be previously serialize                 serialize file = self zip file get all write record                   if name not in serialize file                      if obj device type  =  cpu                           obj = obj cpu                       num bytes = obj size     obj element size                       self zip file write record name  obj data ptr    num bytes                  self serialize storages add name              return   storage   storage type  obj key  location  obj size             if hasattr obj     reduce package                  if self serialize reduce get id obj   be none                  self serialize reduce id obj   =   reduce package   id obj    obj   reduce package   self                return self serialize reduce id obj            return none      def   enter   self           return self      def   exit   self  exc type  exc value  traceback             if   exit   be call because an exception be raise  we do not attempt to           attempt to finalize the package  instead  control be return to the           caller to continue raise the exception          if exc type be not none                do the bare minimum to leave the open buffer in a valid state              self  finalize zip               return          self close        def  write self  filename  str or bytes           if be mangle filename               raise runtimeerror                  f try to save a torch package d module as   filename                       directly save torch package d modules be not allow                         if isinstance str or bytes  str               str or bytes = str or bytes encode  utf-8           self zip file write record filename  str or bytes  len str or bytes        def  validate dependency graph self             1  check the graph for any errors insert during dependency analysis          for module name  attrs in self dependency graph nod items                if  error  in attrs                  raise packagingerror self dependency graph             2  check that all pattern for which allow empty=false have be match at least once          for pattern  pattern info in self pattern items                if not pattern info allow empty and not pattern info be match                  raise emptymatcherror                      f exporter do not match any modules to  pattern   which be mark as allow empty=false                         def  execute dependency graph self              take a finalize dependency graph describe how to package all         modules and execute it  write to the zip archive                      self  validate dependency graph            extern modules =             mock write = false         for module name  attrs in self dependency graph nod items                action = attrs  action                if action ==  moduleprovideraction extern                  for hook in self  extern hook value                        hook self  module name                   extern modules append module name               elif action ==  moduleprovideraction mock                  for hook in self  mock hook value                        hook self  module name                   if not  mock write                      mock file = str path   file    parent /   mock py                       self  write source string                            mock    read file mock file   be package=false                                            mock write = true                  be package = hasattr self  import module module name      path                     self  write source string module name   mock impl  be package               elif action ==  moduleprovideraction intern                  for hook in self  intern hook value                        hook self  module name                     the node in the dependency graph contain metadata that tell us                   how to intern the module                  if  provide  not in attrs                      raise assertionerror                          f module be mark `intern` but not provide   module name                                          if attrs get  be pickle   be true                        this node come from save source pickle  we don t need to write any source for it                      continue                  be package = attrs  be package                   source = attrs  source                   self  write source string module name  source  be package               else                  raise assertionerror                      f invalid action   module name    action   please report a bug to pytorch                              extern file content =  \n  join extern modules     \n          self  write   data/extern modules   extern file content       def close self              write the package to the filesystem  any call after  meth `close` be now invalid          it be preferable to use resource guard syntax instead                with packageexporter  file zip   as e                                          if self verbose              print f dependency graph for export package  \n self  write dep graph               self  execute dependency graph            self script module serializer write file           self  finalize zip        def  finalize zip self              call at the very end of package to leave the zipfile in a close but valid state             del self zip file         if self buffer              self buffer flush        def  filename self  package  resource           package path = package replace       /           resource =  normalize path resource          return f  package path / resource        def  can implicitly extern self  module name  str           top level package name = module name partition      0          return top level package name ==  torch  or               top level package name not in  disallow modules             and be stdlib module top level package name            
class packageimporter importer       string      string     modules  dict str  type moduletype       def   init            self          file or buffer  union str  torch  c pytorchfilereader  path  binaryio           module allow  callable  str   bool  = lambda module name  true                 string         self zip reader  any         if isinstance file or buffer  torch  c pytorchfilereader               self filename = string             self zip reader = file or buffer         elif isinstance file or buffer   path  str                self filename = str file or buffer              if not os path isdir self filename                   self zip reader = torch  c pytorchfilereader self filename              else                  self zip reader = mockzipreader self filename          else              self filename = string             self zip reader = torch  c pytorchfilereader file or buffer           self root =  packagenode none          self modules =            self extern modules = self  read extern            for extern module in self extern modules              if not module allow extern module                   raise importerror                      f package   file or buffer   need the external module   extern module                         f but that module have be disallow                                self  add extern extern module           for fname in self zip reader get all record                self  add file fname           self patch builtins = builtins   dict   copy           self patch builtins string  = self   import                    self modules string  = self            self  mangler = packagemangler                     self storage context  any = none         self last map location = none                   self unpickler = lambda  args    kwargs  packageunpickler self   args    kwargs       def import module self  name  str  package=none           string         return self  gcd import name       def load binary self  package  str  resource  str  -> bytes          string          path = self  zipfile path package  resource          return self zip reader get record path       def load text          self          package  str          resource  str          encode  str = string          errors  str = string        -> str          string         data = self load binary package  resource          return data decode encode  errors       def load pickle self  package  str  resource  str  map location=none  -> any          string         pickle file = self  zipfile path package  resource          restore location =  get restore location map location          load storages =            load reduce =            storage context = torch  c storagecontext            def load tensor data type  size  key  location  restore location               name = f  key  storage              dtype = data type 0  dtype              if storage context have storage name                   storage = storage context get storage name  dtype  storage               else                  tensor = self zip reader get storage from record                      string   name  size  dtype                                   if isinstance self zip reader  torch  c pytorchfilereader                       storage context add storage name  tensor                  storage = tensor storage               load storages key  = restore location storage  location           def persistent load save id               assert isinstance save id  tuple              typename =  maybe decode ascii save id 0               data = save id 1                if typename == string                  data type  key  location  size = data                 if key not in load storages                      load tensor                          data type                          size                          key                           maybe decode ascii location                           restore location                                        storage = load storages key                  return storage             elif typename == string                  reduce id  func  args = data                 if reduce id not in load reduce                      load reduce reduce id  = func self   args                  return load reduce reduce id              else                  f unknown typename for persistent load  expect  storage  or  reduce package  but get   typename                      data file = io bytesio self zip reader get record pickle file           unpickler = self unpickler data file          unpickler persistent load = persistent load           contextmanager         def set deserialization context                             self storage context = storage context             self last map location = map location             try                  yield             finally                  self storage context = none                 self last map location = none          with set deserialization context                result = unpickler load                                                torch  utils  validate load sparse tensors            return result      def id self           string         return self  mangler parent name        def file structure          self     include  string = string  exclude  string =          -> directory          string         return  create directory from file list              self filename  self zip reader get all record    include  exclude                def  read extern self           return               self zip reader get record string               decode string               splitlines keepends=false                 def  make module          self  name  str  filename  optional str   be package  bool  parent  str                mangle filename = self  mangler mangle filename  if filename else none         spec = importlib machinery modulespec name  self  be package=is package            module = importlib util module from spec spec          self modules name  = module         module   name   = self  mangler mangle name          ns = module   dict           ns string  = spec         ns string  = self         ns string  = mangle filename         ns string  = none         ns string  = self patch builtins         ns string  = true                   assert module   name   not in  package import modules          package import modules module   name    = module                            self  install on parent parent  name  module           if filename be not none              assert mangle filename be not none                                       assert filename not in linecache cache               linecache lazycache mangle filename  ns               code = self  compile source filename  mangle filename              exec code  ns           return module      def  load module self  name  str  parent  str           cur   pathnode = self root         for atom in name split string               if not isinstance cur   packagenode  or atom not in cur children                  raise modulenotfounderror                      f no module name   name   in self-contained archive   self filename                        f  and the module be also not in the list of allow external modules   self extern modules                        name=name                                cur = cur children atom              if isinstance cur   externnode                   module = self modules name  = importlib import module name                  return module         return self  make module name  cur source file  isinstance cur   packagenode   parent         def  compile source self  fullpath  str  mangle filename  str           source = self zip reader get record fullpath          source =  normalize line end source          return compile source  mangle filename  string  dont inherit=true                 def get source self  module name  -> str                   module = self import module demangle module name           return self zip reader get record demangle module   file     decode string                 def get resource reader self  fullname           try              package = self  get package fullname          except importerror              return none         if package   loader   be not self              return none         return  packageresourcereader self  fullname       def  install on parent self  parent  str  name  str  module  type moduletype           if not parent              return                  parent module = self modules parent          if parent module   loader   be self                setattr parent module  name rpartition string  2   module            def  do find and load self  name           path = none         parent = name rpartition string  0          if parent              if parent not in self modules                  self  gcd import parent                           if name in self modules                  return self modules name              parent module = self modules parent              try                  path = parent module   path                 except attributeerror                  msg =   err msg   string  format name  parent                  raise modulenotfounderror msg  name=name  from none          module = self  load module name  parent           self  install on parent parent  name  module           return module           def  find and load self  name           module = self modules get name   need load          if module be  need load              return self  do find and load name           if module be none              message = string string format name              raise modulenotfounderror message  name=name           return module      def  gcd import self  name  package=none  level=0           string          sanity check name  package  level          if level > 0              name =  resolve name name  package  level           return self  find and load name            def  handle fromlist self  module  fromlist     recursive=false           string         module name = demangle module   name                              if hasattr module  string               for x in fromlist                  if not isinstance x  str                       if recursive                          where = module name   string                     else                          where = string                     raise typeerror                          f item in  where  must be str    f not  type x    name                                           elif x == string                      if not recursive and hasattr module  string                           self  handle fromlist module  module   all    recursive=true                  elif not hasattr module  x                       from name = string format module name  x                      try                          self  gcd import from name                      except modulenotfounderror as exc                                                                                                     if                               exc name == from name                             and self modules get from name   need load  be not none                                                        continue                         raise         return module      def   import   self  name  globals=none  locals=none  fromlist=    level=0           if level == 0              module = self  gcd import name          else              globals  = globals if globals be not none else                package =  calc   package   globals               module = self  gcd import name  package  level          if not fromlist                                        if level == 0                  return self  gcd import name partition string  0               elif not name                  return module             else                                                    cut off = len name  - len name partition string  0                                                     module name = demangle module   name                    return self modules module name   len module name  - cut off           else              return self  handle fromlist module  fromlist       def  get package self  package           string         if hasattr package  string               if package   spec   submodule search locations be none                  raise typeerror string format package   spec   name               else                  return package         else              module = self import module package              if module   spec   submodule search locations be none                  raise typeerror string format package               else                  return module      def  zipfile path self  package  resource=none           package = self  get package package          assert package   loader   be self         name = demangle package   name            if resource be not none              resource =  normalize path resource              return f  name replace       /   / resource           else              return f  name replace       /          def  get or create package          self  atoms  list str        -> string          cur = self root         for i  atom in enumerate atoms               node = cur children get atom  none              if node be none                  node = cur children atom  =  packagenode none              if isinstance node   externnode                   return node             if isinstance node   modulenode                   name = string join atoms  i                   raise importerror                      f inconsistent module structure  module  name  be not a package  but have submodules                                assert isinstance node   packagenode              cur = node         return cur      def  add file self  filename  str           string          prefix  last = filename split string          if len prefix  > 1 and prefix 0  == string              return         package = self  get or create package prefix          if isinstance package   externnode               raise importerror                  f inconsistent module structure  package contain a module file  filename                   f  that be a subpackage of a module mark external                         if last == string              package source file = filename         elif last endswith string               package name = last   -len string               package children package name  =  modulenode filename       def  add extern self  extern name  str            prefix  last = extern name split string          package = self  get or create package prefix          if isinstance package   externnode               return           package children last  =  externnode   
class directory      string      def   init   self  name  str  be dir  bool           self name = name         self be dir = be dir         self children  dict str  directory  =         def  get dir self  dirs  list str   -> string          string         if len dirs  == 0              return self         dir name = dirs 0          if dir name not in self children              self children dir name  = directory dir name  true          return self children dir name   get dir dirs 1         def  add file self  file path  str           string          dirs  file = file path split string          dir = self  get dir dirs          dir children file  = directory file  false       def have file self  filename  str  -> bool          string         lineage = filename split string  maxsplit=1          child = lineage 0          grandchildren = lineage 1  if len lineage  > 1 else none         if child in self children key                if grandchildren be none                  return true             else                  return self children child  have file grandchildren          return false      def   str   self           str list  list str  =            self  stringify tree str list          return string join str list       def  stringify tree          self  str list  list str   preamble  str = string  dir ptr  str = string                string         space = string         branch = string         tee = string         last = string                   str list append f  preamble  dir ptr  self name \n                     if dir ptr == tee              preamble = preamble   branch         else              preamble = preamble   space          file key  list str  =            dir key  list str  =            for key  val in self children items                if val be dir                  dir key append key              else                  file key append key           for index  key in enumerate sort dir key                if  index == len dir key  - 1  and len file key  == 0                  self children key   stringify tree str list  preamble  last              else                  self children key   stringify tree str list  preamble  tee          for index  file in enumerate sort file key                pointer = last if  index == len file key  - 1  else tee             str list append f  preamble  pointer  file \n   
def   init        self      f  union str  path  binaryio       importer  union importer  sequence importer   = sys importer      verbose  bool = true         string     if isinstance f   path  str            f = str f          self buffer  optional binaryio  = none     else            self buffer = f      self zip file = torch  c pytorchfilewriter f      self zip file set min version 6      self serialize reduce  dict int  any  =        self serialize storages  set str  = set                                 self dependency graph = digraph       self verbose = verbose     self script module serializer = torch  c scriptmoduleserializer self zip file                      self  extern hook  ordereddict = ordereddict       self  mock hook  ordereddict = ordereddict       self  intern hook  ordereddict = ordereddict        if isinstance importer  importer           self importer = importer     else          if not isinstance importer  collections abc sequence               raise typeerror                  string                 f get  type importer   instead                         self importer = orderedimporter  importer       self pattern  dict globgroup   patterninfo  =        self  unique id = 0 
def close self       string     if self verbose          print f dependency graph for export package  \n self  write dep graph           self  execute dependency graph        self script module serializer write file       self  finalize zip   
def deny self  include  string     exclude  string =          string     self pattern globgroup include  exclude=exclude   =  patterninfo           moduleprovideraction deny  allow empty=true       
def extern      self      include  string             exclude  string =         allow empty  bool = true         string     self pattern globgroup include  exclude=exclude   =  patterninfo           moduleprovideraction extern  allow empty       
def get unique id self  -> str      string     ret = str self  unique id      self  unique id  = 1     return ret 
def intern      self      include  string             exclude  string =         allow empty  bool = true         string     self pattern globgroup include  exclude=exclude   =  patterninfo           moduleprovideraction intern  allow empty       
def mock      self      include  string             exclude  string =         allow empty  bool = true         string     self pattern globgroup include  exclude=exclude   =  patterninfo           moduleprovideraction mock  allow empty       
def register extern hook self  hook  actionhook  -> removablehandle      string     handle = removablehandle self  extern hook      self  extern hook handle id  = hook     return handle 
def register intern hook self  hook  actionhook  -> removablehandle      string     handle = removablehandle self  intern hook      self  intern hook handle id  = hook     return handle 
def register mock hook self  hook  actionhook  -> removablehandle      string     handle = removablehandle self  mock hook      self  mock hook handle id  = hook     return handle 
def save binary self  package  resource  binary  bytes       string     filename = self  filename package  resource      self  write filename  binary  
def save module self  module name  str  dependencies=true       string     if not isinstance module name  str           raise typeerror              string                self dependency graph add node          module name  provided=true  action= moduleprovideraction intern           self  add module to dependency graph module name  dependencies  
def save pickle      self  package  str  resource  str  obj  any  dependencies  bool = true        string     filename = self  filename package  resource           data buf = io bytesio       pickler = create pickler data buf  self importer      pickler persistent id = self  persistent id     pickler dump obj      data value = data buf getvalue        name in dependency graph = f < package   resource >      self dependency graph add node          name in dependency graph          action= moduleprovideraction intern          provided=true          be pickle=true             if dependencies          all dependencies =            for opcode  arg  pos in pickletools genops data value               if opcode name == string                    assert isinstance arg  str                  module  field = arg split string                  if module not in all dependencies                      all dependencies append module           if self verbose              dep string = string join f    dep \n  for dep in all dependencies              print f  resource  depend on \n dep string \n            for module name in all dependencies              self dependency graph add edge name in dependency graph  module name              self require module if not provide module name       self  write filename  data value  
def save source string      self      module name  str      src  str      be package  bool = false      dependencies  bool = true         string     self dependency graph add node          module name          source=src          be package=is package          provided=true          action= moduleprovideraction intern             if dependencies          deps = self  get dependencies src  module name  be package           for dep in deps              self dependency graph add edge module name  dep              self require module if not provide dep  
def save text self  package  str  resource  str  text  str       string     return self save binary package  resource  text encode string   
def   init        self      file or buffer  union str  torch  c pytorchfilereader  path  binaryio       module allow  callable  str   bool  = lambda module name  true         string     self zip reader  any     if isinstance file or buffer  torch  c pytorchfilereader           self filename = string         self zip reader = file or buffer     elif isinstance file or buffer   path  str            self filename = str file or buffer          if not os path isdir self filename               self zip reader = torch  c pytorchfilereader self filename          else              self zip reader = mockzipreader self filename      else          self filename = string         self zip reader = torch  c pytorchfilereader file or buffer       self root =  packagenode none      self modules =        self extern modules = self  read extern        for extern module in self extern modules          if not module allow extern module               raise importerror                  f package   file or buffer   need the external module   extern module                     f but that module have be disallow                        self  add extern extern module       for fname in self zip reader get all record            self  add file fname       self patch builtins = builtins   dict   copy       self patch builtins string  = self   import            self modules string  = self        self  mangler = packagemangler             self storage context  any = none     self last map location = none           self unpickler = lambda  args    kwargs  packageunpickler self   args    kwargs  
def file structure      self     include  string = string  exclude  string =      -> directory      string     return  create directory from file list          self filename  self zip reader get all record    include  exclude       
def id self       string     return self  mangler parent name   
def import module self  name  str  package=none       string     return self  gcd import name  
def load binary self  package  str  resource  str  -> bytes      string      path = self  zipfile path package  resource      return self zip reader get record path  
def load pickle self  package  str  resource  str  map location=none  -> any      string     pickle file = self  zipfile path package  resource      restore location =  get restore location map location      load storages =        load reduce =        storage context = torch  c storagecontext        def load tensor data type  size  key  location  restore location           name = f  key  storage          dtype = data type 0  dtype          if storage context have storage name               storage = storage context get storage name  dtype  storage           else              tensor = self zip reader get storage from record                  string   name  size  dtype                           if isinstance self zip reader  torch  c pytorchfilereader                   storage context add storage name  tensor              storage = tensor storage           load storages key  = restore location storage  location       def persistent load save id           assert isinstance save id  tuple          typename =  maybe decode ascii save id 0           data = save id 1            if typename == string              data type  key  location  size = data             if key not in load storages                  load tensor                      data type                      size                      key                       maybe decode ascii location                       restore location                                storage = load storages key              return storage         elif typename == string              reduce id  func  args = data             if reduce id not in load reduce                  load reduce reduce id  = func self   args              return load reduce reduce id          else              f unknown typename for persistent load  expect  storage  or  reduce package  but get   typename              data file = io bytesio self zip reader get record pickle file       unpickler = self unpickler data file      unpickler persistent load = persistent load       contextmanager     def set deserialization context                     self storage context = storage context         self last map location = map location         try              yield         finally              self storage context = none             self last map location = none      with set deserialization context            result = unpickler load                            torch  utils  validate load sparse tensors        return result 
def load text      self      package  str      resource  str      encode  str = string      errors  str = string    -> str      string     data = self load binary package  resource      return data decode encode  errors  
def have file self  filename  str  -> bool      string     lineage = filename split string  maxsplit=1      child = lineage 0      grandchildren = lineage 1  if len lineage  > 1 else none     if child in self children key            if grandchildren be none              return true         else              return self children child  have file grandchildren      return false 
class profile object       string     def   init                self                             activities  optional iterable profileractivity   = none              schedule  optional callable  int   profileraction   = none              on trace ready  optional callable      any   = none              record shape  bool = false              profile memory  bool = false              with stack  bool = false              with flop  bool = false                           use cuda  optional bool  = none           if activities              self activities = set activities          else              self activities = support activities            if use cuda be not none              warn string              if use cuda                  self activities add profileractivity cuda              elif profileractivity cuda in self activities                  self activities remove profileractivity cuda           assert len self activities  > 0  string          if schedule              self schedule = schedule                          self record step = true         else              self schedule =  default schedule fn             self record step = false         self on trace ready = on trace ready         self record shape = record shape         self with flop = with flop         self profile memory = profile memory         self with stack = with stack         self step num = 0         self current action = self schedule self step num          self profiler  optional prof profile  = none         self step rec fn  optional prof record function  = none      def   enter   self           self  enter action           if self record step              self step rec fn = prof record function string   str self step num               self step rec fn   enter             return self      def   exit   self  exc type  exc val  exc tb           if self record step and self step rec fn              self step rec fn   exit   none  none  none          self  exit action        def step self           string         if self record step and self step rec fn              self step rec fn   exit   none  none  none          prev action = self current action         self step num  = 1         self current action = self schedule self step num           if self current action == profileraction none              if prev action == profileraction none                  pass             elif prev action == profileraction warmup                  warn string                  self  start trace                   self  stop trace               elif prev action == profileraction record                  warn string                  self  stop trace               else                  assert prev action == profileraction record and save                 self  stop trace                   if self on trace ready                      self on trace ready self          elif self current action == profileraction warmup              if prev action == profileraction none                  self  start warmup               elif prev action == profileraction warmup                  pass             elif prev action == profileraction record                  warn string                  self  stop trace               else                  assert prev action == profileraction record and save                 self  stop trace                   if self on trace ready                      self on trace ready self                  self  start warmup           elif self current action in \                  profileraction record  profileraction record and save               if prev action == profileraction none                  self  start warmup                   self  start trace               elif prev action == profileraction warmup                  self  start trace               elif prev action == profileraction record                  pass             else                  assert prev action == profileraction record and save                 self  stop trace                   if self on trace ready                      self on trace ready self                  self  start warmup                   self  start trace            if self record step              self step rec fn = prof record function string   str self step num               self step rec fn   enter          def export chrome trace self  path  str           string         assert self profiler         if path endswith string               fp = tempfile namedtemporaryfile string  suffix=string  delete=false              fp close               retvalue = self profiler export chrome trace fp name              with open fp name  as fin                  with gzip open path  string  as fout                      fout writelines fin              os remove fp name              return retvalue         else              return self profiler export chrome trace path       def export stack self  path  str  metric  str = string           string         assert self profiler         return self profiler export stack path  metric       def key average self  group by input shape  bool = false  group by stack n  int = 0           string         assert self profiler         return self profiler key average group by input shape  group by stack n       def events self           string         assert self profiler         return self profiler function events      def add metadata self  key  str  value  str           string         wrap value = string   value replace string  string    string         torch autograd  add metadata json key  wrap value       def add metadata json self  key  str  value  str           string         torch autograd  add metadata json key  value       def  get distribute info self           import torch distribute as dist         if not dist be available   or not dist be initialize                return none          return               string  dist get backend                string  dist get rank                string  dist get world size                  def  enter action self           if self current action == profileraction warmup              self  start warmup           elif self current action in \                  profileraction record  profileraction record and save               self  start warmup               self  start trace        def  exit action self           if self current action == profileraction warmup              self  start trace               self  stop trace           elif self current action in \                  profileraction record  profileraction record and save               self  stop trace               if self on trace ready                  self on trace ready self       def  start warmup self           self profiler = prof profile              use cuda= profileractivity cuda in self activities               use cpu= profileractivity cpu in self activities               record shapes=self record shape              with flops=self with flop              profile memory=self profile memory              with stack=self with stack              use kineto=true                    self profiler  prepare trace        def  start trace self           assert self profiler be not none         self profiler  start trace            if kineto available                dist info = self  get distribute info               if dist info                  self add metadata json string  json dump dist info        def  stop trace self           assert self profiler be not none         self profiler   exit   none  none  none  
class profileraction enum       string     none = 0     warmup = 1     record = 2     record and save = 3 
def step self       string     if self record step and self step rec fn          self step rec fn   exit   none  none  none      prev action = self current action     self step num  = 1     self current action = self schedule self step num       if self current action == profileraction none          if prev action == profileraction none              pass         elif prev action == profileraction warmup              warn string              self  start trace               self  stop trace           elif prev action == profileraction record              warn string              self  stop trace           else              assert prev action == profileraction record and save             self  stop trace               if self on trace ready                  self on trace ready self      elif self current action == profileraction warmup          if prev action == profileraction none              self  start warmup           elif prev action == profileraction warmup              pass         elif prev action == profileraction record              warn string              self  stop trace           else              assert prev action == profileraction record and save             self  stop trace               if self on trace ready                  self on trace ready self              self  start warmup       elif self current action in \              profileraction record  profileraction record and save           if prev action == profileraction none              self  start warmup               self  start trace           elif prev action == profileraction warmup              self  start trace           elif prev action == profileraction record              pass         else              assert prev action == profileraction record and save             self  stop trace               if self on trace ready                  self on trace ready self              self  start warmup               self  start trace        if self record step          self step rec fn = prof record function string   str self step num           self step rec fn   enter     
def schedule    wait  int  warmup  int  active  int  repeat  int = 0  skip first  int = 0  -> callable      string     def schedule fn step  int  -> profileraction          assert step >= 0         if step < skip first              return profileraction none         else              step -= skip first         num step = wait   warmup   active         if repeat > 0 and step / num step >= repeat              return profileraction none         mod step = step   num step         if mod step < wait              return profileraction none         elif mod step < wait   warmup              return profileraction warmup         else              return profileraction record if mod step < num step - 1 \                 else profileraction record and save     assert wait >= 0 and warmup >= 0 and active > 0 and \            repeat >= 0 and skip first >= 0  string     if warmup == 0          warn string      return schedule fn 
def tensorboard trace handler dir name  str  worker name  optional str  = none  use gzip  bool = false       string     import os     import socket     import time      def handler fn prof  -> none          nonlocal worker name         if not os path isdir dir name               try                  os makedirs dir name  exist ok=true              except exception                  raise runtimeerror string   dir name          if not worker name              worker name = string format socket gethostname    str os getpid             file name = string format worker name  int time time     1000           if use gzip              file name = file name   string         prof export chrome trace os path join dir name  file name       return handler fn 
def calculate gain nonlinearity  param=none       r   return the recommend gain value for the give nonlinearity function      the value be as follow       ================= ====================================================     nonlinearity      gain     ================= ====================================================     linear / identity  math `1`     conv 1 2 3 d       math `1`     sigmoid            math `1`     tanh               math `\frac 5  3 `     relu               math `\sqrt 2 `     leaky relu         math `\sqrt \frac 2  1   \text negative\ slope  2  `     selu               math `\frac 3  4 `     ================= ====================================================         warn           in order to implement `self-normalizing neural networks`            you should use ``nonlinearity= linear `` instead of ``nonlinearity= selu ``          this give the initial weight a variance of ``1 / n``          which be necessary to induce a stable fix point in the forward pass          in contrast  the default gain for ``selu`` sacrifice the normalisation         effect for more stable gradient flow in rectangular layer       args          nonlinearity  the non-linear function  `nn functional` name          param  optional parameter for the non-linear function      examples          >>> gain = nn init calculate gain  leaky relu   0 2     leaky relu with negative slope=0 2          self-normalizing neural network  https //papers nip cc/paper/2017/hash/5d44ee6f2c3f71b73125876103c8f6c4-abstract html             linear fns =  string  string  string  string  string  string  string      if nonlinearity in linear fns or nonlinearity == string          return 1     elif nonlinearity == string          return 5 0 / 3     elif nonlinearity == string          return math sqrt 2 0      elif nonlinearity == string          if param be none              negative slope = 0 01         elif not isinstance param  bool  and isinstance param  int  or isinstance param  float                            negative slope = param         else              raise valueerror string format param           return math sqrt 2 0 /  1   negative slope    2       elif nonlinearity == string          return 3 0 / 4       else          raise valueerror string format nonlinearity   
def uniform  tensor  tensor  a  float = 0   b  float = 1   -> tensor      r   fill the input tensor with value draw from the uniform     distribution  math `\mathcal u  a  b `       args          tensor  an n-dimensional `torch tensor`         a  the lower bind of the uniform distribution         b  the upper bind of the uniform distribution      examples          >>> w = torch empty 3  5          >>> nn init uniform  w              return  no grad uniform  tensor  a  b  
def normal  tensor  tensor  mean  float = 0   std  float = 1   -> tensor      r   fill the input tensor with value draw from the normal     distribution  math `\mathcal n  \text mean   \text std  2 `       args          tensor  an n-dimensional `torch tensor`         mean  the mean of the normal distribution         std  the standard deviation of the normal distribution      examples          >>> w = torch empty 3  5          >>> nn init normal  w              return  no grad normal  tensor  mean  std  
def constant  tensor  tensor  val  float  -> tensor      r   fill the input tensor with the value  math `\text val `       args          tensor  an n-dimensional `torch tensor`         val  the value to fill the tensor with      examples          >>> w = torch empty 3  5          >>> nn init constant  w  0 3              return  no grad fill  tensor  val  
def ones  tensor  tensor  -> tensor      r   fill the input tensor with the scalar value `1`       args          tensor  an n-dimensional `torch tensor`      examples          >>> w = torch empty 3  5          >>> nn init ones  w              return  no grad fill  tensor  1   
def zero  tensor  tensor  -> tensor      r   fill the input tensor with the scalar value `0`       args          tensor  an n-dimensional `torch tensor`      examples          >>> w = torch empty 3  5          >>> nn init zero  w              return  no grad zero  tensor  
def eye  tensor       r   fill the 2-dimensional input `tensor` with the identity     matrix  preserve the identity of the input in `linear` layer  where as     many input be preserve as possible       args          tensor  a 2-dimensional `torch tensor`      examples          >>> w = torch empty 3  5          >>> nn init eye  w              if tensor ndimension    = 2          raise valueerror string       with torch no grad            torch eye  tensor shape  out=tensor  require grad=tensor require grad      return tensor 
def dirac  tensor  groups=1       r   fill the  3  4  5 -dimensional input `tensor` with the dirac     delta function  preserve the identity of the input in `convolutional`     layer  where as many input channel be preserve as possible  in case     of groups>1  each group of channel preserve identity      args          tensor  a  3  4  5 -dimensional `torch tensor`         group  optional   number of group in the conv layer  default  1      examples          >>> w = torch empty 3  16  5  5          >>> nn init dirac  w          >>> w = torch empty 3  24  5  5          >>> nn init dirac  w  3              dimension = tensor ndimension       if dimension not in  3  4  5           raise valueerror string       size = tensor size        if size 0    group  = 0          raise valueerror string       out chans per grp = size 0  // group     min dim = min out chans per grp  size 1        with torch no grad            tensor zero             for g in range group               for d in range min dim                   if dimension == 3                        tensor g   out chans per grp   d  d  tensor size 2  // 2  = 1                 elif dimension == 4                        tensor g   out chans per grp   d  d  tensor size 2  // 2                             tensor size 3  // 2  = 1                 else                        tensor g   out chans per grp   d  d  tensor size 2  // 2                             tensor size 3  // 2  tensor size 4  // 2  = 1     return tensor 
def xavier uniform  tensor  tensor  gain  float = 1   -> tensor      r   fill the input `tensor` with value accord to the method     describe in `understanding the difficulty of train deep feedforward     neural networks` - glorot  x    bengio  y   2010   use a uniform     distribution  the result tensor will have value sample from      math `\mathcal u  -a  a ` where         math           a = \text gain  \times \sqrt \frac 6  \text fan\ in    \text fan\ out         also know as glorot initialization       args          tensor  an n-dimensional `torch tensor`         gain  an optional scale factor      examples          >>> w = torch empty 3  5          >>> nn init xavier uniform  w  gain=nn init calculate gain  relu                fan in  fan out =  calculate fan in and fan out tensor      std = gain   math sqrt 2 0 / float fan in   fan out       a = math sqrt 3 0    std        return  no grad uniform  tensor  -a  a  
def xavier normal  tensor  tensor  gain  float = 1   -> tensor      r   fill the input `tensor` with value accord to the method     describe in `understanding the difficulty of train deep feedforward     neural networks` - glorot  x    bengio  y   2010   use a normal     distribution  the result tensor will have value sample from      math `\mathcal n  0  \text std  2 ` where         math           \text std  = \text gain  \times \sqrt \frac 2  \text fan\ in    \text fan\ out         also know as glorot initialization       args          tensor  an n-dimensional `torch tensor`         gain  an optional scale factor      examples          >>> w = torch empty 3  5          >>> nn init xavier normal  w              fan in  fan out =  calculate fan in and fan out tensor      std = gain   math sqrt 2 0 / float fan in   fan out        return  no grad normal  tensor  0   std  
def kaiming uniform  tensor  a=0  mode=string  nonlinearity=string       r   fill the input `tensor` with value accord to the method     describe in `delving deep into rectifiers  surpass human-level     performance on imagenet classification` - he  k  et al   2015   use a     uniform distribution  the result tensor will have value sample from      math `\mathcal u  -\text bind   \text bind  ` where         math           \text bind  = \text gain  \times \sqrt \frac 3  \text fan\ mode         also know as he initialization       args          tensor  an n-dimensional `torch tensor`         a  the negative slope of the rectifier use after this layer  only             use with `` leaky relu ``          mode  either `` fan in ``  default  or `` fan out ``  choose `` fan in ``             preserve the magnitude of the variance of the weight in the             forward pass  choose `` fan out `` preserve the magnitudes in the             backwards pass          nonlinearity  the non-linear function  `nn functional` name               recommend to use only with `` relu `` or `` leaky relu ``  default        examples          >>> w = torch empty 3  5          >>> nn init kaiming uniform  w  mode= fan in   nonlinearity= relu               if 0 in tensor shape          warn warn string          return tensor     fan =  calculate correct fan tensor  mode      gain = calculate gain nonlinearity  a      std = gain / math sqrt fan      bind = math sqrt 3 0    std       with torch no grad            return tensor uniform  -bound  bind  
def kaiming normal  tensor  a=0  mode=string  nonlinearity=string       r   fill the input `tensor` with value accord to the method     describe in `delving deep into rectifiers  surpass human-level     performance on imagenet classification` - he  k  et al   2015   use a     normal distribution  the result tensor will have value sample from      math `\mathcal n  0  \text std  2 ` where         math           \text std  = \frac \text gain   \sqrt \text fan\ mode         also know as he initialization       args          tensor  an n-dimensional `torch tensor`         a  the negative slope of the rectifier use after this layer  only             use with `` leaky relu ``          mode  either `` fan in ``  default  or `` fan out ``  choose `` fan in ``             preserve the magnitude of the variance of the weight in the             forward pass  choose `` fan out `` preserve the magnitudes in the             backwards pass          nonlinearity  the non-linear function  `nn functional` name               recommend to use only with `` relu `` or `` leaky relu ``  default        examples          >>> w = torch empty 3  5          >>> nn init kaiming normal  w  mode= fan out   nonlinearity= relu               if 0 in tensor shape          warn warn string          return tensor     fan =  calculate correct fan tensor  mode      gain = calculate gain nonlinearity  a      std = gain / math sqrt fan      with torch no grad            return tensor normal  0  std  
def orthogonal  tensor  gain=1       r   fill the input `tensor` with a  semi  orthogonal matrix  as     describe in `exact solutions to the nonlinear dynamics of learn in deep     linear neural networks` - saxe  a  et al   2013   the input tensor must have     at least 2 dimension  and for tensors with more than 2 dimension the     trail dimension be flatten       args          tensor  an n-dimensional `torch tensor`  where  math `n \geq 2`         gain  optional scale factor      examples          >>> w = torch empty 3  5          >>> nn init orthogonal  w              if tensor ndimension   < 2          raise valueerror string       row = tensor size 0      cols = tensor numel   // row     flatten = tensor new row  cols  normal  0  1       if row < cols          flatten t              q  r = torch linalg qr flatten           d = torch diag r  0      ph = d sign       q  = ph      if row < cols          q t         with torch no grad            tensor view as q  copy  q          tensor mul  gain      return tensor 
def sparse  tensor  sparsity  std=0 01       r   fill the 2d input `tensor` as a sparse matrix  where the     non-zero elements will be draw from the normal distribution      math `\mathcal n  0  0 01 `  as describe in `deep learn via     hessian-free optimization` - martens  j   2010        args          tensor  an n-dimensional `torch tensor`         sparsity  the fraction of elements in each column to be set to zero         std  the standard deviation of the normal distribution use to generate             the non-zero value      examples          >>> w = torch empty 3  5          >>> nn init sparse  w  sparsity=0 1              if tensor ndimension    = 2          raise valueerror string       row  cols = tensor shape     num zero = int math ceil sparsity   row        with torch no grad            tensor normal  0  std          for col idx in range cols               row indices = torch randperm row              zero indices = row indices  num zero              tensor zero indices  col idx  = 0     return tensor 
def export model  args  f  export params=true  verbose=false  training=trainingmode eval             input names=none  output names=none  aten=false  export raw ir=false             operator export type=none  opset version=none   retain param name=true             do constant folding=true  example outputs=none  strip doc string=true             dynamic axes=none  keep initializers as inputs=none  custom opsets=none             enable onnx checker=true  use external data format=false       r        export a model into onnx format   this exporter run your model     once in order to get a trace of its execution to be export      at the moment  it support a limit set of dynamic model  e g   rnns        args          model  torch nn module   the model to be export          args  tuple of arguments or torch tensor  a dictionary consist of name arguments  optional                a dictionary to specify the input to the correspond name parameter              - key  str  name parameter             - value  correspond input             args can be structure either as               1  only a tuple of arguments or torch tensor                     args =  x  y  z                the input to the model  e g   such that ``model  args `` be a valid invocation             of the model  any non-tensor arguments will be hard-coded into the export model              any tensor arguments will become input of the export model  in the order they             occur in args  if args be a tensor  this be equivalent to have             call it with a 1-ary tuple of that tensor               2  a tuple of arguements with a dictionary of name parameters                     args =  x                                                     y   input y                           z   input z                                          the input to the model be structure as a tuple consist of             non-keyword arguments and the last value of this tuple be a dictionary             consist of name parameters and the correspond input as key-value pair              if certain name argument be not present in the dictionary  it be assign             the default value  or none if default value be not provide               case in which an dictionary input be the last input of the args tuple             would cause a conflict when a dictionary of name parameters be use              the model below provide such an example                   class model torch nn module                       def forward self  k  x                                                       return x                  m = model                   k = torch randn 2  3                  x =  torch tensor 1    torch randn 2  3                    in the previous iteration  the call to export api would look like                      torch onnx export model   k  x    test onnx                    this would work as intend  however  the export function                 would now assume that the `x` input be intend to represent the optional                 dictionary consist of name arguments  in order to prevent this from be                 an issue a constraint be place to provide an empty dictionary as the last                 input in the tuple args in such case  the new call would look like this                       torch onnx export model   k  x        test onnx            f  a file-like object  have to implement fileno that return a file descriptor              or a string contain a file name   a binary protobuf will be write             to this file          export params  bool  default true   if specify  all parameters will             be export   set this to false if you want to export an untrained model              in this case  the export model will first take all of its parameters             as arguments  the order as specify by ``model state dict   value  ``         verbose  bool  default false   if specify  we will print out a debug             description of the trace be export          train  enum  default trainingmode eval               trainingmode eval  export the model in inference mode              trainingmode preserve  export the model in inference mode if model train be             false and to a train friendly mode if model train be true              trainingmode train  export the model in a train friendly mode          input name list of string  default empty list   name to assign to the             input nod of the graph  in order         output name list of string  default empty list   name to assign to the             output nod of the graph  in order         aten  bool  default false    deprecate  use operator export type  export the             model in aten mode  if use aten mode  all the ops original export             by the function in symbolic opset<version> py be export as aten ops          export raw ir  bool  default false    deprecate  use operator export type              export the internal ir directly instead of convert it to onnx ops          operator export type  enum  default operatorexporttypes onnx               operatorexporttypes onnx  all ops be export as regular onnx ops              with onnx namespace               operatorexporttypes onnx aten  all ops be export as aten ops              with aten namespace               operatorexporttypes onnx aten fallback  if an aten op be not support             in onnx or its symbolic be miss  fall back on aten op  register ops             be export to onnx regularly              example graph                    graph  0   float                       3   int = prim  constant value=0                       4   float = aten  triu  0   3    miss op                    5   float = aten  mul  4   0    register op                   return   5               be export as                    graph  0   float                       1   long   = onnx  constant value= 0                        2   float = aten  aten operator= triu    0   1     miss op                    3   float = onnx  mul  2   0    register op                   return   3               in the above example  aten  triu be not support in onnx  hence             exporter fall back on this op              operatorexporttypes raw  export raw ir              operatorexporttypes onnx fallthrough  if an op be not support             in onnx  fall through and export the operator as be  as a custom             onnx op  use this mode  the op can be export and implement by             the user for their runtime backend              example graph                    graph  x 1   long 1  strides= 1                         1   none = prim  constant                      2   tensor = aten  sum  x 1   1                     y 1   tensor   = prim  listconstruct  2                    return   y 1               be export as                    graph  x 1   long 1  strides= 1                         1   tensor = onnx  reducesum keepdims=0   x 1                     y 1   long   = prim  listconstruct  1                    return   y 1               in the above example  prim  listconstruct be not support  hence             exporter fall through           opset version  int  default be 9   by default we export the model to the             opset version of the onnx submodule  since onnx s latest opset may             evolve before next stable release  by default we export to one stable             opset version  right now  support stable opset version be 9              the opset version must be  onnx main opset or in  onnx stable opsets             which be define in torch/onnx/symbolic helper py         do constant fold  bool  default false   if true  the constant-folding             optimization be apply to the model during export  constant-folding             optimization will replace some of the ops that have all constant             input  with pre-computed constant nod          example output  tuple of tensors  list of tensors  tensor  int  float  bool  default none               model s example output be export   example output  must be provide when export             a scriptmodule or torchscript function  if there be more than one item  it should be pass             in tuple format  e g   example output =  x  y  z   otherwise  only one item should             be pass as the example output  e g  example outputs=x              example output must be provide when export a scriptmodule or torchscript function          strip doc string  bool  default true   if true  strip the field              doc string  from the export model  which information about the stack             trace          dynamic ax  dict<string  dict<int  string>> or dict<string  list int >  default empty dict               a dictionary to specify dynamic ax of input/output  such that              - key   input and/or output name             - value  index of dynamic ax for give key and potentially the name to be use for             export dynamic ax  in general the value be define accord to one of the follow             ways or a combination of both               1   a list of integers specify the dynamic ax of provide input  in this scenario             automate name will be generate and apply to dynamic ax of provide input/output             during export              or  2   an inner dictionary that specify a map from the index of dynamic axis in             correspond input/output to the name that be desire to be apply on such axis of             such input/output during export               example  if we have the follow shape for input and output                  code-block   none                  shape input 1  =   b   3   w    h                   and shape input 2  =   b   4                  and shape output   =   b    d   5               then `dynamic axes` can be define either as               1  only indices                    ``dynamic ax =   input 1   0  2  3                                      input 2   0                                      output   0  1  ``                 where automatic name will be generate for export dynamic ax              2  indices with correspond name                    ``dynamic ax =   input 1   0  batch                                                1  width                                                2  height                                       input 2   0  batch                                       output   0  batch                                               1  detections   ``                 where provide name will be apply to export dynamic ax              3  mix mode of  1  and  2                     ``dynamic ax =   input 1   0  2  3                                      input 2   0  batch                                       output   0 1  ``          keep initializers as input  bool  default none   if true  all the             initializers  typically correspond to parameters  in the             export graph will also be add as input to the graph  if false              then initializers be not add as input to the graph  and only             the non-parameter input be add as input               this may allow for better optimizations  such as constant fold             etc   by backends/runtimes that execute these graph  if             unspecified  default none   then the behavior be choose             automatically as follow  if operator export type be             operatorexporttypes onnx  the behavior be equivalent to set             this argument to false  for other value of operator export type              the behavior be equivalent to set this argument to true  note             that for onnx opset version < 9  initializers must be part of graph             input  therefore  if opset version argument be set to a 8 or             lower  this argument will be ignore          custom opsets  dict<string  int>  default empty dict   a dictionary to indicate             custom opset domain and version at export  if model contain a custom opset              it be optional to specify the domain and opset version in the dictionary              - key  opset domain name             - value  opset version             if the custom opset be not provide in this dictionary  opset version be set             to 1 by default          enable onnx checker  bool  default true   if true the onnx model checker will be run             as part of the export  to ensure the export model be a valid onnx model          use external data format  bool  default false   if true  then the model be export             in onnx external data format  in which case some of the model parameters be store             in external binary file and not in the onnx model file itself  see link for format             detail              https //github com/onnx/onnx/blob/8b3f7e2e7a0f2aba0e629e23d89f07c7fc0e6a5e/onnx/onnx proto l423             also  in this case   argument  f  must be a string specify the location of the model              the external binary file will be store in the same location specify by the model             location  f   if false  then the model be store in regular format  i e  model and             parameters be all in one file  this argument be ignore for all export type other             than onnx               from torch onnx import utils     return utils export model  args  f  export params  verbose  train                          input name  output name  aten  export raw ir                          operator export type  opset version   retain param name                          do constant fold  example output                          strip doc string  dynamic ax  keep initializers as input                          custom opsets  enable onnx checker  use external data format  
def export to pretty string  args    kwargs       from torch onnx import utils     return utils export to pretty string  args    kwargs  
def register custom op symbolic symbolic name  symbolic fn  opset version       from torch onnx import utils     return utils register custom op symbolic symbolic name  symbolic fn  opset version  
def select model mode for export model  mode       r        a context manager to temporarily set the train mode of  model      to  mode   reset it when we exit the with-block   a no-op if     mode be none       in version 1 6 change to this from set train              from torch onnx import utils     return utils select model mode for export model  mode  
def be in onnx export        r        check whether it s in the middle of the onnx export      this function return true in the middle of torch onnx export        torch onnx export should be execute with single thread               from torch onnx import utils     return utils be in onnx export   
class optimizer object       r   base class for all optimizers          warn           parameters need to be specify as collections that have a deterministic         order that be consistent between run  examples of object that don t         satisfy those properties be set and iterators over value of dictionaries       args          params  iterable   an iterable of  class `torch tensor` s or              class `dict` s  specify what tensors should be optimize          default   dict   a dict contain default value of optimization             options  use when a parameter group doesn t specify them                def   init   self  params  default           torch  c  log api usage once string          self default = default          self  hook for profile            if isinstance params  torch tensor               raise typeerror string                             string                               torch typename params            self state = defaultdict dict          self param group =             param group = list params          if len param group  == 0              raise valueerror string          if not isinstance param group 0   dict               param group =   string  param group            for param group in param group              self add param group param group       def   getstate   self           return               string  self default              string  self state              string  self param group                 def   setstate   self  state           self   dict   update state          self  hook for profile          def   repr   self           format string = self   class     name     string         for i  group in enumerate self param group               format string  = string             format string  = string format i              for key in sort group key                     if key  = string                      format string  = string format key  group key           format string  = string         return format string      def  hook for profile self           self  zero grad profile name = string format self   class     name             def profile hook step func                 functools wrap func              def wrapper  args    kwargs                   obj     = args                 profile name = string format obj   class     name                    with torch autograd profiler record function profile name                       return func  args    kwargs              return wrapper          hook = getattr self   class   step  string  none          if not hook              self   class   step = profile hook step self   class   step              self   class   step hook = true      def state dict self           r   return the state of the optimizer as a  class `dict`           it contain two entries             state - a dict hold current optimization state  its content             differ between optimizer class            param group - a dict contain all parameter group                              param mappings =            start index = 0          def pack group group               nonlocal start index             pack =  k  v for k  v in group items   if k  = string              param mappings update  id p   i for i  p in enumerate group string   start index                                     if id p  not in param mappings               pack string  =  param mappings id p   for p in group string               start index  = len pack string               return pack         param group =  pack group g  for g in self param group                   pack state =   param mappings id k   if isinstance k  torch tensor  else k   v                         for k  v in self state items            return               string  pack state              string  param group                 def load state dict self  state dict           r   load the optimizer state           args              state dict  dict   optimizer state  should be an object return                 from a call to  meth `state dict`                               state dict = deepcopy state dict                   group = self param group         save group = state dict string           if len group   = len save group               raise valueerror string                              string          param lens =  len g string   for g in group          save lens =  len g string   for g in save group          if any p len  = s len for p len  s len in zip param lens  save lens                raise valueerror string                              string                    id map =  old id  p for old id  p in                   zip chain from iterable  g string  for g in save group                          chain from iterable  g string  for g in group              def cast param  value               r   make a deep copy of value  cast all tensors to device of param                 if isinstance value  torch tensor                                                     if param be float point                        value = value to param dtype                  value = value to param device                  return value             elif isinstance value  dict                   return  k  cast param  v  for k  v in value items                elif isinstance value  container abcs iterable                   return type value  cast param  v  for v in value              else                  return value                                     state = defaultdict dict          for k  v in state dict string  items                if k in id map                  param = id map k                  state param  = cast param  v              else                  state k  = v                   def update group group  new group               new group string  = group string              return new group         param group =               update group g  ng  for g  ng in zip group  save group           self   setstate    string  state  string  param group        def zero grad self  set to none  bool = false           r   set the gradients of all optimize  class `torch tensor` s to zero           args              set to none  bool   instead of set to zero  set the grads to none                  this will in general have lower memory footprint  and can modestly improve performance                  however  it change certain behaviors  for example                  1  when the user try to access a gradient and perform manual ops on it                  a none attribute or a tensor full of 0s will behave differently                  2  if the user request ``zero grad set to none=true `` follow by a backward pass  `` grad``\ s                 be guarantee to be none for params that do not receive a gradient                  3  ``torch optim`` optimizers have a different behavior if the gradient be 0 or none                  in one case it do the step with a gradient of 0 and in the other it skip                 the step altogether                       if not hasattr self  string               self  hook for profile           with torch autograd profiler record function self  zero grad profile name               for group in self param group                  for p in group string                       if p grad be not none                          if set to none                              p grad = none                         else                              if p grad grad fn be not none                                  p grad detach                                else                                  p grad require grad  false                              p grad zero         def step self  closure           r   perform a single optimization step  parameter update            args              closure  callable   a closure that reevaluate the model and                 return the loss  optional for most optimizers              note               unless otherwise specify  this function should not modify the             `` grad`` field of the parameters                      raise notimplementederror      def add param group self  param group           r   add a param group to the  class `optimizer` s `param groups`           this can be useful when fine tune a pre-trained network as freeze layer can be make         trainable and add to the  class `optimizer` as train progress           args              param group  dict   specify what tensors should be optimize along with group             specific optimization options                      assert isinstance param group  dict   string          params = param group string          if isinstance params  torch tensor               param group string  =  params          elif isinstance params  set               raise typeerror string                             string          else              param group string  = list params           for param in param group string               if not isinstance param  torch tensor                   raise typeerror string                                 string   torch typename param               if not param be leaf                  raise valueerror string           for name  default in self default items                if default be require and name not in param group                  raise valueerror string                                    name              else                  param group setdefault name  default           params = param group string          if len params   = len set params                warn warn string                           string                           string  stacklevel=3           param set = set           for group in self param group              param set update set group string             if not param set isdisjoint set param group string                 raise valueerror string           self param group append param group  
class context object       string     def   enter   self           self autograd context =  new context           return self autograd context  context id        def   exit   self  type  value  traceback            release context self autograd context  context id    
 contextlib contextmanager def fork rng devices=none  enabled=true   caller=string   devices kw=string       string      import torch cuda     global  fork rng warn already                      if not enable          yield         return      if devices be none          num devices = torch cuda device count           if num devices > 1 and not  fork rng warn already              warn warn                   string                  string                  string                  string                  string                  string                  string                  string                  string                  string                  string                    format num devices=num devices  caller= caller  devices kw= devices kw                fork rng warn already = true         devices = list range num devices       else                            devices = list devices       cpu rng state = torch get rng state       gpu rng state =        for device in devices          gpu rng state append torch cuda get rng state device        try          yield     finally          torch set rng state cpu rng state          for device  gpu rng state in zip devices  gpu rng state               torch cuda set rng state gpu rng state  device  
def get rng state   -> torch tensor      r   return the random number generator state as a `torch bytetensor`         return default generator get state   
def initial seed   -> int      r   return the initial seed for generate random number as a     python `long`              return default generator initial seed   
def manual seed seed  -> torch  c generator      r   set the seed for generate random number  return a     `torch generator` object       args          seed  int   the desire seed  value must be within the inclusive range             ` -0x8000 0000 0000 0000  0xffff ffff ffff ffff `  otherwise  a runtimeerror             be raise  negative input be remapped to positive value with the formula             `0xffff ffff ffff ffff   seed`              seed = int seed      import torch cuda      if not torch cuda  be in bad fork            torch cuda manual seed all seed       return default generator manual seed seed  
def seed   -> int      r   set the seed for generate random number to a non-deterministic     random number  return a 64 bite number use to seed the rng              seed = default generator seed       import torch cuda      if not torch cuda  be in bad fork            torch cuda manual seed all seed       return seed 
def set rng state new state  torch tensor  -> none      r   set the random number generator state       args          new state  torch bytetensor   the desire state             default generator set state new state  
class doublestorage  c doublestoragebase   storagebase       pass 
class floatstorage  c floatstoragebase   storagebase       pass 
class halfstorage  c halfstoragebase   storagebase       pass 
class longstorage  c longstoragebase   storagebase       pass 
class intstorage  c intstoragebase   storagebase       pass 
class shortstorage  c shortstoragebase   storagebase       pass 
class charstorage  c charstoragebase   storagebase       pass 
class bytestorage  c bytestoragebase   storagebase       pass 
class boolstorage  c boolstoragebase   storagebase       pass 
class bfloat16storage  c bfloat16storagebase   storagebase       pass 
class complexdoublestorage  c complexdoublestoragebase   storagebase       pass 
class complexfloatstorage  c complexfloatstoragebase   storagebase       pass 
class quint8storage  c quint8storagebase   storagebase       pass 
class qint8storage  c qint8storagebase   storagebase       pass 
class qint32storage  c qint32storagebase   storagebase       pass 
class quint4x2storage  c quint4x2storagebase   storagebase       pass 
def  cuda self  device=none  non blocking=false    kwargs       string     non block =  get async or non block string  non block  kwargs      if self be cuda          if device be none              device = torch cuda current device           if self get device   == device              return self     else          if device be none              device = -1     with torch cuda device device           if self be sparse              new type = getattr torch cuda sparse  self   class     name                indices = torch tensor  indices self  cuda device  non block              value = torch tensor  value self  cuda device  non block              return new type indices  value  self size            else              new type = getattr torch cuda  self   class     name                return new type self size    copy  self  non block  
def  type self  dtype=none  non blocking=false    kwargs       string     non block =  get async or non block string  non block  kwargs      if dtype be none          return self   module     string   self   class     name        if isinstance dtype  str           dtype =  import dot name dtype      if dtype == type self           return self     if self be sparse          if not dtype be sparse              raise runtimeerror string          new module name = dtype   module   replace string  string          new value type name = new module name   string   dtype   name           new value = torch tensor  value self  type new value type name  non block          new indices type name = new module name   string         new indices = torch tensor  indices self  type new indices type name  non block          return dtype new indices  new value  self size        if dtype be sparse          raise runtimeerror string      return dtype self size    copy  self  non block  
def  cuda self  device=none  non blocking=false    kwargs       string     non block =  get async or non block string  non block  kwargs      if self be cuda          if device be none              device = torch cuda current device           if self get device   == device              return self     else          if device be none              device = -1     with torch cuda device device           if self be sparse              new type = getattr torch cuda sparse  self   class     name                indices = torch tensor  indices self  cuda device  non block              value = torch tensor  value self  cuda device  non block              return new type indices  value  self size            else              new type = getattr torch cuda  self   class     name                return new type self size    copy  self  non block  
def  type self  dtype=none  non blocking=false    kwargs       string     non block =  get async or non block string  non block  kwargs      if dtype be none          return self   module     string   self   class     name        if isinstance dtype  str           dtype =  import dot name dtype      if dtype == type self           return self     if self be sparse          if not dtype be sparse              raise runtimeerror string          new module name = dtype   module   replace string  string          new value type name = new module name   string   dtype   name           new value = torch tensor  value self  type new value type name  non block          new indices type name = new module name   string         new indices = torch tensor  indices self  type new indices type name  non block          return dtype new indices  new value  self size        if dtype be sparse          raise runtimeerror string      return dtype self size    copy  self  non block  
def  cuda self  device=none  non blocking=false    kwargs       string     non block =  get async or non block string  non block  kwargs      if self be cuda          if device be none              device = torch cuda current device           if self get device   == device              return self     else          if device be none              device = -1     with torch cuda device device           if self be sparse              new type = getattr torch cuda sparse  self   class     name                indices = torch tensor  indices self  cuda device  non block              value = torch tensor  value self  cuda device  non block              return new type indices  value  self size            else              new type = getattr torch cuda  self   class     name                return new type self size    copy  self  non block  
def  type self  dtype=none  non blocking=false    kwargs       string     non block =  get async or non block string  non block  kwargs      if dtype be none          return self   module     string   self   class     name        if isinstance dtype  str           dtype =  import dot name dtype      if dtype == type self           return self     if self be sparse          if not dtype be sparse              raise runtimeerror string          new module name = dtype   module   replace string  string          new value type name = new module name   string   dtype   name           new value = torch tensor  value self  type new value type name  non block          new indices type name = new module name   string         new indices = torch tensor  indices self  type new indices type name  non block          return dtype new indices  new value  self size        if dtype be sparse          raise runtimeerror string      return dtype self size    copy  self  non block  
def  cuda self  device=none  non blocking=false    kwargs       string     non block =  get async or non block string  non block  kwargs      if self be cuda          if device be none              device = torch cuda current device           if self get device   == device              return self     else          if device be none              device = -1     with torch cuda device device           if self be sparse              new type = getattr torch cuda sparse  self   class     name                indices = torch tensor  indices self  cuda device  non block              value = torch tensor  value self  cuda device  non block              return new type indices  value  self size            else              new type = getattr torch cuda  self   class     name                return new type self size    copy  self  non block  
def  type self  dtype=none  non blocking=false    kwargs       string     non block =  get async or non block string  non block  kwargs      if dtype be none          return self   module     string   self   class     name        if isinstance dtype  str           dtype =  import dot name dtype      if dtype == type self           return self     if self be sparse          if not dtype be sparse              raise runtimeerror string          new module name = dtype   module   replace string  string          new value type name = new module name   string   dtype   name           new value = torch tensor  value self  type new value type name  non block          new indices type name = new module name   string         new indices = torch tensor  indices self  type new indices type name  non block          return dtype new indices  new value  self size        if dtype be sparse          raise runtimeerror string      return dtype self size    copy  self  non block  
def  cuda self  device=none  non blocking=false    kwargs       string     non block =  get async or non block string  non block  kwargs      if self be cuda          if device be none              device = torch cuda current device           if self get device   == device              return self     else          if device be none              device = -1     with torch cuda device device           if self be sparse              new type = getattr torch cuda sparse  self   class     name                indices = torch tensor  indices self  cuda device  non block              value = torch tensor  value self  cuda device  non block              return new type indices  value  self size            else              new type = getattr torch cuda  self   class     name                return new type self size    copy  self  non block  
def  type self  dtype=none  non blocking=false    kwargs       string     non block =  get async or non block string  non block  kwargs      if dtype be none          return self   module     string   self   class     name        if isinstance dtype  str           dtype =  import dot name dtype      if dtype == type self           return self     if self be sparse          if not dtype be sparse              raise runtimeerror string          new module name = dtype   module   replace string  string          new value type name = new module name   string   dtype   name           new value = torch tensor  value self  type new value type name  non block          new indices type name = new module name   string         new indices = torch tensor  indices self  type new indices type name  non block          return dtype new indices  new value  self size        if dtype be sparse          raise runtimeerror string      return dtype self size    copy  self  non block  
def  cuda self  device=none  non blocking=false    kwargs       string     non block =  get async or non block string  non block  kwargs      if self be cuda          if device be none              device = torch cuda current device           if self get device   == device              return self     else          if device be none              device = -1     with torch cuda device device           if self be sparse              new type = getattr torch cuda sparse  self   class     name                indices = torch tensor  indices self  cuda device  non block              value = torch tensor  value self  cuda device  non block              return new type indices  value  self size            else              new type = getattr torch cuda  self   class     name                return new type self size    copy  self  non block  
def  type self  dtype=none  non blocking=false    kwargs       string     non block =  get async or non block string  non block  kwargs      if dtype be none          return self   module     string   self   class     name        if isinstance dtype  str           dtype =  import dot name dtype      if dtype == type self           return self     if self be sparse          if not dtype be sparse              raise runtimeerror string          new module name = dtype   module   replace string  string          new value type name = new module name   string   dtype   name           new value = torch tensor  value self  type new value type name  non block          new indices type name = new module name   string         new indices = torch tensor  indices self  type new indices type name  non block          return dtype new indices  new value  self size        if dtype be sparse          raise runtimeerror string      return dtype self size    copy  self  non block  
def  cuda self  device=none  non blocking=false    kwargs       string     non block =  get async or non block string  non block  kwargs      if self be cuda          if device be none              device = torch cuda current device           if self get device   == device              return self     else          if device be none              device = -1     with torch cuda device device           if self be sparse              new type = getattr torch cuda sparse  self   class     name                indices = torch tensor  indices self  cuda device  non block              value = torch tensor  value self  cuda device  non block              return new type indices  value  self size            else              new type = getattr torch cuda  self   class     name                return new type self size    copy  self  non block  
def  type self  dtype=none  non blocking=false    kwargs       string     non block =  get async or non block string  non block  kwargs      if dtype be none          return self   module     string   self   class     name        if isinstance dtype  str           dtype =  import dot name dtype      if dtype == type self           return self     if self be sparse          if not dtype be sparse              raise runtimeerror string          new module name = dtype   module   replace string  string          new value type name = new module name   string   dtype   name           new value = torch tensor  value self  type new value type name  non block          new indices type name = new module name   string         new indices = torch tensor  indices self  type new indices type name  non block          return dtype new indices  new value  self size        if dtype be sparse          raise runtimeerror string      return dtype self size    copy  self  non block  
def  cuda self  device=none  non blocking=false    kwargs       string     non block =  get async or non block string  non block  kwargs      if self be cuda          if device be none              device = torch cuda current device           if self get device   == device              return self     else          if device be none              device = -1     with torch cuda device device           if self be sparse              new type = getattr torch cuda sparse  self   class     name                indices = torch tensor  indices self  cuda device  non block              value = torch tensor  value self  cuda device  non block              return new type indices  value  self size            else              new type = getattr torch cuda  self   class     name                return new type self size    copy  self  non block  
def  type self  dtype=none  non blocking=false    kwargs       string     non block =  get async or non block string  non block  kwargs      if dtype be none          return self   module     string   self   class     name        if isinstance dtype  str           dtype =  import dot name dtype      if dtype == type self           return self     if self be sparse          if not dtype be sparse              raise runtimeerror string          new module name = dtype   module   replace string  string          new value type name = new module name   string   dtype   name           new value = torch tensor  value self  type new value type name  non block          new indices type name = new module name   string         new indices = torch tensor  indices self  type new indices type name  non block          return dtype new indices  new value  self size        if dtype be sparse          raise runtimeerror string      return dtype self size    copy  self  non block  
def  cuda self  device=none  non blocking=false    kwargs       string     non block =  get async or non block string  non block  kwargs      if self be cuda          if device be none              device = torch cuda current device           if self get device   == device              return self     else          if device be none              device = -1     with torch cuda device device           if self be sparse              new type = getattr torch cuda sparse  self   class     name                indices = torch tensor  indices self  cuda device  non block              value = torch tensor  value self  cuda device  non block              return new type indices  value  self size            else              new type = getattr torch cuda  self   class     name                return new type self size    copy  self  non block  
def  type self  dtype=none  non blocking=false    kwargs       string     non block =  get async or non block string  non block  kwargs      if dtype be none          return self   module     string   self   class     name        if isinstance dtype  str           dtype =  import dot name dtype      if dtype == type self           return self     if self be sparse          if not dtype be sparse              raise runtimeerror string          new module name = dtype   module   replace string  string          new value type name = new module name   string   dtype   name           new value = torch tensor  value self  type new value type name  non block          new indices type name = new module name   string         new indices = torch tensor  indices self  type new indices type name  non block          return dtype new indices  new value  self size        if dtype be sparse          raise runtimeerror string      return dtype self size    copy  self  non block  
def  cuda self  device=none  non blocking=false    kwargs       string     non block =  get async or non block string  non block  kwargs      if self be cuda          if device be none              device = torch cuda current device           if self get device   == device              return self     else          if device be none              device = -1     with torch cuda device device           if self be sparse              new type = getattr torch cuda sparse  self   class     name                indices = torch tensor  indices self  cuda device  non block              value = torch tensor  value self  cuda device  non block              return new type indices  value  self size            else              new type = getattr torch cuda  self   class     name                return new type self size    copy  self  non block  
def  type self  dtype=none  non blocking=false    kwargs       string     non block =  get async or non block string  non block  kwargs      if dtype be none          return self   module     string   self   class     name        if isinstance dtype  str           dtype =  import dot name dtype      if dtype == type self           return self     if self be sparse          if not dtype be sparse              raise runtimeerror string          new module name = dtype   module   replace string  string          new value type name = new module name   string   dtype   name           new value = torch tensor  value self  type new value type name  non block          new indices type name = new module name   string         new indices = torch tensor  indices self  type new indices type name  non block          return dtype new indices  new value  self size        if dtype be sparse          raise runtimeerror string      return dtype self size    copy  self  non block  
def  cuda self  device=none  non blocking=false    kwargs       string     non block =  get async or non block string  non block  kwargs      if self be cuda          if device be none              device = torch cuda current device           if self get device   == device              return self     else          if device be none              device = -1     with torch cuda device device           if self be sparse              new type = getattr torch cuda sparse  self   class     name                indices = torch tensor  indices self  cuda device  non block              value = torch tensor  value self  cuda device  non block              return new type indices  value  self size            else              new type = getattr torch cuda  self   class     name                return new type self size    copy  self  non block  
def  type self  dtype=none  non blocking=false    kwargs       string     non block =  get async or non block string  non block  kwargs      if dtype be none          return self   module     string   self   class     name        if isinstance dtype  str           dtype =  import dot name dtype      if dtype == type self           return self     if self be sparse          if not dtype be sparse              raise runtimeerror string          new module name = dtype   module   replace string  string          new value type name = new module name   string   dtype   name           new value = torch tensor  value self  type new value type name  non block          new indices type name = new module name   string         new indices = torch tensor  indices self  type new indices type name  non block          return dtype new indices  new value  self size        if dtype be sparse          raise runtimeerror string      return dtype self size    copy  self  non block  
def  cuda self  device=none  non blocking=false    kwargs       string     non block =  get async or non block string  non block  kwargs      if self be cuda          if device be none              device = torch cuda current device           if self get device   == device              return self     else          if device be none              device = -1     with torch cuda device device           if self be sparse              new type = getattr torch cuda sparse  self   class     name                indices = torch tensor  indices self  cuda device  non block              value = torch tensor  value self  cuda device  non block              return new type indices  value  self size            else              new type = getattr torch cuda  self   class     name                return new type self size    copy  self  non block  
def  type self  dtype=none  non blocking=false    kwargs       string     non block =  get async or non block string  non block  kwargs      if dtype be none          return self   module     string   self   class     name        if isinstance dtype  str           dtype =  import dot name dtype      if dtype == type self           return self     if self be sparse          if not dtype be sparse              raise runtimeerror string          new module name = dtype   module   replace string  string          new value type name = new module name   string   dtype   name           new value = torch tensor  value self  type new value type name  non block          new indices type name = new module name   string         new indices = torch tensor  indices self  type new indices type name  non block          return dtype new indices  new value  self size        if dtype be sparse          raise runtimeerror string      return dtype self size    copy  self  non block  
def  cuda self  device=none  non blocking=false    kwargs       string     non block =  get async or non block string  non block  kwargs      if self be cuda          if device be none              device = torch cuda current device           if self get device   == device              return self     else          if device be none              device = -1     with torch cuda device device           if self be sparse              new type = getattr torch cuda sparse  self   class     name                indices = torch tensor  indices self  cuda device  non block              value = torch tensor  value self  cuda device  non block              return new type indices  value  self size            else              new type = getattr torch cuda  self   class     name                return new type self size    copy  self  non block  
def  type self  dtype=none  non blocking=false    kwargs       string     non block =  get async or non block string  non block  kwargs      if dtype be none          return self   module     string   self   class     name        if isinstance dtype  str           dtype =  import dot name dtype      if dtype == type self           return self     if self be sparse          if not dtype be sparse              raise runtimeerror string          new module name = dtype   module   replace string  string          new value type name = new module name   string   dtype   name           new value = torch tensor  value self  type new value type name  non block          new indices type name = new module name   string         new indices = torch tensor  indices self  type new indices type name  non block          return dtype new indices  new value  self size        if dtype be sparse          raise runtimeerror string      return dtype self size    copy  self  non block  
def  cuda self  device=none  non blocking=false    kwargs       string     non block =  get async or non block string  non block  kwargs      if self be cuda          if device be none              device = torch cuda current device           if self get device   == device              return self     else          if device be none              device = -1     with torch cuda device device           if self be sparse              new type = getattr torch cuda sparse  self   class     name                indices = torch tensor  indices self  cuda device  non block              value = torch tensor  value self  cuda device  non block              return new type indices  value  self size            else              new type = getattr torch cuda  self   class     name                return new type self size    copy  self  non block  
def  type self  dtype=none  non blocking=false    kwargs       string     non block =  get async or non block string  non block  kwargs      if dtype be none          return self   module     string   self   class     name        if isinstance dtype  str           dtype =  import dot name dtype      if dtype == type self           return self     if self be sparse          if not dtype be sparse              raise runtimeerror string          new module name = dtype   module   replace string  string          new value type name = new module name   string   dtype   name           new value = torch tensor  value self  type new value type name  non block          new indices type name = new module name   string         new indices = torch tensor  indices self  type new indices type name  non block          return dtype new indices  new value  self size        if dtype be sparse          raise runtimeerror string      return dtype self size    copy  self  non block  
def  cuda self  device=none  non blocking=false    kwargs       string     non block =  get async or non block string  non block  kwargs      if self be cuda          if device be none              device = torch cuda current device           if self get device   == device              return self     else          if device be none              device = -1     with torch cuda device device           if self be sparse              new type = getattr torch cuda sparse  self   class     name                indices = torch tensor  indices self  cuda device  non block              value = torch tensor  value self  cuda device  non block              return new type indices  value  self size            else              new type = getattr torch cuda  self   class     name                return new type self size    copy  self  non block  
def  type self  dtype=none  non blocking=false    kwargs       string     non block =  get async or non block string  non block  kwargs      if dtype be none          return self   module     string   self   class     name        if isinstance dtype  str           dtype =  import dot name dtype      if dtype == type self           return self     if self be sparse          if not dtype be sparse              raise runtimeerror string          new module name = dtype   module   replace string  string          new value type name = new module name   string   dtype   name           new value = torch tensor  value self  type new value type name  non block          new indices type name = new module name   string         new indices = torch tensor  indices self  type new indices type name  non block          return dtype new indices  new value  self size        if dtype be sparse          raise runtimeerror string      return dtype self size    copy  self  non block  
def  cuda self  device=none  non blocking=false    kwargs       string     non block =  get async or non block string  non block  kwargs      if self be cuda          if device be none              device = torch cuda current device           if self get device   == device              return self     else          if device be none              device = -1     with torch cuda device device           if self be sparse              new type = getattr torch cuda sparse  self   class     name                indices = torch tensor  indices self  cuda device  non block              value = torch tensor  value self  cuda device  non block              return new type indices  value  self size            else              new type = getattr torch cuda  self   class     name                return new type self size    copy  self  non block  
def  type self  dtype=none  non blocking=false    kwargs       string     non block =  get async or non block string  non block  kwargs      if dtype be none          return self   module     string   self   class     name        if isinstance dtype  str           dtype =  import dot name dtype      if dtype == type self           return self     if self be sparse          if not dtype be sparse              raise runtimeerror string          new module name = dtype   module   replace string  string          new value type name = new module name   string   dtype   name           new value = torch tensor  value self  type new value type name  non block          new indices type name = new module name   string         new indices = torch tensor  indices self  type new indices type name  non block          return dtype new indices  new value  self size        if dtype be sparse          raise runtimeerror string      return dtype self size    copy  self  non block  
def assert close      actual  any      expect  any             rtol  optional float  = none      atol  optional float  = none      equal nan  union bool  str  = false      check device  bool = true      check dtype  bool = true      check stride  bool = true      msg  optional union str  callable  tensor  tensor  simplenamespace   str    = none    -> none      r   assert that  attr `actual` and  attr `expected` be close       if  attr `actual` and  attr `expected` be real-valued and finite  they be consider close if         math            \lvert \text actual  - \text expect  \rvert \le \texttt atol    \texttt rtol  \cdot \lvert \text expect  \rvert      and they have the same  attr `~torch tensor device`  if  attr `check device` be ``true``   same ``dtype``  if      attr `check dtype` be ``true``   and the same stride  if  attr `check stride` be ``true``   non-finite value      ``-inf`` and ``inf``  be only consider close if and only if they be equal  ``nan`` s be only consider equal     to each other if  attr `equal nan` be ``true``       if  attr `actual` and  attr `expected` be complex-valued  they be consider close if both their real and     imaginary components be consider close accord to the definition above        attr `actual` and  attr `expected` can be  class `~torch tensor` s or any array-or-scalar-like of the same type      from which  class `torch tensor` s can be construct with  func `torch as tensor`  in addition   attr `actual` and      attr `expected` can be  class `~collections abc sequence` s or  class `~collections abc mapping` s in which case     they be consider close if their structure match and all their elements be consider close accord to the     above definition       args          actual  any   actual input          expect  any   expect input          rtol  optional float    relative tolerance  if specify  attr `atol` must also be specify  if omit              default value base on the  attr `~torch tensor dtype` be select with the below table          atol  optional float    absolute tolerance  if specify  attr `rtol` must also be specify  if omit              default value base on the  attr `~torch tensor dtype` be select with the below table          equal nan  union bool  str    if ``true``  two ``nan`` value will be consider equal  if `` relax ``              complex value be consider as ``nan`` if either the real   or   imaginary component be ``nan``          check device  bool   if ``true``  default   assert that correspond tensors be on the same              attr `~torch tensor device`  if this check be disable  tensors on different              attr `~torch tensor device` s be move to the cpu before be compare          check dtype  bool   if ``true``  default   assert that correspond tensors have the same ``dtype``  if this             check be disable  tensors with different ``dtype`` s be promote  to a common ``dtype``  accord to              func `torch promote types`  before be compare          check stride  bool   if ``true``  default   assert that correspond tensors have the same stride          msg  optional union str  callable  tensor  tensor  diagnosticinfo   str      optional error message to use if             the value of correspond tensors mismatch  can be pass as callable in which case it will be call             with the mismatch tensors and a namespace of diagnostic info about the mismatch  see below for detail       raise          usageerror  if a  class `torch tensor` can t be construct from an array-or-scalar-like          usageerror  if any tensor be quantize or sparse  this be a temporary restriction and will be relax in the             future          usageerror  if only  attr `rtol` or  attr `atol` be specify          assertionerror  if correspond array-likes have different type          assertionerror  if the input be  class `~collections abc sequence` s  but their length do not match          assertionerror  if the input be  class `~collections abc mapping` s  but their set of key do not match          assertionerror  if correspond tensors do not have the same  attr `~torch tensor shape`          assertionerror  if  attr `check device`  but correspond tensors be not on the same              attr `~torch tensor device`          assertionerror  if  attr `check dtype`  but correspond tensors do not have the same ``dtype``          assertionerror  if  attr `check stride`  but correspond tensors do not have the same stride          assertionerror  if the value of correspond tensors be not close       the follow table display the default ``rtol`` and ``atol`` for different ``dtype`` s  note that the ``dtype``     refer to the promote type in case  attr `actual` and  attr `expected` do not have the same ``dtype``        --------------------------- ------------ ----------        ``dtype``                   ``rtol``     ``atol``        =========================== ============ ==========         attr `~torch float16`      ``1e-3``     ``1e-5``        --------------------------- ------------ ----------         attr `~torch bfloat16`     ``1 6e-2``   ``1e-5``        --------------------------- ------------ ----------         attr `~torch float32`      ``1 3e-6``   ``1e-5``        --------------------------- ------------ ----------         attr `~torch float64`      ``1e-7``     ``1e-7``        --------------------------- ------------ ----------         attr `~torch complex32`    ``1e-3``     ``1e-5``        --------------------------- ------------ ----------         attr `~torch complex64`    ``1 3e-6``   ``1e-5``        --------------------------- ------------ ----------         attr `~torch complex128`   ``1e-7``     ``1e-7``        --------------------------- ------------ ----------        other                       ``0 0``      ``0 0``         --------------------------- ------------ ----------       the namespace of diagnostic information that will be pass to  attr `msg` if its a callable have the follow     attribute       - ``number of elements``  int   number of elements in each tensor be compare      - ``total mismatches``  int   total number of mismatch      - ``mismatch ratio``  float   total mismatch divide by number of elements      - ``max abs diff``  union int  float    greatest absolute difference of the input      - ``max abs diff idx``  union int  tuple int          index of greatest absolute difference      - ``max rel diff``  union int  float    greatest relative difference of the input      - ``max rel diff idx``  union int  tuple int          index of greatest relative difference       for ``max abs diff`` and ``max rel diff`` the type depend on the  attr `~torch tensor dtype` of the input       examples          >>>   tensor to tensor comparison         >>> expect = torch tensor  1e0  1e-1  1e-2           >>> actual = torch acos torch cos expect           >>> torch test assert close actual  expect           >>>   scalar to scalar comparison         >>> import math         >>> expect = math sqrt 2 0          >>> actual = 2 0 / math sqrt 2 0          >>> torch test assert close actual  expect           >>>   numpy array to numpy array comparison         >>> import numpy as np         >>> expect = np array  1e0  1e-1  1e-2           >>> actual = np arccos np cos expect           >>> torch test assert close actual  expect           >>>   sequence to sequence comparison         >>> import numpy as np         >>>   the type of the sequence do not have to match  they only have to have the same         >>>   length and their elements have to match          >>> expect =  torch tensor  1 0    2 0  np array 3 0           >>> actual = tuple expect          >>> torch test assert close actual  expect           >>>   map to map comparison         >>> from collections import ordereddict         >>> import numpy as np         >>> foo = torch tensor 1 0          >>> bar = 2 0         >>> baz = np array 3 0          >>>   the type and a possible order of mappings do not have to match  they only         >>>   have to have the same set of key and their elements have to match          >>> expect = ordereddict    foo   foo     bar   bar     baz   baz            >>> actual =   baz   baz   bar   bar   foo   foo          >>> torch test assert close actual  expect           >>>   different input type be never consider close          >>> expect = torch tensor  1 0  2 0  3 0           >>> actual = expect numpy           >>> torch test assert close actual  expect          assertionerror  except for scalars  type equality be require  but get         <class  numpy ndarray > and <class  torch tensor > instead          >>>   scalars of different type be an exception and can be compare with         >>>   check dtype=false          >>> torch test assert close 1 0  1  check dtype=false           >>>   nan  = nan by default          >>> expect = torch tensor float  nan            >>> actual = expect clone           >>> torch test assert close actual  expect          assertionerror  tensors be not close          >>> torch test assert close actual  expect  equal nan=true           >>>   if equal nan=true  the real and imaginary nan s of complex input have to match          >>> expect = torch tensor complex float  nan    0           >>> actual = torch tensor complex 0  float  nan             >>> torch test assert close actual  expect  equal nan=true          assertionerror  tensors be not close          >>>   if equal nan= relax   however  then complex number be treat as nan if any         >>>   of the real or imaginary component be nan          >>> torch test assert close actual  expect  equal nan= relax            >>> expect = torch tensor  1 0  2 0  3 0           >>> actual = torch tensor  1 0  4 0  5 0           >>>   the default mismatch message can be overwrite          >>> torch test assert close actual  expect  msg= argh  the tensors be not close            assertionerror  argh  the tensors be not close          >>>   the error message can also create at runtime by pass a callable          >>> def custom msg actual  expect  diagnostic info                   return                       f argh  we find  diagnostic info total mismatch  mismatch                        f that be  diagnostic info mismatch ratio  1                               >>> torch test assert close actual  expect  msg=custom msg          assertionerror  argh  we find 2 mismatch  that be 66 7               exc  pair =  parse input actual  expect      if exc          raise exc     else          pair = cast union  tensorpair  list  dict   pair       check tensors = functools partial           check tensors close          rtol=rtol          atol=atol          equal nan=equal nan          check device=check device          check dtype=check dtype          check stride=check stride          msg=msg            exc =  check pair pair  check tensors      if exc          raise exc 
class dataloader generic t co        r        data loader  combine a dataset and a sampler  and provide an iterable over     the give dataset       the  class `~torch utils data dataloader` support both map-style and     iterable-style datasets with single- or multi-process load  customize     load order and optional automatic batch  collation  and memory pin       see  py mod `torch utils data` documentation page for more detail       args          dataset  dataset   dataset from which to load the data          batch size  int  optional   how many sample per batch to load              default  ``1``           shuffle  bool  optional   set to ``true`` to have the data reshuffle             at every epoch  default  ``false``           sampler  sampler or iterable  optional   define the strategy to draw             sample from the dataset  can be any ``iterable`` with ``  len  ``             implement  if specify   attr `shuffle` must not be specify          batch sampler  sampler or iterable  optional   like  attr `sampler`  but             return a batch of indices at a time  mutually exclusive with              attr `batch size`   attr `shuffle`   attr `sampler`              and  attr `drop last`          num workers  int  optional   how many subprocesses to use for data             load  ``0`` mean that the data will be load in the main process               default  ``0``          collate fn  callable  optional   merge a list of sample to form a             mini-batch of tensor s    use when use batch load from a             map-style dataset          pin memory  bool  optional   if ``true``  the data loader will copy tensors             into cuda pin memory before return them   if your data elements             be a custom type  or your  attr `collate fn` return a batch that be a custom type              see the example below          drop last  bool  optional   set to ``true`` to drop the last incomplete batch              if the dataset size be not divisible by the batch size  if ``false`` and             the size of dataset be not divisible by the batch size  then the last batch             will be smaller   default  ``false``          timeout  numeric  optional   if positive  the timeout value for collect a batch             from workers  should always be non-negative   default  ``0``          worker init fn  callable  optional   if not ``none``  this will be call on each             worker subprocess with the worker id  an int in `` 0  num workers - 1 ``  as             input  after seed and before data load   default  ``none``          generator  torch generator  optional   if not ``none``  this rng will be use             by randomsampler to generate random index and multiprocessing to generate             `base seed` for workers   default  ``none``          prefetch factor  int  optional  keyword-only arg   number of sample load             in advance by each worker  ``2`` mean there will be a total of             2   num workers sample prefetched across all workers   default  ``2``          persistent workers  bool  optional   if ``true``  the data loader will not shutdown             the worker process after a dataset have be consume once  this allow to             maintain the workers `dataset` instance alive   default  ``false``           warn   if the ``spawn`` start method be use   attr `worker init fn`                  cannot be an unpicklable object  e g   a lambda function  see                   ref `multiprocessing-best-practices` on more detail relate                  to multiprocessing in pytorch          warn   ``len dataloader `` heuristic be base on the length of the sampler use                   when  attr `dataset` be an  class `~torch utils data iterabledataset`                   it instead return an estimate base on ``len dataset  / batch size``  with proper                  round depend on  attr `drop last`  regardless of multi-process load                  configurations  this represent the best guess pytorch can make because pytorch                  trust user  attr `dataset` code in correctly handle multi-process                  load to avoid duplicate data                    however  if sharding result in multiple workers have incomplete last batch                   this estimate can still be inaccurate  because  1  an otherwise complete batch can                  be break into multiple ones and  2  more than one batch worth of sample can be                  drop when  attr `drop last` be set  unfortunately  pytorch can not detect such                  case in general                    see `dataset types`  for more detail on these two type of datasets and how                   class `~torch utils data iterabledataset` interact with                  `multi-process data loading`           warn   see  ref `reproducibility`  and  ref `dataloader-workers-random-seed`  and                   ref `data-loading-randomness` note for random seed relate question              dataset  dataset t co      batch size  optional int      num workers  int     pin memory  bool     drop last  bool     timeout  float     sampler  sampler     prefetch factor  int      iterator   optional string        initialize = false      def   init   self  dataset  dataset t co   batch size  optional int  = 1                   shuffle  bool = false  sampler  optional sampler int   = none                   batch sampler  optional sampler sequence int    = none                   num workers  int = 0  collate fn  optional  collate fn t  = none                   pin memory  bool = false  drop last  bool = false                   timeout  float = 0  worker init fn  optional  worker init fn t  = none                   multiprocessing context=none  generator=none                      prefetch factor  int = 2                   persistent workers  bool = false           torch  c  log api usage once string           if num workers < 0              raise valueerror string                              string           if timeout < 0              raise valueerror string           if num workers == 0 and prefetch factor  = 2              raise valueerror string                              string          assert prefetch factor > 0          if persistent workers and num workers == 0              raise valueerror string           self dataset = dataset         self num workers = num workers         self prefetch factor = prefetch factor         self pin memory = pin memory         self timeout = timeout         self worker init fn = worker init fn         self multiprocessing context = multiprocessing context                                              if isinstance dataset  iterabledataset               self  dataset kind =  datasetkind iterable                                                                                                                                                                                                                                                                                                                                                  if shuffle be not false                  raise valueerror                      string                     string format shuffle               elif sampler be not none                                   raise valueerror                      string                     string format sampler               elif batch sampler be not none                                   raise valueerror                      string                     string format batch sampler           else              self  dataset kind =  datasetkind map          if sampler be not none and shuffle              raise valueerror string                              string           if batch sampler be not none                           if batch size  = 1 or shuffle or sampler be not none or drop last                  raise valueerror string                                  string                                  string              batch size = none             drop last = false         elif batch size be none                           if drop last                  raise valueerror string                                  string           if sampler be none                if self  dataset kind ==  datasetkind iterable                                   sampler =  infiniteconstantsampler               else                    if shuffle                                                                sampler = randomsampler dataset  generator=generator                    else                      sampler = sequentialsampler dataset             if batch size be not none and batch sampler be none                           batch sampler = batchsampler sampler  batch size  drop last           self batch size = batch size         self drop last = drop last         self sampler = sampler         self batch sampler = batch sampler         self generator = generator          if collate fn be none              if self  auto collation                  collate fn =  utils collate default collate             else                  collate fn =  utils collate default convert          self collate fn = collate fn         self persistent workers = persistent workers          self   initialize = true         self  iterabledataset len call = none            self  iterator = none          self check worker number rationality        def  get iterator self  -> string          if self num workers == 0              return  singleprocessdataloaderiter self          else              self check worker number rationality               return  multiprocessingdataloaderiter self        property     def multiprocessing context self           return self   multiprocessing context       multiprocessing context setter     def multiprocessing context self  multiprocessing context           if multiprocessing context be not none              if self num workers > 0                  if isinstance multiprocessing context  string class                       valid start methods = multiprocessing get all start methods                       if multiprocessing context not in valid start methods                          raise valueerror                               string                              string                              string  format valid start methods  multiprocessing context                                            multiprocessing context = multiprocessing get context multiprocessing context                     if not isinstance multiprocessing context  python multiprocessing context basecontext                       raise typeerror  string                                      string                                      string  format multiprocessing context               else                  raise valueerror  string                                   string                                   string  format self num workers            self   multiprocessing context = multiprocessing context      def   setattr   self  attr  val           if self   initialize and attr in                   string  string  string  string  string  string               raise valueerror string                              string format attr  self   class     name              super dataloader  self    setattr   attr  val                 def   iter   self  -> string                                                       if self persistent workers and self num workers > 0              if self  iterator be none                  self  iterator = self  get iterator               else                  self  iterator  reset self              return self  iterator         else              return self  get iterator         property     def  auto collation self           return self batch sampler be not none       property     def  index sampler self                                                        if self  auto collation              return self batch sampler         else              return self sampler      def   len   self  -> int          if self  dataset kind ==  datasetkind iterable                                                                                                                                                                                                                  length = self  iterabledataset len call = len self dataset                if self batch size be not none                    from math import ceil                 if self drop last                      length = length // self batch size                 else                      length = ceil length / self batch size              return length         else              return len self  index sampler       def check worker number rationality self                                                                                                                                                                                                                                                     def  create warn msg num worker suggest  num worker create  cpuset check                suggest max worker msg =                    string                 string  format                      num worker suggest                       string if cpuset check else string                 if num worker suggest be not none else                   string               warn msg =                   string                 string                 string  format                      num worker create                      suggest max worker msg              return warn msg          if not self num workers or self num workers == 0              return                   max num worker suggest = none         cpuset check = false         if hasattr os  string               try                  max num worker suggest = len os sched getaffinity 0                   cpuset check = true             except exception                  pass         if max num worker suggest be none                                        cpu count = os cpu count               if cpu count be not none                  max num worker suggest = cpu count          if max num worker suggest be none              warn warn  create warn msg                  max num worker suggest                  self num workers                  cpuset check               return          if self num workers > max num worker suggest              warn warn  create warn msg                  max num worker suggest                  self num workers                  cpuset check   
class dataset generic t co        r   an abstract class represent a  class `dataset`       all datasets that represent a map from key to data sample should subclass     it  all subclasses should overwrite  meth `  getitem  `  support fetch a     data sample for a give key  subclasses could also optionally overwrite      meth `  len  `  which be expect to return the size of the dataset by many      class `~torch utils data sampler` implementations and the default options     of  class `~torch utils data dataloader`          note          class `~torch utils data dataloader` by default construct a index       sampler that yield integral indices   to make it work with a map-style       dataset with non-integral indices/keys  a custom sampler must be provide               def   getitem   self  index  -> t co          raise notimplementederror      def   add   self  other  string  -> string          return concatdataset  self  other   
class iterabledataset dataset t co   metaclass= datapipemeta       r   an iterable dataset       all datasets that represent an iterable of data sample should subclass it      such form of datasets be particularly useful when data come from a stream       all subclasses should overwrite  meth `  iter  `  which would return an     iterator of sample in this dataset       when a subclass be use with  class `~torch utils data dataloader`  each     item in the dataset will be yield from the  class `~torch utils data dataloader`     iterator  when  attr `num workers > 0`  each worker process will have a     different copy of the dataset object  so it be often desire to configure     each copy independently to avoid have duplicate data return from the     workers   func `~torch utils data get worker info`  when call in a worker     process  return information about the worker  it can be use in either the     dataset s  meth `  iter  ` method or the  class `~torch utils data dataloader`  s      attr `worker init fn` option to modify each copy s behavior       example 1  split workload across all workers in  meth `  iter  `            >>> class myiterabledataset torch utils data iterabledataset                   def   init   self  start  end                       super myiterabledataset    init                         assert end > start   this example code only work with end >= start                      self start = start                     self end = end                             def   iter   self                       worker info = torch utils data get worker info                       if worker info be none     single-process data load  return the full iterator                         iter start = self start                         iter end = self end                     else     in a worker process                           split workload                         per worker = int math ceil  self end - self start  / float worker info num workers                            worker id = worker info id                         iter start = self start   worker id   per worker                         iter end = min iter start   per worker  self end                      return iter range iter start  iter end                       >>>   should give same set of data as range 3  7   i e    3  4  5  6           >>> ds = myiterabledataset start=3  end=7           >>>   single-process load         >>> print list torch utils data dataloader ds  num workers=0             3  4  5  6           >>>   mult-process load with two worker process         >>>   worker 0 fetch  3  4    worker 1 fetch  5  6           >>> print list torch utils data dataloader ds  num workers=2             3  5  4  6           >>>   with even more workers         >>> print list torch utils data dataloader ds  num workers=20             3  4  5  6       example 2  split workload across all workers use  attr `worker init fn`            >>> class myiterabledataset torch utils data iterabledataset                   def   init   self  start  end                       super myiterabledataset    init                         assert end > start   this example code only work with end >= start                      self start = start                     self end = end                             def   iter   self                       return iter range self start  self end                       >>>   should give same set of data as range 3  7   i e    3  4  5  6           >>> ds = myiterabledataset start=3  end=7           >>>   single-process load         >>> print list torch utils data dataloader ds  num workers=0             3  4  5  6          >>>         >>>   directly do multi-process load yield duplicate data         >>> print list torch utils data dataloader ds  num workers=2             3  3  4  4  5  5  6  6           >>>   define a `worker init fn` that configure each dataset copy differently         >>> def worker init fn worker id                   worker info = torch utils data get worker info                   dataset = worker info dataset    the dataset copy in this worker process                 overall start = dataset start                 overall end = dataset end                   configure the dataset to only process the split workload                 per worker = int math ceil  overall end - overall start  / float worker info num workers                    worker id = worker info id                 dataset start = overall start   worker id   per worker                 dataset end = min dataset start   per worker  overall end                       >>>   mult-process load with the custom `worker init fn`         >>>   worker 0 fetch  3  4    worker 1 fetch  5  6           >>> print list torch utils data dataloader ds  num workers=2  worker init fn=worker init fn             3  5  4  6           >>>   with even more workers         >>> print list torch utils data dataloader ds  num workers=20  worker init fn=worker init fn             3  4  5  6              function  dict str  callable  =        reduce ex hook   optional callable  = none      def   iter   self  -> iterator t co           raise notimplementederror      def   add   self  other  dataset t co            return chaindataset  self  other          no `def   len   self ` default?       see note   lack of default `  len  ` in python abstract base class        def   getattr   self  attribute name           if attribute name in iterabledataset function              function = functools partial iterabledataset function attribute name   self              return function         else              raise attributeerror       classmethod     def register function cls  function name  function           iterabledataset function function name  = function       classmethod     def register datapipe as function cls  function name  cls to register           if function name in iterabledataset function              raise exception  unable to add datapipe function name    as it be already take  format function name            def class function cls  source dp   args    kwargs               return cls source dp   args    kwargs          function = functools partial class function  cls to register          iterabledataset function function name  = function      def   reduce ex   self   args    kwargs           if iterabledataset reduce ex hook be not none              try                  return iterabledataset reduce ex hook self              except notimplementederror                  pass         return super     reduce ex    args    kwargs        classmethod     def set reduce ex hook cls  hook fn           if iterabledataset reduce ex hook be not none and hook fn be not none              raise exception  attempt to override exist reduce ex hook           iterabledataset reduce ex hook = hook fn 
class tensordataset dataset tuple tensor              r   dataset wrap tensors       each sample will be retrieve by index tensors along the first dimension       args           tensors  tensor   tensors that have the same size of the first dimension              tensors  tuple tensor            def   init   self   tensors  tensor  -> none          assert all tensors 0  size 0  == tensor size 0  for tensor in tensors   string         self tensors = tensors      def   getitem   self  index           return tuple tensor index  for tensor in self tensors       def   len   self           return self tensors 0  size 0  
class concatdataset dataset t co        r   dataset as a concatenation of multiple datasets       this class be useful to assemble different exist datasets       args          datasets  sequence   list of datasets to be concatenate             datasets  list dataset t co       cumulative size  list int        staticmethod     def cumsum sequence           r  s =     0         for e in sequence              l = len e              r append l   s              s  = l         return r      def   init   self  datasets  iterable dataset   -> none          super concatdataset  self    init                      assert len datasets  > 0  string           self datasets = list datasets          for d in self datasets              assert not isinstance d  iterabledataset   string         self cumulative size = self cumsum self datasets       def   len   self           return self cumulative size -1       def   getitem   self  idx           if idx < 0              if -idx > len self                   raise valueerror string              idx = len self    idx         dataset idx = bisect bisect right self cumulative size  idx          if dataset idx == 0              sample idx = idx         else              sample idx = idx - self cumulative size dataset idx - 1          return self datasets dataset idx  sample idx        property     def cummulative size self           warn warn string                       string  deprecationwarning  stacklevel=2          return self cumulative size 
class chaindataset iterabledataset       r   dataset for chainning multiple  class `iterabledataset` s       this class be useful to assemble different exist dataset stream  the     chainning operation be do on-the-fly  so concatenate large-scale     datasets with this class will be efficient       args          datasets  iterable of iterabledataset   datasets to be chain together             def   init   self  datasets  iterable dataset   -> none          super chaindataset  self    init             self datasets = datasets      def   iter   self           for d in self datasets              assert isinstance d  iterabledataset   string             for x in d                  yield x      def   len   self           total = 0         for d in self datasets              assert isinstance d  iterabledataset   string                          total  = len d          return total 
class subset dataset t co        r        subset of a dataset at specify indices       args          dataset  dataset   the whole dataset         indices  sequence   indices in the whole set select for subset             dataset  dataset t co      indices  sequence int       def   init   self  dataset  dataset t co   indices  sequence int   -> none          self dataset = dataset         self indices = indices      def   getitem   self  idx           return self dataset self indices idx        def   len   self           return len self indices  
class sampler generic t co        r   base class for all samplers       every sampler subclass have to provide an  meth `  iter  ` method  provide a     way to iterate over indices of dataset elements  and a  meth `  len  ` method     that return the length of the return iterators          note   the  meth `  len  ` method isn t strictly require by                class `~torch utils data dataloader`  but be expect in any               calculation involve the length of a  class `~torch utils data dataloader`               def   init   self  data source  optional size   -> none          pass      def   iter   self  -> iterator t co           raise notimplementederror 
class sequentialsampler sampler int        r   sample elements sequentially  always in the same order       args          data source  dataset   dataset to sample from             data source  size      def   init   self  data source  size  -> none          self data source = data source      def   iter   self  -> iterator int           return iter range len self data source         def   len   self  -> int          return len self data source  
class randomsampler sampler int        r   sample elements randomly  if without replacement  then sample from a shuffle dataset      if with replacement  then user can specify  attr `num samples` to draw       args          data source  dataset   dataset to sample from         replacement  bool   sample be draw on-demand with replacement if ``true``  default=``false``         num sample  int   number of sample to draw  default=`len dataset `  this argument             be suppose to be specify only when `replacement` be ``true``          generator  generator   generator use in sample              data source  size     replacement  bool      def   init   self  data source  size  replacement  bool = false                   num sample  optional int  = none  generator=none  -> none          self data source = data source         self replacement = replacement         self  num sample = num sample         self generator = generator          if not isinstance self replacement  bool               raise typeerror string                             string format self replacement            if self  num sample be not none and not replacement              raise valueerror string                              string           if not isinstance self num sample  int  or self num sample <= 0              raise valueerror string                              string format self num sample         property     def num sample self  -> int                   if self  num sample be none              return len self data source          return self  num sample      def   iter   self  -> iterator int           n = len self data source          if self generator be none              generator = torch generator               generator manual seed int torch empty     dtype=torch int64  random    item             else              generator = self generator         if self replacement              for   in range self num sample // 32                   yield from torch randint high=n  size= 32    dtype=torch int64  generator=generator  tolist               yield from torch randint high=n  size= self num sample   32    dtype=torch int64  generator=generator  tolist           else              yield from torch randperm n  generator=generator  tolist        def   len   self  -> int          return self num sample 
class subsetrandomsampler sampler int        r   sample elements randomly from a give list of indices  without replacement       args          indices  sequence   a sequence of indices         generator  generator   generator use in sample              indices  sequence int       def   init   self  indices  sequence int   generator=none  -> none          self indices = indices         self generator = generator      def   iter   self  -> iterator int           return  self indices i  for i in torch randperm len self indices   generator=self generator        def   len   self  -> int          return len self indices  
class weightedrandomsampler sampler int        r   sample elements from `` 0    len weight -1 `` with give probabilities  weight        args          weight  sequence      a sequence of weight  not necessary sum up to one         num sample  int   number of sample to draw         replacement  bool   if ``true``  sample be draw with replacement              if not  they be draw without replacement  which mean that when a             sample index be draw for a row  it cannot be draw again for that row          generator  generator   generator use in sample       example          >>> list weightedrandomsampler  0 1  0 9  0 4  0 7  3 0  0 6   5  replacement=true            4  4  1  4  5          >>> list weightedrandomsampler  0 9  0 4  0 05  0 2  0 3  0 1   5  replacement=false            0  1  4  3  2              weight  tensor     num sample  int     replacement  bool      def   init   self  weight  sequence float   num sample  int                   replacement  bool = true  generator=none  -> none          if not isinstance num sample  int  or isinstance num sample  bool  or \                 num sample <= 0              raise valueerror string                              string format num sample           if not isinstance replacement  bool               raise valueerror string                              string format replacement           self weight = torch as tensor weight  dtype=torch double          self num sample = num sample         self replacement = replacement         self generator = generator      def   iter   self  -> iterator int           rand tensor = torch multinomial self weight  self num sample  self replacement  generator=self generator          return iter rand tensor tolist         def   len   self  -> int          return self num sample 
class batchsampler sampler list int         r   wrap another sampler to yield a mini-batch of indices       args          sampler  sampler or iterable   base sampler  can be any iterable object         batch size  int   size of mini-batch          drop last  bool   if ``true``  the sampler will drop the last batch if             its size would be less than ``batch size``      example          >>> list batchsampler sequentialsampler range 10    batch size=3  drop last=false             0  1  2    3  4  5    6  7  8    9           >>> list batchsampler sequentialsampler range 10    batch size=3  drop last=true             0  1  2    3  4  5    6  7  8                def   init   self  sampler  sampler int   batch size  int  drop last  bool  -> none                                     if not isinstance batch size  int  or isinstance batch size  bool  or \                 batch size <= 0              raise valueerror string                              string format batch size           if not isinstance drop last  bool               raise valueerror string                              string format drop last           self sampler = sampler         self batch size = batch size         self drop last = drop last      def   iter   self  -> iterator list int            batch =            for idx in self sampler              batch append idx              if len batch  == self batch size                  yield batch                 batch =            if len batch  > 0 and not self drop last              yield batch      def   len   self  -> int                                              if self drop last              return len self sampler  // self batch size           else              return  len self sampler    self batch size - 1  // self batch size   
class distributedsampler sampler t co        r   sampler that restrict data load to a subset of the dataset       it be especially useful in conjunction with      class `torch nn parallel distributeddataparallel`  in such a case  each     process can pass a  class `~torch utils data distributedsampler` instance as a      class `~torch utils data dataloader` sampler  and load a subset of the     original dataset that be exclusive to it          note           dataset be assume to be of constant size       args          dataset  dataset use for sample          num replicas  int  optional   number of process participate in             distribute train  by default   attr `world size` be retrieve from the             current distribute group          rank  int  optional   rank of the current process within  attr `num replicas`              by default   attr `rank` be retrieve from the current distribute             group          shuffle  bool  optional   if ``true``  default   sampler will shuffle the             indices          seed  int  optional   random seed use to shuffle the sampler if              attr `shuffle=true`  this number should be identical across all             process in the distribute group  default  ``0``          drop last  bool  optional   if ``true``  then the sampler will drop the             tail of the data to make it evenly divisible across the number of             replicas  if ``false``  the sampler will add extra indices to make             the data evenly divisible across the replicas  default  ``false``          warn           in distribute mode  call the  meth `set epoch` method at         the begin of each epoch   before   create the  class `dataloader` iterator         be necessary to make shuffle work properly across multiple epochs  otherwise          the same order will be always use       example            >>> sampler = distributedsampler dataset  if be distribute else none         >>> loader = dataloader dataset  shuffle= sampler be none                                   sampler=sampler          >>> for epoch in range start epoch  n epochs                   if be distribute                      sampler set epoch epoch                  train loader               def   init   self  dataset  dataset  num replicas  optional int  = none                   rank  optional int  = none  shuffle  bool = true                   seed  int = 0  drop last  bool = false  -> none          if num replicas be none              if not dist be available                    raise runtimeerror string              num replicas = dist get world size           if rank be none              if not dist be available                    raise runtimeerror string              rank = dist get rank           if rank >= num replicas or rank < 0              raise valueerror                  string                 string format rank  num replicas - 1           self dataset = dataset         self num replicas = num replicas         self rank = rank         self epoch = 0         self drop last = drop last                           if self drop last and len self dataset    self num replicas  = 0                                                       self num sample = math ceil                                                     len self dataset  - self num replicas  / self num replicas                         else              self num sample = math ceil len self dataset  / self num replicas            self total size = self num sample   self num replicas         self shuffle = shuffle         self seed = seed      def   iter   self  -> iterator t co           if self shuffle                           g = torch generator               g manual seed self seed   self epoch              indices = torch randperm len self dataset   generator=g  tolist             else              indices = list range len self dataset               if not self drop last                           pad size = self total size - len indices              if pad size <= len indices                   indices  = indices  pad size              else                  indices  =  indices   math ceil pad size / len indices     pad size          else                           indices = indices  self total size          assert len indices  == self total size                   indices = indices self rank self total size self num replicas          assert len indices  == self num sample          return iter indices       def   len   self  -> int          return self num sample      def set epoch self  epoch  int  -> none          r            set the epoch for this sampler  when  attr `shuffle=true`  this ensure all replicas         use a different random order for each epoch  otherwise  the next iteration of this         sampler will yield the same order           args              epoch  int   epoch number                      self epoch = epoch 
def get worker info        r   return the information about the current      class `~torch utils data dataloader` iterator worker process       when call in a worker  this return an object guarantee to have the     follow attribute          attr `id`  the current worker id         attr `num workers`  the total number of workers         attr `seed`  the random seed set for the current worker  this value be       determine by main process rng and the worker id  see        class `~torch utils data dataloader` s documentation for more detail         attr `dataset`  the copy of the dataset object in   this   process  note       that this will be a different object in a different process than the one       in the main process       when call in the main process  this return ``none``          note          when use in a  attr `worker init fn` pass over to         class `~torch utils data dataloader`  this method can be useful to        set up each worker process differently  for instance  use ``worker id``        to configure the ``dataset`` object to only read a specific fraction of a        sharded dataset  or use ``seed`` to seed other libraries use in dataset        code              return  worker info 
def random split dataset  dataset t   lengths  sequence int                    generator  optional generator  = default generator  -> list subset t        r        randomly split a dataset into non-overlapping new datasets of give lengths      optionally fix the generator for reproducible result  e g        >>> random split range 10    3  7   generator=torch generator   manual seed 42        args          dataset  dataset   dataset to be split         lengths  sequence   lengths of split to be produce         generator  generator   generator use for the random permutation                   if sum lengths   = len dataset             raise valueerror string       indices = randperm sum lengths   generator=generator  tolist       return  subset dataset  indices offset - length   offset   for offset  length in zip  accumulate lengths   lengths   
def rename self   name    rename map       string     if have torch function unary self           return handle torch function tensor rename   self    self   name    rename map            return update name self  name  rename map  inplace=false  
def rename  self   name    rename map       string      if have torch function unary self           return handle torch function tensor rename    self    self   name    rename map                                return update name self  name  rename map  inplace=true  
def refine name self   name       r   refine the dimension name of  attr `self` accord to  attr `names`       refine be a special case of rename that  lift  unnamed dimension      a ``none`` dim can be refine to have any name  a name dim can only be     refine to have the same name       because name tensors can coexist with unnamed tensors  refine name     give a nice way to write named-tensor-aware code that work with both     name and unnamed tensors        attr `names` may contain up to one ellipsis  ``   ``       the ellipsis be expand greedily  it be expand in-place to fill      attr `names` to the same length as ``self dim  `` use name from the     correspond indices of ``self names``       python 2 do not support ellipsis but one may use a string literal     instead  ``     ``        args          name  iterable of str   the desire name of the output tensor  may             contain up to one ellipsis       examples            >>> imgs = torch randn 32  3  128  128          >>> name imgs = imgs refine name  n    c    h    w           >>> name imgs name           n    c    h    w            >>> tensor = torch randn 2  3  5  7  11          >>> tensor = tensor refine name  a         b    c           >>> tensor name           a   none  none   b    c           warn           the name tensor api be experimental and subject to change               if have torch function unary self           return handle torch function tensor refine name   self    self   name      name = resolve ellipsis name  self name  string      return super tensor  self  refine name name  
def align to self   name       r   permute the dimension of the  attr `self` tensor to match the order     specify in  attr `names`  add size-one dim for any new name       all of the dim of  attr `self` must be name in order to use this method      the result tensor be a view on the original tensor       all dimension name of  attr `self` must be present in  attr `names`       attr `names` may contain additional name that be not in ``self names``      the output tensor have a size-one dimension for each of those new name        attr `names` may contain up to one ellipsis  ``   ``       the ellipsis be expand to be equal to all dimension name of  attr `self`     that be not mention in  attr `names`  in the order that they appear     in  attr `self`       python 2 do not support ellipsis but one may use a string literal     instead  ``     ``        args          name  iterable of str   the desire dimension order of the             output tensor  may contain up to one ellipsis that be expand             to all unmentioned dim name of  attr `self`       examples            >>> tensor = torch randn 2  2  2  2  2  2          >>> name tensor = tensor refine name  a    b    c    d    e    f              move the f and e dim to the front while keep the rest in order         >>> name tensor align to  f    e                warn           the name tensor api be experimental and subject to change               if have torch function unary self           return handle torch function tensor align to   self    self   name      ellipsis idx = single ellipsis index name  string      if ellipsis idx be none          return super tensor  self  align to name      return super tensor  self  align to           name for name in name if not be ellipsis name            ellipsis idx  
def unflatten self  dim  size       r   expand the dimension  attr `dim` of the  attr `self` tensor over multiple dimension     of size give by  attr `sizes`          attr `sizes` be the new shape of the unflattened dimension and it can be a `tuple int ` as well       as `torch size` if  attr `self` be a `tensor`  or `namedshape`  tuple  name  str  size  int          if  attr `self` be a `namedtensor`  the total number of elements in size must match the number       of elements in the original dim be unflattened       args          dim  union int  str    dimension to unflatten         size  union tuple int  or torch size  tuple tuple str  int      new shape of the unflattened dimension      examples          >>> torch randn 3  4  1  unflatten 1   2  2   shape         torch size  3  2  2  1           >>> torch randn 3  4  1  unflatten 1   -1  2   shape   the size -1 be infer from the size of dimension 1         torch size  3  2  2  1           >>> torch randn 2  4  names=  a    b    unflatten  b      b1   2     b2   2            tensor    -1 1772   0 0180                     0 2412   0 1431                      -1 1819  -0 8899                     1 5813   0 2274     names=  a    b1    b2            >>> torch randn 2  names=  a     unflatten  a      b1   -1     b2   1            tensor   -0 8591                     0 3100    names=  b1    b2            warn           the name tensor api be experimental and subject to change               if have torch function unary self           return handle torch function tensor unflatten   self    self  dim  size       if not size          raise runtimeerror string       name = none     if isinstance size  ordereddict  or  isinstance size   tuple  list   and isinstance size 0    tuple  list             name  size = unzip namedshape size      return super tensor  self  unflatten dim  size  name  
def be tensor obj       r   return true if `obj` be a pytorch tensor       note that this function be simply do ``isinstance obj  tensor ``      use that ``isinstance`` check be better for typechecking with mypy      and more explicit - so it s recommend to use that instead of     ``is tensor``       args          obj  object   object to test     example            >>> x=torch tensor  1 2 3           >>> torch be tensor x          true              return isinstance obj  torch tensor  
def be storage obj       r   return true if `obj` be a pytorch storage object       args          obj  object   object to test             return type obj  in  storage class 
def set default dtype d       r   set the default float point dtype to  attr `d`      this dtype be       1  the infer dtype for python float in  func `torch tensor`      2  use to infer dtype for python complex number  the default complex dtype be set to        ``torch complex128`` if default float point dtype be ``torch float64``         otherwise it s set to ``torch complex64``      the default float point dtype be initially ``torch float32``       args          d   class `torch dtype`   the float point dtype to make the default      example          >>>   initial default for float point be torch float32         >>> torch tensor  1 2  3   dtype         torch float32         >>>   initial default for float point be torch complex64         >>> torch tensor  1 2  3j   dtype         torch complex64         >>> torch set default dtype torch float64          >>> torch tensor  1 2  3   dtype      a new float point tensor         torch float64         >>> torch tensor  1 2  3j   dtype     a new complex tensor         torch complex128               c  set default dtype d  
def set default tensor type t       r   set the default ``torch tensor`` type to float point tensor type     ``t``  this type will also be use as default float point type for     type inference in  func `torch tensor`       the default float point tensor type be initially ``torch floattensor``       args          t  type or string   the float point tensor type or its name      example            >>> torch tensor  1 2  3   dtype      initial default for float point be torch float32         torch float32         >>> torch set default tensor type torch doubletensor          >>> torch tensor  1 2  3   dtype      a new float point tensor         torch float64              if isinstance t   string class           t =  import dot name t       c  set default tensor type t  
def set printoptions          precision=none          threshold=none          edgeitems=none          linewidth=none          profile=none          sci mode=none        r   set options for print  items shamelessly take from numpy      args          precision  number of digits of precision for float point output              default = 4           threshold  total number of array elements which trigger summarization             rather than full `repr`  default = 1000           edgeitems  number of array items in summary at begin and end of             each dimension  default = 3           linewidth  the number of character per line for the purpose of             insert line break  default = 80   thresholded matrices will             ignore this parameter          profile  sane default for pretty print  can override with any of             the above options   any one of `default`  `short`  `full`          sci mode  enable  true  or disable  false  scientific notation  if             none  default  be specify  the value be define by             `torch  tensor str  formatter`  this value be automatically choose             by the framework              if profile be not none          if profile == string              print opt precision = 4             print opt threshold = 1000             print opt edgeitems = 3             print opt linewidth = 80         elif profile == string              print opt precision = 2             print opt threshold = 1000             print opt edgeitems = 2             print opt linewidth = 80         elif profile == string              print opt precision = 4             print opt threshold = inf             print opt edgeitems = 3             print opt linewidth = 80      if precision be not none          print opt precision = precision     if threshold be not none          print opt threshold = threshold     if edgeitems be not none          print opt edgeitems = edgeitems     if linewidth be not none          print opt linewidth = linewidth     print opt sci mode = sci mode 
def split tensor  split size or section  dim=0       r   split the tensor into chunk  each chunk be a view of the original tensor       if  attr `split size or sections` be an integer type  then  attr `tensor` will     be split into equally size chunk  if possible   last chunk will be smaller if     the tensor size along the give dimension  attr `dim` be not divisible by      attr `split size`       if  attr `split size or sections` be a list  then  attr `tensor` will be split     into ``len split size or section `` chunk with size in  attr `dim` accord     to  attr `split size or sections`       args          tensor  tensor   tensor to split          split size or section  int  or  list int    size of a single chunk or             list of size for each chunk         dim  int   dimension along which to split the tensor       example            >>> a = torch arange 10  reshape 5 2          >>> a         tensor   0  1                    2  3                    4  5                    6  7                    8  9            >>> torch split a  2           tensor   0  1                     2  3              tensor   4  5                     6  7              tensor   8  9             >>> torch split a   1 4            tensor   0  1              tensor   2  3                     4  5                     6  7                     8  9                 if have torch function unary tensor           return handle torch function              split   tensor    tensor  split size or section  dim=dim                          return tensor split split size or section  dim  
def seed   -> int      r   set the seed for generate random number to a non-deterministic     random number  return a 64 bite number use to seed the rng              seed = default generator seed       import torch cuda      if not torch cuda  be in bad fork            torch cuda manual seed all seed       return seed 
def manual seed seed  -> torch  c generator      r   set the seed for generate random number  return a     `torch generator` object       args          seed  int   the desire seed  value must be within the inclusive range             ` -0x8000 0000 0000 0000  0xffff ffff ffff ffff `  otherwise  a runtimeerror             be raise  negative input be remapped to positive value with the formula             `0xffff ffff ffff ffff   seed`              seed = int seed      import torch cuda      if not torch cuda  be in bad fork            torch cuda manual seed all seed       return default generator manual seed seed  
def initial seed   -> int      r   return the initial seed for generate random number as a     python `long`              return default generator initial seed   
def get rng state   -> torch tensor      r   return the random number generator state as a `torch bytetensor`         return default generator get state   
def set rng state new state  torch tensor  -> none      r   set the random number generator state       args          new state  torch bytetensor   the desire state             default generator set state new state  
class sobolengine object       r        the  class `torch quasirandom sobolengine` be an engine for generate      scramble  sobol sequence  sobol sequence be an example of low     discrepancy quasi-random sequence       this implementation of an engine for sobol sequence be capable of     sample sequence up to a maximum dimension of 21201  it use direction     number from https //web maths unsw edu au/~fkuo/sobol/ obtain use the     search criterion d 6  up to the dimension 21201  this be the recommend     choice by the author       reference        - art b  owen  scramble sobol and niederreiter-xing point          journal of complexity  14 4  466-489  december 1998         - i  m  sobol  the distribution of point in a cube and the accurate         evaluation of integrals          zh  vychisl  mat  i mat  phys   7 784-802  1967       args          dimension  int   the dimensionality of the sequence to be draw         scramble  bool  optional   set this to ``true`` will produce                                    scramble sobol sequence  scramble be                                    capable of produce better sobol                                    sequence  default  ``false``          seed  int  optional   this be the seed for the scramble  the seed                               of the random number generator be set to this                                if specify  otherwise  it use a random seed                                default  ``none``      examples            >>> soboleng = torch quasirandom sobolengine dimension=5          >>> soboleng draw 3          tensor   0 5000  0 5000  0 5000  0 5000  0 5000                    0 7500  0 2500  0 7500  0 2500  0 7500                    0 2500  0 7500  0 2500  0 7500  0 2500                maxbit = 30     maxdim = 21201      def   init   self  dimension  scramble=false  seed=none           if dimension > self maxdim or dimension < 1              raise valueerror string                              f for sobolengine be  1   self maxdim              self seed = seed         self scramble = scramble         self dimension = dimension          cpu = torch device string           self sobolstate = torch zero dimension  self maxbit  device=cpu  dtype=torch long          torch  sobol engine initialize state  self sobolstate  self dimension           if not self scramble              self shift = torch zero self dimension  device=cpu  dtype=torch long          else              self  scramble            self quasi = self shift clone memory format=torch contiguous format          self  first point =  self quasi / 2    self maxbit  reshape 1  -1          self num generate = 0      def draw self  n  int = 1  out  optional torch tensor  = none               dtype  torch dtype = torch float32  -> torch tensor          r            function to draw a sequence of  attr `n` point from a sobol sequence          note that the sample be dependent on the previous sample  the size         of the result be  math ` n  dimension `           args              n  int  optional   the length of sequence of point to draw                                 default  1             out  tensor  optional   the output tensor             dtype   class `torch dtype`  optional   the desire data type of the                                                     return tensor                                                      default  ``torch float32``                     if self num generate == 0              if n == 1                  result = self  first point to dtype              else                  result  self quasi = torch  sobol engine draw                      self quasi  n - 1  self sobolstate  self dimension  self num generate  dtype=dtype                                    result = torch cat  self  first point  result   dim=-2          else              result  self quasi = torch  sobol engine draw                  self quasi  n  self sobolstate  self dimension  self num generate - 1  dtype=dtype                         self num generate  = n          if out be not none              out resize as  result  copy  result              return out          return result      def draw base2 self  m  int  out  optional torch tensor  = none                     dtype  torch dtype = torch float32  -> torch tensor          r            function to draw a sequence of  attr `2  m` point from a sobol sequence          note that the sample be dependent on the previous sample  the size         of the result be  math ` 2  m  dimension `           args              m  int   the  base2  exponent of the number of point to draw              out  tensor  optional   the output tensor             dtype   class `torch dtype`  optional   the desire data type of the                                                     return tensor                                                      default  ``torch float32``                     n = 2    m         total n = self num generate   n         if not  total n    total n - 1  == 0               raise valueerror string                              string                              string                              string                              string                               format self num generate  m  total n           return self draw n=n  out=out  dtype=dtype       def reset self           r            function to reset the ``sobolengine`` to base state                      self quasi copy  self shift          self num generate = 0         return self      def fast forward self  n           r            function to fast-forward the state of the ``sobolengine`` by          attr `n` step  this be equivalent to draw  attr `n` sample         without use the sample           args              n  int   the number of step to fast-forward by                      if self num generate == 0              torch  sobol engine ff  self quasi  n - 1  self sobolstate  self dimension  self num generate          else              torch  sobol engine ff  self quasi  n  self sobolstate  self dimension  self num generate - 1          self num generate  = n         return self      def  scramble self           g  optional torch generator  = none         if self seed be not none              g = torch generator               g manual seed self seed           cpu = torch device string                    shift ints = torch randint 2   self dimension  self maxbit   device=cpu  generator=g          self shift = torch mv shift ints  torch pow 2  torch arange 0  self maxbit  device=cpu                      ltm dim =  self dimension  self maxbit  self maxbit          ltm = torch randint 2  ltm dim  device=cpu  generator=g  tril            torch  sobol engine scramble  self sobolstate  ltm  self dimension       def   repr   self           fmt string =  f dimension= self dimension            if self scramble              fmt string  =  string          if self seed be not none              fmt string  =  f seed= self seed            return self   class     name     string   string join fmt string    string 
def save obj  f  union str  os pathlike  binaryio  io bytes             pickle module=pickle  pickle protocol=default protocol   use new zipfile serialization=true  -> none                           string      check dill version pickle module       with  open file like f  string  as open file          if  use new zipfile serialization              with  open zipfile writer open file  as open zipfile                   save obj  open zipfile  pickle module  pickle protocol                  return          legacy save obj  open file  pickle module  pickle protocol  
def load f  map location=none  pickle module=pickle    pickle load args                            string      check dill version pickle module       if string not in pickle load args key            pickle load args string  = string      with  open file like f  string  as open file          if  be zipfile open file                                                      orig position = open file tell               with  open zipfile reader open file  as open zipfile                  if  be torchscript zip open zipfile                       warn warn string                                   string                                   string  userwarning                      open file seek orig position                      return torch jit load open file                  return  load open zipfile  map location  pickle module    pickle load args          return  legacy load open file  map location  pickle module    pickle load args  
class no grad  decoratorcontextmanager       r   context-manager that disable gradient calculation       disable gradient calculation be useful for inference  when you be sure     that you will not call  meth `tensor backward  `  it will reduce memory     consumption for computations that would otherwise have `requires grad=true`       in this mode  the result of every computation will have     `requires grad=false`  even when the input have `requires grad=true`       this context manager be thread local  it will not affect computation     in other thread       also function as a decorator   make sure to instantiate with parenthesis           note           no-grad be one of several mechanisms that can enable or         disable gradients locally see  ref `locally-disable-grad-doc` for         more information on how they compare       example            >>> x = torch tensor  1   require grad=true          >>> with torch no grad                  y = x   2         >>> y require grad         false         >>>  torch no grad               def doubler x                   return x   2         >>> z = doubler x          >>> z require grad         false             def   init   self           if not torch  jit internal be script                super     init             self prev = false      def   enter   self           self prev = torch be grad enable           torch set grad enable false       def   exit   self  exc type  any  exc value  any  traceback  any  -> none          torch set grad enable self prev  
class enable grad  decoratorcontextmanager       r   context-manager that enable gradient calculation       enable gradient calculation  if it have be disable via  class `~no grad`     or  class `~set grad enabled`       this context manager be thread local  it will not affect computation     in other thread       also function as a decorator   make sure to instantiate with parenthesis           note           enable grad be one of several mechanisms that can enable or         disable gradients locally see  ref `locally-disable-grad-doc` for         more information on how they compare       example            >>> x = torch tensor  1    require grad=true          >>> with torch no grad                  with torch enable grad                    y = x   2         >>> y require grad         true         >>> y backward           >>> x grad         >>>  torch enable grad               def doubler x                   return x   2         >>> with torch no grad                    z = doubler x          >>> z require grad         true              def   enter   self  -> none          self prev = torch be grad enable           torch  c  set grad enable true       def   exit   self  exc type  any  exc value  any  traceback  any  -> none          torch  c  set grad enable self prev  
class set grad enable object       r   context-manager that set gradient calculation to on or off       ``set grad enabled`` will enable or disable grads base on its argument  attr `mode`      it can be use as a context-manager or as a function       this context manager be thread local  it will not affect computation     in other thread       args          mode  bool   flag whether to enable grad  ``true``   or disable                       ``false``   this can be use to conditionally enable                      gradients          note           set grad enable be one of several mechanisms that can enable or         disable gradients locally see  ref `locally-disable-grad-doc` for         more information on how they compare       example            >>> x = torch tensor  1   require grad=true          >>> be train = false         >>> with torch set grad enable be train                 y = x   2         >>> y require grad         false         >>> torch set grad enable true          >>> y = x   2         >>> y require grad         true         >>> torch set grad enable false          >>> y = x   2         >>> y require grad         false               def   init   self  mode  bool  -> none          self prev = torch be grad enable           torch  c  set grad enable mode       def   enter   self  -> none          pass      def   exit   self  exc type  any  exc value  any  traceback  any  -> none          torch  c  set grad enable self prev  
class inference mode  decoratorcontextmanager       r   context-manager that enable or disable inference mode      inferencemode be a new context manager analogous to  class `~no grad`     to be use when you be certain your operations will have no interactions     with autograd  e g   model train   code run under this mode get better     performance by disable view track and version counter bump       this context manager be thread local  it will not affect computation     in other thread       also function as a decorator   make sure to instantiate with parenthesis           note           inference mode be one of several mechanisms that can enable or         disable gradients locally see  ref `locally-disable-grad-doc` for         more information on how they compare       args          mode  bool   flag whether to enable or disable inference mode      example           >>> import torch         >>> x = torch ones 1  2  3  require grad=true          >>> with torch inference mode                  y = x   x         >>> y require grad         false         >>> y  version         traceback  most recent call last           file  <stdin>   line 1  in <module>         runtimeerror  inference tensors do not track version counter          >>>  torch inference mode               def func x                 return x   x         >>> out = func x          >>> out require grad         false              def   init   self  mode=true           if not torch  jit internal be script                super     init                               self  inference mode raii guard = none         self mode = mode      def   enter   self           self  inference mode raii guard = torch  c  inferencemode self mode       def   exit   self  exc type  any  exc value  any  traceback  any  -> none          del self  inference mode raii guard 
def norm input  p=string  dim=none  keepdim=false  out=none  dtype=none         r   return the matrix norm or vector norm of a give tensor          warn            torch norm be deprecate and may be remove in a future pytorch release           use  func `torch linalg norm`  instead  or  func `torch linalg vector norm`         when compute vector norms and  func `torch linalg matrix norm` when         compute matrix norms  note  however  the signature for these function         be slightly different than the signature for torch norm       args          input  tensor   the input tensor  its data type must be either a float             point or complex type  for complex input  the norm be calculate use the             absolute value of each element  if the input be complex and neither              attr `dtype` nor  attr `out` be specify  the result s data type will             be the correspond float point type  e g  float if  attr `input` be             complexfloat            p  int  float  inf  -inf   fro    nuc   optional   the order of norm  default  `` fro ``             the follow norms can be calculate               ======  ==============  ==========================             ord     matrix norm     vector norm             ======  ==============  ==========================              fro    frobenius norm  --              nuc    nuclear norm    --             number  --              sum abs x   ord    1 /ord              ======  ==============  ==========================              the vector norm can be calculate across any number of dimension              the correspond dimension of  attr `input` be flatten into             one dimension  and the norm be calculate on the flatten             dimension               frobenius norm produce the same result as ``p=2`` in all case             except when  attr `dim` be a list of three or more dim  in which             case frobenius norm throw an error               nuclear norm can only be calculate across exactly two dimension           dim  int  tuple of ints  list of ints  optional               specify which dimension or dimension of  attr `input` to             calculate the norm across  if  attr `dim` be ``none``  the norm will             be calculate across all dimension of  attr `input`  if the norm             type indicate by  attr `p` do not support the specify number of             dimension  an error will occur          keepdim  bool  optional   whether the output tensors have  attr `dim`             retain or not  ignore if  attr `dim` = ``none`` and              attr `out` = ``none``  default  ``false``         out  tensor  optional   the output tensor  ignore if              attr `dim` = ``none`` and  attr `out` = ``none``          dtype   class `torch dtype`  optional   the desire data type of             return tensor  if specify  the input tensor be cast to              attr  dtype  while perform the operation  default  none          note           even though ``p= fro `` support any number of dimension  the true         mathematical definition of frobenius norm only apply to tensors with         exactly two dimension   func `torch linalg norm` with ``ord= fro `` align         with the mathematical definition  since it can only be apply across         exactly two dimension       example            >>> import torch         >>> a = torch arange 9  dtype= torch float  - 4         >>> b = a reshape  3  3           >>> torch norm a          tensor 7 7460          >>> torch norm b          tensor 7 7460          >>> torch norm a  float  inf            tensor 4           >>> torch norm b  float  inf            tensor 4           >>> c = torch tensor    1  2  3   -1  1  4     dtype= torch float          >>> torch norm c  dim=0          tensor  1 4142  2 2361  5 0000           >>> torch norm c  dim=1          tensor  3 7417  4 2426           >>> torch norm c  p=1  dim=1          tensor  6   6            >>> d = torch arange 8  dtype= torch float  reshape 2 2 2          >>> torch norm d  dim= 1 2           tensor   3 7417  11 2250           >>> torch norm d 0          torch norm d 1                  tensor 3 7417   tensor 11 2250                if have torch function unary input           return handle torch function              norm   input    input  p=p  dim=dim  keepdim=keepdim  out=out  dtype=dtype       ndim = input dim             if dim be none and out be none and dtype be none and p be not none          if isinstance p  str               if p == string                  return  vf frobenius norm input  dim=    keepdim=keepdim          if not isinstance p  str                dim =  i for i in range ndim                 return  vf norm input  p  dim= dim  keepdim=keepdim                        if dim be not none          if isinstance dim  int                dim =  dim          else               dim = dim     else           dim = none        if isinstance p  str           if p == string              if dtype be not none                  raise valueerror string               if  dim be none                   dim = list range ndim               if out be none                  return  vf frobenius norm input   dim  keepdim=keepdim              else                  return  vf frobenius norm input   dim  keepdim=keepdim  out=out          elif p == string              if dtype be not none                  raise valueerror string              if  dim be none                  if out be none                      return  vf nuclear norm input  keepdim=keepdim                  else                      return  vf nuclear norm input  keepdim=keepdim  out=out              else                  if out be none                      return  vf nuclear norm input   dim  keepdim=keepdim                  else                      return  vf nuclear norm input   dim  keepdim=keepdim  out=out          raise runtimeerror f only valid string value be  fro  and  nuc   find  p        else          if  dim be none               dim = list range ndim            if out be none              if dtype be none                  return  vf norm input  p   dim  keepdim=keepdim                else                  return  vf norm input  p   dim  keepdim=keepdim  dtype=dtype            else              if dtype be none                  return  vf norm input  p   dim  keepdim=keepdim  out=out                else                  return  vf norm input  p   dim  keepdim=keepdim  dtype=dtype  out=out    
def fn  args    kwargs       dispatch flag = false     if arg name in kwargs          dispatch flag = kwargs arg name      elif arg index < len args           dispatch flag = args arg index       if dispatch flag          return if true  args    kwargs      else          return if false  args    kwargs  
def fn  args    kwargs       dispatch flag = false     if arg name in kwargs          dispatch flag = kwargs arg name      elif arg index < len args           dispatch flag = args arg index       if dispatch flag          return if true  args    kwargs      else          return if false  args    kwargs  
def stft input  tensor  n fft  int  hop length  optional int  = none           win length  optional int  = none  window  optional tensor  = none           center  bool = true  pad mode  str = string  normalize  bool = false           onesided  optional bool  = none           return complex  optional bool  = none  -> tensor      r   short-time fourier transform  stft           warn           from version 1 8 0   attr `return complex` must always be give         explicitly for real input and `return complex=false` have be         deprecate  strongly prefer `return complex=true` as in a future         pytorch release  this function will only return complex tensors           note that  func `torch view as real` can be use to recover a real         tensor with an extra last dimension for real and imaginary components       the stft compute the fourier transform of short overlap windows of the     input  this give frequency components of the signal as they change over     time  the interface of this function be model after the librosa  stft function           librosa  https //librosa org/doc/latest/generated/librosa stft html      ignore the optional batch dimension  this method compute the follow     expression          math           x \omega  m  = \sum  k = 0   \text win\ length-1                                \text window  k \ \text input  m \times \text hop\ length    k \                               \exp\left - j \frac 2 \pi \cdot \omega k  \text win\ length  \right        where  math `m` be the index of the slide window  and  math `\omega` be     the frequency  math `0 \leq \omega < \text n\ fft ` for ``onesided=false``      or  math `0 \leq \omega < \lfloor \text n\ fft  / 2 \rfloor   1` for ``onesided=true``          attr `input` must be either a 1-d time sequence or a 2-d batch of time       sequence         if  attr `hop length` be ``none``  default   it be treat as equal to       ``floor n fft / 4 ``         if  attr `win length` be ``none``  default   it be treat as equal to        attr `n fft`          attr `window` can be a 1-d tensor of size  attr `win length`  e g   from        meth `torch hann window`  if  attr `window` be ``none``  default   it be       treat as if have  math `1` everywhere in the window  if        math `\text win\ length  < \text n\ fft `   attr `window` will be pad on       both side to length  attr `n fft` before be apply         if  attr `center` be ``true``  default    attr `input` will be pad on       both side so that the  math `t`-th frame be center at time        math `t \times \text hop\ length `  otherwise  the  math `t`-th frame       begin at time   math `t \times \text hop\ length `          attr `pad mode` determine the pad method use on  attr `input` when        attr `center` be ``true``  see  meth `torch nn functional pad` for       all available options  default be `` reflect ``         if  attr `onesided` be ``true``  default for real input   only value for        math `\omega` in  math `\left 0  1  2  \dots  \left\lfloor       \frac \text n\ fft   2  \right\rfloor   1\right ` be return because       the real-to-complex fourier transform satisfy the conjugate symmetry        i e    math `x m  \omega  = x m  \text n\ fft  - \omega   `        note if the input or window tensors be complex  then  attr `onesided`       output be not possible         if  attr `normalized` be ``true``  default be ``false``   the function       return the normalize stft result  i e   multiply by  math ` \text frame\ length    -0 5 `         if  attr `return complex` be ``true``  default if input be complex   the       return be a ``input dim     1`` dimensional complex tensor  if ``false``        the output be a ``input dim     2`` dimensional real tensor where the last       dimension represent the real and imaginary components       return either a complex tensor of size  math `   \times n \times t ` if      attr `return complex` be true  or a real tensor of size  math `   \times n     \times t \times 2 `  where  math ` ` be the optional batch size of      attr `input`   math `n` be the number of frequencies where stft be apply     and  math `t` be the total number of frame use          warn         this function change signature at version 0 4 1  call with the       previous signature may cause error or return incorrect result       args          input  tensor   the input tensor         n fft  int   size of fourier transform         hop length  int  optional   the distance between neighbor slide window             frame  default  ``none``  treat as equal to ``floor n fft / 4 ``          win length  int  optional   the size of window frame and stft filter              default  ``none``   treat as equal to  attr `n fft`          window  tensor  optional   the optional window function              default  ``none``  treat as window of all  math `1` s          center  bool  optional   whether to pad  attr `input` on both side so             that the  math `t`-th frame be center at time  math `t \times \text hop\ length `              default  ``true``         pad mode  string  optional   control the pad method use when              attr `center` be ``true``  default  `` reflect ``         normalize  bool  optional   control whether to return the normalize stft result              default  ``false``         onesided  bool  optional   control whether to return half of result to             avoid redundancy for real input              default  ``true`` for real  attr `input` and  attr `window`  ``false`` otherwise          return complex  bool  optional   whether to return a complex tensor  or             a real tensor with an extra last dimension for the real and             imaginary components       return          tensor  a tensor contain the stft result with shape describe above              if have torch function unary input           return handle torch function              stft   input    input  n fft  hop length=hop length  win length=win length              window=window  center=center  pad mode=pad mode  normalized=normalized              onesided=onesided  return complex=return complex                if center          signal dim = input dim           extend shape =  1     3 - signal dim    list input size            pad = int n fft // 2          input = f pad input view extend shape    pad  pad   pad mode          input = input view input shape -signal dim        return  vf stft input  n fft  hop length  win length  window                        normalize  onesided  return complex  
def istft input  tensor  n fft  int  hop length  optional int  = none            win length  optional int  = none  window  optional tensor  = none            center  bool = true  normalize  bool = false            onesided  optional bool  = none  length  optional int  = none            return complex  bool = false  -> tensor      r   inverse short time fourier transform  this be expect to be the inverse of  func `~torch stft`      it have the same parameters    additional optional parameter of  attr `length`  and it should return the     least square estimation of the original signal  the algorithm will check use the nola condition       nonzero overlap        important consideration in the parameters  attr `window` and  attr `center` so that the envelop     create by the summation of all the windows be never zero at certain point in time  specifically       math `\sum  t=-\infty   \infty   w  2 n-t\times hop\ length  \cancel =  0`       since  func `~torch stft` discard elements at the end of the signal if they do not fit in a frame      ``istft`` may return a shorter signal than the original signal  can occur if  attr `center` be false     since the signal isn t pad        if  attr `center` be ``true``  then there will be pad e g  `` constant ``  `` reflect ``  etc      leave pad can be trim off exactly because they can be calculate but right pad cannot be     calculate without additional information       example  suppose the last window be      `` 17  18  0  0  0 `` vs `` 18  0  0  0  0 ``      the  attr `n fft`   attr `hop length`   attr `win length` be all the same which prevent the calculation     of right pad  these additional value could be zero or a reflection of the signal so provide      attr `length` could be useful  if  attr `length` be ``none`` then pad will be aggressively remove      some loss of signal         1  d  w  griffin and j  s  lim   signal estimation from modify short-time fourier transform       ieee trans  assp  vol 32  no 2  pp 236-243  apr  1984       args          input  tensor   the input tensor  expect to be output of  func `~torch stft`              can either be complex  ``channel``  ``fft size``  ``n frame``   or real              ``channel``  ``fft size``  ``n frame``  2  where the ``channel``             dimension be optional                  deprecate   1 8 0                real input be deprecate  use complex input as return by                ``stft      return complex=true `` instead          n fft  int   size of fourier transform         hop length  optional int    the distance between neighbor slide window frame               default  ``n fft // 4``          win length  optional int    the size of window frame and stft filter   default  ``n fft``          window  optional torch tensor    the optional window function               default  ``torch ones win length ``          center  bool   whether  attr `input` be pad on both side so that the  math `t`-th frame be             center at time  math `t \times \text hop\ length `               default  ``true``          normalize  bool   whether the stft be normalize   default  ``false``          onesided  optional bool    whether the stft be onesided               default  ``true`` if ``n fft  = fft size`` in the input size          length  optional int    the amount to trim the signal by  i e  the             original signal length    default  whole signal          return complex  optional bool                whether the output should be complex  or if the input should be             assume to derive from a real signal and window              note that this be incompatible with ``onesided=true``               default  ``false``       return          tensor  least square estimation of the original signal of size       signal length              if have torch function unary input           return handle torch function              istft   input    input  n fft  hop length=hop length  win length=win length              window=window  center=center  normalized=normalized  onesided=onesided              length=length  return complex=return complex       return  vf istft input  n fft  hop length  win length  window  center                         normalize  onesided  length  return complex  
def atleast 1d  tensors       r        return a 1-dimensional view of each input tensor with zero dimension      input tensors with one or more dimension be return as-is       args          input  tensor or list of tensors       return          output  tensor or tuple of tensors       example            >>> x = torch randn 2          >>> x         tensor  1 4584  0 7583           >>> torch atleast 1d x          tensor  1 4584  0 7583           >>> x = torch tensor 1           >>> x         tensor 1           >>> torch atleast 1d x          tensor  1            >>> x = torch tensor 0 5          >>> y = torch tensor 1           >>> torch atleast 1d  x y            tensor  0 5000    tensor  1                 if have torch function tensors           return handle torch function atleast 1d  tensors   tensors      if len tensors  == 1          tensors = tensors 0      return  vf atleast 1d tensors    
def atleast 2d  tensors       r        return a 2-dimensional view of each input tensor with zero dimension      input tensors with two or more dimension be return as-is       args          input  tensor or list of tensors       return          output  tensor or tuple of tensors       example            >>> x = torch tensor 1           >>> x         tensor 1           >>> torch atleast 2d x          tensor   1             >>> x = torch randn 2 2          >>> x         tensor   2 2086  2 5165                    0 1757  0 5194            >>> torch atleast 2d x          tensor   2 2086  2 5165                    0 1757  0 5194            >>> x = torch tensor 0 5          >>> y = torch tensor 1           >>> torch atleast 2d  x y            tensor   0 5000     tensor   1                  if have torch function tensors           return handle torch function atleast 2d  tensors   tensors      if len tensors  == 1          tensors = tensors 0      return  vf atleast 2d tensors    
def atleast 3d  tensors       r        return a 3-dimensional view of each input tensor with zero dimension      input tensors with three or more dimension be return as-is       args          input  tensor or list of tensors       return          output  tensor or tuple of tensors       example           >>> x = torch tensor 0 5          >>> x         tensor 0 5000          >>> torch atleast 3d x          tensor    0 5000             >>> y = torch randn 2 2          >>> y         tensor   -0 8079   0 7460                    -1 1647   1 4734            >>> torch atleast 3d y          tensor    -0 8079                     0 7460                    <blankline>                   -1 1647                     1 4734             >>> x = torch randn 1 1 1          >>> x         tensor    -1 5689             >>> torch atleast 3d x          tensor    -1 5689             >>> x = torch tensor 0 5          >>> y = torch tensor 1           >>> torch atleast 3d  x y            tensor    0 5000      tensor    1                   if have torch function tensors           return handle torch function atleast 3d  tensors   tensors      if len tensors  == 1          tensors = tensors 0      return  vf atleast 3d tensors    
def block diag  tensors       string     if have torch function tensors           return handle torch function block diag  tensors   tensors      return torch  c  variablefunctions block diag tensors    
def broadcast tensors  tensors       r   broadcast tensors  tensors  -> list of tensors      broadcast the give tensors accord to  ref `broadcasting-semantics`       args           tensors  any number of tensors of the same type         warn            more than one element of a broadcast tensor may refer to a single         memory location  as a result  in-place operations  especially ones that         be vectorized  may result in incorrect behavior  if you need to write         to the tensors  please clone them first       example            >>> x = torch arange 3  view 1  3          >>> y = torch arange 2  view 2  1          >>> a  b = torch broadcast tensors x  y          >>> a size           torch size  2  3           >>> a         tensor   0  1  2                    0  1  2                if have torch function tensors           return handle torch function broadcast tensors  tensors   tensors      return  vf broadcast tensors tensors    
def broadcast shape  shape       r   broadcast shape  shape  -> size      similar to  func `broadcast tensors` but for shape       this be equivalent to     ``torch broadcast tensors  map torch empty  shape   0  shape``     but avoid the need create to intermediate tensors  this be useful for     broadcast tensors of common batch shape but different rightmost shape      e g  to broadcast mean vectors with covariance matrices       example            >>> torch broadcast shape  2     3  1    1  1  1           torch size  1  3  2        args          \ shape  torch size   shape of tensors       return          shape  torch size   a shape compatible with all input shape       raise          runtimeerror  if shape be incompatible                   with torch no grad            scalar = torch zero     device=string          tensors =  scalar expand shape  for shape in shape          tensors = broadcast tensors  tensors          return tensors 0  shape 
def cartesian prod  tensors       string     if have torch function tensors           return handle torch function cartesian prod  tensors   tensors      return  vf cartesian prod tensors    
def cdist x1  x2  p=2   compute mode=string            r   compute batch the p-norm distance between each pair of the two collections of row vectors       args          x1  tensor   input tensor of shape  math `b \times p \times m`          x2  tensor   input tensor of shape  math `b \times r \times m`          p  p value for the p-norm distance to calculate between each vector pair              math `\in  0  \infty `          compute mode               use mm for euclid dist if necessary  - will use matrix multiplication approach to calculate             euclidean distance  p = 2  if p > 25 or r > 25              use mm for euclid dist  - will always use matrix multiplication approach to calculate             euclidean distance  p = 2               donot use mm for euclid dist  - will never use matrix multiplication approach to calculate             euclidean distance  p = 2              default  use mm for euclid dist if necessary       if x1 have shape  math `b \times p \times m` and x2 have shape  math `b \times r \times m` then the     output will have shape  math `b \times p \times r`       this function be equivalent to `scipy spatial distance cdist input  minkowski   p=p `     if  math `p \in  0  \infty `  when  math `p = 0` it be equivalent to     `scipy spatial distance cdist input   ham     m`  when  math `p = \infty`  the closest     scipy function be `scipy spatial distance cdist xn  lambda x  y  np abs x - y  max   `       example           >>> a = torch tensor   0 9041   0 0196    -0 3108  -2 4423    -0 4821   1 059            >>> a         tensor    0 9041   0 0196                    -0 3108  -2 4423                    -0 4821   1 0590            >>> b = torch tensor   -2 1763  -0 4713    -0 6986   1 3702            >>> b         tensor   -2 1763  -0 4713                    -0 6986   1 3702            >>> torch cdist a  b  p=2          tensor   3 1193  2 0959                    2 7138  3 8322                    2 2830  0 3791                if have torch function variadic x1  x2           return handle torch function              cdist   x1  x2   x1  x2  p=p  compute mode=compute mode      if compute mode == string          return  vf cdist x1  x2  p  none        elif compute mode == string          return  vf cdist x1  x2  p  1        elif compute mode == string          return  vf cdist x1  x2  p  2        else          raise valueerror f  compute mode  be not a valid value for compute mode   
def einsum equation   operands       r   einsum equation   operands  -> tensor      sum the product of the elements of the input  attr `operands` along dimension specify use a notation     base on the einstein summation convention       einsum allow compute many common multi-dimensional linear algebraic array operations by represent them     in a short-hand format base on the einstein summation convention  give by  attr `equation`  the detail of     this format be describe below  but the general idea be to label every dimension of the input  attr `operands`     with some subscript and define which subscripts be part of the output  the output be then compute by sum     the product of the elements of the  attr `operands` along the dimension whose subscripts be not part of the     output  for example  matrix multiplication can be compute use einsum as `torch einsum  ij jk->ik   a  b `      here  j be the summation subscript and i and k the output subscripts  see section below for more detail on why        equation           the  attr `equation` string specify the subscripts  lower case letter `  a    z  `  for each dimension of         the input  attr `operands` in the same order as the dimension  separate subcripts for each operand by a         comma        e g  ` ij jk ` specify subscripts for two 2d operands  the dimension label with the same subscript         must be broadcastable  that be  their size must either match or be `1`  the exception be if a subscript be         repeat for the same input operand  in which case the dimension label with this subscript for this operand         must match in size and the operand will be replace by its diagonal along these dimension  the subscripts that         appear exactly once in the  attr `equation` will be part of the output  sort in increase alphabetical order          the output be compute by multiply the input  attr `operands` element-wise  with their dimension align base         on the subscripts  and then sum out the dimension whose subscripts be not part of the output           optionally  the output subscripts can be explicitly define by add an arrow   ->   at the end of the equation         follow by the subscripts for the output  for instance  the follow equation compute the transpose of a         matrix multiplication   ij jk->ki   the output subscripts must appear at least once for some input operand and         at most once for the output           ellipsis         can be use in place of subscripts to broadcast the dimension cover by the ellipsis          each input operand may contain at most one ellipsis which will cover the dimension not cover by subscripts          e g  for an input operand with 5 dimension  the ellipsis in the equation ` ab   c ` cover the third and fourth         dimension  the ellipsis do not need to cover the same number of dimension across the  attr `operands` but the          shape  of the ellipsis  the size of the dimension cover by them  must broadcast together  if the output be not         explicitly define with the arrow   ->   notation  the ellipsis will come first in the output  left-most dimension           before the subscript label that appear exactly once for the input operands  e g  the follow equation implement         batch matrix multiplication `    ij    jk `           a few final note  the equation may contain whitespaces between the different elements  subscripts  ellipsis          arrow and comma  but something like `       ` be not valid  an empty string `  ` be valid for scalar operands          note            ``torch einsum`` handle ellipsis         differently from numpy in that it allow dimension         cover by the ellipsis to be sum over  that be  ellipsis be not require to be part of the output          note            this function do not optimize the give expression  so a different formula for the same computation may         run faster or consume less memory  project like opt einsum  https //optimized-einsum readthedocs io/en/stable/          can optimize the formula for you       args          equation  string   the subscripts for the einstein summation          operands  tensor   the operands to compute the einstein sum of       examples              trace         >>> torch einsum  ii   torch randn 4  4           tensor -1 2104             diagonal         >>> torch einsum  ii->i   torch randn 4  4           tensor  -0 1034   0 7952  -0 2433   0 4545              outer product         >>> x = torch randn 5          >>> y = torch randn 4          >>> torch einsum  i j->ij   x  y          tensor    0 1156  -0 2897  -0 3918   0 4963                    -0 3744   0 9381   1 2685  -1 6070                     0 7208  -1 8058  -2 4419   3 0936                     0 1713  -0 4291  -0 5802   0 7350                     0 5704  -1 4290  -1 9323   2 4480               batch matrix multiplication         >>> as = torch randn 3 2 5          >>> bs = torch randn 3 5 4          >>> torch einsum  bij bjk->bik   as  bs          tensor    -1 0564  -1 5904   3 2023   3 1271                    -1 6706  -0 8097  -0 8025  -2 1183                        4 2239   0 3107  -0 5756  -0 2354                    -1 4558  -0 3460   1 5087  -0 8530                        2 8153   1 8787  -4 3839  -1 2112                     0 3728  -2 1131   0 0921   0 8305                batch permute         >>> a = torch randn 2  3  4  5          >>> torch einsum     ij->   ji   a  shape         torch size  2  3  5  4              equivalent to torch nn functional bilinear         >>> a = torch randn 3 5 4          >>> l = torch randn 2 5          >>> r = torch randn 2 4          >>> torch einsum  bn anm bm->ba   l  a  r          tensor   -0 3430  -5 2405   0 4494                     0 3311   5 5201  -3 0356                if have torch function operands           return handle torch function einsum  operands  equation   operands      if len operands  == 1 and isinstance operands 0    list  tuple                      operands = operands 0                            return einsum equation    operands       return  vf einsum equation  operands    
def meshgrid  tensors       r   take  math `n` tensors  each of which can be either scalar or 1-dimensional     vector  and create  math `n` n-dimensional grids  where the  math `i`  sup `th` grid be define by     expand the  math `i`  sup `th` input over dimension define by other input       args          tensors  list of tensor   list of scalars or 1 dimensional tensors  scalars will be             treat as tensors of size  math ` 1  ` automatically      return          seq  sequence of tensors   if the input have  math `k` tensors of size          math ` n 1     n 2    \ldots    n k  `  then the output would also have  math `k` tensors          where all tensors be of size  math ` n 1  n 2  \ldots   n k `       example            >>> x = torch tensor  1  2  3           >>> y = torch tensor  4  5  6           >>> grid x  grid y = torch meshgrid x  y          >>> grid x         tensor   1  1  1                    2  2  2                    3  3  3            >>> grid y         tensor   4  5  6                    4  5  6                    4  5  6                return  meshgrid  tensors  
def tensordot a  b  dims=2  out  optional torch tensor  = none         r   return a contraction of a and b over multiple dimension        attr `tensordot` implement a generalize matrix product       args        a  tensor   leave tensor to contract       b  tensor   right tensor to contract       dim  int or tuple list int   list int   or list list int   contain two list or tensor   number of dimension to          contract or explicit list of dimension for  attr `a` and           attr `b` respectively      when call with a non-negative integer argument  attr `dims` =  math `d`  and     the number of dimension of  attr `a` and  attr `b` be  math `m` and  math `n`      respectively   func `~torch tensordot` compute         math           r  i 0     i  m-d   i d     i n            = \sum  k 0     k  d-1   a  i 0     i  m-d  k 0     k  d-1   \times b  k 0     k  d-1   i d     i n        when call with  attr `dims` of the list form  the give dimension will be contract     in place of the last  math `d` of  attr `a` and the first  math `d` of  math `b`  the size     in these dimension must match  but  func `~torch tensordot` will deal with broadcast     dimension       examples            >>> a = torch arange 60   reshape 3  4  5          >>> b = torch arange 24   reshape 4  3  2          >>> torch tensordot a  b  dims=  1  0    0  1            tensor   4400   4730                     4532   4874                     4664   5018                     4796   5162                     4928   5306              >>> a = torch randn 3  4  5  device= cuda           >>> b = torch randn 4  5  6  device= cuda           >>> c = torch tensordot a  b  dims=2  cpu           tensor    8 3504  -2 5436   6 2922   2 7556  -1 0732   3 2741                     3 3161   0 0704   5 0187  -0 4079  -4 3126   4 8744                     0 8223   3 9445   3 2168  -0 2400   3 4117   1 7780             >>> a = torch randn 3  5  4  6          >>> b = torch randn 6  4  5  3          >>> torch tensordot a  b  dims=  2  1  3    1  2  0            tensor     7 7193   -2 4867  -10 3204                      1 5513  -14 4737   -6 5113                     -0 2850    4 2573   -3 5997                if have torch function variadic a  b           return handle torch function tensordot   a  b   a  b  dims=dims       dim a  list int  =        dim b  list int  =         if isinstance dim   tuple  list            dim a  dim b = dim      if isinstance dim  torch tensor           num elements = dim numel           if num elements > 1              assert dim size   0  == 2             dim a = torch jit annotate list int   dim 0  tolist                dim b = torch jit annotate list int   dim 1  tolist            else              dim val = int dim item                if dim val < 0                  raise runtimeerror f tensordot expect dim >= 0  but get dims= dim                dim a = list range -dims val  0               dim b = list range dim val        if isinstance dim  int           if dim < 0              raise runtimeerror f tensordot expect dim >= 0  but get dims= dim            dim a = list range -dims  0           dim b = list range dim        if len dim a  == 0 or len dim b  == 0          raise runtimeerror f unsupported input to tensordot  get dims= dim         if out be none          return  vf tensordot a  b  dim a  dim b        else          return  vf tensordot a  b  dim a  dim b  out=out    
def chain matmul  matrices  out=none       r   return the matrix product of the  math `n` 2-d tensors  this product be efficiently compute     use the matrix chain order algorithm which select the order in which incur the lowest cost in term     of arithmetic operations  ` clrs `    note that since this be a function to compute the product   math `n`     need to be greater than or equal to 2  if equal to 2 then a trivial matrix-matrix product be return      if  math `n` be 1  then this be a no-op - the original matrix be return as be          warn             func `torch chain matmul` be deprecate and will be remove in a future pytorch release          use  func `torch linalg multi dot` instead  which accept a list of two or more tensors         rather than multiple arguments       args          matrices  tensors      a sequence of 2 or more 2-d tensors whose product be to be determine          out  tensor  optional   the output tensor  ignore if  attr `out` = ``none``       return          tensor  if the  math `i  th ` tensor be of dimension  math `p  i  \times p  i   1 `  then the product         would be of dimension  math `p  1  \times p  n   1 `       example            >>> a = torch randn 3  4          >>> b = torch randn 4  5          >>> c = torch randn 5  6          >>> d = torch randn 6  7          >>> torch chain matmul a  b  c  d          tensor    -2 3375   -3 9790   -4 1119   -6 6577    9 5609  -11 5095   -3 2614                     21 4038    3 3378   -8 4982   -5 2457  -10 2561   -2 4684    2 7163                     -0 9647   -5 8917   -2 3213   -5 2284   12 8615  -12 2816   -2 5095             ` clrs `  https //mitpress mit edu/books/introduction-algorithms-third-edition             if have torch function matrices           return handle torch function chain matmul  matrices   matrices       if out be none          return  vf chain matmul matrices        else          return  vf chain matmul matrices  out=out    
def fn  args    kwargs       dispatch flag = false     if arg name in kwargs          dispatch flag = kwargs arg name      elif arg index < len args           dispatch flag = args arg index       if dispatch flag          return if true  args    kwargs      else          return if false  args    kwargs  
def svd lowrank a  tensor  q  optional int  = 6  niter  optional int  = 2                  m  optional tensor  = none  -> tuple tensor  tensor  tensor       r   return the singular value decomposition `` u  s  v `` of a matrix      batch of matrices  or a sparse matrix  math `a` such that      math `a \approx u diag s  v t`  in case  math `m` be give  then     svd be compute for the matrix  math `a - m`          note   the implementation be base on the algorithm 5 1 from               halko et al  2009          note   to obtain repeatable result  reset the seed for the               pseudorandom number generator         note   the input be assume to be a low-rank matrix          note   in general  use the full-rank svd implementation                func `torch linalg svd` for dense matrices due to its 10-fold               higher performance characteristics  the low-rank svd               will be useful for huge sparse matrices that                func `torch linalg svd` cannot handle       args           a  tensor   the input tensor of size  math `    m  n `          q  int  optional   a slightly overestimate rank of a           niter  int  optional   the number of subspace iterations to                                conduct  niter must be a nonnegative                                integer  and default to 2          m  tensor  optional   the input tensor s mean of size                                math `    1  n `       reference           - nathan halko  per-gunnar martinsson  and joel tropp  find           structure with randomness  probabilistic algorithms for           construct approximate matrix decompositions            arxiv 0909 4061  math na  math pr   2009  available at           `arxiv <https //arxiv org/abs/0909 4061>`                 if not torch jit be script            tensor ops =  a  m          if  not set map type  tensor ops   issubset  torch tensor  type none    and have torch function tensor ops                return handle torch function svd lowrank  tensor ops  a  q=q  niter=niter  m=m      return  svd lowrank a  q=q  niter=niter  m=m  
def pca lowrank a  tensor  q  optional int  = none  center  bool = true                  niter  int = 2  -> tuple tensor  tensor  tensor       r   perform linear principal component analysis  pca  on a low-rank     matrix  batch of such matrices  or sparse matrix       this function return a namedtuple `` u  s  v `` which be the     nearly optimal approximation of a singular value decomposition of     a center matrix  math `a` such that  math `a = u diag s  v t`          note   the relation of `` u  s  v `` to pca be as follow                   -  math `a` be a data matrix with ``m`` sample and                   ``n`` feature                  - the  math `v` columns represent the principal directions                  -  math `s    2 /  m - 1 ` contain the eigenvalues of                    math `a t a /  m - 1 ` which be the covariance of                   ``a`` when ``center=true`` be provide                   - ``matmul a  v     k  `` project data to the first k                   principal components         note   different from the standard svd  the size of return               matrices depend on the specify rank and q               value as follow                   -  math `u` be m x q matrix                  -  math `s` be q-vector                  -  math `v` be n x q matrix         note   to obtain repeatable result  reset the seed for the               pseudorandom number generator      args           a  tensor   the input tensor of size  math `    m  n `          q  int  optional   a slightly overestimate rank of                             math `a`  by default  ``q = min 6  m                             n ``           center  bool  optional   if true  center the input tensor                                   otherwise  assume that the input be                                  center           niter  int  optional   the number of subspace iterations to                                conduct  niter must be a nonnegative                                integer  and default to 2       reference            - nathan halko  per-gunnar martinsson  and joel tropp  find           structure with randomness  probabilistic algorithms for           construct approximate matrix decompositions            arxiv 0909 4061  math na  math pr   2009  available at           `arxiv <http //arxiv org/abs/0909 4061>`                  if not torch jit be script            if type a  be not torch tensor and have torch function  a                 return handle torch function pca lowrank   a    a  q=q  center=center  niter=niter        m  n  = a shape -2        if q be none          q = min 6  m  n      elif not  q >= 0 and q <= min m  n            raise valueerror string                          string                           format q  min m  n        if not  niter >= 0           raise valueerror string                           format niter        dtype =  utils get float dtype a       if not center          return  svd lowrank a  q  niter=niter  m=none       if  utils be sparse a           if len a shape   = 2              raise valueerror string          c = torch sparse sum a  dim= -2    / m                  column indices = c indices   0          indices = torch zero 2  len column indices                                 dtype=column indices dtype                                device=column indices device          indices 0  = column indices         c t = torch sparse coo tensor              indices  c value     n  1   dtype=dtype  device=a device           ones m1 t = torch ones a shape  -2     1  m   dtype=dtype  device=a device          m =  utils transpose torch sparse mm c t  ones m1 t           return  svd lowrank a  q  niter=niter  m=m      else          c = a mean dim= -2    keepdim=true          return  svd lowrank a - c  q  niter=niter  m=none  
def lobpcg a  tensor             k  optional int  = none             b  optional tensor  = none             x  optional tensor  = none             n  optional int  = none             ik  optional tensor  = none             niter  optional int  = none             tol  optional float  = none             largest  optional bool  = none             method  optional str  = none             tracker  optional none  = none             ortho iparams  optional dict str  int   = none             ortho fparams  optional dict str  float   = none             ortho bparams  optional dict str  bool   = none              -> tuple tensor  tensor        string      if not torch jit be script            tensor ops =  a  b  x  ik          if  not set map type  tensor ops   issubset  torch tensor  type none    and have torch function tensor ops                return handle torch function                  lobpcg  tensor ops  a  k=k                  b=b  x=x  n=n  ik=ik  niter=niter  tol=tol                  largest=largest  method=method  tracker=tracker                  ortho iparams=ortho iparams                  ortho fparams=ortho fparams                  ortho bparams=ortho bparams       if not torch  jit internal be script            if a require grad or  b be not none and b require grad                                                                                             a sym =  a   a transpose -2  -1   / 2             b sym =  b   b transpose -2  -1   / 2 if  b be not none  else none              return lobpcgautogradfunction apply                  a sym  k  b sym  x  n  ik  niter  tol  largest                  method  tracker  ortho iparams  ortho fparams  ortho bparams                   else          if a require grad or  b be not none and b require grad               raise runtimeerror                  string                 string                 string                    return  lobpcg          a  k  b  x          n  ik  niter  tol  largest  method  tracker          ortho iparams  ortho fparams  ortho bparams       
def compile with cxx11 abi        r   return whether pytorch be build with  glibcxx use cxx11 abi=1        return  c  glibcxx use cxx11 abi 
def use deterministic algorithms mode       r    set whether pytorch operations must use  deterministic      algorithms  that be  algorithms which  give the same input  and when     run on the same software and hardware  always produce the same output      when enable  operations will use deterministic algorithms when available      and if only nondeterministic algorithms be available they will throw a      class `runtimeerror` when call       the follow normally-nondeterministic operations will act     deterministically when ``mode=true``              class `torch nn conv1d` when call on cuda tensor            class `torch nn conv2d` when call on cuda tensor            class `torch nn conv3d` when call on cuda tensor            class `torch nn convtranspose1d` when call on cuda tensor            class `torch nn convtranspose2d` when call on cuda tensor            class `torch nn convtranspose3d` when call on cuda tensor            func `torch bmm` when call on sparse-dense cuda tensors            func `torch tensor   getitem  ` when attempt to differentiate a cpu tensor           and the index be a list of tensors            func `torch tensor index put` with ``accumulate=false``            func `torch tensor index put` with ``accumulate=true`` when call on a cpu           tensor            func `torch tensor put ` with ``accumulate=true`` when call on a cpu           tensor            func `torch gather` when ``input`` dimension be one and call           on a cuda tensor that require grad            func `torch index add` when call on cuda tensor            func `torch index select` when attempt to differentiate a cuda tensor            func `torch repeat interleave` when attempt to differentiate a cuda tensor            func `torch tensor index copy` when call on a cpu or cuda tensor      the follow normally-nondeterministic operations will throw a      class `runtimeerror` when ``mode=true``              class `torch nn avgpool3d` when attempt to differentiate a cuda tensor            class `torch nn adaptiveavgpool2d` when attempt to differentiate a cuda tensor            class `torch nn adaptiveavgpool3d` when attempt to differentiate a cuda tensor            class `torch nn maxpool3d` when attempt to differentiate a cuda tensor            class `torch nn adaptivemaxpool2d` when attempt to differentiate a cuda tensor            class `torch nn fractionalmaxpool2d` when attempt to differentiate a cuda tensor            class `torch nn fractionalmaxpool3d` when attempt to differentiate a cuda tensor            func `torch nn functional interpolate` when attempt to differentiate a cuda tensor           and one of the follow modes be use             - ``linear``           - ``bilinear``           - ``bicubic``           - ``trilinear``             class `torch nn reflectionpad1d` when attempt to differentiate a cuda tensor            class `torch nn reflectionpad2d` when attempt to differentiate a cuda tensor            class `torch nn replicationpad1d` when attempt to differentiate a cuda tensor            class `torch nn replicationpad2d` when attempt to differentiate a cuda tensor            class `torch nn replicationpad3d` when attempt to differentiate a cuda tensor            class `torch nn nllloss` when call on a cuda tensor            class `torch nn ctcloss` when attempt to differentiate a cuda tensor            class `torch nn embeddingbag` when attempt to differentiate a cuda tensor when           ``mode= max ``            func `torch tensor scatter add ` when call on a cuda tensor            func `torch tensor put ` when ``accumulate=false``            func `torch tensor put ` when ``accumulate=true`` and call on a cuda tensor            func `torch histc` when call on a cuda tensor            func `torch bincount` when call on a cuda tensor            func `torch kthvalue` with call on a cuda tensor            func `torch median` with indices output when call on a cuda tensor            func `torch gather` when ``input`` dimension be larger than one           and call on a cuda tensor that require grad            func `torch nn functional grid sample` when attempt to differentiate a cuda tensor      a handful of cuda operations be nondeterministic if the cuda version be     10 2 or greater  unless the environment variable ``cublas workspace config= 4096 8``     or ``cublas workspace config= 16 8`` be set  see the cuda documentation for more     detail  `<https //docs nvidia com/cuda/cublas/index html cublasapi reproducibility>`      if one of these environment variable configurations be not set  a  class `runtimeerror`     will be raise from these operations when call with cuda tensors              func `torch mm`            func `torch mv`            func `torch bmm`      note that deterministic operations tend to have worse performance than     nondeterministic operations          note            this flag do not detect or prevent nondeterministic behavior cause         by call an inplace operation on a tensor with an internal memory         overlap or by give such a tensor as the  attr `out` argument for an         operation  in these case  multiple write of different data may target         a single memory location  and the order of write be not guarantee       args          mode   class `bool`   if true  make potentially nondeterministic             operations switch to a deterministic algorithm or throw a runtime             error  if false  allow nondeterministic operations       example            >>> torch use deterministic algorithms true             forward mode nondeterministic error         >>> torch randn 10  index copy 0  torch tensor  0    torch randn 1                       runtimeerror  index copy do not have a deterministic implementation               backward mode nondeterministic error         >>> torch randn 10  require grad=true  device= cuda   index select 0  torch tensor  0   device= cuda    backward                       runtimeerror  index add cuda  do not have a deterministic implementation                 c  set deterministic algorithms mode  
def be deterministic algorithms enable        r   return true if the global deterministic flag be turn on  refer to      func `torch use deterministic algorithms` documentation for more detail              return  c  get deterministic algorithms   
def set warn always b       r   when this flag be false  default  then some pytorch warn may only     appear once per process  this help avoid excessive warn information      set it to true cause these warn to always appear  which may be     helpful when debug       args          b   class `bool`   if true  force warn to always be emit                            if false  set to the default behaviour              c  set warnalways b  
def be warn always enable        r   return true if the global warn always flag be turn on  refer to      func `torch set warn always` documentation for more detail              return  c  get warnalways   
class parameter torch tensor       r   a kind of tensor that be to be consider a module parameter       parameters be  class `~torch tensor` subclasses  that have a     very special property when use with  class `module` s - when they re     assign as module attribute they be automatically add to the list of     its parameters  and will appear e g  in  meth `~module parameters` iterator      assign a tensor doesn t have such effect  this be because one might     want to cache some temporary state  like last hide state of the rnn  in     the model  if there be no such class as  class `parameter`  these     temporaries would get register too       args          data  tensor   parameter tensor          require grad  bool  optional   if the parameter require gradient  see              ref `locally-disable-grad-doc` for more detail  default  `true`             def   new   cls  data=none  require grad=true           if data be none              data = torch tensor             return torch tensor  make subclass cls  data  require grad       def   deepcopy   self  memo           if id self  in memo              return memo id self           else              result = type self  self data clone memory format=torch preserve format   self require grad              memo id self   = result             return result      def   repr   self           return string   super parameter  self    repr          def   reduce ex   self  proto                    return               torch  utils  rebuild parameter               self data  self require grad  ordereddict                     torch function   =  disable torch function impl 
class uninitializedparameter uninitializedtensormixin  parameter       r   a parameter that be not initialize       unitialized parameters be a a special case of  class `torch nn parameter`     where the shape of the data be still unknown       unlike a  class `torch nn parameter`  uninitialized parameters     hold no data and attempt to access some properties  like their shape      will throw a runtime error  the only operations that can be perform on a uninitialized     parameter be change its datatype  move it to a different device and     convert it to a regular  class `torch nn parameter`       the default device or dtype to use when the parameter be materialize can be set     during construction use e g  ``device= cuda ``               cls to become = parameter      def   new   cls  require grad=true  device=none  dtype=none  -> none          factory kwargs =  string  device  string  dtype          data = torch tensor       factory kwargs          return torch tensor  make subclass cls  data  require grad  
class uninitializedbuffer uninitializedtensormixin  torch tensor       r   a buffer that be not initialize       unitialized buffer be a a special case of  class `torch tensor`     where the shape of the data be still unknown       unlike a  class `torch tensor`  uninitialized parameters     hold no data and attempt to access some properties  like their shape      will throw a runtime error  the only operations that can be perform on a uninitialized     parameter be change its datatype  move it to a different device and     convert it to a regular  class `torch tensor`       the default device or dtype to use when the buffer be materialize can be set     during construction use e g  ``device= cuda ``               cls to become = torch tensor      def   new   cls  require grad=false  device=none  dtype=none  -> none          factory kwargs =  string  device  string  dtype          data = torch tensor       factory kwargs          return torch tensor  make subclass cls  data  require grad  
class module      r   base class for all neural network modules       your model should also subclass this class       modules can also contain other modules  allow to nest them in     a tree structure  you can assign the submodules as regular attribute            import torch nn as nn         import torch nn functional as f          class model nn module               def   init   self                   super model  self    init                     self conv1 = nn conv2d 1  20  5                  self conv2 = nn conv2d 20  20  5               def forward self  x                   x = f relu self conv1 x                   return f relu self conv2 x        submodules assign in this way will be register  and will have their     parameters convert too when you call  meth `to`  etc        ivar train  boolean represent whether this module be in train or                     evaluation mode       vartype train  bool              dump patch  bool = false      r   this allow better bc support for  meth `load state dict`  in      meth `state dict`  the version number will be save as in the attribute     ` metadata` of the return state dict  and thus pickle  ` metadata` be a     dictionary with key that follow the name convention of state dict  see     `` load from state dict`` on how to use this information in load       if new parameters/buffers be added/removed from a module  this number shall     be bump  and the module s ` load from state dict` method can compare the     version number and do appropriate change if the state dict be from before     the change          version  int = 1      train  bool      be full backward hook  optional bool       def   init   self           string         torch  c  log api usage once string           self train = true         self  parameters = ordereddict           self  buffer = ordereddict           self  non persistent buffer set = set           self  backward hook = ordereddict           self  be full backward hook = none         self  forward hook = ordereddict           self  forward pre hook = ordereddict           self  state dict hook = ordereddict           self  load state dict pre hook = ordereddict           self  modules = ordereddict        forward  callable      any  =  forward unimplemented      def register buffer self  name  str  tensor  optional tensor   persistent  bool = true  -> none          r   add a buffer to the module           this be typically use to register a buffer that should not to be         consider a model parameter  for example  batchnorm s ``running mean``         be not a parameter  but be part of the module s state  buffer  by         default  be persistent and will be save alongside parameters  this         behavior can be change by set  attr `persistent` to ``false``  the         only difference between a persistent buffer and a non-persistent buffer         be that the latter will not be a part of this module s          attr `state dict`           buffer can be access as attribute use give name           args              name  string   name of the buffer  the buffer can be access                 from this module use the give name             tensor  tensor   buffer to be register              persistent  bool   whether the buffer be part of this module s                  attr `state dict`           example                >>> self register buffer  run mean   torch zero num feature                        if persistent be false and isinstance self  torch jit scriptmodule               raise runtimeerror string           if string not in self   dict                raise attributeerror                  string          elif not isinstance name  torch  six string class               raise typeerror string                             string format torch typename name            elif string in name              raise keyerror string          elif name == string              raise keyerror string          elif hasattr self  name  and name not in self  buffer              raise keyerror string format name           elif tensor be not none and not isinstance tensor  torch tensor               raise typeerror string                             string                              format torch typename tensor   name           else              self  buffer name  = tensor             if persistent                  self  non persistent buffer set discard name              else                  self  non persistent buffer set add name       def register parameter self  name  str  param  optional parameter   -> none          r   add a parameter to the module           the parameter can be access as an attribute use give name           args              name  string   name of the parameter  the parameter can be access                 from this module use the give name             param  parameter   parameter to be add to the module                      if string not in self   dict                raise attributeerror                  string           elif not isinstance name  torch  six string class               raise typeerror string                             string format torch typename name            elif string in name              raise keyerror string          elif name == string              raise keyerror string          elif hasattr self  name  and name not in self  parameters              raise keyerror string format name            if param be none              self  parameters name  = none         elif not isinstance param  parameter               raise typeerror string                             string                              format torch typename param   name           elif param grad fn              raise valueerror                  string                 string                 string                 string format name           else              self  parameters name  = param      def add module self  name  str  module  optional string   -> none          r   add a child module to the current module           the module can be access as an attribute use the give name           args              name  string   name of the child module  the child module can be                 access from this module use the give name             module  module   child module to be add to the module                      if not isinstance module  module  and module be not none              raise typeerror string format                  torch typename module            elif not isinstance name  torch  six string class               raise typeerror string format                  torch typename name            elif hasattr self  name  and name not in self  modules              raise keyerror string format name           elif string in name              raise keyerror string format name           elif name == string              raise keyerror string          self  modules name  = module      def get submodule self  target  str  -> string          string         if target == string              return self          atoms  list str  = target split string          mod  torch nn module = self          for item in atoms               if not hasattr mod  item                   raise attributeerror mod  get name     string                                      string   item   string               mod = getattr mod  item               if not isinstance mod  torch nn module                   raise attributeerror string   item   string                                      string           return mod      def get parameter self  target  str  -> string          string         module path     param name = target rpartition string           mod  torch nn module = self get submodule module path           if not hasattr mod  param name               raise attributeerror mod  get name     string                                    param name   string           param  torch nn parameter = getattr mod  param name           if not isinstance param  torch nn parameter               raise attributeerror string   param name   string                                  string           return param      def get buffer self  target  str  -> string          string         module path     buffer name = target rpartition string           mod  torch nn module = self get submodule module path           if not hasattr mod  buffer name               raise attributeerror mod  get name     string                                    buffer name   string           buffer  torch tensor = getattr mod  buffer name           if buffer not in mod  buffer value                raise attributeerror string   buffer name   string           return buffer      def  apply self  fn           for module in self children                module  apply fn           def compute should use set data tensor  tensor apply               if torch  have compatible shallow copy type tensor  tensor apply                                                                                                                                                           return not torch   future   get overwrite module params on conversion               else                  return false          for key  param in self  parameters items                if param be not none                                                                     with torch no grad                        param apply = fn param                  should use set data = compute should use set data param  param apply                  if should use set data                      param data = param apply                 else                      assert isinstance param  parameter                      assert param be leaf                     self  parameters key  = parameter param apply  param require grad                   if param grad be not none                      with torch no grad                            grad apply = fn param grad                      should use set data = compute should use set data param grad  grad apply                      if should use set data                          param grad data = grad apply                     else                          assert param grad be leaf                         self  parameters key  grad = grad apply require grad  param grad require grad           for key  buf in self  buffer items                if buf be not none                  self  buffer key  = fn buf           return self      def apply self  t  fn  callable  string   none   -> t          r   apply ``fn`` recursively to every submodule  as return by `` children  ``          as well as self  typical use include initialize the parameters of a model          see also  ref `nn-init-doc`            args              fn   class `module` -> none   function to be apply to each submodule          return              module  self          example                >>>  torch no grad               >>> def init weight m               >>>     print m              >>>     if type m  == nn linear              >>>         m weight fill  1 0              >>>         print m weight              >>> net = nn sequential nn linear 2  2   nn linear 2  2               >>> net apply init weight              linear in features=2  out features=2  bias=true              parameter contain              tensor    1    1                          1    1                 linear in features=2  out features=2  bias=true              parameter contain              tensor    1    1                          1    1                 sequential                 0   linear in features=2  out features=2  bias=true                 1   linear in features=2  out features=2  bias=true                            sequential                 0   linear in features=2  out features=2  bias=true                 1   linear in features=2  out features=2  bias=true                                    for module in self children                module apply fn          fn self          return self      def cuda self  t  device  optional union int  device   = none  -> t          r   move all model parameters and buffer to the gpu           this also make associate parameters and buffer different object  so         it should be call before construct optimizer if the module will         live on gpu while be optimize              note               this method modify the module in-place           args              device  int  optional   if specify  all parameters will be                 copy to that device          return              module  self                     return self  apply lambda t  t cuda device        def xpu self  t  device  optional union int  device   = none  -> t          r   move all model parameters and buffer to the xpu           this also make associate parameters and buffer different object  so         it should be call before construct optimizer if the module will         live on xpu while be optimize              note               this method modify the module in-place           arguments              device  int  optional   if specify  all parameters will be                 copy to that device          return              module  self                     return self  apply lambda t  t xpu device        def cpu self  t  -> t          r   move all model parameters and buffer to the cpu              note               this method modify the module in-place           return              module  self                     return self  apply lambda t  t cpu         def type self  t  dst type  union dtype  str   -> t          r   cast all parameters and buffer to  attr `dst type`              note               this method modify the module in-place           args              dst type  type or string   the desire type          return              module  self                     return self  apply lambda t  t type dst type        def float self  t  -> t          r   cast all float point parameters and buffer to ``float`` datatype              note               this method modify the module in-place           return              module  self                     return self  apply lambda t  t float   if t be float point   else t       def double self  t  -> t          r   cast all float point parameters and buffer to ``double`` datatype              note               this method modify the module in-place           return              module  self                     return self  apply lambda t  t double   if t be float point   else t       def half self  t  -> t          r   cast all float point parameters and buffer to ``half`` datatype              note               this method modify the module in-place           return              module  self                     return self  apply lambda t  t half   if t be float point   else t       def bfloat16 self  t  -> t          r   cast all float point parameters and buffer to ``bfloat16`` datatype              note               this method modify the module in-place           return              module  self                     return self  apply lambda t  t bfloat16   if t be float point   else t       def to empty self  t     device  union str  device   -> t          r   move the parameters and buffer to the specify device without copy storage           args              device   class `torch device`   the desire device of the parameters                 and buffer in this module           return              module  self                     return self  apply lambda t  torch empty like t  device=device         overload     def to self  t  device  optional union int  device   =      dtype  optional union dtype  str   =                 non block  bool =      -> t                    overload     def to self  t  dtype  union dtype  str   non block  bool =      -> t                    overload     def to self  t  tensor  tensor  non block  bool =      -> t                   def to self   args    kwargs           r   move and/or cast the parameters and buffer           this can be call as             function   to device=none  dtype=none  non blocking=false              function   to dtype  non blocking=false              function   to tensor  non blocking=false              function   to memory format=torch channel last           its signature be similar to  meth `torch tensor to`  but only accept         float point or complex  attr `dtype`s  in addition  this method will         only cast the float point or complex parameters and buffer to  attr `dtype`          if give   the integral parameters and buffer will be move          attr `device`  if that be give  but with dtypes unchanged  when          attr `non blocking` be set  it try to convert/move asynchronously         with respect to the host if possible  e g   move cpu tensors with         pin memory to cuda devices           see below for examples              note               this method modify the module in-place           args              device   class `torch device`   the desire device of the parameters                 and buffer in this module             dtype   class `torch dtype`   the desire float point or complex dtype of                 the parameters and buffer in this module             tensor  torch tensor   tensor whose dtype and device be the desire                 dtype and device for all parameters and buffer in this module             memory format   class `torch memory format`   the desire memory                 format for 4d parameters and buffer in this module  keyword                 only argument           return              module  self          examples                >>> linear = nn linear 2  2              >>> linear weight             parameter contain              tensor    0 1913  -0 3420                        -0 5113  -0 2325                >>> linear to torch double              linear in features=2  out features=2  bias=true              >>> linear weight             parameter contain              tensor    0 1913  -0 3420                        -0 5113  -0 2325    dtype=torch float64              >>> gpu1 = torch device  cuda 1               >>> linear to gpu1  dtype=torch half  non blocking=true              linear in features=2  out features=2  bias=true              >>> linear weight             parameter contain              tensor    0 1914  -0 3420                        -0 5112  -0 2324    dtype=torch float16  device= cuda 1               >>> cpu = torch device  cpu               >>> linear to cpu              linear in features=2  out features=2  bias=true              >>> linear weight             parameter contain              tensor    0 1914  -0 3420                        -0 5112  -0 2324    dtype=torch float16               >>> linear = nn linear 2  2  bias=none  to torch cdouble              >>> linear weight             parameter contain              tensor    0 3741 0 j   0 2382 0 j                         0 5593 0 j  -0 4443 0 j    dtype=torch complex128              >>> linear torch ones 3  2  dtype=torch cdouble               tensor   0 6122 0 j  0 1150 0 j                        0 6122 0 j  0 1150 0 j                        0 6122 0 j  0 1150 0 j    dtype=torch complex128                        device  dtype  non block  convert to format = torch  c  nn  parse to  args    kwargs           if dtype be not none              if not  dtype be float point or dtype be complex                   raise typeerror string                                 string format dtype               if dtype be complex                  warn warn                      string                     string                     string                     string           def convert t               if convert to format be not none and t dim   in  4  5                   return t to device  dtype if t be float point   or t be complex   else none                              non block  memory format=convert to format              return t to device  dtype if t be float point   or t be complex   else none  non block           return self  apply convert       def register backward hook          self  hook  callable  string   grad t   grad t   union none  tensor         -> removablehandle          r   register a backward hook on the module           this function be deprecate in favor of  meth `nn module register full backward hook` and         the behavior of this function will change in future versions           return               class `torch utils hook removablehandle`                  a handle that can be use to remove the add hook by call                 ``handle remove  ``                      if self  be full backward hook be true              raise runtimeerror string                                string           self  be full backward hook = false          handle = hook removablehandle self  backward hook          self  backward hook handle id  = hook         return handle      def register full backward hook          self  hook  callable  string   grad t   grad t   union none  tensor         -> removablehandle          r   register a backward hook on the module           the hook will be call every time the gradients with respect to module         input be compute  the hook should have the follow signature                hook module  grad input  grad output  -> tuple tensor  or none          the  attr `grad input` and  attr `grad output` be tuples that contain the gradients         with respect to the input and output respectively  the hook should         not modify its arguments  but it can optionally return a new gradient with         respect to the input that will be use in place of  attr `grad input` in         subsequent computations   attr `grad input` will only correspond to the input give         as positional arguments and all kwarg arguments be ignore  entries         in  attr `grad input` and  attr `grad output` will be ``none`` for all non-tensor         arguments              warn                modify input or output inplace be not allow when use backward hook and             will raise an error           return               class `torch utils hook removablehandle`                  a handle that can be use to remove the add hook by call                 ``handle remove  ``                      if self  be full backward hook be false              raise runtimeerror string                                string           self  be full backward hook = true          handle = hook removablehandle self  backward hook          self  backward hook handle id  = hook         return handle      def  get backward hook self           r   return the backward hook for use in the call function          it return two list  one with the full backward hook and one with the non-full         backward hook                      full backward hook  list callable  =            if   global be full backward hook be true               full backward hook  =  global backward hook value           if  self  be full backward hook be true               full backward hook  = self  backward hook value            non full backward hook  list callable  =            if   global be full backward hook be false               non full backward hook  =  global backward hook value           if  self  be full backward hook be false               non full backward hook  = self  backward hook value            return full backward hook  non full backward hook      def  maybe warn non full backward hook self  input  result  grad fn           if not isinstance result  torch tensor               if not  isinstance result  tuple  and all  isinstance r  torch tensor  for r in result                     warn warn string                               string                               string                               string                  return         else              result =  result            if not isinstance input  torch tensor               if not  isinstance input  tuple  and all  isinstance i  torch tensor  for i in input                     warn warn string                               string                               string                               string                  return         else              input =  input                     out grad fn =  r grad fn for r in result if r grad fn be not none          if len out grad fn  == 0 or  len out grad fn  == 1 and grad fn not in out grad fn               warn warn string                           string                           string          elif len out grad fn  > 1              warn warn string                           string                           string          else                           input grad fn =  i grad fn for i in input if i grad fn be not none               next function =  n 0  for n in grad fn next function               if input grad fn  = next function                  warn warn string                               string                               string                               string       def register forward pre hook self  hook  callable      none   -> removablehandle          r   register a forward pre-hook on the module           the hook will be call every time before  func `forward` be invoke          it should have the follow signature                hook module  input  -> none or modify input          the input contain only the positional arguments give to the module          keyword arguments win t be pass to the hook and only to the ``forward``          the hook can modify the input  user can either return a tuple or a         single modify value in the hook  we will wrap the value into a tuple         if a single value be return unless that value be already a tuple            return               class `torch utils hook removablehandle`                  a handle that can be use to remove the add hook by call                 ``handle remove  ``                     handle = hook removablehandle self  forward pre hook          self  forward pre hook handle id  = hook         return handle      def register forward hook self  hook  callable      none   -> removablehandle          r   register a forward hook on the module           the hook will be call every time after  func `forward` have compute an output          it should have the follow signature                hook module  input  output  -> none or modify output          the input contain only the positional arguments give to the module          keyword arguments win t be pass to the hook and only to the ``forward``          the hook can modify the output  it can modify the input inplace but         it will not have effect on forward since this be call after          func `forward` be call           return               class `torch utils hook removablehandle`                  a handle that can be use to remove the add hook by call                 ``handle remove  ``                     handle = hook removablehandle self  forward hook          self  forward hook handle id  = hook         return handle      def  slow forward self   input    kwargs           trace state = torch  c  get trace state           if not trace state or isinstance self forward  torch  c scriptmethod               return self forward  input    kwargs          record scopes = torch jit  trace  trace module map be not none         if record scopes                                        name = torch jit  trace  trace module map self  if self in torch jit  trace  trace module map else none               if name                  trace state push scope name              else                  record scopes = false         try              result = self forward  input    kwargs          finally              if record scopes                  trace state pop scope           return result      def  call impl self   input    kwargs           forward call =  self  slow forward if torch  c  get trace state   else self forward                            if not  self  backward hook or self  forward hook or self  forward pre hook or  global backward hook                 or  global forward hook or  global forward pre hook               return forward call  input    kwargs                   full backward hook  non full backward hook =                if self  backward hook or  global backward hook              full backward hook  non full backward hook = self  get backward hook           if  global forward pre hook or self  forward pre hook              for hook in itertools chain                       global forward pre hook value                        self  forward pre hook value                     result = hook self  input                  if result be not none                      if not isinstance result  tuple                           result =  result                       input = result          bw hook = none         if full backward hook              bw hook = hook backwardhook self  full backward hook              input = bw hook setup input hook input           result = forward call  input    kwargs          if  global forward hook or self  forward hook              for hook in itertools chain                       global forward hook value                        self  forward hook value                     hook result = hook self  input  result                  if hook result be not none                      result = hook result          if bw hook              result = bw hook setup output hook result                    if non full backward hook              var = result             while not isinstance var  torch tensor                   if isinstance var  dict                       var = next  v for v in var value   if isinstance v  torch tensor                    else                      var = var 0              grad fn = var grad fn             if grad fn be not none                  for hook in non full backward hook                      wrapper = functools partial hook  self                      functools update wrapper wrapper  hook                      grad fn register hook wrapper                  self  maybe warn non full backward hook input  result  grad fn           return result        call     callable      any  =  call impl      def   setstate   self  state           self   dict   update state                   if string not in self   dict                self  forward pre hook = ordereddict           if string not in self   dict                self  state dict hook = ordereddict           if string not in self   dict                self  load state dict pre hook = ordereddict           if string not in self   dict                self  non persistent buffer set = set           if string not in self   dict                self  be full backward hook = none      def   getattr   self  name  str  -> union tensor  string           if string in self   dict                 parameters = self   dict   string              if name in  parameters                  return  parameters name          if string in self   dict                 buffer = self   dict   string              if name in  buffer                  return  buffer name          if string in self   dict                modules = self   dict   string              if name in modules                  return modules name          raise attributeerror string format              type self    name    name        def   setattr   self  name  str  value  union tensor  string   -> none          def remove from  dicts or set               for d in dicts or set                  if name in d                      if isinstance d  dict                           del d name                      else                          d discard name           params = self   dict   get string          if isinstance value  parameter               if params be none                  raise attributeerror                      string              remove from self   dict    self  buffer  self  modules  self  non persistent buffer set              self register parameter name  value          elif params be not none and name in params              if value be not none                  raise typeerror string                                 string                                  format torch typename value   name               self register parameter name  value          else              modules = self   dict   get string              if isinstance value  module                   if modules be none                      raise attributeerror                          string                  remove from self   dict    self  parameters  self  buffer  self  non persistent buffer set                  modules name  = value             elif modules be not none and name in modules                  if value be not none                      raise typeerror string                                     string                                      format torch typename value   name                   modules name  = value             else                  buffer = self   dict   get string                  if buffer be not none and name in buffer                      if value be not none and not isinstance value  torch tensor                           raise typeerror string                                         string                                          format torch typename value   name                       buffer name  = value                 else                      object   setattr   self  name  value       def   delattr   self  name           if name in self  parameters              del self  parameters name          elif name in self  buffer              del self  buffer name              self  non persistent buffer set discard name          elif name in self  modules              del self  modules name          else              object   delattr   self  name       def  register state dict hook self  hook           r   these hook will be call with arguments  `self`  `state dict`          `prefix`  `local metadata`  after the `state dict` of `self` be set          note that only parameters and buffer of `self` or its children be         guarantee to exist in `state dict`  the hook may modify `state dict`         inplace or return a new one                      handle = hook removablehandle self  state dict hook          self  state dict hook handle id  = hook         return handle      def  save to state dict self  destination  prefix  keep vars           r   save module state to `destination` dictionary  contain a state         of the module  but not its descendants  this be call on every         submodule in  meth `~torch nn module state dict`           in rare case  subclasses can achieve class-specific behavior by         override this method with custom logic           args              destination  dict   a dict where state will be store             prefix  str   the prefix for parameters and buffer use in this                 module                     for name  param in self  parameters items                if param be not none                  destination prefix   name  = param if keep vars else param detach           for name  buf in self  buffer items                if buf be not none and name not in self  non persistent buffer set                  destination prefix   name  = buf if keep vars else buf detach                  t destination = typevar string  bound=mapping str  tensor         overload     def state dict self  destination  t destination  prefix  str =      keep vars  bool =      -> t destination                              overload     def state dict self  prefix  str =      keep vars  bool =      -> string                   def state dict self  destination=none  prefix=string  keep vars=false           r   return a dictionary contain a whole state of the module           both parameters and persistent buffer  e g  run average  be         include  key be correspond parameter and buffer name           return              dict                  a dictionary contain a whole state of the module          example                >>> module state dict   key                 bias    weight                        if destination be none              destination = ordereddict               destination  metadata = ordereddict           destination  metadata prefix  -1   = local metadata = dict version=self  version          self  save to state dict destination  prefix  keep vars          for name  module in self  modules items                if module be not none                  module state dict destination  prefix   name   string  keep vars=keep vars          for hook in self  state dict hook value                hook result = hook self  destination  prefix  local metadata              if hook result be not none                  destination = hook result         return destination      def  register load state dict pre hook self  hook           r   these hook will be call with arguments  `state dict`  `prefix`          `local metadata`  `strict`  `missing keys`  `unexpected keys`          `error msgs`  before load `state dict` into `self`  these arguments         be exactly the same as those of ` load from state dict`                      handle = hook removablehandle self  load state dict pre hook          self  load state dict pre hook handle id  = hook         return handle      def  load from state dict self  state dict  prefix  local metadata  strict                                miss key  unexpected key  error msgs           r   copy parameters and buffer from  attr `state dict` into only         this module  but not its descendants  this be call on every submodule         in  meth `~torch nn module load state dict`  metadata save for this         module in input  attr `state dict` be provide as  attr `local metadata`          for state dicts without metadata   attr `local metadata` be empty          subclasses can achieve class-specific backward compatible load use         the version number at `local metadata get  version   none `              note                attr `state dict` be not the same object as the input              attr `state dict` to  meth `~torch nn module load state dict`  so             it can be modify           args              state dict  dict   a dict contain parameters and                 persistent buffer              prefix  str   the prefix for parameters and buffer use in this                 module             local metadata  dict   a dict contain the metadata for this module                  see             strict  bool   whether to strictly enforce that the key in                  attr `state dict` with  attr `prefix` match the name of                 parameters and buffer in this module             miss key  list of str   if ``strict=true``  add miss key to                 this list             unexpected key  list of str   if ``strict=true``  add unexpected                 key to this list             error msgs  list of str   error message should be add to this                 list  and will be report together in                  meth `~torch nn module load state dict`                     for hook in self  load state dict pre hook value                hook state dict  prefix  local metadata  strict  miss key  unexpected key  error msgs           persistent buffer =  k  v for k  v in self  buffer items   if k not in self  non persistent buffer set          local name params = itertools chain self  parameters items    persistent buffer items            local state =  k  v for k  v in local name params if v be not none           for name  param in local state items                key = prefix   name             if key in state dict                  input param = state dict key                                                                     be param lazy = torch nn parameter be lazy param                                   if not be param lazy and len param shape  == 0 and len input param shape  == 1                      input param = input param 0                   if not be param lazy and input param shape  = param shape                                           error msgs append string                                       string                                        format key  input param shape  param shape                       continue                 try                      with torch no grad                            param copy  input param                  except exception as ex                      error msgs append string                                       string                                       string                                       string                                        format key  param size    input param size    ex args               elif strict                  miss key append key           if strict              for key in state dict key                    if key startswith prefix                       input name = key len prefix                        input name = input name split string  1  0                        if input name not in self  modules and input name not in local state                          unexpected key append key       def load state dict self  state dict  string                          strict  bool = true           r   copy parameters and buffer from  attr `state dict` into         this module and its descendants  if  attr `strict` be ``true``  then         the key of  attr `state dict` must exactly match the key return         by this module s  meth `~torch nn module state dict` function           args              state dict  dict   a dict contain parameters and                 persistent buffer              strict  bool  optional   whether to strictly enforce that the key                 in  attr `state dict` match the key return by this module s                  meth `~torch nn module state dict` function  default  ``true``          return              ``namedtuple`` with ``missing keys`` and ``unexpected keys`` field                      miss key   be a list of str contain the miss key                     unexpected key   be a list of str contain the unexpected key                     miss key  list str  =            unexpected key  list str  =            error msgs  list str  =                      metadata = getattr state dict  string  none          state dict = state dict copy           if metadata be not none                           state dict  metadata = metadata            def load module  prefix=string               local metadata =    if metadata be none else metadata get prefix  -1                   module  load from state dict                  state dict  prefix  local metadata  true  miss key  unexpected key  error msgs              for name  child in module  modules items                    if child be not none                      load child  prefix   name   string           load self          del load          if strict              if len unexpected key  > 0                  error msgs insert                      0  string format                          string join string format k  for k in unexpected key                if len miss key  > 0                  error msgs insert                      0  string format                          string join string format k  for k in miss key             if len error msgs  > 0              raise runtimeerror string format                                 self   class     name    string join error msgs            return  incompatiblekeys miss key  unexpected key       def  name members self  get members fn  prefix=string  recurse=true           r   helper method for yield various name   members of modules             memo = set           modules = self name modules prefix=prefix  if recurse else   prefix  self           for module prefix  module in modules              members = get members fn module              for k  v in members                  if v be none or v in memo                      continue                 memo add v                  name = module prefix    string if module prefix else string    k                 yield name  v      def parameters self  recurse  bool = true  -> iterator parameter           r   return an iterator over module parameters           this be typically pass to an optimizer           args              recurse  bool   if true  then yield parameters of this module                 and all submodules  otherwise  yield only parameters that                 be direct members of this module           yield              parameter  module parameter          example                >>> for param in model parameters                >>>     print type param   param size                <class  torch tensor >  20l               <class  torch tensor >  20l  1l  5l  5l                       for name  param in self name parameters recurse=recurse               yield param      def name parameters self  prefix  str = string  recurse  bool = true  -> iterator tuple str  parameter            r   return an iterator over module parameters  yield both the         name of the parameter as well as the parameter itself           args              prefix  str   prefix to prepend to all parameter name              recurse  bool   if true  then yield parameters of this module                 and all submodules  otherwise  yield only parameters that                 be direct members of this module           yield               string  parameter   tuple contain the name and parameter          example                >>> for name  param in self name parameters                >>>    if name in   bias                >>>        print param size                         gen = self  name members              lambda module  module  parameters items                prefix=prefix  recurse=recurse          for elem in gen              yield elem      def buffer self  recurse  bool = true  -> iterator tensor           r   return an iterator over module buffer           args              recurse  bool   if true  then yield buffer of this module                 and all submodules  otherwise  yield only buffer that                 be direct members of this module           yield              torch tensor  module buffer          example                >>> for buf in model buffer                >>>     print type buf   buf size                <class  torch tensor >  20l               <class  torch tensor >  20l  1l  5l  5l                       for    buf in self name buffer recurse=recurse               yield buf      def name buffer self  prefix  str = string  recurse  bool = true  -> iterator tuple str  tensor            r   return an iterator over module buffer  yield both the         name of the buffer as well as the buffer itself           args              prefix  str   prefix to prepend to all buffer name              recurse  bool   if true  then yield buffer of this module                 and all submodules  otherwise  yield only buffer that                 be direct members of this module           yield               string  torch tensor   tuple contain the name and buffer          example                >>> for name  buf in self name buffer                >>>    if name in   run var                >>>        print buf size                         gen = self  name members              lambda module  module  buffer items                prefix=prefix  recurse=recurse          for elem in gen              yield elem      def children self  -> iterator string           r   return an iterator over immediate children modules           yield              module  a child module                     for name  module in self name children                yield module      def name children self  -> iterator tuple str  string            r   return an iterator over immediate children modules  yield both         the name of the module as well as the module itself           yield               string  module   tuple contain a name and child module          example                >>> for name  module in model name children                >>>     if name in   conv4    conv5                >>>         print module                       memo = set           for name  module in self  modules items                if module be not none and module not in memo                  memo add module                  yield name  module      def modules self  -> iterator string           r   return an iterator over all modules in the network           yield              module  a module in the network          note              duplicate modules be return only once  in the follow             example  ``l`` will be return only once           example                >>> l = nn linear 2  2              >>> net = nn sequential l  l              >>> for idx  m in enumerate net modules                         print idx   ->   m               0 -> sequential                 0   linear in features=2  out features=2  bias=true                 1   linear in features=2  out features=2  bias=true                            1 -> linear in features=2  out features=2  bias=true                       for    module in self name modules                yield module      def name modules self  memo  optional set string   = none  prefix  str = string  remove duplicate  bool = true           r   return an iterator over all modules in the network  yield         both the name of the module as well as the module itself           args              memo  a memo to store the set of modules already add to the result             prefix  a prefix that will be add to the name of the module             remove duplicate  whether to remove the duplicate module instance in the result             or not          yield               string  module   tuple of name and module          note              duplicate modules be return only once  in the follow             example  ``l`` will be return only once           example                >>> l = nn linear 2  2              >>> net = nn sequential l  l              >>> for idx  m in enumerate net name modules                         print idx   ->   m               0 ->      sequential                 0   linear in features=2  out features=2  bias=true                 1   linear in features=2  out features=2  bias=true                             1 ->   0   linear in features=2  out features=2  bias=true                         if memo be none              memo = set           if self not in memo              if remove duplicate                  memo add self              yield prefix  self             for name  module in self  modules items                    if module be none                      continue                 submodule prefix = prefix    string if prefix else string    name                 for m in module name modules memo  submodule prefix  remove duplicate                       yield m      def train self  t  mode  bool = true  -> t          r   set the module in train mode           this have any effect only on certain modules  see documentations of         particular modules for detail of their behaviors in training/evaluation         mode  if they be affect  e g   class `dropout`   class `batchnorm`          etc           args              mode  bool   whether to set train mode  ``true``  or evaluation                          mode  ``false``   default  ``true``           return              module  self                     if not isinstance mode  bool               raise valueerror string          self train = mode         for module in self children                module train mode          return self      def eval self  t  -> t          r   set the module in evaluation mode           this have any effect only on certain modules  see documentations of         particular modules for detail of their behaviors in training/evaluation         mode  if they be affect  e g   class `dropout`   class `batchnorm`          etc           this be equivalent with  meth `self train false  <torch nn module train>`           see  ref `locally-disable-grad-doc` for a comparison between         ` eval  ` and several similar mechanisms that may be confuse with it           return              module  self                     return self train false       def require grad  self  t  require grad  bool = true  -> t          r   change if autograd should record operations on parameters in this         module           this method set the parameters   attr `requires grad` attribute         in-place           this method be helpful for freeze part of the module for finetuning         or train part of a model individually  e g   gin train            see  ref `locally-disable-grad-doc` for a comparison between         ` require grad   ` and several similar mechanisms that may be confuse with it           args              require grad  bool   whether autograd should record operations on                                   parameters in this module  default  ``true``           return              module  self                     for p in self parameters                p require grad  require grad          return self      def zero grad self  set to none  bool = false  -> none          r   set gradients of all model parameters to zero  see similar function         under  class `torch optim optimizer` for more context           args              set to none  bool   instead of set to zero  set the grads to none                  see  meth `torch optim optimizer zero grad` for detail                      if getattr self  string  false               warn warn                  string                 string                 string                 string           for p in self parameters                if p grad be not none                  if set to none                      p grad = none                 else                      if p grad grad fn be not none                          p grad detach                        else                          p grad require grad  false                      p grad zero         def share memory self  t  -> t          r   see  meth `torch tensor share memory `            return self  apply lambda t  t share memory          def  get name self           return self   class     name        def extra repr self  -> str          r   set the extra representation of the module          to print customize extra information  you should re-implement         this method in your own modules  both single-line and multi-line         string be acceptable                      return string      def   repr   self                    extra line =            extra repr = self extra repr                    if extra repr              extra line = extra repr split string          child line =            for key  module in self  modules items                mod str = repr module              mod str =  addindent mod str  2              child line append string   key   string   mod str          line = extra line   child line          main str = self  get name     string         if line                           if len extra line  == 1 and not child line                  main str  = extra line 0              else                  main str  = string   string join line    string          main str  = string         return main str      def   dir   self           module attrs = dir self   class            attrs = list self   dict   key            parameters = list self  parameters key            modules = list self  modules key            buffer = list self  buffer key            key = module attrs   attrs   parameters   modules   buffer                   key =  key for key in key if not key 0  isdigit             return sort key       def  replicate for data parallel self           replica = self   new   type self           replica   dict   = self   dict   copy                              replica  parameters = ordereddict           replica  buffer = replica  buffer copy           replica  modules = replica  modules copy           replica  be replica = true          return replica 
def add module self  name  str  module  optional string   -> none      r   add a child module to the current module       the module can be access as an attribute use the give name       args          name  string   name of the child module  the child module can be             access from this module use the give name         module  module   child module to be add to the module              if not isinstance module  module  and module be not none          raise typeerror string format              torch typename module        elif not isinstance name  torch  six string class           raise typeerror string format              torch typename name        elif hasattr self  name  and name not in self  modules          raise keyerror string format name       elif string in name          raise keyerror string format name       elif name == string          raise keyerror string      self  modules name  = module 
def buffer self  recurse  bool = true  -> iterator tensor       r   return an iterator over module buffer       args          recurse  bool   if true  then yield buffer of this module             and all submodules  otherwise  yield only buffer that             be direct members of this module       yield          torch tensor  module buffer      example            >>> for buf in model buffer            >>>     print type buf   buf size            <class  torch tensor >  20l           <class  torch tensor >  20l  1l  5l  5l               for    buf in self name buffer recurse=recurse           yield buf 
def children self  -> iterator string       r   return an iterator over immediate children modules       yield          module  a child module             for name  module in self name children            yield module 
def cpu self  t  -> t      r   move all model parameters and buffer to the cpu          note           this method modify the module in-place       return          module  self             return self  apply lambda t  t cpu    
def cuda self  t  device  optional union int  device   = none  -> t      r   move all model parameters and buffer to the gpu       this also make associate parameters and buffer different object  so     it should be call before construct optimizer if the module will     live on gpu while be optimize          note           this method modify the module in-place       args          device  int  optional   if specify  all parameters will be             copy to that device      return          module  self             return self  apply lambda t  t cuda device   
def eval self  t  -> t      r   set the module in evaluation mode       this have any effect only on certain modules  see documentations of     particular modules for detail of their behaviors in training/evaluation     mode  if they be affect  e g   class `dropout`   class `batchnorm`      etc       this be equivalent with  meth `self train false  <torch nn module train>`       see  ref `locally-disable-grad-doc` for a comparison between     ` eval  ` and several similar mechanisms that may be confuse with it       return          module  self             return self train false  
def modules self  -> iterator string       r   return an iterator over all modules in the network       yield          module  a module in the network      note          duplicate modules be return only once  in the follow         example  ``l`` will be return only once       example            >>> l = nn linear 2  2          >>> net = nn sequential l  l          >>> for idx  m in enumerate net modules                     print idx   ->   m           0 -> sequential             0   linear in features=2  out features=2  bias=true             1   linear in features=2  out features=2  bias=true                    1 -> linear in features=2  out features=2  bias=true               for    module in self name modules            yield module 
def name buffer self  prefix  str = string  recurse  bool = true  -> iterator tuple str  tensor        r   return an iterator over module buffer  yield both the     name of the buffer as well as the buffer itself       args          prefix  str   prefix to prepend to all buffer name          recurse  bool   if true  then yield buffer of this module             and all submodules  otherwise  yield only buffer that             be direct members of this module       yield           string  torch tensor   tuple contain the name and buffer      example            >>> for name  buf in self name buffer            >>>    if name in   run var            >>>        print buf size                 gen = self  name members          lambda module  module  buffer items            prefix=prefix  recurse=recurse      for elem in gen          yield elem 
def name children self  -> iterator tuple str  string        r   return an iterator over immediate children modules  yield both     the name of the module as well as the module itself       yield           string  module   tuple contain a name and child module      example            >>> for name  module in model name children            >>>     if name in   conv4    conv5            >>>         print module               memo = set       for name  module in self  modules items            if module be not none and module not in memo              memo add module              yield name  module 
def name modules self  memo  optional set string   = none  prefix  str = string  remove duplicate  bool = true       r   return an iterator over all modules in the network  yield     both the name of the module as well as the module itself       args          memo  a memo to store the set of modules already add to the result         prefix  a prefix that will be add to the name of the module         remove duplicate  whether to remove the duplicate module instance in the result         or not      yield           string  module   tuple of name and module      note          duplicate modules be return only once  in the follow         example  ``l`` will be return only once       example            >>> l = nn linear 2  2          >>> net = nn sequential l  l          >>> for idx  m in enumerate net name modules                     print idx   ->   m           0 ->      sequential             0   linear in features=2  out features=2  bias=true             1   linear in features=2  out features=2  bias=true                     1 ->   0   linear in features=2  out features=2  bias=true                 if memo be none          memo = set       if self not in memo          if remove duplicate              memo add self          yield prefix  self         for name  module in self  modules items                if module be none                  continue             submodule prefix = prefix    string if prefix else string    name             for m in module name modules memo  submodule prefix  remove duplicate                   yield m 
def name parameters self  prefix  str = string  recurse  bool = true  -> iterator tuple str  parameter        r   return an iterator over module parameters  yield both the     name of the parameter as well as the parameter itself       args          prefix  str   prefix to prepend to all parameter name          recurse  bool   if true  then yield parameters of this module             and all submodules  otherwise  yield only parameters that             be direct members of this module       yield           string  parameter   tuple contain the name and parameter      example            >>> for name  param in self name parameters            >>>    if name in   bias            >>>        print param size                 gen = self  name members          lambda module  module  parameters items            prefix=prefix  recurse=recurse      for elem in gen          yield elem 
def parameters self  recurse  bool = true  -> iterator parameter       r   return an iterator over module parameters       this be typically pass to an optimizer       args          recurse  bool   if true  then yield parameters of this module             and all submodules  otherwise  yield only parameters that             be direct members of this module       yield          parameter  module parameter      example            >>> for param in model parameters            >>>     print type param   param size            <class  torch tensor >  20l           <class  torch tensor >  20l  1l  5l  5l               for name  param in self name parameters recurse=recurse           yield param 
def register backward hook      self  hook  callable  string   grad t   grad t   union none  tensor     -> removablehandle      r   register a backward hook on the module       this function be deprecate in favor of  meth `nn module register full backward hook` and     the behavior of this function will change in future versions       return           class `torch utils hook removablehandle`              a handle that can be use to remove the add hook by call             ``handle remove  ``              if self  be full backward hook be true          raise runtimeerror string                            string       self  be full backward hook = false      handle = hook removablehandle self  backward hook      self  backward hook handle id  = hook     return handle 
def register buffer self  name  str  tensor  optional tensor   persistent  bool = true  -> none      r   add a buffer to the module       this be typically use to register a buffer that should not to be     consider a model parameter  for example  batchnorm s ``running mean``     be not a parameter  but be part of the module s state  buffer  by     default  be persistent and will be save alongside parameters  this     behavior can be change by set  attr `persistent` to ``false``  the     only difference between a persistent buffer and a non-persistent buffer     be that the latter will not be a part of this module s      attr `state dict`       buffer can be access as attribute use give name       args          name  string   name of the buffer  the buffer can be access             from this module use the give name         tensor  tensor   buffer to be register          persistent  bool   whether the buffer be part of this module s              attr `state dict`       example            >>> self register buffer  run mean   torch zero num feature                if persistent be false and isinstance self  torch jit scriptmodule           raise runtimeerror string       if string not in self   dict            raise attributeerror              string      elif not isinstance name  torch  six string class           raise typeerror string                         string format torch typename name        elif string in name          raise keyerror string      elif name == string          raise keyerror string      elif hasattr self  name  and name not in self  buffer          raise keyerror string format name       elif tensor be not none and not isinstance tensor  torch tensor           raise typeerror string                         string                          format torch typename tensor   name       else          self  buffer name  = tensor         if persistent              self  non persistent buffer set discard name          else              self  non persistent buffer set add name  
def register forward hook self  hook  callable      none   -> removablehandle      r   register a forward hook on the module       the hook will be call every time after  func `forward` have compute an output      it should have the follow signature            hook module  input  output  -> none or modify output      the input contain only the positional arguments give to the module      keyword arguments win t be pass to the hook and only to the ``forward``      the hook can modify the output  it can modify the input inplace but     it will not have effect on forward since this be call after      func `forward` be call       return           class `torch utils hook removablehandle`              a handle that can be use to remove the add hook by call             ``handle remove  ``             handle = hook removablehandle self  forward hook      self  forward hook handle id  = hook     return handle 
def register forward pre hook self  hook  callable      none   -> removablehandle      r   register a forward pre-hook on the module       the hook will be call every time before  func `forward` be invoke      it should have the follow signature            hook module  input  -> none or modify input      the input contain only the positional arguments give to the module      keyword arguments win t be pass to the hook and only to the ``forward``      the hook can modify the input  user can either return a tuple or a     single modify value in the hook  we will wrap the value into a tuple     if a single value be return unless that value be already a tuple        return           class `torch utils hook removablehandle`              a handle that can be use to remove the add hook by call             ``handle remove  ``             handle = hook removablehandle self  forward pre hook      self  forward pre hook handle id  = hook     return handle 
def register full backward hook      self  hook  callable  string   grad t   grad t   union none  tensor     -> removablehandle      r   register a backward hook on the module       the hook will be call every time the gradients with respect to module     input be compute  the hook should have the follow signature            hook module  grad input  grad output  -> tuple tensor  or none      the  attr `grad input` and  attr `grad output` be tuples that contain the gradients     with respect to the input and output respectively  the hook should     not modify its arguments  but it can optionally return a new gradient with     respect to the input that will be use in place of  attr `grad input` in     subsequent computations   attr `grad input` will only correspond to the input give     as positional arguments and all kwarg arguments be ignore  entries     in  attr `grad input` and  attr `grad output` will be ``none`` for all non-tensor     arguments          warn            modify input or output inplace be not allow when use backward hook and         will raise an error       return           class `torch utils hook removablehandle`              a handle that can be use to remove the add hook by call             ``handle remove  ``              if self  be full backward hook be false          raise runtimeerror string                            string       self  be full backward hook = true      handle = hook removablehandle self  backward hook      self  backward hook handle id  = hook     return handle 
def register parameter self  name  str  param  optional parameter   -> none      r   add a parameter to the module       the parameter can be access as an attribute use give name       args          name  string   name of the parameter  the parameter can be access             from this module use the give name         param  parameter   parameter to be add to the module              if string not in self   dict            raise attributeerror              string       elif not isinstance name  torch  six string class           raise typeerror string                         string format torch typename name        elif string in name          raise keyerror string      elif name == string          raise keyerror string      elif hasattr self  name  and name not in self  parameters          raise keyerror string format name        if param be none          self  parameters name  = none     elif not isinstance param  parameter           raise typeerror string                         string                          format torch typename param   name       elif param grad fn          raise valueerror              string             string             string             string format name       else          self  parameters name  = param 
def require grad  self  t  require grad  bool = true  -> t      r   change if autograd should record operations on parameters in this     module       this method set the parameters   attr `requires grad` attribute     in-place       this method be helpful for freeze part of the module for finetuning     or train part of a model individually  e g   gin train        see  ref `locally-disable-grad-doc` for a comparison between     ` require grad   ` and several similar mechanisms that may be confuse with it       args          require grad  bool   whether autograd should record operations on                               parameters in this module  default  ``true``       return          module  self             for p in self parameters            p require grad  require grad      return self 
def state dict self  destination=none  prefix=string  keep vars=false       r   return a dictionary contain a whole state of the module       both parameters and persistent buffer  e g  run average  be     include  key be correspond parameter and buffer name       return          dict              a dictionary contain a whole state of the module      example            >>> module state dict   key             bias    weight                if destination be none          destination = ordereddict           destination  metadata = ordereddict       destination  metadata prefix  -1   = local metadata = dict version=self  version      self  save to state dict destination  prefix  keep vars      for name  module in self  modules items            if module be not none              module state dict destination  prefix   name   string  keep vars=keep vars      for hook in self  state dict hook value            hook result = hook self  destination  prefix  local metadata          if hook result be not none              destination = hook result     return destination 
def to self   args    kwargs       r   move and/or cast the parameters and buffer       this can be call as         function   to device=none  dtype=none  non blocking=false          function   to dtype  non blocking=false          function   to tensor  non blocking=false          function   to memory format=torch channel last       its signature be similar to  meth `torch tensor to`  but only accept     float point or complex  attr `dtype`s  in addition  this method will     only cast the float point or complex parameters and buffer to  attr `dtype`      if give   the integral parameters and buffer will be move      attr `device`  if that be give  but with dtypes unchanged  when      attr `non blocking` be set  it try to convert/move asynchronously     with respect to the host if possible  e g   move cpu tensors with     pin memory to cuda devices       see below for examples          note           this method modify the module in-place       args          device   class `torch device`   the desire device of the parameters             and buffer in this module         dtype   class `torch dtype`   the desire float point or complex dtype of             the parameters and buffer in this module         tensor  torch tensor   tensor whose dtype and device be the desire             dtype and device for all parameters and buffer in this module         memory format   class `torch memory format`   the desire memory             format for 4d parameters and buffer in this module  keyword             only argument       return          module  self      examples            >>> linear = nn linear 2  2          >>> linear weight         parameter contain          tensor    0 1913  -0 3420                    -0 5113  -0 2325            >>> linear to torch double          linear in features=2  out features=2  bias=true          >>> linear weight         parameter contain          tensor    0 1913  -0 3420                    -0 5113  -0 2325    dtype=torch float64          >>> gpu1 = torch device  cuda 1           >>> linear to gpu1  dtype=torch half  non blocking=true          linear in features=2  out features=2  bias=true          >>> linear weight         parameter contain          tensor    0 1914  -0 3420                    -0 5112  -0 2324    dtype=torch float16  device= cuda 1           >>> cpu = torch device  cpu           >>> linear to cpu          linear in features=2  out features=2  bias=true          >>> linear weight         parameter contain          tensor    0 1914  -0 3420                    -0 5112  -0 2324    dtype=torch float16           >>> linear = nn linear 2  2  bias=none  to torch cdouble          >>> linear weight         parameter contain          tensor    0 3741 0 j   0 2382 0 j                     0 5593 0 j  -0 4443 0 j    dtype=torch complex128          >>> linear torch ones 3  2  dtype=torch cdouble           tensor   0 6122 0 j  0 1150 0 j                    0 6122 0 j  0 1150 0 j                    0 6122 0 j  0 1150 0 j    dtype=torch complex128                device  dtype  non block  convert to format = torch  c  nn  parse to  args    kwargs       if dtype be not none          if not  dtype be float point or dtype be complex               raise typeerror string                             string format dtype           if dtype be complex              warn warn                  string                 string                 string                 string       def convert t           if convert to format be not none and t dim   in  4  5               return t to device  dtype if t be float point   or t be complex   else none                          non block  memory format=convert to format          return t to device  dtype if t be float point   or t be complex   else none  non block       return self  apply convert  
def to empty self  t     device  union str  device   -> t      r   move the parameters and buffer to the specify device without copy storage       args          device   class `torch device`   the desire device of the parameters             and buffer in this module       return          module  self             return self  apply lambda t  torch empty like t  device=device   
def train self  t  mode  bool = true  -> t      r   set the module in train mode       this have any effect only on certain modules  see documentations of     particular modules for detail of their behaviors in training/evaluation     mode  if they be affect  e g   class `dropout`   class `batchnorm`      etc       args          mode  bool   whether to set train mode  ``true``  or evaluation                      mode  ``false``   default  ``true``       return          module  self             if not isinstance mode  bool           raise valueerror string      self train = mode     for module in self children            module train mode      return self 
def xpu self  t  device  optional union int  device   = none  -> t      r   move all model parameters and buffer to the xpu       this also make associate parameters and buffer different object  so     it should be call before construct optimizer if the module will     live on xpu while be optimize          note           this method modify the module in-place       arguments          device  int  optional   if specify  all parameters will be             copy to that device      return          module  self             return self  apply lambda t  t xpu device   
class sequential module       r   a sequential container      modules will be add to it in the order they be pass in the     constructor  alternatively  an ``ordereddict`` of modules can be     pass in  the ``forward  `` method of ``sequential`` accept any     input and forward it to the first module it contain  it then      chain  output to input sequentially for each subsequent module      finally return the output of the last module       the value a ``sequential`` provide over manually call a sequence     of modules be that it allow treat the whole container as a     single module  such that perform a transformation on the     ``sequential`` apply to each of the modules it store  which be     each a register submodule of the ``sequential``        what s the difference between a ``sequential`` and a      class `torch nn modulelist`? a ``modulelist`` be exactly what it     sound like--a list for store ``module`` s  on the other hand      the layer in a ``sequential`` be connect in a cascade way       example              use sequential to create a small model  when `model` be run            input will first be pass to `conv2d 1 20 5 `  the output of           `conv2d 1 20 5 ` will be use as the input to the first           `relu`  the output of the first `relu` will become the input           for `conv2d 20 64 5 `  finally  the output of           `conv2d 20 64 5 ` will be use as input to the second `relu`         model = nn sequential                    nn conv2d 1 20 5                     nn relu                      nn conv2d 20 64 5                     nn relu                                use sequential with ordereddict  this be functionally the           same as the above code         model = nn sequential ordereddict                       conv1   nn conv2d 1 20 5                        relu1   nn relu                         conv2   nn conv2d 20 64 5                        relu2   nn relu                                      overload     def   init   self   args  module  -> none                    overload     def   init   self  arg  string  -> none                   def   init   self   args           super sequential  self    init             if len args  == 1 and isinstance args 0   ordereddict               for key  module in args 0  items                    self add module key  module          else              for idx  module in enumerate args                   self add module str idx   module       def  get item by idx self  iterator  idx  -> t          string         size = len self          idx = operator index idx          if not -size <= idx < size              raise indexerror string format idx           idx  = size         return next islice iterator  idx  none          copy to script wrapper     def   getitem   self  idx  -> union string  t           if isinstance idx  slice               return self   class   ordereddict list self  modules items    idx            else              return self  get item by idx self  modules value    idx       def   setitem   self  idx  int  module  module  -> none          key  str = self  get item by idx self  modules key    idx          return setattr self  key  module       def   delitem   self  idx  union slice  int   -> none          if isinstance idx  slice               for key in list self  modules key    idx                   delattr self  key          else              key = self  get item by idx self  modules key    idx              delattr self  key         copy to script wrapper     def   len   self  -> int          return len self  modules         copy to script wrapper     def   dir   self           key = super sequential  self    dir             key =  key for key in key if not key isdigit            return key        copy to script wrapper     def   iter   self  -> iterator module           return iter self  modules value                             def forward self  input           for module in self              input = module input          return input 
class modulelist module       r   hold submodules in a list        class `~torch nn modulelist` can be index like a regular python list  but     modules it contain be properly register  and will be visible by all      class `~torch nn module` methods       args          modules  iterable  optional   an iterable of modules to add      example            class mymodule nn module               def   init   self                   super mymodule  self    init                     self linears = nn modulelist  nn linear 10  10  for i in range 10                 def forward self  x                     modulelist can act as an iterable  or be index use ints                 for i  l in enumerate self linears                       x = self linears i // 2  x    l x                  return x              def   init   self  modules  optional iterable module   = none  -> none          super modulelist  self    init             if modules be not none              self  = modules      def  get abs string index self  idx           string         idx = operator index idx          if not  -len self  <= idx < len self                raise indexerror string format idx           if idx < 0              idx  = len self          return str idx         copy to script wrapper     def   getitem   self  idx  int  -> module          if isinstance idx  slice               return self   class   list self  modules value    idx           else              return self  modules self  get abs string index idx        def   setitem   self  idx  int  module  module  -> none          idx = self  get abs string index idx          return setattr self  str idx   module       def   delitem   self  idx  union int  slice   -> none          if isinstance idx  slice               for k in range len self  modules   idx                   delattr self  str k           else              delattr self  self  get abs string index idx                    str indices =  str i  for i in range len self  modules            self  modules = ordereddict list zip str indices  self  modules value             copy to script wrapper     def   len   self  -> int          return len self  modules         copy to script wrapper     def   iter   self  -> iterator module           return iter self  modules value         def   iadd   self  modules  iterable module   -> string          return self extend modules         copy to script wrapper     def   dir   self           key = super modulelist  self    dir             key =  key for key in key if not key isdigit            return key      def insert self  index  int  module  module  -> none          r   insert a give module before a give index in the list           args              index  int   index to insert              module  nn module   module to insert                     for i in range len self  modules   index  -1               self  modules str i   = self  modules str i - 1           self  modules str index   = module      def append self  module  module  -> string          r   append a give module to the end of the list           args              module  nn module   module to append                     self add module str len self    module          return self      def extend self  modules  iterable module   -> string          r   append modules from a python iterable to the end of the list           args              modules  iterable   iterable of modules to append                     if not isinstance modules  container abcs iterable               raise typeerror string                             string   type modules    name            offset = len self          for i  module in enumerate modules               self add module str offset   i   module          return self 
def append self  module  module  -> string      r   append a give module to the end of the list       args          module  nn module   module to append             self add module str len self    module      return self 
def extend self  modules  iterable module   -> string      r   append modules from a python iterable to the end of the list       args          modules  iterable   iterable of modules to append             if not isinstance modules  container abcs iterable           raise typeerror string                         string   type modules    name        offset = len self      for i  module in enumerate modules           self add module str offset   i   module      return self 
def insert self  index  int  module  module  -> none      r   insert a give module before a give index in the list       args          index  int   index to insert          module  nn module   module to insert             for i in range len self  modules   index  -1           self  modules str i   = self  modules str i - 1       self  modules str index   = module 
class moduledict module       r   hold submodules in a dictionary        class `~torch nn moduledict` can be index like a regular python dictionary      but modules it contain be properly register  and will be visible by all      class `~torch nn module` methods        class `~torch nn moduledict` be an   order   dictionary that respect        the order of insertion  and        in  meth `~torch nn moduledict update`  the order of the merge       ``ordereddict``  ``dict``  start from python 3 6  or another        class `~torch nn moduledict`  the argument to        meth `~torch nn moduledict update`        note that  meth `~torch nn moduledict update` with other unordered map     type  e g   python s plain ``dict`` before python version 3 6  do not     preserve the order of the merge map       args          modules  iterable  optional   a map  dictionary  of  string  module              or an iterable of key-value pair of type  string  module       example            class mymodule nn module               def   init   self                   super mymodule  self    init                     self choices = nn moduledict                            conv   nn conv2d 10  10  3                            pool   nn maxpool2d 3                                     self activations = nn moduledict                             lrelu   nn leakyrelu                               prelu   nn prelu                                    def forward self  x  choice  act                   x = self choices choice  x                  x = self activations act  x                  return x              def   init   self  modules  optional map str  module   = none  -> none          super moduledict  self    init             if modules be not none              self update modules         copy to script wrapper     def   getitem   self  key  str  -> module          return self  modules key       def   setitem   self  key  str  module  module  -> none          self add module key  module       def   delitem   self  key  str  -> none          del self  modules key         copy to script wrapper     def   len   self  -> int          return len self  modules         copy to script wrapper     def   iter   self  -> iterator str           return iter self  modules         copy to script wrapper     def   contain   self  key  str  -> bool          return key in self  modules      def clear self  -> none          string         self  modules clear        def pop self  key  str  -> module          r   remove key from the moduledict and return its module           args              key  string   key to pop from the moduledict                     v = self key          del self key          return v        copy to script wrapper     def key self  -> iterable str           r   return an iterable of the moduledict key                      return self  modules key          copy to script wrapper     def items self  -> iterable tuple str  module            r   return an iterable of the moduledict key/value pair                      return self  modules items          copy to script wrapper     def value self  -> iterable module           r   return an iterable of the moduledict value                      return self  modules value        def update self  modules  map str  module   -> none          r   update the  class `~torch nn moduledict` with the key-value pair from a         map or an iterable  overwrite exist key              note               if  attr `modules` be an ``ordereddict``  a  class `~torch nn moduledict`  or             an iterable of key-value pair  the order of new elements in it be preserve           args              modules  iterable   a map  dictionary  from string to  class `~torch nn module`                  or an iterable of key-value pair of type  string   class `~torch nn module`                      if not isinstance modules  container abcs iterable               raise typeerror string                             string                               type modules    name             if isinstance modules   ordereddict  moduledict  container abcs map                for key  module in modules items                    self key  = module         else                           for j  m in enumerate modules                   if not isinstance m  container abcs iterable                       raise typeerror string                                     string   str j    string                                       type m    name                    if not len m  == 2                      raise valueerror string                                      string   str j    string   str len m                                          string                                                    self m 0   = m 1    
def pop self  key  str  -> module      r   remove key from the moduledict and return its module       args          key  string   key to pop from the moduledict             v = self key      del self key      return v 
class parameterlist module       r   hold parameters in a list        class `~torch nn parameterlist` can be index like a regular python     list  but parameters it contain be properly register  and will be     visible by all  class `~torch nn module` methods       args          parameters  iterable  optional   an iterable of  class `~torch nn parameter` to add      example            class mymodule nn module               def   init   self                   super mymodule  self    init                     self params = nn parameterlist  nn parameter torch randn 10  10   for i in range 10                 def forward self  x                     parameterlist can act as an iterable  or be index use ints                 for i  p in enumerate self params                       x = self params i // 2  mm x    p mm x                  return x              def   init   self  parameters  optional iterable string   = none  -> none          super parameterlist  self    init             self  initialize = true         if parameters be not none              self  = parameters      def   setstate   self  state           state string  = false         super parameterlist  self    setstate   state          self  initialize = true      def  get abs string index self  idx           string         idx = operator index idx          if not  -len self  <= idx < len self                raise indexerror string format idx           if idx < 0              idx  = len self          return str idx        overload     def   getitem   self  idx  int  -> string                    overload     def   getitem   self  t  idx  slice  -> t                   def   getitem   self  idx           if isinstance idx  slice               return self   class   list self  parameters value    idx           else              idx = self  get abs string index idx              return self  parameters str idx        def   setitem   self  idx  int  param  string  -> none          idx = self  get abs string index idx          return self register parameter str idx   param       def   setattr   self  key  any  value  any  -> none          if getattr self  string  false               if not hasattr self  key  and not isinstance value  torch nn parameter                   warn warn string          super parameterlist  self    setattr   key  value       def   len   self  -> int          return len self  parameters       def   iter   self  -> iterator string           return iter self  parameters value         def   iadd   self  parameters  iterable string   -> string          return self extend parameters       def   dir   self           key = super parameterlist  self    dir             key =  key for key in key if not key isdigit            return key      def append self  parameter  string  -> string          string         self register parameter str len self    parameter          return self      def extend self  parameters  iterable string   -> string          string         if not isinstance parameters  container abcs iterable               raise typeerror string                             string   type parameters    name            offset = len self          for i  param in enumerate parameters               self register parameter str offset   i   param          return self      def extra repr self  -> str          child line =            for k  p in self  parameters items                size str = string join str size  for size in p size                device str = string if not p be cuda else string format p get device                parastr = string format                  torch typename p   size str  device str              child line append string   str k    string   parastr          tmpstr = string join child line          return tmpstr      def   call   self  input           raise runtimeerror string       def  replicate for data parallel self           warn warn string                       string                       string           return super parameterlist  self   replicate for data parallel   
def append self  parameter  string  -> string      string     self register parameter str len self    parameter      return self 
def extend self  parameters  iterable string   -> string      string     if not isinstance parameters  container abcs iterable           raise typeerror string                         string   type parameters    name        offset = len self      for i  param in enumerate parameters           self register parameter str offset   i   param      return self 
class parameterdict module       r   hold parameters in a dictionary       parameterdict can be index like a regular python dictionary  but parameters it     contain be properly register  and will be visible by all module methods        class `~torch nn parameterdict` be an   order   dictionary that respect        the order of insertion  and        in  meth `~torch nn parameterdict update`  the order of the merge ``ordereddict``       or another  class `~torch nn parameterdict`  the argument to        meth `~torch nn parameterdict update`        note that  meth `~torch nn parameterdict update` with other unordered map     type  e g   python s plain ``dict``  do not preserve the order of the     merge map       args          parameters  iterable  optional   a map  dictionary  of              string    class `~torch nn parameter`  or an iterable of key-value pair             of type  string   class `~torch nn parameter`       example            class mymodule nn module               def   init   self                   super mymodule  self    init                     self params = nn parameterdict                            leave   nn parameter torch randn 5  10                             right   nn parameter torch randn 5  10                                   def forward self  x  choice                   x = self params choice  mm x                  return x              def   init   self  parameters  optional map str  string   = none  -> none          super parameterdict  self    init             self  initialize = true         if parameters be not none              self update parameters       def   setstate   self  state           state string  = false         super parameterdict  self    setstate   state          self  initialize = true      def   getitem   self  key  str  -> string          return self  parameters key       def   setitem   self  key  str  parameter  string  -> none          self register parameter key  parameter       def   delitem   self  key  str  -> none          del self  parameters key       def   setattr   self  key  any  value  any  -> none          if getattr self  string  false               if not hasattr self  key  and not isinstance value  torch nn parameter                   warn warn string          super parameterdict  self    setattr   key  value       def   len   self  -> int          return len self  parameters       def   iter   self  -> iterator str           return iter self  parameters key         def   contain   self  key  str  -> bool          return key in self  parameters      def clear self  -> none          string         self  parameters clear        def pop self  key  str  -> string          r   remove key from the parameterdict and return its parameter           args              key  string   key to pop from the parameterdict                     v = self key          del self key          return v      def key self  -> iterable str           r   return an iterable of the parameterdict key                      return self  parameters key        def items self  -> iterable tuple str  string            r   return an iterable of the parameterdict key/value pair                      return self  parameters items        def value self  -> iterable string           r   return an iterable of the parameterdict value                      return self  parameters value        def update self  parameters  map str  string   -> none          r   update the  class `~torch nn parameterdict` with the key-value pair from a         map or an iterable  overwrite exist key              note               if  attr `parameters` be an ``ordereddict``  a  class `~torch nn parameterdict`  or             an iterable of key-value pair  the order of new elements in it be preserve           args              parameters  iterable   a map  dictionary  from string to                  class `~torch nn parameter`  or an iterable of                 key-value pair of type  string   class `~torch nn parameter`                      if not isinstance parameters  container abcs iterable               raise typeerror string                             string                               type parameters    name             if isinstance parameters   ordereddict  parameterdict                for key  parameter in parameters items                    self key  = parameter         elif isinstance parameters  container abcs map               for key  parameter in sort parameters items                     self key  = parameter         else              for j  p in enumerate parameters                   if not isinstance p  container abcs iterable                       raise typeerror string                                     string   str j    string                                       type p    name                    if not len p  == 2                      raise valueerror string                                      string   str j    string   str len p                                          string                                   self p 0   = p 1         def extra repr self  -> str          child line =            for k  p in self  parameters items                size str = string join str size  for size in p size                device str = string if not p be cuda else string format p get device                parastr = string format                  torch typename p   size str  device str              child line append string   k   string   parastr          tmpstr = string join child line          return tmpstr      def   call   self  input           raise runtimeerror string       def  replicate for data parallel self           warn warn string                       string                       string           return super parameterdict  self   replicate for data parallel   
def pop self  key  str  -> string      r   remove key from the parameterdict and return its parameter       args          key  string   key to pop from the parameterdict             v = self key      del self key      return v 
def register module forward pre hook hook  callable      none   -> removablehandle      r   register a forward pre-hook common to all modules          warn             this add global state to the `nn module` module         and it be only intend for debugging/profiling purpose       the hook will be call every time before  func `forward` be invoke      it should have the follow signature            hook module  input  -> none or modify input      the input contain only the positional arguments give to the module      keyword arguments win t be pass to the hook and only to the ``forward``      the hook can modify the input  user can either return a tuple or a     single modify value in the hook  we will wrap the value into a tuple     if a single value be return unless that value be already a tuple        this hook have precedence over the specific module hook register with     ``register forward pre hook``       return           class `torch utils hook removablehandle`              a handle that can be use to remove the add hook by call             ``handle remove  ``             handle = hook removablehandle  global forward pre hook       global forward pre hook handle id  = hook     return handle 
def register module forward hook hook  callable      none   -> removablehandle      r   register a global forward hook for all the modules         warn             this add global state to the `nn module` module         and it be only intend for debugging/profiling purpose       the hook will be call every time after  func `forward` have compute an output      it should have the follow signature            hook module  input  output  -> none or modify output      the input contain only the positional arguments give to the module      keyword arguments win t be pass to the hook and only to the ``forward``      the hook can modify the output  it can modify the input inplace but     it will not have effect on forward since this be call after      func `forward` be call       return           class `torch utils hook removablehandle`              a handle that can be use to remove the add hook by call             ``handle remove  ``      this hook will be execute before specific module hook register with     ``register forward hook``              handle = hook removablehandle  global forward hook       global forward hook handle id  = hook     return handle 
def register module backward hook      hook  callable  string   grad t   grad t   union none  tensor     -> removablehandle      r   register a backward hook common to all the modules       this function be deprecate in favor of  meth `nn module register module full backward hook`     and the behavior of this function will change in future versions       return           class `torch utils hook removablehandle`              a handle that can be use to remove the add hook by call             ``handle remove  ``              global  global be full backward hook     if  global be full backward hook be true          raise runtimeerror string                            string        global be full backward hook = false      handle = hook removablehandle  global backward hook       global backward hook handle id  = hook     return handle 
def register module full backward hook      hook  callable  string   grad t   grad t   union none  tensor     -> removablehandle      r   register a backward hook common to all the modules          warn            this add global state to the `nn module` module         and it be only intend for debugging/profiling purpose           the current implementation will not have the present behavior         for complex  class `module` that perform many operations          in some failure case   attr `grad input` and  attr `grad output` will only         contain the gradients for a subset of the input and output          for such  class `module`  you should use  func `torch tensor register hook`         directly on a specific input or output to get the require gradients       the hook will be call every time the gradients with respect to module     input be compute  the hook should have the follow signature            hook module  grad input  grad output  -> tensor or none      the  attr `grad input` and  attr `grad output` be tuples  the hook should     not modify its arguments  but it can optionally return a new gradient with     respect to the input that will be use in place of  attr `grad input` in     subsequent computations   attr `grad input` will only correspond to the input give     as positional arguments and all kwarg arguments will not appear in the hook  entries     in  attr `grad input` and  attr `grad output` will be ``none`` for all non-tensor     arguments       global hook be call before hook register with `register backward hook`      return           class `torch utils hook removablehandle`              a handle that can be use to remove the add hook by call             ``handle remove  ``              global  global be full backward hook     if  global be full backward hook be false          raise runtimeerror string                            string        global be full backward hook = true      handle = hook removablehandle  global backward hook       global backward hook handle id  = hook     return handle 
class conv1d  convnd         doc   = r   apply a 1d convolution over an input signal compose of several input     plan       in the simplest case  the output value of the layer with input size      math ` n  c  \text in    l ` and output  math ` n  c  \text out    l  \text out   ` can be     precisely describe as          math           \text out  n i  c  \text out  j   = \text bias  c  \text out  j             \sum  k = 0   c  in  - 1  \text weight  c  \text out  j   k          \star \text input  n i  k       where  math `\star` be the valid `cross-correlation`  operator       math `n` be a batch size   math `c` denote a number of channel       math `l` be a length of signal sequence            r         this module support  ref `tensorfloat32<tf32 on ampere>`          attr `stride` control the stride for the cross-correlation  a single       number or a one-element tuple          attr `padding` control the amount of pad apply to the input  it       can be either a string    valid    same    or a tuple of ints give the       amount of implicit pad apply on both side          attr `dilation` control the space between the kernel point  also       know as the  trous algorithm  it be harder to describe  but this `link`        have a nice visualization of what  attr `dilation` do        group note       note           depthwise separable note      note           cudnn reproducibility note       note          ``padding= valid `` be the same as no pad  ``padding= same `` pad         the input so the output have the shape as the input  however  this mode         doesn t support any stride value other than 1       args          in channel  int   number of channel in the input image         out channel  int   number of channel produce by the convolution         kernel size  int or tuple   size of the convolve kernel         stride  int or tuple  optional   stride of the convolution  default  1         pad  int  tuple or str  optional   pad add to both side of             the input  default  0         pad mode  string  optional   `` zero ``  `` reflect ``              `` replicate `` or `` circular ``  default  `` zero ``         dilation  int or tuple  optional   space between kernel             elements  default  1         group  int  optional   number of block connections from input             channel to output channel  default  1         bias  bool  optional   if ``true``  add a learnable bias to the             output  default  ``true``          format   reproducibility note    convolution note    r         shape          - input   math ` n  c  in   l  in  `         - output   math ` n  c  out   l  out  ` where               math                 l  out  = \left\lfloor\frac l  in    2 \times \text pad  - \text dilation                          \times  \text kernel\ size  - 1  - 1  \text stride     1\right\rfloor      attribute          weight  tensor   the learnable weight of the module of shape              math ` \text out\ channel               \frac \text in\ channel   \text group    \text kernel\ size  `              the value of these weight be sample from              math `\mathcal u  -\sqrt k   \sqrt k  ` where              math `k = \frac group  c \text in    \text kernel\ size  `         bias  tensor     the learnable bias of the module of shape              out channel   if  attr `bias` be ``true``  then the value of these weight be             sample from  math `\mathcal u  -\sqrt k   \sqrt k  ` where              math `k = \frac group  c \text in    \text kernel\ size  `      examples            >>> m = nn conv1d 16  33  3  stride=2          >>> input = torch randn 20  16  50          >>> output = m input           cross-correlation          https //en wikipedia org/wiki/cross-correlation          link          https //github com/vdumoulin/conv arithmetic/blob/master/readme md              def   init            self          in channel  int          out channel  int          kernel size   size 1 t          stride   size 1 t = 1          pad  union str   size 1 t  = 0          dilation   size 1 t = 1          group  int = 1          bias  bool = true          pad mode  str = string            device=none          dtype=none       -> none          factory kwargs =  string  device  string  dtype                            kernel size  =  single kernel size          stride  =  single stride          pad  = pad if isinstance pad  str  else  single pad          dilation  =  single dilation          super conv1d  self    init                in channel  out channel  kernel size   stride   pad   dilation               false   single 0   group  bias  pad mode    factory kwargs       def  conv forward self  input  tensor  weight  tensor  bias  optional tensor            if self pad mode  = string              return f conv1d f pad input  self  reverse pad repeat twice  mode=self pad mode                               weight  bias  self stride                               single 0   self dilation  self group          return f conv1d input  weight  bias  self stride                          self pad  self dilation  self group       def forward self  input  tensor  -> tensor          return self  conv forward input  self weight  self bias  
class conv2d  convnd         doc   = r   apply a 2d convolution over an input signal compose of several input     plan       in the simplest case  the output value of the layer with input size      math ` n  c  \text in    h  w ` and output  math ` n  c  \text out    h  \text out    w  \text out   `     can be precisely describe as          math           \text out  n i  c  \text out  j   = \text bias  c  \text out  j             \sum  k = 0   c  \text in   - 1  \text weight  c  \text out  j   k  \star \text input  n i  k        where  math `\star` be the valid 2d `cross-correlation`  operator       math `n` be a batch size   math `c` denote a number of channel       math `h` be a height of input plan in pixels  and  math `w` be     width in pixels            r         this module support  ref `tensorfloat32<tf32 on ampere>`          attr `stride` control the stride for the cross-correlation  a single       number or a tuple          attr `padding` control the amount of pad apply to the input  it       can be either a string    valid    same    or a tuple of ints give the       amount of implicit pad apply on both side          attr `dilation` control the space between the kernel point  also       know as the  trous algorithm  it be harder to describe  but this `link`        have a nice visualization of what  attr `dilation` do        group note       the parameters  attr `kernel size`   attr `stride`   attr `padding`   attr `dilation` can either be           - a single ``int`` -- in which case the same value be use for the height and width dimension         - a ``tuple`` of two ints -- in which case  the first `int` be use for the height dimension            and the second `int` for the width dimension      note           depthwise separable note       note           cudnn reproducibility note       note          ``padding= valid `` be the same as no pad  ``padding= same `` pad         the input so the output have the shape as the input  however  this mode         doesn t support any stride value other than 1       args          in channel  int   number of channel in the input image         out channel  int   number of channel produce by the convolution         kernel size  int or tuple   size of the convolve kernel         stride  int or tuple  optional   stride of the convolution  default  1         pad  int  tuple or str  optional   pad add to all four side of             the input  default  0         pad mode  string  optional   `` zero ``  `` reflect ``              `` replicate `` or `` circular ``  default  `` zero ``         dilation  int or tuple  optional   space between kernel elements  default  1         group  int  optional   number of block connections from input             channel to output channel  default  1         bias  bool  optional   if ``true``  add a learnable bias to the             output  default  ``true``         format   reproducibility note    convolution note    r         shape          - input   math ` n  c  in   h  in   w  in  `         - output   math ` n  c  out   h  out   w  out  ` where               math                 h  out  = \left\lfloor\frac h  in     2 \times \text pad  0  - \text dilation  0                          \times  \text kernel\ size  0  - 1  - 1  \text stride  0     1\right\rfloor               math                 w  out  = \left\lfloor\frac w  in     2 \times \text pad  1  - \text dilation  1                          \times  \text kernel\ size  1  - 1  - 1  \text stride  1     1\right\rfloor      attribute          weight  tensor   the learnable weight of the module of shape              math ` \text out\ channel   \frac \text in\ channel   \text group   `              math `\text kernel\ size 0    \text kernel\ size 1   `              the value of these weight be sample from              math `\mathcal u  -\sqrt k   \sqrt k  ` where              math `k = \frac group  c \text in    \prod  i=0   1 \text kernel\ size  i  `         bias  tensor     the learnable bias of the module of shape              out channel   if  attr `bias` be ``true``              then the value of these weight be             sample from  math `\mathcal u  -\sqrt k   \sqrt k  ` where              math `k = \frac group  c \text in    \prod  i=0   1 \text kernel\ size  i  `      examples           >>>   with square kernels and equal stride         >>> m = nn conv2d 16  33  3  stride=2          >>>   non-square kernels and unequal stride and with pad         >>> m = nn conv2d 16  33   3  5   stride= 2  1   padding= 4  2           >>>   non-square kernels and unequal stride and with pad and dilation         >>> m = nn conv2d 16  33   3  5   stride= 2  1   padding= 4  2   dilation= 3  1           >>> input = torch randn 20  16  50  100          >>> output = m input           cross-correlation          https //en wikipedia org/wiki/cross-correlation          link          https //github com/vdumoulin/conv arithmetic/blob/master/readme md              def   init            self          in channel  int          out channel  int          kernel size   size 2 t          stride   size 2 t = 1          pad  union str   size 2 t  = 0          dilation   size 2 t = 1          group  int = 1          bias  bool = true          pad mode  str = string            device=none          dtype=none       -> none          factory kwargs =  string  device  string  dtype          kernel size  =  pair kernel size          stride  =  pair stride          pad  = pad if isinstance pad  str  else  pair pad          dilation  =  pair dilation          super conv2d  self    init                in channel  out channel  kernel size   stride   pad   dilation               false   pair 0   group  bias  pad mode    factory kwargs       def  conv forward self  input  tensor  weight  tensor  bias  optional tensor            if self pad mode  = string              return f conv2d f pad input  self  reverse pad repeat twice  mode=self pad mode                               weight  bias  self stride                               pair 0   self dilation  self group          return f conv2d input  weight  bias  self stride                          self pad  self dilation  self group       def forward self  input  tensor  -> tensor          return self  conv forward input  self weight  self bias  
class conv3d  convnd         doc   = r   apply a 3d convolution over an input signal compose of several input     plan       in the simplest case  the output value of the layer with input size  math ` n  c  in   d  h  w `     and output  math ` n  c  out   d  out   h  out   w  out  ` can be precisely describe as          math           out n i  c  out j   = bias c  out j                                     \sum  k = 0   c  in  - 1  weight c  out j   k  \star input n i  k       where  math `\star` be the valid 3d `cross-correlation`  operator           r         this module support  ref `tensorfloat32<tf32 on ampere>`          attr `stride` control the stride for the cross-correlation          attr `padding` control the amount of pad apply to the input  it       can be either a string    valid    same    or a tuple of ints give the       amount of implicit pad apply on both side          attr `dilation` control the space between the kernel point  also know as the  trous algorithm        it be harder to describe  but this `link`  have a nice visualization of what  attr `dilation` do        group note       the parameters  attr `kernel size`   attr `stride`   attr `padding`   attr `dilation` can either be           - a single ``int`` -- in which case the same value be use for the depth  height and width dimension         - a ``tuple`` of three ints -- in which case  the first `int` be use for the depth dimension            the second `int` for the height dimension and the third `int` for the width dimension      note           depthwise separable note       note           cudnn reproducibility note       note          ``padding= valid `` be the same as no pad  ``padding= same `` pad         the input so the output have the shape as the input  however  this mode         doesn t support any stride value other than 1       args          in channel  int   number of channel in the input image         out channel  int   number of channel produce by the convolution         kernel size  int or tuple   size of the convolve kernel         stride  int or tuple  optional   stride of the convolution  default  1         pad  int  tuple or str  optional   pad add to all six side of             the input  default  0         pad mode  string  optional   `` zero ``  `` reflect ``  `` replicate `` or `` circular ``  default  `` zero ``         dilation  int or tuple  optional   space between kernel elements  default  1         group  int  optional   number of block connections from input channel to output channel  default  1         bias  bool  optional   if ``true``  add a learnable bias to the output  default  ``true``         format   reproducibility note    convolution note    r         shape          - input   math ` n  c  in   d  in   h  in   w  in  `         - output   math ` n  c  out   d  out   h  out   w  out  ` where               math                 d  out  = \left\lfloor\frac d  in    2 \times \text pad  0  - \text dilation  0                      \times  \text kernel\ size  0  - 1  - 1  \text stride  0     1\right\rfloor               math                 h  out  = \left\lfloor\frac h  in    2 \times \text pad  1  - \text dilation  1                      \times  \text kernel\ size  1  - 1  - 1  \text stride  1     1\right\rfloor               math                 w  out  = \left\lfloor\frac w  in    2 \times \text pad  2  - \text dilation  2                      \times  \text kernel\ size  2  - 1  - 1  \text stride  2     1\right\rfloor      attribute          weight  tensor   the learnable weight of the module of shape                           math ` \text out\ channel   \frac \text in\ channel   \text group   `                           math `\text kernel\ size 0    \text kernel\ size 1    \text kernel\ size 2   `                           the value of these weight be sample from                           math `\mathcal u  -\sqrt k   \sqrt k  ` where                           math `k = \frac group  c \text in    \prod  i=0   2 \text kernel\ size  i  `         bias  tensor     the learnable bias of the module of shape  out channel   if  attr `bias` be ``true``                           then the value of these weight be                          sample from  math `\mathcal u  -\sqrt k   \sqrt k  ` where                           math `k = \frac group  c \text in    \prod  i=0   2 \text kernel\ size  i  `      examples            >>>   with square kernels and equal stride         >>> m = nn conv3d 16  33  3  stride=2          >>>   non-square kernels and unequal stride and with pad         >>> m = nn conv3d 16  33   3  5  2   stride= 2  1  1   padding= 4  2  0           >>> input = torch randn 20  16  10  50  100          >>> output = m input           cross-correlation          https //en wikipedia org/wiki/cross-correlation          link          https //github com/vdumoulin/conv arithmetic/blob/master/readme md              def   init            self          in channel  int          out channel  int          kernel size   size 3 t          stride   size 3 t = 1          pad  union str   size 3 t  = 0          dilation   size 3 t = 1          group  int = 1          bias  bool = true          pad mode  str = string          device=none          dtype=none       -> none          factory kwargs =  string  device  string  dtype          kernel size  =  triple kernel size          stride  =  triple stride          pad  = pad if isinstance pad  str  else  triple pad          dilation  =  triple dilation          super conv3d  self    init                in channel  out channel  kernel size   stride   pad   dilation               false   triple 0   group  bias  pad mode    factory kwargs       def  conv forward self  input  tensor  weight  tensor  bias  optional tensor            if self pad mode  = string              return f conv3d                  f pad                      input  self  reverse pad repeat twice  mode=self pad mode                                    weight                  bias                  self stride                   triple 0                   self dilation                  self group                        return f conv3d              input  weight  bias  self stride  self pad  self dilation  self group                def forward self  input  tensor  -> tensor          return self  conv forward input  self weight  self bias  
class convtranspose1d  convtransposend         doc   = r   apply a 1d transpose convolution operator over an input image     compose of several input plan       this module can be see as the gradient of conv1d with respect to its input      it be also know as a fractionally-strided convolution or     a deconvolution  although it be not an actual deconvolution operation        this module support  ref `tensorfloat32<tf32 on ampere>`          attr `stride` control the stride for the cross-correlation          attr `padding` control the amount of implicit zero pad on both       side for ``dilation    kernel size - 1  - padding`` number of point  see note       below for detail          attr `output padding` control the additional size add to one side       of the output shape  see note below for detail          attr `dilation` control the space between the kernel point  also know as the  trous algorithm        it be harder to describe  but this `link`  have a nice visualization of what  attr `dilation` do        group note       note          the  attr `padding` argument effectively add ``dilation    kernel size - 1  - padding``         amount of zero pad to both size of the input  this be set so that         when a  class `~torch nn conv1d` and a  class `~torch nn convtranspose1d`         be initialize with same parameters  they be inverses of each other in         regard to the input and output shape  however  when ``stride > 1``           class `~torch nn conv1d` map multiple input shape to the same output         shape   attr `output padding` be provide to resolve this ambiguity by         effectively increase the calculate output shape on one side  note         that  attr `output padding` be only use to find output shape  but do         not actually add zero-padding to output       note          in some circumstances when use the cuda backend with cudnn  this operator         may select a nondeterministic algorithm to increase performance  if this be         undesirable  you can try to make the operation deterministic  potentially at         a performance cost  by set ``torch backends cudnn deterministic =         true``          please see the note on  doc `/notes/randomness` for background        args          in channel  int   number of channel in the input image         out channel  int   number of channel produce by the convolution         kernel size  int or tuple   size of the convolve kernel         stride  int or tuple  optional   stride of the convolution  default  1         pad  int or tuple  optional   ``dilation    kernel size - 1  - padding`` zero-padding             will be add to both side of the input  default  0         output pad  int or tuple  optional   additional size add to one side             of the output shape  default  0         group  int  optional   number of block connections from input channel to output channel  default  1         bias  bool  optional   if ``true``  add a learnable bias to the output  default  ``true``         dilation  int or tuple  optional   space between kernel elements  default  1         format   reproducibility note    convolution note    r         shape          - input   math ` n  c  in   l  in  `         - output   math ` n  c  out   l  out  ` where               math                 l  out  =  l  in  - 1  \times \text stride  - 2 \times \text pad    \text dilation                          \times  \text kernel\ size  - 1    \text output\ pad    1      attribute          weight  tensor   the learnable weight of the module of shape                           math ` \text in\ channel   \frac \text out\ channel   \text group   `                           math `\text kernel\ size  `                           the value of these weight be sample from                           math `\mathcal u  -\sqrt k   \sqrt k  ` where                           math `k = \frac group  c \text out    \text kernel\ size  `         bias  tensor     the learnable bias of the module of shape  out channel                            if  attr `bias` be ``true``  then the value of these weight be                          sample from  math `\mathcal u  -\sqrt k   \sqrt k  ` where                           math `k = \frac group  c \text out    \text kernel\ size  `          cross-correlation          https //en wikipedia org/wiki/cross-correlation          link          https //github com/vdumoulin/conv arithmetic/blob/master/readme md              def   init            self          in channel  int          out channel  int          kernel size   size 1 t          stride   size 1 t = 1          pad   size 1 t = 0          output pad   size 1 t = 0          group  int = 1          bias  bool = true          dilation   size 1 t = 1          pad mode  str = string          device=none          dtype=none       -> none          factory kwargs =  string  device  string  dtype          kernel size =  single kernel size          stride =  single stride          pad =  single pad          dilation =  single dilation          output pad =  single output pad          super convtranspose1d  self    init                in channel  out channel  kernel size  stride  pad  dilation              true  output pad  group  bias  pad mode    factory kwargs       def forward self  input  tensor  output size  optional list int   = none  -> tensor          if self pad mode  = string              raise valueerror string           assert isinstance self pad  tuple                            output pad = self  output pad              input  output size  self stride  self pad  self kernel size  self dilation            return f conv transpose1d              input  self weight  self bias  self stride  self pad              output pad  self group  self dilation  
class convtranspose2d  convtransposend         doc   = r   apply a 2d transpose convolution operator over an input image     compose of several input plan       this module can be see as the gradient of conv2d with respect to its input      it be also know as a fractionally-strided convolution or     a deconvolution  although it be not an actual deconvolution operation        this module support  ref `tensorfloat32<tf32 on ampere>`          attr `stride` control the stride for the cross-correlation          attr `padding` control the amount of implicit zero pad on both       side for ``dilation    kernel size - 1  - padding`` number of point  see note       below for detail          attr `output padding` control the additional size add to one side       of the output shape  see note below for detail          attr `dilation` control the space between the kernel point  also know as the  trous algorithm        it be harder to describe  but this `link`  have a nice visualization of what  attr `dilation` do        group note       the parameters  attr `kernel size`   attr `stride`   attr `padding`   attr `output padding`     can either be           - a single ``int`` -- in which case the same value be use for the height and width dimension         - a ``tuple`` of two ints -- in which case  the first `int` be use for the height dimension            and the second `int` for the width dimension      note          the  attr `padding` argument effectively add ``dilation    kernel size - 1  - padding``         amount of zero pad to both size of the input  this be set so that         when a  class `~torch nn conv2d` and a  class `~torch nn convtranspose2d`         be initialize with same parameters  they be inverses of each other in         regard to the input and output shape  however  when ``stride > 1``           class `~torch nn conv2d` map multiple input shape to the same output         shape   attr `output padding` be provide to resolve this ambiguity by         effectively increase the calculate output shape on one side  note         that  attr `output padding` be only use to find output shape  but do         not actually add zero-padding to output       note           cudnn reproducibility note       args          in channel  int   number of channel in the input image         out channel  int   number of channel produce by the convolution         kernel size  int or tuple   size of the convolve kernel         stride  int or tuple  optional   stride of the convolution  default  1         pad  int or tuple  optional   ``dilation    kernel size - 1  - padding`` zero-padding             will be add to both side of each dimension in the input  default  0         output pad  int or tuple  optional   additional size add to one side             of each dimension in the output shape  default  0         group  int  optional   number of block connections from input channel to output channel  default  1         bias  bool  optional   if ``true``  add a learnable bias to the output  default  ``true``         dilation  int or tuple  optional   space between kernel elements  default  1         format   reproducibility note    convolution note    r         shape          - input   math ` n  c  in   h  in   w  in  `         - output   math ` n  c  out   h  out   w  out  ` where             math                 h  out  =  h  in  - 1  \times \text stride  0  - 2 \times \text pad  0    \text dilation  0                          \times  \text kernel\ size  0  - 1    \text output\ pad  0    1            math                 w  out  =  w  in  - 1  \times \text stride  1  - 2 \times \text pad  1    \text dilation  1                          \times  \text kernel\ size  1  - 1    \text output\ pad  1    1      attribute          weight  tensor   the learnable weight of the module of shape                           math ` \text in\ channel   \frac \text out\ channel   \text group   `                           math `\text kernel\ size 0    \text kernel\ size 1   `                           the value of these weight be sample from                           math `\mathcal u  -\sqrt k   \sqrt k  ` where                           math `k = \frac group  c \text out    \prod  i=0   1 \text kernel\ size  i  `         bias  tensor     the learnable bias of the module of shape  out channel                           if  attr `bias` be ``true``  then the value of these weight be                          sample from  math `\mathcal u  -\sqrt k   \sqrt k  ` where                           math `k = \frac group  c \text out    \prod  i=0   1 \text kernel\ size  i  `      examples            >>>   with square kernels and equal stride         >>> m = nn convtranspose2d 16  33  3  stride=2          >>>   non-square kernels and unequal stride and with pad         >>> m = nn convtranspose2d 16  33   3  5   stride= 2  1   padding= 4  2           >>> input = torch randn 20  16  50  100          >>> output = m input          >>>   exact output size can be also specify as an argument         >>> input = torch randn 1  16  12  12          >>> downsample = nn conv2d 16  16  3  stride=2  padding=1          >>> upsample = nn convtranspose2d 16  16  3  stride=2  padding=1          >>> h = downsample input          >>> h size           torch size  1  16  6  6           >>> output = upsample h  output size=input size            >>> output size           torch size  1  16  12  12            cross-correlation          https //en wikipedia org/wiki/cross-correlation          link          https //github com/vdumoulin/conv arithmetic/blob/master/readme md              def   init            self          in channel  int          out channel  int          kernel size   size 2 t          stride   size 2 t = 1          pad   size 2 t = 0          output pad   size 2 t = 0          group  int = 1          bias  bool = true          dilation  int = 1          pad mode  str = string          device=none          dtype=none       -> none          factory kwargs =  string  device  string  dtype          kernel size =  pair kernel size          stride =  pair stride          pad =  pair pad          dilation =  pair dilation          output pad =  pair output pad          super convtranspose2d  self    init                in channel  out channel  kernel size  stride  pad  dilation              true  output pad  group  bias  pad mode    factory kwargs       def forward self  input  tensor  output size  optional list int   = none  -> tensor          if self pad mode  = string              raise valueerror string           assert isinstance self pad  tuple                            output pad = self  output pad              input  output size  self stride  self pad  self kernel size  self dilation             return f conv transpose2d              input  self weight  self bias  self stride  self pad              output pad  self group  self dilation  
class convtranspose3d  convtransposend         doc   = r   apply a 3d transpose convolution operator over an input image compose of several input     plan      the transpose convolution operator multiply each input value element-wise by a learnable kernel      and sum over the output from all input feature plan       this module can be see as the gradient of conv3d with respect to its input      it be also know as a fractionally-strided convolution or     a deconvolution  although it be not an actual deconvolution operation        this module support  ref `tensorfloat32<tf32 on ampere>`          attr `stride` control the stride for the cross-correlation          attr `padding` control the amount of implicit zero pad on both       side for ``dilation    kernel size - 1  - padding`` number of point  see note       below for detail          attr `output padding` control the additional size add to one side       of the output shape  see note below for detail          attr `dilation` control the space between the kernel point  also know as the  trous algorithm        it be harder to describe  but this `link`  have a nice visualization of what  attr `dilation` do        group note       the parameters  attr `kernel size`   attr `stride`   attr `padding`   attr `output padding`     can either be           - a single ``int`` -- in which case the same value be use for the depth  height and width dimension         - a ``tuple`` of three ints -- in which case  the first `int` be use for the depth dimension            the second `int` for the height dimension and the third `int` for the width dimension      note          the  attr `padding` argument effectively add ``dilation    kernel size - 1  - padding``         amount of zero pad to both size of the input  this be set so that         when a  class `~torch nn conv3d` and a  class `~torch nn convtranspose3d`         be initialize with same parameters  they be inverses of each other in         regard to the input and output shape  however  when ``stride > 1``           class `~torch nn conv3d` map multiple input shape to the same output         shape   attr `output padding` be provide to resolve this ambiguity by         effectively increase the calculate output shape on one side  note         that  attr `output padding` be only use to find output shape  but do         not actually add zero-padding to output       note           cudnn reproducibility note       args          in channel  int   number of channel in the input image         out channel  int   number of channel produce by the convolution         kernel size  int or tuple   size of the convolve kernel         stride  int or tuple  optional   stride of the convolution  default  1         pad  int or tuple  optional   ``dilation    kernel size - 1  - padding`` zero-padding             will be add to both side of each dimension in the input  default  0         output pad  int or tuple  optional   additional size add to one side             of each dimension in the output shape  default  0         group  int  optional   number of block connections from input channel to output channel  default  1         bias  bool  optional   if ``true``  add a learnable bias to the output  default  ``true``         dilation  int or tuple  optional   space between kernel elements  default  1         format   reproducibility note    convolution note    r         shape          - input   math ` n  c  in   d  in   h  in   w  in  `         - output   math ` n  c  out   d  out   h  out   w  out  ` where             math                 d  out  =  d  in  - 1  \times \text stride  0  - 2 \times \text pad  0    \text dilation  0                          \times  \text kernel\ size  0  - 1    \text output\ pad  0    1            math                 h  out  =  h  in  - 1  \times \text stride  1  - 2 \times \text pad  1    \text dilation  1                          \times  \text kernel\ size  1  - 1    \text output\ pad  1    1            math                 w  out  =  w  in  - 1  \times \text stride  2  - 2 \times \text pad  2    \text dilation  2                          \times  \text kernel\ size  2  - 1    \text output\ pad  2    1       attribute          weight  tensor   the learnable weight of the module of shape                           math ` \text in\ channel   \frac \text out\ channel   \text group   `                           math `\text kernel\ size 0    \text kernel\ size 1    \text kernel\ size 2   `                           the value of these weight be sample from                           math `\mathcal u  -\sqrt k   \sqrt k  ` where                           math `k = \frac group  c \text out    \prod  i=0   2 \text kernel\ size  i  `         bias  tensor     the learnable bias of the module of shape  out channel                           if  attr `bias` be ``true``  then the value of these weight be                          sample from  math `\mathcal u  -\sqrt k   \sqrt k  ` where                           math `k = \frac group  c \text out    \prod  i=0   2 \text kernel\ size  i  `      examples            >>>   with square kernels and equal stride         >>> m = nn convtranspose3d 16  33  3  stride=2          >>>   non-square kernels and unequal stride and with pad         >>> m = nn convtranspose3d 16  33   3  5  2   stride= 2  1  1   padding= 0  4  2           >>> input = torch randn 20  16  10  50  100          >>> output = m input           cross-correlation          https //en wikipedia org/wiki/cross-correlation          link          https //github com/vdumoulin/conv arithmetic/blob/master/readme md              def   init            self          in channel  int          out channel  int          kernel size   size 3 t          stride   size 3 t = 1          pad   size 3 t = 0          output pad   size 3 t = 0          group  int = 1          bias  bool = true          dilation   size 3 t = 1          pad mode  str = string          device=none          dtype=none       -> none          factory kwargs =  string  device  string  dtype          kernel size =  triple kernel size          stride =  triple stride          pad =  triple pad          dilation =  triple dilation          output pad =  triple output pad          super convtranspose3d  self    init                in channel  out channel  kernel size  stride  pad  dilation              true  output pad  group  bias  pad mode    factory kwargs       def forward self  input  tensor  output size  optional list int   = none  -> tensor          if self pad mode  = string              raise valueerror string           assert isinstance self pad  tuple                            output pad = self  output pad              input  output size  self stride  self pad  self kernel size  self dilation             return f conv transpose3d              input  self weight  self bias  self stride  self pad              output pad  self group  self dilation  
class lazyconv1d  lazyconvxdmixin  conv1d         r   a  class `torch nn conv1d` module with lazy initialization of     the ``in channels`` argument of the  class `conv1d` that be infer from     the ``input size 1 ``      the attribute that will be lazily initialize be `weight` and `bias`       check the  class `torch nn modules lazy lazymodulemixin` for further documentation     on lazy modules and their limitations       args          out channel  int   number of channel produce by the convolution         kernel size  int or tuple   size of the convolve kernel         stride  int or tuple  optional   stride of the convolution  default  1         pad  int or tuple  optional   zero-padding add to both side of             the input  default  0         pad mode  string  optional   `` zero ``  `` reflect ``              `` replicate `` or `` circular ``  default  `` zero ``         dilation  int or tuple  optional   space between kernel             elements  default  1         group  int  optional   number of block connections from input             channel to output channel  default  1         bias  bool  optional   if ``true``  add a learnable bias to the             output  default  ``true``         seealso    class `torch nn conv1d` and  class `torch nn modules lazy lazymodulemixin`                        cls to become = conv1d        def   init            self          out channel  int          kernel size   size 1 t          stride   size 1 t = 1          pad   size 1 t = 0          dilation   size 1 t = 1          group  int = 1          bias  bool = true          pad mode  str = string          device=none          dtype=none       -> none          factory kwargs =  string  device  string  dtype          super     init                0              0              kernel size              stride              pad              dilation              group                                        false              pad mode                factory kwargs                   self weight = uninitializedparameter   factory kwargs          self out channel = out channel         if bias              self bias = uninitializedparameter   factory kwargs  
class lazyconv2d  lazyconvxdmixin  conv2d         r   a  class `torch nn conv2d` module with lazy initialization of     the ``in channels`` argument of the  class `conv2d` that be infer from     the ``input size 1 ``      the attribute that will be lazily initialize be `weight` and `bias`       check the  class `torch nn modules lazy lazymodulemixin` for further documentation     on lazy modules and their limitations       args          out channel  int   number of channel produce by the convolution         kernel size  int or tuple   size of the convolve kernel         stride  int or tuple  optional   stride of the convolution  default  1         pad  int or tuple  optional   zero-padding add to both side of             the input  default  0         pad mode  string  optional   `` zero ``  `` reflect ``              `` replicate `` or `` circular ``  default  `` zero ``         dilation  int or tuple  optional   space between kernel             elements  default  1         group  int  optional   number of block connections from input             channel to output channel  default  1         bias  bool  optional   if ``true``  add a learnable bias to the             output  default  ``true``         seealso    class `torch nn conv2d` and  class `torch nn modules lazy lazymodulemixin`                        cls to become = conv2d        def   init            self          out channel  int          kernel size   size 2 t          stride   size 2 t = 1          pad   size 2 t = 0          dilation   size 2 t = 1          group  int = 1          bias  bool = true          pad mode  str = string            device=none          dtype=none       -> none          factory kwargs =  string  device  string  dtype          super     init                0              0              kernel size              stride              pad              dilation              group                                        false              pad mode                factory kwargs                   self weight = uninitializedparameter   factory kwargs          self out channel = out channel         if bias              self bias = uninitializedparameter   factory kwargs  
class lazyconv3d  lazyconvxdmixin  conv3d         r   a  class `torch nn conv3d` module with lazy initialization of     the ``in channels`` argument of the  class `conv3d` that be infer from     the ``input size 1 ``      the attribute that will be lazily initialize be `weight` and `bias`       check the  class `torch nn modules lazy lazymodulemixin` for further documentation     on lazy modules and their limitations       args          out channel  int   number of channel produce by the convolution         kernel size  int or tuple   size of the convolve kernel         stride  int or tuple  optional   stride of the convolution  default  1         pad  int or tuple  optional   zero-padding add to both side of             the input  default  0         pad mode  string  optional   `` zero ``  `` reflect ``              `` replicate `` or `` circular ``  default  `` zero ``         dilation  int or tuple  optional   space between kernel             elements  default  1         group  int  optional   number of block connections from input             channel to output channel  default  1         bias  bool  optional   if ``true``  add a learnable bias to the             output  default  ``true``         seealso    class `torch nn conv3d` and  class `torch nn modules lazy lazymodulemixin`                        cls to become = conv3d        def   init            self          out channel  int          kernel size   size 3 t          stride   size 3 t = 1          pad   size 3 t = 0          dilation   size 3 t = 1          group  int = 1          bias  bool = true          pad mode  str = string          device=none          dtype=none       -> none          factory kwargs =  string  device  string  dtype          super     init                0              0              kernel size              stride              pad              dilation              group                                        false              pad mode                factory kwargs                   self weight = uninitializedparameter   factory kwargs          self out channel = out channel         if bias              self bias = uninitializedparameter   factory kwargs  
class lazyconvtranspose1d  lazyconvxdmixin  convtranspose1d         r   a  class `torch nn convtranspose1d` module with lazy initialization of     the ``in channels`` argument of the  class `convtranspose1d` that be infer from     the ``input size 1 ``      the attribute that will be lazily initialize be `weight` and `bias`       check the  class `torch nn modules lazy lazymodulemixin` for further documentation     on lazy modules and their limitations       args          out channel  int   number of channel produce by the convolution         kernel size  int or tuple   size of the convolve kernel         stride  int or tuple  optional   stride of the convolution  default  1         pad  int or tuple  optional   ``dilation    kernel size - 1  - padding`` zero-padding             will be add to both side of the input  default  0         output pad  int or tuple  optional   additional size add to one side             of the output shape  default  0         group  int  optional   number of block connections from input channel to output channel  default  1         bias  bool  optional   if ``true``  add a learnable bias to the output  default  ``true``         dilation  int or tuple  optional   space between kernel elements  default  1         seealso    class `torch nn convtranspose1d` and  class `torch nn modules lazy lazymodulemixin`                        cls to become = convtranspose1d        def   init            self          out channel  int          kernel size   size 1 t          stride   size 1 t = 1          pad   size 1 t = 0          output pad   size 1 t = 0          group  int = 1          bias  bool = true          dilation   size 1 t = 1          pad mode  str = string          device=none          dtype=none       -> none          factory kwargs =  string  device  string  dtype          super     init                0              0              kernel size              stride              pad              output pad              group                                        false              dilation              pad mode                factory kwargs                   self weight = uninitializedparameter   factory kwargs          self out channel = out channel         if bias              self bias = uninitializedparameter   factory kwargs  
class lazyconvtranspose2d  lazyconvxdmixin  convtranspose2d         r   a  class `torch nn convtranspose2d` module with lazy initialization of     the ``in channels`` argument of the  class `convtranspose2d` that be infer from     the ``input size 1 ``      the attribute that will be lazily initialize be `weight` and `bias`       check the  class `torch nn modules lazy lazymodulemixin` for further documentation     on lazy modules and their limitations       args          out channel  int   number of channel produce by the convolution         kernel size  int or tuple   size of the convolve kernel         stride  int or tuple  optional   stride of the convolution  default  1         pad  int or tuple  optional   ``dilation    kernel size - 1  - padding`` zero-padding             will be add to both side of each dimension in the input  default  0         output pad  int or tuple  optional   additional size add to one side             of each dimension in the output shape  default  0         group  int  optional   number of block connections from input channel to output channel  default  1         bias  bool  optional   if ``true``  add a learnable bias to the output  default  ``true``         dilation  int or tuple  optional   space between kernel elements  default  1         seealso    class `torch nn convtranspose2d` and  class `torch nn modules lazy lazymodulemixin`                        cls to become = convtranspose2d        def   init            self          out channel  int          kernel size   size 2 t          stride   size 2 t = 1          pad   size 2 t = 0          output pad   size 2 t = 0          group  int = 1          bias  bool = true          dilation  int = 1          pad mode  str = string          device=none          dtype=none       -> none          factory kwargs =  string  device  string  dtype          super     init                0              0              kernel size              stride              pad              output pad              group                                        false              dilation              pad mode                factory kwargs                   self weight = uninitializedparameter   factory kwargs          self out channel = out channel         if bias              self bias = uninitializedparameter   factory kwargs  
class lazyconvtranspose3d  lazyconvxdmixin  convtranspose3d         r   a  class `torch nn convtranspose3d` module with lazy initialization of     the ``in channels`` argument of the  class `convtranspose3d` that be infer from     the ``input size 1 ``      the attribute that will be lazily initialize be `weight` and `bias`       check the  class `torch nn modules lazy lazymodulemixin` for further documentation     on lazy modules and their limitations       args          out channel  int   number of channel produce by the convolution         kernel size  int or tuple   size of the convolve kernel         stride  int or tuple  optional   stride of the convolution  default  1         pad  int or tuple  optional   ``dilation    kernel size - 1  - padding`` zero-padding             will be add to both side of each dimension in the input  default  0         output pad  int or tuple  optional   additional size add to one side             of each dimension in the output shape  default  0         group  int  optional   number of block connections from input channel to output channel  default  1         bias  bool  optional   if ``true``  add a learnable bias to the output  default  ``true``         dilation  int or tuple  optional   space between kernel elements  default  1         seealso    class `torch nn convtranspose3d` and  class `torch nn modules lazy lazymodulemixin`                        cls to become = convtranspose3d        def   init            self          out channel  int          kernel size   size 3 t          stride   size 3 t = 1          pad   size 3 t = 0          output pad   size 3 t = 0          group  int = 1          bias  bool = true          dilation   size 3 t = 1          pad mode  str = string          device=none          dtype=none       -> none          factory kwargs =  string  device  string  dtype          super     init                0              0              kernel size              stride              pad              output pad              group                                        false              dilation              pad mode                factory kwargs                   self weight = uninitializedparameter   factory kwargs          self out channel = out channel         if bias              self bias = uninitializedparameter   factory kwargs  
class unfold module       r   extract slide local block from a batch input tensor       consider a batch  attr `input` tensor of shape  math ` n  c    `      where  math `n` be the batch dimension   math `c` be the channel dimension      and  math ` ` represent arbitrary spatial dimension  this operation flatten     each slide  attr `kernel size`-sized block within the spatial dimension     of  attr `input` into a column  i e   last dimension  of a 3-d  attr `output`     tensor of shape  math ` n  c \times \prod \text kernel\ size    l `  where      math `c \times \prod \text kernel\ size  ` be the total number of value     within each block  a block have  math `\prod \text kernel\ size  ` spatial     locations each contain a  math `c`-channeled vector   and  math `l` be     the total number of such block          math           l = \prod d \left\lfloor\frac \text spatial\ size  d    2 \times \text pad  d                - \text dilation  d  \times  \text kernel\ size  d  - 1  - 1  \text stride  d     1\right\rfloor       where  math `\text spatial\ size ` be form by the spatial dimension     of  attr `input`   math ` ` above   and  math `d` be over all spatial     dimension       therefore  index  attr `output` at the last dimension  column dimension      give all value within a certain block       the  attr `padding`   attr `stride` and  attr `dilation` arguments specify     how the slide block be retrieve          attr `stride` control the stride for the slide block          attr `padding` control the amount of implicit zero-paddings on both       side for  attr `padding` number of point for each dimension before       reshape          attr `dilation` control the space between the kernel point  also know as the  trous algorithm        it be harder to describe  but this `link`  have a nice visualization of what  attr `dilation` do       args          kernel size  int or tuple   the size of the slide block         stride  int or tuple  optional   the stride of the slide block in the input                                          spatial dimension  default  1         pad  int or tuple  optional   implicit zero pad to be add on                                           both side of input  default  0         dilation  int or tuple  optional   a parameter that control the                                            stride of elements within the                                            neighborhood  default  1        if  attr `kernel size`   attr `dilation`   attr `padding` or        attr `stride` be an int or a tuple of length 1  their value will be       replicate across all spatial dimension         for the case of two input spatial dimension this operation be sometimes       call ``im2col``          note            class `~torch nn fold` calculate each combine value in the result         large tensor by sum all value from all contain block           class `~torch nn unfold` extract the value in the local block by         copy from the large tensor  so  if the block overlap  they be not         inverses of each other           in general  fold and unfold operations be relate as         follow  consider  class `~torch nn fold` and          class `~torch nn unfold` instance create with the same         parameters           >>> fold params = dict kernel size=     dilation=     padding=     stride=             >>> fold = nn fold output size=       fold params          >>> unfold = nn unfold   fold params           then for any  support  ``input`` tensor the follow         equality hold                           fold unfold input   == divisor   input          where ``divisor`` be a tensor that depend only on the shape         and dtype of the ``input``           >>> input ones = torch ones input shape  dtype=input dtype          >>> divisor = fold unfold input ones            when the ``divisor`` tensor contain no zero elements  then         ``fold`` and ``unfold`` operations be inverses of each         other  up to constant divisor           warn           currently  only 4-d input tensors  batch image-like tensors  be         support       shape          - input   math ` n  c    `         - output   math ` n  c \times \prod \text kernel\ size    l ` as describe above      examples            >>> unfold = nn unfold kernel size= 2  3           >>> input = torch randn 2  5  3  4          >>> output = unfold input          >>>   each patch contain 30 value  2x3=6 vectors  each of 5 channel          >>>   4 block  2x3 kernels  in total in the 3x4 input         >>> output size           torch size  2  30  4            >>>   convolution be equivalent with unfold   matrix multiplication   fold  or view to output shape          >>> inp = torch randn 1  3  10  12          >>> w = torch randn 2  3  4  5          >>> inp unf = torch nn functional unfold inp   4  5           >>> out unf = inp unf transpose 1  2  matmul w view w size 0   -1  t    transpose 1  2          >>> out = torch nn functional fold out unf   7  8    1  1           >>>   or equivalently  and avoid a copy           >>>   out = out unf view 1  2  7  8          >>>  torch nn functional conv2d inp  w  - out  abs   max           tensor 1 9073e-06           link          https //github com/vdumoulin/conv arithmetic/blob/master/readme md                constants   =  string  string  string  string      kernel size   size any t     dilation   size any t     pad   size any t     stride   size any t      def   init            self          kernel size   size any t          dilation   size any t = 1          pad   size any t = 0          stride   size any t = 1       -> none          super unfold  self    init             self kernel size = kernel size         self dilation = dilation         self pad = pad         self stride = stride      def forward self  input  tensor  -> tensor          return f unfold input  self kernel size  self dilation                          self pad  self stride       def extra repr self  -> str          return string \             string format   self   dict    
class fold module       r   combine an array of slide local block into a large contain     tensor       consider a batch  attr `input` tensor contain slide local block      e g   patch of image  of shape  math ` n  c \times  \prod \text kernel\ size    l `      where  math `n` be batch dimension   math `c \times \prod \text kernel\ size  `     be the number of value within a block  a block have  math `\prod \text kernel\ size  `     spatial locations each contain a  math `c`-channeled vector   and      math `l` be the total number of block   this be exactly the     same specification as the output shape of  class `~torch nn unfold`   this     operation combine these local block into the large  attr `output` tensor     of shape  math ` n  c  \text output\ size  0   \text output\ size  1   \dots `     by sum the overlap value  similar to  class `~torch nn unfold`  the     arguments must satisfy         math           l = \prod d \left\lfloor\frac \text output\ size  d    2 \times \text pad  d                - \text dilation  d  \times  \text kernel\ size  d  - 1  - 1  \text stride  d     1\right\rfloor       where  math `d` be over all spatial dimension          attr `output size` describe the spatial shape of the large contain       tensor of the slide local block  it be useful to resolve the ambiguity       when multiple input shape map to same number of slide block  e g         with ``stride > 0``       the  attr `padding`   attr `stride` and  attr `dilation` arguments specify     how the slide block be retrieve          attr `stride` control the stride for the slide block          attr `padding` control the amount of implicit zero-paddings on both       side for  attr `padding` number of point for each dimension before       reshape          attr `dilation` control the space between the kernel point  also know as the  trous algorithm        it be harder to describe  but this `link`  have a nice visualization of what  attr `dilation` do       args          output size  int or tuple   the shape of the spatial dimension of the                                     output  i e   ``output size   2  ``          kernel size  int or tuple   the size of the slide block         stride  int or tuple   the stride of the slide block in the input                                spatial dimension  default  1         pad  int or tuple  optional   implicit zero pad to be add on                                           both side of input  default  0         dilation  int or tuple  optional   a parameter that control the                                            stride of elements within the                                            neighborhood  default  1        if  attr `output size`   attr `kernel size`   attr `dilation`         attr `padding` or  attr `stride` be an int or a tuple of length 1 then       their value will be replicate across all spatial dimension         for the case of two output spatial dimension this operation be sometimes       call ``col2im``          note            class `~torch nn fold` calculate each combine value in the result         large tensor by sum all value from all contain block           class `~torch nn unfold` extract the value in the local block by         copy from the large tensor  so  if the block overlap  they be not         inverses of each other           in general  fold and unfold operations be relate as         follow  consider  class `~torch nn fold` and          class `~torch nn unfold` instance create with the same         parameters           >>> fold params = dict kernel size=     dilation=     padding=     stride=             >>> fold = nn fold output size=       fold params          >>> unfold = nn unfold   fold params           then for any  support  ``input`` tensor the follow         equality hold                           fold unfold input   == divisor   input          where ``divisor`` be a tensor that depend only on the shape         and dtype of the ``input``           >>> input ones = torch ones input shape  dtype=input dtype          >>> divisor = fold unfold input ones            when the ``divisor`` tensor contain no zero elements  then         ``fold`` and ``unfold`` operations be inverses of each         other  up to constant divisor           warn           currently  only 4-d output tensors  batch image-like tensors  be         support       shape          - input   math ` n  c \times \prod \text kernel\ size    l `         - output   math ` n  c  \text output\ size  0   \text output\ size  1   \dots ` as describe above      examples            >>> fold = nn fold output size= 4  5   kernel size= 2  2           >>> input = torch randn 1  3   2   2  12          >>> output = fold input          >>> output size           torch size  1  3  4  5            link          https //github com/vdumoulin/conv arithmetic/blob/master/readme md                constants   =  string  string  string  string                       string      output size   size any t     kernel size   size any t     dilation   size any t     pad   size any t     stride   size any t      def   init            self          output size   size any t          kernel size   size any t          dilation   size any t = 1          pad   size any t = 0          stride   size any t = 1       -> none          super fold  self    init             self output size = output size         self kernel size = kernel size         self dilation = dilation         self pad = pad         self stride = stride      def forward self  input  tensor  -> tensor          return f fold input  self output size  self kernel size  self dilation                        self pad  self stride       def extra repr self  -> str          return string \             string format                    self   dict                 
class maxpool1d  maxpoolnd       r   apply a 1d max pool over an input signal compose of several input     plan       in the simplest case  the output value of the layer with input size  math ` n  c  l `     and output  math ` n  c  l  out  ` can be precisely describe as          math           out n i  c j  k  = \max  m=0  \ldots  \text kernel\ size  - 1                  input n i  c j  stride \times k   m       if  attr `padding` be non-zero  then the input be implicitly pad with negative infinity on both side     for  attr `padding` number of point   attr `dilation` be the stride between the elements within the     slide window  this `link`  have a nice visualization of the pool parameters       note          when ceil mode=true  slide windows be allow to go off-bounds if they start within the leave pad         or the input  slide windows that would start in the right pad region be ignore       args          kernel size  the size of the slide window  must be > 0          stride  the stride of the slide window  must be > 0  default value be  attr `kernel size`          pad  implicit negative infinity pad to be add on both side  must be >= 0 and <= kernel size / 2          dilation  the stride between elements within a slide window  must be > 0          return indices  if ``true``  will return the argmax along with the max value                          useful for  class `torch nn maxunpool1d` later         ceil mode  if ``true``  will use `ceil` instead of `floor` to compute the output shape  this                    ensure that every element in the input tensor be cover by a slide window       shape          - input   math ` n  c  l  in  `         - output   math ` n  c  l  out  `  where               math                 l  out  = \left\lfloor \frac l  in    2 \times \text pad  - \text dilation                      \times  \text kernel\ size  - 1  - 1  \text stride     1\right\rfloor      examples            >>>   pool of size=3  stride=2         >>> m = nn maxpool1d 3  stride=2          >>> input = torch randn 20  16  50          >>> output = m input           link          https //github com/vdumoulin/conv arithmetic/blob/master/readme md              kernel size   size 1 t     stride   size 1 t     pad   size 1 t     dilation   size 1 t      def forward self  input  tensor  -> tensor          return f max pool1d input  self kernel size  self stride                              self pad  self dilation  self ceil mode                              self return indices  
class maxpool2d  maxpoolnd       r   apply a 2d max pool over an input signal compose of several input     plan       in the simplest case  the output value of the layer with input size  math ` n  c  h  w `      output  math ` n  c  h  out   w  out  ` and  attr `kernel size`  math ` kh  kw `     can be precisely describe as          math           \begin align              out n i  c j  h  w  =     \max  m=0  \ldots  kh-1  \max  n=0  \ldots  kw-1  \\                                       \text input  n i  c j  \text stride 0   \times h   m                                                     \text stride 1   \times w   n          \end align       if  attr `padding` be non-zero  then the input be implicitly zero-padded on both side     for  attr `padding` number of point   attr `dilation` control the space between the kernel point      it be harder to describe  but this `link`  have a nice visualization of what  attr `dilation` do       note          when ceil mode=true  slide windows be allow to go off-bounds if they start within the leave pad         or the input  slide windows that would start in the right pad region be ignore       the parameters  attr `kernel size`   attr `stride`   attr `padding`   attr `dilation` can either be           - a single ``int`` -- in which case the same value be use for the height and width dimension         - a ``tuple`` of two ints -- in which case  the first `int` be use for the height dimension            and the second `int` for the width dimension      args          kernel size  the size of the window to take a max over         stride  the stride of the window  default value be  attr `kernel size`         pad  implicit zero pad to be add on both side         dilation  a parameter that control the stride of elements in the window         return indices  if ``true``  will return the max indices along with the output                          useful for  class `torch nn maxunpool2d` later         ceil mode  when true  will use `ceil` instead of `floor` to compute the output shape      shape          - input   math ` n  c  h  in   w  in  `         - output   math ` n  c  h  out   w  out  `  where               math                 h  out  = \left\lfloor\frac h  in    2   \text pad 0   - \text dilation 0                       \times  \text kernel\ size 0   - 1  - 1  \text stride 0      1\right\rfloor               math                 w  out  = \left\lfloor\frac w  in    2   \text pad 1   - \text dilation 1                       \times  \text kernel\ size 1   - 1  - 1  \text stride 1      1\right\rfloor      examples            >>>   pool of square window of size=3  stride=2         >>> m = nn maxpool2d 3  stride=2          >>>   pool of non-square window         >>> m = nn maxpool2d  3  2   stride= 2  1           >>> input = torch randn 20  16  50  32          >>> output = m input           link          https //github com/vdumoulin/conv arithmetic/blob/master/readme md              kernel size   size 2 t     stride   size 2 t     pad   size 2 t     dilation   size 2 t      def forward self  input  tensor  -> tensor          return f max pool2d input  self kernel size  self stride                              self pad  self dilation  self ceil mode                              self return indices  
class maxpool3d  maxpoolnd       r   apply a 3d max pool over an input signal compose of several input     plan       in the simplest case  the output value of the layer with input size  math ` n  c  d  h  w `      output  math ` n  c  d  out   h  out   w  out  ` and  attr `kernel size`  math ` kd  kh  kw `     can be precisely describe as          math           \begin align              \text out  n i  c j  d  h  w  =     \max  k=0  \ldots  kd-1  \max  m=0  \ldots  kh-1  \max  n=0  \ldots  kw-1  \\                                                 \text input  n i  c j  \text stride 0   \times d   k                                                               \text stride 1   \times h   m  \text stride 2   \times w   n          \end align       if  attr `padding` be non-zero  then the input be implicitly zero-padded on both side     for  attr `padding` number of point   attr `dilation` control the space between the kernel point      it be harder to describe  but this `link`  have a nice visualization of what  attr `dilation` do       note          when ceil mode=true  slide windows be allow to go off-bounds if they start within the leave pad         or the input  slide windows that would start in the right pad region be ignore       the parameters  attr `kernel size`   attr `stride`   attr `padding`   attr `dilation` can either be           - a single ``int`` -- in which case the same value be use for the depth  height and width dimension         - a ``tuple`` of three ints -- in which case  the first `int` be use for the depth dimension            the second `int` for the height dimension and the third `int` for the width dimension      args          kernel size  the size of the window to take a max over         stride  the stride of the window  default value be  attr `kernel size`         pad  implicit zero pad to be add on all three side         dilation  a parameter that control the stride of elements in the window         return indices  if ``true``  will return the max indices along with the output                          useful for  class `torch nn maxunpool3d` later         ceil mode  when true  will use `ceil` instead of `floor` to compute the output shape      shape          - input   math ` n  c  d  in   h  in   w  in  `         - output   math ` n  c  d  out   h  out   w  out  `  where               math                 d  out  = \left\lfloor\frac d  in    2 \times \text pad  0  - \text dilation  0  \times                  \text kernel\ size  0  - 1  - 1  \text stride  0     1\right\rfloor               math                 h  out  = \left\lfloor\frac h  in    2 \times \text pad  1  - \text dilation  1  \times                  \text kernel\ size  1  - 1  - 1  \text stride  1     1\right\rfloor               math                 w  out  = \left\lfloor\frac w  in    2 \times \text pad  2  - \text dilation  2  \times                  \text kernel\ size  2  - 1  - 1  \text stride  2     1\right\rfloor      examples            >>>   pool of square window of size=3  stride=2         >>> m = nn maxpool3d 3  stride=2          >>>   pool of non-square window         >>> m = nn maxpool3d  3  2  2   stride= 2  1  2           >>> input = torch randn 20  16  50 44  31          >>> output = m input           link          https //github com/vdumoulin/conv arithmetic/blob/master/readme md                kernel size   size 3 t     stride   size 3 t     pad   size 3 t     dilation   size 3 t      def forward self  input  tensor  -> tensor          return f max pool3d input  self kernel size  self stride                              self pad  self dilation  self ceil mode                              self return indices  
class maxunpool1d  maxunpoolnd       r   compute a partial inverse of  class `maxpool1d`        class `maxpool1d` be not fully invertible  since the non-maximal value be lose        class `maxunpool1d` take in as input the output of  class `maxpool1d`     include the indices of the maximal value and compute a partial inverse     in which all non-maximal value be set to zero          note    class `maxpool1d` can map several input size to the same output               size  hence  the inversion process can get ambiguous                to accommodate this  you can provide the need output size               as an additional argument  attr `output size` in the forward call                see the input and example below       args          kernel size  int or tuple   size of the max pool window          stride  int or tuple   stride of the max pool window              it be set to  attr `kernel size` by default          pad  int or tuple   pad that be add to the input      input          - `input`  the input tensor to invert         - `indices`  the indices give out by  class `~torch nn maxpool1d`         - `output size`  optional   the target output size      shape          - input   math ` n  c  h  in  `         - output   math ` n  c  h  out  `  where               math                 h  out  =  h  in  - 1  \times \text stride  0  - 2 \times \text pad  0    \text kernel\ size  0             or as give by  attr `output size` in the call operator      example            >>> pool = nn maxpool1d 2  stride=2  return indices=true          >>> unpool = nn maxunpool1d 2  stride=2          >>> input = torch tensor    1   2  3  4  5  6  7  8             >>> output  indices = pool input          >>> unpool output  indices          tensor     0    2    0    4    0    6    0   8               >>>   example showcasing the use of output size         >>> input = torch tensor    1   2  3  4  5  6  7  8  9             >>> output  indices = pool input          >>> unpool output  indices  output size=input size            tensor     0    2    0    4    0    6    0   8    0               >>> unpool output  indices          tensor     0    2    0    4    0    6    0   8                   kernel size   size 1 t     stride   size 1 t     pad   size 1 t      def   init   self  kernel size   size 1 t  stride  optional  size 1 t  = none  pad   size 1 t = 0  -> none          super maxunpool1d  self    init             self kernel size =  single kernel size          self stride =  single stride if  stride be not none  else kernel size          self pad =  single pad       def forward self  input  tensor  indices  tensor  output size  optional list int   = none  -> tensor          return f max unpool1d input  indices  self kernel size  self stride                                self pad  output size  
class maxunpool2d  maxunpoolnd       r   compute a partial inverse of  class `maxpool2d`        class `maxpool2d` be not fully invertible  since the non-maximal value be lose        class `maxunpool2d` take in as input the output of  class `maxpool2d`     include the indices of the maximal value and compute a partial inverse     in which all non-maximal value be set to zero          note    class `maxpool2d` can map several input size to the same output               size  hence  the inversion process can get ambiguous                to accommodate this  you can provide the need output size               as an additional argument  attr `output size` in the forward call                see the input and example below       args          kernel size  int or tuple   size of the max pool window          stride  int or tuple   stride of the max pool window              it be set to  attr `kernel size` by default          pad  int or tuple   pad that be add to the input      input          - `input`  the input tensor to invert         - `indices`  the indices give out by  class `~torch nn maxpool2d`         - `output size`  optional   the target output size      shape          - input   math ` n  c  h  in   w  in  `         - output   math ` n  c  h  out   w  out  `  where               math               h  out  =  h  in  - 1  \times \text stride 0   - 2 \times \text pad 0     \text kernel\ size 0                 math               w  out  =  w  in  - 1  \times \text stride 1   - 2 \times \text pad 1     \text kernel\ size 1              or as give by  attr `output size` in the call operator      example            >>> pool = nn maxpool2d 2  stride=2  return indices=true          >>> unpool = nn maxunpool2d 2  stride=2          >>> input = torch tensor      1    2   3   4                                         5   6   7   8                                         9  10  11  12                                        13  14  15  16              >>> output  indices = pool input          >>> unpool output  indices          tensor       0     0     0     0                         0     6     0     8                         0     0     0     0                         0    14     0    16                >>>   specify a different output size than input size         >>> unpool output  indices  output size=torch size  1  1  5  5            tensor       0     0     0     0     0                         6     0     8     0     0                         0     0     0    14     0                        16     0     0     0     0                         0     0     0     0     0                    kernel size   size 2 t     stride   size 2 t     pad   size 2 t      def   init   self  kernel size   size 2 t  stride  optional  size 2 t  = none  pad   size 2 t = 0  -> none          super maxunpool2d  self    init             self kernel size =  pair kernel size          self stride =  pair stride if  stride be not none  else kernel size          self pad =  pair pad       def forward self  input  tensor  indices  tensor  output size  optional list int   = none  -> tensor          return f max unpool2d input  indices  self kernel size  self stride                                self pad  output size  
class maxunpool3d  maxunpoolnd       r   compute a partial inverse of  class `maxpool3d`        class `maxpool3d` be not fully invertible  since the non-maximal value be lose       class `maxunpool3d` take in as input the output of  class `maxpool3d`     include the indices of the maximal value and compute a partial inverse     in which all non-maximal value be set to zero          note    class `maxpool3d` can map several input size to the same output               size  hence  the inversion process can get ambiguous                to accommodate this  you can provide the need output size               as an additional argument  attr `output size` in the forward call                see the input section below       args          kernel size  int or tuple   size of the max pool window          stride  int or tuple   stride of the max pool window              it be set to  attr `kernel size` by default          pad  int or tuple   pad that be add to the input      input          - `input`  the input tensor to invert         - `indices`  the indices give out by  class `~torch nn maxpool3d`         - `output size`  optional   the target output size      shape          - input   math ` n  c  d  in   h  in   w  in  `         - output   math ` n  c  d  out   h  out   w  out  `  where               math                 d  out  =  d  in  - 1  \times \text stride 0   - 2 \times \text pad 0     \text kernel\ size 0                 math                 h  out  =  h  in  - 1  \times \text stride 1   - 2 \times \text pad 1     \text kernel\ size 1                 math                 w  out  =  w  in  - 1  \times \text stride 2   - 2 \times \text pad 2     \text kernel\ size 2              or as give by  attr `output size` in the call operator      example            >>>   pool of square window of size=3  stride=2         >>> pool = nn maxpool3d 3  stride=2  return indices=true          >>> unpool = nn maxunpool3d 3  stride=2          >>> output  indices = pool torch randn 20  16  51  33  15           >>> unpooled output = unpool output  indices          >>> unpooled output size           torch size  20  16  51  33  15                kernel size   size 3 t     stride   size 3 t     pad   size 3 t      def   init   self  kernel size   size 3 t  stride  optional  size 3 t  = none  pad   size 3 t = 0  -> none          super maxunpool3d  self    init             self kernel size =  triple kernel size          self stride =  triple stride if  stride be not none  else kernel size          self pad =  triple pad       def forward self  input  tensor  indices  tensor  output size  optional list int   = none  -> tensor          return f max unpool3d input  indices  self kernel size  self stride                                self pad  output size  
class avgpool1d  avgpoolnd       r   apply a 1d average pool over an input signal compose of several     input plan       in the simplest case  the output value of the layer with input size  math ` n  c  l `      output  math ` n  c  l  out  ` and  attr `kernel size`  math `k`     can be precisely describe as          math            \text out  n i  c j  l  = \frac 1  k  \sum  m=0   k-1                                 \text input  n i  c j  \text stride  \times l   m       if  attr `padding` be non-zero  then the input be implicitly zero-padded on both side     for  attr `padding` number of point       note          when ceil mode=true  slide windows be allow to go off-bounds if they start within the leave pad         or the input  slide windows that would start in the right pad region be ignore       the parameters  attr `kernel size`   attr `stride`   attr `padding` can each be     an ``int`` or a one-element tuple       args          kernel size  the size of the window         stride  the stride of the window  default value be  attr `kernel size`         pad  implicit zero pad to be add on both side         ceil mode  when true  will use `ceil` instead of `floor` to compute the output shape         count include pad  when true  will include the zero-padding in the average calculation      shape          - input   math ` n  c  l  in  `         - output   math ` n  c  l  out  `  where               math                 l  out  = \left\lfloor \frac l  in                  2 \times \text pad  - \text kernel\ size   \text stride     1\right\rfloor      examples            >>>   pool with window of size=3  stride=2         >>> m = nn avgpool1d 3  stride=2          >>> m torch tensor    1  2 3 4 5 6 7              tensor     2    4    6                   kernel size   size 1 t     stride   size 1 t     pad   size 1 t     ceil mode  bool     count include pad  bool      def   init   self  kernel size   size 1 t  stride   size 1 t = none  pad   size 1 t = 0  ceil mode  bool = false                   count include pad  bool = true  -> none          super avgpool1d  self    init             self kernel size =  single kernel size          self stride =  single stride if stride be not none else kernel size          self pad =  single pad          self ceil mode = ceil mode         self count include pad = count include pad      def forward self  input  tensor  -> tensor          return f avg pool1d              input  self kernel size  self stride  self pad  self ceil mode              self count include pad  
class avgpool2d  avgpoolnd       r   apply a 2d average pool over an input signal compose of several input     plan       in the simplest case  the output value of the layer with input size  math ` n  c  h  w `      output  math ` n  c  h  out   w  out  ` and  attr `kernel size`  math ` kh  kw `     can be precisely describe as          math            out n i  c j  h  w   = \frac 1  kh   kw  \sum  m=0   kh-1  \sum  n=0   kw-1                                 input n i  c j  stride 0  \times h   m  stride 1  \times w   n       if  attr `padding` be non-zero  then the input be implicitly zero-padded on both side     for  attr `padding` number of point       note          when ceil mode=true  slide windows be allow to go off-bounds if they start within the leave pad         or the input  slide windows that would start in the right pad region be ignore       the parameters  attr `kernel size`   attr `stride`   attr `padding` can either be           - a single ``int`` -- in which case the same value be use for the height and width dimension         - a ``tuple`` of two ints -- in which case  the first `int` be use for the height dimension            and the second `int` for the width dimension      args          kernel size  the size of the window         stride  the stride of the window  default value be  attr `kernel size`         pad  implicit zero pad to be add on both side         ceil mode  when true  will use `ceil` instead of `floor` to compute the output shape         count include pad  when true  will include the zero-padding in the average calculation         divisor override  if specify  it will be use as divisor  otherwise  attr `kernel size` will be use      shape          - input   math ` n  c  h  in   w  in  `         - output   math ` n  c  h  out   w  out  `  where               math                 h  out  = \left\lfloor\frac h  in     2 \times \text pad  0  -                 \text kernel\ size  0   \text stride  0     1\right\rfloor               math                 w  out  = \left\lfloor\frac w  in     2 \times \text pad  1  -                 \text kernel\ size  1   \text stride  1     1\right\rfloor      examples            >>>   pool of square window of size=3  stride=2         >>> m = nn avgpool2d 3  stride=2          >>>   pool of non-square window         >>> m = nn avgpool2d  3  2   stride= 2  1           >>> input = torch randn 20  16  50  32          >>> output = m input                constants   =  string  string  string  string  string  string       kernel size   size 2 t     stride   size 2 t     pad   size 2 t     ceil mode  bool     count include pad  bool      def   init   self  kernel size   size 2 t  stride  optional  size 2 t  = none  pad   size 2 t = 0                   ceil mode  bool = false  count include pad  bool = true  divisor override  optional int  = none  -> none          super avgpool2d  self    init             self kernel size = kernel size         self stride = stride if  stride be not none  else kernel size         self pad = pad         self ceil mode = ceil mode         self count include pad = count include pad         self divisor override = divisor override      def forward self  input  tensor  -> tensor          return f avg pool2d input  self kernel size  self stride                              self pad  self ceil mode  self count include pad  self divisor override  
class avgpool3d  avgpoolnd       r   apply a 3d average pool over an input signal compose of several input     plan       in the simplest case  the output value of the layer with input size  math ` n  c  d  h  w `      output  math ` n  c  d  out   h  out   w  out  ` and  attr `kernel size`  math ` kd  kh  kw `     can be precisely describe as          math           \begin align              \text out  n i  c j  d  h  w  =     \sum  k=0   kd-1  \sum  m=0   kh-1  \sum  n=0   kw-1  \\                                                 \frac \text input  n i  c j  \text stride  0  \times d   k                                                        \text stride  1  \times h   m  \text stride  2  \times w   n                                                         kd \times kh \times kw          \end align       if  attr `padding` be non-zero  then the input be implicitly zero-padded on all three side     for  attr `padding` number of point       note          when ceil mode=true  slide windows be allow to go off-bounds if they start within the leave pad         or the input  slide windows that would start in the right pad region be ignore       the parameters  attr `kernel size`   attr `stride` can either be           - a single ``int`` -- in which case the same value be use for the depth  height and width dimension         - a ``tuple`` of three ints -- in which case  the first `int` be use for the depth dimension            the second `int` for the height dimension and the third `int` for the width dimension      args          kernel size  the size of the window         stride  the stride of the window  default value be  attr `kernel size`         pad  implicit zero pad to be add on all three side         ceil mode  when true  will use `ceil` instead of `floor` to compute the output shape         count include pad  when true  will include the zero-padding in the average calculation         divisor override  if specify  it will be use as divisor  otherwise  attr `kernel size` will be use      shape          - input   math ` n  c  d  in   h  in   w  in  `         - output   math ` n  c  d  out   h  out   w  out  `  where               math                 d  out  = \left\lfloor\frac d  in    2 \times \text pad  0  -                     \text kernel\ size  0   \text stride  0     1\right\rfloor               math                 h  out  = \left\lfloor\frac h  in    2 \times \text pad  1  -                     \text kernel\ size  1   \text stride  1     1\right\rfloor               math                 w  out  = \left\lfloor\frac w  in    2 \times \text pad  2  -                     \text kernel\ size  2   \text stride  2     1\right\rfloor      examples            >>>   pool of square window of size=3  stride=2         >>> m = nn avgpool3d 3  stride=2          >>>   pool of non-square window         >>> m = nn avgpool3d  3  2  2   stride= 2  1  2           >>> input = torch randn 20  16  50 44  31          >>> output = m input                constants   =  string  string  string  string  string  string       kernel size   size 3 t     stride   size 3 t     pad   size 3 t     ceil mode  bool     count include pad  bool      def   init   self  kernel size   size 3 t  stride  optional  size 3 t  = none  pad   size 3 t = 0                   ceil mode  bool = false  count include pad  bool = true  divisor override  optional int  = none  -> none          super avgpool3d  self    init             self kernel size = kernel size         self stride = stride if  stride be not none  else kernel size         self pad = pad         self ceil mode = ceil mode         self count include pad = count include pad         self divisor override = divisor override      def forward self  input  tensor  -> tensor          return f avg pool3d input  self kernel size  self stride                              self pad  self ceil mode  self count include pad  self divisor override       def   setstate   self  d           super avgpool3d  self    setstate   d          self   dict   setdefault string  0          self   dict   setdefault string  false          self   dict   setdefault string  true  
class fractionalmaxpool2d module       r   apply a 2d fractional max pool over an input signal compose of several input plan       fractional maxpooling be describe in detail in the paper `fractional maxpooling`  by ben graham      the max-pooling operation be apply in  math `kh \times kw` regions by a stochastic     step size determine by the target output size      the number of output feature be equal to the number of input plan       args          kernel size  the size of the window to take a max over                       can be a single number k  for a square kernel of k x k  or a tuple ` kh  kw `         output size  the target output size of the image of the form `oh x ow`                       can be a tuple ` oh  ow ` or a single number oh for a square image `oh x oh`         output ratio  if one want to have an output size as a ratio of the input size  this option can be give                        this have to be a number or tuple in the range  0  1          return indices  if ``true``  will return the indices along with the output                          useful to pass to  meth `nn maxunpool2d`  default  ``false``      examples          >>>   pool of square window of size=3  and target output size 13x12         >>> m = nn fractionalmaxpool2d 3  output size= 13  12           >>>   pool of square window and target output size be half of input image size         >>> m = nn fractionalmaxpool2d 3  output ratio= 0 5  0 5           >>> input = torch randn 20  16  50  32          >>> output = m input           fractional maxpooling          https //arxiv org/abs/1412 6071               constants   =  string  string  string                       string       kernel size   size 2 t     return indices  bool     output size   size 2 t     output ratio   ratio 2 t      def   init   self  kernel size   size 2 t  output size  optional  size 2 t  = none                   output ratio  optional  ratio 2 t  = none                   return indices  bool = false   random samples=none  -> none          super fractionalmaxpool2d  self    init             self kernel size =  pair kernel size          self return indices = return indices         self register buffer string   random sample          self output size =  pair output size  if output size be not none else none         self output ratio =  pair output ratio  if output ratio be not none else none         if output size be none and output ratio be none              raise valueerror string                              string          if output size be not none and output ratio be not none              raise valueerror string          if self output ratio be not none              if not  0 < self output ratio 0  < 1 and 0 < self output ratio 1  < 1                   raise valueerror string                                   format output ratio        def forward self  input  tensor  -> tensor          return f fractional max pool2d              input  self kernel size  self output size  self output ratio              self return indices               random samples=self  random sample  
class fractionalmaxpool3d module       r   apply a 3d fractional max pool over an input signal compose of several input plan       fractional maxpooling be describe in detail in the paper `fractional maxpooling`  by ben graham      the max-pooling operation be apply in  math `ktxkhxkw` regions by a stochastic     step size determine by the target output size      the number of output feature be equal to the number of input plan       args          kernel size  the size of the window to take a max over                       can be a single number k  for a square kernel of k x k x k  or a tuple ` kt x kh x kw `         output size  the target output size of the image of the form `ot x oh x ow`                       can be a tuple ` ot  oh  ow ` or a single number oh for a square image `oh x oh x oh`         output ratio  if one want to have an output size as a ratio of the input size  this option can be give                        this have to be a number or tuple in the range  0  1          return indices  if ``true``  will return the indices along with the output                          useful to pass to  meth `nn maxunpool3d`  default  ``false``      examples          >>>   pool of cubic window of size=3  and target output size 13x12x11         >>> m = nn fractionalmaxpool3d 3  output size= 13  12  11           >>>   pool of cubic window and target output size be half of input size         >>> m = nn fractionalmaxpool3d 3  output ratio= 0 5  0 5  0 5           >>> input = torch randn 20  16  50  32  16          >>> output = m input           fractional maxpooling          https //arxiv org/abs/1412 6071               constants   =  string  string  string                       string      kernel size   size 3 t     return indices  bool     output size   size 3 t     output ratio   ratio 3 t      def   init   self  kernel size   size 3 t  output size  optional  size 3 t  = none                   output ratio  optional  ratio 3 t  = none                   return indices  bool = false   random samples=none  -> none          super fractionalmaxpool3d  self    init             self kernel size =  triple kernel size          self return indices = return indices         self register buffer string   random sample          self output size =  triple output size  if output size be not none else none         self output ratio =  triple output ratio  if output ratio be not none else none         if output size be none and output ratio be none              raise valueerror string                              string          if output size be not none and output ratio be not none              raise valueerror string          if self output ratio be not none              if not  0 < self output ratio 0  < 1 and 0 < self output ratio 1  < 1 and 0 < self output ratio 2  < 1                   raise valueerror string                                   format output ratio        def forward self  input  tensor  -> tensor          return f fractional max pool3d              input  self kernel size  self output size  self output ratio              self return indices               random samples=self  random sample  
class lppool1d  lppoolnd       r   apply a 1d power-average pool over an input signal compose of several input     plan       on each window  the function compute be          math           f x  = \sqrt p  \sum  x \in x  x  p        - at p =  math `\infty`  one get max pool     - at p = 1  one get sum pool  which be proportional to average pool          note   if the sum to the power of `p` be zero  the gradient of this function be               not define  this implementation will set the gradient to zero in this case       args          kernel size  a single int  the size of the window         stride  a single int  the stride of the window  default value be  attr `kernel size`         ceil mode  when true  will use `ceil` instead of `floor` to compute the output shape      shape          - input   math ` n  c  l  in  `         - output   math ` n  c  l  out  `  where               math                 l  out  = \left\lfloor\frac l  in  - \text kernel\ size   \text stride     1\right\rfloor      examples           >>>   power-2 pool of window of length 3  with stride 2          >>> m = nn lppool1d 2  3  stride=2          >>> input = torch randn 20  16  50          >>> output = m input               kernel size   size 1 t     stride   size 1 t      def forward self  input  tensor  -> tensor          return f lp pool1d input  float self norm type   self kernel size                             self stride  self ceil mode  
class lppool2d  lppoolnd       r   apply a 2d power-average pool over an input signal compose of several input     plan       on each window  the function compute be          math           f x  = \sqrt p  \sum  x \in x  x  p        - at p =  math `\infty`  one get max pool     - at p = 1  one get sum pool  which be proportional to average pool       the parameters  attr `kernel size`   attr `stride` can either be           - a single ``int`` -- in which case the same value be use for the height and width dimension         - a ``tuple`` of two ints -- in which case  the first `int` be use for the height dimension            and the second `int` for the width dimension         note   if the sum to the power of `p` be zero  the gradient of this function be               not define  this implementation will set the gradient to zero in this case       args          kernel size  the size of the window         stride  the stride of the window  default value be  attr `kernel size`         ceil mode  when true  will use `ceil` instead of `floor` to compute the output shape      shape          - input   math ` n  c  h  in   w  in  `         - output   math ` n  c  h  out   w  out  `  where               math                 h  out  = \left\lfloor\frac h  in  - \text kernel\ size  0   \text stride  0     1\right\rfloor               math                 w  out  = \left\lfloor\frac w  in  - \text kernel\ size  1   \text stride  1     1\right\rfloor      examples            >>>   power-2 pool of square window of size=3  stride=2         >>> m = nn lppool2d 2  3  stride=2          >>>   pool of non-square window of power 1 2         >>> m = nn lppool2d 1 2   3  2   stride= 2  1           >>> input = torch randn 20  16  50  32          >>> output = m input                kernel size   size 2 t     stride   size 2 t      def forward self  input  tensor  -> tensor          return f lp pool2d input  float self norm type   self kernel size                             self stride  self ceil mode  
class adaptivemaxpool1d  adaptivemaxpoolnd       r   apply a 1d adaptive max pool over an input signal compose of several input plan       the output size be h  for any input size      the number of output feature be equal to the number of input plan       args          output size  the target output size h         return indices  if ``true``  will return the indices along with the output                          useful to pass to nn maxunpool1d  default  ``false``      examples          >>>   target output size of 5         >>> m = nn adaptivemaxpool1d 5          >>> input = torch randn 1  64  8          >>> output = m input                output size   size 1 t      def forward self  input  tensor  -> tensor          return f adaptive max pool1d input  self output size  self return indices  
class adaptivemaxpool2d  adaptivemaxpoolnd       r   apply a 2d adaptive max pool over an input signal compose of several input plan       the output be of size h x w  for any input size      the number of output feature be equal to the number of input plan       args          output size  the target output size of the image of the form h x w                       can be a tuple  h  w  or a single h for a square image h x h                       h and w can be either a ``int``  or ``none`` which mean the size will                      be the same as that of the input          return indices  if ``true``  will return the indices along with the output                          useful to pass to nn maxunpool2d  default  ``false``      examples          >>>   target output size of 5x7         >>> m = nn adaptivemaxpool2d  5 7           >>> input = torch randn 1  64  8  9          >>> output = m input          >>>   target output size of 7x7  square          >>> m = nn adaptivemaxpool2d 7          >>> input = torch randn 1  64  10  9          >>> output = m input          >>>   target output size of 10x7         >>> m = nn adaptivemaxpool2d  none  7           >>> input = torch randn 1  64  10  9          >>> output = m input                output size   size 2 opt t      def forward self  input  tensor  -> tensor          return f adaptive max pool2d input  self output size  self return indices  
class adaptivemaxpool3d  adaptivemaxpoolnd       r   apply a 3d adaptive max pool over an input signal compose of several input plan       the output be of size d x h x w  for any input size      the number of output feature be equal to the number of input plan       args          output size  the target output size of the image of the form d x h x w                       can be a tuple  d  h  w  or a single d for a cube d x d x d                       d  h and w can be either a ``int``  or ``none`` which mean the size will                      be the same as that of the input           return indices  if ``true``  will return the indices along with the output                          useful to pass to nn maxunpool3d  default  ``false``      examples          >>>   target output size of 5x7x9         >>> m = nn adaptivemaxpool3d  5 7 9           >>> input = torch randn 1  64  8  9  10          >>> output = m input          >>>   target output size of 7x7x7  cube          >>> m = nn adaptivemaxpool3d 7          >>> input = torch randn 1  64  10  9  8          >>> output = m input          >>>   target output size of 7x9x8         >>> m = nn adaptivemaxpool3d  7  none  none           >>> input = torch randn 1  64  10  9  8          >>> output = m input                output size   size 3 opt t      def forward self  input  tensor  -> tensor          return f adaptive max pool3d input  self output size  self return indices  
class adaptiveavgpool1d  adaptiveavgpoolnd       r   apply a 1d adaptive average pool over an input signal compose of several input plan       the output size be h  for any input size      the number of output feature be equal to the number of input plan       args          output size  the target output size h      examples          >>>   target output size of 5         >>> m = nn adaptiveavgpool1d 5          >>> input = torch randn 1  64  8          >>> output = m input                output size   size 1 t      def forward self  input  tensor  -> tensor          return f adaptive avg pool1d input  self output size  
class adaptiveavgpool2d  adaptiveavgpoolnd       r   apply a 2d adaptive average pool over an input signal compose of several input plan       the output be of size h x w  for any input size      the number of output feature be equal to the number of input plan       args          output size  the target output size of the image of the form h x w                       can be a tuple  h  w  or a single h for a square image h x h                       h and w can be either a ``int``  or ``none`` which mean the size will                      be the same as that of the input       examples          >>>   target output size of 5x7         >>> m = nn adaptiveavgpool2d  5 7           >>> input = torch randn 1  64  8  9          >>> output = m input          >>>   target output size of 7x7  square          >>> m = nn adaptiveavgpool2d 7          >>> input = torch randn 1  64  10  9          >>> output = m input          >>>   target output size of 10x7         >>> m = nn adaptiveavgpool2d  none  7           >>> input = torch randn 1  64  10  9          >>> output = m input                output size   size 2 opt t      def forward self  input  tensor  -> tensor          return f adaptive avg pool2d input  self output size  
class adaptiveavgpool3d  adaptiveavgpoolnd       r   apply a 3d adaptive average pool over an input signal compose of several input plan       the output be of size d x h x w  for any input size      the number of output feature be equal to the number of input plan       args          output size  the target output size of the form d x h x w                       can be a tuple  d  h  w  or a single number d for a cube d x d x d                       d  h and w can be either a ``int``  or ``none`` which mean the size will                      be the same as that of the input       examples          >>>   target output size of 5x7x9         >>> m = nn adaptiveavgpool3d  5 7 9           >>> input = torch randn 1  64  8  9  10          >>> output = m input          >>>   target output size of 7x7x7  cube          >>> m = nn adaptiveavgpool3d 7          >>> input = torch randn 1  64  10  9  8          >>> output = m input          >>>   target output size of 7x9x8         >>> m = nn adaptiveavgpool3d  7  none  none           >>> input = torch randn 1  64  10  9  8          >>> output = m input                output size   size 3 opt t      def forward self  input  tensor  -> tensor          return f adaptive avg pool3d input  self output size  
class reflectionpad1d  reflectionpadnd       r   pad the input tensor use the reflection of the input boundary       for `n`-dimensional pad  use  func `torch nn functional pad  `       args          pad  int  tuple   the size of the pad  if be `int`  use the same             pad in all boundaries  if a 2-`tuple`  use               math `\text padding\ leave `   math `\text padding\ right `       shape          - input   math ` n  c  w  in  `         - output   math ` n  c  w  out  ` where             math `w  out  = w  in    \text padding\ leave    \text padding\ right `      examples            >>> m = nn reflectionpad1d 2          >>> input = torch arange 8  dtype=torch float  reshape 1  2  4          >>> input         tensor    0   1   2   3                      4   5   6   7              >>> m input          tensor    2   1   0   1   2   3   2   1                      6   5   4   5   6   7   6   5              >>>   use different paddings for different side         >>> m = nn reflectionpad1d  3  1           >>> m input          tensor    3   2   1   0   1   2   3   2                      7   6   5   4   5   6   7   6                   pad  tuple int  int       def   init   self  pad   size 2 t  -> none          super reflectionpad1d  self    init             self pad =  pair pad  
class reflectionpad2d  reflectionpadnd       r   pad the input tensor use the reflection of the input boundary       for `n`-dimensional pad  use  func `torch nn functional pad  `       args          pad  int  tuple   the size of the pad  if be `int`  use the same             pad in all boundaries  if a 4-`tuple`  use   math `\text padding\ leave `               math `\text padding\ right `   math `\text padding\ top `   math `\text padding\ bottom `       shape          - input   math ` n  c  h  in   w  in  `         - output   math ` n  c  h  out   w  out  ` where             math `h  out  = h  in    \text padding\ top    \text padding\ bottom `             math `w  out  = w  in    \text padding\ leave    \text padding\ right `      examples            >>> m = nn reflectionpad2d 2          >>> input = torch arange 9  dtype=torch float  reshape 1  1  3  3          >>> input         tensor     0   1   2                       3   4   5                       6   7   8               >>> m input          tensor     8   7   6   7   8   7   6                       5   4   3   4   5   4   3                       2   1   0   1   2   1   0                       5   4   3   4   5   4   3                       8   7   6   7   8   7   6                       5   4   3   4   5   4   3                       2   1   0   1   2   1   0               >>>   use different paddings for different side         >>> m = nn reflectionpad2d  1  1  2  0           >>> m input          tensor     7   6   7   8   7                       4   3   4   5   4                       1   0   1   2   1                       4   3   4   5   4                       7   6   7   8   7                    pad  tuple int  int  int  int       def   init   self  pad   size 4 t  -> none          super reflectionpad2d  self    init             self pad =  quadruple pad  
class replicationpad1d  replicationpadnd       r   pad the input tensor use replication of the input boundary       for `n`-dimensional pad  use  func `torch nn functional pad  `       args          pad  int  tuple   the size of the pad  if be `int`  use the same             pad in all boundaries  if a 2-`tuple`  use               math `\text padding\ leave `   math `\text padding\ right `       shape          - input   math ` n  c  w  in  `         - output   math ` n  c  w  out  ` where             math `w  out  = w  in    \text padding\ leave    \text padding\ right `      examples            >>> m = nn replicationpad1d 2          >>> input = torch arange 8  dtype=torch float  reshape 1  2  4          >>> input         tensor    0   1   2   3                      4   5   6   7              >>> m input          tensor    0   0   0   1   2   3   3   3                      4   4   4   5   6   7   7   7              >>>   use different paddings for different side         >>> m = nn replicationpad1d  3  1           >>> m input          tensor    0   0   0   0   1   2   3   3                      4   4   4   4   5   6   7   7                   pad  tuple int  int       def   init   self  pad   size 2 t  -> none          super replicationpad1d  self    init             self pad =  pair pad  
class replicationpad2d  replicationpadnd       r   pad the input tensor use replication of the input boundary       for `n`-dimensional pad  use  func `torch nn functional pad  `       args          pad  int  tuple   the size of the pad  if be `int`  use the same             pad in all boundaries  if a 4-`tuple`  use   math `\text padding\ leave `               math `\text padding\ right `   math `\text padding\ top `   math `\text padding\ bottom `       shape          - input   math ` n  c  h  in   w  in  `         - output   math ` n  c  h  out   w  out  ` where             math `h  out  = h  in    \text padding\ top    \text padding\ bottom `             math `w  out  = w  in    \text padding\ leave    \text padding\ right `      examples            >>> m = nn replicationpad2d 2          >>> input = torch arange 9  dtype=torch float  reshape 1  1  3  3          >>> input         tensor     0   1   2                       3   4   5                       6   7   8               >>> m input          tensor     0   0   0   1   2   2   2                       0   0   0   1   2   2   2                       0   0   0   1   2   2   2                       3   3   3   4   5   5   5                       6   6   6   7   8   8   8                       6   6   6   7   8   8   8                       6   6   6   7   8   8   8               >>>   use different paddings for different side         >>> m = nn replicationpad2d  1  1  2  0           >>> m input          tensor     0   0   1   2   2                       0   0   1   2   2                       0   0   1   2   2                       3   3   4   5   5                       6   6   7   8   8                    pad  tuple int  int  int  int       def   init   self  pad   size 4 t  -> none          super replicationpad2d  self    init             self pad =  quadruple pad  
class replicationpad3d  replicationpadnd       r   pad the input tensor use replication of the input boundary       for `n`-dimensional pad  use  func `torch nn functional pad  `       args          pad  int  tuple   the size of the pad  if be `int`  use the same             pad in all boundaries  if a 6-`tuple`  use               math `\text padding\ leave `   math `\text padding\ right `               math `\text padding\ top `   math `\text padding\ bottom `               math `\text padding\ front `   math `\text padding\ back `       shape          - input   math ` n  c  d  in   h  in   w  in  `         - output   math ` n  c  d  out   h  out   w  out  ` where             math `d  out  = d  in    \text padding\ front    \text padding\ back `             math `h  out  = h  in    \text padding\ top    \text padding\ bottom `             math `w  out  = w  in    \text padding\ leave    \text padding\ right `      examples            >>> m = nn replicationpad3d 3          >>> input = torch randn 16  3  8  320  480          >>> output = m input          >>>   use different paddings for different side         >>> m = nn replicationpad3d  3  3  6  6  1  1           >>> output = m input               pad  tuple int  int  int  int  int  int       def   init   self  pad   size 6 t  -> none          super replicationpad3d  self    init             self pad =  ntuple 6  pad  
class zeropad2d constantpad2d       r   pad the input tensor boundaries with zero       for `n`-dimensional pad  use  func `torch nn functional pad  `       args          pad  int  tuple   the size of the pad  if be `int`  use the same             pad in all boundaries  if a 4-`tuple`  use   math `\text padding\ leave `               math `\text padding\ right `   math `\text padding\ top `   math `\text padding\ bottom `       shape          - input   math ` n  c  h  in   w  in  `         - output   math ` n  c  h  out   w  out  ` where             math `h  out  = h  in    \text padding\ top    \text padding\ bottom `             math `w  out  = w  in    \text padding\ leave    \text padding\ right `      examples            >>> m = nn zeropad2d 2          >>> input = torch randn 1  1  3  3          >>> input         tensor     -0 1678  -0 4418   1 9466                       0 9604  -0 4219  -0 5241                      -0 9162  -0 5436  -0 6446              >>> m input          tensor      0 0000   0 0000   0 0000   0 0000   0 0000   0 0000   0 0000                       0 0000   0 0000   0 0000   0 0000   0 0000   0 0000   0 0000                       0 0000   0 0000  -0 1678  -0 4418   1 9466   0 0000   0 0000                       0 0000   0 0000   0 9604  -0 4219  -0 5241   0 0000   0 0000                       0 0000   0 0000  -0 9162  -0 5436  -0 6446   0 0000   0 0000                       0 0000   0 0000   0 0000   0 0000   0 0000   0 0000   0 0000                       0 0000   0 0000   0 0000   0 0000   0 0000   0 0000   0 0000              >>>   use different paddings for different side         >>> m = nn zeropad2d  1  1  2  0           >>> m input          tensor      0 0000   0 0000   0 0000   0 0000   0 0000                       0 0000   0 0000   0 0000   0 0000   0 0000                       0 0000  -0 1678  -0 4418   1 9466   0 0000                       0 0000   0 9604  -0 4219  -0 5241   0 0000                       0 0000  -0 9162  -0 5436  -0 6446   0 0000                   pad  tuple int  int  int  int       def   init   self  pad   size 4 t  -> none          super zeropad2d  self    init   pad  0   
class constantpad1d  constantpadnd       r   pad the input tensor boundaries with a constant value       for `n`-dimensional pad  use  func `torch nn functional pad  `       args          pad  int  tuple   the size of the pad  if be `int`  use the same             pad in both boundaries  if a 2-`tuple`  use               math `\text padding\ leave `   math `\text padding\ right `       shape          - input   math ` n  c  w  in  `         - output   math ` n  c  w  out  ` where             math `w  out  = w  in    \text padding\ leave    \text padding\ right `      examples            >>> m = nn constantpad1d 2  3 5          >>> input = torch randn 1  2  4          >>> input         tensor    -1 0491  -0 7152  -0 0749   0 8530                     -1 3287   1 8966   0 1466  -0 2771             >>> m input          tensor     3 5000   3 5000  -1 0491  -0 7152  -0 0749   0 8530   3 5000                     3 5000                      3 5000   3 5000  -1 3287   1 8966   0 1466  -0 2771   3 5000                     3 5000             >>> m = nn constantpad1d 2  3 5          >>> input = torch randn 1  2  3          >>> input         tensor     1 6616   1 4523  -1 1255                     -3 6372   0 1182  -1 8652             >>> m input          tensor     3 5000   3 5000   1 6616   1 4523  -1 1255   3 5000   3 5000                      3 5000   3 5000  -3 6372   0 1182  -1 8652   3 5000   3 5000             >>>   use different paddings for different side         >>> m = nn constantpad1d  3  1   3 5          >>> m input          tensor     3 5000   3 5000   3 5000   1 6616   1 4523  -1 1255   3 5000                      3 5000   3 5000   3 5000  -3 6372   0 1182  -1 8652   3 5000                  pad  tuple int  int       def   init   self  pad   size 2 t  value  float           super constantpad1d  self    init   value          self pad =  pair pad  
class constantpad2d  constantpadnd       r   pad the input tensor boundaries with a constant value       for `n`-dimensional pad  use  func `torch nn functional pad  `       args          pad  int  tuple   the size of the pad  if be `int`  use the same             pad in all boundaries  if a 4-`tuple`  use   math `\text padding\ leave `               math `\text padding\ right `   math `\text padding\ top `   math `\text padding\ bottom `       shape          - input   math ` n  c  h  in   w  in  `         - output   math ` n  c  h  out   w  out  ` where             math `h  out  = h  in    \text padding\ top    \text padding\ bottom `             math `w  out  = w  in    \text padding\ leave    \text padding\ right `      examples            >>> m = nn constantpad2d 2  3 5          >>> input = torch randn 1  2  2          >>> input         tensor     1 6585   0 4320                     -0 8701  -0 4649             >>> m input          tensor     3 5000   3 5000   3 5000   3 5000   3 5000   3 5000                      3 5000   3 5000   3 5000   3 5000   3 5000   3 5000                      3 5000   3 5000   1 6585   0 4320   3 5000   3 5000                      3 5000   3 5000  -0 8701  -0 4649   3 5000   3 5000                      3 5000   3 5000   3 5000   3 5000   3 5000   3 5000                      3 5000   3 5000   3 5000   3 5000   3 5000   3 5000             >>>   use different paddings for different side         >>> m = nn constantpad2d  3  0  2  1   3 5          >>> m input          tensor     3 5000   3 5000   3 5000   3 5000   3 5000                      3 5000   3 5000   3 5000   3 5000   3 5000                      3 5000   3 5000   3 5000   1 6585   0 4320                      3 5000   3 5000   3 5000  -0 8701  -0 4649                      3 5000   3 5000   3 5000   3 5000   3 5000                    constants   =  string  string      pad  tuple int  int  int  int       def   init   self  pad   size 4 t  value  float  -> none          super constantpad2d  self    init   value          self pad =  quadruple pad  
class constantpad3d  constantpadnd       r   pad the input tensor boundaries with a constant value       for `n`-dimensional pad  use  func `torch nn functional pad  `       args          pad  int  tuple   the size of the pad  if be `int`  use the same             pad in all boundaries  if a 6-`tuple`  use               math `\text padding\ leave `   math `\text padding\ right `               math `\text padding\ top `   math `\text padding\ bottom `               math `\text padding\ front `   math `\text padding\ back `       shape          - input   math ` n  c  d  in   h  in   w  in  `         - output   math ` n  c  d  out   h  out   w  out  ` where             math `d  out  = d  in    \text padding\ front    \text padding\ back `             math `h  out  = h  in    \text padding\ top    \text padding\ bottom `             math `w  out  = w  in    \text padding\ leave    \text padding\ right `      examples            >>> m = nn constantpad3d 3  3 5          >>> input = torch randn 16  3  10  20  30          >>> output = m input          >>>   use different paddings for different side         >>> m = nn constantpad3d  3  3  6  6  0  1   3 5          >>> output = m input               pad  tuple int  int  int  int  int  int       def   init   self  pad   size 6 t  value  float  -> none          super constantpad3d  self    init   value          self pad =  ntuple 6  pad  
class elu module       r   apply the element-wise function          math           \text elu  x  = \begin case          x    \text  if   x > 0\\         \alpha    \exp x  - 1     \text  if   x \leq 0         \end case       args          alpha  the  math `\alpha` value for the elu formulation  default  1 0         inplace  can optionally do the operation in-place  default  ``false``      shape          - input   math ` n    ` where ` ` mean  any number of additional           dimension         - output   math ` n    `  same shape as the input         image     /scripts/activation images/elu png      examples            >>> m = nn elu           >>> input = torch randn 2          >>> output = m input                constants   =  string  string      alpha  float     inplace  bool      def   init   self  alpha  float = 1   inplace  bool = false  -> none          super elu  self    init             self alpha = alpha         self inplace = inplace      def forward self  input  tensor  -> tensor          return f elu input  self alpha  self inplace       def extra repr self  -> str          inplace str = string if self inplace else string         return string format self alpha  inplace str  
class hardshrink module       r   apply the hard shrinkage function element-wise          math           \text hardshrink  x  =         \begin case          x    \text  if   x > \lambda \\         x    \text  if   x < -\lambda \\         0    \text  otherwise           \end case       args          lambd  the  math `\lambda` value for the hardshrink formulation  default  0 5      shape          - input   math ` n    ` where ` ` mean  any number of additional           dimension         - output   math ` n    `  same shape as the input         image     /scripts/activation images/hardshrink png      examples            >>> m = nn hardshrink           >>> input = torch randn 2          >>> output = m input                constants   =  string      lambd  float      def   init   self  lambd  float = 0 5  -> none          super hardshrink  self    init             self lambd = lambd      def forward self  input  tensor  -> tensor          return f hardshrink input  self lambd       def extra repr self  -> str          return string format self lambd  
class hardsigmoid module       r   apply the element-wise function          math           \text hardsigmoid  x  = \begin case              0   \text if~  x \le -3  \\             1   \text if~  x \ge  3  \\             x / 6   1 / 2   \text otherwise          \end case       args          inplace  can optionally do the operation in-place  default  ``false``      shape          - input   math ` n    ` where ` ` mean  any number of additional           dimension         - output   math ` n    `  same shape as the input      examples            >>> m = nn hardsigmoid           >>> input = torch randn 2          >>> output = m input                constants   =  string       inplace  bool      def   init   self  inplace   bool = false  -> none          super hardsigmoid  self    init             self inplace = inplace      def forward self  input  tensor  -> tensor          return f hardsigmoid input  self inplace  
class hardtanh module       r   apply the hardtanh function element-wise      hardtanh be define as          math           \text hardtanh  x  = \begin case              1   \text  if   x > 1 \\             -1   \text  if   x < -1 \\             x   \text  otherwise   \\         \end case       the range of the linear region  math ` -1  1 ` can be adjust use      attr `min val` and  attr `max val`       args          min val  minimum value of the linear region range  default  -1         max val  maximum value of the linear region range  default  1         inplace  can optionally do the operation in-place  default  ``false``      keyword arguments  attr `min value` and  attr `max value`     have be deprecate in favor of  attr `min val` and  attr `max val`       shape          - input   math ` n    ` where ` ` mean  any number of additional           dimension         - output   math ` n    `  same shape as the input         image     /scripts/activation images/hardtanh png      examples            >>> m = nn hardtanh -2  2          >>> input = torch randn 2          >>> output = m input                constants   =  string  string  string       min val  float     max val  float     inplace  bool      def   init            self          min val  float = -1           max val  float = 1           inplace  bool = false          min value  optional float  = none          max value  optional float  = none       -> none          super hardtanh  self    init             if min value be not none              warn warn string              min val = min value         if max value be not none              warn warn string              max val = max value          self min val = min val         self max val = max val         self inplace = inplace         assert self max val > self min val      def forward self  input  tensor  -> tensor          return f hardtanh input  self min val  self max val  self inplace       def extra repr self  -> str          inplace str = string if self inplace else string         return string format              self min val  self max val  inplace str           
class hardswish module       r   apply the hardswish function  element-wise  as describe in the paper       `searching for mobilenetv3`           math           \text hardswish  x  = \begin case              0   \text if~  x \le -3  \\             x   \text if~  x \ge  3  \\             x \cdot  x   3  /6   \text otherwise          \end case       args          inplace  can optionally do the operation in-place  default  ``false``      shape          - input   math ` n    ` where ` ` mean  any number of additional           dimension         - output   math ` n    `  same shape as the input      examples            >>> m = nn hardswish           >>> input = torch randn 2          >>> output = m input           `searching for mobilenetv3`          https //arxiv org/abs/1905 02244               constants   =  string       inplace  bool      def   init   self  inplace   bool = false  -> none          super hardswish  self    init             self inplace = inplace      def forward self  input  tensor  -> tensor          return f hardswish input  self inplace  
class leakyrelu module       r   apply the element-wise function          math           \text leakyrelu  x  = \max 0  x    \text negative\ slope    \min 0  x        or         math           \text leakyrelu  x  =         \begin case          x    \text  if   x \geq 0 \\         \text negative\ slope  \times x    \text  otherwise           \end case       args          negative slope  control the angle of the negative slope  default  1e-2         inplace  can optionally do the operation in-place  default  ``false``      shape          - input   math ` n    ` where ` ` mean  any number of additional           dimension         - output   math ` n    `  same shape as the input         image     /scripts/activation images/leakyrelu png      examples            >>> m = nn leakyrelu 0 1          >>> input = torch randn 2          >>> output = m input                constants   =  string  string      inplace  bool     negative slope  float      def   init   self  negative slope  float = 1e-2  inplace  bool = false  -> none          super leakyrelu  self    init             self negative slope = negative slope         self inplace = inplace      def forward self  input  tensor  -> tensor          return f leaky relu input  self negative slope  self inplace       def extra repr self  -> str          inplace str = string if self inplace else string         return string format self negative slope  inplace str  
class logsigmoid module       r   apply the element-wise function          math           \text logsigmoid  x  = \log\left \frac  1    1   \exp -x  \right       shape          - input   math ` n    ` where ` ` mean  any number of additional           dimension         - output   math ` n    `  same shape as the input         image     /scripts/activation images/logsigmoid png      examples            >>> m = nn logsigmoid           >>> input = torch randn 2          >>> output = m input               def forward self  input  tensor  -> tensor          return f logsigmoid input  
class multiheadattention module       r   allow the model to jointly attend to information     from different representation subspaces      see `attention be all you need <https //arxiv org/abs/1706 03762>`          math           \text multihead  q  k  v  = \text concat  head 1 \dots head h w o      where  math `head i = \text attention  qw i q  kw i k  vw i v `       args          embed dim  total dimension of the model          num head  parallel attention head          dropout  a dropout layer on attn output weight  default  0 0          bias  add bias as module parameter  default  true          add bias kv  add bias to the key and value sequence at dim=0          add zero attn  add a new batch of zero to the key and                        value sequence at dim=1          kdim  total number of feature in key  default  none          vdim  total number of feature in value  default  none          batch first  if ``true``  then the input and output tensors be provide             as  batch  seq  feature   default  ``false``  seq  batch  feature        note that if  attr `kdim` and  attr `vdim` be none  they will be set     to  attr `embed dim` such that query  key  and value have the same     number of feature       examples            >>> multihead attn = nn multiheadattention embed dim  num head          >>> attn output  attn output weight = multihead attn query  key  value                constants   =  string      bias k  optional torch tensor      bias v  optional torch tensor       def   init   self  embed dim  num head  dropout=0   bias=true  add bias kv=false  add zero attn=false                   kdim=none  vdim=none  batch first=false  device=none  dtype=none  -> none          factory kwargs =  string  device  string  dtype          super multiheadattention  self    init             self embed dim = embed dim         self kdim = kdim if kdim be not none else embed dim         self vdim = vdim if vdim be not none else embed dim         self  qkv same embed dim = self kdim == embed dim and self vdim == embed dim          self num head = num head         self dropout = dropout         self batch first = batch first         self head dim = embed dim // num head         assert self head dim   num head == self embed dim  string          if self  qkv same embed dim be false              self q proj weight = parameter torch empty  embed dim  embed dim     factory kwargs               self k proj weight = parameter torch empty  embed dim  self kdim     factory kwargs               self v proj weight = parameter torch empty  embed dim  self vdim     factory kwargs               self register parameter string  none          else              self in proj weight = parameter torch empty  3   embed dim  embed dim     factory kwargs               self register parameter string  none              self register parameter string  none              self register parameter string  none           if bias              self in proj bias = parameter torch empty 3   embed dim    factory kwargs           else              self register parameter string  none          self out proj = nondynamicallyquantizablelinear embed dim  embed dim  bias=bias    factory kwargs           if add bias kv              self bias k = parameter torch empty  1  1  embed dim     factory kwargs               self bias v = parameter torch empty  1  1  embed dim     factory kwargs           else              self bias k = self bias v = none          self add zero attn = add zero attn          self  reset parameters        def  reset parameters self           if self  qkv same embed dim              xavier uniform  self in proj weight          else              xavier uniform  self q proj weight              xavier uniform  self k proj weight              xavier uniform  self v proj weight           if self in proj bias be not none              constant  self in proj bias  0               constant  self out proj bias  0           if self bias k be not none              xavier normal  self bias k          if self bias v be not none              xavier normal  self bias v       def   setstate   self  state                    if string not in state              state string  = true          super multiheadattention  self    setstate   state       def forward self  query  tensor  key  tensor  value  tensor  key pad mask  optional tensor  = none                  need weight  bool = true  attn mask  optional tensor  = none  -> tuple tensor  optional tensor            r        args          query  key  value  map a query and a set of key-value pair to an output              see  attention be all you need  for more detail          key pad mask  if provide  specify pad elements in the key will             be ignore by the attention  when give a binary mask and a value be true              the correspond value on the attention layer will be ignore  when give             a byte mask and a value be non-zero  the correspond value on the attention             layer will be ignore         need weight  output attn output weight          attn mask  2d or 3d mask that prevent attention to certain position  a 2d mask will be broadcast for all             the batch while a 3d mask allow to specify a different mask for the entries of each batch       shape for input          - query   math ` l  n  e ` where l be the target sequence length  n be the batch size  e be           the embed dimension   math ` n  l  e ` if ``batch first`` be ``true``          - key   math ` s  n  e `  where s be the source sequence length  n be the batch size  e be           the embed dimension   math ` n  s  e ` if ``batch first`` be ``true``          - value   math ` s  n  e ` where s be the source sequence length  n be the batch size  e be           the embed dimension   math ` n  s  e ` if ``batch first`` be ``true``          - key pad mask   math ` n  s ` where n be the batch size  s be the source sequence length            if a bytetensor be provide  the non-zero position will be ignore while the position           with the zero position will be unchanged  if a booltensor be provide  the position with the           value of ``true`` will be ignore while the position with the value of ``false`` will be unchanged          - attn mask  if a 2d mask   math ` l  s ` where l be the target sequence length  s be the           source sequence length             if a 3d mask   math ` n\cdot\text num\ head   l  s ` where n be the batch size  l be the target sequence           length  s be the source sequence length  ``attn mask`` ensure that position i be allow to attend           the unmask position  if a bytetensor be provide  the non-zero position be not allow to attend           while the zero position will be unchanged  if a booltensor be provide  position with ``true``           be not allow to attend while ``false`` value will be unchanged  if a floattensor           be provide  it will be add to the attention weight       shape for output          - attn output   math ` l  n  e ` where l be the target sequence length  n be the batch size            e be the embed dimension   math ` n  l  e ` if ``batch first`` be ``true``          - attn output weight   math ` n  l  s ` where n be the batch size            l be the target sequence length  s be the source sequence length                      if self batch first              query  key  value =  x transpose 1  0  for x in  query  key  value            if not self  qkv same embed dim              attn output  attn output weight = f multi head attention forward                  query  key  value  self embed dim  self num head                  self in proj weight  self in proj bias                  self bias k  self bias v  self add zero attn                  self dropout  self out proj weight  self out proj bias                  training=self train                  key pad mask=key pad mask  need weights=need weight                  attn mask=attn mask  use separate proj weight=true                  q proj weight=self q proj weight  k proj weight=self k proj weight                  v proj weight=self v proj weight          else              attn output  attn output weight = f multi head attention forward                  query  key  value  self embed dim  self num head                  self in proj weight  self in proj bias                  self bias k  self bias v  self add zero attn                  self dropout  self out proj weight  self out proj bias                  training=self train                  key pad mask=key pad mask  need weights=need weight                  attn mask=attn mask          if self batch first              return attn output transpose 1  0   attn output weight         else              return attn output  attn output weight 
class prelu module       r   apply the element-wise function          math           \text prelu  x  = \max 0 x    a   \min 0 x       or         math           \text prelu  x  =         \begin case          x    \text  if   x \geq 0 \\         ax    \text  otherwise           \end case       here  math `a` be a learnable parameter  when call without arguments  `nn prelu  ` use a single     parameter  math `a` across all input channel  if call with `nn prelu nchannels `      a separate  math `a` be use for each input channel           note           weight decay should not be use when learn  math `a` for good performance          note           channel dim be the 2nd dim of input  when input have dim < 2  then there be         no channel dim and the number of channel = 1       args          num parameters  int   number of  math `a` to learn              although it take an int as input  there be only two value be legitimate              1  or the number of channel at input  default  1         init  float   the initial value of  math `a`  default  0 25      shape          - input   math ` n    ` where ` ` mean  any number of additional           dimension         - output   math ` n    `  same shape as the input      attribute          weight  tensor   the learnable weight of shape   attr `num parameters`           image     /scripts/activation images/prelu png      examples            >>> m = nn prelu           >>> input = torch randn 2          >>> output = m input                constants   =  string      num parameters  int      def   init   self  num parameters  int = 1  init  float = 0 25                   device=none  dtype=none  -> none          factory kwargs =  string  device  string  dtype          self num parameters = num parameters         super prelu  self    init             self weight = parameter torch empty num parameters    factory kwargs  fill  init        def forward self  input  tensor  -> tensor          return f prelu input  self weight       def extra repr self  -> str          return string format self num parameters  
class relu module       r   apply the rectify linear unit function element-wise        math `\text relu  x  =  x    = \max 0  x `      args          inplace  can optionally do the operation in-place  default  ``false``      shape          - input   math ` n    ` where ` ` mean  any number of additional           dimension         - output   math ` n    `  same shape as the input         image     /scripts/activation images/relu png      examples            >>> m = nn relu           >>> input = torch randn 2          >>> output = m input          an implementation of crelu - https //arxiv org/abs/1603 05201          >>> m = nn relu           >>> input = torch randn 2  unsqueeze 0          >>> output = torch cat  m input  m -input                  constants   =  string      inplace  bool      def   init   self  inplace  bool = false           super relu  self    init             self inplace = inplace      def forward self  input  tensor  -> tensor          return f relu input  inplace=self inplace       def extra repr self  -> str          inplace str = string if self inplace else string         return inplace str 
class relu6 hardtanh       r   apply the element-wise function          math           \text relu6  x  = \min \max 0 x   6       args          inplace  can optionally do the operation in-place  default  ``false``      shape          - input   math ` n    ` where ` ` mean  any number of additional           dimension         - output   math ` n    `  same shape as the input         image     /scripts/activation images/relu6 png      examples            >>> m = nn relu6           >>> input = torch randn 2          >>> output = m input               def   init   self  inplace  bool = false           super relu6  self    init   0   6   inplace       def extra repr self  -> str          inplace str = string if self inplace else string         return inplace str 
class rrelu module       r   apply the randomize leaky rectify liner unit function  element-wise      as describe in the paper       `empirical evaluation of rectify activations in convolutional network`        the function be define as          math           \text rrelu  x  =         \begin case              x   \text if   x \geq 0 \\             ax   \text  otherwise           \end case       where  math `a` be randomly sample from uniform distribution      math `\mathcal u  \text lower   \text upper  `        see  https //arxiv org/pdf/1505 00853 pdf      args          lower  lower bind of the uniform distribution  default   math `\frac 1  8 `         upper  upper bind of the uniform distribution  default   math `\frac 1  3 `         inplace  can optionally do the operation in-place  default  ``false``      shape          - input   math ` n    ` where ` ` mean  any number of additional           dimension         - output   math ` n    `  same shape as the input      examples            >>> m = nn rrelu 0 1  0 3          >>> input = torch randn 2          >>> output = m input           `empirical evaluation of rectify activations in convolutional network`          https //arxiv org/abs/1505 00853               constants   =  string  string  string       lower  float     upper  float     inplace  bool      def   init            self          lower  float = 1  / 8          upper  float = 1  / 3          inplace  bool = false                super rrelu  self    init             self lower = lower         self upper = upper         self inplace = inplace      def forward self  input  tensor  -> tensor          return f rrelu input  self lower  self upper  self train  self inplace       def extra repr self           inplace str = string if self inplace else string         return string format self lower  self upper  inplace str  
class selu module       r   apply element-wise  as          math           \text selu  x  = \text scale     \max 0 x    \min 0  \alpha    \exp x  - 1         with  math `\alpha = 1 6732632423543772848170429916717` and      math `\text scale  = 1 0507009873554804934193349852946`          warn           when use ``kaiming normal`` or ``kaiming normal `` for initialisation          ``nonlinearity= linear `` should be use instead of ``nonlinearity= selu ``         in order to get `self-normalizing neural networks`           see  func `torch nn init calculate gain` for more information       more detail can be find in the paper `self-normalizing neural networks`         args          inplace  bool  optional   can optionally do the operation in-place  default  ``false``      shape          - input   math ` n    ` where ` ` mean  any number of additional           dimension         - output   math ` n    `  same shape as the input         image     /scripts/activation images/selu png      examples            >>> m = nn selu           >>> input = torch randn 2          >>> output = m input           self-normalizing neural network  https //arxiv org/abs/1706 02515               constants   =  string      inplace  bool      def   init   self  inplace  bool = false  -> none          super selu  self    init             self inplace = inplace      def forward self  input  tensor  -> tensor          return f selu input  self inplace       def extra repr self  -> str          inplace str = string if self inplace else string         return inplace str 
class celu module       r   apply the element-wise function          math           \text celu  x  = \max 0 x    \min 0  \alpha    \exp x/\alpha  - 1        more detail can be find in the paper `continuously differentiable exponential linear units`         args          alpha  the  math `\alpha` value for the celu formulation  default  1 0         inplace  can optionally do the operation in-place  default  ``false``      shape          - input   math ` n    ` where ` ` mean  any number of additional           dimension         - output   math ` n    `  same shape as the input         image     /scripts/activation images/celu png      examples            >>> m = nn celu           >>> input = torch randn 2          >>> output = m input           `continuously differentiable exponential linear units`          https //arxiv org/abs/1704 07483               constants   =  string  string      alpha  float     inplace  bool      def   init   self  alpha  float = 1   inplace  bool = false  -> none          super celu  self    init             self alpha = alpha         self inplace = inplace      def forward self  input  tensor  -> tensor          return f celu input  self alpha  self inplace       def extra repr self  -> str          inplace str = string if self inplace else string         return string format self alpha  inplace str  
class gelu module       r   apply the gaussian error linear units function          math   \text gelu  x  = x   \phi x       where  math `\phi x ` be the cumulative distribution function for gaussian distribution       shape          - input   math ` n    ` where ` ` mean  any number of additional           dimension         - output   math ` n    `  same shape as the input         image     /scripts/activation images/gelu png      examples            >>> m = nn gelu           >>> input = torch randn 2          >>> output = m input              def forward self  input  tensor  -> tensor          return f gelu input  
class sigmoid module       r   apply the element-wise function          math           \text sigmoid  x  = \sigma x  = \frac 1  1   \exp -x         shape          - input   math ` n    ` where ` ` mean  any number of additional           dimension         - output   math ` n    `  same shape as the input         image     /scripts/activation images/sigmoid png      examples            >>> m = nn sigmoid           >>> input = torch randn 2          >>> output = m input               def forward self  input  tensor  -> tensor          return torch sigmoid input  
class silu module       r   apply the sigmoid linear unit  silu  function  element-wise      the silu function be also know as the swish function          math           \text silu  x  = x   \sigma x   \text where   \sigma x  \text  be the logistic sigmoid           note           see `gaussian error linear units  gelus  <https //arxiv org/abs/1606 08415>`          where the silu  sigmoid linear unit  be originally coin  and see         `sigmoid-weighted linear units for neural network function approximation         in reinforcement learn <https //arxiv org/abs/1702 03118>`  and `swish          a self-gated activation function <https //arxiv org/abs/1710 05941v1>`          where the silu be experiment with later       shape          - input   math ` n    ` where ` ` mean  any number of additional           dimension         - output   math ` n    `  same shape as the input      examples            >>> m = nn silu           >>> input = torch randn 2          >>> output = m input                constants   =  string      inplace  bool      def   init   self  inplace  bool = false           super silu  self    init             self inplace = inplace      def forward self  input  tensor  -> tensor          return f silu input  inplace=self inplace       def extra repr self  -> str          inplace str = string if self inplace else string         return inplace str 
class mish module       r   apply the mish function  element-wise      mish  a self regularize non-monotonic neural activation function          math           \text mish  x  = x   \text tanh  \text softplus  x           note           see `mish  a self regularize non-monotonic neural activation function <https //arxiv org/abs/1908 08681>`       shape          - input   math ` n    ` where ` ` mean  any number of additional           dimension         - output   math ` n    `  same shape as the input      examples            >>> m = nn mish           >>> input = torch randn 2          >>> output = m input                constants   =  string      inplace  bool      def   init   self  inplace  bool = false           super mish  self    init             self inplace = inplace      def forward self  input  tensor  -> tensor          return f mish input  inplace=self inplace       def extra repr self  -> str          inplace str = string if self inplace else string         return inplace str 
class softplus module       r   apply the element-wise function          math           \text softplus  x  = \frac 1  \beta    \log 1   \exp \beta   x        softplus be a smooth approximation to the relu function and can be use     to constrain the output of a machine to always be positive       for numerical stability the implementation revert to the linear function     when  math `input \times \beta > threshold`       args          beta  the  math `\beta` value for the softplus formulation  default  1         threshold  value above this revert to a linear function  default  20      shape          - input   math ` n    ` where ` ` mean  any number of additional           dimension         - output   math ` n    `  same shape as the input         image     /scripts/activation images/softplus png      examples            >>> m = nn softplus           >>> input = torch randn 2          >>> output = m input                constants   =  string  string      beta  int     threshold  int      def   init   self  beta  int = 1  threshold  int = 20  -> none          super softplus  self    init             self beta = beta         self threshold = threshold      def forward self  input  tensor  -> tensor          return f softplus input  self beta  self threshold       def extra repr self  -> str          return string format self beta  self threshold  
class softshrink module       r   apply the soft shrinkage function elementwise          math           \text softshrinkage  x  =         \begin case          x - \lambda    \text  if   x > \lambda \\         x   \lambda    \text  if   x < -\lambda \\         0    \text  otherwise           \end case       args          lambd  the  math `\lambda`  must be no less than zero  value for the softshrink formulation  default  0 5      shape          - input   math ` n    ` where ` ` mean  any number of additional           dimension         - output   math ` n    `  same shape as the input         image     /scripts/activation images/softshrink png      examples            >>> m = nn softshrink           >>> input = torch randn 2          >>> output = m input                constants   =  string      lambd  float      def   init   self  lambd  float = 0 5  -> none          super softshrink  self    init             self lambd = lambd      def forward self  input  tensor  -> tensor          return f softshrink input  self lambd       def extra repr self  -> str          return str self lambd  
class softsign module       r   apply the element-wise function          math           \text softsign  x  = \frac x   1    x        shape          - input   math ` n    ` where ` ` mean  any number of additional           dimension         - output   math ` n    `  same shape as the input         image     /scripts/activation images/softsign png      examples            >>> m = nn softsign           >>> input = torch randn 2          >>> output = m input               def forward self  input  tensor  -> tensor          return f softsign input  
class tanh module       r   apply the element-wise function          math           \text tanh  x  = \tanh x  = \frac \exp x  - \exp -x    \exp x    \exp -x        shape          - input   math ` n    ` where ` ` mean  any number of additional           dimension         - output   math ` n    `  same shape as the input         image     /scripts/activation images/tanh png      examples            >>> m = nn tanh           >>> input = torch randn 2          >>> output = m input               def forward self  input  tensor  -> tensor          return torch tanh input  
class tanhshrink module       r   apply the element-wise function          math           \text tanhshrink  x  = x - \tanh x       shape          - input   math ` n    ` where ` ` mean  any number of additional           dimension         - output   math ` n    `  same shape as the input         image     /scripts/activation images/tanhshrink png      examples            >>> m = nn tanhshrink           >>> input = torch randn 2          >>> output = m input               def forward self  input  tensor  -> tensor          return f tanhshrink input  
class threshold module       r   thresholds each element of the input tensor       threshold be define as          math           y =         \begin case          x   \text  if   x > \text threshold  \\         \text value    \text  otherwise           \end case       args          threshold  the value to threshold at         value  the value to replace with         inplace  can optionally do the operation in-place  default  ``false``      shape          - input   math ` n    ` where ` ` mean  any number of additional           dimension         - output   math ` n    `  same shape as the input      examples            >>> m = nn threshold 0 1  20          >>> input = torch randn 2          >>> output = m input                constants   =  string  string  string       threshold  float     value  float     inplace  bool      def   init   self  threshold  float  value  float  inplace  bool = false  -> none          super threshold  self    init             self threshold = threshold         self value = value         self inplace = inplace               def forward self  input  tensor  -> tensor          return f threshold input  self threshold  self value  self inplace       def extra repr self           inplace str = string if self inplace else string         return string format              self threshold  self value  inplace str           
class glu module       r   apply the gate linear unit function      math ` glu  a  b = a \otimes \sigma b ` where  math `a` be the first half     of the input matrices and  math `b` be the second half       args          dim  int   the dimension on which to split the input  default  -1      shape          - input   math ` \ast 1  n  \ast 2 ` where ` ` mean  any number of additional           dimension         - output   math ` \ast 1  m  \ast 2 ` where  math `m=n/2`      examples            >>> m = nn glu           >>> input = torch randn 4  2          >>> output = m input                constants   =  string      dim  int      def   init   self  dim  int = -1  -> none          super glu  self    init             self dim = dim      def forward self  input  tensor  -> tensor          return f glu input  self dim       def extra repr self  -> str          return string format self dim  
class softmin module       r   apply the softmin function to an n-dimensional input tensor     rescale them so that the elements of the n-dimensional output tensor     lie in the range ` 0  1 ` and sum to 1       softmin be define as          math           \text softmin  x  i   = \frac \exp -x i   \sum j \exp -x j        shape          - input   math `   ` where ` ` mean  any number of additional           dimension         - output   math `   `  same shape as the input      args          dim  int   a dimension along which softmin will be compute  so every slice             along dim will sum to 1        return          a tensor of the same dimension and shape as the input  with         value in the range  0  1       examples            >>> m = nn softmin           >>> input = torch randn 2  3          >>> output = m input                constants   =  string      dim  optional int       def   init   self  dim  optional int  = none  -> none          super softmin  self    init             self dim = dim      def   setstate   self  state           self   dict   update state          if not hasattr self  string               self dim = none      def forward self  input  tensor  -> tensor          return f softmin input  self dim   stacklevel=5       def extra repr self           return string format dim=self dim  
class softmax module       r   apply the softmax function to an n-dimensional input tensor     rescale them so that the elements of the n-dimensional output tensor     lie in the range  0 1  and sum to 1       softmax be define as          math           \text softmax  x  i   = \frac \exp x i   \sum j \exp x j        when the input tensor be a sparse tensor then the unspecifed     value be treat as ``-inf``       shape          - input   math `   ` where ` ` mean  any number of additional           dimension         - output   math `   `  same shape as the input      return          a tensor of the same dimension and shape as the input with         value in the range  0  1       args          dim  int   a dimension along which softmax will be compute  so every slice             along dim will sum to 1           note           this module doesn t work directly with nllloss          which expect the log to be compute between the softmax and itself          use `logsoftmax` instead  it s faster and have better numerical properties        examples            >>> m = nn softmax dim=1          >>> input = torch randn 2  3          >>> output = m input                 constants   =  string      dim  optional int       def   init   self  dim  optional int  = none  -> none          super softmax  self    init             self dim = dim      def   setstate   self  state           self   dict   update state          if not hasattr self  string               self dim = none      def forward self  input  tensor  -> tensor          return f softmax input  self dim   stacklevel=5       def extra repr self  -> str          return string format dim=self dim  
class softmax2d module       r   apply softmax over feature to each spatial location       when give an image of ``channels x height x width``  it will     apply `softmax` to each location  math ` channel  h i  w j `      shape          - input   math ` n  c  h  w `         - output   math ` n  c  h  w `  same shape as input       return          a tensor of the same dimension and shape as the input with         value in the range  0  1       examples            >>> m = nn softmax2d           >>>   you softmax over the 2nd dimension         >>> input = torch randn 2  3  12  13          >>> output = m input               def forward self  input  tensor  -> tensor          assert input dim   == 4  string         return f softmax input  1   stacklevel=5  
class logsoftmax module       r   apply the  math `\log \text softmax  x  ` function to an n-dimensional     input tensor  the logsoftmax formulation can be simplify as          math           \text logsoftmax  x  i   = \log\left \frac \exp x i     \sum j \exp x j   \right       shape          - input   math `   ` where ` ` mean  any number of additional           dimension         - output   math `   `  same shape as the input      args          dim  int   a dimension along which logsoftmax will be compute       return          a tensor of the same dimension and shape as the input with         value in the range  -inf  0       examples            >>> m = nn logsoftmax           >>> input = torch randn 2  3          >>> output = m input                constants   =  string      dim  optional int       def   init   self  dim  optional int  = none  -> none          super logsoftmax  self    init             self dim = dim      def   setstate   self  state           self   dict   update state          if not hasattr self  string               self dim = none      def forward self  input  tensor  -> tensor          return f log softmax input  self dim   stacklevel=5       def extra repr self           return string format dim=self dim  
class adaptivelogsoftmaxwithloss module       r   efficient softmax approximation as describe in     `efficient softmax approximation for gpus by edouard grave  armand joulin      moustapha ciss  david grangier  and herv jgou     <https //arxiv org/abs/1609 04309>`         adaptive softmax be an approximate strategy for train model with large     output space  it be most effective when the label distribution be highly     imbalanced  for example in natural language model  where the word     frequency distribution approximately follow the `zipf s law`        adaptive softmax partition the label into several cluster  accord to     their frequency  these cluster may contain different number of target     each      additionally  cluster contain less frequent label assign lower     dimensional embeddings to those label  which speed up the computation      for each minibatch  only cluster for which at least one target be     present be evaluate       the idea be that the cluster which be access frequently      like the first one  contain most frequent label   should also be cheap     to compute -- that be  contain a small number of assign label       we highly recommend take a look at the original paper for more detail          attr `cutoffs` should be an order sequence of integers sort       in the increase order        it control number of cluster and the partition of target into       cluster  for example set ``cutoffs =  10  100  1000 ``       mean that first `10` target will be assign       to the  head  of the adaptive softmax  target `11  12       100` will be       assign to the first cluster  and target `101  102       1000` will be       assign to the second cluster  while target       `1001  1002       n class - 1` will be assign       to the last  third cluster          attr `div value` be use to compute the size of each additional cluster        which be give as        math `\left\lfloor\frac \texttt in\ feature   \texttt div\ value   idx  \right\rfloor`        where  math `idx` be the cluster index  with cluster       for less frequent word have larger indices        and indices start from  math `1`           attr `head bias` if set to true  add a bias term to the  head  of the       adaptive softmax  see paper for detail  set to false in the official       implementation          warn           label pass as input to this module should be sort accord to         their frequency  this mean that the most frequent label should be         represent by the index `0`  and the least frequent         label should be represent by the index `n class - 1`          note           this module return a ``namedtuple`` with ``output``         and ``loss`` field  see further documentation for detail          note           to compute log-probabilities for all class  the ``log prob``         method can be use       args          in feature  int   number of feature in the input tensor         n class  int   number of class in the dataset         cutoffs  sequence   cutoffs use to assign target to their bucket         div value  float  optional   value use as an exponent to compute size             of the cluster  default  4 0         head bias  bool  optional   if ``true``  add a bias term to the  head  of the             adaptive softmax  default  ``false``      return          ``namedtuple`` with ``output`` and ``loss`` field                  output   be a tensor of size ``n`` contain compute target               log probabilities for each example                 loss   be a scalar represent the compute negative               log likelihood loss      shape          - input   math ` n  \texttt in\ feature  `         - target   math ` n ` where each value satisfy  math `0 <= \texttt target i   <= \texttt n\ class `         - output1   math ` n `         - output2  ``scalar``          zipf s law  https //en wikipedia org/wiki/zipf 27s law              in feature  int     n class  int     cutoffs  list int      div value  float     head bias  bool     head  linear     tail  modulelist      def   init            self          in feature  int          n class  int          cutoffs  sequence int           div value  float = 4           head bias  bool = false          device=none          dtype=none       -> none          factory kwargs =  string  device  string  dtype          super adaptivelogsoftmaxwithloss  self    init              cutoffs = list cutoffs           if  cutoffs  = sort cutoffs   \                 or  min cutoffs  <= 0  \                 or  max cutoffs  >  n class - 1   \                 or  len set cutoffs    = len cutoffs   \                 or any  int c   = c for c in cutoffs                 raise valueerror string                              string                              string           self in feature = in feature         self n class = n class         self cutoffs = cutoffs    n class          self div value = div value         self head bias = head bias          self shortlist size = self cutoffs 0          self n cluster = len self cutoffs  - 1         self head size = self shortlist size   self n cluster          self head = linear self in feature  self head size  bias=self head bias                               factory kwargs          self tail = modulelist            for i in range self n cluster                hsz = int self in feature //  self div value     i   1                osz = self cutoffs i   1  - self cutoffs i               projection = sequential                  linear self in feature  hsz  bias=false    factory kwargs                   linear hsz  osz  bias=false    factory kwargs                              self tail append projection       def reset parameters self  -> none          self head reset parameters           for i2h  h2o in self tail              i2h reset parameters               h2o reset parameters        def forward self  input  tensor  target  tensor  ->  asmoutput          if input size 0   = target size 0               raise runtimeerror string                                string           use row = 0         batch size = target size 0           output = input new zero batch size          gather inds = target new empty batch size           cutoff value =  0    self cutoffs         for i in range len cutoff value  - 1                low idx = cutoff value i              high idx = cutoff value i   1               target mask =  target >= low idx     target < high idx              row indices = target mask nonzero   squeeze                if row indices numel   == 0                  continue              if i == 0                  gather inds index copy  0  row indices  target target mask                else                  relative target = target target mask  - low idx                 input subset = input index select 0  row indices                   cluster output = self tail i - 1  input subset                  cluster index = self shortlist size   i - 1                  gather inds index fill  0  row indices  cluster index                   cluster logprob = log softmax cluster output  dim=1                  local logprob = cluster logprob gather 1  relative target unsqueeze 1                   output index copy  0  row indices  local logprob squeeze 1                use row  = row indices numel            if use row  = batch size              raise runtimeerror string                                string                                string format self n class - 1                                                       target min   item                                                         target max   item              head output = self head input          head logprob = log softmax head output  dim=1          output  = head logprob gather 1  gather inds unsqueeze 1   squeeze           loss =  -output  mean            return  asmoutput output  loss       def  get full log prob self  input  head output           string          out = input new empty  head output size 0   self n class           head logprob = log softmax head output  dim=1           out     self shortlist size  = head logprob     self shortlist size           for i   start idx  stop idx  in enumerate zip self cutoffs  self cutoffs 1                  cluster output = self tail i  input              cluster logprob = log softmax cluster output  dim=1              output logprob = cluster logprob   head logprob    self shortlist size   i  unsqueeze 1               out    start idx stop idx  = output logprob          return out      def log prob self  input  tensor  -> tensor          r    compute log probabilities for all  math `\texttt n\ class `          args              input  tensor   a minibatch of examples          return              log-probabilities of for each class  math `c`             in range  math `0 <= c <= \texttt n\ class `  where  math `\texttt n\ class ` be a             parameter pass to ``adaptivelogsoftmaxwithloss`` constructor           shape              - input   math ` n  \texttt in\ feature  `             - output   math ` n  \texttt n\ class  `                       head output = self head input          return self  get full log prob input  head output       def predict self  input  tensor  -> tensor          r    this be equivalent to `self log pob input  argmax dim=1 `          but be more efficient in some case           args              input  tensor   a minibatch of examples          return              output  tensor   a class with the highest probability for each example          shape              - input   math ` n  \texttt in\ feature  `             - output   math ` n `                      head output = self head input          output = torch argmax head output  dim=1          not in shortlist =  output >= self shortlist size          all in shortlist = not  not in shortlist any             if all in shortlist              return output          elif not in shortlist all                log prob = self  get full log prob input  head output              return torch argmax log prob  dim=1           else              log prob = self  get full log prob input not in shortlist                                                  head output not in shortlist               output not in shortlist  = torch argmax log prob  dim=1              return output 
class batchnorm1d  batchnorm       r   apply batch normalization over a 2d or 3d input  a mini-batch of 1d     input with optional additional channel dimension  as describe in the paper     `batch normalization  accelerate deep network train by reduce     internal covariate shift <https //arxiv org/abs/1502 03167>`             math            y = \frac x - \mathrm e  x   \sqrt \mathrm var  x    \epsilon     \gamma   \beta      the mean and standard-deviation be calculate per-dimension over     the mini-batches and  math `\gamma` and  math `\beta` be learnable parameter vectors     of size `c`  where `c` be the input size   by default  the elements of  math `\gamma` be set     to 1 and the elements of  math `\beta` be set to 0  the standard-deviation be calculate     via the bias estimator  equivalent to `torch var input  unbiased=false `       also by default  during train this layer keep run estimate of its     compute mean and variance  which be then use for normalization during     evaluation  the run estimate be keep with a default  attr `momentum`     of 0 1       if  attr `track run stats` be set to ``false``  this layer then do not     keep run estimate  and batch statistics be instead use during     evaluation time as well          note           this  attr `momentum` argument be different from one use in optimizer         class and the conventional notion of momentum  mathematically  the         update rule for run statistics here be          math `\hat x  \text new  =  1 - \text momentum   \times \hat x    \text momentum  \times x t`          where  math `\hat x ` be the estimate statistic and  math `x t` be the         new observe value       because the batch normalization be do over the `c` dimension  compute statistics     on ` n  l ` slice  it s common terminology to call this temporal batch normalization       args          num feature   math `c` from an expect input of size              math ` n  c  l ` or  math `l` from input of size  math ` n  l `         eps  a value add to the denominator for numerical stability              default  1e-5         momentum  the value use for the run mean and run var             computation  can be set to ``none`` for cumulative move average              i e  simple average   default  0 1         affine  a boolean value that when set to ``true``  this module have             learnable affine parameters  default  ``true``         track run stats  a boolean value that when set to ``true``  this             module track the run mean and variance  and when set to ``false``              this module do not track such statistics  and initialize statistics             buffer  attr `running mean` and  attr `running var` as ``none``              when these buffer be ``none``  this module always use batch statistics              in both train and eval modes  default  ``true``      shape          - input   math ` n  c ` or  math ` n  c  l `         - output   math ` n  c ` or  math ` n  c  l `  same shape as input       examples            >>>   with learnable parameters         >>> m = nn batchnorm1d 100          >>>   without learnable parameters         >>> m = nn batchnorm1d 100  affine=false          >>> input = torch randn 20  100          >>> output = m input               def  check input dim self  input           if input dim    = 2 and input dim    = 3              raise valueerror                  string format input dim                  
class batchnorm2d  batchnorm       r   apply batch normalization over a 4d input  a mini-batch of 2d input     with additional channel dimension  as describe in the paper     `batch normalization  accelerate deep network train by reduce     internal covariate shift <https //arxiv org/abs/1502 03167>`             math            y = \frac x - \mathrm e  x    \sqrt \mathrm var  x    \epsilon     \gamma   \beta      the mean and standard-deviation be calculate per-dimension over     the mini-batches and  math `\gamma` and  math `\beta` be learnable parameter vectors     of size `c`  where `c` be the input size   by default  the elements of  math `\gamma` be set     to 1 and the elements of  math `\beta` be set to 0  the standard-deviation be calculate     via the bias estimator  equivalent to `torch var input  unbiased=false `       also by default  during train this layer keep run estimate of its     compute mean and variance  which be then use for normalization during     evaluation  the run estimate be keep with a default  attr `momentum`     of 0 1       if  attr `track run stats` be set to ``false``  this layer then do not     keep run estimate  and batch statistics be instead use during     evaluation time as well          note           this  attr `momentum` argument be different from one use in optimizer         class and the conventional notion of momentum  mathematically  the         update rule for run statistics here be          math `\hat x  \text new  =  1 - \text momentum   \times \hat x    \text momentum  \times x t`          where  math `\hat x ` be the estimate statistic and  math `x t` be the         new observe value       because the batch normalization be do over the `c` dimension  compute statistics     on ` n  h  w ` slice  it s common terminology to call this spatial batch normalization       args          num feature   math `c` from an expect input of size              math ` n  c  h  w `         eps  a value add to the denominator for numerical stability              default  1e-5         momentum  the value use for the run mean and run var             computation  can be set to ``none`` for cumulative move average              i e  simple average   default  0 1         affine  a boolean value that when set to ``true``  this module have             learnable affine parameters  default  ``true``         track run stats  a boolean value that when set to ``true``  this             module track the run mean and variance  and when set to ``false``              this module do not track such statistics  and initialize statistics             buffer  attr `running mean` and  attr `running var` as ``none``              when these buffer be ``none``  this module always use batch statistics              in both train and eval modes  default  ``true``      shape          - input   math ` n  c  h  w `         - output   math ` n  c  h  w `  same shape as input       examples            >>>   with learnable parameters         >>> m = nn batchnorm2d 100          >>>   without learnable parameters         >>> m = nn batchnorm2d 100  affine=false          >>> input = torch randn 20  100  35  45          >>> output = m input               def  check input dim self  input           if input dim    = 4              raise valueerror string format input dim     
class batchnorm3d  batchnorm       r   apply batch normalization over a 5d input  a mini-batch of 3d input     with additional channel dimension  as describe in the paper     `batch normalization  accelerate deep network train by reduce     internal covariate shift <https //arxiv org/abs/1502 03167>`             math            y = \frac x - \mathrm e  x    \sqrt \mathrm var  x    \epsilon     \gamma   \beta      the mean and standard-deviation be calculate per-dimension over     the mini-batches and  math `\gamma` and  math `\beta` be learnable parameter vectors     of size `c`  where `c` be the input size   by default  the elements of  math `\gamma` be set     to 1 and the elements of  math `\beta` be set to 0  the standard-deviation be calculate     via the bias estimator  equivalent to `torch var input  unbiased=false `       also by default  during train this layer keep run estimate of its     compute mean and variance  which be then use for normalization during     evaluation  the run estimate be keep with a default  attr `momentum`     of 0 1       if  attr `track run stats` be set to ``false``  this layer then do not     keep run estimate  and batch statistics be instead use during     evaluation time as well          note           this  attr `momentum` argument be different from one use in optimizer         class and the conventional notion of momentum  mathematically  the         update rule for run statistics here be          math `\hat x  \text new  =  1 - \text momentum   \times \hat x    \text momentum  \times x t`          where  math `\hat x ` be the estimate statistic and  math `x t` be the         new observe value       because the batch normalization be do over the `c` dimension  compute statistics     on ` n  d  h  w ` slice  it s common terminology to call this volumetric batch normalization     or spatio-temporal batch normalization       args          num feature   math `c` from an expect input of size              math ` n  c  d  h  w `         eps  a value add to the denominator for numerical stability              default  1e-5         momentum  the value use for the run mean and run var             computation  can be set to ``none`` for cumulative move average              i e  simple average   default  0 1         affine  a boolean value that when set to ``true``  this module have             learnable affine parameters  default  ``true``         track run stats  a boolean value that when set to ``true``  this             module track the run mean and variance  and when set to ``false``              this module do not track such statistics  and initialize statistics             buffer  attr `running mean` and  attr `running var` as ``none``              when these buffer be ``none``  this module always use batch statistics              in both train and eval modes  default  ``true``      shape          - input   math ` n  c  d  h  w `         - output   math ` n  c  d  h  w `  same shape as input       examples            >>>   with learnable parameters         >>> m = nn batchnorm3d 100          >>>   without learnable parameters         >>> m = nn batchnorm3d 100  affine=false          >>> input = torch randn 20  100  35  45  10          >>> output = m input               def  check input dim self  input           if input dim    = 5              raise valueerror string format input dim     
class lazybatchnorm1d  lazybatchnorm       r   a  class `torch nn batchnorm1d` module with lazy initialization of     the ``num features`` argument of the  class `batchnorm1d` that be infer     from the ``input size 1 ``      the attribute that will be lazily initialize be `weight`  `bias`      `running mean` and `running var`       check the  class `torch nn modules lazy lazymodulemixin` for further documentation     on lazy modules and their limitations       args          eps  a value add to the denominator for numerical stability              default  1e-5         momentum  the value use for the run mean and run var             computation  can be set to ``none`` for cumulative move average              i e  simple average   default  0 1         affine  a boolean value that when set to ``true``  this module have             learnable affine parameters  default  ``true``         track run stats  a boolean value that when set to ``true``  this             module track the run mean and variance  and when set to ``false``              this module do not track such statistics  and initialize statistics             buffer  attr `running mean` and  attr `running var` as ``none``              when these buffer be ``none``  this module always use batch statistics              in both train and eval modes  default  ``true``              cls to become = batchnorm1d        def  check input dim self  input           if input dim    = 2 and input dim    = 3              raise valueerror                  string format input dim                  
class lazybatchnorm2d  lazybatchnorm       r   a  class `torch nn batchnorm2d` module with lazy initialization of     the ``num features`` argument of the  class `batchnorm2d` that be infer     from the ``input size 1 ``      the attribute that will be lazily initialize be `weight`  `bias`      `running mean` and `running var`       check the  class `torch nn modules lazy lazymodulemixin` for further documentation     on lazy modules and their limitations       args          eps  a value add to the denominator for numerical stability              default  1e-5         momentum  the value use for the run mean and run var             computation  can be set to ``none`` for cumulative move average              i e  simple average   default  0 1         affine  a boolean value that when set to ``true``  this module have             learnable affine parameters  default  ``true``         track run stats  a boolean value that when set to ``true``  this             module track the run mean and variance  and when set to ``false``              this module do not track such statistics  and initialize statistics             buffer  attr `running mean` and  attr `running var` as ``none``              when these buffer be ``none``  this module always use batch statistics              in both train and eval modes  default  ``true``              cls to become = batchnorm2d        def  check input dim self  input           if input dim    = 4              raise valueerror string format input dim     
class lazybatchnorm3d  lazybatchnorm       r   a  class `torch nn batchnorm3d` module with lazy initialization of     the ``num features`` argument of the  class `batchnorm3d` that be infer     from the ``input size 1 ``      the attribute that will be lazily initialize be `weight`  `bias`      `running mean` and `running var`       check the  class `torch nn modules lazy lazymodulemixin` for further documentation     on lazy modules and their limitations       args          eps  a value add to the denominator for numerical stability              default  1e-5         momentum  the value use for the run mean and run var             computation  can be set to ``none`` for cumulative move average              i e  simple average   default  0 1         affine  a boolean value that when set to ``true``  this module have             learnable affine parameters  default  ``true``         track run stats  a boolean value that when set to ``true``  this             module track the run mean and variance  and when set to ``false``              this module do not track such statistics  and initialize statistics             buffer  attr `running mean` and  attr `running var` as ``none``              when these buffer be ``none``  this module always use batch statistics              in both train and eval modes  default  ``true``              cls to become = batchnorm3d        def  check input dim self  input           if input dim    = 5              raise valueerror string format input dim     
class groupnorm module       r   apply group normalization over a mini-batch of input as describe in     the paper `group normalization <https //arxiv org/abs/1803 08494>`           math           y = \frac x - \mathrm e  x    \sqrt \mathrm var  x    \epsilon     \gamma   \beta      the input channel be separate into  attr `num groups` group  each contain     ``num channel / num groups`` channel  the mean and standard-deviation be calculate     separately over the each group   math `\gamma` and  math `\beta` be learnable     per-channel affine transform parameter vectors of size  attr `num channels` if      attr `affine` be ``true``      the standard-deviation be calculate via the bias estimator  equivalent to     `torch var input  unbiased=false `       this layer use statistics compute from input data in both train and     evaluation modes       args          num group  int   number of group to separate the channel into         num channel  int   number of channel expect in input         eps  a value add to the denominator for numerical stability  default  1e-5         affine  a boolean value that when set to ``true``  this module             have learnable per-channel affine parameters initialize to ones  for weight              and zero  for bias   default  ``true``       shape          - input   math ` n  c    ` where  math `c=\text num\ channel `         - output   math ` n  c    `  same shape as input       examples            >>> input = torch randn 20  6  10  10          >>>   separate 6 channel into 3 group         >>> m = nn groupnorm 3  6          >>>   separate 6 channel into 6 group  equivalent with instancenorm          >>> m = nn groupnorm 6  6          >>>   put all 6 channel into a single group  equivalent with layernorm          >>> m = nn groupnorm 1  6          >>>   activate the module         >>> output = m input                constants   =  string  string  string  string      num group  int     num channel  int     eps  float     affine  bool      def   init   self  num group  int  num channel  int  eps  float = 1e-5  affine  bool = true                   device=none  dtype=none  -> none          factory kwargs =  string  device  string  dtype          super groupnorm  self    init             self num group = num group         self num channel = num channel         self eps = eps         self affine = affine         if self affine              self weight = parameter torch empty num channel    factory kwargs               self bias = parameter torch empty num channel    factory kwargs           else              self register parameter string  none              self register parameter string  none           self reset parameters        def reset parameters self  -> none          if self affine              init ones  self weight              init zero  self bias       def forward self  input  tensor  -> tensor          return f group norm              input  self num group  self weight  self bias  self eps       def extra repr self  -> str          return string \             string format   self   dict    
class syncbatchnorm  batchnorm       r   apply batch normalization over a n-dimensional input  a mini-batch of  n-2 d input     with additional channel dimension  as describe in the paper     `batch normalization  accelerate deep network train by reduce     internal covariate shift <https //arxiv org/abs/1502 03167>`             math            y = \frac x - \mathrm e  x    \sqrt \mathrm var  x    \epsilon     \gamma   \beta      the mean and standard-deviation be calculate per-dimension over all     mini-batches of the same process group   math `\gamma` and  math `\beta`     be learnable parameter vectors of size `c`  where `c` be the input size       by default  the elements of  math `\gamma` be sample from      math `\mathcal u  0  1 ` and the elements of  math `\beta` be set to 0      the standard-deviation be calculate via the bias estimator  equivalent to     `torch var input  unbiased=false `       also by default  during train this layer keep run estimate of its     compute mean and variance  which be then use for normalization during     evaluation  the run estimate be keep with a default  attr `momentum`     of 0 1       if  attr `track run stats` be set to ``false``  this layer then do not     keep run estimate  and batch statistics be instead use during     evaluation time as well          note           this  attr `momentum` argument be different from one use in optimizer         class and the conventional notion of momentum  mathematically  the         update rule for run statistics here be          math `\hat x  \text new  =  1 - \text momentum   \times \hat x    \text momentum  \times x t`          where  math `\hat x ` be the estimate statistic and  math `x t` be the         new observe value       because the batch normalization be do for each channel in the ``c`` dimension  compute     statistics on `` n    `` slice  it s common terminology to call this volumetric batch     normalization or spatio-temporal batch normalization       currently  class `syncbatchnorm` only support      class `~torch nn distributeddataparallel`  ddp  with single gpu per process  use      meth `torch nn syncbatchnorm convert sync batchnorm  ` to convert      attr `batchnorm d` layer to  class `syncbatchnorm` before wrap     network with ddp       args          num feature   math `c` from an expect input of size              math ` n  c    `         eps  a value add to the denominator for numerical stability              default  ``1e-5``         momentum  the value use for the run mean and run var             computation  can be set to ``none`` for cumulative move average              i e  simple average   default  0 1         affine  a boolean value that when set to ``true``  this module have             learnable affine parameters  default  ``true``         track run stats  a boolean value that when set to ``true``  this             module track the run mean and variance  and when set to ``false``              this module do not track such statistics  and initialize statistics             buffer  attr `running mean` and  attr `running var` as ``none``              when these buffer be ``none``  this module always use batch statistics              in both train and eval modes  default  ``true``         process group  synchronization of stats happen within each process group             individually  default behavior be synchronization across the whole             world      shape          - input   math ` n  c    `         - output   math ` n  c    `  same shape as input          note           synchronization of batchnorm statistics occur only while train  i e          synchronization be disable when ``model eval  `` be set or if         ``self training`` be otherwise ``false``       examples            >>>   with learnable parameters         >>> m = nn syncbatchnorm 100          >>>   create process group  optional          >>>   rank be a list of int identify rank ids          >>> rank = list range 8           >>> r1  r2 = rank  4   rank 4           >>>   note  every rank call into new group for every         >>>   process group create  even if that rank be not         >>>   part of the group          >>> process group =  torch distribute new group pids  for pids in  r1  r2           >>> process group = process group 0 if dist get rank   <= 3 else 1          >>>   without learnable parameters         >>> m = nn batchnorm3d 100  affine=false  process group=process group          >>> input = torch randn 20  100  35  45  10          >>> output = m input           >>>   network be nn batchnorm layer         >>> sync bn network = nn syncbatchnorm convert sync batchnorm network  process group          >>>   only single gpu per process be currently support         >>> ddp sync bn network = torch nn parallel distributeddataparallel          >>>                         sync bn network          >>>                         device ids= args local rank           >>>                         output device=args local rank               def   init            self          num feature  int          eps  float = 1e-5          momentum  float = 0 1          affine  bool = true          track run stats  bool = true          process group  optional any  = none          device=none          dtype=none       -> none          factory kwargs =  string  device  string  dtype          super syncbatchnorm  self    init                num feature  eps  momentum  affine  track run stats    factory kwargs                   self process group = process group      def  check input dim self  input           if input dim   < 2              raise valueerror                  string format input dim                       def  check non zero input channel self  input           if input size 1  == 0              raise valueerror                  string                    def forward self  input  tensor  -> tensor                   if not input be cuda              raise valueerror string           self  check input dim input          self  check non zero input channel input                                      if self momentum be none              exponential average factor = 0 0         else              exponential average factor = self momentum          if self train and self track run stats              assert self num batch track be not none             self num batch track = self num batch track   1             if self momentum be none                    exponential average factor = 1 0 / self num batch track item               else                    exponential average factor = self momentum          r            decide whether the mini-batch stats should be use for normalization rather than the buffer          mini-batch stats be use in train mode  and in eval mode when buffer be none                      if self train              bn train = true         else              bn train =  self run mean be none  and  self run var be none           r            buffer be only update if they be to be track and we be in train mode  thus they only need to be         pass when the update should occur  i e  in train mode when they be track   or when buffer stats be         use for normalization  i e  in eval mode when buffer be not none                                run mean =               self run mean if not self train or self track run stats else none                   run var =               self run var if not self train or self track run stats else none                             need sync =  bn train and self train          if need sync              process group = torch distribute group world             if self process group                  process group = self process group             world size = torch distribute get world size process group              need sync = world size > 1                   if not need sync              return f batch norm                  input                  run mean                  run var                  self weight                  self bias                  bn train                  exponential average factor                  self eps                        else              assert bn train             return sync batch norm apply                  input                  self weight                  self bias                  run mean                  run var                  self eps                  exponential average factor                  process group                  world size                      classmethod     def convert sync batchnorm cls  module  process group=none           r   helper function to convert all  attr `batchnorm d` layer in the model to          class `torch nn syncbatchnorm` layer           args              module  nn module   module contain one or more attr `batchnorm d` layer             process group  optional   process group to scope synchronization                  default be the whole world          return              the original  attr `module` with the convert  class `torch nn syncbatchnorm`             layer  if the original  attr `module` be a  attr `batchnorm d` layer              a new  class `torch nn syncbatchnorm` layer object will be return             instead           example                >>>   network with nn batchnorm layer             >>> module = torch nn sequential              >>>            torch nn linear 20  100               >>>            torch nn batchnorm1d 100               >>>            cuda               >>>   create process group  optional              >>>   rank be a list of int identify rank ids              >>> rank = list range 8               >>> r1  r2 = rank  4   rank 4               >>>   note  every rank call into new group for every             >>>   process group create  even if that rank be not             >>>   part of the group              >>> process group =  torch distribute new group pids  for pids in  r1  r2               >>> process group = process group 0 if dist get rank   <= 3 else 1              >>> sync bn module = torch nn syncbatchnorm convert sync batchnorm module  process group                       module output = module         if isinstance module  torch nn modules batchnorm  batchnorm               module output = torch nn syncbatchnorm                  module num feature                  module eps                  module momentum                  module affine                  module track run stats                  process group                            if module affine                  with torch no grad                        module output weight = module weight                     module output bias = module bias             module output run mean = module run mean             module output run var = module run var             module output num batch track = module num batch track             if hasattr module  string                   module output qconfig = module qconfig         for name  child in module name children                module output add module                  name  cls convert sync batchnorm child  process group                        del module         return module output 
class instancenorm1d  instancenorm       r   apply instance normalization over a 3d input  a mini-batch of 1d     input with optional additional channel dimension  as describe in the paper     `instance normalization  the miss ingredient for fast stylization     <https //arxiv org/abs/1607 08022>`            math            y = \frac x - \mathrm e  x    \sqrt \mathrm var  x    \epsilon     \gamma   \beta      the mean and standard-deviation be calculate per-dimension separately     for each object in a mini-batch   math `\gamma` and  math `\beta` be learnable parameter vectors     of size `c`  where `c` be the input size  if  attr `affine` be ``true``      the standard-deviation be calculate via the bias estimator  equivalent to     `torch var input  unbiased=false `       by default  this layer use instance statistics compute from input data in     both train and evaluation modes       if  attr `track run stats` be set to ``true``  during train this     layer keep run estimate of its compute mean and variance  which be     then use for normalization during evaluation  the run estimate be     keep with a default  attr `momentum` of 0 1          note           this  attr `momentum` argument be different from one use in optimizer         class and the conventional notion of momentum  mathematically  the         update rule for run statistics here be          math `\hat x  \text new  =  1 - \text momentum   \times \hat x    \text momentum  \times x t`          where  math `\hat x ` be the estimate statistic and  math `x t` be the         new observe value          note            class `instancenorm1d` and  class `layernorm` be very similar  but         have some subtle differences   class `instancenorm1d` be apply         on each channel of channel data like multidimensional time series  but          class `layernorm` be usually apply on entire sample and often in nlp         task  additionally   class `layernorm` apply elementwise affine         transform  while  class `instancenorm1d` usually don t apply affine         transform       args          num feature   math `c` from an expect input of size              math ` n  c  l ` or  math `l` from input of size  math ` n  l `         eps  a value add to the denominator for numerical stability  default  1e-5         momentum  the value use for the run mean and run var computation  default  0 1         affine  a boolean value that when set to ``true``  this module have             learnable affine parameters  initialize the same way as do for batch normalization              default  ``false``          track run stats  a boolean value that when set to ``true``  this             module track the run mean and variance  and when set to ``false``              this module do not track such statistics and always use batch             statistics in both train and eval modes  default  ``false``      shape          - input   math ` n  c  l `         - output   math ` n  c  l `  same shape as input       examples            >>>   without learnable parameters         >>> m = nn instancenorm1d 100          >>>   with learnable parameters         >>> m = nn instancenorm1d 100  affine=true          >>> input = torch randn 20  100  40          >>> output = m input               def  check input dim self  input           if input dim   == 2              raise valueerror                  string                 string                 string                 string                       if input dim    = 3              raise valueerror string                               format input dim     
class instancenorm2d  instancenorm       r   apply instance normalization over a 4d input  a mini-batch of 2d input     with additional channel dimension  as describe in the paper     `instance normalization  the miss ingredient for fast stylization     <https //arxiv org/abs/1607 08022>`            math            y = \frac x - \mathrm e  x    \sqrt \mathrm var  x    \epsilon     \gamma   \beta      the mean and standard-deviation be calculate per-dimension separately     for each object in a mini-batch   math `\gamma` and  math `\beta` be learnable parameter vectors     of size `c`  where `c` be the input size  if  attr `affine` be ``true``      the standard-deviation be calculate via the bias estimator  equivalent to     `torch var input  unbiased=false `       by default  this layer use instance statistics compute from input data in     both train and evaluation modes       if  attr `track run stats` be set to ``true``  during train this     layer keep run estimate of its compute mean and variance  which be     then use for normalization during evaluation  the run estimate be     keep with a default  attr `momentum` of 0 1          note           this  attr `momentum` argument be different from one use in optimizer         class and the conventional notion of momentum  mathematically  the         update rule for run statistics here be          math `\hat x  \text new  =  1 - \text momentum   \times \hat x    \text momentum  \times x t`          where  math `\hat x ` be the estimate statistic and  math `x t` be the         new observe value          note            class `instancenorm2d` and  class `layernorm` be very similar  but         have some subtle differences   class `instancenorm2d` be apply         on each channel of channel data like rgb image  but          class `layernorm` be usually apply on entire sample and often in nlp         task  additionally   class `layernorm` apply elementwise affine         transform  while  class `instancenorm2d` usually don t apply affine         transform       args          num feature   math `c` from an expect input of size              math ` n  c  h  w `         eps  a value add to the denominator for numerical stability  default  1e-5         momentum  the value use for the run mean and run var computation  default  0 1         affine  a boolean value that when set to ``true``  this module have             learnable affine parameters  initialize the same way as do for batch normalization              default  ``false``          track run stats  a boolean value that when set to ``true``  this             module track the run mean and variance  and when set to ``false``              this module do not track such statistics and always use batch             statistics in both train and eval modes  default  ``false``      shape          - input   math ` n  c  h  w `         - output   math ` n  c  h  w `  same shape as input       examples            >>>   without learnable parameters         >>> m = nn instancenorm2d 100          >>>   with learnable parameters         >>> m = nn instancenorm2d 100  affine=true          >>> input = torch randn 20  100  35  45          >>> output = m input               def  check input dim self  input           if input dim    = 4              raise valueerror string                               format input dim     
class instancenorm3d  instancenorm       r   apply instance normalization over a 5d input  a mini-batch of 3d input     with additional channel dimension  as describe in the paper     `instance normalization  the miss ingredient for fast stylization     <https //arxiv org/abs/1607 08022>`            math            y = \frac x - \mathrm e  x    \sqrt \mathrm var  x    \epsilon     \gamma   \beta      the mean and standard-deviation be calculate per-dimension separately     for each object in a mini-batch   math `\gamma` and  math `\beta` be learnable parameter vectors     of size c  where c be the input size  if  attr `affine` be ``true``      the standard-deviation be calculate via the bias estimator  equivalent to     `torch var input  unbiased=false `       by default  this layer use instance statistics compute from input data in     both train and evaluation modes       if  attr `track run stats` be set to ``true``  during train this     layer keep run estimate of its compute mean and variance  which be     then use for normalization during evaluation  the run estimate be     keep with a default  attr `momentum` of 0 1          note           this  attr `momentum` argument be different from one use in optimizer         class and the conventional notion of momentum  mathematically  the         update rule for run statistics here be          math `\hat x  \text new  =  1 - \text momentum   \times \hat x    \text momentum  \times x t`          where  math `\hat x ` be the estimate statistic and  math `x t` be the         new observe value          note            class `instancenorm3d` and  class `layernorm` be very similar  but         have some subtle differences   class `instancenorm3d` be apply         on each channel of channel data like 3d model with rgb color  but          class `layernorm` be usually apply on entire sample and often in nlp         task  additionally   class `layernorm` apply elementwise affine         transform  while  class `instancenorm3d` usually don t apply affine         transform       args          num feature   math `c` from an expect input of size              math ` n  c  d  h  w `         eps  a value add to the denominator for numerical stability  default  1e-5         momentum  the value use for the run mean and run var computation  default  0 1         affine  a boolean value that when set to ``true``  this module have             learnable affine parameters  initialize the same way as do for batch normalization              default  ``false``          track run stats  a boolean value that when set to ``true``  this             module track the run mean and variance  and when set to ``false``              this module do not track such statistics and always use batch             statistics in both train and eval modes  default  ``false``      shape          - input   math ` n  c  d  h  w `         - output   math ` n  c  d  h  w `  same shape as input       examples            >>>   without learnable parameters         >>> m = nn instancenorm3d 100          >>>   with learnable parameters         >>> m = nn instancenorm3d 100  affine=true          >>> input = torch randn 20  100  35  45  10          >>> output = m input               def  check input dim self  input           if input dim    = 5              raise valueerror string                               format input dim     
class layernorm module       r   apply layer normalization over a mini-batch of input as describe in     the paper `layer normalization <https //arxiv org/abs/1607 06450>`           math           y = \frac x - \mathrm e  x    \sqrt \mathrm var  x    \epsilon     \gamma   \beta      the mean and standard-deviation be calculate separately over the last     certain number dimension which have to be of the shape specify by      attr `normalized shape`       math `\gamma` and  math `\beta` be learnable affine transform parameters of      attr `normalized shape` if  attr `elementwise affine` be ``true``      the standard-deviation be calculate via the bias estimator  equivalent to     `torch var input  unbiased=false `          note           unlike batch normalization and instance normalization  which apply         scalar scale and bias for each entire channel/plane with the          attr `affine` option  layer normalization apply per-element scale and         bias with  attr `elementwise affine`       this layer use statistics compute from input data in both train and     evaluation modes       args          normalize shape  int or list or torch size   input shape from an expect input             of size                 math                      \times \text normalized\ shape  0  \times \text normalized\ shape  1                      \times \ldots \times \text normalized\ shape  -1                if a single integer be use  it be treat as a singleton list  and this module will             normalize over the last dimension which be expect to be of that specific size          eps  a value add to the denominator for numerical stability  default  1e-5         elementwise affine  a boolean value that when set to ``true``  this module             have learnable per-element affine parameters initialize to ones  for weight              and zero  for bias   default  ``true``       shape          - input   math ` n    `         - output   math ` n    `  same shape as input       examples            >>> input = torch randn 20  5  10  10          >>>   with learnable parameters         >>> m = nn layernorm input size   1            >>>   without learnable parameters         >>> m = nn layernorm input size   1    elementwise affine=false          >>>   normalize over last two dimension         >>> m = nn layernorm  10  10           >>>   normalize over last dimension of size 10         >>> m = nn layernorm 10          >>>   activate the module         >>> output = m input                constants   =  string  string  string      normalize shape  tuple int           eps  float     elementwise affine  bool      def   init   self  normalize shape   shape t  eps  float = 1e-5  elementwise affine  bool = true                   device=none  dtype=none  -> none          factory kwargs =  string  device  string  dtype          super layernorm  self    init             if isinstance normalize shape  number integral                            normalize shape =  normalize shape             self normalize shape = tuple normalize shape            self eps = eps         self elementwise affine = elementwise affine         if self elementwise affine              self weight = parameter torch empty self normalize shape    factory kwargs               self bias = parameter torch empty self normalize shape    factory kwargs           else              self register parameter string  none              self register parameter string  none           self reset parameters        def reset parameters self  -> none          if self elementwise affine              init ones  self weight              init zero  self bias       def forward self  input  tensor  -> tensor          return f layer norm              input  self normalize shape  self weight  self bias  self eps       def extra repr self  -> str          return string \             string format   self   dict    
class localresponsenorm module       r   apply local response normalization over an input signal compose     of several input plan  where channel occupy the second dimension      apply normalization across channel          math           b  c  = a  c \left k   \frac \alpha  n          \sum  c =\max 0  c-n/2    \min n-1 c n/2  a  c   2\right   -\beta       args          size  amount of neighbour channel use for normalization         alpha  multiplicative factor  default  0 0001         beta  exponent  default  0 75         k  additive factor  default  1      shape          - input   math ` n  c    `         - output   math ` n  c    `  same shape as input       examples            >>> lrn = nn localresponsenorm 2          >>> signal 2d = torch randn 32  5  24  24          >>> signal 4d = torch randn 16  5  7  7  7  7          >>> output 2d = lrn signal 2d          >>> output 4d = lrn signal 4d                 constants   =  string  string  string  string      size  int     alpha  float     beta  float     k  float      def   init   self  size  int  alpha  float = 1e-4  beta  float = 0 75  k  float = 1   -> none          super localresponsenorm  self    init             self size = size         self alpha = alpha         self beta = beta         self k = k      def forward self  input  tensor  -> tensor          return f local response norm input  self size  self alpha  self beta                                       self k       def extra repr self           return string format   self   dict    
class rnn rnnbase       r   apply a multi-layer elman rnn with  math `\tanh` or  math `\text relu ` non-linearity to an     input sequence        for each element in the input sequence  each layer compute the follow     function          math           h t = \tanh w  ih  x t   b  ih    w  hh  h   t-1     b  hh        where  math `h t` be the hide state at time `t`   math `x t` be     the input at time `t`  and  math `h   t-1  ` be the hide state of the     previous layer at time `t-1` or the initial hide state at time `0`      if  attr `nonlinearity` be `` relu ``  then  math `\text relu ` be use instead of  math `\tanh`       args          input size  the number of expect feature in the input `x`         hide size  the number of feature in the hide state `h`         num layer  number of recurrent layer  e g   set ``num layers=2``             would mean stack two rnns together to form a `stacked rnn`              with the second rnn take in output of the first rnn and             compute the final result  default  1         nonlinearity  the non-linearity to use  can be either `` tanh `` or `` relu ``  default  `` tanh ``         bias  if ``false``  then the layer do not use bias weight `b ih` and `b hh`              default  ``true``         batch first  if ``true``  then the input and output tensors be provide             as ` batch  seq  feature ` instead of ` seq  batch  feature `              note that this do not apply to hide or cell state  see the             inputs/outputs section below for detail   default  ``false``         dropout  if non-zero  introduce a `dropout` layer on the output of each             rnn layer except the last layer  with dropout probability equal to              attr `dropout`  default  0         bidirectional  if ``true``  become a bidirectional rnn  default  ``false``      input  input  h 0             input    tensor of shape  math ` l  n  h  in  ` when ``batch first=false`` or            math ` n  l  h  in  ` when ``batch first=true`` contain the feature of           the input sequence   the input can also be a pack variable length sequence            see  func `torch nn utils rnn pack pad sequence` or            func `torch nn utils rnn pack sequence` for detail              h 0    tensor of shape  math ` d   \text num\ layer   n  h  out  ` contain the initial hide           state for each element in the batch  default to zero if not provide           where              math               \begin align                  n =     \text batch size  \\                 l =     \text sequence length  \\                 d =     2 \text  if bidirectional=true otherwise   1 \\                 h  in  =     \text input\ size  \\                 h  out  =     \text hidden\ size              \end align       output  output  h n             output    tensor of shape  math ` l  n  d   h  out  ` when ``batch first=false`` or            math ` n  l  d   h  out  ` when ``batch first=true`` contain the output feature           ` h t ` from the last layer of the rnn  for each `t`  if a            class `torch nn utils rnn packedsequence` have be give as the input  the output           will also be a pack sequence              h n    tensor of shape  math ` d   \text num\ layer   n  h  out  ` contain the final hide state           for each element in the batch       attribute          weight ih l k   the learnable input-hidden weight of the k-th layer              of shape ` hide size  input size ` for `k = 0`  otherwise  the shape be             ` hide size  num directions   hide size `         weight hh l k   the learnable hidden-hidden weight of the k-th layer              of shape ` hide size  hide size `         bias ih l k   the learnable input-hidden bias of the k-th layer              of shape ` hide size `         bias hh l k   the learnable hidden-hidden bias of the k-th layer              of shape ` hide size `         note           all the weight and bias be initialize from  math `\mathcal u  -\sqrt k   \sqrt k  `         where  math `k = \frac 1  \text hidden\ size  `         note           for bidirectional rnns  forward and backward be directions 0 and 1 respectively          example of split the output layer when ``batch first=false``          ``output view seq len  batch  num directions  hide size ``          include     /cudnn rnn determinism rst         include     /cudnn persistent rnn rst      examples            >>> rnn = nn rnn 10  20  2          >>> input = torch randn 5  3  10          >>> h0 = torch randn 2  3  20          >>> output  hn = rnn input  h0               def   init   self   args    kwargs           if string in kwargs              raise valueerror string          self nonlinearity = kwargs pop string  string          if self nonlinearity == string              mode = string         elif self nonlinearity == string              mode = string         else              raise valueerror string format self nonlinearity           super rnn  self    init   mode   args    kwargs  
class lstm rnnbase       r   apply a multi-layer long short-term memory  lstm  rnn to an input     sequence        for each element in the input sequence  each layer compute the follow     function          math           \begin array  ll  \\             i t = \sigma w  ii  x t   b  ii    w  hi  h  t-1    b  hi   \\             f t = \sigma w  if  x t   b  if    w  hf  h  t-1    b  hf   \\             g t = \tanh w  ig  x t   b  ig    w  hg  h  t-1    b  hg   \\             o t = \sigma w  io  x t   b  io    w  ho  h  t-1    b  ho   \\             c t = f t \odot c  t-1    i t \odot g t \\             h t = o t \odot \tanh c t  \\         \end array       where  math `h t` be the hide state at time `t`   math `c t` be the cell     state at time `t`   math `x t` be the input at time `t`   math `h  t-1 `     be the hide state of the layer at time `t-1` or the initial hide     state at time `0`  and  math `i t`   math `f t`   math `g t`       math `o t` be the input  forget  cell  and output gate  respectively       math `\sigma` be the sigmoid function  and  math `\odot` be the hadamard product       in a multilayer lstm  the input  math `x   l   t` of the  math `l` -th layer       math `l >= 2`  be the hide state  math `h   l-1   t` of the previous layer multiply by     dropout  math `\delta   l-1   t` where each  math `\delta   l-1   t` be a bernoulli random     variable which be  math `0` with probability  attr `dropout`       if ``proj size > 0`` be specify  lstm with projections will be use  this change     the lstm cell in the follow way  first  the dimension of  math `h t` will be change from     ``hidden size`` to ``proj size``  dimension of  math `w  hi ` will be change accordingly       second  the output hide state of each layer will be multiply by a learnable projection     matrix   math `h t = w  hr h t`  note that as a consequence of this  the output     of lstm network will be of different shape as well  see inputs/outputs section below for exact     dimension of all variables  you can find more detail in https //arxiv org/abs/1402 1128       args          input size  the number of expect feature in the input `x`         hide size  the number of feature in the hide state `h`         num layer  number of recurrent layer  e g   set ``num layers=2``             would mean stack two lstms together to form a `stacked lstm`              with the second lstm take in output of the first lstm and             compute the final result  default  1         bias  if ``false``  then the layer do not use bias weight `b ih` and `b hh`              default  ``true``         batch first  if ``true``  then the input and output tensors be provide             as ` batch  seq  feature ` instead of ` seq  batch  feature `              note that this do not apply to hide or cell state  see the             inputs/outputs section below for detail   default  ``false``         dropout  if non-zero  introduce a `dropout` layer on the output of each             lstm layer except the last layer  with dropout probability equal to              attr `dropout`  default  0         bidirectional  if ``true``  become a bidirectional lstm  default  ``false``         proj size  if ``> 0``  will use lstm with projections of correspond size  default  0      input  input   h 0  c 0              input    tensor of shape  math ` l  n  h  in  ` when ``batch first=false`` or            math ` n  l  h  in  ` when ``batch first=true`` contain the feature of           the input sequence   the input can also be a pack variable length sequence            see  func `torch nn utils rnn pack pad sequence` or            func `torch nn utils rnn pack sequence` for detail              h 0    tensor of shape  math ` d   \text num\ layer   n  h  out  ` contain the           initial hide state for each element in the batch            default to zero if  h 0  c 0  be not provide              c 0    tensor of shape  math ` d   \text num\ layer   n  h  cell  ` contain the           initial cell state for each element in the batch            default to zero if  h 0  c 0  be not provide           where              math               \begin align                  n =     \text batch size  \\                 l =     \text sequence length  \\                 d =     2 \text  if bidirectional=true otherwise   1 \\                 h  in  =     \text input\ size  \\                 h  cell  =     \text hidden\ size  \\                 h  out  =     \text proj\ size if   \text proj\ size >0 \text  otherwise hidden\ size  \\             \end align       output  output   h n  c n              output    tensor of shape  math ` l  n  d   h  out  ` when ``batch first=false`` or            math ` n  l  d   h  out  ` when ``batch first=true`` contain the output feature           ` h t ` from the last layer of the lstm  for each `t`  if a            class `torch nn utils rnn packedsequence` have be give as the input  the output           will also be a pack sequence              h n    tensor of shape  math ` d   \text num\ layer   n  h  out  ` contain the           final hide state for each element in the batch              c n    tensor of shape  math ` d   \text num\ layer   n  h  cell  ` contain the           final cell state for each element in the batch       attribute          weight ih l k    the learnable input-hidden weight of the  math `\text k   th ` layer             ` w ii w if w ig w io `  of shape ` 4 hide size  input size ` for `k = 0`              otherwise  the shape be ` 4 hide size  num directions   hide size `         weight hh l k    the learnable hidden-hidden weight of the  math `\text k   th ` layer             ` w hi w hf w hg w ho `  of shape ` 4 hide size  hide size `  if ``proj size > 0``             be specify  the shape will be ` 4 hide size  proj size `          bias ih l k    the learnable input-hidden bias of the  math `\text k   th ` layer             ` b ii b if b ig b io `  of shape ` 4 hide size `         bias hh l k    the learnable hidden-hidden bias of the  math `\text k   th ` layer             ` b hi b hf b hg b ho `  of shape ` 4 hide size `         weight hr l k    the learnable projection weight of the  math `\text k   th ` layer             of shape ` proj size  hide size `  only present when ``proj size > 0`` be             specify          note           all the weight and bias be initialize from  math `\mathcal u  -\sqrt k   \sqrt k  `         where  math `k = \frac 1  \text hidden\ size  `         note           for bidirectional lstms  forward and backward be directions 0 and 1 respectively          example of split the output layer when ``batch first=false``          ``output view seq len  batch  num directions  hide size ``          include     /cudnn rnn determinism rst         include     /cudnn persistent rnn rst      examples            >>> rnn = nn lstm 10  20  2          >>> input = torch randn 5  3  10          >>> h0 = torch randn 2  3  20          >>> c0 = torch randn 2  3  20          >>> output   hn  cn  = rnn input   h0  c0                def   init   self   args    kwargs           super lstm  self    init   string   args    kwargs       def get expect cell size self  input  tensor  batch size  optional tensor   -> tuple int  int  int           if batch size be not none              mini batch = int batch size 0           else              mini batch = input size 0  if self batch first else input size 1          num directions = 2 if self bidirectional else 1         expect hide size =  self num layer   num directions                                  mini batch  self hide size          return expect hide size                def check forward args self                               input  tensor                             hide  tuple tensor  tensor                              batch size  optional tensor                                         self check input input  batch size          self check hide size hide 0   self get expect hide size input  batch size                                  string          self check hide size hide 1   self get expect cell size input  batch size                                  string            def permute hide self                           hx  tuple tensor  tensor                          permutation  optional tensor                           -> tuple tensor  tensor           if permutation be none              return hx         return apply permutation hx 0   permutation   apply permutation hx 1   permutation             overload        torch  jit internal  overload method       def forward self  input  tensor  hx  optional tuple tensor  tensor   = none                   -> tuple tensor  tuple tensor  tensor              pass            overload      torch  jit internal  overload method       def forward self  input  packedsequence  hx  optional tuple tensor  tensor   = none                   -> tuple packedsequence  tuple tensor  tensor              pass      def forward self  input  hx=none             orig input = input                  if isinstance orig input  packedsequence               input  batch size  sort indices  unsorted indices = input             max batch size = batch size 0              max batch size = int max batch size          else              batch size = none             max batch size = input size 0  if self batch first else input size 1              sort indices = none             unsorted indices = none          if hx be none              num directions = 2 if self bidirectional else 1             real hide size = self proj size if self proj size > 0 else self hide size             h zero = torch zero self num layer   num directions                                    max batch size  real hide size                                    dtype=input dtype  device=input device              c zero = torch zero self num layer   num directions                                    max batch size  self hide size                                    dtype=input dtype  device=input device              hx =  h zero  c zero          else                                        hx = self permute hide hx  sort indices           self check forward args input  hx  batch size          if batch size be none              result =  vf lstm input  hx  self  flat weight  self bias  self num layer                                self dropout  self train  self bidirectional  self batch first          else              result =  vf lstm input  batch size  hx  self  flat weight  self bias                                self num layer  self dropout  self train  self bidirectional          output = result 0          hide = result 1                    if isinstance orig input  packedsequence               output pack = packedsequence output  batch size  sort indices  unsorted indices              return output pack  self permute hide hide  unsorted indices          else              return output  self permute hide hide  unsorted indices  
class gru rnnbase       r   apply a multi-layer gate recurrent unit  gru  rnn to an input sequence        for each element in the input sequence  each layer compute the follow     function          math           \begin array  ll              r t = \sigma w  ir  x t   b  ir    w  hr  h   t-1     b  hr   \\             z t = \sigma w  iz  x t   b  iz    w  hz  h   t-1     b  hz   \\             n t = \tanh w  in  x t   b  in    r t    w  hn  h   t-1    b  hn    \\             h t =  1 - z t    n t   z t   h   t-1           \end array       where  math `h t` be the hide state at time `t`   math `x t` be the input     at time `t`   math `h   t-1  ` be the hide state of the layer     at time `t-1` or the initial hide state at time `0`  and  math `r t`       math `z t`   math `n t` be the reset  update  and new gate  respectively       math `\sigma` be the sigmoid function  and  math ` ` be the hadamard product       in a multilayer gru  the input  math `x   l   t` of the  math `l` -th layer       math `l >= 2`  be the hide state  math `h   l-1   t` of the previous layer multiply by     dropout  math `\delta   l-1   t` where each  math `\delta   l-1   t` be a bernoulli random     variable which be  math `0` with probability  attr `dropout`       args          input size  the number of expect feature in the input `x`         hide size  the number of feature in the hide state `h`         num layer  number of recurrent layer  e g   set ``num layers=2``             would mean stack two grus together to form a `stacked gru`              with the second gru take in output of the first gru and             compute the final result  default  1         bias  if ``false``  then the layer do not use bias weight `b ih` and `b hh`              default  ``true``         batch first  if ``true``  then the input and output tensors be provide             as ` batch  seq  feature ` instead of ` seq  batch  feature `              note that this do not apply to hide or cell state  see the             inputs/outputs section below for detail   default  ``false``         dropout  if non-zero  introduce a `dropout` layer on the output of each             gru layer except the last layer  with dropout probability equal to              attr `dropout`  default  0         bidirectional  if ``true``  become a bidirectional gru  default  ``false``      input  input  h 0             input    tensor of shape  math ` l  n  h  in  ` when ``batch first=false`` or            math ` n  l  h  in  ` when ``batch first=true`` contain the feature of           the input sequence   the input can also be a pack variable length sequence            see  func `torch nn utils rnn pack pad sequence` or            func `torch nn utils rnn pack sequence` for detail              h 0    tensor of shape  math ` d   \text num\ layer   n  h  out  ` contain the initial hide           state for each element in the batch  default to zero if not provide           where              math               \begin align                  n =     \text batch size  \\                 l =     \text sequence length  \\                 d =     2 \text  if bidirectional=true otherwise   1 \\                 h  in  =     \text input\ size  \\                 h  out  =     \text hidden\ size              \end align       output  output  h n             output    tensor of shape  math ` l  n  d   h  out  ` when ``batch first=false`` or            math ` n  l  d   h  out  ` when ``batch first=true`` contain the output feature           ` h t ` from the last layer of the gru  for each `t`  if a            class `torch nn utils rnn packedsequence` have be give as the input  the output           will also be a pack sequence              h n    tensor of shape  math ` d   \text num\ layer   n  h  out  ` contain the final hide state           for each element in the batch       attribute          weight ih l k    the learnable input-hidden weight of the  math `\text k   th ` layer              w ir w iz w in   of shape ` 3 hide size  input size ` for `k = 0`              otherwise  the shape be ` 3 hide size  num directions   hide size `         weight hh l k    the learnable hidden-hidden weight of the  math `\text k   th ` layer              w hr w hz w hn   of shape ` 3 hide size  hide size `         bias ih l k    the learnable input-hidden bias of the  math `\text k   th ` layer              b ir b iz b in   of shape ` 3 hide size `         bias hh l k    the learnable hidden-hidden bias of the  math `\text k   th ` layer              b hr b hz b hn   of shape ` 3 hide size `         note           all the weight and bias be initialize from  math `\mathcal u  -\sqrt k   \sqrt k  `         where  math `k = \frac 1  \text hidden\ size  `         note           for bidirectional grus  forward and backward be directions 0 and 1 respectively          example of split the output layer when ``batch first=false``          ``output view seq len  batch  num directions  hide size ``          include     /cudnn persistent rnn rst      examples            >>> rnn = nn gru 10  20  2          >>> input = torch randn 5  3  10          >>> h0 = torch randn 2  3  20          >>> output  hn = rnn input  h0               def   init   self   args    kwargs           if string in kwargs              raise valueerror string          super gru  self    init   string   args    kwargs        overload        torch  jit internal  overload method       def forward self  input  tensor  hx  optional tensor  = none  -> tuple tensor  tensor             pass       overload      torch  jit internal  overload method       def forward self  input  packedsequence  hx  optional tensor  = none  -> tuple packedsequence  tensor             pass      def forward self  input  hx=none             orig input = input                  if isinstance orig input  packedsequence               input  batch size  sort indices  unsorted indices = input             max batch size = batch size 0              max batch size = int max batch size          else              batch size = none             max batch size = input size 0  if self batch first else input size 1              sort indices = none             unsorted indices = none          if hx be none              num directions = 2 if self bidirectional else 1             hx = torch zero self num layer   num directions                               max batch size  self hide size                               dtype=input dtype  device=input device          else                                        hx = self permute hide hx  sort indices           self check forward args input  hx  batch size          if batch size be none              result =  vf gru input  hx  self  flat weight  self bias  self num layer                               self dropout  self train  self bidirectional  self batch first          else              result =  vf gru input  batch size  hx  self  flat weight  self bias                               self num layer  self dropout  self train  self bidirectional          output = result 0          hide = result 1                    if isinstance orig input  packedsequence               output pack = packedsequence output  batch size  sort indices  unsorted indices              return output pack  self permute hide hide  unsorted indices          else              return output  self permute hide hide  unsorted indices  
class rnncell rnncellbase       r   an elman rnn cell with tanh or relu non-linearity          math            h  = \tanh w  ih  x   b  ih      w  hh  h   b  hh        if  attr `nonlinearity` be ` relu `  then relu be use in place of tanh       args          input size  the number of expect feature in the input `x`         hide size  the number of feature in the hide state `h`         bias  if ``false``  then the layer do not use bias weight `b ih` and `b hh`              default  ``true``         nonlinearity  the non-linearity to use  can be either `` tanh `` or `` relu ``  default  `` tanh ``      input  input  hide         -   input   of shape ` batch  input size `  tensor contain input feature         -   hide   of shape ` batch  hide size `  tensor contain the initial hide           state for each element in the batch            default to zero if not provide       output  h          -   h    of shape ` batch  hide size `  tensor contain the next hide state           for each element in the batch      shape          - input1   math ` n  h  in  ` tensor contain input feature where            math `h  in ` = `input size`         - input2   math ` n  h  out  ` tensor contain the initial hide           state for each element in the batch where  math `h  out ` = `hidden size`           default to zero if not provide          - output   math ` n  h  out  ` tensor contain the next hide state           for each element in the batch      attribute          weight ih  the learnable input-hidden weight  of shape             ` hide size  input size `         weight hh  the learnable hidden-hidden weight  of shape             ` hide size  hide size `         bias ih  the learnable input-hidden bias  of shape ` hide size `         bias hh  the learnable hidden-hidden bias  of shape ` hide size `         note           all the weight and bias be initialize from  math `\mathcal u  -\sqrt k   \sqrt k  `         where  math `k = \frac 1  \text hidden\ size  `      examples            >>> rnn = nn rnncell 10  20          >>> input = torch randn 6  3  10          >>> hx = torch randn 3  20          >>> output =            >>> for i in range 6                   hx = rnn input i   hx                  output append hx                constants   =  string  string  string  string      nonlinearity  str      def   init   self  input size  int  hide size  int  bias  bool = true  nonlinearity  str = string                   device=none  dtype=none  -> none          factory kwargs =  string  device  string  dtype          super rnncell  self    init   input size  hide size  bias  num chunks=1    factory kwargs          self nonlinearity = nonlinearity      def forward self  input  tensor  hx  optional tensor  = none  -> tensor          if hx be none              hx = torch zero input size 0   self hide size  dtype=input dtype  device=input device          if self nonlinearity == string              ret =  vf rnn tanh cell                  input  hx                  self weight ih  self weight hh                  self bias ih  self bias hh                        elif self nonlinearity == string              ret =  vf rnn relu cell                  input  hx                  self weight ih  self weight hh                  self bias ih  self bias hh                        else              ret = input               raise runtimeerror                  string format self nonlinearity           return ret 
class lstmcell rnncellbase       r   a long short-term memory  lstm  cell          math            \begin array  ll          i = \sigma w  ii  x   b  ii    w  hi  h   b  hi   \\         f = \sigma w  if  x   b  if    w  hf  h   b  hf   \\         g = \tanh w  ig  x   b  ig    w  hg  h   b  hg   \\         o = \sigma w  io  x   b  io    w  ho  h   b  ho   \\         c  = f   c   i   g \\         h  = o   \tanh c   \\         \end array       where  math `\sigma` be the sigmoid function  and  math ` ` be the hadamard product       args          input size  the number of expect feature in the input `x`         hide size  the number of feature in the hide state `h`         bias  if ``false``  then the layer do not use bias weight `b ih` and             `b hh`  default  ``true``      input  input   h 0  c 0          -   input   of shape ` batch  input size `  tensor contain input feature         -   h 0   of shape ` batch  hide size `  tensor contain the initial hide           state for each element in the batch          -   c 0   of shape ` batch  hide size `  tensor contain the initial cell state           for each element in the batch             if ` h 0  c 0 ` be not provide  both   h 0   and   c 0   default to zero       output   h 1  c 1          -   h 1   of shape ` batch  hide size `  tensor contain the next hide state           for each element in the batch         -   c 1   of shape ` batch  hide size `  tensor contain the next cell state           for each element in the batch      attribute          weight ih  the learnable input-hidden weight  of shape             ` 4 hide size  input size `         weight hh  the learnable hidden-hidden weight  of shape             ` 4 hide size  hide size `         bias ih  the learnable input-hidden bias  of shape ` 4 hide size `         bias hh  the learnable hidden-hidden bias  of shape ` 4 hide size `         note           all the weight and bias be initialize from  math `\mathcal u  -\sqrt k   \sqrt k  `         where  math `k = \frac 1  \text hidden\ size  `      examples            >>> rnn = nn lstmcell 10  20     input size  hide size          >>> input = torch randn 2  3  10     time step  batch  input size          >>> hx = torch randn 3  20     batch  hide size          >>> cx = torch randn 3  20          >>> output =            >>> for i in range input size   0                    hx  cx = rnn input i    hx  cx                   output append hx          >>> output = torch stack output  dim=0               def   init   self  input size  int  hide size  int  bias  bool = true                   device=none  dtype=none  -> none          factory kwargs =  string  device  string  dtype          super lstmcell  self    init   input size  hide size  bias  num chunks=4    factory kwargs       def forward self  input  tensor  hx  optional tuple tensor  tensor   = none  -> tuple tensor  tensor           if hx be none              zero = torch zero input size 0   self hide size  dtype=input dtype  device=input device              hx =  zero  zero          return  vf lstm cell              input  hx              self weight ih  self weight hh              self bias ih  self bias hh            
class grucell rnncellbase       r   a gate recurrent unit  gru  cell         math            \begin array  ll          r = \sigma w  ir  x   b  ir    w  hr  h   b  hr   \\         z = \sigma w  iz  x   b  iz    w  hz  h   b  hz   \\         n = \tanh w  in  x   b  in    r    w  hn  h   b  hn    \\         h  =  1 - z    n   z   h         \end array       where  math `\sigma` be the sigmoid function  and  math ` ` be the hadamard product       args          input size  the number of expect feature in the input `x`         hide size  the number of feature in the hide state `h`         bias  if ``false``  then the layer do not use bias weight `b ih` and             `b hh`  default  ``true``      input  input  hide         -   input   of shape ` batch  input size `  tensor contain input feature         -   hide   of shape ` batch  hide size `  tensor contain the initial hide           state for each element in the batch            default to zero if not provide       output  h          -   h    of shape ` batch  hide size `  tensor contain the next hide state           for each element in the batch      shape          - input1   math ` n  h  in  ` tensor contain input feature where            math `h  in ` = `input size`         - input2   math ` n  h  out  ` tensor contain the initial hide           state for each element in the batch where  math `h  out ` = `hidden size`           default to zero if not provide          - output   math ` n  h  out  ` tensor contain the next hide state           for each element in the batch      attribute          weight ih  the learnable input-hidden weight  of shape             ` 3 hide size  input size `         weight hh  the learnable hidden-hidden weight  of shape             ` 3 hide size  hide size `         bias ih  the learnable input-hidden bias  of shape ` 3 hide size `         bias hh  the learnable hidden-hidden bias  of shape ` 3 hide size `         note           all the weight and bias be initialize from  math `\mathcal u  -\sqrt k   \sqrt k  `         where  math `k = \frac 1  \text hidden\ size  `      examples            >>> rnn = nn grucell 10  20          >>> input = torch randn 6  3  10          >>> hx = torch randn 3  20          >>> output =            >>> for i in range 6                   hx = rnn input i   hx                  output append hx               def   init   self  input size  int  hide size  int  bias  bool = true                   device=none  dtype=none  -> none          factory kwargs =  string  device  string  dtype          super grucell  self    init   input size  hide size  bias  num chunks=3    factory kwargs       def forward self  input  tensor  hx  optional tensor  = none  -> tensor          if hx be none              hx = torch zero input size 0   self hide size  dtype=input dtype  device=input device          return  vf gru cell              input  hx              self weight ih  self weight hh              self bias ih  self bias hh            
class transformer module       r   a transformer model  user be able to modify the attribute as need  the architecture     be base on the paper  attention be all you need   ashish vaswani  noam shazeer      niki parmar  jakob uszkoreit  llion jones  aidan n gomez  lukasz kaiser  and     illia polosukhin  2017  attention be all you need  in advance in neural information     process systems  page 6000-6010  users can build the bert https //arxiv org/abs/1810 04805      model with correspond parameters       args          d model  the number of expect feature in the encoder/decoder input  default=512           nhead  the number of head in the multiheadattention model  default=8           num encoder layer  the number of sub-encoder-layers in the encoder  default=6           num decoder layer  the number of sub-decoder-layers in the decoder  default=6           dim feedforward  the dimension of the feedforward network model  default=2048           dropout  the dropout value  default=0 1           activation  the activation function of encoder/decoder intermediate layer  relu or gelu  default=relu           custom encoder  custom encoder  default=none           custom decoder  custom decoder  default=none           layer norm eps  the eps value in layer normalization components  default=1e-5           batch first  if ``true``  then the input and output tensors be provide             as  batch  seq  feature   default  ``false``  seq  batch  feature        examples           >>> transformer model = nn transformer nhead=16  num encoder layers=12          >>> src = torch rand  10  32  512           >>> tgt = torch rand  20  32  512           >>> out = transformer model src  tgt       note  a full example to apply nn transformer module for the word language model be available in     https //github com/pytorch/examples/tree/master/word language model              def   init   self  d model  int = 512  nhead  int = 8  num encoder layer  int = 6                   num decoder layer  int = 6  dim feedforward  int = 2048  dropout  float = 0 1                   activation  str = string  custom encoder  optional any  = none  custom decoder  optional any  = none                   layer norm eps  float = 1e-5  batch first  bool = false                   device=none  dtype=none  -> none          factory kwargs =  string  device  string  dtype          super transformer  self    init              if custom encoder be not none              self encoder = custom encoder         else              encoder layer = transformerencoderlayer d model  nhead  dim feedforward  dropout                                                      activation  layer norm eps  batch first                                                        factory kwargs              encoder norm = layernorm d model  eps=layer norm eps    factory kwargs              self encoder = transformerencoder encoder layer  num encoder layer  encoder norm           if custom decoder be not none              self decoder = custom decoder         else              decoder layer = transformerdecoderlayer d model  nhead  dim feedforward  dropout                                                      activation  layer norm eps  batch first                                                        factory kwargs              decoder norm = layernorm d model  eps=layer norm eps    factory kwargs              self decoder = transformerdecoder decoder layer  num decoder layer  decoder norm           self  reset parameters            self d model = d model         self nhead = nhead          self batch first = batch first      def forward self  src  tensor  tgt  tensor  src mask  optional tensor  = none  tgt mask  optional tensor  = none                  memory mask  optional tensor  = none  src key pad mask  optional tensor  = none                  tgt key pad mask  optional tensor  = none  memory key pad mask  optional tensor  = none  -> tensor          r   take in and process mask source/target sequence           args              src  the sequence to the encoder  require               tgt  the sequence to the decoder  require               src mask  the additive mask for the src sequence  optional               tgt mask  the additive mask for the tgt sequence  optional               memory mask  the additive mask for the encoder output  optional               src key pad mask  the bytetensor mask for src key per batch  optional               tgt key pad mask  the bytetensor mask for tgt key per batch  optional               memory key pad mask  the bytetensor mask for memory key per batch  optional            shape              - src   math ` s  n  e `  ` n  s  e ` if batch first              - tgt   math ` t  n  e `  ` n  t  e ` if batch first              - src mask   math ` s  s `              - tgt mask   math ` t  t `              - memory mask   math ` t  s `              - src key pad mask   math ` n  s `              - tgt key pad mask   math ` n  t `              - memory key pad mask   math ` n  s `               note   src/tgt/memory  mask ensure that position i be allow to attend the unmask             position  if a bytetensor be provide  the non-zero position be not allow to attend             while the zero position will be unchanged  if a booltensor be provide  position with ``true``             be not allow to attend while ``false`` value will be unchanged  if a floattensor             be provide  it will be add to the attention weight               src/tgt/memory  key pad mask provide specify elements in the key to be ignore by             the attention  if a bytetensor be provide  the non-zero position will be ignore while the zero             position will be unchanged  if a booltensor be provide  the position with the             value of ``true`` will be ignore while the position with the value of ``false`` will be unchanged               - output   math ` t  n  e `  ` n  t  e ` if batch first               note  due to the multi-head attention architecture in the transformer model              the output sequence length of a transformer be same as the input sequence              i e  target  length of the decode               where s be the source sequence length  t be the target sequence length  n be the             batch size  e be the feature number          examples              >>> output = transformer model src  tgt  src mask=src mask  tgt mask=tgt mask                       if not self batch first and src size 1   = tgt size 1               raise runtimeerror string          elif self batch first and src size 0   = tgt size 0               raise runtimeerror string           if src size 2   = self d model or tgt size 2   = self d model              raise runtimeerror string           memory = self encoder src  mask=src mask  src key pad mask=src key pad mask          output = self decoder tgt  memory  tgt mask=tgt mask  memory mask=memory mask                                tgt key pad mask=tgt key pad mask                                memory key pad mask=memory key pad mask          return output      def generate square subsequent mask self  sz  int  -> tensor          r   generate a square mask for the sequence  the mask position be fill with float  -inf                unmask position be fill with float 0 0                       mask =  torch triu torch ones sz  sz   == 1  transpose 0  1          mask = mask float   mask fill mask == 0  float string   mask fill mask == 1  float 0 0           return mask      def  reset parameters self           r   initiate parameters in the transformer model              for p in self parameters                if p dim   > 1                  xavier uniform  p  
def forward self  src  tensor  tgt  tensor  src mask  optional tensor  = none  tgt mask  optional tensor  = none              memory mask  optional tensor  = none  src key pad mask  optional tensor  = none              tgt key pad mask  optional tensor  = none  memory key pad mask  optional tensor  = none  -> tensor      r   take in and process mask source/target sequence       args          src  the sequence to the encoder  require           tgt  the sequence to the decoder  require           src mask  the additive mask for the src sequence  optional           tgt mask  the additive mask for the tgt sequence  optional           memory mask  the additive mask for the encoder output  optional           src key pad mask  the bytetensor mask for src key per batch  optional           tgt key pad mask  the bytetensor mask for tgt key per batch  optional           memory key pad mask  the bytetensor mask for memory key per batch  optional        shape          - src   math ` s  n  e `  ` n  s  e ` if batch first          - tgt   math ` t  n  e `  ` n  t  e ` if batch first          - src mask   math ` s  s `          - tgt mask   math ` t  t `          - memory mask   math ` t  s `          - src key pad mask   math ` n  s `          - tgt key pad mask   math ` n  t `          - memory key pad mask   math ` n  s `           note   src/tgt/memory  mask ensure that position i be allow to attend the unmask         position  if a bytetensor be provide  the non-zero position be not allow to attend         while the zero position will be unchanged  if a booltensor be provide  position with ``true``         be not allow to attend while ``false`` value will be unchanged  if a floattensor         be provide  it will be add to the attention weight           src/tgt/memory  key pad mask provide specify elements in the key to be ignore by         the attention  if a bytetensor be provide  the non-zero position will be ignore while the zero         position will be unchanged  if a booltensor be provide  the position with the         value of ``true`` will be ignore while the position with the value of ``false`` will be unchanged           - output   math ` t  n  e `  ` n  t  e ` if batch first           note  due to the multi-head attention architecture in the transformer model          the output sequence length of a transformer be same as the input sequence          i e  target  length of the decode           where s be the source sequence length  t be the target sequence length  n be the         batch size  e be the feature number      examples          >>> output = transformer model src  tgt  src mask=src mask  tgt mask=tgt mask               if not self batch first and src size 1   = tgt size 1           raise runtimeerror string      elif self batch first and src size 0   = tgt size 0           raise runtimeerror string       if src size 2   = self d model or tgt size 2   = self d model          raise runtimeerror string       memory = self encoder src  mask=src mask  src key pad mask=src key pad mask      output = self decoder tgt  memory  tgt mask=tgt mask  memory mask=memory mask                            tgt key pad mask=tgt key pad mask                            memory key pad mask=memory key pad mask      return output 
class transformerencoder module       r   transformerencoder be a stack of n encoder layer      args          encoder layer  an instance of the transformerencoderlayer   class  require           num layer  the number of sub-encoder-layers in the encoder  require           norm  the layer normalization component  optional        examples           >>> encoder layer = nn transformerencoderlayer d model=512  nhead=8          >>> transformer encoder = nn transformerencoder encoder layer  num layers=6          >>> src = torch rand 10  32  512          >>> out = transformer encoder src                constants   =  string       def   init   self  encoder layer  num layer  norm=none           super transformerencoder  self    init             self layer =  get clone encoder layer  num layer          self num layer = num layer         self norm = norm      def forward self  src  tensor  mask  optional tensor  = none  src key pad mask  optional tensor  = none  -> tensor          r   pass the input through the encoder layer in turn           args              src  the sequence to the encoder  require               mask  the mask for the src sequence  optional               src key pad mask  the mask for the src key per batch  optional            shape              see the docs in transformer class                      output = src          for mod in self layer              output = mod output  src mask=mask  src key pad mask=src key pad mask           if self norm be not none              output = self norm output           return output 
def forward self  src  tensor  mask  optional tensor  = none  src key pad mask  optional tensor  = none  -> tensor      r   pass the input through the encoder layer in turn       args          src  the sequence to the encoder  require           mask  the mask for the src sequence  optional           src key pad mask  the mask for the src key per batch  optional        shape          see the docs in transformer class              output = src      for mod in self layer          output = mod output  src mask=mask  src key pad mask=src key pad mask       if self norm be not none          output = self norm output       return output 
class transformerdecoder module       r   transformerdecoder be a stack of n decoder layer      args          decoder layer  an instance of the transformerdecoderlayer   class  require           num layer  the number of sub-decoder-layers in the decoder  require           norm  the layer normalization component  optional        examples           >>> decoder layer = nn transformerdecoderlayer d model=512  nhead=8          >>> transformer decoder = nn transformerdecoder decoder layer  num layers=6          >>> memory = torch rand 10  32  512          >>> tgt = torch rand 20  32  512          >>> out = transformer decoder tgt  memory                constants   =  string       def   init   self  decoder layer  num layer  norm=none           super transformerdecoder  self    init             self layer =  get clone decoder layer  num layer          self num layer = num layer         self norm = norm      def forward self  tgt  tensor  memory  tensor  tgt mask  optional tensor  = none                  memory mask  optional tensor  = none  tgt key pad mask  optional tensor  = none                  memory key pad mask  optional tensor  = none  -> tensor          r   pass the input  and mask  through the decoder layer in turn           args              tgt  the sequence to the decoder  require               memory  the sequence from the last layer of the encoder  require               tgt mask  the mask for the tgt sequence  optional               memory mask  the mask for the memory sequence  optional               tgt key pad mask  the mask for the tgt key per batch  optional               memory key pad mask  the mask for the memory key per batch  optional            shape              see the docs in transformer class                      output = tgt          for mod in self layer              output = mod output  memory  tgt mask=tgt mask                           memory mask=memory mask                           tgt key pad mask=tgt key pad mask                           memory key pad mask=memory key pad mask           if self norm be not none              output = self norm output           return output 
def forward self  tgt  tensor  memory  tensor  tgt mask  optional tensor  = none              memory mask  optional tensor  = none  tgt key pad mask  optional tensor  = none              memory key pad mask  optional tensor  = none  -> tensor      r   pass the input  and mask  through the decoder layer in turn       args          tgt  the sequence to the decoder  require           memory  the sequence from the last layer of the encoder  require           tgt mask  the mask for the tgt sequence  optional           memory mask  the mask for the memory sequence  optional           tgt key pad mask  the mask for the tgt key per batch  optional           memory key pad mask  the mask for the memory key per batch  optional        shape          see the docs in transformer class              output = tgt      for mod in self layer          output = mod output  memory  tgt mask=tgt mask                       memory mask=memory mask                       tgt key pad mask=tgt key pad mask                       memory key pad mask=memory key pad mask       if self norm be not none          output = self norm output       return output 
class transformerencoderlayer module       r   transformerencoderlayer be make up of self-attn and feedforward network      this standard encoder layer be base on the paper  attention be all you need       ashish vaswani  noam shazeer  niki parmar  jakob uszkoreit  llion jones  aidan n gomez      lukasz kaiser  and illia polosukhin  2017  attention be all you need  in advance in     neural information process systems  page 6000-6010  users may modify or implement     in a different way during application       args          d model  the number of expect feature in the input  require           nhead  the number of head in the multiheadattention model  require           dim feedforward  the dimension of the feedforward network model  default=2048           dropout  the dropout value  default=0 1           activation  the activation function of intermediate layer  relu or gelu  default=relu           layer norm eps  the eps value in layer normalization components  default=1e-5           batch first  if ``true``  then the input and output tensors be provide             as  batch  seq  feature   default  ``false``       examples           >>> encoder layer = nn transformerencoderlayer d model=512  nhead=8          >>> src = torch rand 10  32  512          >>> out = encoder layer src       alternatively  when ``batch first`` be ``true``          >>> encoder layer = nn transformerencoderlayer d model=512  nhead=8  batch first=true          >>> src = torch rand 32  10  512          >>> out = encoder layer src                constants   =  string       def   init   self  d model  nhead  dim feedforward=2048  dropout=0 1  activation=string                   layer norm eps=1e-5  batch first=false                   device=none  dtype=none  -> none          factory kwargs =  string  device  string  dtype          super transformerencoderlayer  self    init             self self attn = multiheadattention d model  nhead  dropout=dropout  batch first=batch first                                                factory kwargs                   self linear1 = linear d model  dim feedforward    factory kwargs          self dropout = dropout dropout          self linear2 = linear dim feedforward  d model    factory kwargs           self norm1 = layernorm d model  eps=layer norm eps    factory kwargs          self norm2 = layernorm d model  eps=layer norm eps    factory kwargs          self dropout1 = dropout dropout          self dropout2 = dropout dropout           self activation =  get activation fn activation       def   setstate   self  state           if string not in state              state string  = f relu         super transformerencoderlayer  self    setstate   state       def forward self  src  tensor  src mask  optional tensor  = none  src key pad mask  optional tensor  = none  -> tensor          r   pass the input through the encoder layer           args              src  the sequence to the encoder layer  require               src mask  the mask for the src sequence  optional               src key pad mask  the mask for the src key per batch  optional            shape              see the docs in transformer class                      src2 = self self attn src  src  src  attn mask=src mask                                key pad mask=src key pad mask  0          src = src   self dropout1 src2          src = self norm1 src          src2 = self linear2 self dropout self activation self linear1 src             src = src   self dropout2 src2          src = self norm2 src          return src 
def forward self  src  tensor  src mask  optional tensor  = none  src key pad mask  optional tensor  = none  -> tensor      r   pass the input through the encoder layer       args          src  the sequence to the encoder layer  require           src mask  the mask for the src sequence  optional           src key pad mask  the mask for the src key per batch  optional        shape          see the docs in transformer class              src2 = self self attn src  src  src  attn mask=src mask                            key pad mask=src key pad mask  0      src = src   self dropout1 src2      src = self norm1 src      src2 = self linear2 self dropout self activation self linear1 src         src = src   self dropout2 src2      src = self norm2 src      return src 
class transformerdecoderlayer module       r   transformerdecoderlayer be make up of self-attn  multi-head-attn and feedforward network      this standard decoder layer be base on the paper  attention be all you need       ashish vaswani  noam shazeer  niki parmar  jakob uszkoreit  llion jones  aidan n gomez      lukasz kaiser  and illia polosukhin  2017  attention be all you need  in advance in     neural information process systems  page 6000-6010  users may modify or implement     in a different way during application       args          d model  the number of expect feature in the input  require           nhead  the number of head in the multiheadattention model  require           dim feedforward  the dimension of the feedforward network model  default=2048           dropout  the dropout value  default=0 1           activation  the activation function of intermediate layer  relu or gelu  default=relu           layer norm eps  the eps value in layer normalization components  default=1e-5           batch first  if ``true``  then the input and output tensors be provide             as  batch  seq  feature   default  ``false``       examples           >>> decoder layer = nn transformerdecoderlayer d model=512  nhead=8          >>> memory = torch rand 10  32  512          >>> tgt = torch rand 20  32  512          >>> out = decoder layer tgt  memory       alternatively  when ``batch first`` be ``true``          >>> decoder layer = nn transformerdecoderlayer d model=512  nhead=8  batch first=true          >>> memory = torch rand 32  10  512          >>> tgt = torch rand 32  20  512          >>> out = decoder layer tgt  memory                constants   =  string       def   init   self  d model  nhead  dim feedforward=2048  dropout=0 1  activation=string                   layer norm eps=1e-5  batch first=false  device=none  dtype=none  -> none          factory kwargs =  string  device  string  dtype          super transformerdecoderlayer  self    init             self self attn = multiheadattention d model  nhead  dropout=dropout  batch first=batch first                                                factory kwargs          self multihead attn = multiheadattention d model  nhead  dropout=dropout  batch first=batch first                                                     factory kwargs                   self linear1 = linear d model  dim feedforward    factory kwargs          self dropout = dropout dropout          self linear2 = linear dim feedforward  d model    factory kwargs           self norm1 = layernorm d model  eps=layer norm eps    factory kwargs          self norm2 = layernorm d model  eps=layer norm eps    factory kwargs          self norm3 = layernorm d model  eps=layer norm eps    factory kwargs          self dropout1 = dropout dropout          self dropout2 = dropout dropout          self dropout3 = dropout dropout           self activation =  get activation fn activation       def   setstate   self  state           if string not in state              state string  = f relu         super transformerdecoderlayer  self    setstate   state       def forward self  tgt  tensor  memory  tensor  tgt mask  optional tensor  = none  memory mask  optional tensor  = none                  tgt key pad mask  optional tensor  = none  memory key pad mask  optional tensor  = none  -> tensor          r   pass the input  and mask  through the decoder layer           args              tgt  the sequence to the decoder layer  require               memory  the sequence from the last layer of the encoder  require               tgt mask  the mask for the tgt sequence  optional               memory mask  the mask for the memory sequence  optional               tgt key pad mask  the mask for the tgt key per batch  optional               memory key pad mask  the mask for the memory key per batch  optional            shape              see the docs in transformer class                      tgt2 = self self attn tgt  tgt  tgt  attn mask=tgt mask                                key pad mask=tgt key pad mask  0          tgt = tgt   self dropout1 tgt2          tgt = self norm1 tgt          tgt2 = self multihead attn tgt  memory  memory  attn mask=memory mask                                     key pad mask=memory key pad mask  0          tgt = tgt   self dropout2 tgt2          tgt = self norm2 tgt          tgt2 = self linear2 self dropout self activation self linear1 tgt             tgt = tgt   self dropout3 tgt2          tgt = self norm3 tgt          return tgt 
def forward self  tgt  tensor  memory  tensor  tgt mask  optional tensor  = none  memory mask  optional tensor  = none              tgt key pad mask  optional tensor  = none  memory key pad mask  optional tensor  = none  -> tensor      r   pass the input  and mask  through the decoder layer       args          tgt  the sequence to the decoder layer  require           memory  the sequence from the last layer of the encoder  require           tgt mask  the mask for the tgt sequence  optional           memory mask  the mask for the memory sequence  optional           tgt key pad mask  the mask for the tgt key per batch  optional           memory key pad mask  the mask for the memory key per batch  optional        shape          see the docs in transformer class              tgt2 = self self attn tgt  tgt  tgt  attn mask=tgt mask                            key pad mask=tgt key pad mask  0      tgt = tgt   self dropout1 tgt2      tgt = self norm1 tgt      tgt2 = self multihead attn tgt  memory  memory  attn mask=memory mask                                 key pad mask=memory key pad mask  0      tgt = tgt   self dropout2 tgt2      tgt = self norm2 tgt      tgt2 = self linear2 self dropout self activation self linear1 tgt         tgt = tgt   self dropout3 tgt2      tgt = self norm3 tgt      return tgt 
class identity module       r   a placeholder identity operator that be argument-insensitive       args          args  any argument  unused          kwargs  any keyword argument  unused       examples            >>> m = nn identity 54  unused argument1=0 1  unused argument2=false          >>> input = torch randn 128  20          >>> output = m input          >>> print output size            torch size  128  20                def   init   self   args    kwargs           super identity  self    init          def forward self  input  tensor  -> tensor          return input 
class linear module       r   apply a linear transformation to the incoming data   math `y = xa t   b`      this module support  ref `tensorfloat32<tf32 on ampere>`       args          in feature  size of each input sample         out feature  size of each output sample         bias  if set to ``false``  the layer will not learn an additive bias              default  ``true``      shape          - input   math ` n     h  in  ` where  math ` ` mean any number of           additional dimension and  math `h  in  = \text in\ feature `         - output   math ` n     h  out  ` where all but the last dimension           be the same shape as the input and  math `h  out  = \text out\ feature `       attribute          weight  the learnable weight of the module of shape              math ` \text out\ feature   \text in\ feature  `  the value be             initialize from  math `\mathcal u  -\sqrt k   \sqrt k  `  where              math `k = \frac 1  \text in\ feature  `         bias    the learnable bias of the module of shape  math ` \text out\ feature  `                  if  attr `bias` be ``true``  the value be initialize from                  math `\mathcal u  -\sqrt k   \sqrt k  ` where                  math `k = \frac 1  \text in\ feature  `      examples            >>> m = nn linear 20  30          >>> input = torch randn 128  20          >>> output = m input          >>> print output size            torch size  128  30                 constants   =  string  string      in feature  int     out feature  int     weight  tensor      def   init   self  in feature  int  out feature  int  bias  bool = true                   device=none  dtype=none  -> none          factory kwargs =  string  device  string  dtype          super linear  self    init             self in feature = in feature         self out feature = out feature         self weight = parameter torch empty  out feature  in feature     factory kwargs           if bias              self bias = parameter torch empty out feature    factory kwargs           else              self register parameter string  none          self reset parameters        def reset parameters self  -> none          init kaiming uniform  self weight  a=math sqrt 5           if self bias be not none              fan in    = init  calculate fan in and fan out self weight              bind = 1 / math sqrt fan in  if fan in > 0 else 0             init uniform  self bias  -bound  bind       def forward self  input  tensor  -> tensor          return f linear input  self weight  self bias       def extra repr self  -> str          return string format              self in feature  self out feature  self bias be not none           
class bilinear module       r   apply a bilinear transformation to the incoming data       math `y = x 1 t a x 2   b`      args          in1 feature  size of each first input sample         in2 feature  size of each second input sample         out feature  size of each output sample         bias  if set to false  the layer will not learn an additive bias              default  ``true``      shape          - input1   math ` n     h  in1  ` where  math `h  in1 =\text in1\ feature ` and            math ` ` mean any number of additional dimension  all but the last dimension           of the input should be the same          - input2   math ` n     h  in2  ` where  math `h  in2 =\text in2\ feature `          - output   math ` n     h  out  ` where  math `h  out =\text out\ feature `           and all but the last dimension be the same shape as the input       attribute          weight  the learnable weight of the module of shape              math ` \text out\ feature   \text in1\ feature   \text in2\ feature  `              the value be initialize from  math `\mathcal u  -\sqrt k   \sqrt k  `  where              math `k = \frac 1  \text in1\ feature  `         bias    the learnable bias of the module of shape  math ` \text out\ feature  `                  if  attr `bias` be ``true``  the value be initialize from                  math `\mathcal u  -\sqrt k   \sqrt k  `  where                  math `k = \frac 1  \text in1\ feature  `      examples            >>> m = nn bilinear 20  30  40          >>> input1 = torch randn 128  20          >>> input2 = torch randn 128  30          >>> output = m input1  input2          >>> print output size            torch size  128  40                 constants   =  string  string  string      in1 feature  int     in2 feature  int     out feature  int     weight  tensor      def   init   self  in1 feature  int  in2 feature  int  out feature  int  bias  bool = true                   device=none  dtype=none  -> none          factory kwargs =  string  device  string  dtype          super bilinear  self    init             self in1 feature = in1 feature         self in2 feature = in2 feature         self out feature = out feature         self weight = parameter torch empty  out feature  in1 feature  in2 feature     factory kwargs            if bias              self bias = parameter torch empty out feature    factory kwargs           else              self register parameter string  none          self reset parameters        def reset parameters self  -> none          bind = 1 / math sqrt self weight size 1           init uniform  self weight  -bound  bind          if self bias be not none              init uniform  self bias  -bound  bind       def forward self  input1  tensor  input2  tensor  -> tensor          return f bilinear input1  input2  self weight  self bias       def extra repr self  -> str          return string format              self in1 feature  self in2 feature  self out feature  self bias be not none           
class lazylinear lazymodulemixin  linear       r   a  class `torch nn linear` module where `in features` be infer       in this module  the `weight` and `bias` be of  class `torch nn uninitializedparameter`     class  they will be initialize after the first call to ``forward`` be do and the     module will become a regular  class `torch nn linear` module  the ``in features`` argument     of the  class `linear` be infer from the ``input shape -1 ``       check the  class `torch nn modules lazy lazymodulemixin` for further documentation     on lazy modules and their limitations       args          out feature  size of each output sample         bias  if set to ``false``  the layer will not learn an additive bias              default  ``true``      attribute          weight  the learnable weight of the module of shape              math ` \text out\ feature   \text in\ feature  `  the value be             initialize from  math `\mathcal u  -\sqrt k   \sqrt k  `  where              math `k = \frac 1  \text in\ feature  `         bias    the learnable bias of the module of shape  math ` \text out\ feature  `                  if  attr `bias` be ``true``  the value be initialize from                  math `\mathcal u  -\sqrt k   \sqrt k  ` where                  math `k = \frac 1  \text in\ feature  `                cls to become = linear       weight  uninitializedparameter     bias  uninitializedparameter        def   init   self  out feature  int  bias  bool = true                   device=none  dtype=none  -> none          factory kwargs =  string  device  string  dtype                            super     init   0  0  false          self weight = uninitializedparameter   factory kwargs          self out feature = out feature         if bias              self bias = uninitializedparameter   factory kwargs       def reset parameters self  -> none          if not self have uninitialized params   and self in feature  = 0              super   reset parameters        def initialize parameters self  input  -> none            if self have uninitialized params                with torch no grad                    self in feature = input shape -1                  self weight materialize  self out feature  self in feature                   if self bias be not none                      self bias materialize  self out feature                    self reset parameters   
class dropout  dropoutnd       r   during train  randomly zero some of the elements of the input     tensor with probability  attr `p` use sample from a bernoulli     distribution  each channel will be zero out independently on every forward     call       this have prove to be an effective technique for regularization and     prevent the co-adaptation of neurons as describe in the paper     `improving neural network by prevent co-adaptation of feature     detectors`         furthermore  the output be scale by a factor of  math `\frac 1  1-p ` during     train  this mean that during evaluation the module simply compute an     identity function       args          p  probability of an element to be zero  default  0 5         inplace  if set to ``true``  will do this operation in-place  default  ``false``      shape          - input   math `   `  input can be of any shape         - output   math `   `  output be of the same shape as input      examples            >>> m = nn dropout p=0 2          >>> input = torch randn 20  16          >>> output = m input           improve neural network by prevent co-adaptation of feature         detectors  https //arxiv org/abs/1207 0580              def forward self  input  tensor  -> tensor          return f dropout input  self p  self train  self inplace  
class dropout2d  dropoutnd       r   randomly zero out entire channel  a channel be a 2d feature map      e g   the  math `j`-th channel of the  math `i`-th sample in the     batch input be a 2d tensor  math `\text input  i  j `       each channel will be zero out independently on every forward call with     probability  attr `p` use sample from a bernoulli distribution       usually the input come from  class `nn conv2d` modules       as describe in the paper     `efficient object localization use convolutional networks`        if adjacent pixels within feature map be strongly correlate      as be normally the case in early convolution layer  then i i d  dropout     will not regularize the activations and will otherwise just result     in an effective learn rate decrease       in this case   func `nn dropout2d` will help promote independence between     feature map and should be use instead       args          p  float  optional   probability of an element to be zero-ed          inplace  bool  optional   if set to ``true``  will do this operation             in-place      shape          - input   math ` n  c  h  w `         - output   math ` n  c  h  w `  same shape as input       examples            >>> m = nn dropout2d p=0 2          >>> input = torch randn 20  16  32  32          >>> output = m input           efficient object localization use convolutional network         https //arxiv org/abs/1411 4280              def forward self  input  tensor  -> tensor          return f dropout2d input  self p  self train  self inplace  
class dropout3d  dropoutnd       r   randomly zero out entire channel  a channel be a 3d feature map      e g   the  math `j`-th channel of the  math `i`-th sample in the     batch input be a 3d tensor  math `\text input  i  j `       each channel will be zero out independently on every forward call with     probability  attr `p` use sample from a bernoulli distribution       usually the input come from  class `nn conv3d` modules       as describe in the paper     `efficient object localization use convolutional networks`        if adjacent pixels within feature map be strongly correlate      as be normally the case in early convolution layer  then i i d  dropout     will not regularize the activations and will otherwise just result     in an effective learn rate decrease       in this case   func `nn dropout3d` will help promote independence between     feature map and should be use instead       args          p  float  optional   probability of an element to be zero          inplace  bool  optional   if set to ``true``  will do this operation             in-place      shape          - input   math ` n  c  d  h  w `         - output   math ` n  c  d  h  w `  same shape as input       examples            >>> m = nn dropout3d p=0 2          >>> input = torch randn 20  16  4  32  32          >>> output = m input           efficient object localization use convolutional network         https //arxiv org/abs/1411 4280              def forward self  input  tensor  -> tensor          return f dropout3d input  self p  self train  self inplace  
class alphadropout  dropoutnd       r   apply alpha dropout over the input       alpha dropout be a type of dropout that maintain the self-normalizing     property      for an input with zero mean and unit standard deviation  the output of     alpha dropout maintain the original mean and standard deviation of the     input      alpha dropout go hand-in-hand with selu activation function  which ensure     that the output have zero mean and unit standard deviation       during train  it randomly mask some of the elements of the input     tensor with probability  p  use sample from a bernoulli distribution      the elements to mask be randomize on every forward call  and scale     and shift to maintain zero mean and unit standard deviation       during evaluation the module simply compute an identity function       more detail can be find in the paper `self-normalizing neural networks`         args          p  float   probability of an element to be drop  default  0 5         inplace  bool  optional   if set to ``true``  will do this operation             in-place      shape          - input   math `   `  input can be of any shape         - output   math `   `  output be of the same shape as input      examples            >>> m = nn alphadropout p=0 2          >>> input = torch randn 20  16          >>> output = m input           self-normalizing neural network  https //arxiv org/abs/1706 02515              def forward self  input  tensor  -> tensor          return f alpha dropout input  self p  self train  
class featurealphadropout  dropoutnd       r   randomly mask out entire channel  a channel be a feature map      e g  the  math `j`-th channel of the  math `i`-th sample in the batch input     be a tensor  math `\text input  i  j `  of the input tensor   instead of     set activations to zero  as in regular dropout  the activations be set     to the negative saturation value of the selu activation function  more detail     can be find in the paper `self-normalizing neural networks`         each element will be mask independently for each sample on every forward     call with probability  attr `p` use sample from a bernoulli distribution      the elements to be mask be randomize on every forward call  and scale     and shift to maintain zero mean and unit variance       usually the input come from  class `nn alphadropout` modules       as describe in the paper     `efficient object localization use convolutional networks`        if adjacent pixels within feature map be strongly correlate      as be normally the case in early convolution layer  then i i d  dropout     will not regularize the activations and will otherwise just result     in an effective learn rate decrease       in this case   func `nn alphadropout` will help promote independence between     feature map and should be use instead       args          p  float  optional   probability of an element to be zero  default  0 5         inplace  bool  optional   if set to ``true``  will do this operation             in-place      shape          - input   math ` n  c  d  h  w `         - output   math ` n  c  d  h  w `  same shape as input       examples            >>> m = nn featurealphadropout p=0 2          >>> input = torch randn 20  16  4  32  32          >>> output = m input           self-normalizing neural network  https //arxiv org/abs/1706 02515         efficient object localization use convolutional network         https //arxiv org/abs/1411 4280              def forward self  input  tensor  -> tensor          return f feature alpha dropout input  self p  self train  
class embed module       r   a simple lookup table that store embeddings of a fix dictionary and size       this module be often use to store word embeddings and retrieve them use indices      the input to the module be a list of indices  and the output be the correspond     word embeddings       args          num embeddings  int   size of the dictionary of embeddings         embed dim  int   the size of each embed vector         pad idx  int  optional   if specify  the entries at  attr `padding idx` do not contribute to the gradient                                       therefore  the embed vector at  attr `padding idx` be not update during train                                       i e  it remain as a fix  pad   for a newly construct embed                                       the embed vector at  attr `padding idx` will default to all zero                                       but can be update to another value to be use as the pad vector          max norm  float  optional   if give  each embed vector with norm larger than  attr `max norm`                                     be renormalize to have norm  attr `max norm`          norm type  float  optional   the p of the p-norm to compute for the  attr `max norm` option  default ``2``          scale grad by freq  boolean  optional   if give  this will scale gradients by the inverse of frequency of                                                 the word in the mini-batch  default ``false``          sparse  bool  optional   if ``true``  gradient w r t   attr `weight` matrix will be a sparse tensor                                   see note for more detail regard sparse gradients       attribute          weight  tensor   the learnable weight of the module of shape  num embeddings  embed dim                           initialize from  math `\mathcal n  0  1 `      shape          - input   math `   `  inttensor or longtensor of arbitrary shape contain the indices to extract         - output   math `    h `  where ` ` be the input shape and  math `h=\text embedding\ dim `         note           keep in mind that only a limit number of optimizers support         sparse gradients  currently it s  class `optim sgd`  `cuda` and `cpu`            class `optim sparseadam`  `cuda` and `cpu`  and  class `optim adagrad`  `cpu`          note           when  attr `max norm` be not ``none``   class `embedding` s forward method will modify the          attr `weight` tensor in-place  since tensors need for gradient computations cannot be         modify in-place  perform a differentiable operation on ``embedding weight`` before         call  class `embedding` s forward method require clone ``embedding weight`` when          attr `max norm` be not ``none``  for example                n  d  m = 3  5  7             embed = nn embed n  d  max norm=true              w = torch randn  m  d   require grad=true              idx = torch tensor  1  2               a = embed weight clone     w t      weight must be clone for this to be differentiable             b = embed idx    w t      modify weight in-place             out =  a unsqueeze 0    b unsqueeze 1               loss = out sigmoid   prod               loss backward        examples            >>>   an embed module contain 10 tensors of size 3         >>> embed = nn embed 10  3          >>>   a batch of 2 sample of 4 indices each         >>> input = torch longtensor   1 2 4 5   4 3 2 9            >>> embed input          tensor    -0 0251  -1 6902   0 7172                     -0 6431   0 0748   0 6969                      1 4970   1 3448  -0 9685                     -0 3677  -2 7265  -0 1685                        1 4970   1 3448  -0 9685                      0 4362  -0 4004   0 9400                     -0 6431   0 0748   0 6969                      0 9124  -2 3616   1 1151               >>>   example with pad idx         >>> embed = nn embed 10  3  pad idx=0          >>> input = torch longtensor   0 2 0 5            >>> embed input          tensor     0 0000   0 0000   0 0000                      0 1535  -2 0309   0 9315                      0 0000   0 0000   0 0000                     -0 1655   0 9897   0 0635              >>>   example of change `pad` vector         >>> pad idx = 0         >>> embed = nn embed 3  3  pad idx=padding idx          >>> embed weight         parameter contain          tensor    0 0000   0 0000   0 0000                    -0 7895  -0 7089  -0 0364                     0 6778   0 5803   0 2678    require grad=true          >>> with torch no grad                    embed weight pad idx  = torch ones 3          >>> embed weight         parameter contain          tensor    1 0000   1 0000   1 0000                    -0 7895  -0 7089  -0 0364                     0 6778   0 5803   0 2678    require grad=true                constants   =  string  string  string  string                       string  string  string       num embeddings  int     embed dim  int     pad idx  optional int      max norm  optional float      norm type  float     scale grad by freq  bool     weight  tensor     sparse  bool      def   init   self  num embeddings  int  embed dim  int  pad idx  optional int  = none                   max norm  optional float  = none  norm type  float = 2   scale grad by freq  bool = false                   sparse  bool = false   weight  optional tensor  = none                   device=none  dtype=none  -> none          factory kwargs =  string  device  string  dtype          super embed  self    init             self num embeddings = num embeddings         self embed dim = embed dim         if pad idx be not none              if pad idx > 0                  assert pad idx < self num embeddings  string             elif pad idx < 0                  assert pad idx >= -self num embeddings  string                 pad idx = self num embeddings   pad idx         self pad idx = pad idx         self max norm = max norm         self norm type = norm type         self scale grad by freq = scale grad by freq         if  weight be none              self weight = parameter torch empty  num embeddings  embed dim     factory kwargs               self reset parameters           else              assert list  weight shape  ==  num embeddings  embed dim   \                 string             self weight = parameter  weight           self sparse = sparse      def reset parameters self  -> none          init normal  self weight          self  fill pad idx with zero        def  fill pad idx with zero self  -> none          if self pad idx be not none              with torch no grad                    self weight self pad idx  fill  0       def forward self  input  tensor  -> tensor          return f embed              input  self weight  self pad idx  self max norm              self norm type  self scale grad by freq  self sparse       def extra repr self  -> str          s = string         if self pad idx be not none              s  = string         if self max norm be not none              s  = string         if self norm type  = 2              s  = string         if self scale grad by freq be not false              s  = string         if self sparse be not false              s  = string         return s format   self   dict          classmethod     def from pretrained cls  embeddings  freeze=true  pad idx=none                          max norm=none  norm type=2   scale grad by freq=false                          sparse=false           r   create embed instance from give 2-dimensional floattensor           args              embeddings  tensor   floattensor contain weight for the embed                  first dimension be be pass to embed as ``num embeddings``  second as ``embedding dim``              freeze  boolean  optional   if ``true``  the tensor do not get update in the learn process                  equivalent to ``embedding weight require grad = false``  default  ``true``             pad idx  int  optional   if specify  the entries at  attr `padding idx` do not contribute to the gradient                                           therefore  the embed vector at  attr `padding idx` be not update during train                                           i e  it remain as a fix  pad               max norm  float  optional   see module initialization documentation              norm type  float  optional   see module initialization documentation  default ``2``              scale grad by freq  boolean  optional   see module initialization documentation  default ``false``              sparse  bool  optional   see module initialization documentation           examples                >>>   floattensor contain pretrained weight             >>> weight = torch floattensor   1  2 3  3    4  5 1  6 3                >>> embed = nn embed from pretrained weight              >>>   get embeddings for index 1             >>> input = torch longtensor  1               >>> embed input              tensor    4 0000   5 1000   6 3000                        assert embeddings dim   == 2  \             string         row  cols = embeddings shape         embed = cls              num embeddings=rows              embed dim=cols               weight=embeddings              pad idx=padding idx              max norm=max norm              norm type=norm type              scale grad by freq=scale grad by freq              sparse=sparse          embed weight require grad = not freeze         return embed 
class embeddingbag module       r   compute sum or mean of  bag  of embeddings  without instantiate the     intermediate embeddings       for bag of constant length  no  attr `per sample weights`  no indices equal to  attr `padding idx`      and with 2d input  this class            with ``mode= sum `` be equivalent to  class `~torch nn embedding` follow by ``torch sum dim=1 ``            with ``mode= mean `` be equivalent to  class `~torch nn embedding` follow by ``torch mean dim=1 ``            with ``mode= max `` be equivalent to  class `~torch nn embedding` follow by ``torch max dim=1 ``       however   class `~torch nn embeddingbag` be much more time and memory efficient than use a chain of these     operations       embeddingbag also support per-sample weight as an argument to the forward     pass  this scale the output of the embed before perform a weight     reduction as specify by ``mode``  if  attr `per sample weights` be pass  the     only support ``mode`` be `` sum ``  which compute a weight sum accord to      attr `per sample weights`       args          num embeddings  int   size of the dictionary of embeddings         embed dim  int   the size of each embed vector         max norm  float  optional   if give  each embed vector with norm larger than  attr `max norm`                                     be renormalize to have norm  attr `max norm`          norm type  float  optional   the p of the p-norm to compute for the  attr `max norm` option  default ``2``          scale grad by freq  boolean  optional   if give  this will scale gradients by the inverse of frequency of                                                 the word in the mini-batch  default ``false``                                                  note  this option be not support when ``mode= max ``          mode  string  optional   `` sum ``  `` mean `` or `` max ``  specify the way to reduce the bag                                   `` sum `` compute the weight sum  take  attr `per sample weights`                                  into consideration  `` mean `` compute the average of the value                                  in the bag  `` max `` compute the max value over each bag                                   default  `` mean ``         sparse  bool  optional   if ``true``  gradient w r t   attr `weight` matrix will be a sparse tensor  see                                  note for more detail regard sparse gradients  note  this option be not                                  support when ``mode= max ``          include last offset  bool  optional   if ``true``   attr `offsets` have one additional element  where the last element                                       be equivalent to the size of `indices`  this match the csr format          pad idx  int  optional   if specify  the entries at  attr `padding idx` do not contribute to the                                      gradient  therefore  the embed vector at  attr `padding idx` be not update                                      during train  i e  it remain as a fix  pad   for a newly construct                                      embeddingbag  the embed vector at  attr `padding idx` will default to all                                      zero  but can be update to another value to be use as the pad vector                                       note that the embed vector at  attr `padding idx` be exclude from the                                      reduction       attribute          weight  tensor   the learnable weight of the module of shape ` num embeddings  embed dim `                          initialize from  math `\mathcal n  0  1 `       examples            >>>   an embeddingbag module contain 10 tensors of size 3         >>> embed sum = nn embeddingbag 10  3  mode= sum           >>>   a batch of 2 sample of 4 indices each         >>> input = torch tensor  1 2 4 5 4 3 2 9   dtype=torch long          >>> offset = torch tensor  0 4   dtype=torch long          >>> embed sum input  offset          tensor   -0 8861  -5 4350  -0 0523                     1 1306  -2 5798  -1 0044             >>>   example with pad idx         >>> embed sum = nn embeddingbag 10  3  mode= sum   pad idx=2          >>> input = torch tensor  2  2  2  2  4  3  2  9   dtype=torch long          >>> offset = torch tensor  0 4   dtype=torch long          >>> embed sum input  offset          tensor    0 0000   0 0000   0 0000                    -0 7082   3 2145  -2 6251             >>>   an embeddingbag can be load from an embed like so         >>> embed = nn embed 10  3  pad idx=2          >>> embed sum = nn embeddingbag from pretrained                  embed weight                  pad idx=embedding pad idx                  mode= sum                 constants   =  string  string  string  string                       string  string  string  string                       string       num embeddings  int     embed dim  int     max norm  optional float      norm type  float     scale grad by freq  bool     weight  tensor     mode  str     sparse  bool     include last offset  bool     pad idx  optional int       def   init   self  num embeddings  int  embed dim  int                   max norm  optional float  = none  norm type  float = 2   scale grad by freq  bool = false                   mode  str = string  sparse  bool = false   weight  optional tensor  = none                   include last offset  bool = false  pad idx  optional int  = none                   device=none  dtype=none  -> none          factory kwargs =  string  device  string  dtype          super embeddingbag  self    init             self num embeddings = num embeddings         self embed dim = embed dim         self max norm = max norm         self norm type = norm type         self scale grad by freq = scale grad by freq         if pad idx be not none              if pad idx > 0                  assert pad idx < self num embeddings  string             elif pad idx < 0                  assert pad idx >= -self num embeddings  string                 pad idx = self num embeddings   pad idx         self pad idx = pad idx         if  weight be none              self weight = parameter torch empty  num embeddings  embed dim     factory kwargs               self reset parameters           else              assert list  weight shape  ==  num embeddings  embed dim   \                 string             self weight = parameter  weight          self mode = mode         self sparse = sparse         self include last offset = include last offset      def reset parameters self  -> none          init normal  self weight          self  fill pad idx with zero        def  fill pad idx with zero self  -> none          if self pad idx be not none              with torch no grad                    self weight self pad idx  fill  0       def forward self  input  tensor  offset  optional tensor  = none  per sample weight  optional tensor  = none  -> tensor          string         return f embed bag input  self weight  offset                                 self max norm  self norm type                                 self scale grad by freq  self mode  self sparse                                 per sample weight  self include last offset                                 self pad idx       def extra repr self  -> str          s = string         if self max norm be not none              s  = string         if self norm type  = 2              s  = string         if self scale grad by freq be not false              s  = string         s  = string         if self pad idx be not none              s  = string         return s format   self   dict          classmethod     def from pretrained cls  embeddings  tensor  freeze  bool = true  max norm  optional float  = none                          norm type  float = 2   scale grad by freq  bool = false                          mode  str = string  sparse  bool = false  include last offset  bool = false                          pad idx  optional int  = none  -> string          r   create embeddingbag instance from give 2-dimensional floattensor           args              embeddings  tensor   floattensor contain weight for the embeddingbag                  first dimension be be pass to embeddingbag as  num embeddings   second as  embed dim               freeze  boolean  optional   if ``true``  the tensor do not get update in the learn process                  equivalent to ``embeddingbag weight require grad = false``  default  ``true``             max norm  float  optional   see module initialization documentation  default  ``none``             norm type  float  optional   see module initialization documentation  default ``2``              scale grad by freq  boolean  optional   see module initialization documentation  default ``false``              mode  string  optional   see module initialization documentation  default  `` mean ``             sparse  bool  optional   see module initialization documentation  default  ``false``              include last offset  bool  optional   see module initialization documentation  default  ``false``              pad idx  int  optional   see module initialization documentation  default  ``none``           examples                >>>   floattensor contain pretrained weight             >>> weight = torch floattensor   1  2 3  3    4  5 1  6 3                >>> embeddingbag = nn embeddingbag from pretrained weight              >>>   get embeddings for index 1             >>> input = torch longtensor   1  0                >>> embeddingbag input              tensor    2 5000   3 7000   4 6500                        assert embeddings dim   == 2  \             string         row  cols = embeddings shape         embeddingbag = cls              num embeddings=rows              embed dim=cols               weight=embeddings              max norm=max norm              norm type=norm type              scale grad by freq=scale grad by freq              mode=mode              sparse=sparse              include last offset=include last offset              pad idx=padding idx          embeddingbag weight require grad = not freeze         return embeddingbag 
def forward self  input  tensor  offset  optional tensor  = none  per sample weight  optional tensor  = none  -> tensor      string     return f embed bag input  self weight  offset                             self max norm  self norm type                             self scale grad by freq  self mode  self sparse                             per sample weight  self include last offset                             self pad idx  
class cosinesimilarity module       r   return cosine similarity between  math `x 1` and  math `x 2`  compute along dim          math            \text similarity  = \dfrac x 1 \cdot x 2  \max \vert x 1 \vert  2 \cdot \vert x 2 \vert  2  \epsilon         args          dim  int  optional   dimension where cosine similarity be compute  default  1         eps  float  optional   small value to avoid division by zero              default  1e-8     shape          - input1   math ` \ast 1  d  \ast 2 ` where d be at position `dim`         - input2   math ` \ast 1  d  \ast 2 `  same shape as the input1         - output   math ` \ast 1  \ast 2 `     examples           >>> input1 = torch randn 100  128          >>> input2 = torch randn 100  128          >>> cos = nn cosinesimilarity dim=1  eps=1e-6          >>> output = cos input1  input2                constants   =  string  string      dim  int     eps  float      def   init   self  dim  int = 1  eps  float = 1e-8  -> none          super cosinesimilarity  self    init             self dim = dim         self eps = eps      def forward self  x1  tensor  x2  tensor  -> tensor          return f cosine similarity x1  x2  self dim  self eps  
class pairwisedistance module       r        compute the batchwise pairwise distance between vectors  math `v 1`   math `v 2` use the p-norm          math            \vert x \vert  p = \left  \sum  i=1  n  \vert x i \vert   p \right     1/p        args          p  real   the norm degree  default  2         eps  float  optional   small value to avoid division by zero              default  1e-6         keepdim  bool  optional   determine whether or not to keep the vector dimension              default  false     shape          - input1   math ` n  d ` where `d = vector dimension`         - input2   math ` n  d `  same shape as the input1         - output   math ` n `  if  attr `keepdim` be ``true``  then  math ` n  1 `      examples           >>> pdist = nn pairwisedistance p=2          >>> input1 = torch randn 100  128          >>> input2 = torch randn 100  128          >>> output = pdist input1  input2                constants   =  string  string  string      norm  float     eps  float     keepdim  bool      def   init   self  p  float = 2   eps  float = 1e-6  keepdim  bool = false  -> none          super pairwisedistance  self    init             self norm = p         self eps = eps         self keepdim = keepdim      def forward self  x1  tensor  x2  tensor  -> tensor          return f pairwise distance x1  x2  self norm  self eps  self keepdim  
class l1loss  loss       r   create a criterion that measure the mean absolute error  mae  between each element in     the input  math `x` and target  math `y`       the unreduced  i e  with  attr `reduction` set to `` none ``  loss can be describe as          math           \ell x  y  = l = \ l 1 \dots l n\  \top  \quad         l n = \left  x n - y n \right        where  math `n` be the batch size  if  attr `reduction` be not `` none ``      default `` mean ``   then          math           \ell x  y  =         \begin case              \operatorname mean  l     \text if reduction  = \text `mean   \\             \operatorname sum  l      \text if reduction  = \text `sum            \end case        math `x` and  math `y` be tensors of arbitrary shape with a total     of  math `n` elements each       the sum operation still operate over all the elements  and divide by  math `n`       the division by  math `n` can be avoid if one set ``reduction =  sum ``       support real-valued and complex-valued input       args          size average  bool  optional   deprecate  see  attr `reduction`   by default              the losses be average over each loss element in the batch  note that for             some losses  there be multiple elements per sample  if the field  attr `size average`             be set to ``false``  the losses be instead sum for each minibatch  ignore             when  attr `reduce` be ``false``  default  ``true``         reduce  bool  optional   deprecate  see  attr `reduction`   by default  the             losses be average or sum over observations for each minibatch depend             on  attr `size average`  when  attr `reduce` be ``false``  return a loss per             batch element instead and ignore  attr `size average`  default  ``true``         reduction  string  optional   specify the reduction to apply to the output              `` none ``   `` mean ``   `` sum ``  `` none ``  no reduction will be apply              `` mean ``  the sum of the output will be divide by the number of             elements in the output  `` sum ``  the output will be sum  note   attr `size average`             and  attr `reduce` be in the process of be deprecate  and in the meantime              specify either of those two args will override  attr `reduction`  default  `` mean ``      shape          - input   math ` n    ` where  math ` ` mean  any number of additional           dimension         - target   math ` n    `  same shape as the input         - output  scalar  if  attr `reduction` be `` none ``  then            math ` n    `  same shape as the input      examples            >>> loss = nn l1loss           >>> input = torch randn 3  5  require grad=true          >>> target = torch randn 3  5          >>> output = loss input  target          >>> output backward                 constants   =  string       def   init   self  size average=none  reduce=none  reduction  str = string  -> none          super l1loss  self    init   size average  reduce  reduction       def forward self  input  tensor  target  tensor  -> tensor          return f l1 loss input  target  reduction=self reduction  
class mseloss  loss       r   create a criterion that measure the mean square error  square l2 norm  between     each element in the input  math `x` and target  math `y`       the unreduced  i e  with  attr `reduction` set to `` none ``  loss can be describe as          math           \ell x  y  = l = \ l 1 \dots l n\  \top  \quad         l n = \left  x n - y n \right  2       where  math `n` be the batch size  if  attr `reduction` be not `` none ``      default `` mean ``   then          math           \ell x  y  =         \begin case              \operatorname mean  l      \text if reduction  = \text `mean   \\             \operatorname sum  l       \text if reduction  = \text `sum            \end case        math `x` and  math `y` be tensors of arbitrary shape with a total     of  math `n` elements each       the mean operation still operate over all the elements  and divide by  math `n`       the division by  math `n` can be avoid if one set ``reduction =  sum ``       args          size average  bool  optional   deprecate  see  attr `reduction`   by default              the losses be average over each loss element in the batch  note that for             some losses  there be multiple elements per sample  if the field  attr `size average`             be set to ``false``  the losses be instead sum for each minibatch  ignore             when  attr `reduce` be ``false``  default  ``true``         reduce  bool  optional   deprecate  see  attr `reduction`   by default  the             losses be average or sum over observations for each minibatch depend             on  attr `size average`  when  attr `reduce` be ``false``  return a loss per             batch element instead and ignore  attr `size average`  default  ``true``         reduction  string  optional   specify the reduction to apply to the output              `` none ``   `` mean ``   `` sum ``  `` none ``  no reduction will be apply              `` mean ``  the sum of the output will be divide by the number of             elements in the output  `` sum ``  the output will be sum  note   attr `size average`             and  attr `reduce` be in the process of be deprecate  and in the meantime              specify either of those two args will override  attr `reduction`  default  `` mean ``      shape          - input   math ` n    ` where  math ` ` mean  any number of additional           dimension         - target   math ` n    `  same shape as the input      examples            >>> loss = nn mseloss           >>> input = torch randn 3  5  require grad=true          >>> target = torch randn 3  5          >>> output = loss input  target          >>> output backward                 constants   =  string       def   init   self  size average=none  reduce=none  reduction  str = string  -> none          super mseloss  self    init   size average  reduce  reduction       def forward self  input  tensor  target  tensor  -> tensor          return f mse loss input  target  reduction=self reduction  
class crossentropyloss  weightedloss       r   this criterion combine  class `~torch nn logsoftmax` and  class `~torch nn nllloss` in one single class       it be useful when train a classification problem with `c` class      if provide  the optional argument  attr `weight` should be a 1d `tensor`     assign weight to each of the class      this be particularly useful when you have an unbalance train set       the `input` be expect to contain raw  unnormalized score for each class       `input` have to be a tensor of size either  math ` minibatch  c ` or      math ` minibatch  c  d 1  d 2       d k `     with  math `k \geq 1` for the `k`-dimensional case  describe later        this criterion expect a class index in the range  math ` 0  c-1 ` as the     `target` for each value of a 1d tensor of size `minibatch`  if `ignore index`     be specify  this criterion also accept this class index  this index may not     necessarily be in the class range        the loss can be describe as          math           \text loss  x  class  = -\log\left \frac \exp x class    \sum j \exp x j   \right                         = -x class    \log\left \sum j \exp x j  \right       or in the case of the  attr `weight` argument be specify          math           \text loss  x  class  = weight class  \left -x class    \log\left \sum j \exp x j  \right \right       the losses be average across observations for each minibatch  if the      attr `weight` argument be specify then this be a weight average          math           \text loss  = \frac \sum  n   i=1  loss i  class i    \sum  n   i=1  weight class i         can also be use for higher dimension input  such as 2d image  by provide     an input of size  math ` minibatch  c  d 1  d 2       d k ` with  math `k \geq 1`      where  math `k` be the number of dimension  and a target of appropriate shape      see below         args          weight  tensor  optional   a manual rescale weight give to each class              if give  have to be a tensor of size `c`         size average  bool  optional   deprecate  see  attr `reduction`   by default              the losses be average over each loss element in the batch  note that for             some losses  there be multiple elements per sample  if the field  attr `size average`             be set to ``false``  the losses be instead sum for each minibatch  ignore             when  attr `reduce` be ``false``  default  ``true``         ignore index  int  optional   specify a target value that be ignore             and do not contribute to the input gradient  when  attr `size average` be             ``true``  the loss be average over non-ignored target          reduce  bool  optional   deprecate  see  attr `reduction`   by default  the             losses be average or sum over observations for each minibatch depend             on  attr `size average`  when  attr `reduce` be ``false``  return a loss per             batch element instead and ignore  attr `size average`  default  ``true``         reduction  string  optional   specify the reduction to apply to the output              `` none ``   `` mean ``   `` sum ``  `` none ``  no reduction will             be apply  `` mean ``  the weight mean of the output be take              `` sum ``  the output will be sum  note   attr `size average`             and  attr `reduce` be in the process of be deprecate  and in             the meantime  specify either of those two args will override              attr `reduction`  default  `` mean ``      shape          - input   math ` n  c ` where `c = number of classes`  or            math ` n  c  d 1  d 2       d k ` with  math `k \geq 1`           in the case of `k`-dimensional loss          - target   math ` n ` where each value be  math `0 \leq \text target  i  \leq c-1`  or            math ` n  d 1  d 2       d k ` with  math `k \geq 1` in the case of           k-dimensional loss          - output  scalar            if  attr `reduction` be `` none ``  then the same size as the target             math ` n `  or            math ` n  d 1  d 2       d k ` with  math `k \geq 1` in the case           of k-dimensional loss       examples            >>> loss = nn crossentropyloss           >>> input = torch randn 3  5  require grad=true          >>> target = torch empty 3  dtype=torch long  random  5          >>> output = loss input  target          >>> output backward                 constants   =  string  string      ignore index  int      def   init   self  weight  optional tensor  = none  size average=none  ignore index  int = -100                   reduce=none  reduction  str = string  -> none          super crossentropyloss  self    init   weight  size average  reduce  reduction          self ignore index = ignore index      def forward self  input  tensor  target  tensor  -> tensor          return f cross entropy input  target  weight=self weight                                 ignore index=self ignore index  reduction=self reduction  
class ctcloss  loss       r   the connectionist temporal classification loss       calculate loss between a continuous  unsegmented  time series and a target sequence  ctcloss sum over the     probability of possible alignments of input to target  produce a loss value which be differentiable     with respect to each input node  the alignment of input to target be assume to be  many-to-one   which     limit the length of the target sequence such that it must be  math `\leq` the input length       args          blank  int  optional   blank label  default  math `0`          reduction  string  optional   specify the reduction to apply to the output              `` none ``   `` mean ``   `` sum ``  `` none ``  no reduction will be apply              `` mean ``  the output losses will be divide by the target lengths and             then the mean over the batch be take  default  `` mean ``         zero infinity  bool  optional               whether to zero infinite losses and the associate gradients              default  ``false``             infinite losses mainly occur when the input be too short             to be align to the target       shape          - log probs  tensor of size  math ` t  n  c `            where  math `t = \text input length `             math `n = \text batch size `  and            math `c = \text number of class  include blank  `            the logarithmized probabilities of the output  e g  obtain with            func `torch nn functional log softmax`           - target  tensor of size  math ` n  s ` or            math ` \operatorname sum  \text target\ lengths   `            where  math `n = \text batch size ` and            math `s = \text max target length  if shape be    n  s `            it represent the target sequence  each element in the target           sequence be a class index  and the target index cannot be blank  default=0             in the  math ` n  s ` form  target be pad to the           length of the longest sequence  and stack            in the  math ` \operatorname sum  \text target\ lengths   ` form            the target be assume to be un-padded and           concatenate within 1 dimension          - input lengths  tuple or tensor of size  math ` n `            where  math `n = \text batch size `  it represent the lengths of the           input  must each be  math `\leq t`   and the lengths be specify           for each sequence to achieve mask under the assumption that sequence           be pad to equal lengths          - target lengths  tuple or tensor of size  math ` n `            where  math `n = \text batch size `  it represent lengths of the target            lengths be specify for each sequence to achieve mask under the           assumption that sequence be pad to equal lengths  if target shape be            math ` n s `  target lengths be effectively the stop index            math `s n` for each target sequence  such that ``target n = target n 0 s n `` for           each target in a batch  lengths must each be  math `\leq s`           if the target be give as a 1d tensor that be the concatenation of individual           target  the target lengths must add up to the total length of the tensor          - output  scalar  if  attr `reduction` be `` none ``  then            math ` n `  where  math `n = \text batch size `       examples            >>>   target be to be pad         >>> t = 50        input sequence length         >>> c = 20        number of class  include blank          >>> n = 16        batch size         >>> s = 30        target sequence length of longest target in batch  pad length          >>> s min = 10    minimum target length  for demonstration purpose         >>>         >>>   initialize random batch of input vectors  for  size =  t n c          >>> input = torch randn t  n  c  log softmax 2  detach   require grad            >>>         >>>   initialize random batch of target  0 = blank  1 c = class          >>> target = torch randint low=1  high=c  size= n  s   dtype=torch long          >>>         >>> input lengths = torch full size= n    fill value=t  dtype=torch long          >>> target lengths = torch randint low=s min  high=s  size= n    dtype=torch long          >>> ctc loss = nn ctcloss           >>> loss = ctc loss input  target  input lengths  target lengths          >>> loss backward           >>>         >>>         >>>   target be to be un-padded         >>> t = 50        input sequence length         >>> c = 20        number of class  include blank          >>> n = 16        batch size         >>>         >>>   initialize random batch of input vectors  for  size =  t n c          >>> input = torch randn t  n  c  log softmax 2  detach   require grad            >>> input lengths = torch full size= n    fill value=t  dtype=torch long          >>>         >>>   initialize random batch of target  0 = blank  1 c = class          >>> target lengths = torch randint low=1  high=t  size= n    dtype=torch long          >>> target = torch randint low=1  high=c  size= sum target lengths     dtype=torch long          >>> ctc loss = nn ctcloss           >>> loss = ctc loss input  target  input lengths  target lengths          >>> loss backward        reference          a  grave et al   connectionist temporal classification          label unsegmented sequence data with recurrent neural network          https //www cs toronto edu/~graves/icml 2006 pdf      note          in order to use cudnn  the follow must be satisfy   attr `targets` must be         in concatenate format  all  attr `input lengths` must be `t`    math `blank=0`           attr `target lengths`  math `\leq 256`  the integer arguments must be of         dtype  attr `torch int32`           the regular implementation use the  more common in pytorch  `torch long` dtype        note          in some circumstances when use the cuda backend with cudnn  this operator         may select a nondeterministic algorithm to increase performance  if this be         undesirable  you can try to make the operation deterministic  potentially at         a performance cost  by set ``torch backends cudnn deterministic =         true``          please see the note on  doc `/notes/randomness` for background                constants   =  string  string      blank  int     zero infinity  bool      def   init   self  blank  int = 0  reduction  str = string  zero infinity  bool = false           super ctcloss  self    init   reduction=reduction          self blank = blank         self zero infinity = zero infinity      def forward self  log probs  tensor  target  tensor  input lengths  tensor  target lengths  tensor  -> tensor          return f ctc loss log probs  target  input lengths  target lengths  self blank  self reduction                            self zero infinity  
class nllloss  weightedloss       r   the negative log likelihood loss  it be useful to train a classification     problem with `c` class       if provide  the optional argument  attr `weight` should be a 1d tensor assign     weight to each of the class  this be particularly useful when you have an     unbalance train set       the `input` give through a forward call be expect to contain     log-probabilities of each class  `input` have to be a tensor of size either      math ` minibatch  c ` or  math ` minibatch  c  d 1  d 2       d k `     with  math `k \geq 1` for the `k`-dimensional case  describe later        obtain log-probabilities in a neural network be easily achieve by     add a  `logsoftmax`  layer in the last layer of your network      you may use `crossentropyloss` instead  if you prefer not to add an extra     layer       the `target` that this loss expect should be a class index in the range  math ` 0  c-1 `     where `c = number of classes`  if `ignore index` be specify  this loss also accept     this class index  this index may not necessarily be in the class range        the unreduced  i e  with  attr `reduction` set to `` none ``  loss can be describe as          math           \ell x  y  = l = \ l 1 \dots l n\  \top  \quad         l n = - w  y n  x  n y n   \quad         w  c  = \text weight  c  \cdot \mathbb 1 \ c \not= \text ignore\ index \        where  math `x` be the input   math `y` be the target   math `w` be the weight  and      math `n` be the batch size  if  attr `reduction` be not `` none ``      default `` mean ``   then         math           \ell x  y  = \begin case              \sum  n=1  n \frac 1  \sum  n=1  n w  y n   l n                \text if reduction  = \text `mean   \\             \sum  n=1  n l n                 \text if reduction  = \text `sum            \end case       can also be use for higher dimension input  such as 2d image  by provide     an input of size  math ` minibatch  c  d 1  d 2       d k ` with  math `k \geq 1`      where  math `k` be the number of dimension  and a target of appropriate shape      see below   in the case of image  it compute nll loss per-pixel       args          weight  tensor  optional   a manual rescale weight give to each             class  if give  it have to be a tensor of size `c`  otherwise  it be             treat as if have all ones          size average  bool  optional   deprecate  see  attr `reduction`   by default              the losses be average over each loss element in the batch  note that for             some losses  there be multiple elements per sample  if the field  attr `size average`             be set to ``false``  the losses be instead sum for each minibatch  ignore             when  attr `reduce` be ``false``  default  ``true``         ignore index  int  optional   specify a target value that be ignore             and do not contribute to the input gradient  when              attr `size average` be ``true``  the loss be average over             non-ignored target          reduce  bool  optional   deprecate  see  attr `reduction`   by default  the             losses be average or sum over observations for each minibatch depend             on  attr `size average`  when  attr `reduce` be ``false``  return a loss per             batch element instead and ignore  attr `size average`  default  ``true``         reduction  string  optional   specify the reduction to apply to the output              `` none ``   `` mean ``   `` sum ``  `` none ``  no reduction will             be apply  `` mean ``  the weight mean of the output be take              `` sum ``  the output will be sum  note   attr `size average`             and  attr `reduce` be in the process of be deprecate  and in             the meantime  specify either of those two args will override              attr `reduction`  default  `` mean ``      shape          - input   math ` n  c ` where `c = number of classes`  or            math ` n  c  d 1  d 2       d k ` with  math `k \geq 1`           in the case of `k`-dimensional loss          - target   math ` n ` where each value be  math `0 \leq \text target  i  \leq c-1`  or            math ` n  d 1  d 2       d k ` with  math `k \geq 1` in the case of           k-dimensional loss          - output  scalar            if  attr `reduction` be `` none ``  then the same size as the target   math ` n `  or            math ` n  d 1  d 2       d k ` with  math `k \geq 1` in the case           of k-dimensional loss       examples            >>> m = nn logsoftmax dim=1          >>> loss = nn nllloss           >>>   input be of size n x c = 3 x 5         >>> input = torch randn 3  5  require grad=true          >>>   each element in target have to have 0 <= value < c         >>> target = torch tensor  1  0  4           >>> output = loss m input   target          >>> output backward           >>>         >>>         >>>   2d loss example  use  for example  with image input          >>> n  c = 5  4         >>> loss = nn nllloss           >>>   input be of size n x c x height x width         >>> data = torch randn n  16  10  10          >>> conv = nn conv2d 16  c   3  3           >>> m = nn logsoftmax dim=1          >>>   each element in target have to have 0 <= value < c         >>> target = torch empty n  8  8  dtype=torch long  random  0  c          >>> output = loss m conv data    target          >>> output backward                 constants   =  string  string      ignore index  int      def   init   self  weight  optional tensor  = none  size average=none  ignore index  int = -100                   reduce=none  reduction  str = string  -> none          super nllloss  self    init   weight  size average  reduce  reduction          self ignore index = ignore index      def forward self  input  tensor  target  tensor  -> tensor          return f nll loss input  target  weight=self weight  ignore index=self ignore index  reduction=self reduction  
class poissonnllloss  loss       r   negative log likelihood loss with poisson distribution of target       the loss can be describe as          math           \text target  \sim \mathrm poisson  \text input            \text loss  \text input   \text target   = \text input  - \text target    \log \text input                                         \log \text target         the last term can be omit or approximate with stirling formula  the     approximation be use for target value more than 1  for target less or     equal to 1 zero be add to the loss       args          log input  bool  optional   if ``true`` the loss be compute as              math `\exp \text input   - \text target  \text input `  if ``false`` the loss be              math `\text input  - \text target  \log \text input  \text eps  `          full  bool  optional   whether to compute full loss  i  e  to add the             stirling approximation term                 math                   \text target  \log \text target   - \text target    0 5   \log 2\pi\text target            size average  bool  optional   deprecate  see  attr `reduction`   by default              the losses be average over each loss element in the batch  note that for             some losses  there be multiple elements per sample  if the field  attr `size average`             be set to ``false``  the losses be instead sum for each minibatch  ignore             when  attr `reduce` be ``false``  default  ``true``         eps  float  optional   small value to avoid evaluation of  math `\log 0 ` when              attr `log input = false`  default  1e-8         reduce  bool  optional   deprecate  see  attr `reduction`   by default  the             losses be average or sum over observations for each minibatch depend             on  attr `size average`  when  attr `reduce` be ``false``  return a loss per             batch element instead and ignore  attr `size average`  default  ``true``         reduction  string  optional   specify the reduction to apply to the output              `` none ``   `` mean ``   `` sum ``  `` none ``  no reduction will be apply              `` mean ``  the sum of the output will be divide by the number of             elements in the output  `` sum ``  the output will be sum  note   attr `size average`             and  attr `reduce` be in the process of be deprecate  and in the meantime              specify either of those two args will override  attr `reduction`  default  `` mean ``      examples            >>> loss = nn poissonnllloss           >>> log input = torch randn 5  2  require grad=true          >>> target = torch randn 5  2          >>> output = loss log input  target          >>> output backward        shape          - input   math ` n    ` where  math ` ` mean  any number of additional           dimension         - target   math ` n    `  same shape as the input         - output  scalar by default  if  attr `reduction` be `` none ``  then  math ` n    `            the same shape as the input               constants   =  string  string  string  string      log input  bool     full  bool     eps  float      def   init   self  log input  bool = true  full  bool = false  size average=none                   eps  float = 1e-8  reduce=none  reduction  str = string  -> none          super poissonnllloss  self    init   size average  reduce  reduction          self log input = log input         self full = full         self eps = eps      def forward self  log input  tensor  target  tensor  -> tensor          return f poisson nll loss log input  target  log input=self log input  full=self full                                    eps=self eps  reduction=self reduction  
class gaussiannllloss  loss       r   gaussian negative log likelihood loss       the target be treat as sample from gaussian distributions with     expectations and variances predict by the neural network  for a     ``target`` tensor model as have gaussian distribution with a tensor     of expectations ``input`` and a tensor of positive variances ``var`` the loss be          math           \text loss  = \frac 1  2 \left \log\left \text max \left \text var           \ \text eps \right \right    \frac \left \text input  - \text target \right  2           \text max \left \text var   \ \text eps \right  \right    \text const        where  attr `eps` be use for stability  by default  the constant term of     the loss function be omit unless  attr `full` be ``true``  if ``var`` be not the same     size as ``input``  due to a homoscedastic assumption   it must either have a final dimension     of 1 or have one fewer dimension  with all other size be the same  for correct broadcast       args          full  bool  optional   include the constant term in the loss             calculation  default  ``false``          eps  float  optional   value use to clamp ``var``  see note below   for             stability  default  1e-6          reduction  string  optional   specify the reduction to apply to the             output `` none ``   `` mean ``   `` sum ``  `` none ``  no reduction             will be apply  `` mean ``  the output be the average of all batch             member losses  `` sum ``  the output be the sum of all batch member             losses  default  `` mean ``       shape          - input   math ` n    ` where  math ` ` mean any number of additional           dimension         - target   math ` n    `  same shape as the input  or same shape as the input           but with one dimension equal to 1  to allow for broadcast          - var   math ` n    `  same shape as the input  or same shape as the input but           with one dimension equal to 1  or same shape as the input but with one fewer           dimension  to allow for broadcast          - output  scalar if  attr `reduction` be `` mean ``  default  or           `` sum ``  if  attr `reduction` be `` none ``  then  math ` n    `  same           shape as the input      examples           >>> loss = nn gaussiannllloss           >>> input = torch randn 5  2  require grad=true          >>> target = torch randn 5  2          >>> var = torch ones 5  2  require grad=true   heteroscedastic         >>> output = loss input  target  var          >>> output backward            >>> loss = nn gaussiannllloss           >>> input = torch randn 5  2  require grad=true          >>> target = torch randn 5  2          >>> var = torch ones 5  1  require grad=true   homoscedastic         >>> output = loss input  target  var          >>> output backward        note          the clamp of ``var`` be ignore with respect to autograd  and so the         gradients be unaffected by it       reference          nix  d  a  and weigend  a  s    estimate the mean and variance of the         target probability distribution   proceed of 1994 ieee international         conference on neural network  icnn 94   orlando  fl  usa  1994  pp  55-60         vol 1  doi  10 1109/icnn 1994 374138                constants   =  string  string  string      full  bool     eps  float      def   init   self     full  bool = false  eps  float = 1e-6  reduction  str = string  -> none          super gaussiannllloss  self    init   none  none  reduction          self full = full         self eps = eps      def forward self  input  tensor  target  tensor  var  tensor  -> tensor          return f gaussian nll loss input  target  var  full=self full  eps=self eps  reduction=self reduction  
class kldivloss  loss       r   the kullback-leibler divergence loss measure      `kullback-leibler divergence`  be a useful distance measure for continuous     distributions and be often useful when perform direct regression over     the space of  discretely sample  continuous output distributions       as with  class `~torch nn nllloss`  the `input` give be expect to contain      log-probabilities  and be not restrict to a 2d tensor      the target be interpret as  probabilities  by default  but could be consider     as  log-probabilities  with  attr `log target` set to ``true``       this criterion expect a `target` `tensor` of the same size as the     `input` `tensor`       the unreduced  i e  with  attr `reduction` set to `` none ``  loss can be describe as          math           l x y  = l = \  l 1 \dots l n \   \quad         l n = y n \cdot \left  \log y n - x n \right       where the index  math `n` span all dimension of ``input`` and  math `l` have the same     shape as ``input``  if  attr `reduction` be not `` none ``  default `` mean ``   then          math           \ell x  y  = \begin case              \operatorname mean  l     \text if reduction  = \text `mean    \\             \operatorname sum  l      \text if reduction  = \text `sum            \end case       in default  attr `reduction` mode `` mean ``  the losses be average for each minibatch over observations       as well as   over dimension  `` batchmean `` mode give the correct kl divergence where losses     be average over batch dimension only  `` mean `` mode s behavior will be change to the same as     `` batchmean `` in the next major release           `kullback-leibler divergence`  https //en wikipedia org/wiki/kullback-leibler divergence      args          size average  bool  optional   deprecate  see  attr `reduction`   by default              the losses be average over each loss element in the batch  note that for             some losses  there be multiple elements per sample  if the field  attr `size average`             be set to ``false``  the losses be instead sum for each minibatch  ignore             when  attr `reduce` be ``false``  default  ``true``         reduce  bool  optional   deprecate  see  attr `reduction`   by default  the             losses be average or sum over observations for each minibatch depend             on  attr `size average`  when  attr `reduce` be ``false``  return a loss per             batch element instead and ignore  attr `size average`  default  ``true``         reduction  string  optional   specify the reduction to apply to the output              `` none ``   `` batchmean ``   `` sum ``   `` mean ``              `` none ``  no reduction will be apply              `` batchmean ``  the sum of the output will be divide by batchsize              `` sum ``  the output will be sum              `` mean ``  the output will be divide by the number of elements in the output              default  `` mean ``         log target  bool  optional   specify whether `target` be pass in the log space              default  ``false``         note            attr `size average` and  attr `reduce` be in the process of be deprecate          and in the meantime  specify either of those two args will override  attr `reduction`          note            attr `reduction` = `` mean `` doesn t return the true kl divergence value  please use          attr `reduction` = `` batchmean `` which align with kl math definition          in the next major release  `` mean `` will be change to be the same as `` batchmean ``       shape          - input   math ` n    ` where  math ` ` mean  any number of additional           dimension         - target   math ` n    `  same shape as the input         - output  scalar by default  if  attr ``reduction`` be `` none ``  then  math ` n    `            the same shape as the input                constants   =  string       def   init   self  size average=none  reduce=none  reduction  str = string  log target  bool = false  -> none          super kldivloss  self    init   size average  reduce  reduction          self log target = log target      def forward self  input  tensor  target  tensor  -> tensor          return f kl div input  target  reduction=self reduction  log target=self log target  
class bceloss  weightedloss       r   create a criterion that measure the binary cross entropy     between the target and the output       the unreduced  i e  with  attr `reduction` set to `` none ``  loss can be describe as          math           \ell x  y  = l = \ l 1 \dots l n\  \top  \quad         l n = - w n \left  y n \cdot \log x n    1 - y n  \cdot \log  1 - x n  \right        where  math `n` be the batch size  if  attr `reduction` be not `` none ``      default `` mean ``   then         math           \ell x  y  = \begin case              \operatorname mean  l     \text if reduction  = \text `mean   \\             \operatorname sum  l      \text if reduction  = \text `sum            \end case       this be use for measure the error of a reconstruction in for example     an auto-encoder  note that the target  math `y` should be number     between 0 and 1       notice that if  math `x n` be either 0 or 1  one of the log term would be     mathematically undefined in the above loss equation  pytorch choose to set      math `\log  0  = -\infty`  since  math `\lim  x\to 0  \log  x  = -\infty`      however  an infinite term in the loss equation be not desirable for several reason       for one  if either  math `y n = 0` or  math ` 1 - y n  = 0`  then we would be     multiply 0 with infinity  secondly  if we have an infinite loss value  then     we would also have an infinite term in our gradient  since      math `\lim  x\to 0  \frac d  dx  \log  x  = \infty`      this would make bceloss s backward method nonlinear with respect to  math `x n`      and use it for things like linear regression would not be straight-forward       our solution be that bceloss clamp its log function output to be greater than     or equal to -100  this way  we can always have a finite loss value and a linear     backward method        args          weight  tensor  optional   a manual rescale weight give to the loss             of each batch element  if give  have to be a tensor of size `nbatch`          size average  bool  optional   deprecate  see  attr `reduction`   by default              the losses be average over each loss element in the batch  note that for             some losses  there be multiple elements per sample  if the field  attr `size average`             be set to ``false``  the losses be instead sum for each minibatch  ignore             when  attr `reduce` be ``false``  default  ``true``         reduce  bool  optional   deprecate  see  attr `reduction`   by default  the             losses be average or sum over observations for each minibatch depend             on  attr `size average`  when  attr `reduce` be ``false``  return a loss per             batch element instead and ignore  attr `size average`  default  ``true``         reduction  string  optional   specify the reduction to apply to the output              `` none ``   `` mean ``   `` sum ``  `` none ``  no reduction will be apply              `` mean ``  the sum of the output will be divide by the number of             elements in the output  `` sum ``  the output will be sum  note   attr `size average`             and  attr `reduce` be in the process of be deprecate  and in the meantime              specify either of those two args will override  attr `reduction`  default  `` mean ``      shape          - input   math ` n    ` where  math ` ` mean  any number of additional           dimension         - target   math ` n    `  same shape as the input         - output  scalar  if  attr `reduction` be `` none ``  then  math ` n    `  same           shape as input       examples            >>> m = nn sigmoid           >>> loss = nn bceloss           >>> input = torch randn 3  require grad=true          >>> target = torch empty 3  random  2          >>> output = loss m input   target          >>> output backward                 constants   =  string       def   init   self  weight  optional tensor  = none  size average=none  reduce=none  reduction  str = string  -> none          super bceloss  self    init   weight  size average  reduce  reduction       def forward self  input  tensor  target  tensor  -> tensor          return f binary cross entropy input  target  weight=self weight  reduction=self reduction  
class bcewithlogitsloss  loss       r   this loss combine a `sigmoid` layer and the `bceloss` in one single     class  this version be more numerically stable than use a plain `sigmoid`     follow by a `bceloss` as  by combine the operations into one layer      we take advantage of the log-sum-exp trick for numerical stability       the unreduced  i e  with  attr `reduction` set to `` none ``  loss can be describe as          math           \ell x  y  = l = \ l 1 \dots l n\  \top  \quad         l n = - w n \left  y n \cdot \log \sigma x n             1 - y n  \cdot \log  1 - \sigma x n   \right        where  math `n` be the batch size  if  attr `reduction` be not `` none ``      default `` mean ``   then         math           \ell x  y  = \begin case              \operatorname mean  l     \text if reduction  = \text `mean   \\             \operatorname sum  l      \text if reduction  = \text `sum            \end case       this be use for measure the error of a reconstruction in for example     an auto-encoder  note that the target `t i ` should be number     between 0 and 1       it s possible to trade off recall and precision by add weight to positive examples      in the case of multi-label classification the loss can be describe as          math           \ell c x  y  = l c = \ l  1 c  \dots l  n c \  \top  \quad         l  n c  = - w  n c  \left  p c y  n c  \cdot \log \sigma x  n c              1 - y  n c   \cdot \log  1 - \sigma x  n c    \right        where  math `c` be the class number   math `c > 1` for multi-label binary classification       math `c = 1` for single-label binary classification        math `n` be the number of the sample in the batch and      math `p c` be the weight of the positive answer for the class  math `c`        math `p c > 1` increase the recall   math `p c < 1` increase the precision       for example  if a dataset contain 100 positive and 300 negative examples of a single class      then `pos weight` for the class should be equal to  math `\frac 300  100 =3`      the loss would act as if the dataset contain  math `3\times 100=300` positive examples       examples            >>> target = torch ones  10  64   dtype=torch float32     64 class  batch size = 10         >>> output = torch full  10  64   1 5     a prediction  logit          >>> pos weight = torch ones  64      all weight be equal to 1         >>> criterion = torch nn bcewithlogitsloss pos weight=pos weight          >>> criterion output  target     -log sigmoid 1 5           tensor 0 2014       args          weight  tensor  optional   a manual rescale weight give to the loss             of each batch element  if give  have to be a tensor of size `nbatch`          size average  bool  optional   deprecate  see  attr `reduction`   by default              the losses be average over each loss element in the batch  note that for             some losses  there be multiple elements per sample  if the field  attr `size average`             be set to ``false``  the losses be instead sum for each minibatch  ignore             when  attr `reduce` be ``false``  default  ``true``         reduce  bool  optional   deprecate  see  attr `reduction`   by default  the             losses be average or sum over observations for each minibatch depend             on  attr `size average`  when  attr `reduce` be ``false``  return a loss per             batch element instead and ignore  attr `size average`  default  ``true``         reduction  string  optional   specify the reduction to apply to the output              `` none ``   `` mean ``   `` sum ``  `` none ``  no reduction will be apply              `` mean ``  the sum of the output will be divide by the number of             elements in the output  `` sum ``  the output will be sum  note   attr `size average`             and  attr `reduce` be in the process of be deprecate  and in the meantime              specify either of those two args will override  attr `reduction`  default  `` mean ``         pos weight  tensor  optional   a weight of positive examples                  must be a vector with length equal to the number of class       shape          - input   math ` n    ` where  math ` ` mean  any number of additional dimension         - target   math ` n    `  same shape as the input         - output  scalar  if  attr `reduction` be `` none ``  then  math ` n    `  same           shape as input        examples            >>> loss = nn bcewithlogitsloss           >>> input = torch randn 3  require grad=true          >>> target = torch empty 3  random  2          >>> output = loss input  target          >>> output backward               def   init   self  weight  optional tensor  = none  size average=none  reduce=none  reduction  str = string                   pos weight  optional tensor  = none  -> none          super bcewithlogitsloss  self    init   size average  reduce  reduction          self register buffer string  weight          self register buffer string  pos weight          self weight  optional tensor          self pos weight  optional tensor       def forward self  input  tensor  target  tensor  -> tensor          return f binary cross entropy with logits input  target                                                    self weight                                                    pos weight=self pos weight                                                    reduction=self reduction  
class marginrankingloss  loss       r   create a criterion that measure the loss give     input  math `x1`   math `x2`  two 1d mini-batch `tensors`      and a label 1d mini-batch tensor  math `y`  contain 1 or -1        if  math `y = 1` then it assume the first input should be rank higher      have a larger value  than the second input  and vice-versa for  math `y = -1`       the loss function for each pair of sample in the mini-batch be          math           \text loss  x1  x2  y  = \max 0  -y    x1 - x2    \text margin        args          margin  float  optional   have a default value of  math `0`          size average  bool  optional   deprecate  see  attr `reduction`   by default              the losses be average over each loss element in the batch  note that for             some losses  there be multiple elements per sample  if the field  attr `size average`             be set to ``false``  the losses be instead sum for each minibatch  ignore             when  attr `reduce` be ``false``  default  ``true``         reduce  bool  optional   deprecate  see  attr `reduction`   by default  the             losses be average or sum over observations for each minibatch depend             on  attr `size average`  when  attr `reduce` be ``false``  return a loss per             batch element instead and ignore  attr `size average`  default  ``true``         reduction  string  optional   specify the reduction to apply to the output              `` none ``   `` mean ``   `` sum ``  `` none ``  no reduction will be apply              `` mean ``  the sum of the output will be divide by the number of             elements in the output  `` sum ``  the output will be sum  note   attr `size average`             and  attr `reduce` be in the process of be deprecate  and in the meantime              specify either of those two args will override  attr `reduction`  default  `` mean ``      shape          - input1   math ` n ` where `n` be the batch size          - input2   math ` n `  same shape as the input1          - target   math ` n `  same shape as the input          - output  scalar  if  attr `reduction` be `` none ``  then  math ` n `       examples            >>> loss = nn marginrankingloss           >>> input1 = torch randn 3  require grad=true          >>> input2 = torch randn 3  require grad=true          >>> target = torch randn 3  sign           >>> output = loss input1  input2  target          >>> output backward                 constants   =  string  string      margin  float      def   init   self  margin  float = 0   size average=none  reduce=none  reduction  str = string  -> none          super marginrankingloss  self    init   size average  reduce  reduction          self margin = margin      def forward self  input1  tensor  input2  tensor  target  tensor  -> tensor          return f margin rank loss input1  input2  target  margin=self margin  reduction=self reduction  
class hingeembeddingloss  loss       r   measure the loss give an input tensor  math `x` and a label tensor  math `y`      contain 1 or -1       this be usually use for measure whether two input be similar or     dissimilar  e g  use the l1 pairwise distance as  math `x`  and be typically     use for learn nonlinear embeddings or semi-supervised learn       the loss function for  math `n`-th sample in the mini-batch be         math           l n = \begin case              x n    \text if \  y n = 1 \\             \max \ 0  \delta - x n\     \text if \  y n = -1          \end case       and the total loss function be         math           \ell x  y  = \begin case              \operatorname mean  l     \text if reduction  = \text `mean   \\             \operatorname sum  l      \text if reduction  = \text `sum            \end case       where  math `l = \ l 1 \dots l n\  \top`       args          margin  float  optional   have a default value of `1`          size average  bool  optional   deprecate  see  attr `reduction`   by default              the losses be average over each loss element in the batch  note that for             some losses  there be multiple elements per sample  if the field  attr `size average`             be set to ``false``  the losses be instead sum for each minibatch  ignore             when  attr `reduce` be ``false``  default  ``true``         reduce  bool  optional   deprecate  see  attr `reduction`   by default  the             losses be average or sum over observations for each minibatch depend             on  attr `size average`  when  attr `reduce` be ``false``  return a loss per             batch element instead and ignore  attr `size average`  default  ``true``         reduction  string  optional   specify the reduction to apply to the output              `` none ``   `` mean ``   `` sum ``  `` none ``  no reduction will be apply              `` mean ``  the sum of the output will be divide by the number of             elements in the output  `` sum ``  the output will be sum  note   attr `size average`             and  attr `reduce` be in the process of be deprecate  and in the meantime              specify either of those two args will override  attr `reduction`  default  `` mean ``      shape          - input   math `   ` where  math ` ` mean  any number of dimension  the sum operation           operate over all the elements          - target   math `   `  same shape as the input         - output  scalar  if  attr `reduction` be `` none ``  then same shape as the input               constants   =  string  string      margin  float      def   init   self  margin  float = 1 0  size average=none  reduce=none  reduction  str = string  -> none          super hingeembeddingloss  self    init   size average  reduce  reduction          self margin = margin      def forward self  input  tensor  target  tensor  -> tensor          return f hinge embed loss input  target  margin=self margin  reduction=self reduction  
class multilabelmarginloss  loss       r   create a criterion that optimize a multi-class multi-classification     hinge loss  margin-based loss  between input  math `x`  a 2d mini-batch `tensor`      and output  math `y`  which be a 2d `tensor` of target class indices       for each sample in the mini-batch          math           \text loss  x  y  = \sum  ij \frac \max 0  1 -  x y j   - x i     \text x size  0        where  math `x \in \left\ 0  \  \cdots   \  \text x size  0  - 1\right\ `  \      math `y \in \left\ 0  \  \cdots   \  \text y size  0  - 1\right\ `  \      math `0 \leq y j  \leq \text x size  0 -1`  \     and  math `i \neq y j ` for all  math `i` and  math `j`        math `y` and  math `x` must have the same size       the criterion only consider a contiguous block of non-negative target that     start at the front       this allow for different sample to have variable amount of target class       args          size average  bool  optional   deprecate  see  attr `reduction`   by default              the losses be average over each loss element in the batch  note that for             some losses  there be multiple elements per sample  if the field  attr `size average`             be set to ``false``  the losses be instead sum for each minibatch  ignore             when  attr `reduce` be ``false``  default  ``true``         reduce  bool  optional   deprecate  see  attr `reduction`   by default  the             losses be average or sum over observations for each minibatch depend             on  attr `size average`  when  attr `reduce` be ``false``  return a loss per             batch element instead and ignore  attr `size average`  default  ``true``         reduction  string  optional   specify the reduction to apply to the output              `` none ``   `` mean ``   `` sum ``  `` none ``  no reduction will be apply              `` mean ``  the sum of the output will be divide by the number of             elements in the output  `` sum ``  the output will be sum  note   attr `size average`             and  attr `reduce` be in the process of be deprecate  and in the meantime              specify either of those two args will override  attr `reduction`  default  `` mean ``      shape          - input   math ` c ` or  math ` n  c ` where `n` be the batch size and `c`           be the number of class          - target   math ` c ` or  math ` n  c `  label target pad by -1 ensure same shape as the input          - output  scalar  if  attr `reduction` be `` none ``  then  math ` n `       examples            >>> loss = nn multilabelmarginloss           >>> x = torch floattensor   0 1  0 2  0 4  0 8            >>>   for target y  only consider label 3 and 0  not after label -1         >>> y = torch longtensor   3  0  -1  1            >>> loss x  y          >>>   0 25     1- 0 1-0 2      1- 0 1-0 4      1- 0 8-0 2      1- 0 8-0 4            tensor 0 8500                 constants   =  string       def   init   self  size average=none  reduce=none  reduction  str = string  -> none          super multilabelmarginloss  self    init   size average  reduce  reduction       def forward self  input  tensor  target  tensor  -> tensor          return f multilabel margin loss input  target  reduction=self reduction  
class huberloss  loss       r   create a criterion that use a square term if the absolute     element-wise error fall below delta and a delta-scaled l1 term otherwise      this loss combine advantage of both  class `l1loss` and  class `mseloss`  the     delta-scaled l1 region make the loss less sensitive to outliers than  class `mseloss`      while the l2 region provide smoothness over  class `l1loss` near 0  see     `huber loss <https //en wikipedia org/wiki/huber loss>`  for more information       for a batch of size  math `n`  the unreduced loss can be describe as          math           \ell x  y  = l = \ l 1       l n\  t      with         math           l n = \begin case          0 5  x n - y n  2    \text if    x n - y n  < delta \\         delta     x n - y n  - 0 5   delta     \text otherwise           \end case       if `reduction` be not `none`  then          math           \ell x  y  =         \begin case              \operatorname mean  l      \text if reduction  = \text `mean   \\             \operatorname sum  l       \text if reduction  = \text `sum            \end case          note           when delta be set to 1  this loss be equivalent to  class `smoothl1loss`          in general  this loss differ from  class `smoothl1loss` by a factor of delta  aka beta         in smooth l1           see  class `smoothl1loss` for additional discussion on the differences in behavior         between the two losses       args          reduction  string  optional   specify the reduction to apply to the output              `` none ``   `` mean ``   `` sum ``  `` none ``  no reduction will be apply              `` mean ``  the sum of the output will be divide by the number of             elements in the output  `` sum ``  the output will be sum  default  `` mean ``         delta  float  optional   specify the threshold at which to change between delta-scaled l1 and l2 loss              the value must be positive   default  1 0      shape          - input   math ` n    ` where  math ` ` mean any number of additional dimension         - target   math ` n    `  same shape as the input         - output  scalar  if  attr `reduction` be `` none ``  then  math ` n    `  same shape as the input               constants   =  string  string       def   init   self  reduction  str = string  delta  float = 1 0  -> none          super     init   reduction=reduction          self delta = delta      def forward self  input  tensor  target  tensor  -> tensor          return f huber loss input  target  reduction=self reduction  delta=self delta  
class smoothl1loss  loss       r   create a criterion that use a square term if the absolute     element-wise error fall below beta and an l1 term otherwise      it be less sensitive to outliers than  class `torch nn mseloss` and in some case     prevent explode gradients  e g  see the paper `fast r-cnn`  by ross girshick        for a batch of size  math `n`  the unreduced loss can be describe as          math           \ell x  y  = l = \ l 1       l n\  t      with         math           l n = \begin case          0 5  x n - y n  2 / beta    \text if    x n - y n  < beta \\          x n - y n  - 0 5   beta    \text otherwise           \end case       if `reduction` be not `none`  then          math           \ell x  y  =         \begin case              \operatorname mean  l      \text if reduction  = \text `mean   \\             \operatorname sum  l       \text if reduction  = \text `sum            \end case          note           smooth l1 loss can be see as exactly  class `l1loss`  but with the  math ` x - y  < beta`         portion replace with a quadratic function such that its slope be 1 at  math ` x - y  = beta`          the quadratic segment smooth the l1 loss near  math ` x - y  = 0`          note           smooth l1 loss be closely relate to  class `huberloss`  be         equivalent to  math `huber x  y  / beta`  note that smooth l1 s beta hyper-parameter be         also know as delta for huber   this lead to the follow differences             as beta -> 0  smooth l1 loss converge to  class `l1loss`  while  class `huberloss`           converge to a constant 0 loss            as beta ->  math ` \infty`  smooth l1 loss converge to a constant 0 loss  while            class `huberloss` converge to  class `mseloss`            for smooth l1 loss  as beta vary  the l1 segment of the loss have a constant slope of 1            for  class `huberloss`  the slope of the l1 segment be beta           `fast r-cnn`  https //arxiv org/abs/1504 08083      args          size average  bool  optional   deprecate  see  attr `reduction`   by default              the losses be average over each loss element in the batch  note that for             some losses  there be multiple elements per sample  if the field  attr `size average`             be set to ``false``  the losses be instead sum for each minibatch  ignore             when  attr `reduce` be ``false``  default  ``true``         reduce  bool  optional   deprecate  see  attr `reduction`   by default  the             losses be average or sum over observations for each minibatch depend             on  attr `size average`  when  attr `reduce` be ``false``  return a loss per             batch element instead and ignore  attr `size average`  default  ``true``         reduction  string  optional   specify the reduction to apply to the output              `` none ``   `` mean ``   `` sum ``  `` none ``  no reduction will be apply              `` mean ``  the sum of the output will be divide by the number of             elements in the output  `` sum ``  the output will be sum  note   attr `size average`             and  attr `reduce` be in the process of be deprecate  and in the meantime              specify either of those two args will override  attr `reduction`  default  `` mean ``         beta  float  optional   specify the threshold at which to change between l1 and l2 loss              the value must be non-negative  default  1 0      shape          - input   math ` n    ` where  math ` ` mean any number of additional dimension         - target   math ` n    `  same shape as the input         - output  scalar  if  attr `reduction` be `` none ``  then  math ` n    `  same shape as the input               constants   =  string       def   init   self  size average=none  reduce=none  reduction  str = string  beta  float = 1 0  -> none          super smoothl1loss  self    init   size average  reduce  reduction          self beta = beta      def forward self  input  tensor  target  tensor  -> tensor          return f smooth l1 loss input  target  reduction=self reduction  beta=self beta  
class softmarginloss  loss       r   create a criterion that optimize a two-class classification     logistic loss between input tensor  math `x` and target tensor  math `y`      contain 1 or -1           math           \text loss  x  y  = \sum i \frac \log 1   \exp -y i  x i     \text x nelement          args          size average  bool  optional   deprecate  see  attr `reduction`   by default              the losses be average over each loss element in the batch  note that for             some losses  there be multiple elements per sample  if the field  attr `size average`             be set to ``false``  the losses be instead sum for each minibatch  ignore             when  attr `reduce` be ``false``  default  ``true``         reduce  bool  optional   deprecate  see  attr `reduction`   by default  the             losses be average or sum over observations for each minibatch depend             on  attr `size average`  when  attr `reduce` be ``false``  return a loss per             batch element instead and ignore  attr `size average`  default  ``true``         reduction  string  optional   specify the reduction to apply to the output              `` none ``   `` mean ``   `` sum ``  `` none ``  no reduction will be apply              `` mean ``  the sum of the output will be divide by the number of             elements in the output  `` sum ``  the output will be sum  note   attr `size average`             and  attr `reduce` be in the process of be deprecate  and in the meantime              specify either of those two args will override  attr `reduction`  default  `` mean ``      shape          - input   math `   ` where  math ` ` mean  any number of additional           dimension         - target   math `   `  same shape as the input         - output  scalar  if  attr `reduction` be `` none ``  then same shape as the input                constants   =  string       def   init   self  size average=none  reduce=none  reduction  str = string  -> none          super softmarginloss  self    init   size average  reduce  reduction       def forward self  input  tensor  target  tensor  -> tensor          return f soft margin loss input  target  reduction=self reduction  
class multilabelsoftmarginloss  weightedloss       r   create a criterion that optimize a multi-label one-versus-all     loss base on max-entropy  between input  math `x` and target  math `y` of size      math ` n  c `      for each sample in the minibatch          math           loss x  y  = - \frac 1  c    \sum i y i    \log  1   \exp -x i     -1                               1-y i     \log\left \frac \exp -x i     1   \exp -x i    \right       where  math `i \in \left\ 0  \  \cdots   \  \text x nelement    - 1\right\ `       math `y i  \in \left\ 0  \  1\right\ `       args          weight  tensor  optional   a manual rescale weight give to each             class  if give  it have to be a tensor of size `c`  otherwise  it be             treat as if have all ones          size average  bool  optional   deprecate  see  attr `reduction`   by default              the losses be average over each loss element in the batch  note that for             some losses  there be multiple elements per sample  if the field  attr `size average`             be set to ``false``  the losses be instead sum for each minibatch  ignore             when  attr `reduce` be ``false``  default  ``true``         reduce  bool  optional   deprecate  see  attr `reduction`   by default  the             losses be average or sum over observations for each minibatch depend             on  attr `size average`  when  attr `reduce` be ``false``  return a loss per             batch element instead and ignore  attr `size average`  default  ``true``         reduction  string  optional   specify the reduction to apply to the output              `` none ``   `` mean ``   `` sum ``  `` none ``  no reduction will be apply              `` mean ``  the sum of the output will be divide by the number of             elements in the output  `` sum ``  the output will be sum  note   attr `size average`             and  attr `reduce` be in the process of be deprecate  and in the meantime              specify either of those two args will override  attr `reduction`  default  `` mean ``      shape          - input   math ` n  c ` where `n` be the batch size and `c` be the number of class          - target   math ` n  c `  label target pad by -1 ensure same shape as the input          - output  scalar  if  attr `reduction` be `` none ``  then  math ` n `                constants   =  string       def   init   self  weight  optional tensor  = none  size average=none  reduce=none  reduction  str = string  -> none          super multilabelsoftmarginloss  self    init   weight  size average  reduce  reduction       def forward self  input  tensor  target  tensor  -> tensor          return f multilabel soft margin loss input  target  weight=self weight  reduction=self reduction  
class cosineembeddingloss  loss       r   create a criterion that measure the loss give input tensors      math `x 1`   math `x 2` and a `tensor` label  math `y` with value 1 or -1      this be use for measure whether two input be similar or dissimilar      use the cosine distance  and be typically use for learn nonlinear     embeddings or semi-supervised learn       the loss function for each sample be          math           \text loss  x  y  =         \begin case          1 - \cos x 1  x 2     \text if   y = 1 \\         \max 0  \cos x 1  x 2  - \text margin      \text if   y = -1         \end case       args          margin  float  optional   should be a number from  math `-1` to  math `1`               math `0` to  math `0 5` be suggest  if  attr `margin` be miss  the             default value be  math `0`          size average  bool  optional   deprecate  see  attr `reduction`   by default              the losses be average over each loss element in the batch  note that for             some losses  there be multiple elements per sample  if the field  attr `size average`             be set to ``false``  the losses be instead sum for each minibatch  ignore             when  attr `reduce` be ``false``  default  ``true``         reduce  bool  optional   deprecate  see  attr `reduction`   by default  the             losses be average or sum over observations for each minibatch depend             on  attr `size average`  when  attr `reduce` be ``false``  return a loss per             batch element instead and ignore  attr `size average`  default  ``true``         reduction  string  optional   specify the reduction to apply to the output              `` none ``   `` mean ``   `` sum ``  `` none ``  no reduction will be apply              `` mean ``  the sum of the output will be divide by the number of             elements in the output  `` sum ``  the output will be sum  note   attr `size average`             and  attr `reduce` be in the process of be deprecate  and in the meantime              specify either of those two args will override  attr `reduction`  default  `` mean ``      shape          - input1   math ` n  d `  where `n` be the batch size and `d` be the embed dimension          - input2   math ` n  d `  same shape as input1          - target   math ` n `          - output  if  attr `reduction` be `` none ``  then  math ` n `  otherwise scalar                constants   =  string  string      margin  float      def   init   self  margin  float = 0   size average=none  reduce=none  reduction  str = string  -> none          super cosineembeddingloss  self    init   size average  reduce  reduction          self margin = margin      def forward self  input1  tensor  input2  tensor  target  tensor  -> tensor          return f cosine embed loss input1  input2  target  margin=self margin  reduction=self reduction  
class multimarginloss  weightedloss       r   create a criterion that optimize a multi-class classification hinge     loss  margin-based loss  between input  math `x`  a 2d mini-batch `tensor`  and     output  math `y`  which be a 1d tensor of target class indices       math `0 \leq y \leq \text x size  1 -1`        for each mini-batch sample  the loss in term of the 1d input  math `x` and scalar     output  math `y` be          math           \text loss  x  y  = \frac \sum i \max 0  \text margin  - x y    x i    p  \text x size  0        where  math `x \in \left\ 0  \  \cdots   \  \text x size  0  - 1\right\ `     and  math `i \neq y`       optionally  you can give non-equal weight on the class by pass     a 1d  attr `weight` tensor into the constructor       the loss function then become          math           \text loss  x  y  = \frac \sum i \max 0  w y     \text margin  - x y    x i    p   \text x size  0        args          p  int  optional   have a default value of  math `1`   math `1` and  math `2`             be the only support value          margin  float  optional   have a default value of  math `1`          weight  tensor  optional   a manual rescale weight give to each             class  if give  it have to be a tensor of size `c`  otherwise  it be             treat as if have all ones          size average  bool  optional   deprecate  see  attr `reduction`   by default              the losses be average over each loss element in the batch  note that for             some losses  there be multiple elements per sample  if the field  attr `size average`             be set to ``false``  the losses be instead sum for each minibatch  ignore             when  attr `reduce` be ``false``  default  ``true``         reduce  bool  optional   deprecate  see  attr `reduction`   by default  the             losses be average or sum over observations for each minibatch depend             on  attr `size average`  when  attr `reduce` be ``false``  return a loss per             batch element instead and ignore  attr `size average`  default  ``true``         reduction  string  optional   specify the reduction to apply to the output              `` none ``   `` mean ``   `` sum ``  `` none ``  no reduction will be apply              `` mean ``  the sum of the output will be divide by the number of             elements in the output  `` sum ``  the output will be sum  note   attr `size average`             and  attr `reduce` be in the process of be deprecate  and in the meantime              specify either of those two args will override  attr `reduction`  default  `` mean ``               constants   =  string  string  string      margin  float     p  int      def   init   self  p  int = 1  margin  float = 1   weight  optional tensor  = none  size average=none                   reduce=none  reduction  str = string  -> none          super multimarginloss  self    init   weight  size average  reduce  reduction          if p  = 1 and p  = 2              raise valueerror string          assert weight be none or weight dim   == 1         self p = p         self margin = margin      def forward self  input  tensor  target  tensor  -> tensor          return f multi margin loss input  target  p=self p  margin=self margin                                     weight=self weight  reduction=self reduction  
class tripletmarginloss  loss       r   create a criterion that measure the triplet loss give an input     tensors  math `x1`   math `x2`   math `x3` and a margin with a value greater than  math `0`      this be use for measure a relative similarity between sample  a triplet     be compose by `a`  `p` and `n`  i e   `anchor`  `positive examples` and `negative     examples` respectively   the shape of all input tensors should be      math ` n  d `       the distance swap be describe in detail in the paper `learning shallow     convolutional feature descriptors with triplet losses`  by     v  balntas  e  riba et al       the loss function for each sample in the mini-batch be          math           l a  p  n  = \max \ d a i  p i  - d a i  n i     \rm margin   0\        where         math           d x i  y i  = \left\lvert  \bf x  i -  \bf y  i \right\rvert p      see also  class `~torch nn tripletmarginwithdistanceloss`  which compute the     triplet margin loss for input tensors use a custom distance function       args          margin  float  optional   default   math `1`          p  int  optional   the norm degree for pairwise distance  default   math `2`          swap  bool  optional   the distance swap be describe in detail in the paper             `learning shallow convolutional feature descriptors with triplet losses` by             v  balntas  e  riba et al  default  ``false``          size average  bool  optional   deprecate  see  attr `reduction`   by default              the losses be average over each loss element in the batch  note that for             some losses  there be multiple elements per sample  if the field  attr `size average`             be set to ``false``  the losses be instead sum for each minibatch  ignore             when  attr `reduce` be ``false``  default  ``true``         reduce  bool  optional   deprecate  see  attr `reduction`   by default  the             losses be average or sum over observations for each minibatch depend             on  attr `size average`  when  attr `reduce` be ``false``  return a loss per             batch element instead and ignore  attr `size average`  default  ``true``         reduction  string  optional   specify the reduction to apply to the output              `` none ``   `` mean ``   `` sum ``  `` none ``  no reduction will be apply              `` mean ``  the sum of the output will be divide by the number of             elements in the output  `` sum ``  the output will be sum  note   attr `size average`             and  attr `reduce` be in the process of be deprecate  and in the meantime              specify either of those two args will override  attr `reduction`  default  `` mean ``      shape          - input   math ` n  d ` where  math `d` be the vector dimension          - output  a tensor of shape  math ` n ` if  attr `reduction` be `` none ``  or a scalar           otherwise       examples        >>> triplet loss = nn tripletmarginloss margin=1 0  p=2      >>> anchor = torch randn 100  128  require grad=true      >>> positive = torch randn 100  128  require grad=true      >>> negative = torch randn 100  128  require grad=true      >>> output = triplet loss anchor  positive  negative      >>> output backward            learn shallow convolutional feature descriptors with triplet losses          http //www bmva org/bmvc/2016/papers/paper119/index html               constants   =  string  string  string  string  string      margin  float     p  float     eps  float     swap  bool      def   init   self  margin  float = 1 0  p  float = 2   eps  float = 1e-6  swap  bool = false  size average=none                   reduce=none  reduction  str = string           super tripletmarginloss  self    init   size average  reduce  reduction          self margin = margin         self p = p         self eps = eps         self swap = swap      def forward self  anchor  tensor  positive  tensor  negative  tensor  -> tensor          return f triplet margin loss anchor  positive  negative  margin=self margin  p=self p                                       eps=self eps  swap=self swap  reduction=self reduction  
class tripletmarginwithdistanceloss  loss       r   create a criterion that measure the triplet loss give input     tensors  math `a`   math `p`  and  math `n`  represent anchor      positive  and negative examples  respectively   and a nonnegative      real-valued function   distance function   use to compute the relationship     between the anchor and positive example   positive distance   and the     anchor and negative example   negative distance         the unreduced loss  i e   with  attr `reduction` set to `` none ``      can be describe as          math           \ell a  p  n  = l = \ l 1 \dots l n\  \top  \quad         l i = \max \ d a i  p i  - d a i  n i     \rm margin   0\       where  math `n` be the batch size   math `d` be a nonnegative  real-valued function     quantify the closeness of two tensors  refer to as the  attr `distance function`      and  math `margin` be a nonnegative margin represent the minimum difference     between the positive and negative distance that be require for the loss to     be 0   the input tensors have  math `n` elements each and can be of any shape     that the distance function can handle       if  attr `reduction` be not `` none ``      default `` mean ``   then          math           \ell x  y  =         \begin case              \operatorname mean  l      \text if reduction  = \text `mean   \\             \operatorname sum  l       \text if reduction  = \text `sum            \end case       see also  class `~torch nn tripletmarginloss`  which compute the triplet     loss for input tensors use the  math `l p` distance as the distance function       args          distance function  callable  optional   a nonnegative  real-valued function that             quantify the closeness of two tensors  if not specify              `nn pairwisedistance` will be use   default  ``none``         margin  float  optional   a nonnegative margin represent the minimum difference             between the positive and negative distance require for the loss to be 0  larger             margins penalize case where the negative examples be not distant enough from the             anchor  relative to the positives  default   math `1`          swap  bool  optional   whether to use the distance swap describe in the paper             `learning shallow convolutional feature descriptors with triplet losses` by             v  balntas  e  riba et al  if true  and if the positive example be closer to the             negative example than the anchor be  swap the positive example and the anchor in             the loss computation  default  ``false``          reduction  string  optional   specify the  optional  reduction to apply to the output              `` none ``   `` mean ``   `` sum ``  `` none ``  no reduction will be apply              `` mean ``  the sum of the output will be divide by the number of             elements in the output  `` sum ``  the output will be sum  default  `` mean ``       shape          - input   math ` n    ` where  math ` ` represent any number of additional dimension           as support by the distance function          - output  a tensor of shape  math ` n ` if  attr `reduction` be `` none ``  or a scalar           otherwise       examples        >>>   initialize embeddings     >>> embed = nn embed 1000  128      >>> anchor ids = torch randint 0  1000   1        >>> positive ids = torch randint 0  1000   1        >>> negative ids = torch randint 0  1000   1        >>> anchor = embed anchor ids      >>> positive = embed positive ids      >>> negative = embed negative ids      >>>     >>>   built-in distance function     >>> triplet loss = \     >>>     nn tripletmarginwithdistanceloss distance function=nn pairwisedistance        >>> output = triplet loss anchor  positive  negative      >>> output backward       >>>     >>>   custom distance function     >>> def l infinity x1  x2       >>>     return torch max torch abs x1 - x2   dim=1  value     >>>     >>> triplet loss = \     >>>     nn tripletmarginwithdistanceloss distance function=l infinity  margin=1 5      >>> output = triplet loss anchor  positive  negative      >>> output backward       >>>     >>>   custom distance function  lambda      >>> triplet loss = \     >>>     nn tripletmarginwithdistanceloss      >>>         distance function=lambda x  y  1 0 - f cosine similarity x  y       >>> output = triplet loss anchor  positive  negative      >>> output backward        reference          v  balntas  et al   learn shallow convolutional feature descriptors with triplet losses          http //www bmva org/bmvc/2016/papers/paper119/index html               constants   =  string  string  string      margin  float     swap  bool      def   init   self     distance function  optional callable  tensor  tensor   tensor   = none                   margin  float = 1 0  swap  bool = false  reduction  str = string           super tripletmarginwithdistanceloss  self    init   size average=none  reduce=none  reduction=reduction          self distance function  optional callable  tensor  tensor   tensor   = \             distance function if distance function be not none else pairwisedistance           self margin = margin         self swap = swap      def forward self  anchor  tensor  positive  tensor  negative  tensor  -> tensor          return f triplet margin with distance loss anchor  positive  negative                                                     distance function=self distance function                                                     margin=self margin  swap=self swap  reduction=self reduction  
class pixelshuffle module       r   rearrange elements in a tensor of shape  math `    c \times r 2  h  w `     to a tensor of shape  math `    c  h \times r  w \times r `  where r be an upscale factor       this be useful for implement efficient sub-pixel convolution     with a stride of  math `1/r`       see the paper      `real-time single image and video super-resolution use an efficient sub-pixel convolutional neural network`      by shi et  al  2016  for more detail       args          upscale factor  int   factor to increase spatial resolution by      shape          - input   math `    c  in   h  in   w  in  `  where   be zero or more batch dimension         - output   math `    c  out   h  out   w  out  `  where         math           c  out  = c  in  \div \text upscale\ factor  2         math           h  out  = h  in  \times \text upscale\ factor          math           w  out  = w  in  \times \text upscale\ factor       examples            >>> pixel shuffle = nn pixelshuffle 3          >>> input = torch randn 1  9  4  4          >>> output = pixel shuffle input          >>> print output size            torch size  1  1  12  12            real-time single image and video super-resolution use an efficient sub-pixel convolutional neural network          https //arxiv org/abs/1609 05158               constants   =  string      upscale factor  int      def   init   self  upscale factor  int  -> none          super pixelshuffle  self    init             self upscale factor = upscale factor      def forward self  input  tensor  -> tensor          return f pixel shuffle input  self upscale factor       def extra repr self  -> str          return string format self upscale factor  
class pixelunshuffle module       r   reverse the  class `~torch nn pixelshuffle` operation by rearrange elements     in a tensor of shape  math `    c  h \times r  w \times r ` to a tensor of shape      math `    c \times r 2  h  w `  where r be a downscale factor       see the paper      `real-time single image and video super-resolution use an efficient sub-pixel convolutional neural network`      by shi et  al  2016  for more detail       args          downscale factor  int   factor to decrease spatial resolution by      shape          - input   math `    c  in   h  in   w  in  `  where   be zero or more batch dimension         - output   math `    c  out   h  out   w  out  `  where         math           c  out  = c  in  \times \text downscale\ factor  2         math           h  out  = h  in  \div \text downscale\ factor          math           w  out  = w  in  \div \text downscale\ factor       examples            >>> pixel unshuffle = nn pixelunshuffle 3          >>> input = torch randn 1  1  12  12          >>> output = pixel unshuffle input          >>> print output size            torch size  1  9  4  4            real-time single image and video super-resolution use an efficient sub-pixel convolutional neural network          https //arxiv org/abs/1609 05158               constants   =  string      downscale factor  int      def   init   self  downscale factor  int  -> none          super pixelunshuffle  self    init             self downscale factor = downscale factor      def forward self  input  tensor  -> tensor          return f pixel unshuffle input  self downscale factor       def extra repr self  -> str          return string format self downscale factor  
class upsample module       r   upsamples a give multi-channel 1d  temporal   2d  spatial  or 3d  volumetric  data       the input data be assume to be of the form     `minibatch x channel x  optional depth  x  optional height  x width`      hence  for spatial input  we expect a 4d tensor and for volumetric input  we expect a 5d tensor       the algorithms available for upsampling be nearest neighbor and linear      bilinear  bicubic and trilinear for 3d  4d and 5d input tensor      respectively       one can either give a  attr `scale factor` or the target output  attr `size` to     calculate the output size   you cannot give both  as it be ambiguous       args          size  int or tuple int  or tuple int  int  or tuple int  int  int   optional               output spatial size         scale factor  float or tuple float  or tuple float  float  or tuple float  float  float   optional               multiplier for spatial size  have to match input size if it be a tuple          mode  str  optional   the upsampling algorithm  one of `` nearest ``              `` linear ``  `` bilinear ``  `` bicubic `` and `` trilinear ``              default  `` nearest ``         align corner  bool  optional   if ``true``  the corner pixels of the input             and output tensors be align  and thus preserve the value at             those pixels  this only have effect when  attr `mode` be             `` linear ``  `` bilinear ``  or `` trilinear ``  default  ``false``      shape          - input   math ` n  c  w  in  `   math ` n  c  h  in   w  in  ` or  math ` n  c  d  in   h  in   w  in  `         - output   math ` n  c  w  out  `   math ` n  c  h  out   w  out  `           or  math ` n  c  d  out   h  out   w  out  `  where         math           d  out  = \left\lfloor d  in  \times \text scale\ factor  \right\rfloor         math           h  out  = \left\lfloor h  in  \times \text scale\ factor  \right\rfloor         math           w  out  = \left\lfloor w  in  \times \text scale\ factor  \right\rfloor         warn           with ``align corner = true``  the linearly interpolate modes          `linear`  `bilinear`  `bicubic`  and `trilinear`  don t proportionally         align the output and input pixels  and thus the output value can depend         on the input size  this be the default behavior for these modes up to         version 0 3 1  since then  the default behavior be         ``align corner = false``  see below for concrete examples on how this         affect the output          note           if you want downsampling/general resize  you should use  func `~nn functional interpolate`       examples            >>> input = torch arange 1  5  dtype=torch float32  view 1  1  2  2          >>> input         tensor      1    2                        3    4                >>> m = nn upsample scale factor=2  mode= nearest           >>> m input          tensor      1    1    2    2                        1    1    2    2                        3    3    4    4                        3    3    4    4                >>> m = nn upsample scale factor=2  mode= bilinear      align corners=false         >>> m input          tensor      1 0000   1 2500   1 7500   2 0000                       1 5000   1 7500   2 2500   2 5000                       2 5000   2 7500   3 2500   3 5000                       3 0000   3 2500   3 7500   4 0000               >>> m = nn upsample scale factor=2  mode= bilinear   align corners=true          >>> m input          tensor      1 0000   1 3333   1 6667   2 0000                       1 6667   2 0000   2 3333   2 6667                       2 3333   2 6667   3 0000   3 3333                       3 0000   3 3333   3 6667   4 0000               >>>   try scale the same data in a larger tensor         >>>         >>> input 3x3 = torch zero 3  3  view 1  1  3  3          >>> input 3x3        2   2  copy  input          tensor      1    2                        3    4               >>> input 3x3         tensor      1    2    0                        3    4    0                        0    0    0                >>> m = nn upsample scale factor=2  mode= bilinear      align corners=false         >>>   notice that value in top leave corner be the same with the small input  except at boundary          >>> m input 3x3          tensor      1 0000   1 2500   1 7500   1 5000   0 5000   0 0000                       1 5000   1 7500   2 2500   1 8750   0 6250   0 0000                       2 5000   2 7500   3 2500   2 6250   0 8750   0 0000                       2 2500   2 4375   2 8125   2 2500   0 7500   0 0000                       0 7500   0 8125   0 9375   0 7500   0 2500   0 0000                       0 0000   0 0000   0 0000   0 0000   0 0000   0 0000               >>> m = nn upsample scale factor=2  mode= bilinear   align corners=true          >>>   notice that value in top leave corner be now change         >>> m input 3x3          tensor      1 0000   1 4000   1 8000   1 6000   0 8000   0 0000                       1 8000   2 2000   2 6000   2 2400   1 1200   0 0000                       2 6000   3 0000   3 4000   2 8800   1 4400   0 0000                       2 4000   2 7200   3 0400   2 5600   1 2800   0 0000                       1 2000   1 3600   1 5200   1 2800   0 6400   0 0000                       0 0000   0 0000   0 0000   0 0000   0 0000   0 0000                    constants   =  string  string  string  string  string      name  str     size  optional  size any t      scale factor  optional  ratio any t      mode  str     align corner  optional bool       def   init   self  size  optional  size any t  = none  scale factor  optional  ratio any t  = none                   mode  str = string  align corner  optional bool  = none  -> none          super upsample  self    init             self name = type self    name           self size = size         if isinstance scale factor  tuple               self scale factor = tuple float factor  for factor in scale factor          else              self scale factor = float scale factor  if scale factor else none         self mode = mode         self align corner = align corner      def forward self  input  tensor  -> tensor          return f interpolate input  self size  self scale factor  self mode  self align corner       def extra repr self  -> str          if self scale factor be not none              info = string   str self scale factor          else              info = string   str self size          info  = string   self mode         return info 
class upsamplingnearest2d upsample       r   apply a 2d nearest neighbor upsampling to an input signal compose of several input     channel       to specify the scale  it take either the  attr `size` or the  attr `scale factor`     as it s constructor argument       when  attr `size` be give  it be the output size of the image ` h  w `       args          size  int or tuple int  int   optional   output spatial size         scale factor  float or tuple float  float   optional   multiplier for             spatial size          warn           this class be deprecate in favor of  func `~nn functional interpolate`       shape          - input   math ` n  c  h  in   w  in  `         - output   math ` n  c  h  out   w  out  ` where         math             h  out  = \left\lfloor h  in  \times \text scale\ factor  \right\rfloor         math             w  out  = \left\lfloor w  in  \times \text scale\ factor  \right\rfloor      examples            >>> input = torch arange 1  5  dtype=torch float32  view 1  1  2  2          >>> input         tensor      1    2                        3    4                >>> m = nn upsamplingnearest2d scale factor=2          >>> m input          tensor      1    1    2    2                        1    1    2    2                        3    3    4    4                        3    3    4    4                   def   init   self  size  optional  size 2 t  = none  scale factor  optional  ratio 2 t  = none  -> none          super upsamplingnearest2d  self    init   size  scale factor  mode=string  
class upsamplingbilinear2d upsample       r   apply a 2d bilinear upsampling to an input signal compose of several input     channel       to specify the scale  it take either the  attr `size` or the  attr `scale factor`     as it s constructor argument       when  attr `size` be give  it be the output size of the image ` h  w `       args          size  int or tuple int  int   optional   output spatial size         scale factor  float or tuple float  float   optional   multiplier for             spatial size          warn           this class be deprecate in favor of  func `~nn functional interpolate`  it be         equivalent to ``nn functional interpolate      mode= bilinear   align corners=true ``       shape          - input   math ` n  c  h  in   w  in  `         - output   math ` n  c  h  out   w  out  ` where         math           h  out  = \left\lfloor h  in  \times \text scale\ factor  \right\rfloor         math           w  out  = \left\lfloor w  in  \times \text scale\ factor  \right\rfloor      examples            >>> input = torch arange 1  5  dtype=torch float32  view 1  1  2  2          >>> input         tensor      1    2                        3    4                >>> m = nn upsamplingbilinear2d scale factor=2          >>> m input          tensor      1 0000   1 3333   1 6667   2 0000                       1 6667   2 0000   2 3333   2 6667                       2 3333   2 6667   3 0000   3 3333                       3 0000   3 3333   3 6667   4 0000                  def   init   self  size  optional  size 2 t  = none  scale factor  optional  ratio 2 t  = none  -> none          super upsamplingbilinear2d  self    init   size  scale factor  mode=string  align corners=true  
class channelshuffle module       r   divide the channel in a tensor of shape  math `    c   h  w `     into g group and rearrange them as  math `    c \frac g  g  h  w `      while keep the original tensor shape       args          group  int   number of group to divide channel in       examples            >>> channel shuffle = nn channelshuffle 2          >>> input = torch randn 1  4  2  2          >>> print input              1  2               3  4                5  6               7  8                9  10               11  12                13  14               15  16                        >>> output = channel shuffle input          >>> print output              1  2               3  4                9  10               11  12                5  6               7  8                13  14               15  16                              constants   =  string      group  int      def   init   self  group  int  -> none          super channelshuffle  self    init             self group = group      def forward self  input  tensor  -> tensor          return f channel shuffle input  self group       def extra repr self  -> str          return string format self group  
class dataparallel module       r   implement data parallelism at the module level       this container parallelize the application of the give  attr `module` by     split the input across the specify devices by chunk in the batch     dimension  other object will be copy once per device   in the forward     pass  the module be replicate on each device  and each replica handle a     portion of the input  during the backwards pass  gradients from each replica     be sum into the original module       the batch size should be larger than the number of gpus use          warn           it be recommend to use  class `~torch nn parallel distributeddataparallel`          instead of this class  to do multi-gpu train  even if there be only a single         node  see   ref `cuda-nn-ddp-instead` and  ref `ddp`       arbitrary positional and keyword input be allow to be pass into     dataparallel but some type be specially handle  tensors will be       scatter   on dim specify  default 0   tuple  list and dict type will     be shallow copy  the other type will be share among different thread     and can be corrupt if write to in the model s forward pass       the parallelize  attr `module` must have its parameters and buffer on     ``device ids 0 `` before run this  class `~torch nn dataparallel`     module          warn           in each forward   attr `module` be   replicate   on each device  so any         update to the run module in ``forward`` will be lose  for example          if  attr `module` have a counter attribute that be incremented in each         ``forward``  it will always stay at the initial value because the update         be do on the replicas which be destroy after ``forward``  however           class `~torch nn dataparallel` guarantee that the replica on         ``device 0 `` will have its parameters and buffer share storage with         the base parallelize  attr `module`  so   in-place   update to the         parameters or buffer on ``device 0 `` will be record  e g            class `~torch nn batchnorm2d` and  func `~torch nn utils spectral norm`         rely on this behavior to update the buffer          warn           forward and backward hook define on  attr `module` and its submodules         will be invoke ``len device ids `` time  each with input locate on         a particular device  particularly  the hook be only guarantee to be         execute in correct order with respect to operations on correspond         devices  for example  it be not guarantee that hook set via          meth `~torch nn module register forward pre hook` be execute before         `all` ``len device ids ``  meth `~torch nn module forward` call  but         that each such hook be execute before the correspond          meth `~torch nn module forward` call of that device          warn           when  attr `module` return a scalar  i e   0-dimensional tensor  in          func `forward`  this wrapper will return a vector of length equal to         number of devices use in data parallelism  contain the result from         each device          note           there be a subtlety in use the         ``pack sequence -> recurrent network -> unpack sequence`` pattern in a          class `~torch nn module` wrap in  class `~torch nn dataparallel`          see  ref `pack-rnn-unpack-with-data-parallelism` section in faq for         detail        args          module  module   module to be parallelize         device ids  list of int or torch device   cuda devices  default  all devices          output device  int or torch device   device location of output  default  device ids 0        attribute          module  module   the module to be parallelize      example            >>> net = torch nn dataparallel model  device ids= 0  1  2           >>> output = net input var     input var can be on any device  include cpu                    def   init   self  module  device ids=none  output device=none  dim=0           super dataparallel  self    init              device type =  get available device type           if device type be none              self module = module             self device ids =                return          if device ids be none              device ids =  get all device indices            if output device be none              output device = device ids 0           self dim = dim         self module = module         self device ids =   get device index x  true  for x in device ids          self output device =  get device index output device  true          self src device obj = torch device device type  self device ids 0             check balance self device ids           if len self device ids  == 1              self module to self src device obj       def forward self   input    kwargs           with torch autograd profiler record function string               if not self device ids                  return self module  input    kwargs               for t in chain self module parameters    self module buffer                     if t device  = self src device obj                      raise runtimeerror string                                        string                                        string format self src device obj  t device                input  kwargs = self scatter input  kwargs  self device ids                                        if not input and not kwargs                  input =                       kwargs =                    if len self device ids  == 1                  return self module  input 0     kwargs 0               replicas = self replicate self module  self device ids  len input                output = self parallel apply replicas  input  kwargs              return self gather output  self output device       def replicate self  module  device ids           return replicate module  device ids  not torch be grad enable         def scatter self  input  kwargs  device ids           return scatter kwargs input  kwargs  device ids  dim=self dim       def parallel apply self  replicas  input  kwargs           return parallel apply replicas  input  kwargs  self device ids  len replicas         def gather self  output  output device           return gather output  output device  dim=self dim  
class distributeddataparallel module       r   implement distribute data parallelism that be base on     ``torch distributed`` package at the module level       this container parallelize the application of the give module by     split the input across the specify devices by chunk in the batch     dimension  the module be replicate on each machine and each device  and     each such replica handle a portion of the input  during the backwards     pass  gradients from each node be average       the batch size should be larger than the number of gpus use locally       see also   ref `distributed-basics` and  ref `cuda-nn-ddp-instead`      the same constraints on input as in  class `torch nn dataparallel` apply       creation of this class require that ``torch distributed`` to be already     initialize  by call  func `torch distribute init process group`       ``distributeddataparallel`` be prove to be significantly faster than      class `torch nn dataparallel` for single-node multi-gpu data     parallel train       to use ``distributeddataparallel`` on a host with n gpus  you should spawn     up ``n`` process  ensure that each process exclusively work on a single     gpu from 0 to n-1  this can be do by either set     ``cuda visible devices`` for every process or by call           >>> torch cuda set device i       where i be from 0 to n-1  in each process  you should refer the follow     to construct this module           >>> torch distribute init process group          >>>     backend= nccl   world size=n  init method=              >>>           >>> model = distributeddataparallel model  device ids= i   output device=i       in order to spawn up multiple process per node  you can use either     ``torch distribute launch`` or ``torch multiprocessing spawn``          note           please refer to `pytorch distribute overview <https //pytorch org/tutorials/beginner/dist overview html>`           for a brief introduction to all feature relate to distribute train          note           ``distributeddataparallel`` can be use in conjunction with          class `torch distribute optim zeroredundancyoptimizer` to reduce         per-rank optimizer state memory footprint  please refer to         `zeroredundancyoptimizer recipe <https //pytorch org/tutorials/recipes/zero redundancy optimizer html>`           for more detail          note   ``nccl`` backend be currently the fastest and highly recommend         backend when use gpus  this apply to both single-node and         multi-node distribute train          note   this module also support mixed-precision distribute train          this mean that your model can have different type of parameters such         as mix type of ``fp16`` and ``fp32``  the gradient reduction on these         mix type of parameters will just work fine          note   if you use ``torch save`` on one process to checkpoint the module          and ``torch load`` on some other process to recover it  make sure that         ``map location`` be configure properly for every process  without         ``map location``  ``torch load`` would recover the module to devices         where the module be save from          note   when a model be train on ``m`` nod with ``batch=n``  the         gradient will be ``m`` time smaller when compare to the same model         train on a single node with ``batch=m n`` if the loss be sum  not         average as usual  across instance in a batch  because the gradients         between different nod be average   you should take this into         consideration when you want to obtain a mathematically equivalent         train process compare to the local train counterpart  but in most         case  you can just treat a distributeddataparallel wrap model  a         dataparallel wrap model and an ordinary model on a single gpu as the         same  e g  use the same learn rate for equivalent batch size           note           parameters be never broadcast between process  the module perform         an all-reduce step on gradients and assume that they will be modify         by the optimizer in all process in the same way  buffer          e g  batchnorm stats  be broadcast from the module in process of rank         0  to all other replicas in the system in every iteration          note           if you be use distributeddataparallel in conjunction with the          ref `distributed-rpc-framework`  you should always use          meth `torch distribute autograd backward` to compute gradients and          class `torch distribute optim distributedoptimizer` for optimize         parameters           example                >>> import torch distribute autograd as dist autograd             >>> from torch nn parallel import distributeddataparallel as ddp             >>> from torch import optim             >>> from torch distribute optim import distributedoptimizer             >>> from torch distribute rpc import rref             >>>             >>> t1 = torch rand  3  3   require grad=true              >>> t2 = torch rand  3  3   require grad=true              >>> rref = rpc remote  worker1   torch add  args= t1  t2               >>> ddp model = ddp my model              >>>             >>>   setup optimizer             >>> optimizer params =  rref              >>> for param in ddp model parameters                >>>     optimizer params append rref param               >>>             >>> dist optim = distributedoptimizer              >>>     optim sgd              >>>     optimizer params              >>>     lr=0 05              >>>               >>>             >>> with dist autograd context   as context id              >>>     pred = ddp model rref to here                >>>     loss = loss func pred  loss              >>>     dist autograd backward context id  loss              >>>     dist optim step           note           to let a non-ddp model load a state dict from a ddp model           meth `~torch nn modules utils consume prefix in state dict if present`         need to be apply to strip the prefix  module   in the ddp state dict before load          warn           constructor  forward method  and differentiation of the output  or a         function of the output of this module  be distribute synchronization         point  take that into account in case different process might be         execute different code          warn           this module assume all parameters be register in the model by the         time it be create  no parameters should be add nor remove later          same apply to buffer          warn           this module assume all parameters be register in the model of each         distribute process be in the same order  the module itself will         conduct gradient ``allreduce`` follow the reverse order of the         register parameters of the model  in other word  it be users          responsibility to ensure that each distribute process have the exact         same model and thus the exact same parameter registration order          warn           this module allow parameters with non-rowmajor-contiguous stride          for example  your model may contain some parameters whose          class `torch memory format` be ``torch contiguous format``         and others whose format be ``torch channel last``   however          correspond parameters in different process must have the         same stride          warn           this module doesn t work with  func `torch autograd grad`  i e  it will         only work if gradients be to be accumulate in `` grad`` attribute of         parameters           warn           if you plan on use this module with a ``nccl`` backend or a ``gloo``         backend  that use infiniband   together with a dataloader that use         multiple workers  please change the multiprocessing start method to         ``forkserver``  python 3 only  or ``spawn``  unfortunately         gloo  that use infiniband  and nccl2 be not fork safe  and you will         likely experience deadlocks if you don t change this set          warn           forward and backward hook define on  attr `module` and its submodules         win t be invoke anymore  unless the hook be initialize in the          meth `forward` method          warn           you should never try to change your model s parameters after wrap         up your model with ``distributeddataparallel``  because  when         wrap up your model with ``distributeddataparallel``  the constructor         of ``distributeddataparallel`` will register the additional gradient         reduction function on all the parameters of the model itself at the         time of construction  if you change the model s parameters afterwards          gradient redunction function no longer match the correct set of         parameters          warn           use ``distributeddataparallel`` in conjunction with the          ref `distributed-rpc-framework` be experimental and subject to change       args          module  module   module to be parallelize         device ids  list of int or torch device   cuda devices                     1  for single-device modules  ``device ids`` can                    contain exactly one device id  which represent the only                    cuda device where the input module correspond to this process reside                     alternatively  ``device ids`` can also be ``none``                     2  for multi-device modules and cpu modules                     ``device ids`` must be ``none``                      when ``device ids`` be ``none`` for both case                     both the input data for the forward pass and the actual module                    must be place on the correct device                      default  ``none``          output device  int or torch device   device location of output for                       single-device cuda modules  for multi-device modules and                       cpu modules  it must be ``none``  and the module itself                       dictate the output location   default  ``device ids 0 ``                       for single-device modules          broadcast buffer  bool   flag that enable sync  broadcast                            buffer of the module at begin of the ``forward``                           function   default  ``true``          process group  the process group to be use for distribute data                        all-reduction  if ``none``  the default process group  which                        be create by  func `torch distribute init process group`                         will be use   default  ``none``          bucket cap mb  ``distributeddataparallel`` will bucket parameters into                        multiple bucket so that gradient reduction of each                        bucket can potentially overlap with backward computation                          attr `bucket cap mb` control the bucket size in                        megabytes  mb    default  25          find unused parameters  bool   traverse the autograd graph from all                                tensors contain in the return value of the                                wrap module s ``forward`` function  parameters                                that don t receive gradients as part of this                                graph be preemptively mark as be ready to                                be reduce  note that all ``forward`` output                                that be derive from module parameters must                                participate in calculate loss and later the                                gradient computation  if they don t  this wrapper                                will hang wait for autograd to produce                                gradients for those parameters  any output                                derive from module parameters that be otherwise                                unused can be detach from the autograd graph                                use ``torch tensor detach``   default  ``false``          check reduction  this argument be deprecate          gradient as bucket view  bool   when set to ``true``  gradients will be view                       point to different offset of ``allreduce`` communication                       bucket  this can reduce peak memory usage  where the                       save memory size will be equal to the total gradients                       size  moreover  it avoid the overhead of copy between                       gradients and ``allreduce`` communication bucket  when                       gradients be view  ``detach   `` cannot be call on the                       gradients  if hit such errors  please fix it by                       refer to the  meth `~torch optim optimizer zero grad`                       function in ``torch/optim/optimizer py`` as a solution        attribute          module  module   the module to be parallelize       example            >>> torch distribute init process group backend= nccl   world size=4  init method=               >>> net = torch nn parallel distributeddataparallel model  pg               def   init            self          module          device ids=none          output device=none          dim=0          broadcast buffers=true          process group=none          bucket cap mb=25          find unused parameters=false          check reduction=false          gradient as bucket view=false                  super distributeddataparallel  self    init              assert any  p require grad for p in module parameters                     distributeddataparallel be not need when a module                doesn t have any parameter that require a gradient                      if device ids be not none and len device ids  > 1              raise valueerror  device ids can only be none or contain a single element             self be multi device module = len  p device for p in module parameters     > 1         distinct device type =  p device type for p in module parameters            if len distinct device type   = 1              raise valueerror                   distributeddataparallel s input module must be on                    the same type of devices  but input module parameters locate in      format                      distinct device type                                         self device type = list distinct device type  0           if               device ids be none             or len device ids  == 0    for backward compatibility              or self device type ==  cpu              or self be multi device module                        if device ids or output device                  raise valueerror                       distributeddataparallel device ids and output device arguments                        only work with single-device/multiple-device gpu modules or cpu modules                         but get device ids     output device     and module parameters      format                          device ids                          output device                           p device for p in module parameters                                                          self device ids = none             self output device = none         else              self device ids =   get device index x  true  for x in device ids               if output device be none                  output device = device ids 0               self output device =  get device index output device  true           if process group be none              self process group =  get default group           else              self process group = process group          self static graph = false         self dim = dim         self module = module         self device = list self module parameters    0  device         self broadcast buffer = broadcast buffer         self find unused parameters = find unused parameters         self require backward grad sync = true         self require forward param sync = true         self ddp uneven input config =  ddpuneveninputsconfig              ddp join enabled=false              ddp join divide by initial world size=false              ddp join throw on early termination=false                    self gradient as bucket view = gradient as bucket view         if hasattr module    ddp params and buffer to ignore                self parameters to ignore = module  ddp params and buffer to ignore         else              self parameters to ignore =             if check reduction                this argument be no longer use since the reducer               will ensure reduction complete even if some parameters               do not receive gradients              warn warn                   the `check reduction` argument in `distributeddataparallel`                    module be deprecate  please avoid use it                            check that a module do not have uninitialized parameters         for param in module parameters                if isinstance param  torch nn parameter uninitializedparameter                   raise runtimeerror                       modules with uninitialized parameters can t be use with `distributeddataparallel`                         run a dummy forward pass to correctly initialize the modules                              use for intra-node param sync and inter-node sync as wel         self broadcast bucket size = int 250   1024   1024             reduction bucket size         self bucket bytes cap = int bucket cap mb   1024   1024            whether to perform input tensor cpu to gpu copy on a side-stream         self use side stream for tensor copy =               os environ get  pytorch ddp use side stream    1   ==  1                       todo wayi    remove this field since spmd be no longer support            and also remove all the relevant unnecessary loop            module replication within process  single-process multi device          self  module copy =  self module            build parameters for reducer          parameters  expect sparse gradient = self  build params for reducer             verify model equivalence          dist  verify model across rank self process group  parameters            sync params and buffer  ensure all ddp model start off at the same value          self  sync params and buffer authoritative rank=0            in debug mode  build a map of parameter index -> parameter          if dist  get debug mode    = dist  distributeddebuglevel off              param to name map = self  build param to name map parameters          else              param to name map =              build reducer          self  ddp init helper parameters  expect sparse gradient  param to name map       def  sync params and buffer self  authoritative rank=0           module state =            for name  param in self module state dict   items                if name not in self parameters to ignore                  module state append param           if len module state  > 0              self  distribute broadcast coalesce                  module state  self broadcast bucket size  authoritative rank                    def  ddp init helper self  parameters  expect sparse gradient  param to name map                       initialization helper function that do the follow           1  bucket the parameters for reductions          2  reset the bucket state          3  register the grad hook          4  log constructin-time ddp log data          5  pass a handle of ddp to syncbatchnorm layer                     self num iterations = 0           the bucket size limit be specify in the constructor            additionally  we allow for a single small bucket for parameters           that be define first  such that their gradients don t spill into           a much larger bucket  add unnecessary latency after gradient           computation finish  experiment show 1mb be a reasonable value          bucket indices = dist  compute bucket assignment by size              parameters 0                dist  default first bucket bytes  self bucket bytes cap               expect sparse gradient 0                        note  reverse list of bucket because we want to approximate the           order in which their gradients be produce  and assume they           be use in the forward pass in the order they be define          self reducer = dist reducer              parameters              list reverse bucket indices                self process group              expect sparse gradient              self bucket bytes cap              self find unused parameters              self gradient as bucket view              param to name map                     self logger = dist logger self reducer             set log data that can be get during construction time          self logger set construction data and log              self module   class     name                   if self device ids be none else self device ids              -1 if self output device be none else self output device              self broadcast buffer                       pass a handle to torch nn syncbatchnorm layer         self  pass sync batchnorm handle self  module copy       def   getstate   self           self  check default group           attrs = copy copy self   dict            del attrs  process group           del attrs  reducer           del attrs  logger           return attrs      def   setstate   self  state             if serializable  then the process group should be the default one         self process group =  get default group           super distributeddataparallel  self    setstate   state          self   dict   setdefault  require forward param sync   true          self   dict   setdefault  require backward grad sync   true          parameters  expect sparse gradient = self  build params for reducer             in debug mode  build a map of parameter index -> parameter          if dist  get debug mode    = dist  distributeddebuglevel off              param to name map = self  build param to name map parameters          else              param to name map =              build reducer         self  ddp init helper parameters  expect sparse gradient  param to name map          if self static graph              self  set static graph        def  build params for reducer self             build tuple of  module  parameter  for all parameters that require grads          modules and parameters =                                  module  parameter                  for module name  module in replica name modules                   for parameter in                       param                       note that we access module name parameters instead of                       parameters module   parameters module  be only need in the                       single-process multi device case  where it access replicate                       parameters through  former parameters                      for param name  param in module name parameters recurse=false                      if param require grad                     and f  module name   param name   not in self parameters to ignore                                             for replica in self  module copy                      deduplicate any parameters that might be share across child modules          memo = set           modules and parameters =                  p not in memo  be the deduplication check                 not memo add p   be always true  and it s only there to cause  add p   if need                m  p  for m  p in replica mps if p not in memo and not memo add p               for replica mps in modules and parameters                      build list of parameters          parameters =               list parameter for    parameter in replica              for replica in modules and parameters                      check if a module will produce a sparse gradient          def produce sparse gradient module               if isinstance module  torch nn embed  or isinstance                  module  torch nn embeddingbag                                return module sparse             return false            build list of booleans indicate whether or not to expect sparse           gradients for the correspond parameters          expect sparse gradient =               list produce sparse gradient module  for module    in replica              for replica in modules and parameters                      the follow modules params and modules buffer be use for           param/buffer sync in  sync params          self modules params =               list self  get parameters m   for m in self  module copy                     collect buffer for modules  filter out buffer that should be ignore          name module buffer =                 buffer  buffer name  for buffer name  buffer in m name buffer                for m in self  module copy                   self modules buffer =                                 buffer                 for  buffer  buffer name  in module buffer                 if buffer name not in self parameters to ignore                           for module buffer in name module buffer                    return parameters  expect sparse gradient      def  build param to name map self  parameters           param to param index =               parameters 0  i    i for i in range len parameters 0                      param set = set parameters 0           param index to param fqn =            for module name  module in self module name modules                for param name  param in module name parameters recurse=false                   fqn = f  module name   param name                     bypass ignore parameters since those be not reduce by ddp                   to begin with                  if fqn not in self parameters to ignore and param require grad                      if param not in param set                          raise valueerror                              f param with name  fqn  find in module parameters  but not ddp parameters                                 this indicate a bug in ddp  please report an issue to pytorch                                                 param index = param to param index param                      param index to param fqn param index  = fqn            ensure we cover all parameters         if len param set   = len param index to param fqn               raise valueerror                                         expect param to name map to cover all parameters  but                      f  get conflict lengths   len param set   vs                       f  len param index to param fqn    this indicate a bug in ddp                         please report an issue to pytorch                                            return param index to param fqn      def  get parameters self  m  recurse=true                       return a generator of module parameters                      def model parameters m               ps =                   m  former parameters value                   if hasattr m    former parameters                   else m parameters recurse=false                            for p in ps                  yield p          for m in m modules   if recurse else  m               for p in model parameters m                   yield p      def  check default group self           pickle not support = false         try              if self process group  =  get default group                    pickle not support = true         except runtimeerror              pickle not support = true          if pickle not support              raise runtimeerror                   ddp pickling/unpickling be only support                    when use ddp with the default process                    group  that be  when you have call                    init process group and have not pass                    process group argument to ddp constructor                      contextmanager     def no sync self           r            a context manager to disable gradient synchronizations across ddp         process  within this context  gradients will be accumulate on module         variables  which will later be synchronize in the first         forward-backward pass exit the context           example                >>> ddp = torch nn parallel distributeddataparallel model  pg              >>> with ddp no sync                >>>   for input in input              >>>     ddp input  backward      no synchronization  accumulate grads             >>> ddp another input  backward      synchronize grads                     old require backward grad sync = self require backward grad sync         self require backward grad sync = false         try              yield         finally              self require backward grad sync = old require backward grad sync      def forward self   input    kwargs           with torch autograd profiler record function  distributeddataparallel forward                self reducer save thread local state               if torch be grad enable   and self require backward grad sync                  self logger set runtime stats and log                   self num iterations  = 1                 self reducer prepare for forward               if self ddp uneven input config ddp join enable                  ones = torch ones 1  device=self device                  work = dist all reduce ones  group=self process group  async op=true                  if self ddp uneven input config ddp join throw on early termination                        active rank schedule an allreduce with zero  inactive                       rank schedule them with 1  if the result  = 0 it                       indicate at least one rank have terminate and we should                       throw                      zero = torch zero 1  device=self device                      dist all reduce zero  group=self process group                      should throw stop iteration = zero item                       if should throw stop iteration                          raise runtimeerror                               detect at least one rank that exhaust input  throw across all rank                                             else                      self reducer  set forward pass work handle                          work                          self ddp uneven input config ddp join divide by initial world size                                       call  rebuild bucket before forward compuation                it may allocate new bucket before deallocating old bucket               inside  rebuild bucket  to save peak memory usage                call  rebuild bucket before the peak memory usage increase               during forward computation                this should be call only once during whole train period              if torch be grad enable   and self reducer  rebuild bucket                    log info  reducer bucket have be rebuild in this iteration                 if self require forward param sync                  self  sync params                if self ddp uneven input config ddp join enable                    notify join rank whether they should sync in backwards pass or not                  self  check global require backward grad sync be join rank=false               if self device ids                  input  kwargs = self to kwargs input  kwargs  self device ids 0                   output = self module  input 0     kwargs 0               else                  output = self module  input    kwargs               if torch be grad enable   and self require backward grad sync                  self require forward param sync = true                   we ll return the output object verbatim since it be a freeform                   object  we need to find any tensors in this object  though                    because we need to figure out which parameters be use during                   this forward pass  to ensure we short circuit reduction for any                   unused parameters  only if `find unused parameters` be set                  if self find unused parameters and not self static graph                        do not need to populate this for static graph                      self reducer prepare for backward list  find tensors output                    else                      self reducer prepare for backward                 else                  self require forward param sync = false            todo  right now we add this sink for static graph train only  once           this feature be stable  we will add this sink for all case  e g            this sink can help capture more accuracte backward start time as well          if self static graph and self num iterations == 1                need to grab list of tensors from user output in order to pass               to custom autograd function              output tensor list  treespec = tree flatten output              passthrough tensor list =  ddpsink apply                  self reducer                   output tensor list                             reconstruct output data structure              output = tree unflatten passthrough tensor list  treespec          return output      def scatter self  input  kwargs  device ids           return scatter kwargs input  kwargs  device ids  dim=self dim       def  recursive to self  input  target gpu           r            recursively move input to the target gpu                       def to map obj               if isinstance obj  torch tensor                   if obj device == torch device  cuda   target gpu                       return  obj                   if not self use side stream for tensor copy                      return  obj to target gpu                    else                        perform cpu -> gpu copy in a background stream  this code be                       motivate from similar logic in torch/nn/parallel/ function py                     stream =  get stream target gpu                      with torch cuda stream stream                           output = obj to target gpu                        synchronize with the copy stream                     with torch cuda device target gpu                           current stream = torch cuda current stream                             sync the current stream with the copy stream                         current stream wait stream stream                            ensure tensor memory be not reuse until work on                           main stream be complete                         output record stream current stream                      return  output               if be namedtuple obj                   return  type obj   args  for args in zip  map to map  obj                if isinstance obj  tuple  and len obj  > 0                  return list zip  map to map  obj                if isinstance obj  list  and len obj  > 0                  return  list i  for i in zip  map to map  obj                if isinstance obj  dict  and len obj  > 0                  return  type obj  i  for i in zip  map to map  obj items                  return  obj             avoid reference cycle         try              res = to map input          finally              to map = none         return res      def to kwargs self  input  kwargs  device id           input = self  recursive to input  device id  if input else            kwargs = self  recursive to kwargs  device id  if kwargs else            if len input  < len kwargs               input extend     for   in range len kwargs  - len input             elif len kwargs  < len input               kwargs extend     for   in range len input  - len kwargs             input = tuple input          kwargs = tuple kwargs          return input  kwargs      def gather self  output  output device           return gather output  output device  dim=self dim       def train self  mode=true           super distributeddataparallel  self  train mode          for module in self  module copy 1                module train mode          return self        when run in join mode  schedule an allreduce to match the one in the       forward pass to determine the no  of currently active process and whether       all process have join      def  schedule shadow all reduce for fwd pass self           all active procs = torch zero 1  device=self device          dist all reduce all active procs  group=self process group          return all active procs item          when run in join mode  schedule an allreduce to notify join rank       of whether backwards pass synchronization will run this iteraton or not      def  check global require backward grad sync self  be join rank           if not be join rank and self require backward grad sync              require sync tensor = torch ones 1  device=self device          else              require sync tensor = torch zero 1  device=self device           work = dist all reduce              require sync tensor  group=self process group  async op=true                   return work  require sync tensor        when run in join mode  check and perform sync of module buffer if       the model have buffer that should be synchronize in the forward pass      def  check and sync module buffer self           if self will sync module buffer                authoritative rank = self  find common rank self  distribute rank  false              self  distribute broadcast coalesce                  self modules buffer 0   self broadcast bucket size  authoritative rank                      when run in join model  agree upon a common rank and broadcast model       parameters to all other rank      def  sync final model self  be last joiner             agree upon the process that will be the authoritative model copy            the current rank be a candidate for be the authoritative copy if           be last joiner=true  we break tie via pick the larger rank          self  authoritative rank = self  find common rank              self  distribute rank  be last joiner                   self  sync params and buffer authoritative rank=self  authoritative rank         schedule allreduce ops to match those schedule in the reducer s backward       pass      def  match all reduce for bwd pass self           allreduce work =              schedule allreduce in the same order as reducer schedule them  i e            the order of the bucket  retrieve the bucket order from the reducer           ensure that we keep the same order in join mode  such as when bucket           order be rebuild dynamically          all bucket tensors = self reducer get bucket tensors           for bucket tensors in all bucket tensors                join process contribute zero gradient  in the case that               divide by initial world size=true  we divide grads by the static               world size  if not  the divide factor be reduce by the number               of join process              zero tensors =  torch zero like t  for t in bucket tensors              work = self process group allreduce zero tensors              allreduce work append work          for work in allreduce work              work wait          allreduces the use parameter map across rank      def  match unused params allreduce self           locally use param map = self reducer  get local use map           self process group allreduce locally use param map        contextmanager     def join          self          divide by initial world size=true          enable=true          throw on early termination=false                 r            a context manager to be use in conjunction with an instance of          class `torch nn parallel distributeddataparallel` to be         able to train with uneven input across participate process           this context manager will keep track of already-joined ddp process          and  shadow  the forward and backward pass by insert collective         communication operations to match with the ones create by non-joined         ddp process  this will ensure each collective call have a correspond         call by already-joined ddp process  prevent hang or errors that         would otherwise happen when train with uneven input across         process  alternatively  if the flag ``throw on early termination`` be         specify to be ``true``  all trainers will throw an error once one rank         run out of input  allow these errors to be catch and handle         accord to application logic           once all ddp process have join  the context manager will broadcast         the model correspond to the last join process to all process to         ensure the model be the same across all process          which be guarantee by ddp            to use this to enable train with uneven input across process          simply wrap this context manager around your train loop  no further         modifications to the model or data load be require              warn               if the model or train loop this context manager be wrap around             have additional distribute collective operations  such as             ``syncbatchnorm`` in the model s forward pass  then the flag             ``throw on early termination`` must be enable  this be because this             context manager be not aware of non-ddp collective communication              this flag will cause all rank to throw when any one rank             exhaust input  allow these errors to be catch and recover             from across all rank           args              divide by initial world size  bool   if ``true``  will divide                 gradients by the initial ``world size`` ddp train be launch                 with  if ``false``  will compute the effective world size                  number of rank that have not deplete their input yet  and                 divide gradients by that during allreduce  set                 ``divide by initial world size=true`` to ensure every input                 sample include the uneven input have equal weight in term of                 how much they contribute to the global gradient  this be                 achieve by always divide the gradient by the initial                 ``world size`` even when we encounter uneven input  if you set                 this to ``false``  we divide the gradient by the remain                 number of nod  this ensure parity with train on a smaller                 ``world size`` although it also mean the uneven input would                 contribute more towards the global gradient  typically  you                 would want to set this to ``true`` for case where the last few                 input of your train job be uneven  in extreme case  where                 there be a large discrepancy in the number of input  set                 this to ``false`` might provide better result              enable  bool   whether to enable uneven input detection or not  pass                 in ``enable=false`` to disable in case where you know that                 input be even across participate process  default be                 ``true``              throw on early termination  bool   whether to throw an error                 or continue train when at least one rank have exhaust                 input  if ``true``  will throw upon the first rank reach end                 of data  if ``false``  will continue train with a smaller                 effective world size until all rank be join  note that if                 this flag be specify  then the flag                 ``divide by initial world size`` would be ignore  default                 be ``false``            example              >>>  import torch           >>>  import torch distribute as dist           >>>  import os           >>>  import torch multiprocessing as mp           >>>  import torch nn as nn           >>>    on each spawn worker           >>>  def worker rank             >>>      dist init process group  nccl   rank=rank  world size=2            >>>      torch cuda set device rank            >>>      model = nn linear 1  1  bias=false  to rank            >>>      model = torch nn parallel distributeddataparallel            >>>          model  device ids= rank   output device=rank           >>>                  >>>        rank 1 get one more input than rank 0            >>>      input =  torch tensor  1   float   for   in range 10   rank             >>>      with model join              >>>          for   in range 5             >>>              for inp in input            >>>                  loss = model inp  sum             >>>                  loss backward             >>>    without the join   api  the below synchronization will hang           >>>    block for rank 1 s allreduce to complete            >>>  torch cuda synchronize device=rank                        log uneven input api usage          self logger  set uneven input join           try              have error = false             self ddp uneven input config =  ddpuneveninputsconfig                  ddp join enabled=enable                  ddp join divide by initial world size=divide by initial world size                  ddp join throw on early termination=throw on early termination                            yield         except exception as e                set to skip any process in the finally block              have error = true             raise e         finally                skip any process to let the exception immediately be raise if               there be one              if enable and not have error                  all procs join = false                 be last joiner = true                 i = 0                 warn threshold = 1000                 warn simplefilter  once                   while not all procs join                      if i > warn threshold                          my rank = self  distribute rank                         warn warn                               detect uneven input skew of greater                               f than  warn threshold   this mean that rank  my rank                                f have at least  warn threshold  fewer input than                                other currently active rank  this level of skew could                                lead to performance degradation during train                                                   schedule allreduce to match fwd pass allreduce in non-joined procs                     num active procs = self  schedule shadow all reduce for fwd pass                       if num active procs == 0                          all procs join = true                     else                            some ddp process still need to be join                          if self ddp uneven input config ddp join throw on early termination                                schedule allreduce tell active rank to terminate                             ones = torch ones 1  device=self device                              dist all reduce ones  group=self process group                                raise stopiteration doesn t throw error in python 3 6                               and throw runtimeerror in 3 7   pep 479   so just                               raise runtimeerror here                              raise runtimeerror                                  f rank  self  distribute rank  exhaust all input                                                         if be last joiner                              be last joiner = false                           it will rebuild bucket only once during train period                         self reducer  rebuild bucket                             schedule a correspond broadcast if we be sync module                           buffer in the forward pass                          self  check and sync module buffer                                                          work                              should sync backwards tensor                            = self  check global require backward grad sync                              be join rank=true                                                   work wait                             if nonzero  then we should sync in the bwd pass                          should sync backwards = should sync backwards tensor item    = 0                           forward param sync be disable in the next iteration                           if we be skip grad sync this iteration  hence  we                           set require forward param sync appropriately here                          self require forward param sync = should sync backwards                         if not should sync backwards                              continue                           schedule one allreduce per gradient bucket to match                           the backwards pass allreduce                          self  match all reduce for bwd pass                             check if we need to allreduce locally unused params                          if self find unused parameters                              self  match unused params allreduce                             it will push rebuild params only once during train period                         self reducer  push all rebuild params                           i  = 1                    all procs join  agree on authoritative rank and broadcast the model                  self  sync final model be last joiner       def register comm hook self  state  object  hook  callable           r            register a communication hook which be an enhancement that provide a         flexible hook to users where they can specify how ddp aggregate gradients         across multiple workers           this hook would be very useful for researchers to try out new ideas  for         example  this hook can be use to implement several algorithms like gossipgrad         and gradient compression which involve different communication strategies for         parameter sync while run distribute dataparallel train           args              state  object   pass to the hook to maintain any state information during the train process                              examples include error feedback in gradient compression                              peer to communicate with next in gossipgrad  etc                               it be locally store by each worker                             and share by all the gradient tensors on the worker              hook  callable   average gradient tensors across workers and define as                               ``hook state  object  bucket  dist gradbucket  -> torch futures future``                                this function be call once the bucket be ready  the                              hook can perform whatever process be need and return                              a future indicate completion of any async work  ex  allreduce                                if the hook doesn t perform any communication  it can also                              just return a complete future  the future should hold the                              new value of grad bucket s tensors  once a bucket be ready                               c10d reducer would call this hook and use the tensors return                              by the future and copy grads to individual parameters                                we also provide an api call ``get future`` to retrieve a                              future associate with the completion of ``c10d processgroup work``                               ``get future`` be currently support for mpi and also support for most                              operations on gloo and mpi  except for peer to peer operations  send/recv               warn                grad bucket s tensors will not be predivided by world size  user be responsible             to divide by the world size in case of operations like allreduce              warn                ddp communication hook can only be register once and should be register             before call backward              warn                the future object that hook return should contain a result that have the same             shape with the tensors inside grad bucket              warn                ddp communication hook do not support single-process multiple-device mode              gradbucket tensors should consist of only a single tensor              warn                ``get future`` api support nccl  and partially gloo and mpi backends  no support             for peer-to-peer operations like send/recv  and will return a ``torch  c future``             which be an internal type and should be use with caution  it can still be use by             ``register comm hook`` api  but it be subject to some subtle differences compare             to ``torch futures future``              warn                ddp communication hook be experimental and subject to change           example               below be an example of a noop hook that return the same tensors               >>> def noop state  object  bucket  dist gradbucket   -> torch futures future             >>>     fut = torch futures future               >>>     fut set result bucket get tensors                >>>     return fut              >>> ddp register comm hook state = none  hook = noop           example               below be an example of a parallel sgd algorithm where gradients be encode before             allreduce  and then decode after allreduce               >>> def encode and decode state  object  bucket  dist gradbucket   -> torch futures future             >>>     tensors =  t / process group world size for t in bucket get tensors                >>>     encode tensors = encode tensors    encode gradients             >>>     fut = process group allreduce encode tensors  get future               >>>       define the then callback to decode              >>>     def decode fut               >>>         decode tensors = decode fut value      decode gradients             >>>         return decode tensors             >>>     return fut then decode               >>> ddp register comm hook state = none  hook = encode and decode                      self  check comm hook hook          self logger  set comm hook name hook   qualname            dist  register comm hook self reducer  state  hook       def  register builtin comm hook self  comm hook type           r            register a built-in communication hook that specify how ddp         aggregate gradients across multiple workers          the built-in hook aim to provide efficient c   implementations for certain hook          which might not be as efficient if implement in python use a python communication hook           args              comm hook type  dist builtincommhooktype   type of communication hook  such as             allreduce  fp16 compress  etc              warn                ddp communication hook can only be register once and should be register             before call backward              warn                ddp communication hook do not support single-process multiple-device mode              gradbucket tensors should consist of only a single tensor              warn                ddp communication hook be experimental and subject to change           example               below be an example of a fp16 compression where gradients be             compress into 16-bit floating-point number before allreduce  and             then decompress after allreduce               >>> ddp  register builtin comm hook dist builtincommhooktype fp16 compress                       self logger  set comm hook name str comm hook type           dist  register builtin comm hook self reducer  comm hook type       def  distribute broadcast coalesce          self  tensors  buffer size  authoritative rank=0                dist  broadcast coalesce              self process group  tensors  buffer size  authoritative rank                def will sync module buffer self           return               self require forward param sync             and self broadcast buffer             and len self modules buffer 0   > 0                def  find common rank self  input rank  rank cond             -1 indicate that this rank be not under consideration to be the           common rank         rank to use = torch tensor               input rank if rank cond else -1               device=self device                    dist all reduce rank to use  op=reduceop max  group=self process group          if rank to use item   == -1              raise valueerror                   bug  expect rank cond to be true for at least one process                         return rank to use item        def  sync params self           with torch no grad                  module buffer sync             if self will sync module buffer                      synchronize buffer across process                    if we be run ddp with the join manager  we have to agree                   upon a rank to sync module buffer from  since rank 0 may                   already have be join and have stale module buffer                  if self ddp uneven input config ddp join enable                      authoritative rank = self  find common rank                          self  distribute rank  true                                       else                        the process with rank 0 be consider the authoritative copy                      authoritative rank = 0                 self  distribute broadcast coalesce                      self modules buffer 0                       self broadcast bucket size                      authoritative rank                         def  pass sync batchnorm handle self  module copy           for dev idx  module in enumerate module copy               for layer in module modules                    if isinstance layer  torch nn modules syncbatchnorm                       assert                           self device type  =  cpu                          syncbatchnorm layer only work with gpu modules       def  check comm hook self  hook           if not callable hook               raise typeerror  communication hook must be callable             sig = inspect signature hook          if               sig parameters  bucket   annotation  = inspect  empty             and sig parameters  bucket   annotation  = dist gradbucket                        raise valueerror                   communication hook  bucket annotation should be dist gradbucket                          if sig return annotation  = inspect  empty and               sig return annotation  = torch futures future             and sig return annotation  = torch  c future                        raise valueerror                   communication hook  return annotation should be torch futures future or torch  c future                       property     def  distribute rank self           return dist get rank self process group        staticmethod     def  set params and buffer to ignore for model          module  params and buffer to ignore                            set parameters and buffer to be ignore by ddp  expect format for         parameters be the fully qualify name   module name   param name   and         similarly   module name   buffer name  for buffer  for example          params to ignore =              nb  model here be vanilla pytorch module  not yet wrap with ddp          for module name  module in model name modules                for param name  param in module name parameters recurse=false                   if should ignore param                         create expect format                     fqn = f  module name   param name                       params to ignore append fqn          torch nn parallel distributeddataparallel  set params and buffer to ignore for model              model              params to ignore                                 this be a workaround to set parameters and buffer ddp should ignore           during synchronization  it will be remove when the api be finalize           as part of address https //github com/pytorch/pytorch/issues/43690          module  ddp params and buffer to ignore = params and buffer to ignore      def  get ddp log data self           r            this interface can be call after distributeddataparallel   be         construct  it return a dictionary of log data  it could help         for debug and analysis  the loggind data include distributeddataparallel         constructor input parameters  some internal state of distributeddataparallel         and performance metrics  simply print the dictorinary and see what         these metrics be          this be a prototype interface and subject to change in the future                      ddp log data = self logger  get ddp log data           return    ddp log data strs map    ddp log data ints map       def  set ddp runtime log sample rate self  sample rate           r            this interface allow users to set sample rate of collect         runtime stats  the runtime stats will be record for the         first 10 iterations  after 10 iteratons runtime stats will be         record once every  sample rate  train iterations  in         default  runtime stats be record for the first 10 iterations          after 10 iterations runtime stats be record once every          kddpruntimeloggingsamplerate=100  train iterations          this be a prototype interface and subject to change in the future                      if sample rate < 1              raise valueerror                   ddp runtime log sample rate should be equal or greater than 1                        self reducer  set ddp runtime log sample rate sample rate       def  set static graph self                       users can explicitly let ddp know the train graph be static          when 1  the set of use and unused parameters will not change         during the whole train loop  in this case  it do not matter         whether users set find unsued parameters = true or not          2  how the graph be train will not change during the whole train         loop  mean there be no control flow depend on iterations           when graph be set to be static  ddp will support case that can not         be support in the past  1  reentrant backwards         2  activation checkpointing multiple time 3          activation checkpointing with find unused parameters = true          4  not all output tensors be use in loss calculation          5  there be model parameter that be outside of forward function          6  potentially improve performance when find unsued parameters = true         or there be unused parameters  as ddp will not search graph in each         iteraton to detect unused parameters when static graph be set to be true           this api should be call after distributeddataparallel construction  and         before train loop start  also it should be call in the same way for         all rank  for example              ddp model = distributeddataparallel model              ddp model  set static graph               for i in range n                                             self static graph = true         self reducer  set static graph           self logger  set static graph           if self find unused parameters              warn warn                   you pass find unused parameters=true to distributeddataparallel                     ` set static graph` will detect unused parameters automatically  so                    you do not need to set find unused parameters=true  just be sure these                    unused parameters will not change during train loop while call                    ` set static graph`                 
def register comm hook self  state  object  hook  callable       r        register a communication hook which be an enhancement that provide a     flexible hook to users where they can specify how ddp aggregate gradients     across multiple workers       this hook would be very useful for researchers to try out new ideas  for     example  this hook can be use to implement several algorithms like gossipgrad     and gradient compression which involve different communication strategies for     parameter sync while run distribute dataparallel train       args          state  object   pass to the hook to maintain any state information during the train process                          examples include error feedback in gradient compression                          peer to communicate with next in gossipgrad  etc                           it be locally store by each worker                         and share by all the gradient tensors on the worker          hook  callable   average gradient tensors across workers and define as                           ``hook state  object  bucket  dist gradbucket  -> torch futures future``                            this function be call once the bucket be ready  the                          hook can perform whatever process be need and return                          a future indicate completion of any async work  ex  allreduce                            if the hook doesn t perform any communication  it can also                          just return a complete future  the future should hold the                          new value of grad bucket s tensors  once a bucket be ready                           c10d reducer would call this hook and use the tensors return                          by the future and copy grads to individual parameters                            we also provide an api call ``get future`` to retrieve a                          future associate with the completion of ``c10d processgroup work``                           ``get future`` be currently support for mpi and also support for most                          operations on gloo and mpi  except for peer to peer operations  send/recv           warn            grad bucket s tensors will not be predivided by world size  user be responsible         to divide by the world size in case of operations like allreduce          warn            ddp communication hook can only be register once and should be register         before call backward          warn            the future object that hook return should contain a result that have the same         shape with the tensors inside grad bucket          warn            ddp communication hook do not support single-process multiple-device mode          gradbucket tensors should consist of only a single tensor          warn            ``get future`` api support nccl  and partially gloo and mpi backends  no support         for peer-to-peer operations like send/recv  and will return a ``torch  c future``         which be an internal type and should be use with caution  it can still be use by         ``register comm hook`` api  but it be subject to some subtle differences compare         to ``torch futures future``          warn            ddp communication hook be experimental and subject to change       example           below be an example of a noop hook that return the same tensors           >>> def noop state  object  bucket  dist gradbucket   -> torch futures future         >>>     fut = torch futures future           >>>     fut set result bucket get tensors            >>>     return fut          >>> ddp register comm hook state = none  hook = noop       example           below be an example of a parallel sgd algorithm where gradients be encode before         allreduce  and then decode after allreduce           >>> def encode and decode state  object  bucket  dist gradbucket   -> torch futures future         >>>     tensors =  t / process group world size for t in bucket get tensors            >>>     encode tensors = encode tensors    encode gradients         >>>     fut = process group allreduce encode tensors  get future           >>>       define the then callback to decode          >>>     def decode fut           >>>         decode tensors = decode fut value      decode gradients         >>>         return decode tensors         >>>     return fut then decode           >>> ddp register comm hook state = none  hook = encode and decode              self  check comm hook hook      self logger  set comm hook name hook   qualname        dist  register comm hook self reducer  state  hook  
def clip grad norm           parameters   tensor or tensors  max norm  float  norm type  float = 2 0          error if nonfinite  bool = false  -> torch tensor      r   clip gradient norm of an iterable of parameters       the norm be compute over all gradients together  as if they be     concatenate into a single vector  gradients be modify in-place       args          parameters  iterable tensor  or tensor   an iterable of tensors or a             single tensor that will have gradients normalize         max norm  float or int   max norm of the gradients         norm type  float or int   type of the use p-norm  can be `` inf `` for             infinity norm          error if nonfinite  bool   if true  an error be throw if the total             norm of the gradients from  attr ``parameters`` be ``nan``              ``inf``  or ``-inf``  default  false  will switch to true in the future       return          total norm of the parameters  view as a single vector               if isinstance parameters  torch tensor           parameters =  parameters      parameters =  p for p in parameters if p grad be not none      max norm = float max norm      norm type = float norm type      if len parameters  == 0          return torch tensor 0       device = parameters 0  grad device     if norm type == inf          norms =  p grad detach   abs   max   to device  for p in parameters          total norm = norms 0  if len norms  == 1 else torch max torch stack norms       else          total norm = torch norm torch stack  torch norm p grad detach    norm type  to device  for p in parameters    norm type      if total norm isnan   or total norm isinf            if error if nonfinite              raise runtimeerror                  f the total norm of order  norm type  for gradients from                   string                 string                 string          else              warn warn string                           string                           string                           string                            futurewarning  stacklevel=2      clip coef = max norm /  total norm   1e-6      if clip coef < 1          for p in parameters              p grad detach   mul  clip coef to p grad device       return total norm 
def clip grad value  parameters   tensor or tensors  clip value  float  -> none      r   clip gradient of an iterable of parameters at specify value       gradients be modify in-place       args          parameters  iterable tensor  or tensor   an iterable of tensors or a             single tensor that will have gradients normalize         clip value  float or int   maximum allow value of the gradients              the gradients be clip in the range              math `\left \text -clip\ value   \text clip\ value \right `             if isinstance parameters  torch tensor           parameters =  parameters      clip value = float clip value      for p in filter lambda p  p grad be not none  parameters           p grad data clamp  min=-clip value  max=clip value  
def parameters to vector parameters  iterable torch tensor   -> torch tensor      r   convert parameters to one vector      args          parameters  iterable tensor    an iterator of tensors that be the             parameters of a model       return          the parameters represent by a single vector                  param device = none      vec =        for param in parameters                   param device =  check param device param  param device           vec append param view -1       return torch cat vec  
def vector to parameters vec  torch tensor  parameters  iterable torch tensor   -> none      r   convert one vector to the parameters      args          vec  tensor   a single vector represent the parameters of a model          parameters  iterable tensor    an iterator of tensors that be the             parameters of a model                   if not isinstance vec  torch tensor           raise typeerror string                          format torch typename vec             param device = none           pointer = 0     for param in parameters                   param device =  check param device param  param device                    num param = param numel                    param data = vec pointer pointer   num param  view as param  data                   pointer  = num param 
def remove weight norm module  t module  name  str = string  -> t module      r   remove the weight normalization reparameterization from a module       args          module  module   contain module         name  str  optional   name of weight parameter      example          >>> m = weight norm nn linear 20  40           >>> remove weight norm m              for k  hook in module  forward pre hook items            if isinstance hook  weightnorm  and hook name == name              hook remove module              del module  forward pre hook k              return module      raise valueerror string                       format name  module   
def remove spectral norm module  t module  name  str = string  -> t module      r   remove the spectral normalization reparameterization from a module       args          module  module   contain module         name  str  optional   name of weight parameter      example          >>> m = spectral norm nn linear 40  10           >>> remove spectral norm m              for k  hook in module  forward pre hook items            if isinstance hook  spectralnorm  and hook name == name              hook remove module              del module  forward pre hook k              break     else          raise valueerror string format              name  module        for k  hook in module  state dict hook items            if isinstance hook  spectralnormstatedicthook  and hook fn name == name              del module  state dict hook k              break      for k  hook in module  load state dict pre hook items            if isinstance hook  spectralnormloadstatedictprehook  and hook fn name == name              del module  load state dict pre hook k              break      return module 
def spectral norm module  module                    name  str = string                    n power iterations  int = 1                    eps  float = 1e-12                    dim  optional int  = none  -> module      r   apply spectral normalization to a parameter in the give module          math           \mathbf w   sn  = \dfrac \mathbf w   \sigma \mathbf w             \sigma \mathbf w   = \max  \mathbf h   \mathbf h  \ne 0  \dfrac \ \mathbf w  \mathbf h \  2  \ \mathbf h \  2       spectral normalization stabilize the train of discriminators  critics      in generative adversarial network  gans  by rescale the weight tensor     with spectral norm  math `\sigma` of the weight matrix calculate use     power iteration method  if the dimension of the weight tensor be greater     than 2  it be reshape to 2d in power iteration method to get spectral     norm       see `spectral normalization for generative adversarial networks`             `spectral normalization for generative adversarial networks`  https //arxiv org/abs/1802 05957      args          module  nn module   contain module         name  str  optional   name of weight parameter         n power iterations  int  optional   number of power iterations to             calculate spectral norm         eps  float  optional   epsilon for numerical stability in             calculate norms         dim  int  optional   dimension correspond to number of output              the default be ``0``  except for modules that be instance of             convtranspose 1 2 3 d  when it be ``1``      return          the original module with a new parametrization register to the specify         weight         note           this function be implement use the new parametrization functionality         in  func `torch nn utils parametrize register parametrization`  it be a         reimplementation of  func `torch nn utils spectral norm`          note           if the ` spectralnorm` module  i e   `module parametrization weight idx `          be in train mode on removal  it will perform another power iteration          if you d like to avoid this iteration  set the module to eval mode         before its removal       example            >>> snm = spectral norm nn linear 20  40           >>> snm         parametrizedlinear          in features=20  out features=40  bias=true          parametrizations   moduledict               weight   parametrizationlist               0    spectralnorm                                             >>> snm parametrizations weight 0  u size           torch size  40               if not hasattr module  name           raise valueerror              string format module  name                          weight = getattr module  name       if dim be none          if isinstance module   torch nn convtranspose1d                                 torch nn convtranspose2d                                 torch nn convtranspose3d                dim = 1         else              dim = 0     parametrize register parametrization module  name   spectralnorm weight  n power iterations  dim  eps       return module 
def register parametrization      module  module  tensor name  str  parametrization  module   -> module      r   add a parametrization to a tensor in a module       assume that ``tensor name= weight `` for simplicity  when access ``module weight``      the module will return the parametrized version ``parametrization module weight ``      if the original tensor require a gradient  the backward pass will differentiate     through the  attr `parametrization`  and the optimizer will update the tensor accordingly       the first time that a module register a parametrization  this function will add an attribute     ``parametrizations`` to the module of type  class `~parametrizationlist`       the list of parametrizations on a tensor will be accessible under     ``module parametrizations weight``       the original tensor will be accessible under     ``module parametrizations weight original``       parametrizations may be concatenate by register several parametrizations     on the same attribute       the train mode of the register parametrizations be update on registration     if necessary to match the train mode of the host module      parametrized parameters and buffer have an inbuilt cache system that can be activate     use the context manager  func `cached`       a  attr `parametrization` may optionally implement a method with signature         code-block   python          def right inverse self  x  tensor  -> tensor      if  attr `parametrization` implement this method  it will be possible to assign     to the parametrized tensor  this may be use to initialize the tensor  as show in the example       in most situations  ``right inverse`` will be a function such that     ``forward right inverse x   == x``  see     `right inverse <https //en wikipedia org/wiki/inverse function right inverses>`        sometimes  when the parametrization be not surjective  it may be reasonable     to relax this  as show in the example below       args          module  nn module   module on which to register the parametrization         tensor name  str   name of the parameter or buffer on which to register             the parametrization         parametrization  nn module   the parametrization to register      return          module  module      raise          valueerror  if the module do not have a parameter or a buffer name  attr `tensor name`      examples          >>> import torch         >>> import torch nn utils parametrize as p         >>>         >>> class symmetric torch nn module           >>>     def forward self  x           >>>         return x triu     x triu 1  t    return a symmetric matrix         >>>         >>>     def right inverse self  a           >>>         return a triu           >>>         >>> m = torch nn linear 5  5          >>> p register parametrization m   weight   symmetric            >>> print torch allclose m weight  m weight t      m weight be now symmetric         true         >>> a = torch rand 5  5          >>> a = a   a t     a be now symmetric         >>> m weight = a    initialize the weight to be the symmetric matrix a         >>> print torch allclose m weight  a           true             parametrization train module train      if be parametrized module  tensor name                    module parametrizations tensor name  append parametrization        elif tensor name in module  buffer or tensor name in module  parameters                            original = getattr module  tensor name                   delattr module  tensor name                            if not be parametrized module                             inject new class module                           module parametrizations = moduledict                     inject property module  tensor name                   module parametrizations tensor name  = parametrizationlist                 parametrization   original               else          raise valueerror              string             string format module  tensor name                return module 
def remove parametrizations      module  module  tensor name  str  leave parametrized  bool = true   -> module      r   remove the parametrizations on a tensor in a module       - if ``leave parametrized=true``  ``module tensor name `` will be set to       its current output  in this case  the parametrization shall not change the ``dtype``       of the tensor      - if ``leave parametrized=false``  ``module tensor name `` will be set to       the unparametrised tensor in ``module parametrizations tensor name  original``       args          module  nn module   module from which remove the parametrization         tensor name  str   name of the parametrization to be remove         leave parametrized  bool  optional   leave the attribute  attr `tensor name` parametrized              default  ``true``      return          module  module      raise          valueerror  if ``module tensor name `` be not parametrized         valueerror  if ``leave parametrized=true`` and the parametrization change the size or dtype             of the tensor              if not be parametrized module  tensor name           raise valueerror              string format                  module  tensor name                                   original = module parametrizations tensor name  original       if leave parametrized          with torch no grad                t = getattr module  tensor name                                     if t dtype == original dtype              with torch no grad                    original set  t          else              raise valueerror                  string                 string                 string format original dtype  t dtype                         delattr module   class    tensor name           del module parametrizations tensor name              if isinstance original  parameter           module register parameter tensor name  original      else          module register buffer tensor name  original                 if not be parametrized module           delattr module  string                   orig cls = module   class     base   0          module   class   = orig cls     return module 
 contextmanager def cache        r   context manager that enable the cache system within parametrizations     register with  func `register parametrization`       the value of the parametrized object be compute and cache the first time     they be require when this context manager be active  the cache value be     discard when leave the context manager       this be useful when use a parametrized parameter more than once in the forward pass      an example of this be when parametrizing the recurrent kernel of an rnn or when     share weight       the simplest way to activate the cache be by wrap the forward pass of the neural network         code-block   python          import torch nn utils parametrize as p                     with p cache                output = model input       in train and evaluation  one may also wrap the part of the modules that use     several time the parametrized tensors  for example  the loop of an rnn with a     parametrized recurrent kernel          code-block   python          with p cache                for x in xs                  out rnn = self rnn cell x  out rnn              global  cache     global  cache enable      cache enable  = 1     try          yield     finally           cache enable -= 1         if not  cache enable               cache =    
def be parametrized module  module  tensor name  optional str  = none  -> bool      r   return ``true`` if module have an active parametrization       if the argument  attr `tensor name` be specify  return ``true`` if     ``module tensor name `` be parametrized       args          module  nn module   module to query         name  str  optional   attribute in the module to query             default  ``none``             parametrizations = getattr module  string  none      if parametrizations be none or not isinstance parametrizations  moduledict           return false     if tensor name be none                   return len parametrizations  > 0     else          return tensor name in parametrizations 
class parametrizationlist modulelist       r   a sequential container that hold and manage the ``original`` parameter or buffer of     a parametrized  class `torch nn module`  it be the type of     ``module parametrizations tensor name `` when ``module tensor name `` have be parametrized     with  func `register parametrization`          note            this class be use internally by  func `register parametrization`  it be document         here for completeness  it should not be instantiate by the user       args          modules  iterable   an iterable of modules represent the parametrizations         original  parameter or tensor   parameter or buffer that be parametrized             original  tensor      def   init            self  modules  iterable module   original  union tensor  parameter        -> none          super     init   modules          if isinstance original  parameter               self register parameter string  original          else              self register buffer string  original       def set original  self  value  tensor  -> none          r   this method be call when assign to a parametrized tensor           it call the methods ``right inverse``  see  func `register parametrization`          of the parametrizations in the inverse order that they have be register          then  it assign the result to ``self original``           args              value  tensor   value to which initialize the module          raise              runtimeerror  if any of the parametrizations do not implement a ``right inverse`` method                     with torch no grad                             for module in reverse self                     if hasattr module  string                       value = module right inverse value                  else                      raise runtimeerror                          string                         string                         string format module   class     name                                      self original copy  value       def forward self  -> tensor          x = self original         for module in self              x = module x          if x size    = self original size                raise runtimeerror                  string                 string                 string format self original size    x size                          return x 
class packedsequence packedsequence        r   hold the data and list of  attr `batch sizes` of a pack sequence       all rnn modules accept pack sequence as input       note          instance of this class should never be create manually  they be mean         to be instantiate by function like  func `pack pad sequence`           batch size represent the number elements at each sequence step in         the batch  not the vary sequence lengths pass to          func `pack pad sequence`   for instance  give data ``abc`` and ``x``         the  class `packedsequence` would contain data ``axbc`` with         ``batch sizes= 2 1 1 ``       attribute          data  tensor   tensor contain pack sequence         batch size  tensor   tensor of integers hold             information about the batch size at each sequence step         sort indices  tensor  optional   tensor of integers hold how this              class `packedsequence` be construct from sequence          unsorted indices  tensor  optional   tensor of integers hold how this             to recover the original sequence with correct order          note            attr `data` can be on arbitrary device and of arbitrary dtype           attr `sorted indices` and  attr `unsorted indices` must be ``torch int64``         tensors on the same device as  attr `data`           however   attr `batch sizes` should always be a cpu ``torch int64`` tensor           this invariant be maintain throughout  class `packedsequence` class          and all function that construct a ` class packedsequence` in pytorch          i e   they only pass in tensors conform to this constraint                def   new   cls  data  batch sizes=none  sort indices=none  unsorted indices=none           return super packedsequence  cls    new                cls                pack sequence init args data  batch size  sort indices                                          unsorted indices                            def pin memory self                             return type self  self data pin memory    self batch size                            bind self sort indices  lambda t  t pin memory                               bind self unsorted indices  lambda t  t pin memory          def cuda self   args    kwargs                    ex = torch tensor     dtype=self data dtype  device=self data device  to  args    kwargs          if ex be cuda              return self to  args    kwargs          return self to  args  device=string    kwargs       def cpu self   args    kwargs            ex = torch tensor     dtype=self data dtype  device=self data device  to  args    kwargs          if ex device type == string              return self to  args    kwargs          return self to  args  device=string    kwargs       def double self           return self to dtype=torch double       def float self           return self to dtype=torch float       def half self           return self to dtype=torch half       def long self           return self to dtype=torch long       def int self           return self to dtype=torch int       def short self           return self to dtype=torch short       def char self           return self to dtype=torch int8       def byte self           return self to dtype=torch uint8       def to self   args    kwargs           r   perform dtype and/or device conversion on `self data`           it have similar signature as  meth `torch tensor to`  except optional         arguments like `non blocking` and `copy` should be pass as kwargs          not args  or they will not apply to the index tensors              note                if the ``self data`` tensor already have the correct  class `torch dtype`             and  class `torch device`  then ``self`` be return              otherwise  return a copy with the desire configuration                                         data = self data to  args    kwargs          if data be self data              return self         else                           kwargs =  k   v for k  v in filter lambda t  t 0   = string and t 0   = string  kwargs items                 sort indices = bind self sort indices  lambda t  t to data device    kwargs               unsorted indices = bind self unsorted indices  lambda t  t to data device    kwargs               return type self  data  self batch size  sort indices  unsorted indices        property     def be cuda self           r   return true if `self data` store on a gpu            return self data be cuda      def be pin self           r   return true if `self data` store on in pin memory            return self data be pin   
def pack pad sequence input  lengths  batch first=false  enforce sorted=true            r   pack a tensor contain pad sequence of variable length        attr `input` can be of size ``t x b x  `` where `t` be the length of the     longest sequence  equal to ``lengths 0 ``   ``b`` be the batch size  and     `` `` be any number of dimension  include 0   if ``batch first`` be     ``true``  ``b x t x  ``  attr `input` be expect       for unsorted sequence  use `enforce sort = false`  if  attr `enforce sorted` be     ``true``  the sequence should be sort by length in a decrease order  i e      ``input   0 `` should be the longest sequence  and ``input   b-1 `` the shortest     one  `enforce sort = true` be only necessary for onnx export       note          this function accept any input that have at least two dimension  you         can apply it to pack the label  and use the output of the rnn with         them to compute the loss directly  a tensor can be retrieve from         a  class `packedsequence` object by access its `` data`` attribute       args          input  tensor   pad batch of variable length sequence          lengths  tensor or list int    list of sequence lengths of each batch             element  must be on the cpu if provide as a tensor           batch first  bool  optional   if ``true``  the input be expect in ``b x t x  ``             format          enforce sort  bool  optional   if ``true``  the input be expect to             contain sequence sort by length in a decrease order  if             ``false``  the input will get sort unconditionally  default  ``true``       return          a  class `packedsequence` object             if torch  c  get trace state   and not isinstance lengths  torch tensor           warn warn string                       string                       string                       string                        stacklevel=2      lengths = torch as tensor lengths  dtype=torch int64      if enforce sort          sort indices = none     else          lengths  sort indices = torch sort lengths  descending=true          sort indices = sort indices to input device          batch dim = 0 if batch first else 1         input = input index select batch dim  sort indices       data  batch size = \          vf  pack pad sequence input  lengths  batch first      return  pack sequence init data  batch size  sort indices  none  
def pad pack sequence sequence  batch first=false  pad value=0 0  total length=none            r   pad a pack batch of variable length sequence       it be an inverse operation to  func `pack pad sequence`       the return tensor s data will be of size ``t x b x  ``  where `t` be the length     of the longest sequence and `b` be the batch size  if ``batch first`` be true      the data will be transpose into ``b x t x  `` format       example          >>> from torch nn utils rnn import pack pad sequence  pad pack sequence         >>> seq = torch tensor   1 2 0    3 0 0    4 5 6            >>> lens =  2  1  3          >>> pack = pack pad sequence seq  lens  batch first=true  enforce sorted=false          >>> pack         packedsequence data=tensor  4  1  3  5  2  6    batch sizes=tensor  3  2  1                           sort indices=tensor  2  0  1    unsorted indices=tensor  1  2  0            >>> seq unpack  lens unpack = pad pack sequence pack  batch first=true          >>> seq unpack         tensor   1  2  0                    3  0  0                    4  5  6            >>> lens unpack         tensor  2  1  3           note            attr `total length` be useful to implement the         ``pack sequence -> recurrent network -> unpack sequence`` pattern in a          class `~torch nn module` wrap in  class `~torch nn dataparallel`          see  ref `this faq section <pack-rnn-unpack-with-data-parallelism>` for         detail       args          sequence  packedsequence   batch to pad         batch first  bool  optional   if ``true``  the output will be in ``b x t x  ``             format          pad value  float  optional   value for pad elements          total length  int  optional   if not ``none``  the output will be pad to             have length  attr `total length`  this method will throw  class `valueerror`             if  attr `total length` be less than the max sequence length in              attr `sequence`       return          tuple of tensor contain the pad sequence  and a tensor         contain the list of lengths of each sequence in the batch          batch elements will be re-ordered as they be order originally when         the batch be pass to ``pack pad sequence`` or ``pack sequence``                  max seq length = sequence batch size size 0      if total length be not none          if total length < max seq length              raise valueerror string                              string                              string                               format total length  max seq length           max seq length = total length     pad output  lengths =  vf  pad pack sequence          sequence data  sequence batch size  batch first  pad value  max seq length      unsorted indices = sequence unsorted indices     if unsorted indices be not none          batch dim = 0 if batch first else 1         return pad output index select batch dim  unsorted indices   lengths unsorted indices      return pad output  lengths 
def pad sequence sequence  batch first=false  pad value=0 0            r   pad a list of variable length tensors with ``padding value``      ``pad sequence`` stack a list of tensors along a new dimension      and pad them to equal length  for example  if the input be list of     sequence with size ``l x  `` and if batch first be false  and ``t x b x  ``     otherwise       `b` be batch size  it be equal to the number of elements in ``sequences``      `t` be length of the longest sequence      `l` be length of the sequence      ` ` be any number of trail dimension  include none       example          >>> from torch nn utils rnn import pad sequence         >>> a = torch ones 25  300          >>> b = torch ones 22  300          >>> c = torch ones 15  300          >>> pad sequence  a  b  c   size           torch size  25  3  300        note          this function return a tensor of size ``t x b x  `` or ``b x t x  ``         where `t` be the length of the longest sequence  this function assume         trail dimension and type of all the tensors in sequence be same       args          sequence  list tensor    list of variable length sequence          batch first  bool  optional   output will be in ``b x t x  `` if true  or in             ``t x b x  `` otherwise         pad value  float  optional   value for pad elements  default  0       return          tensor of size ``t x b x  `` if  attr `batch first` be ``false``          tensor of size ``b x t x  `` otherwise                        return torch  c  nn pad sequence sequence  batch first  pad value  
def pack sequence sequence  enforce sorted=true            r   pack a list of variable length tensors      ``sequences`` should be a list of tensors of size ``l x  ``  where `l` be     the length of a sequence and ` ` be any number of trail dimension      include zero       for unsorted sequence  use `enforce sort = false`  if ``enforce sorted``     be ``true``  the sequence should be sort in the order of decrease length      ``enforce sort = true`` be only necessary for onnx export        example          >>> from torch nn utils rnn import pack sequence         >>> a = torch tensor  1 2 3           >>> b = torch tensor  4 5           >>> c = torch tensor  6           >>> pack sequence  a  b  c           packedsequence data=tensor   1   4   6   2   5   3    batch sizes=tensor   3   2   1          args          sequence  list tensor    a list of sequence of decrease length          enforce sort  bool  optional   if ``true``  check that the input             contain sequence sort by length in a decrease order  if             ``false``  this condition be not check  default  ``true``       return          a  class `packedsequence` object             lengths = torch as tensor  v size 0  for v in sequence       return pack pad sequence pad sequence sequence   lengths  enforce sorted=enforce sort  
class flatten module       r        flatten a contiguous range of dim into a tensor  for use with  class `~nn sequential`       shape          - input   math ` n   dim `         - output   math ` n  \prod  dim `  for the default case        args          start dim  first dim to flatten  default = 1           end dim  last dim to flatten  default = -1        examples           >>> input = torch randn 32  1  5  5          >>> m = nn sequential          >>>     nn conv2d 1  32  5  1  1           >>>     nn flatten           >>>           >>> output = m input          >>> output size           torch size  32  288                 constants   =  string  string      start dim  int     end dim  int      def   init   self  start dim  int = 1  end dim  int = -1  -> none          super flatten  self    init             self start dim = start dim         self end dim = end dim      def forward self  input  tensor  -> tensor          return input flatten self start dim  self end dim       def extra repr self  -> str          return string format              self start dim  self end dim           
class unflatten module       r        unflattens a tensor dim expand it to a desire shape  for use with  class `~nn sequential`          attr `dim` specify the dimension of the input tensor to be unflattened  and it can       be either `int` or `str` when `tensor` or `namedtensor` be use  respectively          attr `unflattened size` be the new shape of the unflattened dimension of the tensor and it can be       a `tuple` of ints or a `list` of ints or `torch size` for `tensor` input   a `namedshape`        tuple of ` name  size ` tuples  for `namedtensor` input       shape          - input   math ` n   dim `         - output   math ` n  c  \text out    h  \text out    w  \text out   `      args          dim  union int  str    dimension to be unflattened         unflattened size  union torch size  tuple  list  namedshape    new shape of the unflattened dimension      examples          >>> input = torch randn 2  50          >>>   with tuple of ints         >>> m = nn sequential          >>>     nn linear 50  50           >>>     nn unflatten 1   2  5  5           >>>           >>> output = m input          >>> output size           torch size  2  2  5  5           >>>   with torch size         >>> m = nn sequential          >>>     nn linear 50  50           >>>     nn unflatten 1  torch size  2  5  5            >>>           >>> output = m input          >>> output size           torch size  2  2  5  5           >>>   with namedshape  tuple of tuples          >>> input = torch randn 2  50  names=  n    feature            >>> unflatten = nn unflatten  feature      c   2     h   5     w   5            >>> output = unflatten input          >>> output size           torch size  2  2  5  5               namedshape = tuple tuple str  int          constants   =  string  string      dim  union int  str      unflattened size  union  size  namedshape       def   init   self  dim  union int  str   unflattened size  union  size  namedshape   -> none          super unflatten  self    init              if isinstance dim  int               self  require tuple int unflattened size          elif isinstance dim  str               self  require tuple tuple unflattened size          else              raise typeerror string           self dim = dim         self unflattened size = unflattened size      def  require tuple tuple self  input           if  isinstance input  tuple                for idx  elem in enumerate input                   if not isinstance elem  tuple                       raise typeerror string                                       string format type elem    name    idx               return         raise typeerror string                           string format type input    name          def  require tuple int self  input           if  isinstance input   tuple  list                 for idx  elem in enumerate input                   if not isinstance elem  int                       raise typeerror string                                       string format type elem    name    idx               return         raise typeerror string format type input    name          def forward self  input  tensor  -> tensor          return input unflatten self dim  self unflattened size       def extra repr self  -> str          return string format self dim  self unflattened size  
class lazymodulemixin      r   a mixin for modules that lazily initialize parameters  also know as  lazy modules           warn          lazy modules be an experimental new feature under active development          and their api be likely to change       modules that lazily initialize parameters  or  lazy modules       derive the shape of their parameters from the first input s      to their forward method  until that first forward they contain      class `torch nn uninitializedparameter` s that should not be access     or use  and afterward they contain regular  class `torch nn parameter` s      lazy modules be convenient since they don t require compute some     module arguments  like the  attr `in features` argument of a     typical  class `torch nn linear`       after construction  network with lazy modules should first     be convert to the desire dtype and place on the expect device      this be because lazy modules only perform shape inference so the usual dtype     and device placement behavior apply      the lazy modules should then perform  dry run  to initialize all the components in the module      these  dry run  send input of the correct size  dtype  and device through     the network and to each one of its lazy modules  after this the network can be use as usual       >>> class lazymlp torch nn module              def   init   self                  super     init                    self fc1 = torch nn lazylinear 10                 self relu1 = torch nn relu                  self fc2 = torch nn lazylinear 1                 self relu2 = torch nn relu                      def forward self  input                  x = self relu1 self fc1 input                  y = self relu2 self fc2 x                  return y     >>>   construct a network with lazy modules     >>> lazy mlp = lazymlp       >>>   transform the network s device and dtype     >>>   note  these transform can and should be apply after construction and before any  dry run      >>> lazy mlp = mlp cuda   double       >>> lazy mlp     lazymlp   fc1   lazylinear in features=0  out features=10  bias=true         relu1   relu          fc2   lazylinear in features=0  out features=1  bias=true         relu2   relu             >>>   perform a dry run to initialize the network s lazy modules     >>> lazy mlp torch ones 10 10  cuda        >>>   after initialization  lazylinear modules become regular linear modules     >>> lazy mlp     lazymlp         fc1   linear in features=10  out features=10  bias=true         relu1   relu          fc2   linear in features=10  out features=1  bias=true         relu2   relu             >>>   attach an optimizer  since parameters can now be use as usual     >>> optim = torch optim sgd mlp parameters    lr=0 01       a final caveat when use lazy modules be that the order of initialization of a network s     parameters may change  since the lazy modules be always initialize after other modules      for example  if the lazymlp class define above have a  class `torch nn lazylinear` module     first and then a regular  class `torch nn linear` second  the second module would be     initialize on construction and the first module would be initialize during the first dry run      this can cause the parameters of a network use lazy modules to be initialize differently     than the parameters of a network without lazy modules as the order of parameter initializations      which often depend on a stateful random number generator  be different      check  doc `/notes/randomness` for more detail       lazy modules can be serialize with a state dict like other modules  for example       >>> lazy mlp = lazymlp       >>>   the state dict show the uninitialized parameters     >>> lazy mlp state dict       ordereddict    fc1 weight   uninitialized parameter                      fc1 bias                     tensor  -1 8832e 25   4 5636e-41  -1 8832e 25   4 5636e-41  -6 1598e-30                             4 5637e-41  -1 8788e 22   4 5636e-41  -2 0042e-31   4 5637e-41                        fc2 weight   uninitialized parameter                      fc2 bias   tensor  0 0019            lazy modules can load regular  class `torch nn parameter` s  i e  you can serialize/deserialize     initialize lazymodules and they will remain initialize        >>> full mlp = lazymlp       >>>   dry run to initialize another module     >>> full mlp forward torch ones 10  1       >>>   load an initialize state into a lazy module     >>> lazy mlp load state dict full mlp state dict        >>>   the state dict now hold valid value     >>> lazy mlp state dict       ordereddict    fc1 weight                     tensor   -0 3837                               0 0907                               0 6708                              -0 5223                              -0 9028                               0 2851                              -0 4537                               0 6813                               0 5766                              -0 8678                         fc1 bias                     tensor  -1 8832e 25   4 5636e-41  -1 8832e 25   4 5636e-41  -6 1598e-30                             4 5637e-41  -1 8788e 22   4 5636e-41  -2 0042e-31   4 5637e-41                        fc2 weight                     tensor    0 1320   0 2938   0 0679   0 2793   0 1088  -0 1795  -0 2301   0 2807                              0 2479   0 1091                         fc2 bias   tensor  0 0019           note  however  that the load parameters will not be replace when do a  dry run  if they be initialize     when the state be load  this prevent use initialize modules in different contexts                         cls to become = none      def   init   self   lazyprotocol   args    kwargs                    super     init    args    kwargs            self  load hook = self  register load state dict pre hook self  lazy load hook          self  initialize hook = self register forward pre hook self  infer parameters          warn warn string                       string       def  save to state dict self   lazyprotocol  destination  prefix  keep vars                                      for name  param in self  parameters items                if param be not none                  if not  be lazy param  or keep vars                       param = param detach                   destination prefix   name  = param         for name  buf in self  buffer items                if buf be not none and name not in self  non persistent buffer set                  if not  be lazy buf  or keep vars                       buf = buf detach                   destination prefix   name  = buf      def  lazy load hook              self   lazyprotocol  state dict  prefix  local metadata  strict              miss key  unexpected key  error msgs           string         for name  param in itertools chain self  parameters items    self  buffer items                 key = prefix   name             if key in state dict and param be not none                  input param = state dict key                  if be lazy param                                                                 if not be lazy input param                           with torch no grad                                param materialize input param shape       def initialize parameters self   lazyprotocol   args    kwargs           r   initialize parameters accord to the input batch properties          this add an interface to isolate parameter initialization from the         forward pass when do parameter shape inference                      raise notimplementederror string format self   class     name          def have uninitialized params self   lazyprotocol           r   check if a module have parameters that be not initialize                                       params = self  parameters value           buffer = self  buffer value           for param in itertools chain params  buffer               if be lazy param                   return true         return false      def  infer parameters self   lazyprotocol  module  input           r   infer the size and initialize the parameters accord to the         provide input batch          give a module that contain parameters that be declare inferrable         use  class `torch nn parameter parametermode infer`  run a forward pass         in the complete module use the provide input to initialize all the parameters         as need          the module be set into evaluation mode before run the forward pass in order         to avoid save statistics or calculate gradients                     module initialize parameters  input          if module have uninitialized params                raise runtimeerror string format self  get name             module  initialize hook remove           module  load hook remove           delattr module  string          delattr module  string          if module cls to become be not none              module   class   = module cls to become       def  replicate for data parallel self   lazyprotocol           raise runtimeerror string                            string  
def unfold      input  tensor  kernel size  broadcastinglist2 int       dilation  broadcastinglist2 int  = 1      pad  broadcastinglist2 int  = 0      stride  broadcastinglist2 int  = 1   -> tensor      r   extract slide local block from a batch input tensor          warn           currently  only 4-d input tensors  batch image-like tensors  be         support          warn            more than one element of the unfold tensor may refer to a single         memory location  as a result  in-place operations  especially ones that         be vectorized  may result in incorrect behavior  if you need to write         to the tensor  please clone it first        see  class `torch nn unfold` for detail             if have torch function unary input           return handle torch function              unfold   input    input  kernel size  dilation=dilation  padding=padding  stride=stride               if input dim   == 4          msg = string         assert int or pair kernel size  string  msg          assert int or pair dilation  string  msg          assert int or pair pad  string  msg          assert int or pair stride  string  msg           return torch  c  nn im2col input   pair kernel size    pair dilation    pair pad    pair stride       else          raise notimplementederror string format input dim     
def fold      input  tensor  output size  broadcastinglist2 int       kernel size  broadcastinglist2 int       dilation  broadcastinglist2 int  = 1      pad  broadcastinglist2 int  = 0      stride  broadcastinglist2 int  = 1   -> tensor      r   combine an array of slide local block into a large contain     tensor          warn           currently  only 3-d output tensors  unfold batch image-like tensors  be         support       see  class `torch nn fold` for detail             if have torch function unary input           return handle torch function              fold   input    input  output size  kernel size  dilation=dilation  padding=padding  stride=stride               if input dim   == 3          msg = string         assert int or pair output size  string  msg          assert int or pair kernel size  string  msg          assert int or pair dilation  string  msg          assert int or pair pad  string  msg          assert int or pair stride  string  msg           return torch  c  nn col2im              input   pair output size    pair kernel size    pair dilation    pair pad    pair stride                else          raise notimplementederror string format input dim     
def fn  args    kwargs       dispatch flag = false     if arg name in kwargs          dispatch flag = kwargs arg name      elif arg index < len args           dispatch flag = args arg index       if dispatch flag          return if true  args    kwargs      else          return if false  args    kwargs  
def fn  args    kwargs       dispatch flag = false     if arg name in kwargs          dispatch flag = kwargs arg name      elif arg index < len args           dispatch flag = args arg index       if dispatch flag          return if true  args    kwargs      else          return if false  args    kwargs  
def fn  args    kwargs       dispatch flag = false     if arg name in kwargs          dispatch flag = kwargs arg name      elif arg index < len args           dispatch flag = args arg index       if dispatch flag          return if true  args    kwargs      else          return if false  args    kwargs  
def max unpool1d      input  tensor  indices  tensor      kernel size  broadcastinglist1 int       stride  optional broadcastinglist1 int   = none      pad  broadcastinglist1 int  = 0      output size  optional broadcastinglist1 int   = none   -> tensor      r   compute a partial inverse of  class `maxpool1d`       see  class `~torch nn maxunpool1d` for detail              if have torch function unary input           return handle torch function              max unpool1d               input                input              indices              kernel size              stride=stride              padding=padding              output size=output size                kernel size =  single kernel size      if stride be not none           stride =  single stride      else           stride = kernel size     pad =  single pad      output size =  unpool output size input  kernel size   stride  pad  output size      if isinstance output size  list           output size = output size    1      else          output size = output size    1       return torch  c  nn max unpool2d input unsqueeze 3   indices unsqueeze 3   output size  squeeze 3  
def max unpool2d      input  tensor  indices  tensor      kernel size  broadcastinglist2 int       stride  optional broadcastinglist2 int   = none      pad  broadcastinglist2 int  = 0      output size  optional broadcastinglist2 int   = none   -> tensor      r   compute a partial inverse of  class `maxpool2d`       see  class `~torch nn maxunpool2d` for detail              if have torch function unary input           return handle torch function              max unpool2d               input                input              indices              kernel size              stride=stride              padding=padding              output size=output size                kernel size =  pair kernel size      if stride be not none           stride =  pair stride      else           stride = kernel size     pad =  pair pad      output size =  unpool output size input  kernel size   stride  pad  output size      return torch  c  nn max unpool2d input  indices  output size  
def max unpool3d      input  tensor  indices  tensor      kernel size  broadcastinglist3 int       stride  optional broadcastinglist3 int   = none      pad  broadcastinglist3 int  = 0      output size  optional broadcastinglist3 int   = none   -> tensor      r   compute a partial inverse of  class `maxpool3d`       see  class `~torch nn maxunpool3d` for detail              if have torch function unary input           return handle torch function              max unpool3d               input                input              indices              kernel size              stride=stride              padding=padding              output size=output size                kernel size =  triple kernel size      if stride be not none           stride =  triple stride      else           stride = kernel size     pad =  triple pad      output size =  unpool output size input  kernel size   stride  pad  output size      return torch  c  nn max unpool3d input  indices  output size   stride  pad  
def lp pool1d      input  tensor  norm type  float      kernel size  int      stride  optional broadcastinglist1 int   = none      ceil mode  bool = false   -> tensor      r   apply a 1d power-average pool over an input signal compose of     several input plan  if the sum of all input to the power of `p` be     zero  the gradient be set to zero as well       see  class `~torch nn lppool1d` for detail              if have torch function unary input           return handle torch function              lp pool1d   input    input  norm type  kernel size  stride=stride  ceil mode=ceil mode               if stride be not none          out = avg pool1d input pow norm type   kernel size  stride  0  ceil mode      else          out = avg pool1d input pow norm type   kernel size  padding=0  ceil mode=ceil mode       return  torch sign out    relu torch abs out    mul kernel size  pow 1 0 / norm type  
def lp pool2d      input  tensor  norm type  float      kernel size  int      stride  optional broadcastinglist2 int   = none      ceil mode  bool = false   -> tensor      r   apply a 2d power-average pool over an input signal compose of     several input plan  if the sum of all input to the power of `p` be     zero  the gradient be set to zero as well       see  class `~torch nn lppool2d` for detail              if have torch function unary input           return handle torch function              lp pool2d   input    input  norm type  kernel size  stride=stride  ceil mode=ceil mode               kw  kh = utils  pair kernel size      if stride be not none          out = avg pool2d input pow norm type   kernel size  stride  0  ceil mode      else          out = avg pool2d input pow norm type   kernel size  padding=0  ceil mode=ceil mode       return  torch sign out    relu torch abs out    mul kw   kh  pow 1 0 / norm type  
def fn  args    kwargs       dispatch flag = false     if arg name in kwargs          dispatch flag = kwargs arg name      elif arg index < len args           dispatch flag = args arg index       if dispatch flag          return if true  args    kwargs      else          return if false  args    kwargs  
def fn  args    kwargs       dispatch flag = false     if arg name in kwargs          dispatch flag = kwargs arg name      elif arg index < len args           dispatch flag = args arg index       if dispatch flag          return if true  args    kwargs      else          return if false  args    kwargs  
def fn  args    kwargs       dispatch flag = false     if arg name in kwargs          dispatch flag = kwargs arg name      elif arg index < len args           dispatch flag = args arg index       if dispatch flag          return if true  args    kwargs      else          return if false  args    kwargs  
def adaptive avg pool2d input  tensor  output size  broadcastinglist2 int   -> tensor      r        apply a 2d adaptive average pool over an input signal compose of     several input plan       see  class `~torch nn adaptiveavgpool2d` for detail and output shape       args          output size  the target output size  single integer or             double-integer tuple              if have torch function unary input           return handle torch function adaptive avg pool2d   input    input  output size       output size =  list with default output size  input size        return torch  c  nn adaptive avg pool2d input   output size  
def adaptive avg pool3d input  tensor  output size  broadcastinglist3 int   -> tensor      r        apply a 3d adaptive average pool over an input signal compose of     several input plan       see  class `~torch nn adaptiveavgpool3d` for detail and output shape       args          output size  the target output size  single integer or             triple-integer tuple              if have torch function unary input           return handle torch function adaptive avg pool3d   input    input  output size       output size =  list with default output size  input size        return torch  c  nn adaptive avg pool3d input   output size  
def fn  args    kwargs       dispatch flag = false     if arg name in kwargs          dispatch flag = kwargs arg name      elif arg index < len args           dispatch flag = args arg index       if dispatch flag          return if true  args    kwargs      else          return if false  args    kwargs  
def fn  args    kwargs       dispatch flag = false     if arg name in kwargs          dispatch flag = kwargs arg name      elif arg index < len args           dispatch flag = args arg index       if dispatch flag          return if true  args    kwargs      else          return if false  args    kwargs  
def  threshold input  tensor  threshold  float  value  float  inplace  bool = false  -> tensor      r   thresholds each element of the input tensor       see  class `~torch nn threshold` for more detail              if have torch function unary input           return handle torch function  threshold   input    input  threshold  value  inplace=inplace      if inplace          result =  vf threshold  input  threshold  value      else          result =  vf threshold input  threshold  value      return result 
def relu input  tensor  inplace  bool = false  -> tensor      r   relu input  inplace=false  -> tensor      apply the rectify linear unit function element-wise  see      class `~torch nn relu` for more detail              if have torch function unary input           return handle torch function relu   input    input  inplace=inplace      if inplace          result = torch relu  input      else          result = torch relu input      return result 
def hardtanh input  tensor  min val  float = -1 0  max val  float = 1 0  inplace  bool = false  -> tensor      r        hardtanh input  min val=-1   max val=1   inplace=false  -> tensor      apply the hardtanh function element-wise  see  class `~torch nn hardtanh` for more     detail              if have torch function unary input           return handle torch function hardtanh   input    input  min val=min val  max val=max val  inplace=inplace      if inplace          result = torch  c  nn hardtanh  input  min val  max val      else          result = torch  c  nn hardtanh input  min val  max val      return result 
def hardswish input  tensor  inplace  bool = false  -> tensor      r   apply the hardswish function  element-wise  as describe in the paper       `searching for mobilenetv3`           math           \text hardswish  x  = \begin case              0   \text if~  x \le -3  \\             x   \text if~  x \ge  3  \\             x \cdot  x   3  /6   \text otherwise          \end case       see  class `~torch nn hardswish` for more detail           `searching for mobilenetv3`          https //arxiv org/abs/1905 02244             if have torch function unary input           return handle torch function hardswish   input    input  inplace=inplace      if inplace          return torch  c  nn hardswish  input      return torch  c  nn hardswish input  
def relu6 input  tensor  inplace  bool = false  -> tensor      r   relu6 input  inplace=false  -> tensor      apply the element-wise function  math `\text relu6  x  = \min \max 0 x   6 `       see  class `~torch nn relu6` for more detail              if have torch function unary input           return handle torch function relu6   input    input  inplace=inplace      if inplace          result = torch  c  nn relu6  input      else          result = torch  c  nn relu6 input      return result 
def elu input  tensor  alpha  float = 1 0  inplace  bool = false  -> tensor      r   apply element-wise       math `\text elu  x  = \max 0 x    \min 0  \alpha    \exp x  - 1  `       see  class `~torch nn elu` for more detail              if have torch function unary input           return handle torch function elu   input    input  alpha=alpha  inplace=inplace      if inplace          result = torch  c  nn elu  input  alpha      else          result = torch  c  nn elu input  alpha      return result 
def selu input  tensor  inplace  bool = false  -> tensor      r   selu input  inplace=false  -> tensor      apply element-wise       math `\text selu  x  = scale    \max 0 x    \min 0  \alpha    \exp x  - 1   `      with  math `\alpha=1 6732632423543772848170429916717` and      math `scale=1 0507009873554804934193349852946`       see  class `~torch nn selu` for more detail              if have torch function unary input           return handle torch function selu   input    input  inplace=inplace      if inplace          result = torch selu  input      else          result = torch selu input      return result 
def celu input  tensor  alpha  float = 1 0  inplace  bool = false  -> tensor      r   celu input  alpha=1   inplace=false  -> tensor      apply element-wise       math `\text celu  x  = \max 0 x    \min 0  \alpha    \exp x/\alpha  - 1  `       see  class `~torch nn celu` for more detail              if have torch function unary input           return handle torch function celu   input    input  alpha=alpha  inplace=inplace      if inplace          result = torch celu  input  alpha      else          result = torch celu input  alpha      return result 
def leaky relu input  tensor  negative slope  float = 0 01  inplace  bool = false  -> tensor      r        leaky relu input  negative slope=0 01  inplace=false  -> tensor      apply element-wise       math `\text leakyrelu  x  = \max 0  x    \text negative\ slope    \min 0  x `      see  class `~torch nn leakyrelu` for more detail              if have torch function unary input           return handle torch function leaky relu   input    input  negative slope=negative slope  inplace=inplace      if inplace          result = torch  c  nn leaky relu  input  negative slope      else          result = torch  c  nn leaky relu input  negative slope      return result 
def prelu input  tensor  weight  tensor  -> tensor      r   prelu input  weight  -> tensor      apply element-wise the function      math `\text prelu  x  = \max 0 x    \text weight    \min 0 x ` where weight be a     learnable parameter       see  class `~torch nn prelu` for more detail              if have torch function unary input           return handle torch function prelu   input    input  weight      return torch prelu input  weight  
def rrelu      input  tensor  lower  float = 1 0 / 8  upper  float = 1 0 / 3  train  bool = false  inplace  bool = false   -> tensor      r   rrelu input  lower=1 /8  upper=1 /3  training=false  inplace=false  -> tensor      randomize leaky relu       see  class `~torch nn rrelu` for more detail              if have torch function unary input           return handle torch function              rrelu   input    input  lower=lower  upper=upper  training=training  inplace=inplace               if inplace          result = torch rrelu  input  lower  upper  train      else          result = torch rrelu input  lower  upper  train      return result 
def glu input  tensor  dim  int = -1  -> tensor      r        glu input  dim=-1  -> tensor      the gate linear unit  compute          math            \text glu  a  b  = a \otimes \sigma b       where `input` be split in half along `dim` to form `a` and `b`   math `\sigma`     be the sigmoid function and  math `\otimes` be the element-wise product between matrices       see `language model with gate convolutional network <https //arxiv org/abs/1612 08083>`        args          input  tensor   input tensor         dim  int   dimension on which to split the input  default  -1             if have torch function unary input           return handle torch function glu   input    input  dim=dim      if input dim   == 0          raise runtimeerror string      return torch  c  nn glu input  dim  
def gelu input       r   gelu input  -> tensor      apply element-wise the function      math `\text gelu  x  = x   \phi x `      where  math `\phi x ` be the cumulative distribution function for gaussian distribution       see `gaussian error linear units  gelus  <https //arxiv org/abs/1606 08415>`               if have torch function unary input           return handle torch function gelu   input    input      return torch  c  nn gelu input  
def hardshrink input  tensor  lambd  float = 0 5  -> tensor      r        hardshrink input  lambd=0 5  -> tensor      apply the hard shrinkage function element-wise      see  class `~torch nn hardshrink` for more detail              if have torch function unary input           return handle torch function hardshrink   input    input  lambd=lambd      return torch hardshrink input  lambd  
def tanhshrink input       r   tanhshrink input  -> tensor      apply element-wise   math `\text tanhshrink  x  = x - \text tanh  x `      see  class `~torch nn tanhshrink` for more detail              if have torch function unary input           return handle torch function tanhshrink   input    input      return input - input tanh   
def softsign input       r   softsign input  -> tensor      apply element-wise  the function  math `\text softsign  x  = \frac x  1    x  `      see  class `~torch nn softsign` for more detail              if have torch function unary input           return handle torch function softsign   input    input      return input /  input abs     1  
def softmin input  tensor  dim  optional int  = none   stacklevel  int = 3  dtype  optional int  = none  -> tensor      r   apply a softmin function       note that  math `\text softmin  x  = \text softmax  -x `  see softmax definition for mathematical formula       see  class `~torch nn softmin` for more detail       args          input  tensor   input         dim  int   a dimension along which softmin will be compute  so every slice             along dim will sum to 1           dtype   class `torch dtype`  optional   the desire data type of return tensor            if specify  the input tensor be cast to  attr `dtype` before the operation           be perform  this be useful for prevent data type overflow  default  none              if have torch function unary input           return handle torch function softmin   input    input  dim=dim   stacklevel= stacklevel  dtype=dtype      if dim be none          dim =  get softmax dim string  input dim     stacklevel      if dtype be none          ret =  -input  softmax dim      else          ret =  -input  softmax dim  dtype=dtype      return ret 
def softmax input  tensor  dim  optional int  = none   stacklevel  int = 3  dtype  optional int  = none  -> tensor      r   apply a softmax function       softmax be define as        math `\text softmax  x  i   = \frac \exp x i   \sum j \exp x j  `      it be apply to all slice along dim  and will re-scale them so that the elements     lie in the range ` 0  1 ` and sum to 1       see  class `~torch nn softmax` for more detail       args          input  tensor   input         dim  int   a dimension along which softmax will be compute          dtype   class `torch dtype`  optional   the desire data type of return tensor            if specify  the input tensor be cast to  attr `dtype` before the operation           be perform  this be useful for prevent data type overflow  default  none          note           this function doesn t work directly with nllloss          which expect the log to be compute between the softmax and itself          use log softmax instead  it s faster and have better numerical properties                if have torch function unary input           return handle torch function softmax   input    input  dim=dim   stacklevel= stacklevel  dtype=dtype      if dim be none          dim =  get softmax dim string  input dim     stacklevel      if dtype be none          ret = input softmax dim      else          ret = input softmax dim  dtype=dtype      return ret 
def gumbel softmax logits  tensor  tau  float = 1  hard  bool = false  eps  float = 1e-10  dim  int = -1  -> tensor      r        sample from the gumbel-softmax distribution  `link 1`   `link 2`   and optionally discretizes       args        logits  `      num feature ` unnormalized log probabilities       tau  non-negative scalar temperature       hard  if ``true``  the return sample will be discretized as one-hot vectors              but will be differentiate as if it be the soft sample in autograd       dim  int   a dimension along which softmax will be compute  default  -1       return        sample tensor of same shape as `logits` from the gumbel-softmax distribution        if ``hard=true``  the return sample will be one-hot  otherwise they will       be probability distributions that sum to 1 across `dim`          note         this function be here for legacy reason  may be remove from nn functional in the future          note         the main trick for `hard` be to do  `y hard - y soft detach     y soft`        it achieve two things        - make the output value exactly one-hot        since we add then subtract y soft value        - make the gradient equal to y soft gradient        since we strip all other gradients       examples           >>> logits = torch randn 20  32          >>>   sample soft categorical use reparametrization trick          >>> f gumbel softmax logits  tau=1  hard=false          >>>   sample hard categorical use  straight-through  trick          >>> f gumbel softmax logits  tau=1  hard=true           link 1          https //arxiv org/abs/1611 00712         link 2          https //arxiv org/abs/1611 01144             if have torch function unary logits           return handle torch function gumbel softmax   logits    logits  tau=tau  hard=hard  eps=eps  dim=dim      if eps  = 1e-10          warn warn string       gumbels =           -torch empty like logits  memory format=torch legacy contiguous format  exponential    log               gumbels =  logits   gumbels  / tau       y soft = gumbels softmax dim       if hard                   index = y soft max dim  keepdim=true  1          y hard = torch zero like logits  memory format=torch legacy contiguous format  scatter  dim  index  1 0          ret = y hard - y soft detach     y soft     else                   ret = y soft     return ret 
def log softmax input  tensor  dim  optional int  = none   stacklevel  int = 3  dtype  optional int  = none  -> tensor      r   apply a softmax follow by a logarithm       while mathematically equivalent to log softmax x    do these two     operations separately be slower  and numerically unstable  this function     use an alternative formulation to compute the output and gradient correctly       see  class `~torch nn logsoftmax` for more detail       args          input  tensor   input         dim  int   a dimension along which log softmax will be compute          dtype   class `torch dtype`  optional   the desire data type of return tensor            if specify  the input tensor be cast to  attr `dtype` before the operation           be perform  this be useful for prevent data type overflow  default  none              if have torch function unary input           return handle torch function log softmax   input    input  dim=dim   stacklevel= stacklevel  dtype=dtype      if dim be none          dim =  get softmax dim string  input dim     stacklevel      if dtype be none          ret = input log softmax dim      else          ret = input log softmax dim  dtype=dtype      return ret 
def tanh input       r   tanh input  -> tensor      apply element-wise       math `\text tanh  x  = \tanh x  = \frac \exp x  - \exp -x   \exp x    \exp -x  `      see  class `~torch nn tanh` for more detail              warn warn string      return input tanh   
def sigmoid input       r   sigmoid input  -> tensor      apply the element-wise function  math `\text sigmoid  x  = \frac 1  1   \exp -x  `      see  class `~torch nn sigmoid` for more detail              warn warn string      return input sigmoid   
def hardsigmoid input  tensor  inplace  bool = false  -> tensor      r   apply the element-wise function         math           \text hardsigmoid  x  = \begin case              0   \text if~  x \le -3  \\             1   \text if~  x \ge  3  \\             x / 6   1 / 2   \text otherwise          \end case       args          inplace  if set to ``true``  will do this operation in-place  default  ``false``      see  class `~torch nn hardsigmoid` for more detail              if have torch function unary input           return handle torch function hardsigmoid   input    input  inplace=inplace      if inplace          return torch  c  nn hardsigmoid  input      return torch  c  nn hardsigmoid input  
def silu input  tensor  inplace  bool = false  -> tensor      r   apply the sigmoid linear unit  silu  function  element-wise      the silu function be also know as the swish function          math           \text silu  x  = x   \sigma x   \text where   \sigma x  \text  be the logistic sigmoid           note           see `gaussian error linear units  gelus  <https //arxiv org/abs/1606 08415>`          where the silu  sigmoid linear unit  be originally coin  and see         `sigmoid-weighted linear units for neural network function approximation         in reinforcement learn <https //arxiv org/abs/1702 03118>`  and `swish          a self-gated activation function <https //arxiv org/abs/1710 05941v1>`          where the silu be experiment with later       see  class `~torch nn silu` for more detail              if have torch function unary input           return handle torch function silu   input    input  inplace=inplace      if inplace          return torch  c  nn silu  input      return torch  c  nn silu input  
def mish input  tensor  inplace  bool = false  -> tensor      r   apply the mish function  element-wise      mish  a self regularize non-monotonic neural activation function          math           \text mish  x  = x   \text tanh  \text softplus  x           note           see `mish  a self regularize non-monotonic neural activation function <https //arxiv org/abs/1908 08681>`       see  class `~torch nn mish` for more detail              if have torch function unary input           return handle torch function mish   input    input  inplace=inplace      if inplace          return torch  c  nn mish  input      return torch  c  nn mish input  
def batch norm      input  tensor      run mean  optional tensor       run var  optional tensor       weight  optional tensor  = none      bias  optional tensor  = none      train  bool = false      momentum  float = 0 1      eps  float = 1e-5    -> tensor      r   apply batch normalization for each channel across a batch of data       see  class `~torch nn batchnorm1d`   class `~torch nn batchnorm2d`       class `~torch nn batchnorm3d` for detail              if have torch function unary input           return handle torch function              batch norm               input                input              run mean              run var              weight=weight              bias=bias              training=training              momentum=momentum              eps=eps                if train           verify batch size input size         return torch batch norm          input  weight  bias  run mean  run var  train  momentum  eps  torch backends cudnn enable       
def group norm      input  tensor  num group  int  weight  optional tensor  = none  bias  optional tensor  = none  eps  float = 1e-5   -> tensor      r   apply group normalization for last certain number of dimension       see  class `~torch nn groupnorm` for detail              if have torch function unary input           return handle torch function group norm   input    input  num group  weight=weight  bias=bias  eps=eps       verify batch size  input size 0    input size 1  // num group  num group    list input size   2         return torch group norm input  num group  weight  bias  eps  torch backends cudnn enable  
def instance norm      input  tensor      run mean  optional tensor  = none      run var  optional tensor  = none      weight  optional tensor  = none      bias  optional tensor  = none      use input stats  bool = true      momentum  float = 0 1      eps  float = 1e-5    -> tensor      r   apply instance normalization for each channel in each data sample in a     batch       see  class `~torch nn instancenorm1d`   class `~torch nn instancenorm2d`       class `~torch nn instancenorm3d` for detail              if have torch function unary input           return handle torch function              instance norm               input                input              run mean=running mean              run var=running var              weight=weight              bias=bias              use input stats=use input stats              momentum=momentum              eps=eps                if use input stats           verify spatial size input size        return torch instance norm          input  weight  bias  run mean  run var  use input stats  momentum  eps  torch backends cudnn enable       
def layer norm      input  tensor      normalize shape  list int       weight  optional tensor  = none      bias  optional tensor  = none      eps  float = 1e-5    -> tensor      r   apply layer normalization for last certain number of dimension       see  class `~torch nn layernorm` for detail              if have torch function unary input           return handle torch function              layer norm   input    input  normalize shape  weight=weight  bias=bias  eps=eps               return torch layer norm input  normalize shape  weight  bias  eps  torch backends cudnn enable  
def local response norm input  tensor  size  int  alpha  float = 1e-4  beta  float = 0 75  k  float = 1 0  -> tensor      r   apply local response normalization over an input signal compose of     several input plan  where channel occupy the second dimension      apply normalization across channel       see  class `~torch nn localresponsenorm` for detail              if have torch function unary input           return handle torch function local response norm   input    input  size  alpha=alpha  beta=beta  k=k      dim = input dim       if dim < 3          raise valueerror              string format                  dim                             div = input mul input  unsqueeze 1      if dim == 3          div = pad div   0  0  size // 2   size - 1  // 2           div = avg pool2d div   size  1   stride=1  squeeze 1      else          size = input size           div = div view size 0   1  size 1   size 2   -1          div = pad div   0  0  0  0  size // 2   size - 1  // 2           div = avg pool3d div   size  1  1   stride=1  squeeze 1          div = div view size      div = div mul alpha  add k  pow beta      return input / div 
def normalize input  tensor  p  float = 2 0  dim  int = 1  eps  float = 1e-12  out  optional tensor  = none  -> tensor      r   perform  math `l p` normalization of input over specify dimension       for a tensor  attr `input` of size  math ` n 0       n  dim        n k `  each      math `n  dim ` -element vector  math `v` along dimension  attr `dim` be transform as         math           v = \frac v  \max \lvert v \rvert p  \epsilon         with the default arguments it use the euclidean norm over vectors along dimension  math `1` for normalization       args          input  input tensor of any shape         p  float   the exponent value in the norm formulation  default  2         dim  int   the dimension to reduce  default  1         eps  float   small value to avoid division by zero  default  1e-12         out  tensor  optional   the output tensor  if  attr `out` be use  this                                 operation win t be differentiable              if have torch function unary input           return handle torch function normalize   input    input  p=p  dim=dim  eps=eps  out=out      if out be none          denom = input norm p  dim  keepdim=true  clamp min eps  expand as input          return input / denom     else          denom = input norm p  dim  keepdim=true  clamp min  eps  expand as input          return torch div input  denom  out=out  
def linear input  tensor  weight  tensor  bias  optional tensor  = none  -> tensor      r        apply a linear transformation to the incoming data   math `y = xa t   b`       this operator support  ref `tensorfloat32<tf32 on ampere>`       shape           - input   math ` n     in\ feature ` n be the batch size  ` ` mean any number of           additional dimension         - weight   math ` out\ feature  in\ feature `         - bias   math ` out\ feature `         - output   math ` n     out\ feature `             if have torch function variadic input  weight           return handle torch function linear   input  weight   input  weight  bias=bias      return torch  c  nn linear input  weight  bias  
def bilinear input1  tensor  input2  tensor  weight  tensor  bias  optional tensor  = none  -> tensor      r        apply a bilinear transformation to the incoming data       math `y = x 1 t a x 2   b`      shape           - input1   math ` n     h  in1  ` where  math `h  in1 =\text in1\ feature `           and  math ` ` mean any number of additional dimension            all but the last dimension of the input should be the same          - input2   math ` n     h  in2  ` where  math `h  in2 =\text in2\ feature `         - weight   math ` \text out\ feature   \text in1\ feature             \text in2\ feature  `         - bias   math ` \text out\ feature  `         - output   math ` n     h  out  ` where  math `h  out =\text out\ feature `           and all but the last dimension be the same shape as the input              if have torch function variadic input1  input2  weight           return handle torch function              bilinear               input1  input2  weight               input1  input2  weight              bias=bias               return torch bilinear input1  input2  weight  bias  
def dropout input  tensor  p  float = 0 5  train  bool = true  inplace  bool = false  -> tensor      r        during train  randomly zero some of the elements of the input     tensor with probability  attr `p` use sample from a bernoulli     distribution       see  class `~torch nn dropout` for detail       args          p  probability of an element to be zero  default  0 5         train  apply dropout if be ``true``  default  ``true``         inplace  if set to ``true``  will do this operation in-place  default  ``false``             if have torch function unary input           return handle torch function dropout   input    input  p=p  training=training  inplace=inplace      if p < 0 0 or p > 1 0          raise valueerror string string format p       return  vf dropout  input  p  train  if inplace else  vf dropout input  p  train  
def alpha dropout input  tensor  p  float = 0 5  train  bool = false  inplace  bool = false  -> tensor      r   apply alpha dropout to the input       see  class `~torch nn alphadropout` for detail              if have torch function unary input           return handle torch function alpha dropout   input    input  p=p  training=training  inplace=inplace      if p < 0 0 or p > 1 0          raise valueerror string string format p       return  vf alpha dropout  input  p  train  if inplace else  vf alpha dropout input  p  train  
def feature alpha dropout input  tensor  p  float = 0 5  train  bool = false  inplace  bool = false  -> tensor      r        randomly mask out entire channel  a channel be a feature map      e g  the  math `j`-th channel of the  math `i`-th sample in the batch input     be a tensor  math `\text input  i  j `  of the input tensor   instead of     set activations to zero  as in regular dropout  the activations be set     to the negative saturation value of the selu activation function       each element will be mask independently on every forward call with     probability  attr `p` use sample from a bernoulli distribution      the elements to be mask be randomize on every forward call  and scale     and shift to maintain zero mean and unit variance       see  class `~torch nn featurealphadropout` for detail       args          p  dropout probability of a channel to be zero  default  0 5         train  apply dropout if be ``true``  default  ``true``         inplace  if set to ``true``  will do this operation in-place  default  ``false``             if have torch function unary input           return handle torch function              feature alpha dropout   input    input  p=p  training=training  inplace=inplace               if p < 0 0 or p > 1 0          raise valueerror string string format p       return  vf feature alpha dropout  input  p  train  if inplace else  vf feature alpha dropout input  p  train  
def dropout2d input  tensor  p  float = 0 5  train  bool = true  inplace  bool = false  -> tensor      r        randomly zero out entire channel  a channel be a 2d feature map      e g   the  math `j`-th channel of the  math `i`-th sample in the     batch input be a 2d tensor  math `\text input  i  j `  of the input tensor       each channel will be zero out independently on every forward call with     probability  attr `p` use sample from a bernoulli distribution       see  class `~torch nn dropout2d` for detail       args          p  probability of a channel to be zero  default  0 5         train  apply dropout if be ``true``  default  ``true``         inplace  if set to ``true``  will do this operation in-place  default  ``false``             if have torch function unary input           return handle torch function dropout2d   input    input  p=p  training=training  inplace=inplace      if p < 0 0 or p > 1 0          raise valueerror string string format p       return  vf feature dropout  input  p  train  if inplace else  vf feature dropout input  p  train  
def dropout3d input  tensor  p  float = 0 5  train  bool = true  inplace  bool = false  -> tensor      r        randomly zero out entire channel  a channel be a 3d feature map      e g   the  math `j`-th channel of the  math `i`-th sample in the     batch input be a 3d tensor  math `\text input  i  j `  of the input tensor       each channel will be zero out independently on every forward call with     probability  attr `p` use sample from a bernoulli distribution       see  class `~torch nn dropout3d` for detail       args          p  probability of a channel to be zero  default  0 5         train  apply dropout if be ``true``  default  ``true``         inplace  if set to ``true``  will do this operation in-place  default  ``false``                       if have torch function unary input           return handle torch function dropout3d   input    input  p=p  training=training  inplace=inplace      if p < 0 0 or p > 1 0          raise valueerror string string format p       return  vf feature dropout  input  p  train  if inplace else  vf feature dropout input  p  train  
def embed      input  tensor      weight  tensor      pad idx  optional int  = none      max norm  optional float  = none      norm type  float = 2 0      scale grad by freq  bool = false      sparse  bool = false    -> tensor      r   a simple lookup table that look up embeddings in a fix dictionary and size       this module be often use to retrieve word embeddings use indices      the input to the module be a list of indices  and the embed matrix      and the output be the correspond word embeddings       see  class `torch nn embedding` for more detail       args          input  longtensor   tensor contain indices into the embed matrix         weight  tensor   the embed matrix with number of row equal to the maximum possible index   1              and number of columns equal to the embed size         pad idx  int  optional   if specify  the entries at  attr `padding idx` do not contribute to the gradient                                       therefore  the embed vector at  attr `padding idx` be not update during train                                       i e  it remain as a fix  pad           max norm  float  optional   if give  each embed vector with norm larger than  attr `max norm`                                     be renormalize to have norm  attr `max norm`                                      note  this will modify  attr `weight` in-place          norm type  float  optional   the p of the p-norm to compute for the  attr `max norm` option  default ``2``          scale grad by freq  boolean  optional   if give  this will scale gradients by the inverse of frequency of                                                 the word in the mini-batch  default ``false``          sparse  bool  optional   if ``true``  gradient w r t   attr `weight` will be a sparse tensor  see note under                                   class `torch nn embedding` for more detail regard sparse gradients       shape          - input  longtensor of arbitrary shape contain the indices to extract         - weight  embed matrix of float point type with shape ` v  embed dim `            where v = maximum index   1 and embed dim = the embed size         - output  `    embed dim `  where ` ` be the input shape      examples            >>>   a batch of 2 sample of 4 indices each         >>> input = torch tensor   1 2 4 5   4 3 2 9            >>>   an embed matrix contain 10 tensors of size 3         >>> embed matrix = torch rand 10  3          >>> f embed input  embed matrix          tensor     0 8490   0 9625   0 6753                      0 9666   0 7761   0 6108                      0 6246   0 9751   0 3618                      0 4161   0 2419   0 7383                        0 6246   0 9751   0 3618                      0 0237   0 7794   0 0528                      0 9666   0 7761   0 6108                      0 3385   0 8612   0 1867              >>>   example with pad idx         >>> weight = torch rand 10  3          >>> weight 0     zero            >>> embed matrix = weight         >>> input = torch tensor   0 2 0 5            >>> f embed input  embed matrix  pad idx=0          tensor     0 0000   0 0000   0 0000                      0 5609   0 5384   0 8720                      0 0000   0 0000   0 0000                      0 6262   0 2438   0 7471                  if have torch function variadic input  weight           return handle torch function              embed   input  weight               input  weight  pad idx  max norm  norm type              scale grad by freq  sparse               if pad idx be not none          if pad idx > 0              assert pad idx < weight size 0   string         elif pad idx < 0              assert pad idx >= -weight size 0   string             pad idx = weight size 0    pad idx     else          pad idx = -1     if max norm be not none                                              input = input contiguous                                                         no grad embed renorm  weight  input  max norm  norm type      return torch embed weight  input  pad idx  scale grad by freq  sparse  
def embed bag      input  tensor      weight  tensor      offset  optional tensor  = none      max norm  optional float  = none      norm type  float = 2      scale grad by freq  bool = false      mode  str = string      sparse  bool = false      per sample weight  optional tensor  = none      include last offset  bool = false      pad idx  optional int  = none    -> tensor      r   compute sum  mean or maxes of `bags` of embeddings  without instantiate the     intermediate embeddings       see  class `torch nn embeddingbag` for more detail       note           backward reproducibility note       args          input  longtensor   tensor contain bag of indices into the embed matrix         weight  tensor   the embed matrix with number of row equal to the maximum possible index   1              and number of columns equal to the embed size         offset  longtensor  optional   only use when  attr `input` be 1d   attr `offsets` determine                              the start index position of each bag  sequence  in  attr `input`          max norm  float  optional   if give  each embed vector with norm larger than  attr `max norm`                                     be renormalize to have norm  attr `max norm`                                      note  this will modify  attr `weight` in-place          norm type  float  optional   the ``p`` in the ``p``-norm to compute for the  attr `max norm` option                                       default ``2``          scale grad by freq  boolean  optional   if give  this will scale gradients by the inverse of frequency of                                                 the word in the mini-batch  default ``false``                                                  note  this option be not support when ``mode= max ``          mode  string  optional   `` sum ``  `` mean `` or `` max ``  specify the way to reduce the bag                                   default  `` mean ``         sparse  bool  optional   if ``true``  gradient w r t   attr `weight` will be a sparse tensor  see note under                                   class `torch nn embedding` for more detail regard sparse gradients                                   note  this option be not support when ``mode= max ``          per sample weight  tensor  optional   a tensor of float / double weight  or none             to indicate all weight should be take to be 1  if specify   attr `per sample weights`             must have exactly the same shape as input and be treat as have the same              attr `offsets`  if those be not none           include last offset  bool  optional   if ``true``  the size of offset be equal to the number of bag   1              the last element be the size of the input  or the end index position of the last bag  sequence            pad idx  int  optional   if specify  the entries at  attr `padding idx` do not contribute to the                                      gradient  therefore  the embed vector at  attr `padding idx` be not update                                      during train  i e  it remain as a fix  pad   note that the embed                                      vector at  attr `padding idx` be exclude from the reduction       shape          -  attr `input`  longtensor  and  attr `offsets`  longtensor  optional             - if  attr `input` be 2d of shape ` b  n `  it will be treat as ``b`` bag  sequence              each of fix length ``n``  and this will return ``b`` value aggregate in a way             depend on the  attr `mode`   attr `offsets` be ignore and require to be ``none`` in this case             - if  attr `input` be 1d of shape ` n `  it will be treat as a concatenation of             multiple bag  sequence    attr `offsets` be require to be a 1d tensor contain             the start index position of each bag in  attr `input`  therefore  for  attr `offsets`             of shape ` b `   attr `input` will be view as have ``b`` bag              empty bag  i e   have 0-length  will have return vectors fill by zero           -  attr `weight`  tensor   the learnable weight of the module of shape ` num embeddings  embed dim `          -  attr `per sample weights`  tensor  optional   have the same shape as  attr `input`           -  attr `output`  aggregate embed value of shape ` b  embed dim `      examples            >>>   an embed module contain 10 tensors of size 3         >>> embed matrix = torch rand 10  3          >>>   a batch of 2 sample of 4 indices each         >>> input = torch tensor  1 2 4 5 4 3 2 9           >>> offset = torch tensor  0 4           >>> f embed bag input  embed matrix  offset          tensor    0 3397   0 3552   0 5545                     0 5893   0 4386   0 5882             >>>   example with pad idx         >>> embed matrix = torch rand 10  3          >>> input = torch tensor  2  2  2  2  4  3  2  9           >>> offset = torch tensor  0 4           >>> f embed bag input  embed matrix  offset  pad idx=2  mode= sum           tensor    0 0000   0 0000   0 0000                    -0 7082   3 2145  -2 6251                if have torch function variadic input  weight           return handle torch function              embed bag               input  weight               input              weight              offsets=offsets              max norm=max norm              norm type=norm type              scale grad by freq=scale grad by freq              mode=mode              sparse=sparse              per sample weights=per sample weight              include last offset=include last offset              pad idx=padding idx                               if weight dtype == torch long and input be float point            warn warn              string             string             string                   weight  input = input  weight      if per sample weight be not none and input size    = per sample weight size            raise valueerror              string             string format per sample weight shape  input shape                 if input dim   == 2          if offset be not none              type str = string                          if not torch jit be script                    type str = str type offset               raise valueerror                  string                 string                 string                 string format type str                        offset = torch arange 0  input numel    input size 1   dtype=input dtype  device=input device           input = input reshape -1          if per sample weight be not none              per sample weight = per sample weight reshape -1      elif input dim   == 1          if offset be none              raise valueerror string          if offset dim    = 1              raise valueerror string      else          raise valueerror string string format input dim         if mode == string          mode enum = 0     elif mode == string          mode enum = 1     elif mode == string          mode enum = 2          if scale grad by freq              raise valueerror string           if sparse              raise valueerror string       else          raise valueerror string       if max norm be not none                                               no grad embed renorm  weight  input  max norm  norm type       if per sample weight be not none and mode  = string          raise notimplementederror              string             string             string format mode                 ret          = torch embed bag          weight  input  offset  scale grad by freq  mode enum  sparse  per sample weight  include last offset  pad idx           return ret 
def pairwise distance x1  tensor  x2  tensor  p  float = 2 0  eps  float = 1e-6  keepdim  bool = false  -> tensor      r        see  class `torch nn pairwisedistance` for detail             if have torch function variadic x1  x2           return handle torch function pairwise distance   x1  x2   x1  x2  p=p  eps=eps  keepdim=keepdim      return torch pairwise distance x1  x2  p  eps  keepdim  
def binary cross entropy      input  tensor      target  tensor      weight  optional tensor  = none      size average  optional bool  = none      reduce  optional bool  = none      reduction  str = string    -> tensor      r   function that measure the binary cross entropy     between the target and the output       see  class `~torch nn bceloss` for detail       args          input  tensor of arbitrary shape         target  tensor of the same shape as input         weight  tensor  optional   a manual rescale weight                 if provide it s repeat to match input tensor shape         size average  bool  optional   deprecate  see  attr `reduction`   by default              the losses be average over each loss element in the batch  note that for             some losses  there multiple elements per sample  if the field  attr `size average`             be set to ``false``  the losses be instead sum for each minibatch  ignore             when reduce be ``false``  default  ``true``         reduce  bool  optional   deprecate  see  attr `reduction`   by default  the             losses be average or sum over observations for each minibatch depend             on  attr `size average`  when  attr `reduce` be ``false``  return a loss per             batch element instead and ignore  attr `size average`  default  ``true``         reduction  string  optional   specify the reduction to apply to the output              `` none ``   `` mean ``   `` sum ``  `` none ``  no reduction will be apply              `` mean ``  the sum of the output will be divide by the number of             elements in the output  `` sum ``  the output will be sum  note   attr `size average`             and  attr `reduce` be in the process of be deprecate  and in the meantime              specify either of those two args will override  attr `reduction`  default  `` mean ``      examples            >>> input = torch randn  3  2   require grad=true          >>> target = torch rand  3  2   require grad=false          >>> loss = f binary cross entropy f sigmoid input   target          >>> loss backward               if have torch function variadic input  target           return handle torch function              binary cross entropy               input  target               input              target              weight=weight              size average=size average              reduce=reduce              reduction=reduction                if size average be not none or reduce be not none          reduction enum =  reduction legacy get enum size average  reduce      else          reduction enum =  reduction get enum reduction      if target size    = input size            raise valueerror              string             string format target size    input size                   if weight be not none          new size =  infer size target size    weight size            weight = weight expand new size       return torch  c  nn binary cross entropy input  target  weight  reduction enum  
def binary cross entropy with logits      input  tensor      target  tensor      weight  optional tensor  = none      size average  optional bool  = none      reduce  optional bool  = none      reduction  str = string      pos weight  optional tensor  = none    -> tensor      r   function that measure binary cross entropy between target and output     logits       see  class `~torch nn bcewithlogitsloss` for detail       args          input  tensor of arbitrary shape         target  tensor of the same shape as input         weight  tensor  optional   a manual rescale weight             if provide it s repeat to match input tensor shape         size average  bool  optional   deprecate  see  attr `reduction`   by default              the losses be average over each loss element in the batch  note that for             some losses  there multiple elements per sample  if the field  attr `size average`             be set to ``false``  the losses be instead sum for each minibatch  ignore             when reduce be ``false``  default  ``true``         reduce  bool  optional   deprecate  see  attr `reduction`   by default  the             losses be average or sum over observations for each minibatch depend             on  attr `size average`  when  attr `reduce` be ``false``  return a loss per             batch element instead and ignore  attr `size average`  default  ``true``         reduction  string  optional   specify the reduction to apply to the output              `` none ``   `` mean ``   `` sum ``  `` none ``  no reduction will be apply              `` mean ``  the sum of the output will be divide by the number of             elements in the output  `` sum ``  the output will be sum  note   attr `size average`             and  attr `reduce` be in the process of be deprecate  and in the meantime              specify either of those two args will override  attr `reduction`  default  `` mean ``         pos weight  tensor  optional   a weight of positive examples                  must be a vector with length equal to the number of class       examples             >>> input = torch randn 3  require grad=true           >>> target = torch empty 3  random  2           >>> loss = f binary cross entropy with logits input  target           >>> loss backward               if have torch function variadic input  target           return handle torch function              binary cross entropy with logits               input  target               input              target              weight=weight              size average=size average              reduce=reduce              reduction=reduction              pos weight=pos weight                if size average be not none or reduce be not none          reduction enum =  reduction legacy get enum size average  reduce      else          reduction enum =  reduction get enum reduction       if not  target size   == input size             raise valueerror string format target size    input size          return torch binary cross entropy with logits input  target  weight  pos weight  reduction enum  
def poisson nll loss      input  tensor      target  tensor      log input  bool = true      full  bool = false      size average  optional bool  = none      eps  float = 1e-8      reduce  optional bool  = none      reduction  str = string    -> tensor      r   poisson negative log likelihood loss       see  class `~torch nn poissonnllloss` for detail       args          input  expectation of underlie poisson distribution          target  random sample  math `target \sim \text poisson  input `          log input  if ``true`` the loss be compute as              math `\exp \text input   - \text target    \text input `  if ``false`` then loss be              math `\text input  - \text target    \log \text input  \text eps  `  default  ``true``         full  whether to compute full loss  i  e  to add the stirling             approximation term  default  ``false``              math `\text target    \log \text target   - \text target    0 5   \log 2   \pi   \text target  `          size average  bool  optional   deprecate  see  attr `reduction`   by default              the losses be average over each loss element in the batch  note that for             some losses  there multiple elements per sample  if the field  attr `size average`             be set to ``false``  the losses be instead sum for each minibatch  ignore             when reduce be ``false``  default  ``true``         eps  float  optional   small value to avoid evaluation of  math `\log 0 ` when              attr `log input`=``false``  default  1e-8         reduce  bool  optional   deprecate  see  attr `reduction`   by default  the             losses be average or sum over observations for each minibatch depend             on  attr `size average`  when  attr `reduce` be ``false``  return a loss per             batch element instead and ignore  attr `size average`  default  ``true``         reduction  string  optional   specify the reduction to apply to the output              `` none ``   `` mean ``   `` sum ``  `` none ``  no reduction will be apply              `` mean ``  the sum of the output will be divide by the number of             elements in the output  `` sum ``  the output will be sum  note   attr `size average`             and  attr `reduce` be in the process of be deprecate  and in the meantime              specify either of those two args will override  attr `reduction`  default  `` mean ``              if have torch function variadic input  target           return handle torch function              poisson nll loss               input  target               input              target              log input=log input              full=full              size average=size average              eps=eps              reduce=reduce              reduction=reduction                if size average be not none or reduce be not none          reduction =  reduction legacy get string size average  reduce      if reduction  = string and reduction  = string and reduction  = string          ret = input         raise valueerror reduction   string       ret = torch poisson nll loss input  target  log input  full  eps   reduction get enum reduction       return ret 
def cosine embed loss      input1  tensor      input2  tensor      target  tensor      margin  float = 0      size average  optional bool  = none      reduce  optional bool  = none      reduction  str = string    -> tensor      r   cosine embed loss input1  input2  target  margin=0  size average=none  reduce=none  reduction= mean   -> tensor      see  class `~torch nn cosineembeddingloss` for detail              if have torch function variadic input1  input2  target           return handle torch function              cosine embed loss               input1  input2  target               input1              input2              target              margin=margin              size average=size average              reduce=reduce              reduction=reduction                if size average be not none or reduce be not none          reduction enum =  reduction legacy get enum size average  reduce      else          reduction enum =  reduction get enum reduction      return torch cosine embed loss input1  input2  target  margin  reduction enum  
def cross entropy      input  tensor      target  tensor      weight  optional tensor  = none      size average  optional bool  = none      ignore index  int = -100      reduce  optional bool  = none      reduction  str = string    -> tensor      r   this criterion combine `log softmax` and `nll loss` in a single     function       see  class `~torch nn crossentropyloss` for detail       args          input  tensor     math ` n  c ` where `c = number of classes` or  math ` n  c  h  w `             in case of 2d loss  or  math ` n  c  d 1  d 2       d k ` where  math `k \geq 1`             in the case of k-dimensional loss          target  tensor     math ` n ` where each value be  math `0 \leq \text target  i  \leq c-1`              or  math ` n  d 1  d 2       d k ` where  math `k \geq 1` for             k-dimensional loss          weight  tensor  optional   a manual rescale weight give to each             class  if give  have to be a tensor of size `c`         size average  bool  optional   deprecate  see  attr `reduction`   by default              the losses be average over each loss element in the batch  note that for             some losses  there multiple elements per sample  if the field  attr `size average`             be set to ``false``  the losses be instead sum for each minibatch  ignore             when reduce be ``false``  default  ``true``         ignore index  int  optional   specify a target value that be ignore             and do not contribute to the input gradient  when  attr `size average` be             ``true``  the loss be average over non-ignored target  default  -100         reduce  bool  optional   deprecate  see  attr `reduction`   by default  the             losses be average or sum over observations for each minibatch depend             on  attr `size average`  when  attr `reduce` be ``false``  return a loss per             batch element instead and ignore  attr `size average`  default  ``true``         reduction  string  optional   specify the reduction to apply to the output              `` none ``   `` mean ``   `` sum ``  `` none ``  no reduction will be apply              `` mean ``  the sum of the output will be divide by the number of             elements in the output  `` sum ``  the output will be sum  note   attr `size average`             and  attr `reduce` be in the process of be deprecate  and in the meantime              specify either of those two args will override  attr `reduction`  default  `` mean ``      examples            >>> input = torch randn 3  5  require grad=true          >>> target = torch randint 5   3    dtype=torch int64          >>> loss = f cross entropy input  target          >>> loss backward               if have torch function variadic input  target           return handle torch function              cross entropy               input  target               input              target              weight=weight              size average=size average              ignore index=ignore index              reduce=reduce              reduction=reduction                if size average be not none or reduce be not none          reduction =  reduction legacy get string size average  reduce      return torch  c  nn cross entropy loss input  target  weight   reduction get enum reduction   ignore index  
def ctc loss      log probs  tensor      target  tensor      input lengths  tensor      target lengths  tensor      blank  int = 0      reduction  str = string      zero infinity  bool = false    -> tensor      r   the connectionist temporal classification loss       see  class `~torch nn ctcloss` for detail       note           cudnn reproducibility note       note           backward reproducibility note       args          log probs   math ` t  n  c ` where `c = number of character in alphabet include blank`              `t = input length`  and `n = batch size`              the logarithmized probabilities of the output              e g  obtain with  func `torch nn functional log softmax`           target   math ` n  s ` or ` sum target lengths  `              target cannot be blank  in the second form  the target be assume to be concatenate          input lengths   math ` n `              lengths of the input  must each be  math `\leq t`          target lengths   math ` n `              lengths of the target         blank  int  optional               blank label  default  math `0`          reduction  string  optional   specify the reduction to apply to the output              `` none ``   `` mean ``   `` sum ``  `` none ``  no reduction will be apply              `` mean ``  the output losses will be divide by the target lengths and             then the mean over the batch be take  `` sum ``  the output will be             sum  default  `` mean ``         zero infinity  bool  optional               whether to zero infinite losses and the associate gradients              default  ``false``             infinite losses mainly occur when the input be too short             to be align to the target       example            >>> log probs = torch randn 50  16  20  log softmax 2  detach   require grad            >>> target = torch randint 1  20   16  30   dtype=torch long          >>> input lengths = torch full  16    50  dtype=torch long          >>> target lengths = torch randint 10 30  16    dtype=torch long          >>> loss = f ctc loss log probs  target  input lengths  target lengths          >>> loss backward               if have torch function variadic log probs  target  input lengths  target lengths           return handle torch function              ctc loss               log probs  target  input lengths  target lengths               log probs  target  input lengths  target lengths              blank=blank  reduction=reduction  zero infinity=zero infinity               return torch ctc loss          log probs  target  input lengths  target lengths  blank   reduction get enum reduction   zero infinity       
def gaussian nll loss      input  tensor      target  tensor      var  tensor      full  bool = false      eps  float = 1e-6      reduction  str = string    -> tensor      r   gaussian negative log likelihood loss       see  class `~torch nn gaussiannllloss` for detail       args          input  expectation of the gaussian distribution          target  sample from the gaussian distribution          var  tensor of positive variance s   one for each of the expectations             in the input  heteroscedastic   or a single one  homoscedastic           full  bool  optional   include the constant term in the loss calculation  default  ``false``          eps  float  optional   value add to var  for stability  default  1e-6          reduction  string  optional   specify the reduction to apply to the output              `` none ``   `` mean ``   `` sum ``  `` none ``  no reduction will be apply              `` mean ``  the output be the average of all batch member losses              `` sum ``  the output be the sum of all batch member losses              default  `` mean ``              if have torch function variadic input  target  var           return handle torch function              gaussian nll loss               input  target  var               input              target              var              full=full              eps=eps              reduction=reduction                                if var size    = input size                                                 if input size    -1  == var size                var = torch unsqueeze var  -1                                      elif input size    -1  == var size    -1  and var size -1  == 1                pass                   else              raise valueerror string            if reduction  = string and reduction  = string and reduction  = string          raise valueerror reduction   string            if torch any var < 0           raise valueerror string            var = var clone       with torch no grad            var clamp  min=eps            loss = 0 5    torch log var     input - target   2 / var      if full          loss  = 0 5   math log 2   math pi       if reduction == string          return loss mean       elif reduction == string          return loss sum       else          return loss 
def hinge embed loss      input  tensor      target  tensor      margin  float = 1 0      size average  optional bool  = none      reduce  optional bool  = none      reduction  str = string    -> tensor      r   hinge embed loss input  target  margin=1 0  size average=none  reduce=none  reduction= mean   -> tensor      see  class `~torch nn hingeembeddingloss` for detail              if have torch function variadic input  target           return handle torch function              hinge embed loss               input  target               input              target              margin=margin              size average=size average              reduce=reduce              reduction=reduction                if size average be not none or reduce be not none          reduction enum =  reduction legacy get enum size average  reduce      else          reduction enum =  reduction get enum reduction      return torch hinge embed loss input  target  margin  reduction enum  
def kl div      input  tensor      target  tensor      size average  optional bool  = none      reduce  optional bool  = none      reduction  str = string      log target  bool = false    -> tensor      r   the `kullback-leibler divergence loss     <https //en wikipedia org/wiki/kullback-leibler divergence>`        see  class `~torch nn kldivloss` for detail       args          input  tensor of arbitrary shape         target  tensor of the same shape as input         size average  bool  optional   deprecate  see  attr `reduction`   by default              the losses be average over each loss element in the batch  note that for             some losses  there multiple elements per sample  if the field  attr `size average`             be set to ``false``  the losses be instead sum for each minibatch  ignore             when reduce be ``false``  default  ``true``         reduce  bool  optional   deprecate  see  attr `reduction`   by default  the             losses be average or sum over observations for each minibatch depend             on  attr `size average`  when  attr `reduce` be ``false``  return a loss per             batch element instead and ignore  attr `size average`  default  ``true``         reduction  string  optional   specify the reduction to apply to the output              `` none ``   `` batchmean ``   `` sum ``   `` mean ``              `` none ``  no reduction will be apply             `` batchmean ``  the sum of the output will be divide by the batchsize             `` sum ``  the output will be sum             `` mean ``  the output will be divide by the number of elements in the output             default  `` mean ``         log target  bool   a flag indicate whether ``target`` be pass in the log space              it be recommend to pass certain distributions  like ``softmax``              in the log space to avoid numerical issue cause by explicit ``log``              default  ``false``         note            attr `size average` and  attr `reduce` be in the process of be deprecate          and in the meantime  specify either of those two args will override  attr `reduction`          note            attr ``reduction`` = `` mean `` doesn t return the true kl divergence value  please use          attr ``reduction`` = `` batchmean `` which align with kl math definition          in the next major release  `` mean `` will be change to be the same as  batchmean               if have torch function variadic input  target           return handle torch function              kl div               input  target               input              target              size average=size average              reduce=reduce              reduction=reduction              log target=log target                if size average be not none or reduce be not none          reduction enum =  reduction legacy get enum size average  reduce      else          if reduction == string              warn warn                  string                 string                 string                                 if reduction == string              reduction enum =  reduction get enum string          else              reduction enum =  reduction get enum reduction       reduce = torch kl div input  target  reduction enum  log target=log target       if reduction == string and input dim    = 0          reduce = reduce / input size   0       return reduce 
def l1 loss      input  tensor      target  tensor      size average  optional bool  = none      reduce  optional bool  = none      reduction  str = string    -> tensor      r   l1 loss input  target  size average=none  reduce=none  reduction= mean   -> tensor      function that take the mean element-wise absolute value difference       see  class `~torch nn l1loss` for detail              if have torch function variadic input  target           return handle torch function              l1 loss   input  target   input  target  size average=size average  reduce=reduce  reduction=reduction               if not  target size   == input size             warn warn              string             string             string format target size    input size                 stacklevel=2                if size average be not none or reduce be not none          reduction =  reduction legacy get string size average  reduce       expand input  expand target = torch broadcast tensors input  target      return torch  c  nn l1 loss expand input  expand target   reduction get enum reduction   
def mse loss      input  tensor      target  tensor      size average  optional bool  = none      reduce  optional bool  = none      reduction  str = string    -> tensor      r   mse loss input  target  size average=none  reduce=none  reduction= mean   -> tensor      measure the element-wise mean square error       see  class `~torch nn mseloss` for detail              if have torch function variadic input  target           return handle torch function              mse loss   input  target   input  target  size average=size average  reduce=reduce  reduction=reduction               if not  target size   == input size             warn warn              string             string             string format target size    input size                 stacklevel=2                if size average be not none or reduce be not none          reduction =  reduction legacy get string size average  reduce       expand input  expand target = torch broadcast tensors input  target      return torch  c  nn mse loss expand input  expand target   reduction get enum reduction   
def margin rank loss      input1  tensor      input2  tensor      target  tensor      margin  float = 0      size average  optional bool  = none      reduce  optional bool  = none      reduction  str = string    -> tensor      r   margin rank loss input1  input2  target  margin=0  size average=none  reduce=none  reduction= mean   -> tensor      see  class `~torch nn marginrankingloss` for detail              if have torch function variadic input1  input2  target           return handle torch function              margin rank loss               input1  input2  target               input1              input2              target              margin=margin              size average=size average              reduce=reduce              reduction=reduction                if size average be not none or reduce be not none          reduction enum =  reduction legacy get enum size average  reduce      else          reduction enum =  reduction get enum reduction      if input1 dim   == 0 or input2 dim   == 0 or target dim   == 0          raise runtimeerror                                string                 string format input1 size    input2 size    target size                                return torch margin rank loss input1  input2  target  margin  reduction enum  
def multilabel margin loss      input  tensor      target  tensor      size average  optional bool  = none      reduce  optional bool  = none      reduction  str = string    -> tensor      r   multilabel margin loss input  target  size average=none  reduce=none  reduction= mean   -> tensor      see  class `~torch nn multilabelmarginloss` for detail              if have torch function variadic input  target           return handle torch function              multilabel margin loss               input  target               input              target              size average=size average              reduce=reduce              reduction=reduction                if size average be not none or reduce be not none          reduction enum =  reduction legacy get enum size average  reduce      else          reduction enum =  reduction get enum reduction      return torch  c  nn multilabel margin loss input  target  reduction enum  
def multilabel soft margin loss      input  tensor      target  tensor      weight  optional tensor  = none      size average  optional bool  = none      reduce  optional bool  = none      reduction  str = string    -> tensor      r   multilabel soft margin loss input  target  weight=none  size average=none  -> tensor      see  class `~torch nn multilabelsoftmarginloss` for detail              if have torch function variadic input  target           return handle torch function              multilabel soft margin loss               input  target               input              target              weight=weight              size average=size average              reduce=reduce              reduction=reduction                if size average be not none or reduce be not none          reduction =  reduction legacy get string size average  reduce       loss = - target   logsigmoid input     1 - target    logsigmoid -input        if weight be not none          loss = loss   weight      loss = loss sum dim=1  / input size 1         if reduction == string          ret = loss     elif reduction == string          ret = loss mean       elif reduction == string          ret = loss sum       else          ret = input         raise valueerror reduction   string      return ret 
def multi margin loss      input  tensor      target  tensor      p  int = 1      margin  float = 1 0      weight  optional tensor  = none      size average  optional bool  = none      reduce  optional bool  = none      reduction  str = string    -> tensor      r   multi margin loss input  target  p=1  margin=1  weight=none  size average=none                            reduce=none  reduction= mean   -> tensor      see  class `~torch nn multimarginloss` for detail              if have torch function variadic input  target           return handle torch function              multi margin loss               input  target               input              target              p=p              margin=margin              weight=weight              size average=size average              reduce=reduce              reduction=reduction                if size average be not none or reduce be not none          reduction enum =  reduction legacy get enum size average  reduce      else          reduction enum =  reduction get enum reduction      if p  = 1 and p  = 2          raise valueerror string      if weight be not none          if weight dim    = 1              raise valueerror string       return torch  c  nn multi margin loss input  target  p  margin  weight  reduction enum  
def nll loss      input  tensor      target  tensor      weight  optional tensor  = none      size average  optional bool  = none      ignore index  int = -100      reduce  optional bool  = none      reduction  str = string    -> tensor      r   the negative log likelihood loss       see  class `~torch nn nllloss` for detail       args          input   math ` n  c ` where `c = number of classes` or  math ` n  c  h  w `             in case of 2d loss  or  math ` n  c  d 1  d 2       d k ` where  math `k \geq 1`             in the case of k-dimensional loss          target   math ` n ` where each value be  math `0 \leq \text target  i  \leq c-1`              or  math ` n  d 1  d 2       d k ` where  math `k \geq 1` for             k-dimensional loss          weight  tensor  optional   a manual rescale weight give to each             class  if give  have to be a tensor of size `c`         size average  bool  optional   deprecate  see  attr `reduction`   by default              the losses be average over each loss element in the batch  note that for             some losses  there multiple elements per sample  if the field  attr `size average`             be set to ``false``  the losses be instead sum for each minibatch  ignore             when reduce be ``false``  default  ``true``         ignore index  int  optional   specify a target value that be ignore             and do not contribute to the input gradient  when  attr `size average` be             ``true``  the loss be average over non-ignored target  default  -100         reduce  bool  optional   deprecate  see  attr `reduction`   by default  the             losses be average or sum over observations for each minibatch depend             on  attr `size average`  when  attr `reduce` be ``false``  return a loss per             batch element instead and ignore  attr `size average`  default  ``true``         reduction  string  optional   specify the reduction to apply to the output              `` none ``   `` mean ``   `` sum ``  `` none ``  no reduction will be apply              `` mean ``  the sum of the output will be divide by the number of             elements in the output  `` sum ``  the output will be sum  note   attr `size average`             and  attr `reduce` be in the process of be deprecate  and in the meantime              specify either of those two args will override  attr `reduction`  default  `` mean ``      example            >>>   input be of size n x c = 3 x 5         >>> input = torch randn 3  5  require grad=true          >>>   each element in target have to have 0 <= value < c         >>> target = torch tensor  1  0  4           >>> output = f nll loss f log softmax input   target          >>> output backward               if have torch function variadic input  target           return handle torch function              nll loss               input  target               input              target              weight=weight              size average=size average              ignore index=ignore index              reduce=reduce              reduction=reduction                if size average be not none or reduce be not none          reduction =  reduction legacy get string size average  reduce      return torch  c  nn nll loss nd input  target  weight   reduction get enum reduction   ignore index  
def huber loss      input  tensor      target  tensor      reduction  str = string      delta  float = 1 0    -> tensor      r   function that use a square term if the absolute     element-wise error fall below delta and a delta-scaled l1 term otherwise       see  class `~torch nn huberloss` for detail              if have torch function variadic input  target           return handle torch function              huber loss               input  target               input              target              reduction=reduction              delta=delta                if not  target size   == input size             warn warn string                       string                       string format target size    input size                           stacklevel=2       expand input  expand target = torch broadcast tensors input  target      return torch  c  nn huber loss expand input  expand target   reduction get enum reduction   delta  
def smooth l1 loss      input  tensor      target  tensor      size average  optional bool  = none      reduce  optional bool  = none      reduction  str = string      beta  float = 1 0    -> tensor      r   function that use a square term if the absolute     element-wise error fall below beta and an l1 term otherwise       see  class `~torch nn smoothl1loss` for detail              if have torch function variadic input  target           return handle torch function              smooth l1 loss               input  target               input              target              size average=size average              reduce=reduce              reduction=reduction              beta=beta                if not  target size   == input size             warn warn              string             string             string format target size    input size                 stacklevel=2                if size average be not none or reduce be not none          reduction =  reduction legacy get string size average  reduce       expand input  expand target = torch broadcast tensors input  target      return torch  c  nn smooth l1 loss expand input  expand target   reduction get enum reduction   beta  
def soft margin loss      input  tensor      target  tensor      size average  optional bool  = none      reduce  optional bool  = none      reduction  str = string    -> tensor      r   soft margin loss input  target  size average=none  reduce=none  reduction= mean   -> tensor      see  class `~torch nn softmarginloss` for detail              if have torch function variadic input  target           return handle torch function              soft margin loss   input  target   input  target  size average=size average  reduce=reduce  reduction=reduction               if size average be not none or reduce be not none          reduction enum =  reduction legacy get enum size average  reduce      else          reduction enum =  reduction get enum reduction      return torch  c  nn soft margin loss input  target  reduction enum  
def triplet margin loss      anchor  tensor      positive  tensor      negative  tensor      margin  float = 1 0      p  float = 2      eps  float = 1e-6      swap  bool = false      size average  optional bool  = none      reduce  optional bool  = none      reduction  str = string    -> tensor      r        see  class `~torch nn tripletmarginloss` for detail             if have torch function variadic anchor  positive  negative           return handle torch function              triplet margin loss               anchor  positive  negative               anchor              positive              negative              margin=margin              p=p              eps=eps              swap=swap              size average=size average              reduce=reduce              reduction=reduction                if size average be not none or reduce be not none          reduction enum =  reduction legacy get enum size average  reduce      else          reduction enum =  reduction get enum reduction      return torch triplet margin loss anchor  positive  negative  margin  p  eps  swap  reduction enum  
def triplet margin with distance loss      anchor  tensor      positive  tensor      negative  tensor             distance function  optional callable  tensor  tensor   tensor   = none      margin  float = 1 0      swap  bool = false      reduction  str = string   -> tensor      r        see  class `~torch nn tripletmarginwithdistanceloss` for detail              if torch jit be script            raise notimplementederror              string             string                if have torch function variadic anchor  positive  negative           return handle torch function              triplet margin with distance loss               anchor  positive  negative               anchor              positive              negative              distance function=distance function              margin=margin              swap=swap              reduction=reduction                 distance function = distance function if distance function be not none else pairwise distance      positive dist = distance function anchor  positive      negative dist = distance function anchor  negative       if swap          swap dist = distance function positive  negative          negative dist = torch min negative dist  swap dist       output = torch clamp positive dist - negative dist   margin  min=0 0       reduction enum =  reduction get enum reduction      if reduction enum == 1          return output mean       elif reduction enum == 2          return output sum       else          return output 
def  pad input  tensor  pad  list int   mode  str = string  value  float = 0  -> tensor      r   pad tensor       pad size          the pad size by which to pad some dimension of  attr `input`         be describe start from the last dimension and move forward           math `\left\lfloor\frac \text len pad    2 \right\rfloor` dimension         of ``input`` will be pad          for example  to pad only the last dimension of the input tensor  then          attr `pad` have the form          math ` \text padding\ leave   \text padding\ right  `          to pad the last 2 dimension of the input tensor  then use          math ` \text padding\ leave   \text padding\ right  `          math `\text padding\ top   \text padding\ bottom  `          to pad the last 3 dimension  use          math ` \text padding\ leave   \text padding\ right  `          math `\text padding\ top   \text padding\ bottom `          math `\text padding\ front   \text padding\ back  `       pad mode          see  class `torch nn constantpad2d`   class `torch nn reflectionpad2d`  and          class `torch nn replicationpad2d` for concrete examples on how each of the         pad modes work  constant pad be implement for arbitrary dimension          replicate pad be implement for pad the last 3 dimension of 5d input         tensor  or the last 2 dimension of 4d input tensor  or the last dimension of         3d input tensor  reflect pad be only implement for pad the last 2         dimension of 4d input tensor  or the last dimension of 3d input tensor       note          when use the cuda backend  this operation may induce nondeterministic         behaviour in its backward pass that be not easily switch off          please see the note on  doc `/notes/randomness` for background       args          input  tensor   n-dimensional tensor         pad  tuple   m-elements tuple  where              math `\frac m  2  \leq` input dimension and  math `m` be even          mode  `` constant ``  `` reflect ``  `` replicate `` or `` circular ``              default  `` constant ``         value  fill value for `` constant `` pad  default  ``0``      examples            >>> t4d = torch empty 3  3  4  2          >>> p1d =  1  1    pad last dim by 1 on each side         >>> out = f pad t4d  p1d   constant   0     effectively zero pad         >>> print out size            torch size  3  3  4  4           >>> p2d =  1  1  2  2    pad last dim by  1  1  and 2nd to last by  2  2          >>> out = f pad t4d  p2d   constant   0          >>> print out size            torch size  3  3  8  4           >>> t4d = torch empty 3  3  4  2          >>> p3d =  0  1  2  1  3  3    pad by  0  1    2  1   and  3  3          >>> out = f pad t4d  p3d   constant   0          >>> print out size            torch size  3  9  7  3                if have torch function unary input           return handle torch function  pad   input    input  pad  mode=mode  value=value      assert len pad    2 == 0  string     assert len pad  // 2 <= input dim    string     if mode == string          return  vf constant pad nd input  pad  value      else          assert value == 0  string format mode          if input dim   == 3              assert len pad  == 2  string             if mode == string                  return torch  c  nn reflection pad1d input  pad              elif mode == string                  return torch  c  nn replication pad1d input  pad              elif mode == string                  return  pad circular input  pad              else                  raise notimplementederror          elif input dim   == 4              assert len pad  == 4  string             if mode == string                  return torch  c  nn reflection pad2d input  pad              elif mode == string                  return torch  c  nn replication pad2d input  pad              elif mode == string                  return  pad circular input  pad              else                  raise notimplementederror          elif input dim   == 5              assert len pad  == 6  string             if mode == string                  raise notimplementederror             elif mode == string                  return torch  c  nn replication pad3d input  pad              elif mode == string                  return  pad circular input  pad              else                  raise notimplementederror         else              raise notimplementederror string  
def interpolate input  tensor  size  optional int  = none  scale factor  optional list float   = none  mode  str = string  align corner  optional bool  = none  recompute scale factor  optional bool  = none  -> tensor        r   down/up sample the input to either the give  attr `size` or the give      attr `scale factor`      the algorithm use for interpolation be determine by  attr `mode`       currently temporal  spatial and volumetric sample be support  i e      expect input be 3-d  4-d or 5-d in shape       the input dimension be interpret in the form      `mini-batch x channel x  optional depth  x  optional height  x width`       the modes available for resize be  `nearest`  `linear`  3d-only       `bilinear`  `bicubic`  4d-only   `trilinear`  5d-only   `area`      args          input  tensor   the input tensor         size  int or tuple int  or tuple int  int  or tuple int  int  int                output spatial size          scale factor  float or tuple float    multiplier for spatial size  have to match input size if it be a tuple          mode  str   algorithm use for upsampling              `` nearest ``   `` linear ``   `` bilinear ``   `` bicubic ``               `` trilinear ``   `` area ``  default  `` nearest ``         align corner  bool  optional   geometrically  we consider the pixels of the             input and output as square rather than point              if set to ``true``  the input and output tensors be align by the             center point of their corner pixels  preserve the value at the corner pixels              if set to ``false``  the input and output tensors be align by the corner             point of their corner pixels  and the interpolation use edge value pad             for out-of-boundary value  make this operation  independent  of input size             when  attr `scale factor` be keep the same  this only have an effect when  attr `mode`             be `` linear ``  `` bilinear ``  `` bicubic `` or `` trilinear ``              default  ``false``         recompute scale factor  bool  optional   recompute the scale factor for use in the             interpolation calculation   when `scale factor` be pass as a parameter  it be use             to compute the `output size`   if `recompute scale factor` be ``false`` or not specify              the passed-in `scale factor` will be use in the interpolation computation              otherwise  a new `scale factor` will be compute base on the output and input size for             use in the interpolation computation  i e  the computation will be identical to if the compute             `output size` be passed-in explicitly    note that when `scale factor` be floating-point              the recomputed scale factor may differ from the one pass in due to round and precision             issue          note           with ``mode= bicubic ``  it s possible to cause overshoot  in other word it can produce         negative value or value greater than 255 for image          explicitly call ``result clamp min=0  max=255 `` if you want to reduce the overshoot         when display the image          warn           with ``align corner = true``  the linearly interpolate modes          `linear`  `bilinear`  and `trilinear`  don t proportionally align the         output and input pixels  and thus the output value can depend on the         input size  this be the default behavior for these modes up to version         0 3 1  since then  the default behavior be ``align corner = false``          see  class `~torch nn upsample` for concrete examples on how this         affect the output          warn           when scale factor be specify  if recompute scale factor=true          scale factor be use to compute the output size which will then         be use to infer new scale for the interpolation          the default behavior for recompute scale factor change to false         in 1 6 0  and scale factor be use in the interpolation         calculation       note           backward reproducibility note              if have torch function unary input           return handle torch function              interpolate               input                input              size=size              scale factor=scale factor              mode=mode              align corners=align corner              recompute scale factor=recompute scale factor                 if mode in  string  string           if align corner be not none              raise valueerror                  string                 string                   else          if align corner be none              warn warn                  string                 string                 string                 string format mode                            align corner = false      dim = input dim   - 2                            if size be not none and scale factor be not none          raise valueerror string      elif size be not none          assert scale factor be none         scale factor = none         if isinstance size   list  tuple                if len size   = dim                  raise valueerror                      string string format dim  len size                                 output size = size         else              output size =  size for   in range dim       elif scale factor be not none          assert size be none         output size = none         if isinstance scale factor   list  tuple                if len scale factor   = dim                  raise valueerror                      string                     string format dim  len scale factor                                 scale factor = scale factor         else              scale factor =  scale factor for   in range dim       else          raise valueerror string       if recompute scale factor be none                            if scale factor be not none              for scale in scale factor                  if math floor scale   = scale                      warn warn                          string                         string                         string                         string                         string                                           break     elif recompute scale factor and size be not none          raise valueerror string                 if mode == string and output size be none          recompute scale factor = true      if recompute scale factor be not none and recompute scale factor                            if not torch jit be script   and torch  c  get trace state                             output size =                    torch floor  input size i   2  float     torch tensor scale factor i   dtype=torch float32   float                     for i in range dim                        else              assert scale factor be not none             output size =  int math floor float input size i   2     scale factor i    for i in range dim           scale factor = none      if input dim   == 3 and mode == string          return torch  c  nn upsample nearest1d input  output size  scale factor      if input dim   == 4 and mode == string          return torch  c  nn upsample nearest2d input  output size  scale factor      if input dim   == 5 and mode == string          return torch  c  nn upsample nearest3d input  output size  scale factor       if input dim   == 3 and mode == string          assert output size be not none         return adaptive avg pool1d input  output size      if input dim   == 4 and mode == string          assert output size be not none         return adaptive avg pool2d input  output size      if input dim   == 5 and mode == string          assert output size be not none         return adaptive avg pool3d input  output size       if input dim   == 3 and mode == string          assert align corner be not none         return torch  c  nn upsample linear1d input  output size  align corner  scale factor      if input dim   == 4 and mode == string          assert align corner be not none         return torch  c  nn upsample bilinear2d input  output size  align corner  scale factor      if input dim   == 5 and mode == string          assert align corner be not none         return torch  c  nn upsample trilinear3d input  output size  align corner  scale factor      if input dim   == 4 and mode == string          assert align corner be not none         return torch  c  nn upsample bicubic2d input  output size  align corner  scale factor       if input dim   == 3 and mode == string          raise notimplementederror string      if input dim   == 3 and mode == string          raise notimplementederror string      if input dim   == 4 and mode == string          raise notimplementederror string      if input dim   == 4 and mode == string          raise notimplementederror string      if input dim   == 5 and mode == string          raise notimplementederror string      if input dim   == 5 and mode == string          raise notimplementederror string       raise notimplementederror          string         string         string format input dim    mode        
def upsample input  size=none  scale factor=none  mode=string  align corners=none         r   upsamples the input to either the give  attr `size` or the give      attr `scale factor`         warn           this function be deprecate in favor of  func `torch nn functional interpolate`          this be equivalent with ``nn functional interpolate     ``       note           backward reproducibility note       the algorithm use for upsampling be determine by  attr `mode`       currently temporal  spatial and volumetric upsampling be support  i e      expect input be 3-d  4-d or 5-d in shape       the input dimension be interpret in the form      `mini-batch x channel x  optional depth  x  optional height  x width`       the modes available for upsampling be  `nearest`  `linear`  3d-only       `bilinear`  `bicubic`  4d-only   `trilinear`  5d-only       args          input  tensor   the input tensor         size  int or tuple int  or tuple int  int  or tuple int  int  int                output spatial size          scale factor  float or tuple float    multiplier for spatial size  have to match input size if it be a tuple          mode  string   algorithm use for upsampling              `` nearest ``   `` linear ``   `` bilinear ``   `` bicubic ``               `` trilinear ``  default  `` nearest ``         align corner  bool  optional   geometrically  we consider the pixels of the             input and output as square rather than point              if set to ``true``  the input and output tensors be align by the             center point of their corner pixels  preserve the value at the corner pixels              if set to ``false``  the input and output tensors be align by the corner             point of their corner pixels  and the interpolation use edge value pad             for out-of-boundary value  make this operation  independent  of input size             when  attr `scale factor` be keep the same  this only have an effect when  attr `mode`             be `` linear ``  `` bilinear ``  `` bicubic `` or `` trilinear ``              default  ``false``         note           with ``mode= bicubic ``  it s possible to cause overshoot  in other word it can produce         negative value or value greater than 255 for image          explicitly call ``result clamp min=0  max=255 `` if you want to reduce the overshoot         when display the image          warn           with ``align corner = true``  the linearly interpolate modes          `linear`  `bilinear`  and `trilinear`  don t proportionally align the         output and input pixels  and thus the output value can depend on the         input size  this be the default behavior for these modes up to version         0 3 1  since then  the default behavior be ``align corner = false``          see  class `~torch nn upsample` for concrete examples on how this         affect the output               warn warn string      return interpolate input  size  scale factor  mode  align corner  
def upsample nearest input  size=none  scale factor=none         r   upsamples the input  use nearest neighbour  pixel value          warn           this function be deprecate in favor of  func `torch nn functional interpolate`          this be equivalent with ``nn functional interpolate      mode= nearest  ``       currently spatial and volumetric upsampling be support  i e  expect     input be 4 or 5 dimensional        args          input  tensor   input         size  int or tuple int  int  or tuple int  int  int    output spatia             size          scale factor  int   multiplier for spatial size  have to be an integer       note           backward reproducibility note                   warn warn string      return interpolate input  size  scale factor  mode=string  
def upsample bilinear input  size=none  scale factor=none         r   upsamples the input  use bilinear upsampling          warn           this function be deprecate in favor of  func `torch nn functional interpolate`          this be equivalent with         ``nn functional interpolate      mode= bilinear   align corners=true ``       expect input be spatial  4 dimensional   use `upsample trilinear` fo     volumetric  5 dimensional  input       args          input  tensor   input         size  int or tuple int  int    output spatial size          scale factor  int or tuple int  int    multiplier for spatial size      note           backward reproducibility note                   warn warn string      return interpolate input  size  scale factor  mode=string  align corners=true  
def grid sample      input  tensor      grid  tensor      mode  str = string      pad mode  str = string      align corner  optional bool  = none    -> tensor      r   give an  attr `input` and a flow-field  attr `grid`  compute the     ``output`` use  attr `input` value and pixel locations from  attr `grid`       currently  only spatial  4-d  and volumetric  5-d   attr `input` be     support       in the spatial  4-d  case  for  attr `input` with shape      math ` n  c  h \text in   w \text in  ` and  attr `grid` with shape      math ` n  h \text out   w \text out   2 `  the output will have shape      math ` n  c  h \text out   w \text out  `       for each output location ``output n     h  w ``  the size-2 vector     ``grid n  h  w `` specify  attr `input` pixel locations ``x`` and ``y``      which be use to interpolate the output value ``output n     h  w ``      in the case of 5d input  ``grid n  d  h  w `` specify the     ``x``  ``y``  ``z`` pixel locations for interpolate     ``output n     d  h  w ``   attr `mode` argument specify ``nearest`` or     ``bilinear`` interpolation method to sample the input pixels        attr `grid` specify the sample pixel locations normalize by the      attr `input` spatial dimension  therefore  it should have most value in     the range of `` -1  1 ``  for example  value ``x = -1  y = -1`` be the     left-top pixel of  attr `input`  and value  ``x = 1  y = 1`` be the     right-bottom pixel of  attr `input`       if  attr `grid` have value outside the range of `` -1  1 ``  the correspond     output be handle as define by  attr `padding mode`  options be            ``padding mode= zero ``  use ``0`` for out-of-bound grid locations            ``padding mode= border ``  use border value for out-of-bound grid locations            ``padding mode= reflection ``  use value at locations reflect by           the border for out-of-bound grid locations  for location far away           from the border  it will keep be reflect until become in bind            e g    normalize  pixel location ``x = -3 5`` reflect by border ``-1``           and become ``x  = 1 5``  then reflect by border ``1`` and become           ``x   = -0 5``       note          this function be often use in conjunction with  func `affine grid`         to build `spatial transformer networks`         note          when use the cuda backend  this operation may induce nondeterministic         behaviour in its backward pass that be not easily switch off          please see the note on  doc `/notes/randomness` for background       note          nan value in  attr `grid` would be interpret as ``-1``       args          input  tensor   input of shape  math ` n  c  h \text in   w \text in  `  4-d case                          or  math ` n  c  d \text in   h \text in   w \text in  `  5-d case          grid  tensor   flow-field of shape  math ` n  h \text out   w \text out   2 `  4-d case                         or  math ` n  d \text out   h \text out   w \text out   3 `  5-d case          mode  str   interpolation mode to calculate output value             `` bilinear ``   `` nearest ``   `` bicubic ``  default  `` bilinear ``             note  ``mode= bicubic `` support only 4-d input              when ``mode= bilinear `` and the input be 5-d  the interpolation mode             use internally will actually be trilinear  however  when the input be 4-d              the interpolation mode will legitimately be bilinear          pad mode  str   pad mode for outside grid value             `` zero ``   `` border ``   `` reflection ``  default  `` zero ``         align corner  bool  optional   geometrically  we consider the pixels of the             input  as square rather than point              if set to ``true``  the extrema  ``-1`` and ``1``  be consider as refer             to the center point of the input s corner pixels  if set to ``false``  they             be instead consider as refer to the corner point of the input s corner             pixels  make the sample more resolution agnostic              this option parallel the ``align corners`` option in              func `interpolate`  and so whichever option be use here             should also be use there to resize the input image before grid sample              default  ``false``      return          output  tensor   output tensor          `spatial transformer networks`          https //arxiv org/abs/1506 02025         warn           when ``align corner = true``  the grid position depend on the pixel         size relative to the input image size  and so the locations sample by          func `grid sample` will differ for the same input give at different         resolutions  that be  after be upsampled or downsampled           the default behavior up to version 1 2 0 be ``align corner = true``          since then  the default behavior have be change to ``align corner = false``          in order to bring it in line with the default for  func `interpolate`          note           ``mode= bicubic `` be implement use the `cubic convolution algorithm`  with  math `\alpha=-0 75`          the constant  math `\alpha` might be different from package to package          for example  `pil`  and `opencv`  use -0 5 and -0 75 respectively          this algorithm may  overshoot  the range of value it s interpolate          for example  it may produce negative value or value greater than 255 when interpolate input in  0  255           clamp the result with  func  `torch clamp` to ensure they be within the valid range          `cubic convolution algorithm`  https //en wikipedia org/wiki/bicubic interpolation         `pil`  https //github com/python-pillow/pillow/blob/4634eafe3c695a014267eefdce830b4a825beed7/src/libimaging/resample c l51         `opencv`  https //github com/opencv/opencv/blob/f345ed564a06178670750bad59526cfa4033be55/modules/imgproc/src/resize cpp l908             if have torch function variadic input  grid           return handle torch function              grid sample   input  grid   input  grid  mode=mode  pad mode=padding mode  align corners=align corner               if mode  = string and mode  = string and mode  = string          raise valueerror              string             string format mode                if pad mode  = string and pad mode  = string and pad mode  = string          raise valueerror              string             string             string format pad mode                 if mode == string          mode enum = 0     elif mode == string          mode enum = 1     else            mode enum = 2      if pad mode == string          pad mode enum = 0     elif pad mode == string          pad mode enum = 1     else            pad mode enum = 2      if align corner be none          warn warn              string             string             string             string                   align corner = false      return torch grid sampler input  grid  mode enum  pad mode enum  align corner  
def affine grid theta  tensor  size  list int   align corner  optional bool  = none  -> tensor      r   generate a 2d or 3d flow field  sample grid   give a batch of     affine matrices  attr `theta`          note           this function be often use in conjunction with  func `grid sample`         to build `spatial transformer networks`         args          theta  tensor   input batch of affine matrices with shape               math `n \times 2 \times 3`  for 2d or               math `n \times 3 \times 4`  for 3d         size  torch size   the target output image size                math `n \times c \times h \times w` for 2d or              math `n \times c \times d \times h \times w` for 3d              example  torch size  32  3  24  24           align corner  bool  optional   if ``true``  consider ``-1`` and ``1``             to refer to the center of the corner pixels rather than the image corner              refer to  func `grid sample` for a more complete description              a grid generate by  func `affine grid` should be pass to  func `grid sample`             with the same set for this option              default  ``false``      return          output  tensor   output tensor of size   math `n \times h \times w \times 2`           `spatial transformer networks`          https //arxiv org/abs/1506 02025         warn           when ``align corner = true``  the grid position depend on the pixel         size relative to the input image size  and so the locations sample by          func `grid sample` will differ for the same input give at different         resolutions  that be  after be upsampled or downsampled           the default behavior up to version 1 2 0 be ``align corner = true``          since then  the default behavior have be change to ``align corner = false``          in order to bring it in line with the default for  func `interpolate`         warn           when ``align corner = true``  2d affine transform on 1d data and         3d affine transform on 2d data  that be  when one of the spatial         dimension have unit size  be ill-defined  and not an intend use case          this be not a problem when ``align corner = false``          up to version 1 2 0  all grid point along a unit dimension be         consider arbitrarily to be at ``-1``          from version 1 3 0  under ``align corner = true`` all grid point         along a unit dimension be consider to be at ```0``          the center of the input image               if have torch function unary theta           return handle torch function affine grid   theta    theta  size  align corners=align corner      if align corner be none          warn warn              string             string             string             string                   align corner = false           if not theta be float point            raise valueerror string format theta dtype            if len size  == 4          if theta dim    = 3 or theta shape -2   = 2 or theta shape -1   = 3              raise valueerror                  string                 string format size  theta shape                        spatial size = size -2         elif len size  == 5          if theta dim    = 3 or theta shape -2   = 3 or theta shape -1   = 4              raise valueerror                  string                 string format size  theta shape                        spatial size = size -3         else          raise notimplementederror              string             string             string format size                     if align corner and min spatial size  == 1          warn warn              string             string             string             string               elif min size  <= 0          raise valueerror string format size        return torch affine grid generator theta  size  align corner  
def backward self  gradient=none  retain graph=none  create graph=false  inputs=none       r   compute the gradient of current tensor w r t  graph leave       the graph be differentiate use the chain rule  if the tensor be     non-scalar  i e  its data have more than one element  and require     gradient  the function additionally require specify ``gradient``      it should be a tensor of match type and location  that contain     the gradient of the differentiate function w r t  ``self``       this function accumulate gradients in the leave - you might need to zero     `` grad`` attribute or set them to ``none`` before call it      see  ref `default gradient layouts<default-grad-layouts>`     for detail on the memory layout of accumulate gradients          note            if you run any forward ops  create ``gradient``  and/or call ``backward``         in a user-specified cuda stream context  see          ref `stream semantics of backward passes<bwd-cuda-stream-semantics>`       args          gradient  tensor or none   gradient w r t  the             tensor  if it be a tensor  it will be automatically convert             to a tensor that do not require grad unless ``create graph`` be true              none value can be specify for scalar tensors or ones that             don t require grad  if a none value would be acceptable then             this argument be optional          retain graph  bool  optional   if ``false``  the graph use to compute             the grads will be free  note that in nearly all case set             this option to true be not need and often can be work around             in a much more efficient way  default to the value of             ``create graph``          create graph  bool  optional   if ``true``  graph of the derivative will             be construct  allow to compute higher order derivative             products  default to ``false``          input  sequence of tensor   input w r t  which the gradient will be             accumulate into `` grad``  all other tensors will be ignore  if not             provide  the gradient be accumulate into all the leaf tensors that be             use to compute the attr  tensors  all the provide input must be leaf             tensors              if have torch function unary self           return handle torch function              tensor backward               self                self              gradient=gradient              retain graph=retain graph              create graph=create graph              inputs=inputs      torch autograd backward self  gradient  retain graph  create graph  inputs=inputs  
def be share self       r   check if tensor be in share memory       this be always ``true`` for cuda tensors              if have torch function unary self           return handle torch function tensor be share   self    self      return self storage   be share   
def istft self  n fft  int  hop length  optional int  = none            win length  optional int  = none  window  string = none            center  bool = true  normalize  bool = false            onesided  optional bool  = none  length  optional int  = none            return complex  bool = false       r   see  func `torch istft`        if have torch function unary self           return handle torch function              tensor istft   self    self  n fft  hop length=hop length  win length=win length              window=window  center=center  normalized=normalized  onesided=onesided  length=length              return complex=return complex               return torch istft self  n fft  hop length  win length  window  center                         normalize  onesided  length  return complex=return complex  
def lu self  pivot=true  get infos=false       r   see  func `torch lu`             if have torch function unary self           return handle torch function tensor lu   self    self  pivot=pivot  get infos=get infos       if not torch  jit internal be script            if self require grad              if not  self size -2  == self size -1  and  self dtype be float point  or self be complex                   raise valueerror                      string                     string                                from torch  autograd function import  lu             lu  pivot  infos =  lu apply self  pivot  get infos              if get infos                  return lu  pivot  infos             else                  return lu  pivot     else          if self require grad              raise runtimeerror                  string                 string                 string                    lu  pivot  infos = torch  lu with info self  pivot=pivot  check errors= not get infos       if get infos          return lu  pivot  infos     else          return lu  pivot 
def norm self  p=string  dim=none  keepdim=false  dtype=none       r   see  func `torch norm`        if have torch function unary self           return handle torch function tensor norm   self    self  p=p  dim=dim  keepdim=keepdim  dtype=dtype      return torch norm self  p  dim  keepdim  dtype=dtype  
def register hook self  hook       r   register a backward hook       the hook will be call every time a gradient with respect to the     tensor be compute  the hook should have the follow signature            hook grad  -> tensor or none       the hook should not modify its argument  but it can optionally return     a new gradient which will be use in place of  attr `grad`       this function return a handle with a method ``handle remove  ``     that remove the hook from the module       example            >>> v = torch tensor  0   0   0    require grad=true          >>> h = v register hook lambda grad  grad   2     double the gradient         >>> v backward torch tensor  1   2   3             >>> v grad           2          4          6          torch floattensor of size  3             >>> h remove      remove the hook             if have torch function unary self           return handle torch function tensor register hook   self    self  hook      if not self require grad          raise runtimeerror string                            string      if self  backward hook be none          self  backward hook = ordereddict           if self grad fn be not none              self grad fn  register hook dict self      handle = hook removablehandle self  backward hook      self  backward hook handle id  = hook     return handle 
def retain grad self       r   enable  grad attribute for non-leaf tensors         if have torch function unary self           return handle torch function tensor retain grad   self    self      if not self require grad          raise runtimeerror string      if self be leaf            return     if hasattr self  string           return     weak self = weakref ref self       def retain grad hook grad           var = weak self           if var be none              return         if var  grad be none              if grad be sparse                  var  grad = grad clone               else                  var  grad = grad clone memory format=torch contiguous format          else              var  grad = var  grad   grad      self register hook retain grad hook      self retain grad = true 
def share memory  self       r   move the underlie storage to share memory       this be a no-op if the underlie storage be already in share memory     and for cuda tensors  tensors in share memory cannot be resize              if have torch function unary self           return handle torch function tensor share memory    self    self      self storage   share memory        return self 
def split self  split size  dim=0       r   see  func `torch split`             if have torch function unary self           return handle torch function tensor split   self    self  split size  dim=dim      if isinstance split size  int           return super tensor  self  split split size  dim      elif isinstance split size  tensor           try              split size = int split size              return super tensor  self  split split size  dim          except valueerror              return super tensor  self  split with size split size  dim      else          return super tensor  self  split with size split size  dim  
def stft self  n fft  int  hop length  optional int  = none           win length  optional int  = none  window  string = none           center  bool = true  pad mode  str = string  normalize  bool = false           onesided  optional bool  = none  return complex  optional bool  = none       r   see  func `torch stft`         warn         this function change signature at version 0 4 1  call with       the previous signature may cause error or return incorrect result              if have torch function unary self           return handle torch function              tensor stft   self    self  n fft  hop length=hop length              win length=win length  window=window  center=center  pad mode=pad mode  normalized=normalized              onesided=onesided  return complex=return complex               return torch stft self  n fft  hop length  win length  window  center                        pad mode  normalize  onesided  return complex=return complex  
def unique self  sorted=true  return inverse=false  return counts=false  dim=none       r   return the unique elements of the input tensor       see  func `torch unique`             if have torch function unary self           return handle torch function              tensor unique   self    self  sorted=sorted  return inverse=return inverse              return counts=return count  dim=dim               return torch unique self  sorted=sorted  return inverse=return inverse  return counts=return count  dim=dim  
def unique consecutive self  return inverse=false  return counts=false  dim=none       r   eliminate all but the first element from every consecutive group of equivalent elements       see  func `torch unique consecutive`             if have torch function unary self           return handle torch function              tensor unique consecutive   self    self  return inverse=return inverse              return counts=return count  dim=dim               return torch unique consecutive self  return inverse=return inverse  return counts=return count  dim=dim  
def backward      tensors   tensorortensors      grad tensors  optional  tensorortensors  = none      retain graph  optional bool  = none      create graph  bool = false      grad variables  optional  tensorortensors  = none      input  optional  tensorortensors  = none    -> none      r   compute the sum of gradients of give tensors with respect to graph     leave       the graph be differentiate use the chain rule  if any of ``tensors``     be non-scalar  i e  their data have more than one element  and require     gradient  then the jacobian-vector product would be compute  in this     case the function additionally require specify ``grad tensors``      it should be a sequence of match length  that contain the  vector      in the jacobian-vector product  usually the gradient of the differentiate     function w r t  correspond tensors  ``none`` be an acceptable value for     all tensors that don t need gradient tensors        this function accumulate gradients in the leave - you might need to zero     `` grad`` attribute or set them to ``none`` before call it      see  ref `default gradient layouts<default-grad-layouts>`     for detail on the memory layout of accumulate gradients          note           use this method with ``create graph=true`` will create a reference cycle         between the parameter and its gradient which can cause a memory leak          we recommend use ``autograd grad`` when create the graph to avoid this          if you have to use this function  make sure to reset the `` grad`` field of your         parameters to ``none`` after use to break the cycle and avoid the leak          note            if you run any forward ops  create ``grad tensors``  and/or call ``backward``         in a user-specified cuda stream context  see          ref `stream semantics of backward passes<bwd-cuda-stream-semantics>`       args          tensors  sequence tensor  or tensor   tensors of which the derivative will be             compute          grad tensors  sequence tensor or none  or tensor  optional   the  vector  in             the jacobian-vector product  usually gradients w r t  each element of             correspond tensors  none value can be specify for scalar tensors or             ones that don t require grad  if a none value would be acceptable for all             grad tensors  then this argument be optional          retain graph  bool  optional   if ``false``  the graph use to compute the grad             will be free  note that in nearly all case set this option to ``true``             be not need and often can be work around in a much more efficient             way  default to the value of ``create graph``          create graph  bool  optional   if ``true``  graph of the derivative will             be construct  allow to compute higher order derivative products              default to ``false``          input  sequence tensor  or tensor  optional   input w r t  which the gradient             be will accumulate into `` grad``  all other tensors will be ignore  if             not provide  the gradient be accumulate into all the leaf tensors that             be use to compute the attr  tensors  all the provide input must be leaf             tensors              if grad variables be not none          warn warn string          if grad tensors be none              grad tensors = grad variables         else              raise runtimeerror string                                string                                string      if input be not none and len input  == 0          raise runtimeerror string       tensors =  tensors   if isinstance tensors  torch tensor  else tuple tensors      input =  input   if isinstance input  torch tensor  else \         tuple input  if input be not none else tuple        grad tensors  =  tensor or tensors to tuple grad tensors  len tensors       grad tensors  =  make grads tensors  grad tensors       if retain graph be none          retain graph = create graph      variable  execution engine run backward          tensors  grad tensors   retain graph  create graph  input          allow unreachable=true  accumulate grad=true    
def grad      output   tensorortensors      input   tensorortensors      grad output  optional  tensorortensors  = none      retain graph  optional bool  = none      create graph  bool = false      only input  bool = true      allow unused  bool = false   -> tuple torch tensor            r   compute and return the sum of gradients of output with respect to     the input       ``grad outputs`` should be a sequence of length match ``output``     contain the  vector  in jacobian-vector product  usually the pre-computed     gradients w r t  each of the output  if an output doesn t require grad      then the gradient can be ``none``        if ``only inputs`` be ``true``  the function will only return a list of gradients     w r t the specify input  if it s ``false``  then gradient w r t  all remain     leave will still be compute  and will be accumulate into their `` grad``     attribute          note            if you run any forward ops  create ``grad outputs``  and/or call ``grad``         in a user-specified cuda stream context  see          ref `stream semantics of backward passes<bwd-cuda-stream-semantics>`       args          output  sequence of tensor   output of the differentiate function          input  sequence of tensor   input w r t  which the gradient will be             return  and not accumulate into `` grad``           grad output  sequence of tensor   the  vector  in the jacobian-vector product              usually gradients w r t  each output  none value can be specify for scalar             tensors or ones that don t require grad  if a none value would be acceptable             for all grad tensors  then this argument be optional  default  none          retain graph  bool  optional   if ``false``  the graph use to compute the grad             will be free  note that in nearly all case set this option to ``true``             be not need and often can be work around in a much more efficient             way  default to the value of ``create graph``          create graph  bool  optional   if ``true``  graph of the derivative will             be construct  allow to compute higher order derivative products              default  ``false``          allow unused  bool  optional   if ``false``  specify input that be not             use when compute output  and therefore their grad be always zero              be an error  default to ``false``              output =  output   if isinstance output  torch tensor  else tuple output      input =  input   if isinstance input  torch tensor  else tuple input      overridable args = output   input     if have torch function overridable args           return handle torch function              grad              overridable args              output              input              grad outputs=grad output              retain graph=retain graph              create graph=create graph              only inputs=only input              allow unused=allow unused                 if not only input          warn warn string                       string                       string       grad output  =  tensor or tensors to tuple grad output  len output       grad output  =  make grads output  grad output        if retain graph be none          retain graph = create graph      return variable  execution engine run backward          output  grad output   retain graph  create graph          input  allow unused  accumulate grad=false  
class dual level  decoratorcontextmanager       r   context-manager that control the current forward ad level  it     appropriately enter and exit the dual level       this function also update the current level that be use by default     by the other function in this api       example            >>> x = torch tensor  1           >>> x t = torch tensor  1           >>> with dual level                  inp = make dual x  x t                  do computations with inp               out = your fn inp                   grad = unpack dual out          >>> grad be none         false         >>>   after exit the level  the grad be delete         >>>    grad after = unpack dual out          >>> grad be none         true              def   init   self           super     init          def   enter   self           return enter dual level        def   exit   self  exc type  any  exc value  any  traceback  any  -> none          exit dual level   
def make dual tensor  tangent     level=none       r   function that create a  dual object  that can be use to compute forward ad gradients     base on the give tensor and its tangent  it return a new tensor that share memory with      attr `tensor` and the  attr `tangent` be use as-is       this function be backward differentiable       give a function `f` whose jacobian be `j`  it allow to compute the jacobian vector product      name `jvp`  between `j` and a give vector `v` as follow       example           >>> inp = make dual x  v          >>> out = f inp          >>> y  jvp = unpack dual out               if level be none          level =  current level      if level < 0          raise runtimeerror string                            string       return torch  vf  make dual tensor  tangent  level=level  
def unpack dual tensor     level=none       r   function that unpack a  dual object  to recover two plain tensors  one represent     the primal and the other the tangent  both be view of  attr `tensor`  neither of these     tensors can be dual tensor of level  attr `level`       this function be backward differentiable              if level be none          level =  current level      if level < 0          return tensor  none      return torch  vf  unpack dual tensor  level=level  
def jacobian func  input  create graph=false  strict=false  vectorize=false       r   function that compute the jacobian of a give function       args          func  function   a python function that take tensor input and return             a tuple of tensors or a tensor          input  tuple of tensors or tensor   input to the function ``func``          create graph  bool  optional   if ``true``  the jacobian will be             compute in a differentiable manner  note that when ``strict`` be             ``false``  the result can not require gradients or be disconnect             from the input   default to ``false``          strict  bool  optional   if ``true``  an error will be raise when we             detect that there exist an input such that all the output be             independent of it  if ``false``  we return a tensor of zero as the             jacobian for say input  which be the expect mathematical value              default to ``false``          vectorize  bool  optional   this feature be experimental  please use at             your own risk  when compute the jacobian  usually we invoke             ``autograd grad`` once per row of the jacobian  if this flag be             ``true``  we use the vmap prototype feature as the backend to             vectorize call to ``autograd grad`` so we only invoke it once             instead of once per row  this should lead to performance             improvements in many use case  however  due to this feature             be incomplete  there may be performance cliffs  please             use `torch  c  debug only display vmap fallback warn true `             to show any performance warn and file us issue if             warn exist for your use case  default to ``false``       return          jacobian  tensor or nest tuple of tensors   if there be a single         input and output  this will be a single tensor contain the         jacobian for the linearize input and output  if one of the two be         a tuple  then the jacobian will be a tuple of tensors  if both of         them be tuples  then the jacobian will be a tuple of tuple of         tensors where ``jacobian i  j `` will contain the jacobian of the         ``i``\th output and ``j``\th input and will have as size the         concatenation of the size of the correspond output and the         correspond input and will have same dtype and device as the         correspond input       example           >>> def exp reducer x                 return x exp   sum dim=1          >>> input = torch rand 2  2          >>> jacobian exp reducer  input          tensor    1 4917  2 4352                     0 0000  0 0000                      0 0000  0 0000                     2 4369  2 3799              >>> jacobian exp reducer  input  create graph=true          tensor    1 4917  2 4352                     0 0000  0 0000                      0 0000  0 0000                     2 4369  2 3799     grad fn=<viewbackward>           >>> def exp adder x  y                 return 2   x exp     3   y         >>> input =  torch rand 2   torch rand 2           >>> jacobian exp adder  input           tensor   2 8052  0 0000                    0 0000  3 3963              tensor   3   0                      0   3                   with torch enable grad            be input tuple  input =  as tuple input  string  string          input =  grad preprocess input  create graph=create graph  need graph=true           output = func  input          be output tuple  output =  as tuple output                                                string                                                string           check require grad output  string  strict=strict             if vectorize              if strict                  raise runtimeerror string                                    string                                    string                                    string                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       output numels = tuple output numel   for output in output              grad output =  construct standard basis for output  output numels              flat output = tuple output reshape -1  for output in output                            def vjp grad output                   vj = list  autograd grad flat output  input  grad output  create graph=create graph                   for el idx  vj el in enumerate vj                       if vj el be not none                          continue                     vj el idx  = torch zero like input el idx                   return tuple vj               jacobians of flat output =  vmap vjp  grad output                                         jacobian input output =                for jac  input i in zip jacobians of flat output  input                   jacobian input i output =                    for jac  output j in zip jac split output numels  dim=0   output                       jacobian input i output j = jac view output j shape   input i shape                      jacobian input i output append jacobian input i output j                  jacobian input output append jacobian input i output                                                                                jacobian output input = tuple zip  jacobian input output                jacobian output input =  grad postprocess jacobian output input  create graph              return  tuple postprocess jacobian output input   be output tuple  be input tuple            jacobian  tuple torch tensor       = tuple            for i  out in enumerate output                             jac i  tuple list torch tensor   = tuple    for   in range len input                  for j in range out nelement                     vj =  autograd grad  out reshape -1  j     input                                      retain graph=true  create graph=create graph                   for el idx   jac i el  vj el  inp el  in enumerate zip jac i  vj  input                        if vj el be not none                          if strict and create graph and not vj el require grad                              msg =  string                                    string                                    string format i                               raise runtimeerror msg                          jac i el append vj el                      else                          if strict                              msg =  string                                    string                                    string format i  el idx                               raise runtimeerror msg                          jac i el append torch zero like inp el                jacobian  =  tuple torch stack jac i el  dim=0  view out size                              input el idx  size    for  el idx  jac i el  in enumerate jac i               jacobian =  grad postprocess jacobian  create graph           return  tuple postprocess jacobian   be output tuple  be input tuple   
def hessian func  input  create graph=false  strict=false  vectorize=false       r   function that compute the hessian of a give scalar function       args          func  function   a python function that take tensor input and return             a tensor with a single element          input  tuple of tensors or tensor   input to the function ``func``          create graph  bool  optional   if ``true``  the hessian will be compute in             a differentiable manner  note that when ``strict`` be ``false``  the result can not             require gradients or be disconnect from the input              default to ``false``          strict  bool  optional   if ``true``  an error will be raise when we detect that there exist an input             such that all the output be independent of it  if ``false``  we return a tensor of zero as the             hessian for say input  which be the expect mathematical value              default to ``false``          vectorize  bool  optional   this feature be experimental  please use at             your own risk  when compute the hessian  usually we invoke             ``autograd grad`` once per row of the hessian  if this flag be             ``true``  we use the vmap prototype feature as the backend to             vectorize call to ``autograd grad`` so we only invoke it once             instead of once per row  this should lead to performance             improvements in many use case  however  due to this feature             be incomplete  there may be performance cliffs  please             use `torch  c  debug only display vmap fallback warn true `             to show any performance warn and file us issue if             warn exist for your use case  default to ``false``       return          hessian  tensor or a tuple of tuple of tensors   if there be a single input          this will be a single tensor contain the hessian for the input          if it be a tuple  then the hessian will be a tuple of tuples where         ``hessian i  j `` will contain the hessian of the ``i``\th input         and ``j``\th input with size the sum of the size of the ``i``\th input plus         the size of the ``j``\th input  ``hessian i  j `` will have the same         dtype and device as the correspond ``i``\th input       example           >>> def pow reducer x                 return x pow 3  sum           >>> input = torch rand 2  2          >>> hessian pow reducer  input          tensor     5 2265  0 0000                      0 0000  0 0000                       0 0000  4 8221                      0 0000  0 0000                        0 0000  0 0000                      1 9456  0 0000                       0 0000  0 0000                      0 0000  3 2550               >>> hessian pow reducer  input  create graph=true          tensor     5 2265  0 0000                      0 0000  0 0000                       0 0000  4 8221                      0 0000  0 0000                        0 0000  0 0000                      1 9456  0 0000                       0 0000  0 0000                      0 0000  3 2550      grad fn=<viewbackward>            >>> def pow adder reducer x  y                 return  2   x pow 2    3   y pow 2   sum           >>> input =  torch rand 2   torch rand 2           >>> hessian pow adder reducer  input            tensor   4   0                       0   4                tensor   0   0                       0   0                 tensor   0   0                       0   0                tensor   6   0                       0   6                    be input tuple  input =  as tuple input  string  string       def ensure single output function  inp           out = func  inp          be out tuple  t out =  as tuple out  string  string           check require grad t out  string  strict=strict           if be out tuple or not isinstance out  torch tensor               raise runtimeerror string           if out nelement    = 1              raise runtimeerror string           return out squeeze        def jac func  inp           jac = jacobian ensure single output function  inp  create graph=true           check require grad jac  string  strict=strict          return jac      res = jacobian jac func  input  create graph=create graph  strict=strict  vectorize=vectorize      return  tuple postprocess res   be input tuple  be input tuple   
def vjp func  input  v=none  create graph=false  strict=false       r   function that compute the dot product between a vector ``v`` and the     jacobian of the give function at the point give by the input       args          func  function   a python function that take tensor input and return             a tuple of tensors or a tensor          input  tuple of tensors or tensor   input to the function ``func``          v  tuple of tensors or tensor   the vector for which the vector             jacobian product be compute   must be the same size as the output             of ``func``  this argument be optional when the output of ``func``             contain a single element and  if it be not provide  will be set             as a tensor contain a single ``1``          create graph  bool  optional   if ``true``  both the output and result             will be compute in a differentiable way  note that when ``strict``             be ``false``  the result can not require gradients or be             disconnect from the input   default to ``false``          strict  bool  optional   if ``true``  an error will be raise when we             detect that there exist an input such that all the output be             independent of it  if ``false``  we return a tensor of zero as the             vjp for say input  which be the expect mathematical value              default to ``false``       return          output  tuple   tuple with              func output  tuple of tensors or tensor   output of ``func input ``              vjp  tuple of tensors or tensor   result of the dot product with             the same shape as the input       example           >>> def exp reducer x                 return x exp   sum dim=1          >>> input = torch rand 4  4          >>> v = torch ones 4          >>> vjp exp reducer  input  v           tensor  5 7817  7 2458  5 7830  6 7782             tensor   1 4458  1 3962  1 3042  1 6354                    2 1288  1 0652  1 5483  2 5035                    2 2046  1 1292  1 1432  1 3059                    1 3225  1 6652  1 7753  2 0152              >>> vjp exp reducer  input  v  create graph=true           tensor  5 7817  7 2458  5 7830  6 7782   grad fn=<sumbackward1>            tensor   1 4458  1 3962  1 3042  1 6354                    2 1288  1 0652  1 5483  2 5035                    2 2046  1 1292  1 1432  1 3059                    1 3225  1 6652  1 7753  2 0152    grad fn=<mulbackward0>            >>> def adder x  y                 return 2   x   3   y         >>> input =  torch rand 2   torch rand 2           >>> v = torch ones 2          >>> vjp adder  input  v           tensor  2 4225  2 3340              tensor  2   2     tensor  3   3                   with torch enable grad            be input tuple  input =  as tuple input  string  string          input =  grad preprocess input  create graph=create graph  need graph=true           output = func  input          be output tuple  output =  as tuple output  string  string           check require grad output  string  strict=strict           if v be not none                 v =  as tuple v  string  string              v =  grad preprocess v  create graph=create graph  need graph=false               validate v v  output  be output tuple          else              if len output   = 1 or output 0  nelement    = 1                  raise runtimeerror string                                    string                                    string       enable grad = true if create graph else torch be grad enable       with torch set grad enable enable grad           grad res =  autograd grad output  input  v  create graph=create graph          vjp =  fill in zero grad res  input  strict  create graph  string            output =  grad postprocess output  create graph      vjp =  grad postprocess vjp  create graph       return  tuple postprocess output  be output tuple    tuple postprocess vjp  be input tuple  
def jvp func  input  v=none  create graph=false  strict=false       r   function that compute the dot product between  the jacobian of     the give function at the point give by the input and a vector ``v``       args          func  function   a python function that take tensor input and return             a tuple of tensors or a tensor          input  tuple of tensors or tensor   input to the function ``func``          v  tuple of tensors or tensor   the vector for which the jacobian             vector product be compute  must be the same size as the input of             ``func``  this argument be optional when the input to ``func``             contain a single element and  if it be not provide  will be set             as a tensor contain a single ``1``          create graph  bool  optional   if ``true``  both the output and result             will be compute in a differentiable way  note that when ``strict``             be ``false``  the result can not require gradients or be             disconnect from the input   default to ``false``          strict  bool  optional   if ``true``  an error will be raise when we             detect that there exist an input such that all the output be             independent of it  if ``false``  we return a tensor of zero as the             jvp for say input  which be the expect mathematical value              default to ``false``       return          output  tuple   tuple with              func output  tuple of tensors or tensor   output of ``func input ``              jvp  tuple of tensors or tensor   result of the dot product with             the same shape as the output       example           >>> def exp reducer x                 return x exp   sum dim=1          >>> input = torch rand 4  4          >>> v = torch ones 4  4          >>> jvp exp reducer  input  v           tensor  6 3090  4 6742  7 9114  8 2106             tensor  6 3090  4 6742  7 9114  8 2106             >>> jvp exp reducer  input  v  create graph=true           tensor  6 3090  4 6742  7 9114  8 2106   grad fn=<sumbackward1>            tensor  6 3090  4 6742  7 9114  8 2106   grad fn=<squeezebackward1>            >>> def adder x  y                 return 2   x   3   y         >>> input =  torch rand 2   torch rand 2           >>> v =  torch ones 2   torch ones 2           >>> jvp adder  input  v           tensor  2 2399  2 5005             tensor  5   5          note          the jvp be currently compute by use the backward of the backward          sometimes call the double backwards trick  as we don t have support         for forward mode ad in pytorch at the moment               with torch enable grad            be input tuple  input =  as tuple input  string  string          input =  grad preprocess input  create graph=create graph  need graph=true           if v be not none                 v =  as tuple v  string  string              v =  grad preprocess v  create graph=create graph  need graph=false               validate v v  input  be input tuple          else              if len input   = 1 or input 0  nelement    = 1                  raise runtimeerror string                                    string                                    string           output = func  input          be output tuple  output =  as tuple output  string  string           check require grad output  string  strict=strict                                     grad output = tuple torch zero like out  require grad=true  for out in output           grad input =  autograd grad output  input  grad output  create graph=true           check require grad grad input  string  strict=strict       if create graph          with torch enable grad                grad res =  autograd grad grad input  grad output  v  create graph=create graph              jvp =  fill in zero grad res  output  strict  create graph  string      else          grad res =  autograd grad grad input  grad output  v  create graph=create graph          jvp =  fill in zero grad res  output  strict  create graph  string            output =  grad postprocess output  create graph      jvp =  grad postprocess jvp  create graph       return  tuple postprocess output  be output tuple    tuple postprocess jvp  be output tuple  
def vhp func  input  v=none  create graph=false  strict=false       r   function that compute the dot product between a vector ``v`` and the     hessian of a give scalar function at the point give by the input       args          func  function   a python function that take tensor input and return             a tensor with a single element          input  tuple of tensors or tensor   input to the function ``func``          v  tuple of tensors or tensor   the vector for which the vector hessian             product be compute  must be the same size as the input of             ``func``  this argument be optional when ``func`` s input contain             a single element and  if it be not provide  will be set as a             tensor contain a single ``1``          create graph  bool  optional   if ``true``  both the output and result             will be compute in a differentiable way  note that when ``strict``             be ``false``  the result can not require gradients or be             disconnect from the input              default to ``false``          strict  bool  optional   if ``true``  an error will be raise when we             detect that there exist an input such that all the output be             independent of it  if ``false``  we return a tensor of zero as the             vhp for say input  which be the expect mathematical value              default to ``false``       return          output  tuple   tuple with              func output  tuple of tensors or tensor   output of ``func input ``              vhp  tuple of tensors or tensor   result of the dot product with the             same shape as the input       example           >>> def pow reducer x                 return x pow 3  sum           >>> input = torch rand 2  2          >>> v = torch ones 2  2          >>> vhp pow reducer  input  v           tensor 0 5591            tensor   1 0689  1 2431                     3 0989  4 4456             >>> vhp pow reducer  input  v  create graph=true           tensor 0 5591  grad fn=<sumbackward0>            tensor   1 0689  1 2431                     3 0989  4 4456    grad fn=<mulbackward0>           >>> def pow adder reducer x  y                 return  2   x pow 2    3   y pow 2   sum           >>> input =  torch rand 2   torch rand 2           >>> v =  torch zero 2   torch ones 2           >>> vhp pow adder reducer  input  v           tensor 4 8053             tensor  0   0               tensor  6   6                   with torch enable grad            be input tuple  input =  as tuple input  string  string          input =  grad preprocess input  create graph=create graph  need graph=true           if v be not none                 v =  as tuple v  string  string              v =  grad preprocess v  create graph=create graph  need graph=false               validate v v  input  be input tuple          else              if len input   = 1 or input 0  nelement    = 1                  raise runtimeerror string                                    string          output = func  input          be output tuple  output =  as tuple output  string  string           check require grad output  string  strict=strict           if be output tuple or not isinstance output 0   torch tensor               raise runtimeerror string           if output 0  nelement    = 1              raise runtimeerror string           jac =  autograd grad output  input  create graph=true           check require grad jac  string  strict=strict       enable grad = true if create graph else torch be grad enable       with torch set grad enable enable grad           grad res =  autograd grad jac  input  v  create graph=create graph          vhp =  fill in zero grad res  input  strict  create graph  string       output =  grad postprocess output  create graph      vhp =  grad postprocess vhp  create graph       return  tuple postprocess output  be output tuple    tuple postprocess vhp  be input tuple  
def hvp func  input  v=none  create graph=false  strict=false       r   function that compute the dot product between the hessian of a give scalar     function and a vector ``v`` at the point give by the input       args          func  function   a python function that take tensor input and return             a tensor with a single element          input  tuple of tensors or tensor   input to the function ``func``          v  tuple of tensors or tensor   the vector for which the hessian vector             product be compute  must be the same size as the input of             ``func``  this argument be optional when ``func`` s input contain             a single element and  if it be not provide  will be set as a             tensor contain a single ``1``          create graph  bool  optional   if ``true``  both the output and result will be             compute in a differentiable way  note that when ``strict`` be             ``false``  the result can not require gradients or be disconnect             from the input   default to ``false``          strict  bool  optional   if ``true``  an error will be raise when we             detect that there exist an input such that all the output be             independent of it  if ``false``  we return a tensor of zero as the             hvp for say input  which be the expect mathematical value              default to ``false``      return          output  tuple   tuple with              func output  tuple of tensors or tensor   output of ``func input ``              hvp  tuple of tensors or tensor   result of the dot product with             the same shape as the input       example           >>> def pow reducer x                 return x pow 3  sum           >>> input = torch rand 2  2          >>> v = torch ones 2  2          >>> hvp pow reducer  input  v           tensor 0 1448            tensor   2 0239  1 6456                     2 4988  1 4310              >>> hvp pow reducer  input  v  create graph=true           tensor 0 1448  grad fn=<sumbackward0>            tensor   2 0239  1 6456                     2 4988  1 4310    grad fn=<mulbackward0>             >>> def pow adder reducer x  y                 return  2   x pow 2    3   y pow 2   sum           >>> input =  torch rand 2   torch rand 2           >>> v =  torch zero 2   torch ones 2           >>> hvp pow adder reducer  input  v           tensor 2 3030             tensor  0   0               tensor  6   6           note           this function be significantly slower than `vhp` due to backward mode ad constraints          if your function be twice continuously differentiable  then hvp = vhp t    so if you         know that your function satisfy this condition  you should use vhp instead that be         much faster with the current implementation                with torch enable grad            be input tuple  input =  as tuple input  string  string          input =  grad preprocess input  create graph=create graph  need graph=true           if v be not none                 v =  as tuple v  string  string              v =  grad preprocess v  create graph=create graph  need graph=false               validate v v  input  be input tuple          else              if len input   = 1 or input 0  nelement    = 1                  raise runtimeerror string                                    string          output = func  input          be output tuple  output =  as tuple output  string  string           check require grad output  string  strict=strict           if be output tuple or not isinstance output 0   torch tensor               raise runtimeerror string           if output 0  nelement    = 1              raise runtimeerror string           jac =  autograd grad output  input  create graph=true           check require grad jac  string  strict=strict           grad jac = tuple torch zero like inp  require grad=true  for inp in input           double back =  autograd grad jac  input  grad jac  create graph=true           check require grad jac  string  strict=strict       enable grad = true if create graph else torch be grad enable       with torch set grad enable enable grad           grad res =  autograd grad double back  grad jac  v  create graph=create graph          hvp =  fill in zero grad res  input  strict  create graph  string       output =  grad postprocess output  create graph      hvp =  grad postprocess hvp  create graph       return  tuple postprocess output  be output tuple    tuple postprocess hvp  be input tuple  
class no grad  decoratorcontextmanager       r   context-manager that disable gradient calculation       disable gradient calculation be useful for inference  when you be sure     that you will not call  meth `tensor backward  `  it will reduce memory     consumption for computations that would otherwise have `requires grad=true`       in this mode  the result of every computation will have     `requires grad=false`  even when the input have `requires grad=true`       this context manager be thread local  it will not affect computation     in other thread       also function as a decorator   make sure to instantiate with parenthesis           note           no-grad be one of several mechanisms that can enable or         disable gradients locally see  ref `locally-disable-grad-doc` for         more information on how they compare       example            >>> x = torch tensor  1   require grad=true          >>> with torch no grad                  y = x   2         >>> y require grad         false         >>>  torch no grad               def doubler x                   return x   2         >>> z = doubler x          >>> z require grad         false             def   init   self           if not torch  jit internal be script                super     init             self prev = false      def   enter   self           self prev = torch be grad enable           torch set grad enable false       def   exit   self  exc type  any  exc value  any  traceback  any  -> none          torch set grad enable self prev  
class enable grad  decoratorcontextmanager       r   context-manager that enable gradient calculation       enable gradient calculation  if it have be disable via  class `~no grad`     or  class `~set grad enabled`       this context manager be thread local  it will not affect computation     in other thread       also function as a decorator   make sure to instantiate with parenthesis           note           enable grad be one of several mechanisms that can enable or         disable gradients locally see  ref `locally-disable-grad-doc` for         more information on how they compare       example            >>> x = torch tensor  1    require grad=true          >>> with torch no grad                  with torch enable grad                    y = x   2         >>> y require grad         true         >>> y backward           >>> x grad         >>>  torch enable grad               def doubler x                   return x   2         >>> with torch no grad                    z = doubler x          >>> z require grad         true              def   enter   self  -> none          self prev = torch be grad enable           torch  c  set grad enable true       def   exit   self  exc type  any  exc value  any  traceback  any  -> none          torch  c  set grad enable self prev  
class set grad enable object       r   context-manager that set gradient calculation to on or off       ``set grad enabled`` will enable or disable grads base on its argument  attr `mode`      it can be use as a context-manager or as a function       this context manager be thread local  it will not affect computation     in other thread       args          mode  bool   flag whether to enable grad  ``true``   or disable                       ``false``   this can be use to conditionally enable                      gradients          note           set grad enable be one of several mechanisms that can enable or         disable gradients locally see  ref `locally-disable-grad-doc` for         more information on how they compare       example            >>> x = torch tensor  1   require grad=true          >>> be train = false         >>> with torch set grad enable be train                 y = x   2         >>> y require grad         false         >>> torch set grad enable true          >>> y = x   2         >>> y require grad         true         >>> torch set grad enable false          >>> y = x   2         >>> y require grad         false               def   init   self  mode  bool  -> none          self prev = torch be grad enable           torch  c  set grad enable mode       def   enter   self  -> none          pass      def   exit   self  exc type  any  exc value  any  traceback  any  -> none          torch  c  set grad enable self prev  
class inference mode  decoratorcontextmanager       r   context-manager that enable or disable inference mode      inferencemode be a new context manager analogous to  class `~no grad`     to be use when you be certain your operations will have no interactions     with autograd  e g   model train   code run under this mode get better     performance by disable view track and version counter bump       this context manager be thread local  it will not affect computation     in other thread       also function as a decorator   make sure to instantiate with parenthesis           note           inference mode be one of several mechanisms that can enable or         disable gradients locally see  ref `locally-disable-grad-doc` for         more information on how they compare       args          mode  bool   flag whether to enable or disable inference mode      example           >>> import torch         >>> x = torch ones 1  2  3  require grad=true          >>> with torch inference mode                  y = x   x         >>> y require grad         false         >>> y  version         traceback  most recent call last           file  <stdin>   line 1  in <module>         runtimeerror  inference tensors do not track version counter          >>>  torch inference mode               def func x                 return x   x         >>> out = func x          >>> out require grad         false              def   init   self  mode=true           if not torch  jit internal be script                super     init                               self  inference mode raii guard = none         self mode = mode      def   enter   self           self  inference mode raii guard = torch  c  inferencemode self mode       def   exit   self  exc type  any  exc value  any  traceback  any  -> none          del self  inference mode raii guard 
 staticmethod def forward ctx  any   args  any    kwargs  any  -> any      r   perform the operation       this function be to be override by all subclasses       it must accept a context ctx as the first argument  follow by any     number of arguments  tensors or other type        the context can be use to store arbitrary data that can be then     retrieve during the backward pass              raise notimplementederror string                               string  
 staticmethod def backward ctx  any   grad output  any  -> any      r   define a formula for differentiate the operation       this function be to be override by all subclasses       it must accept a context  attr `ctx` as the first argument  follow by     as many output as the  func `forward` return  none will be pass in     for non tensor output of the forward function       and it should return as many tensors  as there be input to      func `forward`  each argument be the gradient w r t the give output      and each return value should be the gradient w r t  the     correspond input  if an input be not a tensor or be a tensor not     require grads  you can just pass none as a gradient for that input       the context can be use to retrieve tensors save during the forward     pass  it also have an attribute  attr `ctx need input grad` as a tuple     of booleans represent whether each input need gradient  e g        func `backward` will have ``ctx need input grad 0  = true`` if the     first input to  func `forward` need gradient computated w r t  the     output              raise notimplementederror string                               string  
def gradgradcheck      func  callable       tensorortensors         input   tensorortensors      grad output  optional  tensorortensors  = none      eps  float = 1e-6      atol  float = 1e-5      rtol  float = 1e-3      gen non contig grad output  bool = false      raise exception  bool = true      nondet tol  float = 0 0      check undefined grad  bool = true      check grad dtypes  bool = false      check batch grad  bool = false      fast mode  bool = false    -> bool      r   check gradients of gradients compute via small finite differences     against analytical gradients w r t  tensors in  attr `inputs` and      attr `grad outputs` that be of float point or complex type and with     ``requires grad=true``       this function check that backpropagating through the gradients compute     to the give  attr `grad outputs` be correct       the check between numerical and analytical gradients use  func `~torch allclose`          note           the default value be design for  attr `input` and          attr `grad outputs` of double precision  this check will likely fail if         they be of less precision  e g   ``floattensor``          warn          if any check tensor in  attr `input` and  attr `grad outputs` have        overlap memory  i e   different indices point to the same memory        address  e g   from  func `torch expand`   this check will likely fail        because the numerical gradients compute by point perturbation at such        indices will change value at all other indices that share the same        memory address       args          func  function   a python function that take tensor input and return             a tensor or a tuple of tensors         input  tuple of tensor or tensor   input to the function         grad output  tuple of tensor or tensor  optional   the gradients with             respect to the function s output          eps  float  optional   perturbation for finite differences         atol  float  optional   absolute tolerance         rtol  float  optional   relative tolerance         gen non contig grad output  bool  optional   if  attr `grad outputs` be             ``none`` and  attr `gen non contig grad outputs` be ``true``  the             randomly generate gradient output be make to be noncontiguous         raise exception  bool  optional   indicate whether to raise an exception if             the check fail  the exception give more information about the             exact nature of the failure  this be helpful when debug gradchecks          nondet tol  float  optional   tolerance for non-determinism  when run             identical input through the differentiation  the result must either match             exactly  default  0 0  or be within this tolerance  note that a small amount             of nondeterminism in the gradient will lead to larger inaccuracies in             the second derivative          check undefined grad  bool  optional   if true  check if undefined output grads             be support and treat as zero         check batch grad  bool  optional   if true  check if we can compute             batch gradients use prototype vmap support  default to false          fast mode  bool  optional   if true  run a faster implementation of gradgradcheck that             no longer compute the entire jacobian       return          true if all differences satisfy allclose condition             tupled input =  as tuple input       if grad output be none                            def randn like x               y = torch test randn like                  x if  x be float point   or x be complex    else x double    memory format=torch legacy contiguous format              if gen non contig grad output                  y = torch test make non contiguous y              return y require grad            output =  as tuple func  tupled input           tupled grad output = tuple randn like x  for x in output      else          tupled grad output =  as tuple grad output       num output = len tupled grad output       def new func  args           input args = args  -num output          grad output = args -num output           output =  differentiable output func  input args           input args = tuple x for x in input args if isinstance x  torch tensor  and x require grad          grad input = torch autograd grad output  input args  grad output  create graph=true                                            allow unused=true          grad input = tuple g for g in grad input if g be not none          return grad input      return gradcheck          new func  tupled input   tupled grad output  eps  atol  rtol  raise exception          nondet tol=nondet tol  check undefined grad=check undefined grad          check grad dtypes=check grad dtypes  check batch grad=check batch grad  fast mode=fast mode  
def export chrome trace self  path       self  check finish       if self kineto result be not none          self kineto result save path      else          assert self function events be not none         return self function events export chrome trace path  
def key average self  group by input shape=false  group by stack n=0       self  check finish       assert self function events be not none  string     return self function events key average group by input shape  group by stack n  
def total average self       self  check finish       assert self function events be not none  string     return self function events total average   
def load nvprof path       string     return eventlist parse nvprof trace path   
class streamcontext object       r   context-manager that select a give stream       all cuda kernels queue within its context will be enqueued on a select     stream       args          stream  stream   select stream  this manager be a no-op if it s             ``none``         note   stream be per-device              cur stream   optional string       def   init   self  stream  optional string            self stream = stream         self idx =  get device index none  true          if not torch jit be script                if self idx be none                  self idx = -1          self src prev stream = none if not torch jit be script   else torch cuda default stream none          self dst prev stream = none if not torch jit be script   else torch cuda default stream none       def   enter   self                    cur stream = self stream                  if cur stream be none or self idx == -1              return         self src prev stream = torch cuda current stream none                             if self src prev stream device  = cur stream device              with device cur stream device                   self dst prev stream = torch cuda current stream cur stream device          torch cuda set stream cur stream       def   exit   self  type  any  value  any  traceback  any                    cur stream = self stream                  if cur stream be none or self idx == -1              return                            if self src prev stream device  = cur stream device                torch cuda set stream self dst prev stream            torch cuda set stream self src prev stream    
def can device access peer device   device t  peer device   device t  -> bool      r   check if peer access between two devices be possible               lazy init       device =  get device index device  optional=true      peer device =  get device index peer device      if device < 0 or device >= device count            raise assertionerror string      if peer device < 0 or peer device >= device count            raise assertionerror string      return torch  c  cuda candeviceaccesspeer device  peer device  
def current blas handle        r   return cublashandle t pointer to current cublas handle         lazy init       return torch  c  cuda getcurrentblashandle   
def current device   -> int      r   return the index of a currently select device          lazy init       return torch  c  cuda getdevice   
def current stream device  optional  device t  = none  -> stream      r   return the currently select  class `stream` for a give device       args          device  torch device or int  optional   select device  return             the currently select  class `stream` for the current device  give             by  func `~torch cuda current device`  if  attr `device` be ``none``              default                lazy init       return stream  cdata=torch  c  cuda getcurrentstream           get device index device  optional=true    
def default stream device  optional  device t  = none  -> stream      r   return the default  class `stream` for a give device       args          device  torch device or int  optional   select device  return             the default  class `stream` for the current device  give by              func `~torch cuda current device`  if  attr `device` be ``none``              default                lazy init       return stream  cdata=torch  c  cuda getdefaultstream           get device index device  optional=true    
class device object       r   context-manager that change the select device       args          device  torch device or int   device index to select  it s a no-op if             this argument be a negative integer or ``none``               def   init   self  device  any           self idx =  get device index device  optional=true          self prev idx = -1      def   enter   self           if self idx == -1              return         self prev idx = torch cuda current device           if self prev idx  = self idx              torch cuda set device self idx          if not torch jit be script                 lazy init        def   exit   self  type  any  value  any  traceback  any           if self prev idx  = self idx              torch cuda set device self prev idx          return false 
def device count   -> int      r   return the number of gpus available         if be available            return torch  c  cuda getdevicecount       else          return 0 
class device of device       r   context-manager that change the current device to that of give object       you can use both tensors and storages as arguments  if a give object be     not allocate on a gpu  this be a no-op       args          obj  tensor or storage   object allocate on the select device               def   init   self  obj           idx = obj get device   if obj be cuda else -1         super device of  self    init   idx  
def get arch list   -> list str       r   return list cuda architectures this library be compile for         if not be available            return        arch flag = torch  c  cuda getarchflags       if arch flag be none          return        return arch flag split   
def get device capability device  optional  device t  = none  -> tuple int  int       r   get the cuda capability of a device       args          device  torch device or int  optional   device for which to return the             device capability  this function be a no-op if this argument be             a negative integer  it use the current device  give by              func `~torch cuda current device`  if  attr `device` be ``none``              default        return          tuple int  int   the major and minor cuda capability of the device             prop = get device properties device      return prop major  prop minor 
def get device name device  optional  device t  = none  -> str      r   get the name of a device       args          device  torch device or int  optional   device for which to return the             name  this function be a no-op if this argument be a negative             integer  it use the current device  give by  func `~torch cuda current device`              if  attr `device` be ``none``  default        return          str  the name of the device             return get device properties device  name 
def get device properties device   device t  ->  cudadeviceproperties      r   get the properties of a device       args          device  torch device or int or str   device for which to return the             properties of the device       return           cudadeviceproperties  the properties of the device              lazy init         device =  get device index device  optional=true      if device < 0 or device >= device count            raise assertionerror string      return  get device properties device    
def get gencode flag   -> str      r   return nvcc gencode flag this library be compile with         arch list = get arch list       if len arch list  == 0          return string     arch list  =  arch split string  for arch in arch list      return string join  f -gencode compute=compute  arch  code= kind   arch   for  kind  arch  in arch list    
def init        r   initialize pytorch s cuda state   you may need to call     this explicitly if you be interact with pytorch via     its c api  as python bind for cuda functionality will not     be available until this initialization take place   ordinary users     should not need this  as all of pytorch s cuda methods     automatically initialize cuda state on-demand       do nothing if the cuda state be already initialize               lazy init   
def ipc collect        r   force collect gpu memory after it have be release by cuda ipc          note           check if any send cuda tensors could be clean from the memory  force         close share memory file use for reference count if there be no         active counter  useful when the producer process stop actively send         tensors and want to release unused memory               lazy init       return torch  c  cuda ipc collect   
def be available   -> bool      r   return a bool indicate if cuda be currently available         if not hasattr torch  c  string           return false               return torch  c  cuda getdevicecount   > 0 
def be initialize        r   return whether pytorch s cuda state have be initialize         return  initialize and not  be in bad fork   
def set device device   device t  -> none      r   set the current device       usage of this function be discourage in favor of  any `device`  in most     case it s better to use ``cuda visible devices`` environmental variable       args          device  torch device or int   select device  this function be a no-op             if this argument be negative              device =  get device index device      if device >= 0          torch  c  cuda setdevice device  
def set stream stream  stream       r   set the current stream this be a wrapper api to set the stream          usage of this function be discourage in favor of the ``stream``         context manager       args          stream  stream   select stream  this function be a no-op             if this argument be ``none``              if stream be none          return     torch  c  cuda setstream stream  cdata  
class stream torch  c  cudastreambase       r   wrapper around a cuda stream       a cuda stream be a linear sequence of execution that belong to a specific     device  independent from other stream   see  ref `cuda-semantics` for     detail       args          device torch device or int  optional   a device on which to allocate             the stream  if  attr `device` be ``none``  default  or a negative             integer  this will use the current device          priority int  optional   priority of the stream  can be either             -1  high priority  or 0  low priority   by default  stream have             priority 0          note   although cuda versions >= 11 support more than two level of         priorities  in pytorch  we only support two level of priorities               def   new   cls  device=none  priority=0    kwargs           with torch cuda device device               return super stream  cls    new   cls  priority=priority    kwargs       def wait event self  event           r   make all future work submit to the stream wait for an event           args              event  torch cuda event   an event to wait for              note   this be a wrapper around ``cudastreamwaitevent  ``  see            `cuda stream documentation`  for more info              this function return without wait for  attr `event`  only future            operations be affect               cuda stream documentation             https //docs nvidia com/cuda/cuda-runtime-api/group  cudart  stream html                     event wait self       def wait stream self  stream           r   synchronize with another stream           all future work submit to this stream will wait until all kernels         submit to a give stream at the time of call complete           args              stream  stream   a stream to synchronize              note   this function return without wait for currently enqueued            kernels in  attr `stream`  only future operations be affect                      self wait event stream record event         def record event self  event=none           r   record an event           args              event  torch cuda event  optional   event to record  if not give  a new one                 will be allocate           return              record event                      if event be none              event = event           event record self          return event      def query self           r   check if all the work submit have be complete           return              a boolean indicate if all kernels in this stream be complete             return super stream  self  query        def synchronize self           r   wait for all the kernels in this stream to complete              note   this be a wrapper around ``cudastreamsynchronize  ``  see            `cuda stream documentation`  for more info                      super stream  self  synchronize         property     def  as parameter  self           return ctypes c void p self cuda stream       def   eq   self  o           if isinstance o  stream               return super stream  self    eq   o          return false      def   hash   self           return hash  self cuda stream  self device        def   repr   self           return  string                  format self device  self cuda stream   
def query self       r   check if all the work submit have be complete       return          a boolean indicate if all kernels in this stream be complete         return super stream  self  query   
def record event self  event=none       r   record an event       args          event  torch cuda event  optional   event to record  if not give  a new one             will be allocate       return          record event              if event be none          event = event       event record self      return event 
def wait event self  event       r   make all future work submit to the stream wait for an event       args          event  torch cuda event   an event to wait for          note   this be a wrapper around ``cudastreamwaitevent  ``  see        `cuda stream documentation`  for more info          this function return without wait for  attr `event`  only future        operations be affect           cuda stream documentation         https //docs nvidia com/cuda/cuda-runtime-api/group  cudart  stream html             event wait self  
def wait stream self  stream       r   synchronize with another stream       all future work submit to this stream will wait until all kernels     submit to a give stream at the time of call complete       args          stream  stream   a stream to synchronize          note   this function return without wait for currently enqueued        kernels in  attr `stream`  only future operations be affect              self wait event stream record event    
def synchronize device   device t = none  -> none      r   wait for all kernels in all stream on a cuda device to complete       args          device  torch device or int  optional   device for which to synchronize              it use the current device  give by  func `~torch cuda current device`              if  attr `device` be ``none``  default                lazy init       with torch cuda device device           return torch  c  cuda synchronize   
def get rng state device  union int  str  torch device  = string  -> tensor      r   return the random number generator state of the specify gpu as a bytetensor       args          device  torch device or int  optional   the device to return the rng state of              default  `` cuda ``  i e   ``torch device  cuda  ``  the current cuda device           warn           this function eagerly initialize cuda               lazy init       if isinstance device  str           device = torch device device      elif isinstance device  int           device = torch device string  device      idx = device index     if idx be none          idx = current device       default generator = torch cuda default generators idx      return default generator get state   
def get rng state all   -> list tensor       r   return a list of bytetensor represent the random number state of all devices          result =        for i in range device count             result append get rng state i       return result 
def set rng state new state  tensor  device  union int  str  torch device  = string  -> none      r   set the random number generator state of the specify gpu       args          new state  torch bytetensor   the desire state         device  torch device or int  optional   the device to set the rng state              default  `` cuda ``  i e   ``torch device  cuda  ``  the current cuda device               new state copy = new state clone memory format=torch contiguous format      if isinstance device  str           device = torch device device      elif isinstance device  int           device = torch device string  device       def cb            idx = cast torch device  device  index         if idx be none              idx = current device           default generator = torch cuda default generators idx          default generator set state new state copy        lazy call cb  
def set rng state all new state  iterable tensor   -> none      r   set the random number generator state of all devices       args          new state  iterable of torch bytetensor   the desire state for each device        for i  state in enumerate new state           set rng state state  i  
def manual seed seed  int  -> none      r   set the seed for generate random number for the current gpu      it s safe to call this function if cuda be not available  in that     case  it be silently ignore       args          seed  int   the desire seed          warn           if you be work with a multi-gpu model  this function be insufficient         to get determinism   to seed all gpus  use  func `manual seed all`              seed = int seed       def cb            idx = current device           default generator = torch cuda default generators idx          default generator manual seed seed        lazy call cb  
def manual seed all seed  int  -> none      r   set the seed for generate random number on all gpus      it s safe to call this function if cuda be not available  in that     case  it be silently ignore       args          seed  int   the desire seed              seed = int seed       def cb            for i in range device count                 default generator = torch cuda default generators i              default generator manual seed seed        lazy call cb  
def seed   -> none      r   set the seed for generate random number to a random number for the current gpu      it s safe to call this function if cuda be not available  in that     case  it be silently ignore          warn           if you be work with a multi-gpu model  this function will only initialize         the seed on one gpu   to initialize all gpus  use  func `seed all`              def cb            idx = current device           default generator = torch cuda default generators idx          default generator seed         lazy call cb  
def seed all   -> none      r   set the seed for generate random number to a random number on all gpus      it s safe to call this function if cuda be not available  in that     case  it be silently ignore              def cb            random seed = 0         seed = false         for i in range device count                 default generator = torch cuda default generators i              if not seed                  default generator seed                   random seed = default generator initial seed                   seed = true             else                  default generator manual seed random seed        lazy call cb  
def initial seed   -> int      r   return the current random seed of the current gpu          warn           this function eagerly initialize cuda               lazy init       idx = current device       default generator = torch cuda default generators idx      return default generator initial seed   
class event torch  c  cudaeventbase       r   wrapper around a cuda event       cuda events be synchronization markers that can be use to monitor the     device s progress  to accurately measure time  and to synchronize cuda     stream       the underlie cuda events be lazily initialize when the event be first     record or export to another process  after creation  only stream on the     same device may record the event  however  stream on any device can wait on     the event       args          enable time  bool  optional   indicate if the event should measure time              default  ``false``          block  bool  optional   if ``true``   meth `wait` will be block  default  ``false``          interprocess  bool   if ``true``  the event can be share between process              default  ``false``           cuda event documentation         https //docs nvidia com/cuda/cuda-runtime-api/group  cudart  event html              def   new   cls  enable timing=false  blocking=false  interprocess=false           return super event  cls    new                cls              enable timing=enable time  blocking=blocking  interprocess=interprocess        classmethod     def from ipc handle cls  device  handle           r   reconstruct an event from an ipc handle on the give device             return super event  cls  from ipc handle device  handle       def record self  stream=none           r   record the event in a give stream           use ``torch cuda current stream  `` if no stream be specify  the         stream s device must match the event s device             if stream be none              stream = torch cuda current stream           super event  self  record stream       def wait self  stream=none           r   make all future work submit to the give stream wait for this         event           use ``torch cuda current stream  `` if no stream be specify             if stream be none              stream = torch cuda current stream           super event  self  wait stream       def query self           r   check if all work currently capture by event have complete           return              a boolean indicate if all work currently capture by event have             complete                      return super event  self  query        def elapse time self  end event           r   return the time elapse in milliseconds after the event be         record and before the end event be record                      return super event  self  elapse time end event       def synchronize self           r   wait for the event to complete           wait until the completion of all work currently capture in this event          this prevent the cpu thread from proceed until the event complete               note   this be a wrapper around ``cudaeventsynchronize  ``  see             `cuda event documentation`  for more info                      super event  self  synchronize        def ipc handle self           r   return an ipc handle of this event  if not record yet  the event         will use the current device              return super event  self  ipc handle         property     def  as parameter  self           return ctypes c void p self cuda event       def   repr   self           if self cuda event              return string format self  as parameter  value          else              return string 
def query self       r   check if all work currently capture by event have complete       return          a boolean indicate if all work currently capture by event have         complete              return super event  self  query   
def empty cache   -> none      r   release all unoccupied cache memory currently hold by the cache     allocator so that those can be use in other gpu application and visible in     `nvidia-smi`          note            func `~torch cuda empty cache` doesn t increase the amount of gpu         memory available for pytorch  however  it may help reduce fragmentation         of gpu memory in certain case  see  ref `cuda-memory-management` for         more detail about gpu memory management              if be initialize            torch  c  cuda emptycache   
def list gpu process device  union device  int  = none  -> str      r   return a human-readable printout of the run process     and their gpu memory use for a give device       this can be useful to display periodically during train  or when     handle out-of-memory exceptions       args          device  torch device or int  optional   select device  return             printout for the current device  give by  func `~torch cuda current device`              if  attr `device` be ``none``  default                try          import pynvml       except modulenotfounderror          return string      from pynvml import nvmlerror drivernotloaded     try          pynvml nvmlinit       except nvmlerror drivernotloaded          return  string      device =  get device index device  optional=true      handle = pynvml nvmldevicegethandlebyindex device      procs = pynvml nvmldevicegetcomputerunningprocesses handle      line =        line append f gpu  device        if len procs  == 0          line append string      for p in procs          mem = p usedgpumemory /  1024   1024          line append f process  p pid >10d  use  mem >12 3f  mb gpu memory       return string join line  
def memory stats device  union device  int  = none  -> dict str  any       r   return a dictionary of cuda memory allocator statistics for a     give device       the return value of this function be a dictionary of statistics  each of     which be a non-negative integer       core statistics       - `` allocate  all large pool small pool   current peak allocate free  ``        number of allocation request receive by the memory allocator      - `` allocate bytes  all large pool small pool   current peak allocate free  ``        amount of allocate memory      - `` segment  all large pool small pool   current peak allocate free  ``        number of reserve segment from ``cudamalloc  ``      - `` reserve bytes  all large pool small pool   current peak allocate free  ``        amount of reserve memory      - `` active  all large pool small pool   current peak allocate free  ``        number of active memory block      - `` active bytes  all large pool small pool   current peak allocate free  ``        amount of active memory      - `` inactive split  all large pool small pool   current peak allocate free  ``        number of inactive  non-releasable memory block      - `` inactive split bytes  all large pool small pool   current peak allocate free  ``        amount of inactive  non-releasable memory       for these core statistics  value be break down as follow       pool type       - ``all``  combine statistics across all memory pool      - ``large pool``  statistics for the large allocation pool        as of october 2019  for size >= 1mb allocations       - ``small pool``  statistics for the small allocation pool        as of october 2019  for size < 1mb allocations        metric type       - ``current``  current value of this metric      - ``peak``  maximum value of this metric      - ``allocated``  historical total increase in this metric      - ``freed``  historical total decrease in this metric       in addition to the core statistics  we also provide some simple event     counter       - `` num alloc retry ``  number of fail ``cudamalloc`` call that       result in a cache flush and retry      - `` num ooms ``  number of out-of-memory errors throw       args          device  torch device or int  optional   select device  return             statistics for the current device  give by  func `~torch cuda current device`              if  attr `device` be ``none``  default           note           see  ref `cuda-memory-management` for more detail about gpu memory         management              result =         def  recurse add to result prefix  obj           if isinstance obj  dict               if len prefix  > 0                  prefix  = string             for k  v in obj items                     recurse add to result prefix   k  v          else              result append  prefix  obj        stats = memory stats as nest dict device=device       recurse add to result string  stats      result sort        return collections ordereddict result  
def memory summary device  union device  int  = none  abbreviate  bool = false  -> str      r   return a human-readable printout of the current memory allocator     statistics for a give device       this can be useful to display periodically during train  or when     handle out-of-memory exceptions       args          device  torch device or int  optional   select device  return             printout for the current device  give by  func `~torch cuda current device`              if  attr `device` be ``none``  default           abbreviate  bool  optional   whether to return an abbreviate summary              default  false           note           see  ref `cuda-memory-management` for more detail about gpu memory         management              device =  get device index device  optional=true      stats = memory stats device=device       def  format size sz  pref sz           prefix =  string  string  string  string  string  string          prefix = prefix 0          for new prefix in prefix 1                if pref sz < 768   1024                  break             prefix = new prefix             sz //= 1024             pref sz /= 1024         return string format sz  prefix       def  format count cnt  pref cnt           prefix =  string  string  string          prefix = prefix 0          for new prefix in prefix 1                if pref cnt < 750   1000                  break             prefix = new prefix             cnt //= 1000             pref cnt /= 1000         return string format cnt  prefix       metrics to display =            string  string   format size            string  string   format size            string  string   format size            string  string   format size            string  string   format count            string  string   format count            string  string   format count            string  string   format count              line =        line append string   75      line append string      line append string   75      line append string      line append string   75      line append string       for metric key  metric name  formatter in metrics to display          line append string   75          submetrics =   string  metric name           if not abbreviate              submetrics append  string  string               submetrics append  string  string            current prefval  peak prefval  allocate prefval  free prefval = none  none  none  none          for submetric key  submetric name in submetrics              prefix = metric key   string   submetric key   string              current = stats prefix   string              peak = stats prefix   string              allocate = stats prefix   string              free = stats prefix   string               if current prefval be none                  current prefval = current                 peak prefval = peak                 allocate prefval = allocate                 free prefval = free              line append string format                  submetric name                  formatter current  current prefval                   formatter peak  peak prefval                   formatter allocate  allocate prefval                   formatter free  free prefval                       line append string   75       fmt dict =  string  string  string  device      for k  v in stats items            fmt dict k replace string  string   = v     return string   string join line  format   fmt dict    string 
def memory snapshot        r   return a snapshot of the cuda memory allocator state across all devices       interpret the output of this function require familiarity with the     memory allocator internals          note           see  ref `cuda-memory-management` for more detail about gpu memory         management              return torch  c  cuda memorysnapshot   
def memory allocate device  union device  int  = none  -> int      r   return the current gpu memory occupy by tensors in bytes for a give     device       args          device  torch device or int  optional   select device  return             statistic for the current device  give by  func `~torch cuda current device`              if  attr `device` be ``none``  default           note           this be likely less than the amount show in `nvidia-smi` since some         unused memory can be hold by the cache allocator and some context         need to be create on gpu  see  ref `cuda-memory-management` for more         detail about gpu memory management              return memory stats device=device  get string  0  
def max memory allocate device  union device  int  = none  -> int      r   return the maximum gpu memory occupy by tensors in bytes for a give     device       by default  this return the peak allocate memory since the begin of     this program   func `~torch cuda reset peak memory stats` can be use to     reset the start point in track this metric  for example  these two     function can measure the peak allocate memory usage of each iteration in a     train loop       args          device  torch device or int  optional   select device  return             statistic for the current device  give by  func `~torch cuda current device`              if  attr `device` be ``none``  default           note           see  ref `cuda-memory-management` for more detail about gpu memory         management              return memory stats device=device  get string  0  
def reset max memory allocate device  union device  int  = none  -> none      r   reset the start point in track maximum gpu memory occupy by     tensors for a give device       see  func `~torch cuda max memory allocated` for detail       args          device  torch device or int  optional   select device  return             statistic for the current device  give by  func `~torch cuda current device`              if  attr `device` be ``none``  default           warn           this function now call  func `~torch cuda reset peak memory stats`  which reset         /all/ peak memory stats          note           see  ref `cuda-memory-management` for more detail about gpu memory         management              warn warn          string         string          futurewarning      return reset peak memory stats device=device  
def memory reserve device  union device  int  = none  -> int      r   return the current gpu memory manage by the cache allocator in bytes     for a give device       args          device  torch device or int  optional   select device  return             statistic for the current device  give by  func `~torch cuda current device`              if  attr `device` be ``none``  default           note           see  ref `cuda-memory-management` for more detail about gpu memory         management              return memory stats device=device  get string  0  
def max memory reserve device  union device  int  = none  -> int      r   return the maximum gpu memory manage by the cache allocator in bytes     for a give device       by default  this return the peak cache memory since the begin of this     program   func `~torch cuda reset peak memory stats` can be use to reset     the start point in track this metric  for example  these two function     can measure the peak cache memory amount of each iteration in a train     loop       args          device  torch device or int  optional   select device  return             statistic for the current device  give by  func `~torch cuda current device`              if  attr `device` be ``none``  default           note           see  ref `cuda-memory-management` for more detail about gpu memory         management              return memory stats device=device  get string  0  
def set per process memory fraction fraction  device  union device  int  = none  -> none      r   set memory fraction for a process      the fraction be use to limit an cache allocator to allocate memory on a cuda device      the allow value equal the total visible memory multiply fraction      if try to allocate more than the allow value in a process  will raise an out of     memory error in allocator       args          fraction float   range  0~1  allow memory equal total memory   fraction          device  torch device or int  optional   select device  if it be             ``none`` the default cuda device be use         note           in general  the total available free memory be less than the total capacity               lazy init       if device be none          device = torch cuda current device       device =  get device index device      if not isinstance fraction  float           raise typeerror string      if fraction < 0 or fraction > 1          raise valueerror string                          string format fraction        torch  c  cuda setmemoryfraction fraction  device  
def memory cache device  union device  int  = none  -> int      r   deprecate  see  func `~torch cuda memory reserved`         warn warn          string          futurewarning      return memory reserve device=device  
def max memory cache device  union device  int  = none  -> int      r   deprecate  see  func `~torch cuda max memory reserved`         warn warn          string          futurewarning      return max memory reserve device=device  
def reset max memory cache device  union device  int  = none  -> none      r   reset the start point in track maximum gpu memory manage by the     cache allocator for a give device       see  func `~torch cuda max memory cached` for detail       args          device  torch device or int  optional   select device  return             statistic for the current device  give by  func `~torch cuda current device`              if  attr `device` be ``none``  default           warn           this function now call  func `~torch cuda reset peak memory stats`  which reset         /all/ peak memory stats          note           see  ref `cuda-memory-management` for more detail about gpu memory         management              warn warn          string         string          futurewarning      return reset peak memory stats device=device  
def reset peak memory stats device  union device  int  = none  -> none      r   reset the  peak  stats track by the cuda memory allocator       see  func `~torch cuda memory stats` for detail  peak stats correspond to the     ` peak ` key in each individual stat dict       args          device  torch device or int  optional   select device  return             statistic for the current device  give by  func `~torch cuda current device`              if  attr `device` be ``none``  default           note           see  ref `cuda-memory-management` for more detail about gpu memory         management              device =  get device index device  optional=true      return torch  c  cuda resetpeakmemorystats device  
def cache allocator alloc size  device  union device  int  = none  stream=none       r   perform a memory allocation use the cuda memory allocator       memory be allocate for a give device and a stream  this     function be intend to be use for interoperability with other     frameworks  allocate memory be release through      func `~torch cuda cache allocator delete`       args          size  int   number of bytes to be allocate          device  torch device or int  optional   select device  if it be             ``none`` the default cuda device be use          stream  torch cuda stream or int  optional   select stream  if be ``none`` then             the default stream for the select device be use          note           see  ref `cuda-memory-management` for more detail about gpu memory         management              if device be none          device = torch cuda current device       device =  get device index device      if stream be none          stream = torch cuda current stream device      if isinstance stream  torch cuda stream stream           stream = stream cuda stream     if not isinstance stream  int           raise typeerror string                         string                         string      with torch cuda device device           return torch  c  cuda cudacachingallocator raw alloc size  stream  
def cache allocator delete mem ptr       r   delete memory allocate use the cuda memory allocator       memory allocate with  func `~torch cuda cache allocator alloc`      be free here  the associate device and stream be track inside     the allocator       args          mem ptr  int   memory address to be free by the allocator          note           see  ref `cuda-memory-management` for more detail about gpu memory         management              torch  c  cuda cudacachingallocator raw delete mem ptr  
def mark msg       string     return  nvtx marka msg  
def range push msg       string     return  nvtx rangepusha msg  
def range pop        string     return  nvtx rangepop   
def add param group self  param group       r   add a param group to the  class `optimizer` s `param groups`       this can be useful when fine tune a pre-trained network as freeze layer can be make     trainable and add to the  class `optimizer` as train progress       args          param group  dict   specify what tensors should be optimize along with group         specific optimization options              assert isinstance param group  dict   string      params = param group string      if isinstance params  torch tensor           param group string  =  params      elif isinstance params  set           raise typeerror string                         string      else          param group string  = list params       for param in param group string           if not isinstance param  torch tensor               raise typeerror string                             string   torch typename param           if not param be leaf              raise valueerror string       for name  default in self default items            if default be require and name not in param group              raise valueerror string                                name          else              param group setdefault name  default       params = param group string      if len params   = len set params            warn warn string                       string                       string  stacklevel=3       param set = set       for group in self param group          param set update set group string         if not param set isdisjoint set param group string             raise valueerror string       self param group append param group  
def load state dict self  state dict       r   load the optimizer state       args          state dict  dict   optimizer state  should be an object return             from a call to  meth `state dict`                   state dict = deepcopy state dict           group = self param group     save group = state dict string       if len group   = len save group           raise valueerror string                          string      param lens =  len g string   for g in group      save lens =  len g string   for g in save group      if any p len  = s len for p len  s len in zip param lens  save lens            raise valueerror string                          string            id map =  old id  p for old id  p in               zip chain from iterable  g string  for g in save group                      chain from iterable  g string  for g in group          def cast param  value           r   make a deep copy of value  cast all tensors to device of param             if isinstance value  torch tensor                                         if param be float point                    value = value to param dtype              value = value to param device              return value         elif isinstance value  dict               return  k  cast param  v  for k  v in value items            elif isinstance value  container abcs iterable               return type value  cast param  v  for v in value          else              return value                     state = defaultdict dict      for k  v in state dict string  items            if k in id map              param = id map k              state param  = cast param  v          else              state k  = v           def update group group  new group           new group string  = group string          return new group     param group =           update group g  ng  for g  ng in zip group  save group       self   setstate    string  state  string  param group   
def state dict self       r   return the state of the optimizer as a  class `dict`       it contain two entries         state - a dict hold current optimization state  its content         differ between optimizer class        param group - a dict contain all parameter group                  param mappings =        start index = 0      def pack group group           nonlocal start index         pack =  k  v for k  v in group items   if k  = string          param mappings update  id p   i for i  p in enumerate group string   start index                                 if id p  not in param mappings           pack string  =  param mappings id p   for p in group string           start index  = len pack string           return pack     param group =  pack group g  for g in self param group           pack state =   param mappings id k   if isinstance k  torch tensor  else k   v                     for k  v in self state items        return           string  pack state          string  param group        
def step self  closure       r   perform a single optimization step  parameter update        args          closure  callable   a closure that reevaluate the model and             return the loss  optional for most optimizers          note           unless otherwise specify  this function should not modify the         `` grad`` field of the parameters              raise notimplementederror 
def zero grad self  set to none  bool = false       r   set the gradients of all optimize  class `torch tensor` s to zero       args          set to none  bool   instead of set to zero  set the grads to none              this will in general have lower memory footprint  and can modestly improve performance              however  it change certain behaviors  for example              1  when the user try to access a gradient and perform manual ops on it              a none attribute or a tensor full of 0s will behave differently              2  if the user request ``zero grad set to none=true `` follow by a backward pass  `` grad``\ s             be guarantee to be none for params that do not receive a gradient              3  ``torch optim`` optimizers have a different behavior if the gradient be 0 or none              in one case it do the step with a gradient of 0 and in the other it skip             the step altogether               if not hasattr self  string           self  hook for profile       with torch autograd profiler record function self  zero grad profile name           for group in self param group              for p in group string                   if p grad be not none                      if set to none                          p grad = none                     else                          if p grad grad fn be not none                              p grad detach                            else                              p grad require grad  false                          p grad zero    
class adadelta optimizer       string      def   init   self  params  lr=1 0  rho=0 9  eps=1e-6  weight decay=0           if not 0 0 <= lr              raise valueerror string format lr           if not 0 0 <= rho <= 1 0              raise valueerror string format rho           if not 0 0 <= eps              raise valueerror string format eps           if not 0 0 <= weight decay              raise valueerror string format weight decay            default = dict lr=lr  rho=rho  eps=eps  weight decay=weight decay          super adadelta  self    init   params  default        torch no grad       def step self  closure=none           string         loss = none         if closure be not none              with torch enable grad                    loss = closure            for group in self param group              params with grad =                grads =                square avgs =                acc deltas =                lr  rho  eps  weight decay = group string   group string   group string   group string               for p in group string                   if p grad be none                      continue                 params with grad append p                  if p grad be sparse                      raise runtimeerror string                  grads append p grad                   state = self state p                                    if len state  == 0                      state string  = 0                     state string  = torch zero like p  memory format=torch preserve format                      state string  = torch zero like p  memory format=torch preserve format                   square avgs append state string                   acc deltas append state string                    state string   = 1              f adadelta params with grad                         grads                         square avgs                         acc deltas                         lr=lr                         rho=rho                         eps=eps                         weight decay=weight decay           return loss 
def load state dict self  state dict       r   load the optimizer state       args          state dict  dict   optimizer state  should be an object return             from a call to  meth `state dict`                   state dict = deepcopy state dict           group = self param group     save group = state dict string       if len group   = len save group           raise valueerror string                          string      param lens =  len g string   for g in group      save lens =  len g string   for g in save group      if any p len  = s len for p len  s len in zip param lens  save lens            raise valueerror string                          string            id map =  old id  p for old id  p in               zip chain from iterable  g string  for g in save group                      chain from iterable  g string  for g in group          def cast param  value           r   make a deep copy of value  cast all tensors to device of param             if isinstance value  torch tensor                                         if param be float point                    value = value to param dtype              value = value to param device              return value         elif isinstance value  dict               return  k  cast param  v  for k  v in value items            elif isinstance value  container abcs iterable               return type value  cast param  v  for v in value          else              return value                     state = defaultdict dict      for k  v in state dict string  items            if k in id map              param = id map k              state param  = cast param  v          else              state k  = v           def update group group  new group           new group string  = group string          return new group     param group =           update group g  ng  for g  ng in zip group  save group       self   setstate    string  state  string  param group   
 torch no grad   def step self  closure=none       string     loss = none     if closure be not none          with torch enable grad                loss = closure        for group in self param group          params with grad =            grads =            square avgs =            acc deltas =            lr  rho  eps  weight decay = group string   group string   group string   group string           for p in group string               if p grad be none                  continue             params with grad append p              if p grad be sparse                  raise runtimeerror string              grads append p grad               state = self state p                            if len state  == 0                  state string  = 0                 state string  = torch zero like p  memory format=torch preserve format                  state string  = torch zero like p  memory format=torch preserve format               square avgs append state string               acc deltas append state string                state string   = 1          f adadelta params with grad                     grads                     square avgs                     acc deltas                     lr=lr                     rho=rho                     eps=eps                     weight decay=weight decay       return loss 
class adagrad optimizer       string      def   init   self  params  lr=1e-2  lr decay=0  weight decay=0  initial accumulator value=0  eps=1e-10           if not 0 0 <= lr              raise valueerror string format lr           if not 0 0 <= lr decay              raise valueerror string format lr decay           if not 0 0 <= weight decay              raise valueerror string format weight decay           if not 0 0 <= initial accumulator value              raise valueerror string format initial accumulator value           if not 0 0 <= eps              raise valueerror string format eps            default = dict lr=lr  lr decay=lr decay  eps=eps  weight decay=weight decay                          initial accumulator value=initial accumulator value          super adagrad  self    init   params  default           for group in self param group              for p in group string                   state = self state p                  state string  = 0                 state string  = torch full like p  initial accumulator value  memory format=torch preserve format       def share memory self           for group in self param group              for p in group string                   state = self state p                  state string  share memory          torch no grad       def step self  closure=none           string         loss = none         if closure be not none              with torch enable grad                    loss = closure            for group in self param group              params with grad =                grads =                state sum =                state step =                 for p in group string                   if p grad be not none                      params with grad append p                      grads append p grad                      state = self state p                      state sum append state string                                            state string   = 1                                          state step append state string                f adagrad params with grad                        grads                        state sum                        state step                        lr=group string                         weight decay=group string                         lr decay=group string                         eps=group string            return loss 
def load state dict self  state dict       r   load the optimizer state       args          state dict  dict   optimizer state  should be an object return             from a call to  meth `state dict`                   state dict = deepcopy state dict           group = self param group     save group = state dict string       if len group   = len save group           raise valueerror string                          string      param lens =  len g string   for g in group      save lens =  len g string   for g in save group      if any p len  = s len for p len  s len in zip param lens  save lens            raise valueerror string                          string            id map =  old id  p for old id  p in               zip chain from iterable  g string  for g in save group                      chain from iterable  g string  for g in group          def cast param  value           r   make a deep copy of value  cast all tensors to device of param             if isinstance value  torch tensor                                         if param be float point                    value = value to param dtype              value = value to param device              return value         elif isinstance value  dict               return  k  cast param  v  for k  v in value items            elif isinstance value  container abcs iterable               return type value  cast param  v  for v in value          else              return value                     state = defaultdict dict      for k  v in state dict string  items            if k in id map              param = id map k              state param  = cast param  v          else              state k  = v           def update group group  new group           new group string  = group string          return new group     param group =           update group g  ng  for g  ng in zip group  save group       self   setstate    string  state  string  param group   
 torch no grad   def step self  closure=none       string     loss = none     if closure be not none          with torch enable grad                loss = closure        for group in self param group          params with grad =            grads =            state sum =            state step =             for p in group string               if p grad be not none                  params with grad append p                  grads append p grad                  state = self state p                  state sum append state string                                    state string   = 1                                  state step append state string            f adagrad params with grad                    grads                    state sum                    state step                    lr=group string                     weight decay=group string                     lr decay=group string                     eps=group string        return loss 
class adam optimizer       r   implement adam algorithm       it have be propose in `adam  a method for stochastic optimization`       the implementation of the l2 penalty follow change propose in     `decoupled weight decay regularization`        args          params  iterable   iterable of parameters to optimize or dicts define             parameter group         lr  float  optional   learn rate  default  1e-3          betas  tuple float  float   optional   coefficients use for compute             run average of gradient and its square  default   0 9  0 999           eps  float  optional   term add to the denominator to improve             numerical stability  default  1e-8          weight decay  float  optional   weight decay  l2 penalty   default  0          amsgrad  boolean  optional   whether to use the amsgrad variant of this             algorithm from the paper `on the convergence of adam and beyond`               default  false           adam\  a method for stochastic optimization          https //arxiv org/abs/1412 6980         decouple weight decay regularization          https //arxiv org/abs/1711 05101         on the convergence of adam and beyond          https //openreview net/forum?id=ryqu7f-rz              def   init   self  params  lr=1e-3  betas= 0 9  0 999   eps=1e-8                   weight decay=0  amsgrad=false           if not 0 0 <= lr              raise valueerror string format lr           if not 0 0 <= eps              raise valueerror string format eps           if not 0 0 <= betas 0  < 1 0              raise valueerror string format betas 0            if not 0 0 <= betas 1  < 1 0              raise valueerror string format betas 1            if not 0 0 <= weight decay              raise valueerror string format weight decay           default = dict lr=lr  betas=betas  eps=eps                          weight decay=weight decay  amsgrad=amsgrad          super adam  self    init   params  default       def   setstate   self  state           super adam  self    setstate   state          for group in self param group              group setdefault string  false        torch no grad       def step self  closure=none           string         loss = none         if closure be not none              with torch enable grad                    loss = closure            for group in self param group              params with grad =                grads =                exp avgs =                exp avg sqs =                max exp avg sqs =                state step =                beta1  beta2 = group string               for p in group string                   if p grad be not none                      params with grad append p                      if p grad be sparse                          raise runtimeerror string                      grads append p grad                       state = self state p                                           if len state  == 0                          state string  = 0                                                  state string  = torch zero like p  memory format=torch preserve format                                                   state string  = torch zero like p  memory format=torch preserve format                          if group string                                                            state string  = torch zero like p  memory format=torch preserve format                       exp avgs append state string                       exp avg sqs append state string                        if group string                           max exp avg sqs append state string                                             state string   = 1                                          state step append state string                f adam params with grad                     grads                     exp avgs                     exp avg sqs                     max exp avg sqs                     state step                     amsgrad=group string                      beta1=beta1                     beta2=beta2                     lr=group string                      weight decay=group string                      eps=group string           return loss 
def load state dict self  state dict       r   load the optimizer state       args          state dict  dict   optimizer state  should be an object return             from a call to  meth `state dict`                   state dict = deepcopy state dict           group = self param group     save group = state dict string       if len group   = len save group           raise valueerror string                          string      param lens =  len g string   for g in group      save lens =  len g string   for g in save group      if any p len  = s len for p len  s len in zip param lens  save lens            raise valueerror string                          string            id map =  old id  p for old id  p in               zip chain from iterable  g string  for g in save group                      chain from iterable  g string  for g in group          def cast param  value           r   make a deep copy of value  cast all tensors to device of param             if isinstance value  torch tensor                                         if param be float point                    value = value to param dtype              value = value to param device              return value         elif isinstance value  dict               return  k  cast param  v  for k  v in value items            elif isinstance value  container abcs iterable               return type value  cast param  v  for v in value          else              return value                     state = defaultdict dict      for k  v in state dict string  items            if k in id map              param = id map k              state param  = cast param  v          else              state k  = v           def update group group  new group           new group string  = group string          return new group     param group =           update group g  ng  for g  ng in zip group  save group       self   setstate    string  state  string  param group   
 torch no grad   def step self  closure=none       string     loss = none     if closure be not none          with torch enable grad                loss = closure        for group in self param group          params with grad =            grads =            exp avgs =            exp avg sqs =            max exp avg sqs =            state step =            beta1  beta2 = group string           for p in group string               if p grad be not none                  params with grad append p                  if p grad be sparse                      raise runtimeerror string                  grads append p grad                   state = self state p                                   if len state  == 0                      state string  = 0                                          state string  = torch zero like p  memory format=torch preserve format                                           state string  = torch zero like p  memory format=torch preserve format                      if group string                                                    state string  = torch zero like p  memory format=torch preserve format                   exp avgs append state string                   exp avg sqs append state string                    if group string                       max exp avg sqs append state string                                     state string   = 1                                  state step append state string            f adam params with grad                 grads                 exp avgs                 exp avg sqs                 max exp avg sqs                 state step                 amsgrad=group string                  beta1=beta1                 beta2=beta2                 lr=group string                  weight decay=group string                  eps=group string       return loss 
class adamw optimizer       r   implement adamw algorithm       the original adam algorithm be propose in `adam  a method for stochastic optimization`       the adamw variant be propose in `decoupled weight decay regularization`        args          params  iterable   iterable of parameters to optimize or dicts define             parameter group         lr  float  optional   learn rate  default  1e-3          betas  tuple float  float   optional   coefficients use for compute             run average of gradient and its square  default   0 9  0 999           eps  float  optional   term add to the denominator to improve             numerical stability  default  1e-8          weight decay  float  optional   weight decay coefficient  default  1e-2          amsgrad  boolean  optional   whether to use the amsgrad variant of this             algorithm from the paper `on the convergence of adam and beyond`               default  false           adam\  a method for stochastic optimization          https //arxiv org/abs/1412 6980         decouple weight decay regularization          https //arxiv org/abs/1711 05101         on the convergence of adam and beyond          https //openreview net/forum?id=ryqu7f-rz              def   init   self  params  lr=1e-3  betas= 0 9  0 999   eps=1e-8                   weight decay=1e-2  amsgrad=false           if not 0 0 <= lr              raise valueerror string format lr           if not 0 0 <= eps              raise valueerror string format eps           if not 0 0 <= betas 0  < 1 0              raise valueerror string format betas 0            if not 0 0 <= betas 1  < 1 0              raise valueerror string format betas 1            if not 0 0 <= weight decay              raise valueerror string format weight decay           default = dict lr=lr  betas=betas  eps=eps                          weight decay=weight decay  amsgrad=amsgrad          super adamw  self    init   params  default       def   setstate   self  state           super adamw  self    setstate   state          for group in self param group              group setdefault string  false        torch no grad       def step self  closure=none           string         loss = none         if closure be not none              with torch enable grad                    loss = closure            for group in self param group              params with grad =                grads =                exp avgs =                exp avg sqs =                state sum =                max exp avg sqs =                state step =                amsgrad = group string              beta1  beta2 = group string               for p in group string                   if p grad be none                      continue                 params with grad append p                  if p grad be sparse                      raise runtimeerror string                  grads append p grad                   state = self state p                                    if len state  == 0                      state string  = 0                                          state string  = torch zero like p  memory format=torch preserve format                                           state string  = torch zero like p  memory format=torch preserve format                      if amsgrad                                                   state string  = torch zero like p  memory format=torch preserve format                   exp avgs append state string                   exp avg sqs append state string                    if amsgrad                      max exp avg sqs append state string                                     state string   = 1                                  state step append state string                f adamw params with grad                      grads                      exp avgs                      exp avg sqs                      max exp avg sqs                      state step                      amsgrad=amsgrad                      beta1=beta1                      beta2=beta2                      lr=group string                       weight decay=group string                       eps=group string            return loss 
def load state dict self  state dict       r   load the optimizer state       args          state dict  dict   optimizer state  should be an object return             from a call to  meth `state dict`                   state dict = deepcopy state dict           group = self param group     save group = state dict string       if len group   = len save group           raise valueerror string                          string      param lens =  len g string   for g in group      save lens =  len g string   for g in save group      if any p len  = s len for p len  s len in zip param lens  save lens            raise valueerror string                          string            id map =  old id  p for old id  p in               zip chain from iterable  g string  for g in save group                      chain from iterable  g string  for g in group          def cast param  value           r   make a deep copy of value  cast all tensors to device of param             if isinstance value  torch tensor                                         if param be float point                    value = value to param dtype              value = value to param device              return value         elif isinstance value  dict               return  k  cast param  v  for k  v in value items            elif isinstance value  container abcs iterable               return type value  cast param  v  for v in value          else              return value                     state = defaultdict dict      for k  v in state dict string  items            if k in id map              param = id map k              state param  = cast param  v          else              state k  = v           def update group group  new group           new group string  = group string          return new group     param group =           update group g  ng  for g  ng in zip group  save group       self   setstate    string  state  string  param group   
 torch no grad   def step self  closure=none       string     loss = none     if closure be not none          with torch enable grad                loss = closure        for group in self param group          params with grad =            grads =            exp avgs =            exp avg sqs =            state sum =            max exp avg sqs =            state step =            amsgrad = group string          beta1  beta2 = group string           for p in group string               if p grad be none                  continue             params with grad append p              if p grad be sparse                  raise runtimeerror string              grads append p grad               state = self state p                            if len state  == 0                  state string  = 0                                  state string  = torch zero like p  memory format=torch preserve format                                   state string  = torch zero like p  memory format=torch preserve format                  if amsgrad                                           state string  = torch zero like p  memory format=torch preserve format               exp avgs append state string               exp avg sqs append state string                if amsgrad                  max exp avg sqs append state string                             state string   = 1                          state step append state string            f adamw params with grad                  grads                  exp avgs                  exp avg sqs                  max exp avg sqs                  state step                  amsgrad=amsgrad                  beta1=beta1                  beta2=beta2                  lr=group string                   weight decay=group string                   eps=group string        return loss 
class sparseadam optimizer       r   implement lazy version of adam algorithm suitable for sparse tensors       in this variant  only moments that show up in the gradient get update  and     only those portion of the gradient get apply to the parameters       args          params  iterable   iterable of parameters to optimize or dicts define             parameter group         lr  float  optional   learn rate  default  1e-3          betas  tuple float  float   optional   coefficients use for compute             run average of gradient and its square  default   0 9  0 999           eps  float  optional   term add to the denominator to improve             numerical stability  default  1e-8           adam\  a method for stochastic optimization          https //arxiv org/abs/1412 6980              def   init   self  params  lr=1e-3  betas= 0 9  0 999   eps=1e-8           if not 0 0 < lr              raise valueerror string format lr           if not 0 0 < eps              raise valueerror string format eps           if not 0 0 <= betas 0  < 1 0              raise valueerror string format betas 0            if not 0 0 <= betas 1  < 1 0              raise valueerror string format betas 1             params = list params           sparse params =            for index  param in enumerate params               if isinstance param  dict                   for d index  d param in enumerate param get string                            if d param be sparse                          sparse params append  index  d index               elif param be sparse                  sparse params append index          if sparse params              raise valueerror                  f sparse params at indices  sparse params   sparseadam require dense parameter tensors                         default = dict lr=lr  betas=betas  eps=eps          super sparseadam  self    init   params  default        torch no grad       def step self  closure=none           string         loss = none         if closure be not none              with torch enable grad                    loss = closure            for group in self param group              for p in group string                   if p grad be none                      continue                 grad = p grad                 if not grad be sparse                      raise runtimeerror string                   state = self state p                                    if len state  == 0                      state string  = 0                                          state string  = torch zero like p  memory format=torch preserve format                                           state string  = torch zero like p  memory format=torch preserve format                   state string   = 1                  grad = grad coalesce                     grad indices = grad  indices                   grad value = grad  value                   size = grad size                    def make sparse value                       constructor = grad new                     if grad indices dim   == 0 or value dim   == 0                          return constructor   resize as  grad                      return constructor grad indices  value  size                   exp avg  exp avg sq = state string   state string                  beta1  beta2 = group string                                                                      old exp avg value = exp avg sparse mask grad   value                   exp avg update value = grad value sub old exp avg value  mul  1 - beta1                  exp avg add  make sparse exp avg update value                   old exp avg sq value = exp avg sq sparse mask grad   value                   exp avg sq update value = grad value pow 2  sub  old exp avg sq value  mul  1 - beta2                  exp avg sq add  make sparse exp avg sq update value                                     numer = exp avg update value add  old exp avg value                  exp avg sq update value add  old exp avg sq value                  denom = exp avg sq update value sqrt    add  group string                   del exp avg update value  exp avg sq update value                  bias correction1 = 1 - beta1    state string                  bias correction2 = 1 - beta2    state string                  step size = group string    math sqrt bias correction2  / bias correction1                  p add  make sparse -step size   numer div  denom             return loss 
def load state dict self  state dict       r   load the optimizer state       args          state dict  dict   optimizer state  should be an object return             from a call to  meth `state dict`                   state dict = deepcopy state dict           group = self param group     save group = state dict string       if len group   = len save group           raise valueerror string                          string      param lens =  len g string   for g in group      save lens =  len g string   for g in save group      if any p len  = s len for p len  s len in zip param lens  save lens            raise valueerror string                          string            id map =  old id  p for old id  p in               zip chain from iterable  g string  for g in save group                      chain from iterable  g string  for g in group          def cast param  value           r   make a deep copy of value  cast all tensors to device of param             if isinstance value  torch tensor                                         if param be float point                    value = value to param dtype              value = value to param device              return value         elif isinstance value  dict               return  k  cast param  v  for k  v in value items            elif isinstance value  container abcs iterable               return type value  cast param  v  for v in value          else              return value                     state = defaultdict dict      for k  v in state dict string  items            if k in id map              param = id map k              state param  = cast param  v          else              state k  = v           def update group group  new group           new group string  = group string          return new group     param group =           update group g  ng  for g  ng in zip group  save group       self   setstate    string  state  string  param group   
 torch no grad   def step self  closure=none       string     loss = none     if closure be not none          with torch enable grad                loss = closure        for group in self param group          for p in group string               if p grad be none                  continue             grad = p grad             if not grad be sparse                  raise runtimeerror string               state = self state p                            if len state  == 0                  state string  = 0                                  state string  = torch zero like p  memory format=torch preserve format                                   state string  = torch zero like p  memory format=torch preserve format               state string   = 1              grad = grad coalesce                 grad indices = grad  indices               grad value = grad  value               size = grad size                def make sparse value                   constructor = grad new                 if grad indices dim   == 0 or value dim   == 0                      return constructor   resize as  grad                  return constructor grad indices  value  size               exp avg  exp avg sq = state string   state string              beta1  beta2 = group string                                                      old exp avg value = exp avg sparse mask grad   value               exp avg update value = grad value sub old exp avg value  mul  1 - beta1              exp avg add  make sparse exp avg update value               old exp avg sq value = exp avg sq sparse mask grad   value               exp avg sq update value = grad value pow 2  sub  old exp avg sq value  mul  1 - beta2              exp avg sq add  make sparse exp avg sq update value                             numer = exp avg update value add  old exp avg value              exp avg sq update value add  old exp avg sq value              denom = exp avg sq update value sqrt    add  group string               del exp avg update value  exp avg sq update value              bias correction1 = 1 - beta1    state string              bias correction2 = 1 - beta2    state string              step size = group string    math sqrt bias correction2  / bias correction1              p add  make sparse -step size   numer div  denom         return loss 
class adamax optimizer       string      def   init   self  params  lr=2e-3  betas= 0 9  0 999   eps=1e-8                   weight decay=0           if not 0 0 <= lr              raise valueerror string format lr           if not 0 0 <= eps              raise valueerror string format eps           if not 0 0 <= betas 0  < 1 0              raise valueerror string format betas 0            if not 0 0 <= betas 1  < 1 0              raise valueerror string format betas 1            if not 0 0 <= weight decay              raise valueerror string format weight decay            default = dict lr=lr  betas=betas  eps=eps  weight decay=weight decay          super adamax  self    init   params  default        torch no grad       def step self  closure=none           string         loss = none         if closure be not none              with torch enable grad                    loss = closure            for group in self param group              params with grad =                grads =                exp avgs =                exp infs =                state step =                 beta1  beta2 = group string              eps = group string              lr = group string              weight decay = group string               for p in group string                   if p grad be none                      continue                 params with grad append p                  if p grad be sparse                      raise runtimeerror string                  grads append p grad                   state = self state p                                    if len state  == 0                      state string  = 0                     state string  = torch zero like p  memory format=torch preserve format                      state string  = torch zero like p  memory format=torch preserve format                   exp avgs append state string                   exp infs append state string                    state string   = 1                 state step append state string                f adamax params with grad                       grads                       exp avgs                       exp infs                       state step                       eps=eps                       beta1=beta1                       beta2=beta2                       lr=lr                       weight decay=weight decay           return loss 
def load state dict self  state dict       r   load the optimizer state       args          state dict  dict   optimizer state  should be an object return             from a call to  meth `state dict`                   state dict = deepcopy state dict           group = self param group     save group = state dict string       if len group   = len save group           raise valueerror string                          string      param lens =  len g string   for g in group      save lens =  len g string   for g in save group      if any p len  = s len for p len  s len in zip param lens  save lens            raise valueerror string                          string            id map =  old id  p for old id  p in               zip chain from iterable  g string  for g in save group                      chain from iterable  g string  for g in group          def cast param  value           r   make a deep copy of value  cast all tensors to device of param             if isinstance value  torch tensor                                         if param be float point                    value = value to param dtype              value = value to param device              return value         elif isinstance value  dict               return  k  cast param  v  for k  v in value items            elif isinstance value  container abcs iterable               return type value  cast param  v  for v in value          else              return value                     state = defaultdict dict      for k  v in state dict string  items            if k in id map              param = id map k              state param  = cast param  v          else              state k  = v           def update group group  new group           new group string  = group string          return new group     param group =           update group g  ng  for g  ng in zip group  save group       self   setstate    string  state  string  param group   
 torch no grad   def step self  closure=none       string     loss = none     if closure be not none          with torch enable grad                loss = closure        for group in self param group          params with grad =            grads =            exp avgs =            exp infs =            state step =             beta1  beta2 = group string          eps = group string          lr = group string          weight decay = group string           for p in group string               if p grad be none                  continue             params with grad append p              if p grad be sparse                  raise runtimeerror string              grads append p grad               state = self state p                            if len state  == 0                  state string  = 0                 state string  = torch zero like p  memory format=torch preserve format                  state string  = torch zero like p  memory format=torch preserve format               exp avgs append state string               exp infs append state string                state string   = 1             state step append state string            f adamax params with grad                   grads                   exp avgs                   exp infs                   state step                   eps=eps                   beta1=beta1                   beta2=beta2                   lr=lr                   weight decay=weight decay       return loss 
class asgd optimizer       string      def   init   self  params  lr=1e-2  lambd=1e-4  alpha=0 75  t0=1e6  weight decay=0           if not 0 0 <= lr              raise valueerror string format lr           if not 0 0 <= weight decay              raise valueerror string format weight decay            default = dict lr=lr  lambd=lambd  alpha=alpha  t0=t0                          weight decay=weight decay          super asgd  self    init   params  default        torch no grad       def step self  closure=none           string         loss = none         if closure be not none              with torch enable grad                    loss = closure            for group in self param group              for p in group string                   if p grad be none                      continue                 grad = p grad                 if grad be sparse                      raise runtimeerror string                  state = self state p                                    if len state  == 0                      state string  = 0                     state string  = group string                      state string  = 1                     state string  = torch zero like p  memory format=torch preserve format                   state string   = 1                  if group string   = 0                      grad = grad add p  alpha=group string                                     p mul  1 - group string    state string                                     p add  grad  alpha=-state string                                     if state string   = 1                      state string  add  p sub state string   mul state string                    else                      state string  copy  p                                    state string  =  group string  /                                 math pow  1   group string    group string    state string    group string                    state string  = 1 / max 1  state string  - group string            return loss 
def load state dict self  state dict       r   load the optimizer state       args          state dict  dict   optimizer state  should be an object return             from a call to  meth `state dict`                   state dict = deepcopy state dict           group = self param group     save group = state dict string       if len group   = len save group           raise valueerror string                          string      param lens =  len g string   for g in group      save lens =  len g string   for g in save group      if any p len  = s len for p len  s len in zip param lens  save lens            raise valueerror string                          string            id map =  old id  p for old id  p in               zip chain from iterable  g string  for g in save group                      chain from iterable  g string  for g in group          def cast param  value           r   make a deep copy of value  cast all tensors to device of param             if isinstance value  torch tensor                                         if param be float point                    value = value to param dtype              value = value to param device              return value         elif isinstance value  dict               return  k  cast param  v  for k  v in value items            elif isinstance value  container abcs iterable               return type value  cast param  v  for v in value          else              return value                     state = defaultdict dict      for k  v in state dict string  items            if k in id map              param = id map k              state param  = cast param  v          else              state k  = v           def update group group  new group           new group string  = group string          return new group     param group =           update group g  ng  for g  ng in zip group  save group       self   setstate    string  state  string  param group   
 torch no grad   def step self  closure=none       string     loss = none     if closure be not none          with torch enable grad                loss = closure        for group in self param group          for p in group string               if p grad be none                  continue             grad = p grad             if grad be sparse                  raise runtimeerror string              state = self state p                            if len state  == 0                  state string  = 0                 state string  = group string                  state string  = 1                 state string  = torch zero like p  memory format=torch preserve format               state string   = 1              if group string   = 0                  grad = grad add p  alpha=group string                             p mul  1 - group string    state string                             p add  grad  alpha=-state string                             if state string   = 1                  state string  add  p sub state string   mul state string                else                  state string  copy  p                            state string  =  group string  /                             math pow  1   group string    group string    state string    group string                state string  = 1 / max 1  state string  - group string        return loss 
class lbfgs optimizer       string      def   init   self                   params                   lr=1                   max iter=20                   max eval=none                   tolerance grad=1e-7                   tolerance change=1e-9                   history size=100                   line search fn=none           if max eval be none              max eval = max iter   5 // 4         default = dict              lr=lr              max iter=max iter              max eval=max eval              tolerance grad=tolerance grad              tolerance change=tolerance change              history size=history size              line search fn=line search fn          super lbfgs  self    init   params  default           if len self param group   = 1              raise valueerror string                              string           self  params = self param group 0  string          self  numel cache = none      def  numel self           if self  numel cache be none              self  numel cache = reduce lambda total  p  total   p numel    self  params  0          return self  numel cache      def  gather flat grad self           view =            for p in self  params              if p grad be none                  view = p new p numel    zero                elif p grad be sparse                  view = p grad to dense   view -1              else                  view = p grad view -1              view append view          return torch cat view  0       def  add grad self  step size  update           offset = 0         for p in self  params              numel = p numel                            p add  update offset offset   numel  view as p   alpha=step size              offset  = numel         assert offset == self  numel        def  clone param self           return  p clone memory format=torch contiguous format  for p in self  params       def  set param self  params data           for p  pdata in zip self  params  params data               p copy  pdata       def  directional evaluate self  closure  x  t  d           self  add grad t  d          loss = float closure            flat grad = self  gather flat grad           self  set param x          return loss  flat grad       torch no grad       def step self  closure           string         assert len self param group  == 1                   closure = torch enable grad   closure           group = self param group 0          lr = group string          max iter = group string          max eval = group string          tolerance grad = group string          tolerance change = group string          line search fn = group string          history size = group string                             state = self state self  params 0           state setdefault string  0          state setdefault string  0                    orig loss = closure           loss = float orig loss          current evals = 1         state string   = 1          flat grad = self  gather flat grad           opt cond = flat grad abs   max   <= tolerance grad                   if opt cond              return orig loss                   d = state get string          t = state get string          old dirs = state get string          old stps = state get string          ro = state get string          h diag = state get string          prev flat grad = state get string          prev loss = state get string           n iter = 0                  while n iter < max iter                           n iter  = 1             state string   = 1                                                     if state string  == 1                  d = flat grad neg                   old dirs =                    old stps =                    ro =                    h diag = 1             else                                   y = flat grad sub prev flat grad                  s = d mul t                  ys = y dot s                    if ys > 1e-10                                           if len old dirs  == history size                                                   old dirs pop 0                          old stps pop 0                          ro pop 0                                            old dirs append y                      old stps append s                      ro append 1  / ys                                            h diag = ys / y dot y                                                       num old = len old dirs                   if string not in state                      state string  =  none    history size                 al = state string                                    q = flat grad neg                   for i in range num old - 1  -1  -1                       al i  = old stps i  dot q    ro i                      q add  old dirs i   alpha=-al i                                                      d = r = torch mul q  h diag                  for i in range num old                       be i = old dirs i  dot r    ro i                      r add  old stps i   alpha=al i  - be i               if prev flat grad be none                  prev flat grad = flat grad clone memory format=torch contiguous format              else                  prev flat grad copy  flat grad              prev loss = loss                                                                  if state string  == 1                  t = min 1   1  / flat grad abs   sum      lr             else                  t = lr                           gtd = flat grad dot d                              if gtd > -tolerance change                  break                           ls func evals = 0             if line search fn be not none                                   if line search fn  = string                      raise runtimeerror string                  else                      x init = self  clone param                        def obj func x  t  d                           return self  directional evaluate closure  x  t  d                       loss  flat grad  t  ls func evals =  strong wolfe                          obj func  x init  t  d  loss  flat grad  gtd                  self  add grad t  d                  opt cond = flat grad abs   max   <= tolerance grad             else                                   self  add grad t  d                  if n iter  = max iter                                                                                     with torch enable grad                            loss = float closure                        flat grad = self  gather flat grad                       opt cond = flat grad abs   max   <= tolerance grad                     ls func evals = 1                           current evals  = ls func evals             state string   = ls func evals                                                     if n iter == max iter                  break              if current evals >= max eval                  break                           if opt cond                  break                           if d mul t  abs   max   <= tolerance change                  break              if abs loss - prev loss  < tolerance change                  break          state string  = d         state string  = t         state string  = old dirs         state string  = old stps         state string  = ro         state string  = h diag         state string  = prev flat grad         state string  = prev loss          return orig loss 
def load state dict self  state dict       r   load the optimizer state       args          state dict  dict   optimizer state  should be an object return             from a call to  meth `state dict`                   state dict = deepcopy state dict           group = self param group     save group = state dict string       if len group   = len save group           raise valueerror string                          string      param lens =  len g string   for g in group      save lens =  len g string   for g in save group      if any p len  = s len for p len  s len in zip param lens  save lens            raise valueerror string                          string            id map =  old id  p for old id  p in               zip chain from iterable  g string  for g in save group                      chain from iterable  g string  for g in group          def cast param  value           r   make a deep copy of value  cast all tensors to device of param             if isinstance value  torch tensor                                         if param be float point                    value = value to param dtype              value = value to param device              return value         elif isinstance value  dict               return  k  cast param  v  for k  v in value items            elif isinstance value  container abcs iterable               return type value  cast param  v  for v in value          else              return value                     state = defaultdict dict      for k  v in state dict string  items            if k in id map              param = id map k              state param  = cast param  v          else              state k  = v           def update group group  new group           new group string  = group string          return new group     param group =           update group g  ng  for g  ng in zip group  save group       self   setstate    string  state  string  param group   
 torch no grad   def step self  closure       string     assert len self param group  == 1           closure = torch enable grad   closure       group = self param group 0      lr = group string      max iter = group string      max eval = group string      tolerance grad = group string      tolerance change = group string      line search fn = group string      history size = group string                 state = self state self  params 0       state setdefault string  0      state setdefault string  0            orig loss = closure       loss = float orig loss      current evals = 1     state string   = 1      flat grad = self  gather flat grad       opt cond = flat grad abs   max   <= tolerance grad           if opt cond          return orig loss           d = state get string      t = state get string      old dirs = state get string      old stps = state get string      ro = state get string      h diag = state get string      prev flat grad = state get string      prev loss = state get string       n iter = 0          while n iter < max iter                   n iter  = 1         state string   = 1                                     if state string  == 1              d = flat grad neg               old dirs =                old stps =                ro =                h diag = 1         else                           y = flat grad sub prev flat grad              s = d mul t              ys = y dot s                if ys > 1e-10                                   if len old dirs  == history size                                           old dirs pop 0                      old stps pop 0                      ro pop 0                                    old dirs append y                  old stps append s                  ro append 1  / ys                                    h diag = ys / y dot y                                           num old = len old dirs               if string not in state                  state string  =  none    history size             al = state string                            q = flat grad neg               for i in range num old - 1  -1  -1                   al i  = old stps i  dot q    ro i                  q add  old dirs i   alpha=-al i                                          d = r = torch mul q  h diag              for i in range num old                   be i = old dirs i  dot r    ro i                  r add  old stps i   alpha=al i  - be i           if prev flat grad be none              prev flat grad = flat grad clone memory format=torch contiguous format          else              prev flat grad copy  flat grad          prev loss = loss                                              if state string  == 1              t = min 1   1  / flat grad abs   sum      lr         else              t = lr                   gtd = flat grad dot d                      if gtd > -tolerance change              break                   ls func evals = 0         if line search fn be not none                           if line search fn  = string                  raise runtimeerror string              else                  x init = self  clone param                    def obj func x  t  d                       return self  directional evaluate closure  x  t  d                   loss  flat grad  t  ls func evals =  strong wolfe                      obj func  x init  t  d  loss  flat grad  gtd              self  add grad t  d              opt cond = flat grad abs   max   <= tolerance grad         else                           self  add grad t  d              if n iter  = max iter                                                                     with torch enable grad                        loss = float closure                    flat grad = self  gather flat grad                   opt cond = flat grad abs   max   <= tolerance grad                 ls func evals = 1                   current evals  = ls func evals         state string   = ls func evals                                     if n iter == max iter              break          if current evals >= max eval              break                   if opt cond              break                   if d mul t  abs   max   <= tolerance change              break          if abs loss - prev loss  < tolerance change              break      state string  = d     state string  = t     state string  = old dirs     state string  = old stps     state string  = ro     state string  = h diag     state string  = prev flat grad     state string  = prev loss      return orig loss 
class rmsprop optimizer       r   implement rmsprop algorithm       propose by g  hinton in his     `course <https //www cs toronto edu/~tijmen/csc321/slides/lecture slide lec6 pdf>`        the center version first appear in `generating sequence     with recurrent neural network <https //arxiv org/pdf/1308 0850v5 pdf>`        the implementation here take the square root of the gradient average before     add epsilon  note that tensorflow interchange these two operations   the effective     learn rate be thus  math `\alpha/ \sqrt v    \epsilon ` where  math `\alpha`     be the schedule learn rate and  math `v` be the weight move average     of the square gradient       args          params  iterable   iterable of parameters to optimize or dicts define             parameter group         lr  float  optional   learn rate  default  1e-2          momentum  float  optional   momentum factor  default  0          alpha  float  optional   smooth constant  default  0 99          eps  float  optional   term add to the denominator to improve             numerical stability  default  1e-8          center  bool  optional    if ``true``  compute the center rmsprop              the gradient be normalize by an estimation of its variance         weight decay  float  optional   weight decay  l2 penalty   default  0                def   init   self  params  lr=1e-2  alpha=0 99  eps=1e-8  weight decay=0  momentum=0  centered=false           if not 0 0 <= lr              raise valueerror string format lr           if not 0 0 <= eps              raise valueerror string format eps           if not 0 0 <= momentum              raise valueerror string format momentum           if not 0 0 <= weight decay              raise valueerror string format weight decay           if not 0 0 <= alpha              raise valueerror string format alpha            default = dict lr=lr  momentum=momentum  alpha=alpha  eps=eps  centered=centered  weight decay=weight decay          super rmsprop  self    init   params  default       def   setstate   self  state           super rmsprop  self    setstate   state          for group in self param group              group setdefault string  0              group setdefault string  false        torch no grad       def step self  closure=none           string         loss = none         if closure be not none              with torch enable grad                    loss = closure            for group in self param group              params with grad =                grads =                square avgs =                grad avgs =                momentum buffer list =                 for p in group string                   if p grad be none                      continue                 params with grad append p                   if p grad be sparse                      raise runtimeerror string                  grads append p grad                   state = self state p                                    if len state  == 0                      state string  = 0                     state string  = torch zero like p  memory format=torch preserve format                      if group string  > 0                          state string  = torch zero like p  memory format=torch preserve format                      if group string                           state string  = torch zero like p  memory format=torch preserve format                   square avgs append state string                    if group string  > 0                      momentum buffer list append state string                   if group string                       grad avgs append state string                    state string   = 1               f rmsprop params with grad                        grads                        square avgs                        grad avgs                        momentum buffer list                        lr=group string                         alpha=group string                         eps=group string                         weight decay=group string                         momentum=group string                         centered=group string            return loss 
def load state dict self  state dict       r   load the optimizer state       args          state dict  dict   optimizer state  should be an object return             from a call to  meth `state dict`                   state dict = deepcopy state dict           group = self param group     save group = state dict string       if len group   = len save group           raise valueerror string                          string      param lens =  len g string   for g in group      save lens =  len g string   for g in save group      if any p len  = s len for p len  s len in zip param lens  save lens            raise valueerror string                          string            id map =  old id  p for old id  p in               zip chain from iterable  g string  for g in save group                      chain from iterable  g string  for g in group          def cast param  value           r   make a deep copy of value  cast all tensors to device of param             if isinstance value  torch tensor                                         if param be float point                    value = value to param dtype              value = value to param device              return value         elif isinstance value  dict               return  k  cast param  v  for k  v in value items            elif isinstance value  container abcs iterable               return type value  cast param  v  for v in value          else              return value                     state = defaultdict dict      for k  v in state dict string  items            if k in id map              param = id map k              state param  = cast param  v          else              state k  = v           def update group group  new group           new group string  = group string          return new group     param group =           update group g  ng  for g  ng in zip group  save group       self   setstate    string  state  string  param group   
 torch no grad   def step self  closure=none       string     loss = none     if closure be not none          with torch enable grad                loss = closure        for group in self param group          params with grad =            grads =            square avgs =            grad avgs =            momentum buffer list =             for p in group string               if p grad be none                  continue             params with grad append p               if p grad be sparse                  raise runtimeerror string              grads append p grad               state = self state p                            if len state  == 0                  state string  = 0                 state string  = torch zero like p  memory format=torch preserve format                  if group string  > 0                      state string  = torch zero like p  memory format=torch preserve format                  if group string                       state string  = torch zero like p  memory format=torch preserve format               square avgs append state string                if group string  > 0                  momentum buffer list append state string               if group string                   grad avgs append state string                state string   = 1           f rmsprop params with grad                    grads                    square avgs                    grad avgs                    momentum buffer list                    lr=group string                     alpha=group string                     eps=group string                     weight decay=group string                     momentum=group string                     centered=group string        return loss 
class rprop optimizer       string      def   init   self  params  lr=1e-2  etas= 0 5  1 2   step sizes= 1e-6  50            if not 0 0 <= lr              raise valueerror string format lr           if not 0 0 < etas 0  < 1 0 < etas 1               raise valueerror string format etas 0   etas 1             default = dict lr=lr  etas=etas  step sizes=step size          super rprop  self    init   params  default        torch no grad       def step self  closure=none           string         loss = none         if closure be not none              with torch enable grad                    loss = closure            for group in self param group              params =                grads =                prevs =                step size =                 for p in group string                   if p grad be none                      continue                 params append p                  grad = p grad                 if grad be sparse                      raise runtimeerror string                   grads append grad                  state = self state p                                    if len state  == 0                      state string  = 0                     state string  = torch zero like p  memory format=torch preserve format                      state string  = grad new   resize as  grad  fill  group string                    prevs append state string                   step size append state string                    etaminus  etaplus = group string                  step size min  step size max = group string                   state string   = 1              f rprop params                      grads                      prevs                      step size                      step size min=step size min                      step size max=step size max                      etaminus=etaminus                      etaplus=etaplus           return loss 
def load state dict self  state dict       r   load the optimizer state       args          state dict  dict   optimizer state  should be an object return             from a call to  meth `state dict`                   state dict = deepcopy state dict           group = self param group     save group = state dict string       if len group   = len save group           raise valueerror string                          string      param lens =  len g string   for g in group      save lens =  len g string   for g in save group      if any p len  = s len for p len  s len in zip param lens  save lens            raise valueerror string                          string            id map =  old id  p for old id  p in               zip chain from iterable  g string  for g in save group                      chain from iterable  g string  for g in group          def cast param  value           r   make a deep copy of value  cast all tensors to device of param             if isinstance value  torch tensor                                         if param be float point                    value = value to param dtype              value = value to param device              return value         elif isinstance value  dict               return  k  cast param  v  for k  v in value items            elif isinstance value  container abcs iterable               return type value  cast param  v  for v in value          else              return value                     state = defaultdict dict      for k  v in state dict string  items            if k in id map              param = id map k              state param  = cast param  v          else              state k  = v           def update group group  new group           new group string  = group string          return new group     param group =           update group g  ng  for g  ng in zip group  save group       self   setstate    string  state  string  param group   
 torch no grad   def step self  closure=none       string     loss = none     if closure be not none          with torch enable grad                loss = closure        for group in self param group          params =            grads =            prevs =            step size =             for p in group string               if p grad be none                  continue             params append p              grad = p grad             if grad be sparse                  raise runtimeerror string               grads append grad              state = self state p                            if len state  == 0                  state string  = 0                 state string  = torch zero like p  memory format=torch preserve format                  state string  = grad new   resize as  grad  fill  group string                prevs append state string               step size append state string                etaminus  etaplus = group string              step size min  step size max = group string               state string   = 1          f rprop params                  grads                  prevs                  step size                  step size min=step size min                  step size max=step size max                  etaminus=etaminus                  etaplus=etaplus       return loss 
class sgd optimizer       r   implement stochastic gradient descent  optionally with momentum        nesterov momentum be base on the formula from     `on the importance of initialization and momentum in deep learning`         args          params  iterable   iterable of parameters to optimize or dicts define             parameter group         lr  float   learn rate         momentum  float  optional   momentum factor  default  0          weight decay  float  optional   weight decay  l2 penalty   default  0          dampen  float  optional   dampen for momentum  default  0          nesterov  bool  optional   enable nesterov momentum  default  false       example          >>> optimizer = torch optim sgd model parameters    lr=0 1  momentum=0 9          >>> optimizer zero grad           >>> loss fn model input   target  backward           >>> optimizer step           http //www cs toronto edu/ 7ehinton/absps/momentum pdf         note           the implementation of sgd with momentum/nesterov subtly differ from         sutskever et  al  and implementations in some other frameworks           consider the specific case of momentum  the update can be write as             math               \begin align                  v  t 1    = \mu   v  t    g  t 1   \\                 p  t 1    = p  t  - \text lr    v  t 1               \end align           where  math `p`   math `g`   math `v` and  math `\mu` denote the         parameters  gradient  velocity  and momentum respectively           this be in contrast to sutskever et  al  and         other frameworks which employ an update of the form             math               \begin align                  v  t 1    = \mu   v  t    \text lr    g  t 1   \\                 p  t 1    = p  t  - v  t 1               \end align           the nesterov version be analogously modify               def   init   self  params  lr=required  momentum=0  dampening=0                   weight decay=0  nesterov=false           if lr be not require and lr < 0 0              raise valueerror string format lr           if momentum < 0 0              raise valueerror string format momentum           if weight decay < 0 0              raise valueerror string format weight decay            default = dict lr=lr  momentum=momentum  dampening=dampening                          weight decay=weight decay  nesterov=nesterov          if nesterov and  momentum <= 0 or dampen  = 0               raise valueerror string          super sgd  self    init   params  default       def   setstate   self  state           super sgd  self    setstate   state          for group in self param group              group setdefault string  false        torch no grad       def step self  closure=none           string         loss = none         if closure be not none              with torch enable grad                    loss = closure            for group in self param group              params with grad =                d p list =                momentum buffer list =                weight decay = group string              momentum = group string              dampen = group string              nesterov = group string              lr = group string               for p in group string                   if p grad be not none                      params with grad append p                      d p list append p grad                       state = self state p                      if string not in state                          momentum buffer list append none                      else                          momentum buffer list append state string                f sgd params with grad                    d p list                    momentum buffer list                    weight decay=weight decay                    momentum=momentum                    lr=lr                    dampening=dampening                    nesterov=nesterov                            for p  momentum buffer in zip params with grad  momentum buffer list                   state = self state p                  state string  = momentum buffer          return loss 
def load state dict self  state dict       r   load the optimizer state       args          state dict  dict   optimizer state  should be an object return             from a call to  meth `state dict`                   state dict = deepcopy state dict           group = self param group     save group = state dict string       if len group   = len save group           raise valueerror string                          string      param lens =  len g string   for g in group      save lens =  len g string   for g in save group      if any p len  = s len for p len  s len in zip param lens  save lens            raise valueerror string                          string            id map =  old id  p for old id  p in               zip chain from iterable  g string  for g in save group                      chain from iterable  g string  for g in group          def cast param  value           r   make a deep copy of value  cast all tensors to device of param             if isinstance value  torch tensor                                         if param be float point                    value = value to param dtype              value = value to param device              return value         elif isinstance value  dict               return  k  cast param  v  for k  v in value items            elif isinstance value  container abcs iterable               return type value  cast param  v  for v in value          else              return value                     state = defaultdict dict      for k  v in state dict string  items            if k in id map              param = id map k              state param  = cast param  v          else              state k  = v           def update group group  new group           new group string  = group string          return new group     param group =           update group g  ng  for g  ng in zip group  save group       self   setstate    string  state  string  param group   
 torch no grad   def step self  closure=none       string     loss = none     if closure be not none          with torch enable grad                loss = closure        for group in self param group          params with grad =            d p list =            momentum buffer list =            weight decay = group string          momentum = group string          dampen = group string          nesterov = group string          lr = group string           for p in group string               if p grad be not none                  params with grad append p                  d p list append p grad                   state = self state p                  if string not in state                      momentum buffer list append none                  else                      momentum buffer list append state string            f sgd params with grad                d p list                momentum buffer list                weight decay=weight decay                momentum=momentum                lr=lr                dampening=dampening                nesterov=nesterov                    for p  momentum buffer in zip params with grad  momentum buffer list               state = self state p              state string  = momentum buffer      return loss 
class lambdalr  lrscheduler       string      def   init   self  optimizer  lr lambda  last epoch=-1  verbose=false           self optimizer = optimizer          if not isinstance lr lambda  list  and not isinstance lr lambda  tuple               self lr lambdas =  lr lambda    len optimizer param group          else              if len lr lambda   = len optimizer param group                   raise valueerror string format                      len optimizer param group   len lr lambda                self lr lambdas = list lr lambda          super lambdalr  self    init   optimizer  last epoch  verbose       def state dict self           string          state dict =  key  value for key  value in self   dict   items   if key not in  string  string           state dict string  =  none    len self lr lambdas           for idx  fn in enumerate self lr lambdas               if not isinstance fn  type functiontype                   state dict string  idx  = fn   dict   copy            return state dict      def load state dict self  state dict           string          lr lambdas = state dict pop string          self   dict   update state dict                            state dict string  = lr lambdas          for idx  fn in enumerate lr lambdas               if fn be not none                  self lr lambdas idx    dict   update fn       def get lr self           if not self  get lr call within step              warn warn string                           string           return  base lr   lmbda self last epoch                  for lmbda  base lr in zip self lr lambdas  self base lrs   
def load state dict self  state dict       string      lr lambdas = state dict pop string      self   dict   update state dict                state dict string  = lr lambdas      for idx  fn in enumerate lr lambdas           if fn be not none              self lr lambdas idx    dict   update fn  
class multiplicativelr  lrscheduler       string      def   init   self  optimizer  lr lambda  last epoch=-1  verbose=false           self optimizer = optimizer          if not isinstance lr lambda  list  and not isinstance lr lambda  tuple               self lr lambdas =  lr lambda    len optimizer param group          else              if len lr lambda   = len optimizer param group                   raise valueerror string format                      len optimizer param group   len lr lambda                self lr lambdas = list lr lambda          super multiplicativelr  self    init   optimizer  last epoch  verbose       def state dict self           string         state dict =  key  value for key  value in self   dict   items   if key not in  string  string           state dict string  =  none    len self lr lambdas           for idx  fn in enumerate self lr lambdas               if not isinstance fn  type functiontype                   state dict string  idx  = fn   dict   copy            return state dict      def load state dict self  state dict           string         lr lambdas = state dict pop string          self   dict   update state dict                            state dict string  = lr lambdas          for idx  fn in enumerate lr lambdas               if fn be not none                  self lr lambdas idx    dict   update fn       def get lr self           if not self  get lr call within step              warn warn string                           string  userwarning           if self last epoch > 0              return  group string    lmbda self last epoch                      for lmbda  group in zip self lr lambdas  self optimizer param group           else              return list self base lrs  
def load state dict self  state dict       string     lr lambdas = state dict pop string      self   dict   update state dict                state dict string  = lr lambdas      for idx  fn in enumerate lr lambdas           if fn be not none              self lr lambdas idx    dict   update fn  
class steplr  lrscheduler       string      def   init   self  optimizer  step size  gamma=0 1  last epoch=-1  verbose=false           self step size = step size         self gamma = gamma         super steplr  self    init   optimizer  last epoch  verbose       def get lr self           if not self  get lr call within step              warn warn string                           string  userwarning           if  self last epoch == 0  or  self last epoch   self step size  = 0               return  group string  for group in self optimizer param group          return  group string    self gamma                 for group in self optimizer param group       def  get close form lr self           return  base lr   self gamma     self last epoch // self step size                  for base lr in self base lrs  
def load state dict self  state dict       string     self   dict   update state dict  
class multisteplr  lrscheduler       string      def   init   self  optimizer  milestones  gamma=0 1  last epoch=-1  verbose=false           self milestones = counter milestones          self gamma = gamma         super multisteplr  self    init   optimizer  last epoch  verbose       def get lr self           if not self  get lr call within step              warn warn string                           string  userwarning           if self last epoch not in self milestones              return  group string  for group in self optimizer param group          return  group string    self gamma    self milestones self last epoch                  for group in self optimizer param group       def  get close form lr self           milestones = list sort self milestones elements             return  base lr   self gamma    bisect right milestones  self last epoch                  for base lr in self base lrs  
def load state dict self  state dict       string     self   dict   update state dict  
class exponentiallr  lrscheduler       string      def   init   self  optimizer  gamma  last epoch=-1  verbose=false           self gamma = gamma         super exponentiallr  self    init   optimizer  last epoch  verbose       def get lr self           if not self  get lr call within step              warn warn string                           string  userwarning           if self last epoch == 0              return self base lrs         return  group string    self gamma                 for group in self optimizer param group       def  get close form lr self           return  base lr   self gamma    self last epoch                 for base lr in self base lrs  
def load state dict self  state dict       string     self   dict   update state dict  
class cosineannealinglr  lrscheduler       r   set the learn rate of each parameter group use a cosine anneal     schedule  where  math `\eta  max ` be set to the initial lr and      math `t  cur ` be the number of epochs since the last restart in sgdr          math           \begin align              \eta t   = \eta  min    \frac 1  2  \eta  max  - \eta  min  \left 1               \cos\left \frac t  cur   t  max  \pi\right \right                 t  cur  \neq  2k 1 t  max   \\             \eta  t 1    = \eta  t    \frac 1  2  \eta  max  - \eta  min               \left 1 - \cos\left \frac 1  t  max  \pi\right \right                 t  cur  =  2k 1 t  max           \end align       when last epoch=-1  set initial lr as lr  notice that because the schedule     be define recursively  the learn rate can be simultaneously modify     outside this scheduler by other operators  if the learn rate be set     solely by this scheduler  the learn rate at each step become          math           \eta t = \eta  min    \frac 1  2  \eta  max  - \eta  min  \left 1           \cos\left \frac t  cur   t  max  \pi\right \right       it have be propose in     `sgdr  stochastic gradient descent with warm restarts`   note that this only     implement the cosine anneal part of sgdr  and not the restart       args          optimizer  optimizer   wrap optimizer          t max  int   maximum number of iterations          eta min  float   minimum learn rate  default  0          last epoch  int   the index of last epoch  default  -1          verbose  bool   if ``true``  print a message to stdout for             each update  default  ``false``           sgdr\  stochastic gradient descent with warm restart          https //arxiv org/abs/1608 03983              def   init   self  optimizer  t max  eta min=0  last epoch=-1  verbose=false           self t max = t max         self eta min = eta min         super cosineannealinglr  self    init   optimizer  last epoch  verbose       def get lr self           if not self  get lr call within step              warn warn string                           string  userwarning           if self last epoch == 0              return self base lrs         elif  self last epoch - 1 - self t max     2   self t max  == 0              return  group string     base lr - self eta min                         1 - math cos math pi / self t max   / 2                     for base lr  group in                     zip self base lrs  self optimizer param group           return   1   math cos math pi   self last epoch / self t max   /                  1   math cos math pi    self last epoch - 1  / self t max                      group string  - self eta min    self eta min                 for group in self optimizer param group       def  get close form lr self           return  self eta min    base lr - self eta min                     1   math cos math pi   self last epoch / self t max   / 2                 for base lr in self base lrs  
def load state dict self  state dict       string     self   dict   update state dict  
class reducelronplateau object       string      def   init   self  optimizer  mode=string  factor=0 1  patience=10                   threshold=1e-4  threshold mode=string  cooldown=0                   min lr=0  eps=1e-8  verbose=false            if factor >= 1 0              raise valueerror string          self factor = factor                   if not isinstance optimizer  optimizer               raise typeerror string format                  type optimizer    name             self optimizer = optimizer          if isinstance min lr  list  or isinstance min lr  tuple               if len min lr   = len optimizer param group                   raise valueerror string format                      len optimizer param group   len min lr                self min lrs = list min lr          else              self min lrs =  min lr    len optimizer param group           self patience = patience         self verbose = verbose         self cooldown = cooldown         self cooldown counter = 0         self mode = mode         self threshold = threshold         self threshold mode = threshold mode         self best = none         self num bad epochs = none         self mode worse = none           self eps = eps         self last epoch = 0         self  init be better mode=mode  threshold=threshold                               threshold mode=threshold mode          self  reset        def  reset self           string         self best = self mode worse         self cooldown counter = 0         self num bad epochs = 0      def step self  metrics  epoch=none                    current = float metrics          if epoch be none              epoch = self last epoch   1         else              warn warn epoch deprecation warn  userwarning          self last epoch = epoch          if self be better current  self best               self best = current             self num bad epochs = 0         else              self num bad epochs  = 1          if self in cooldown              self cooldown counter -= 1             self num bad epochs = 0            if self num bad epochs > self patience              self  reduce lr epoch              self cooldown counter = self cooldown             self num bad epochs = 0          self  last lr =  group string  for group in self optimizer param group       def  reduce lr self  epoch           for i  param group in enumerate self optimizer param group               old lr = float param group string               new lr = max old lr   self factor  self min lrs i               if old lr - new lr > self eps                  param group string  = new lr                 if self verbose                      print string                           string format epoch  i  new lr         property     def in cooldown self           return self cooldown counter > 0      def be better self  a  best           if self mode == string and self threshold mode == string              rel epsilon = 1  - self threshold             return a < best   rel epsilon          elif self mode == string and self threshold mode == string              return a < best - self threshold          elif self mode == string and self threshold mode == string              rel epsilon = self threshold   1              return a > best   rel epsilon          else                return a > best   self threshold      def  init be better self  mode  threshold  threshold mode           if mode not in  string  string               raise valueerror string   mode   string          if threshold mode not in  string  string               raise valueerror string   threshold mode   string           if mode == string              self mode worse = inf         else                self mode worse = -inf          self mode = mode         self threshold = threshold         self threshold mode = threshold mode      def state dict self           return  key  value for key  value in self   dict   items   if key  = string       def load state dict self  state dict           self   dict   update state dict          self  init be better mode=self mode  threshold=self threshold  threshold mode=self threshold mode  
class cycliclr  lrscheduler       r   set the learn rate of each parameter group accord to     cyclical learn rate policy  clr   the policy cycle the learn     rate between two boundaries with a constant frequency  as detail in     the paper `cyclical learn rat for train neural networks`       the distance between the two boundaries can be scale on a per-iteration     or per-cycle basis       cyclical learn rate policy change the learn rate after every batch      `step` should be call after a batch have be use for train       this class have three built-in policies  as put forth in the paper          triangular   a basic triangular cycle without amplitude scale         triangular2   a basic triangular cycle that scale initial amplitude by half each cycle         exp range   a cycle that scale initial amplitude by  math `\text gamma   \text cycle iterations  `       at each cycle iteration       this implementation be adapt from the github repo  `bckenstler/clr`       args          optimizer  optimizer   wrap optimizer          base lr  float or list   initial learn rate which be the             lower boundary in the cycle for each parameter group          max lr  float or list   upper learn rate boundaries in the cycle             for each parameter group  functionally              it define the cycle amplitude  max lr - base lr               the lr at any cycle be the sum of base lr             and some scale of the amplitude  therefore             max lr may not actually be reach depend on             scale function          step size up  int   number of train iterations in the             increase half of a cycle  default  2000         step size down  int   number of train iterations in the             decrease half of a cycle  if step size down be none              it be set to step size up  default  none         mode  str   one of  triangular  triangular2  exp range               value correspond to policies detail above              if scale fn be not none  this argument be ignore              default   triangular          gamma  float   constant in  exp range  scale function              gamma   cycle iterations              default  1 0         scale fn  function   custom scale policy define by a single             argument lambda function  where             0 <= scale fn x  <= 1 for all x >= 0              if specify  then  mode  be ignore              default  none         scale mode  str     cycle    iterations                define whether scale fn be evaluate on             cycle number or cycle iterations  train             iterations since start of cycle               default   cycle          cycle momentum  bool   if ``true``  momentum be cycle inversely             to learn rate between  base momentum  and  max momentum               default  true         base momentum  float or list   lower momentum boundaries in the cycle             for each parameter group  note that momentum be cycle inversely             to learn rate  at the peak of a cycle  momentum be              base momentum  and learn rate be  max lr               default  0 8         max momentum  float or list   upper momentum boundaries in the cycle             for each parameter group  functionally              it define the cycle amplitude  max momentum - base momentum               the momentum at any cycle be the difference of max momentum             and some scale of the amplitude  therefore             base momentum may not actually be reach depend on             scale function  note that momentum be cycle inversely             to learn rate  at the start of a cycle  momentum be  max momentum              and learn rate be  base lr              default  0 9         last epoch  int   the index of the last batch  this parameter be use when             resume a train job  since `step  ` should be invoke after each             batch instead of after each epoch  this number represent the total             number of  batch  compute  not the total number of epochs compute              when last epoch=-1  the schedule be start from the begin              default  -1         verbose  bool   if ``true``  print a message to stdout for             each update  default  ``false``       example          >>> optimizer = torch optim sgd model parameters    lr=0 1  momentum=0 9          >>> scheduler = torch optim lr scheduler cycliclr optimizer  base lr=0 01  max lr=0 1          >>> data loader = torch utils data dataloader              >>> for epoch in range 10           >>>     for batch in data loader          >>>         train batch              >>>         scheduler step             cyclical learn rat for train neural network  https //arxiv org/abs/1506 01186         bckenstler/clr  https //github com/bckenstler/clr              def   init   self                   optimizer                   base lr                   max lr                   step size up=2000                   step size down=none                   mode=string                   gamma=1                    scale fn=none                   scale mode=string                   cycle momentum=true                   base momentum=0 8                   max momentum=0 9                   last epoch=-1                   verbose=false                     if not isinstance optimizer  optimizer               raise typeerror string format                  type optimizer    name             self optimizer = optimizer          base lrs = self  format param string  optimizer  base lr          if last epoch == -1              for lr  group in zip base lrs  optimizer param group                   group string  = lr          self max lrs = self  format param string  optimizer  max lr           step size up = float step size up          step size down = float step size down  if step size down be not none else step size up         self total size = step size up   step size down         self step ratio = step size up / self total size          if mode not in  string  string  string  \                 and scale fn be none              raise valueerror string           self mode = mode         self gamma = gamma          if scale fn be none              if self mode == string                  self scale fn = self  triangular scale fn                 self scale mode = string             elif self mode == string                  self scale fn = self  triangular2 scale fn                 self scale mode = string             elif self mode == string                  self scale fn = self  exp range scale fn                 self scale mode = string         else              self scale fn = scale fn             self scale mode = scale mode          self cycle momentum = cycle momentum         if cycle momentum              if string not in optimizer default                  raise valueerror string               base momentums = self  format param string  optimizer  base momentum              if last epoch == -1                  for momentum  group in zip base momentums  optimizer param group                       group string  = momentum             self base momentums =  group string  for group in optimizer param group              self max momentums = self  format param string  optimizer  max momentum           super cycliclr  self    init   optimizer  last epoch  verbose          self base lrs = base lrs      def  format param self  name  optimizer  param           string         if isinstance param   list  tuple                if len param   = len optimizer param group                   raise valueerror string format                      len optimizer param group   name  len param                return param         else              return  param    len optimizer param group       def  triangular scale fn self  x           return 1       def  triangular2 scale fn self  x           return 1 /  2      x - 1        def  exp range scale fn self  x           return self gamma   x       def get lr self           string          if not self  get lr call within step              warn warn string                           string  userwarning           cycle = math floor 1   self last epoch / self total size          x = 1    self last epoch / self total size - cycle         if x <= self step ratio              scale factor = x / self step ratio         else              scale factor =  x - 1  /  self step ratio - 1           lrs =            for base lr  max lr in zip self base lrs  self max lrs               base height =  max lr - base lr    scale factor             if self scale mode == string                  lr = base lr   base height   self scale fn cycle              else                  lr = base lr   base height   self scale fn self last epoch              lrs append lr           if self cycle momentum              momentums =                for base momentum  max momentum in zip self base momentums  self max momentums                   base height =  max momentum - base momentum    scale factor                 if self scale mode == string                      momentum = max momentum - base height   self scale fn cycle                  else                      momentum = max momentum - base height   self scale fn self last epoch                  momentums append momentum              for param group  momentum in zip self optimizer param group  momentums                   param group string  = momentum          return lrs 
def load state dict self  state dict       string     self   dict   update state dict  
class onecyclelr  lrscheduler       r   set the learn rate of each parameter group accord to the     1cycle learn rate policy  the 1cycle policy anneal the learn     rate from an initial learn rate to some maximum learn rate and then     from that maximum learn rate to some minimum learn rate much lower     than the initial learn rate      this policy be initially describe in the paper `super-convergence      very fast train of neural network use large learn rates`        the 1cycle learn rate policy change the learn rate after every batch      `step` should be call after a batch have be use for train       this scheduler be not chainable       note also that the total number of step in the cycle can be determine in one     of two ways  list in order of precedence           a value for total step be explicitly provide         a number of epochs  epochs  and a number of step per epoch         step per epoch  be provide         in this case  the number of total step be infer by        total step = epochs   step per epoch      you must either provide a value for total step or provide a value for both     epochs and step per epoch       the default behaviour of this scheduler follow the fastai implementation of 1cycle  which     claim that  unpublished work have show even better result by use only two phase   to     mimic the behaviour of the original paper instead  set ``three phase=true``       args          optimizer  optimizer   wrap optimizer          max lr  float or list   upper learn rate boundaries in the cycle             for each parameter group          total step  int   the total number of step in the cycle  note that             if a value be not provide here  then it must be infer by provide             a value for epochs and step per epoch              default  none         epochs  int   the number of epochs to train for  this be use along             with step per epoch in order to infer the total number of step in the cycle             if a value for total step be not provide              default  none         step per epoch  int   the number of step per epoch to train for  this be             use along with epochs in order to infer the total number of step in the             cycle if a value for total step be not provide              default  none         pct start  float   the percentage of the cycle  in number of step  spend             increase the learn rate              default  0 3         anneal strategy  str     cos    linear               specify the anneal strategy   cos  for cosine anneal   linear  for             linear anneal              default   cos          cycle momentum  bool   if ``true``  momentum be cycle inversely             to learn rate between  base momentum  and  max momentum               default  true         base momentum  float or list   lower momentum boundaries in the cycle             for each parameter group  note that momentum be cycle inversely             to learn rate  at the peak of a cycle  momentum be              base momentum  and learn rate be  max lr               default  0 85         max momentum  float or list   upper momentum boundaries in the cycle             for each parameter group  functionally              it define the cycle amplitude  max momentum - base momentum               note that momentum be cycle inversely             to learn rate  at the start of a cycle  momentum be  max momentum              and learn rate be  base lr              default  0 95         div factor  float   determine the initial learn rate via             initial lr = max lr/div factor             default  25         final div factor  float   determine the minimum learn rate via             min lr = initial lr/final div factor             default  1e4         three phase  bool   if ``true``  use a third phase of the schedule to annihilate the             learn rate accord to  final div factor  instead of modify the second             phase  the first two phase will be symmetrical about the step indicate by              pct start            last epoch  int   the index of the last batch  this parameter be use when             resume a train job  since `step  ` should be invoke after each             batch instead of after each epoch  this number represent the total             number of  batch  compute  not the total number of epochs compute              when last epoch=-1  the schedule be start from the begin              default  -1         verbose  bool   if ``true``  print a message to stdout for             each update  default  ``false``       example          >>> data loader = torch utils data dataloader              >>> optimizer = torch optim sgd model parameters    lr=0 1  momentum=0 9          >>> scheduler = torch optim lr scheduler onecyclelr optimizer  max lr=0 01  step per epoch=len data loader   epochs=10          >>> for epoch in range 10           >>>     for batch in data loader          >>>         train batch              >>>         scheduler step             super-convergence\  very fast train of neural network use large learn rat          https //arxiv org/abs/1708 07120             def   init   self                   optimizer                   max lr                   total steps=none                   epochs=none                   step per epoch=none                   pct start=0 3                   anneal strategy=string                   cycle momentum=true                   base momentum=0 85                   max momentum=0 95                   div factor=25                    final div factor=1e4                   three phase=false                   last epoch=-1                   verbose=false                     if not isinstance optimizer  optimizer               raise typeerror string format                  type optimizer    name             self optimizer = optimizer                   if total step be none and epochs be none and step per epoch be none              raise valueerror string          elif total step be not none              if total step <= 0 or not isinstance total step  int                   raise valueerror string format total step               self total step = total step         else              if epochs <= 0 or not isinstance epochs  int                   raise valueerror string format epochs               if step per epoch <= 0 or not isinstance step per epoch  int                   raise valueerror string format step per epoch               self total step = epochs   step per epoch          if three phase              self  schedule phase =                                         string  float pct start   self total step  - 1                      string  string                      string  string                      string  string                      string  string                                                           string  float 2   pct start   self total step  - 2                      string  string                      string  string                      string  string                      string  string                                                           string  self total step - 1                      string  string                      string  string                      string  string                      string  string                                           else              self  schedule phase =                                         string  float pct start   self total step  - 1                      string  string                      string  string                      string  string                      string  string                                                           string  self total step - 1                      string  string                      string  string                      string  string                      string  string                                                     if pct start < 0 or pct start > 1 or not isinstance pct start  float               raise valueerror string format pct start                     if anneal strategy not in  string  string               raise valueerror string format anneal strategy           elif anneal strategy == string              self anneal func = self  anneal cos         elif anneal strategy == string              self anneal func = self  anneal linear                   max lrs = self  format param string  self optimizer  max lr          if last epoch == -1              for idx  group in enumerate self optimizer param group                   group string  = max lrs idx  / div factor                 group string  = max lrs idx                  group string  = group string  / final div factor                   self cycle momentum = cycle momentum         if self cycle momentum              if string not in self optimizer default and string not in self optimizer default                  raise valueerror string              self use beta1 = string in self optimizer default             max momentums = self  format param string  optimizer  max momentum              base momentums = self  format param string  optimizer  base momentum              if last epoch == -1                  for m momentum  b momentum  group in zip max momentums  base momentums  optimizer param group                       if self use beta1                             beta2 = group string                          group string  =  m momentum  beta2                      else                          group string  = m momentum                     group string  = m momentum                     group string  = b momentum          super onecyclelr  self    init   optimizer  last epoch  verbose       def  format param self  name  optimizer  param           string         if isinstance param   list  tuple                if len param   = len optimizer param group                   raise valueerror string format                      len optimizer param group   name  len param                return param         else              return  param    len optimizer param group       def  anneal cos self  start  end  pct           string         cos out = math cos math pi   pct    1         return end    start - end  / 2 0   cos out      def  anneal linear self  start  end  pct           string         return  end - start    pct   start      def get lr self           if not self  get lr call within step              warn warn string                           string  userwarning           lrs =            step num = self last epoch          if step num > self total step              raise valueerror string                               format step num   1  self total step            for group in self optimizer param group              start step = 0             for i  phase in enumerate self  schedule phase                   end step = phase string                  if step num <= end step or i == len self  schedule phase  - 1                      pct =  step num - start step  /  end step - start step                      compute lr = self anneal func group phase string    group phase string    pct                      if self cycle momentum                          compute momentum = self anneal func group phase string    group phase string    pct                      break                 start step = phase string               lrs append compute lr              if self cycle momentum                  if self use beta1                         beta2 = group string                      group string  =  compute momentum  beta2                  else                      group string  = compute momentum          return lrs 
def load state dict self  state dict       string     self   dict   update state dict  
class cosineannealingwarmrestarts  lrscheduler       r   set the learn rate of each parameter group use a cosine anneal     schedule  where  math `\eta  max ` be set to the initial lr   math `t  cur `     be the number of epochs since the last restart and  math `t  i ` be the number     of epochs between two warm restart in sgdr          math           \eta t = \eta  min    \frac 1  2  \eta  max  - \eta  min  \left 1           \cos\left \frac t  cur   t  i  \pi\right \right       when  math `t  cur =t  i `  set  math `\eta t = \eta  min `      when  math `t  cur =0` after restart  set  math `\eta t=\eta  max `       it have be propose in     `sgdr  stochastic gradient descent with warm restarts`        args          optimizer  optimizer   wrap optimizer          t 0  int   number of iterations for the first restart          t mult  int  optional   a factor increase  math `t  i ` after a restart  default  1          eta min  float  optional   minimum learn rate  default  0          last epoch  int  optional   the index of last epoch  default  -1          verbose  bool   if ``true``  print a message to stdout for             each update  default  ``false``           sgdr\  stochastic gradient descent with warm restart          https //arxiv org/abs/1608 03983              def   init   self  optimizer  t 0  t mult=1  eta min=0  last epoch=-1  verbose=false           if t 0 <= 0 or not isinstance t 0  int               raise valueerror string format t 0           if t mult < 1 or not isinstance t mult  int               raise valueerror string format t mult           self t 0 = t 0         self t i = t 0         self t mult = t mult         self eta min = eta min          super cosineannealingwarmrestarts  self    init   optimizer  last epoch  verbose           self t cur = self last epoch      def get lr self           if not self  get lr call within step              warn warn string                           string  userwarning           return  self eta min    base lr - self eta min     1   math cos math pi   self t cur / self t i   / 2                 for base lr in self base lrs       def step self  epoch=none           string          if epoch be none and self last epoch < 0              epoch = 0          if epoch be none              epoch = self last epoch   1             self t cur = self t cur   1             if self t cur >= self t i                  self t cur = self t cur - self t i                 self t i = self t i   self t mult         else              if epoch < 0                  raise valueerror string format epoch               if epoch >= self t 0                  if self t mult == 1                      self t cur = epoch   self t 0                 else                      n = int math log  epoch / self t 0    self t mult - 1    1   self t mult                       self t cur = epoch - self t 0    self t mult    n - 1  /  self t mult - 1                      self t i = self t 0   self t mult     n              else                  self t i = self t 0                 self t cur = epoch         self last epoch = math floor epoch           class  enable get lr call               def   init   self  o                   self o = o              def   enter   self                   self o  get lr call within step = true                 return self              def   exit   self  type  value  traceback                   self o  get lr call within step = false                 return self          with  enable get lr call self               for i  data in enumerate zip self optimizer param group  self get lr                      param group  lr = data                 param group string  = lr                 self print lr self verbose  i  lr  epoch           self  last lr =  group string  for group in self optimizer param group  
def load state dict self  state dict       string     self   dict   update state dict  
def sum input  tensor  dim  dimordims = none          dtype  optional dtype  = none  -> tensor      r        return the sum of each row of the sparse tensor  attr `input` in the give     dimension  attr `dim`  if  attr `dim` be a list of dimension      reduce over all of them  when sum over all ``sparse dim``  this method     return a dense tensor instead of a sparse tensor       all sum  attr `dim` be squeeze  see  func `torch squeeze`   result an output     tensor have  attr `dim` fewer dimension than  attr `input`       during backward  only gradients at ``nnz`` locations of  attr `input`     will propagate back  note that the gradients of  attr `input` be coalesce       args          input  tensor   the input sparse tensor         dim  int or tuple of ints   a dimension or a list of dimension to reduce  default  reduce             over all dim          dtype   class `torch dtype`  optional   the desire data type of return tensor              default  dtype of  attr `input`       example            >>> nnz = 3         >>> dim =  5  5  2  3          >>> i = torch cat  torch randint 0  dim 0   size= nnz                                torch randint 0  dim 1   size= nnz      0  reshape 2  nnz          >>> v = torch randn nnz  dim 2   dim 3           >>> size = torch size dim          >>> s = torch sparse coo tensor i  v  size          >>> s         tensor indices=tensor   2  0  3                                   2  4  1                    values=tensor    -0 6438  -1 6467   1 4004                                    0 3411   0 0918  -0 2312                                      0 5348   0 0634  -2 0494                                   -0 7125  -1 0646   2 1844                                      0 1276   0 1874  -0 6334                                   -1 9682  -0 5340   0 7483                     size= 5  5  2  3   nnz=3  layout=torch sparse coo             when sum over only part of sparse dim  return a sparse tensor         >>> torch sparse sum s   1  3           tensor indices=tensor   0  2  3                    values=tensor   -1 4512   0 4073                                  -0 8901   0 2017                                  -0 3183  -1 7539                    size= 5  2   nnz=3  layout=torch sparse coo             when sum over all sparse dim  return a dense tensor           with sum dim squeeze         >>> torch sparse sum s   0  1  3           tensor  -2 6596  -1 1450               if dtype be none          if dim be not none              return torch  sparse sum input  dim          else              return torch  sparse sum input      else          if dim be not none              return torch  sparse sum input  dim  dtype=dtype          else              return torch  sparse sum input  dtype=dtype  
def addmm mat  tensor  mat1  tensor  mat2  tensor            beta  float = 1   alpha  float = 1   -> tensor      r        this function do exact same thing as  func `torch addmm` in the forward      except that it support backward for sparse matrix  attr `mat1`   attr `mat1`     need to have `sparse dim = 2`  note that the gradients of  attr `mat1` be a     coalesce sparse tensor       args          mat  tensor   a dense matrix to be add         mat1  tensor   a sparse matrix to be multiply         mat2  tensor   a dense matrix to be multiply         beta  number  optional   multiplier for  attr `mat`   math `\beta`          alpha  number  optional   multiplier for  math `mat1   mat2`   math `\alpha`              return torch  sparse addmm mat  mat1  mat2  beta=beta  alpha=alpha  
def mm mat1  tensor  mat2  tensor  -> tensor      r        perform a matrix multiplication of the sparse matrix  attr `mat1`     and the  sparse or stride  matrix  attr `mat2`  similar to  func `torch mm`  if  attr `mat1` be a      math ` n \times m ` tensor   attr `mat2` be a  math ` m \times p ` tensor  out will be a      math ` n \times p ` tensor   attr `mat1` need to have `sparse dim = 2`      this function also support backward for both matrices  note that the gradients of      attr `mat1` be a coalesce sparse tensor       args          mat1  sparsetensor   the first sparse matrix to be multiply         mat2  tensor   the second matrix to be multiply  which could be sparse or dense      shape          the format of the output tensor of this function follow          - sparse x sparse -> sparse         - sparse x dense -> dense      example            >>> a = torch randn 2  3  to sparse   require grad  true          >>> a         tensor indices=tensor   0  0  0  1  1  1                                   0  1  2  0  1  2                    values=tensor   1 5901   0 0183  -0 6146   1 8061  -0 0112   0 6302                   size= 2  3   nnz=6  layout=torch sparse coo  require grad=true           >>> b = torch randn 3  2  require grad=true          >>> b         tensor   -0 6479   0 7874                    -1 2056   0 5641                    -1 1716  -0 9923    require grad=true           >>> y = torch sparse mm a  b          >>> y         tensor   -0 3323   1 8723                    -1 8951   0 7904    grad fn=<sparseaddmmbackward>          >>> y sum   backward           >>> a grad         tensor indices=tensor   0  0  0  1  1  1                                   0  1  2  0  1  2                    values=tensor   0 1394  -0 6415  -2 1639   0 1394  -0 6415  -2 1639                   size= 2  3   nnz=6  layout=torch sparse coo              if mat1 be sparse and mat2 be sparse          return torch  sparse sparse matmul mat1  mat2      return torch  sparse mm mat1  mat2  
def softmax input  tensor  dim  int  dtype  optional dtype  = none  -> tensor      r   apply a softmax function       softmax be define as        math `\text softmax  x  i   = \frac exp x i   \sum j exp x j  `      where  math `i  j` run over sparse tensor indices and unspecified     entries be ignore  this be equivalent to define unspecified     entries as negative infinity so that  math `exp x k  = 0` when the     entry with index  math `k` have not specify       it be apply to all slice along `dim`  and will re-scale them so     that the elements lie in the range ` 0  1 ` and sum to 1       args          input  tensor   input         dim  int   a dimension along which softmax will be compute          dtype   class `torch dtype`  optional   the desire data type           of return tensor   if specify  the input tensor be           cast to  attr `dtype` before the operation be           perform  this be useful for prevent data type           overflow  default  none             return torch  sparse softmax input  dim  dtype=dtype  
def log softmax input  tensor  dim  int  dtype  optional dtype  = none  -> tensor      r   apply a softmax function follow by logarithm       see  class `~torch sparse softmax` for more detail       args          input  tensor   input         dim  int   a dimension along which softmax will be compute          dtype   class `torch dtype`  optional   the desire data type           of return tensor   if specify  the input tensor be           cast to  attr `dtype` before the operation be           perform  this be useful for prevent data type           overflow  default  none             return torch  sparse log softmax input  dim  dtype=dtype  