def clip self arg s k war gs convenience fluent method for py fun c clip the arguments be the same as for py fun c clip with this array as data return op clip self arg s k war gs
def depth to space self arg s k war gs convenience fluent method for py fun c depth to space the arguments be the same as for py fun c depth to space with this array as data return op depth to space self arg s k war gs
def bernoulli pro b log it size d type c tx out create a bernoulli distribution parameterized by at tr pro b or at tr log it but not both sample be binary 0 or 1 they take the value 1 with probability p and 0 with probability 1 p parameters pro b float nd array the probability of sample 1 only one of pro b or log it should be pass in log it float nd array the log odds of sample 1 only one of pro b or log it should be pass in size int or tuple of in ts optional output shape if the give shape be e g m n k then m n k sample be draw default be none in which case a single value be return d type d type optional desire d type of the result all d type be determine by their name i e int 64 int etc so byte order be not available and a specific precision may have different c type depend on the platform the default value be np float 32 c tx context optional device context of output default be current context out symbol optional the output symbol default be none return out nd array draw sample from the parameterized bernoulli distribution examples pro b np random uniform size 4 4 log it np log pro b np log 1 pro b n px random bernoulli log it log it array 0 1 1 1 0 1 1 1 0 1 0 0 1 0 1 0 n px random bernoulli pro b pro b array 0 1 0 1 1 1 1 1 1 1 1 0 1 0 1 0 from num py import nd array as np nd array tensor type name np nd array if pro b be none log it be none raise value error string string format pro b log it if d type be none d type string if c tx be none c tx current context if size size none if pro b be not none be tensor be instance pro b tensor type name if be tensor return np i bernoulli pro b pro b none log it none be log it false size size c tx c tx d type d type out out else return np i bernoulli pro b pro b log it none be log it false size size c tx c tx d type d type out out else be tensor be instance log it tensor type name if be tensor return np i bernoulli log it pro b none log it none be log it true size size c tx c tx d type d type out out else return np i bernoulli pro b none log it log it be log it true size size c tx c tx d type d type out out
def exponential scale number shape null d type null c tx none out none k war gs r draw sample from an exponential distribution its probability density function be math f x \frac 1 \beta \frac 1 \beta \exp \frac x \beta for x 0 and 0 elsewhere \beta be the scale parameter which be the inverse of the rate parameter \lambda 1/\beta parameters scale float or nd array optional the scale parameter \beta 1/\lambda shape int or tuple of in ts optional the number of sample to draw if shape be e g m n and scale be a scalar output shape will be m n if scale be an nd array with shape e g x y then output will have shape x y m n where m n sample be draw for each entry in scale d type float 16 float 32 float 64 optional data type of output sample default be float 32 c tx context optional device context of output default be current context override by scale context when scale be an nd array out nd array optional store output to an exist nd array return nd array if input shape have shape e g m n and scale be a scalar output shape will be m n if scale be an nd array with shape e g x y then output will have shape x y m n where m n sample be draw for each entry in scale examples mx nd random exponential 1 0 79587454 nd array 1 cpu 0 mx nd random exponential 1 shape 2 0 89856035 1 25593066 nd array 2 cpu 0 scale mx nd array 1 2 3 mx nd random exponential scale shape 2 0 41063145 0 42140478 2 59407091 10 12439728 2 42544937 1 14260709 nd array 3 x 2 cpu 0 return random helper internal random exponential internal sample exponential number/scale shape d type c tx out k war gs
def gamma alpha number beta number shape null d type null c tx none out none k war gs draw random sample from a gamma distribution sample be distribute accord to a gamma distribution para meet rize d by alpha shape and beta scale parameters alpha float or nd array optional the shape of the gamma distribution should be greater than zero beta float or nd array optional the scale of the gamma distribution should be greater than zero default be equal to 1 shape int or tuple of in ts optional the number of sample to draw if shape be e g m n and alpha and beta be scalars output shape will be m n if alpha and beta be nd array with shape e g x y then output will have shape x y m n where m n sample be draw for each alpha beta pair d type float 16 float 32 float 64 optional data type of output sample default be float 32 c tx context optional device context of output default be current context override by alpha context when alpha be an nd array out nd array optional store output to an exist nd array return nd array if input shape have shape e g m n and alpha and beta be scalars output shape will be m n if alpha and beta be nd array with shape e g x y then output will have shape x y m n where m n sample be draw for each alpha beta pair examples mx nd random gamma 1 1 1 93308783 nd array 1 cpu 0 mx nd random gamma 1 1 shape 2 0 48216391 2 09890771 nd array 2 cpu 0 alpha mx nd array 1 2 3 beta mx nd array 2 3 4 mx nd random gamma alpha beta shape 2 3 24343276 0 94137681 3 52734375 0 45568955 14 26264095 14 0170126 nd array 3 x 2 cpu 0 return random helper internal random gamma internal sample gamma alpha beta shape d type c tx out k war gs
def normal loc number scale number shape null d type null c tx none out none k war gs draw random sample from a normal gaussian distribution sample be distribute accord to a normal distribution para meet rize d by loc mean and scale standard deviation parameters loc float or nd array optional mean centre of the distribution scale float or nd array optional standard deviation spread or width of the distribution shape int or tuple of in ts optional the number of sample to draw if shape be e g m n and loc and scale be scalars output shape will be m n if loc and scale be nd array with shape e g x y then output will have shape x y m n where m n sample be draw for each loc scale pair d type float 16 float 32 float 64 optional data type of output sample default be float 32 c tx context optional device context of output default be current context override by loc context when loc be an nd array out nd array optional store output to an exist nd array return nd array an nd array of type d type if input shape have shape e g m n and loc and scale be scalars output shape will be m n if loc and scale be nd array with shape e g x y then output will have shape x y m n where m n sample be draw for each loc scale pair examples mx nd random normal 0 1 2 21220636 nd array 1 cpu 0 mx nd random normal 0 1 c tx mx gpu 0 0 29253659 nd array 1 gpu 0 mx nd random normal 1 1 shape 2 0 2259962 0 51619542 nd array 2 cpu 0 loc mx nd array 1 2 3 scale mx nd array 2 3 4 mx nd random normal loc scale shape 2 0 55912292 3 19566321 1 91728961 2 47706747 2 79666662 5 44254589 nd array 3 x 2 cpu 0 return random helper internal random normal internal sample normal loc scale shape d type c tx out k war gs
def uniform low number high number shape null d type null c tx none out none k war gs draw random sample from a uniform distribution sample be uniformly distribute over the half open interval low high include low but exclude high parameters low float or nd array optional lower boundary of the output interval all value generate will be greater than or equal to low the default value be 0 high float or nd array optional upper boundary of the output interval all value generate will be less than high the default value be 1 0 shape int or tuple of in ts optional the number of sample to draw if shape be e g m n and low and high be scalars output shape will be m n if low and high be nd array with shape e g x y then output will have shape x y m n where m n sample be draw for each low high pair d type float 16 float 32 float 64 optional data type of output sample default be float 32 c tx context optional device context of output default be current context override by low context when low be an nd array out nd array optional store output to an exist nd array return nd array an nd array of type d type if input shape have shape e g m n and low and high be scalars output shape will be m n if low and high be nd array with shape e g x y then the return nd array will have shape x y m n where m n uniformly distribute sample be draw for each low high pair examples mx nd random uniform 0 1 0 54881352 nd array 1 cpu 0 mx nd random uniform 0 1 c tx mx gpu 0 0 92514056 nd array 1 gpu 0 mx nd random uniform 1 1 shape 2 0 71589124 0 08976638 nd array 2 cpu 0 low mx nd array 1 2 3 high mx nd array 2 3 4 mx nd random uniform low high shape 2 1 78653979 1 93707538 2 01311183 2 37081361 3 30491424 3 69977832 nd array 3 x 2 cpu 0 return random helper internal random uniform internal sample uniform low high shape d type c tx out k war gs
class constant initializer initialize the weight to a give value the value pass in can be a scalar or a nd array that match the shape of the parameter to be set parameters value float nd array value to set def in it self value super constant self in it value value self value value def in it weight self arr arr self value def dump self val self k war gs string if not np be scalar val self k war gs string val to list if be instance val np nd array else val as num py to list return json dump self class name lower self k war gs
class xavier initializer return an initializer perform xavier initialization for weight this initializer be design to keep the scale of gradients roughly the same in all layer by default r nd type be uniform and factor type be avg the initializer fill the weight with random number in the range of math c c where math c \\sqrt \\frac 3 0 5 n in n out math n in be the number of neurons feed into weight and math n out be the number of neurons the result be feed to if r nd type be uniform and factor type be in the math c \\sqrt \\frac 3 n in similarly when factor type be out the math c \\sqrt \\frac 3 n out if r nd type be gaussian and factor type be avg the initializer fill the weight with number from normal distribution with a standard deviation of math \\sqrt \\frac 3 0 5 n in n out parameters r nd type str optional random generator type can be gaussian or uniform factor type str optional can be avg in or out magnitude float optional scale of random number def in it self r nd type string factor type string magnitude number super xavier self in it r nd type r nd type factor type factor type magnitude magnitude self r nd type r nd type self factor type factor type self magnitude float magnitude def in it weight self name arr shape arr shape h w scale number if len shape number raise value error string string format name if len shape number h w scale np prod shape number fan in fan out shape number h w scale shape number h w scale factor number if self factor type string factor fan in fan out / number el if self factor type string factor fan in el if self factor type string factor fan out else raise value error string scale np sqr t self magnitude / factor if self r nd type string uniform fn mx np random uniform if be np array else random uniform uniform fn scale scale arr shape out arr el if self r nd type string normal fn mx np random normal if be np array else random normal normal fn number scale arr shape out arr else raise value error string
class one initializer initialize weight to one example give module an instance of mx net module module initialize weight to one in it mx initializer one module in it params in it for dictionary in module get params for key in dictionary print key print dictionary key as num py fully connect 0 weight 1 1 1 def in it self super one self in it def in it weight self arr arr number
class orthogonal initializer initialize weight as orthogonal matrix this initializer implement exact solutions to the nonlinear dynamics of learn in deep linear neural network available at https //arxiv org/abs/1312 6120 parameters scale float optional scale factor of weight rand type string optional use uniform or normal random number to initialize weight def in it self scale number rand type string super orthogonal self in it scale scale rand type rand type self scale scale self rand type rand type def in it weight self arr n out arr shape number nin np prod arr shape number if self rand type string t mp random uniform number number shape n out nin as num py el if self rand type string t mp random normal number number shape n out nin as num py u v np lina lg sv d t mp full matrices false py lint disable invalid name if u shape t mp shape res u else res v res self scale res reshape arr shape arr res
class normal initializer initialize weight with random value sample from a normal distribution with a mean of zero and standard deviation of sigma parameters sigma float optional standard deviation of the normal distribution default standard deviation be 0 01 example give module an instance of mx net module module initialize weight to random value sample from a normal distribution in it mx in it normal 0 5 module in it params in it for dictionary in module get params for key in dictionary print key print dictionary key as num py fully connect 0 weight 0 3214761 0 12660924 0 53789419 def in it self sigma number super normal self in it sigma sigma self sigma sigma def in it weight self arr normal fn mx np random normal if be np array else random normal normal fn number self sigma arr shape out arr
class uniform initializer initialize weight with random value uniformly sample from a give range parameters scale float optional the bind on the range of the generate random value value be generate from the range scale scale default scale be 0 07 example give module an instance of mx net module module initialize weight to random value uniformly sample between 0 1 and 0 1 in it mx in it uniform 0 1 module in it params in it for dictionary in module get params for key in dictionary print key print dictionary key as num py fully connect 0 weight 0 01360891 0 02144304 0 08511933 def in it self scale number super uniform self in it scale scale self scale scale def in it weight self arr uniform fn mx np random uniform if be np array else random uniform uniform fn self scale self scale arr shape out arr
class zero initializer initialize weight to zero example give module an instance of mx net module module initialize weight to zero in it mx initializer zero module in it params in it for dictionary in module get params for key in dictionary print key print dictionary key as num py fully connect 0 weight 0 0 0 def in it self super zero self in it def in it weight self arr arr number
class batch norm batch norm batch normalization layer i off e and sze ged y 2014 normalize the input at each batch i e apply a transformation that maintain the mean activation close to 0 and the activation standard deviation close to 1 parameters axis int default 1 the axis that should be normalize this be typically the channel c axis for instance after a conv 2 d layer with layout nch w set axis 1 in batch norm if layout nh wc then set axis 3 momentum float default 0 9 momentum for the move average epsilon float default 1 e 5 small float add to variance to avoid divide by zero center boo l default true if true add offset of beta to normalize tensor if false beta be ignore scale boo l default true if true multiply by gamma if false gamma be not use when the next layer be linear also e g nn relu this can be disable since the scale will be do by the next layer use global stats boo l default false if true use global move statistics instead of local batch norm this will force change batch norm into a scale shift operator if false use local batch norm beta initializer str or initializer default zero initializer for the beta weight gamma initializer str or initializer default ones initializer for the gamma weight run mean initializer str or initializer default zero initializer for the run mean run variance initializer str or initializer default ones initializer for the run variance in channel int default 0 number of channel feature map in input data if not specify initialization will be defer to the first time forward be call and in channel will be infer from the shape of input data input data input tensor with arbitrary shape output out output tensor with the same shape as data def in it self axis number momentum number epsilon number center true scale true use global stats false beta initializer string gamma initializer string run mean initializer string run variance initializer string in channel number k war gs super batch norm self in it axis axis momentum momentum epsilon epsilon center center scale scale use global stats use global stats fuse relu false beta initializer beta initializer gamma initializer gamma initializer run mean initializer run mean initializer run variance initializer run variance initializer in channel in channel k war gs
class gru rnn layer r apply a multi layer gate recurrent unit gru rnn to an input sequence note this be an implementation of the cudnn version of grus slight modification compare to cho et al 2014 the reset gate math r t be apply after matrix multiplication for each element in the input sequence each layer compute the follow function math \begin array ll r t s igm oid w ir x t b ir w hr h t 1 b hr \\ i t s igm oid w ii x t b ii w hi h t 1 b hi \\ n t \tanh w in x t b in r t w hn h t 1 b hn \\ h t 1 i t n t i t h t 1 \\ \end array where math h t be the hide state at time t math x t be the hide state of the previous layer at time t or math input t for the first layer and math r t math i t math n t be the reset input and new gate respectively parameters hide size int the number of feature in the hide state h num layer int default 1 number of recurrent layer layout str default tnc the format of input and output tensors t n and c stand for sequence length batch size and feature dimension respectively dropout float default 0 if non zero introduce a dropout layer on the output of each rnn layer except the last layer bidirectional boo l default false if true become a bidirectional rnn i 2 h weight initializer str or initializer initializer for the input weight matrix use for the linear transformation of the input h 2 h weight initializer str or initializer initializer for the recurrent weight matrix use for the linear transformation of the recurrent state i 2 h bias initializer str or initializer initializer for the bias vector h 2 h bias initializer str or initializer initializer for the bias vector d type str default float 32 type to initialize the parameters and default state to input size int default 0 the number of expect feature in the input x if not specify it will be infer from input prefix str or none prefix of this block params parameter dic t or none share parameters for this block input data input tensor with shape sequence length batch size input size when layout be tnc for other layouts dimension be permute accordingly use transpose operator which add performance overhead consider create batch in tnc layout during data batch step state initial recurrent state tensor with shape num layer batch size num hide if bidirectional be true shape will instead be 2 num layer batch size num hide if state be none zero will be use as default begin state output out output tensor with shape sequence length batch size num hide when layout be tnc if bidirectional be true output shape will instead be sequence length batch size 2 num hide out state output recurrent state tensor with the same shape as state if state be none out state will not be return examples layer mx gluon rnn gru 100 3 layer initialize input mx nd random uniform shape 5 3 10 by default zero be use as begin state output layer input manually specify begin state h 0 mx nd random uniform shape 3 3 100 output hn layer input h 0 def in it self hide size num layer number layout string dropout number bidirectional false input size number i 2 h weight initializer none h 2 h weight initializer none i 2 h bias initializer string h 2 h bias initializer string d type string k war gs super gru self in it hide size num layer layout dropout bidirectional input size i 2 h weight initializer h 2 h weight initializer i 2 h bias initializer h 2 h bias initializer string none none none none false d type k war gs def state info self batch size number return string self num layer self dir batch size self hide size string string string self d type
class rnn cell hybrid recurrent cell r el man rnn recurrent neural network cell each call compute the follow function math h t \tanh w ih x t b ih w hh h t 1 b hh where math h t be the hide state at time t and math x t be the hide state of the previous layer at time t or math input t for the first layer if non linearity relu then relu be use instead of tan h parameters hide size int number of units in output symbol activation str or symbol default tan h type of activation function i 2 h weight initializer str or initializer initializer for the input weight matrix use for the linear transformation of the input h 2 h weight initializer str or initializer initializer for the recurrent weight matrix use for the linear transformation of the recurrent state i 2 h bias initializer str or initializer default zero initializer for the bias vector h 2 h bias initializer str or initializer default zero initializer for the bias vector prefix str default rnn prefix for name of block s and name of weight if params be none params parameter or none container for weight share between cells create if none input data input tensor with shape batch size input size state a list of one initial recurrent state tensor with shape batch size num hide output out output tensor with shape batch size num hide next state a list of one output recurrent state tensor with the same shape as state def in it self hide size activation string i 2 h weight initializer none h 2 h weight initializer none i 2 h bias initializer string h 2 h bias initializer string input size number prefix none params none super rnn cell self in it prefix prefix params params self hide size hide size self activation activation self input size input size self i 2 h weight self params get string shape hide size input size in it i 2 h weight initializer allow defer in it true self h 2 h weight self params get string shape hide size hide size in it h 2 h weight initializer allow defer in it true self i 2 h bias self params get string shape hide size in it i 2 h bias initializer allow defer in it true self h 2 h bias self params get string shape hide size in it h 2 h bias initializer allow defer in it true def state info self batch size number return string batch size self hide size string string def alias self return string def re pr self s string if have at tr self string s string s string shape self i 2 h weight shape map string format shape number if shape number else none shape number return s format name self class name map map self dic t def hybrid forward self f input state i 2 h weight h 2 h weight i 2 h bias h 2 h bias prefix string self counter i 2 h f fully connect data input weight i 2 h weight bias i 2 h bias num hide self hide size name prefix string h 2 h f fully connect data state number weight h 2 h weight bias h 2 h bias num hide self hide size name prefix string i 2 h plus h 2 h f el em wise add i 2 h h 2 h name prefix string output self get activation f i 2 h plus h 2 h self activation name prefix string return output output
class lstm rnn layer r apply a multi layer long short term memory lstm rnn to an input sequence for each element in the input sequence each layer compute the follow function math \begin array ll i t s igm oid w ii x t b ii w hi h t 1 b hi \\ f t s igm oid w if x t b if w hf h t 1 b hf \\ g t \tanh w ig x t b ig w hc h t 1 b hg \\ o t s igm oid w io x t b io w ho h t 1 b ho \\ c t f t c t 1 i t g t \\ h t o t \tanh c t \end array where math h t be the hide state at time t math c t be the cell state at time t math x t be the hide state of the previous layer at time t or math input t for the first layer and math i t math f t math g t math o t be the input forget cell and out gate respectively parameters hide size int the number of feature in the hide state h num layer int default 1 number of recurrent layer layout str default tnc the format of input and output tensors t n and c stand for sequence length batch size and feature dimension respectively dropout float default 0 if non zero introduce a dropout layer on the output of each rnn layer except the last layer bidirectional boo l default false if true become a bidirectional rnn i 2 h weight initializer str or initializer initializer for the input weight matrix use for the linear transformation of the input h 2 h weight initializer str or initializer initializer for the recurrent weight matrix use for the linear transformation of the recurrent state i 2 h bias initializer str or initializer default lstm bias initializer for the bias vector by default bias for the forget gate be initialize to 1 while all other bias be initialize to zero h 2 h bias initializer str or initializer initializer for the bias vector projection size int default none the number of feature after projection h 2 r weight initializer str or initializer default none initializer for the project recurrent weight matrix use for the linear transformation of the recurrent state to the project space state clip min float or none default none minimum clip value of lstm state this option must be use together with state clip max if none clip be not apply state clip max float or none default none maximum clip value of lstm state this option must be use together with state clip min if none clip be not apply state clip nan boolean default false whether to stop nan from propagate in state by clip it to min/max if the clip range be not specify this option be ignore d type str default float 32 type to initialize the parameters and default state to input size int default 0 the number of expect feature in the input x if not specify it will be infer from input prefix str or none prefix of this block params parameter dic t or none share parameters for this block input data input tensor with shape sequence length batch size input size when layout be tnc for other layouts dimension be permute accordingly use transpose operator which add performance overhead consider create batch in tnc layout during data batch step state a list of two initial recurrent state tensors each have shape num layer batch size num hide if bidirectional be true shape will instead be 2 num layer batch size num hide if state be none zero will be use as default begin state output out output tensor with shape sequence length batch size num hide when layout be tnc if bidirectional be true output shape will instead be sequence length batch size 2 num hide out state a list of two output recurrent state tensors with the same shape as in state if state be none out state will not be return examples layer mx gluon rnn lstm 100 3 layer initialize input mx nd random uniform shape 5 3 10 by default zero be use as begin state output layer input manually specify begin state h 0 mx nd random uniform shape 3 3 100 c 0 mx nd random uniform shape 3 3 100 output hn layer input h 0 c 0 def in it self hide size num layer number layout string dropout number bidirectional false input size number i 2 h weight initializer none h 2 h weight initializer none i 2 h bias initializer string h 2 h bias initializer string projection size none h 2 r weight initializer none state clip min none state clip max none state clip nan false d type string k war gs super lstm self in it hide size num layer layout dropout bidirectional input size i 2 h weight initializer h 2 h weight initializer i 2 h bias initializer h 2 h bias initializer string projection size h 2 r weight initializer state clip min state clip max state clip nan d type k war gs def state info self batch size number if self projection size be none return string self num layer self dir batch size self hide size string string string self d type string self num layer self dir batch size self hide size string string string self d type else return string self num layer self dir batch size self projection size string string string self d type string self num layer self dir batch size self hide size string string string self d type
class lstm cell hybrid recurrent cell r long short term memory lstm network cell each call compute the follow function math \begin array ll i t s igm oid w ii x t b ii w hi h t 1 b hi \\ f t s igm oid w if x t b if w hf h t 1 b hf \\ g t \tanh w ig x t b ig w hc h t 1 b hg \\ o t s igm oid w io x t b io w ho h t 1 b ho \\ c t f t c t 1 i t g t \\ h t o t \tanh c t \end array where math h t be the hide state at time t math c t be the cell state at time t math x t be the hide state of the previous layer at time t or math input t for the first layer and math i t math f t math g t math o t be the input forget cell and out gate respectively parameters hide size int number of units in output symbol i 2 h weight initializer str or initializer initializer for the input weight matrix use for the linear transformation of the input h 2 h weight initializer str or initializer initializer for the recurrent weight matrix use for the linear transformation of the recurrent state i 2 h bias initializer str or initializer default zero initializer for the bias vector h 2 h bias initializer str or initializer default zero initializer for the bias vector prefix str default lstm prefix for name of block s and name of weight if params be none params parameter or none default none container for weight share between cells create if none activation str default tan h activation type to use see nd/symbol activation for support type recurrent activation str default s igm oid activation type to use for the recurrent step see nd/symbol activation for support type input data input tensor with shape batch size input size state a list of two initial recurrent state tensors each have shape batch size num hide output out output tensor with shape batch size num hide next state a list of two output recurrent state tensors each have the same shape as state py lint disable too many instance attribute def in it self hide size i 2 h weight initializer none h 2 h weight initializer none i 2 h bias initializer string h 2 h bias initializer string input size number prefix none params none activation string recurrent activation string super lstm cell self in it prefix prefix params params self hide size hide size self input size input size self i 2 h weight self params get string shape number hide size input size in it i 2 h weight initializer allow defer in it true self h 2 h weight self params get string shape number hide size hide size in it h 2 h weight initializer allow defer in it true self i 2 h bias self params get string shape number hide size in it i 2 h bias initializer allow defer in it true self h 2 h bias self params get string shape number hide size in it h 2 h bias initializer allow defer in it true self activation activation self recurrent activation recurrent activation def state info self batch size number return string batch size self hide size string string string batch size self hide size string string def alias self return string def re pr self s string shape self i 2 h weight shape map string format shape number if shape number else none shape number return s format name self class name map map self dic t def hybrid forward self f input state i 2 h weight h 2 h weight i 2 h bias h 2 h bias py lint disable too many locals prefix string self counter i 2 h f fully connect data input weight i 2 h weight bias i 2 h bias num hide self hide size number name prefix string h 2 h f fully connect data state number weight h 2 h weight bias h 2 h bias num hide self hide size number name prefix string gate f el em wise add i 2 h h 2 h name prefix string slice gate f slice channel gate num output number name prefix string in gate self get activation f slice gate number self recurrent activation name prefix string forget gate self get activation f slice gate number self recurrent activation name prefix string in transform self get activation f slice gate number self activation name prefix string out gate self get activation f slice gate number self recurrent activation name prefix string next c f el em wise add f el em wise mul forget gate state number name prefix string f el em wise mul in gate in transform name prefix string name prefix string next h f el em wise mul out gate f activation next c act type self activation name prefix string name prefix string return next h next h next c
class avg pool 1 d pool average pool operation for temporal data parameters pool size int size of the average pool windows stride int or none factor by which to downscale e g 2 will halve the input size if none it will default to pool size pad int if pad be non zero then the input be implicitly zero pad on both side for pad number of point layout str default n cw dimension order of data and out n cw or nw c n c w stand for batch channel and width time dimension respectively pad be apply on w dimension ce il mode boo l default false when true will use ce il instead of floor to compute the output shape count include pad boo l default true when false will exclude pad elements when compute the average value input data 3 d input tensor with shape batch size in channel width when layout be n cw for other layouts shape be permute accordingly output out 3 d output tensor with shape batch size channel out width when layout be n cw out width be calculate as out width floor width 2 pad pool size /strides 1 when ce il mode be true ce il will be use instead of floor in this equation def in it self pool size number stride none pad number layout string ce il mode false count include pad true k war gs assert layout in string string \ string if be instance pool size numeric type pool size pool size assert len pool size number string super avg pool 1 d self in it pool size stride pad ce il mode false string layout count include pad k war gs
class avg pool 2 d pool average pool operation for spatial data parameters pool size int or list/tuple of 2 in ts size of the average pool windows stride int list/tuple of 2 in ts or none factor by which to downscale e g 2 will halve the input size if none it will default to pool size pad int or list/tuple of 2 in ts if pad be non zero then the input be implicitly zero pad on both side for pad number of point layout str default nch w dimension order of data and out nch w or nh wc n c h w stand for batch channel height and width dimension respectively pad be apply on h and w dimension ce il mode boo l default false when true will use ce il instead of floor to compute the output shape count include pad boo l default true when false will exclude pad elements when compute the average value input data 4 d input tensor with shape batch size in channel height width when layout be nch w for other layouts shape be permute accordingly output out 4 d output tensor with shape batch size channel out height out width when layout be nch w out height and out width be calculate as out height floor height 2 pad 0 pool size 0 /strides 0 1 out width floor width 2 pad 1 pool size 1 /strides 1 1 when ce il mode be true ce il will be use instead of floor in this equation def in it self pool size number number stride none pad number ce il mode false layout string count include pad true k war gs assert layout in string string \ string if be instance pool size numeric type pool size pool size number assert len pool size number string super avg pool 2 d self in it pool size stride pad ce il mode false string layout count include pad k war gs
class avg pool 3 d pool average pool operation for 3 d data spatial or s patio temporal parameters pool size int or list/tuple of 3 in ts size of the average pool windows stride int list/tuple of 3 in ts or none factor by which to downscale e g 2 will halve the input size if none it will default to pool size pad int or list/tuple of 3 in ts if pad be non zero then the input be implicitly zero pad on both side for pad number of point layout str default ncd h w dimension order of data and out ncd h w or ndh wc n c h w d stand for batch channel height width and depth dimension respectively pad be apply on d h and w dimension ce il mode boo l default false when true will use ce il instead of floor to compute the output shape count include pad boo l default true when false will exclude pad elements when compute the average value input data 5 d input tensor with shape batch size in channel depth height width when layout be ncd h w for other layouts shape be permute accordingly output out 5 d output tensor with shape batch size channel out depth out height out width when layout be ncd h w out depth out height and out width be calculate as out depth floor depth 2 pad 0 pool size 0 /strides 0 1 out height floor height 2 pad 1 pool size 1 /strides 1 1 out width floor width 2 pad 2 pool size 2 /strides 2 1 when ce il mode be true ce il will be use instead of floor in this equation def in it self pool size number number number stride none pad number ce il mode false layout string count include pad true k war gs assert layout in string string \ string if be instance pool size numeric type pool size pool size number assert len pool size number string super avg pool 3 d self in it pool size stride pad ce il mode false string layout count include pad k war gs
class conv 1 d conv r 1 d convolution layer e g temporal convolution this layer create a convolution kernel that be conv ol ved with the layer input over a single spatial or temporal dimension to produce a tensor of output if use bias be true a bias vector be create and add to the output finally if activation be not none it be apply to the output as well if in channel be not specify parameter initialization will be defer to the first time forward be call and in channel will be infer from the shape of input data parameters channel int the dimensionality of the output space i e the number of output channel filter in the convolution kernel size int or tuple/list of 1 int specify the dimension of the convolution window stride int or tuple/list of 1 int specify the stride of the convolution pad int or a tuple/list of 1 int if pad be non zero then the input be implicitly zero pad on both side for pad number of point dilation int or tuple/list of 1 int specify the dilation rate to use for dilate convolution group int control the connections between input and output at group 1 all input be conv ol ved to all output at group 2 the operation become equivalent to have two conv layer side by side each see half the input channel and produce half the output channel and both subsequently concatenate layout str default n cw dimension order of data and weight only support n cw layout for now n c w stand for batch channel and width time dimension respectively convolution be apply on the w dimension in channel int default 0 the number of input channel to this layer if not specify initialization will be defer to the first time forward be call and in channel will be infer from the shape of input data activation str activation function to use see fun c ~mxnet nd array activation if you don t specify anything no activation be apply ie linear activation a x x use bias boo l whether the layer use a bias vector weight initializer str or initializer initializer for the weight weight matrix bias initializer str or initializer initializer for the bias vector input data 3 d input tensor with shape batch size in channel width when layout be n cw for other layouts shape be permute accordingly output out 3 d output tensor with shape batch size channel out width when layout be n cw out width be calculate as out width floor width 2 pad dilation kernel size 1 1 /stride 1 def in it self channel kernel size stride number pad number dilation number group number layout string activation none use bias true weight initializer none bias initializer string in channel number k war gs assert layout string string if be instance kernel size numeric type kernel size kernel size assert len kernel size number string op name k war gs pop string string if be np array op name string super conv 1 d self in it channel kernel size stride pad dilation group layout in channel activation use bias weight initializer bias initializer op name k war gs
class conv 2 d conv r 2 d convolution layer e g spatial convolution over image this layer create a convolution kernel that be conv ol ved with the layer input to produce a tensor of output if use bias be true a bias vector be create and add to the output finally if activation be not none it be apply to the output as well if in channel be not specify parameter initialization will be defer to the first time forward be call and in channel will be infer from the shape of input data parameters channel int the dimensionality of the output space i e the number of output channel filter in the convolution kernel size int or tuple/list of 2 int specify the dimension of the convolution window stride int or tuple/list of 2 int specify the stride of the convolution pad int or a tuple/list of 2 int if pad be non zero then the input be implicitly zero pad on both side for pad number of point dilation int or tuple/list of 2 int specify the dilation rate to use for dilate convolution group int control the connections between input and output at group 1 all input be conv ol ved to all output at group 2 the operation become equivalent to have two conv layer side by side each see half the input channel and produce half the output channel and both subsequently concatenate layout str default nch w dimension order of data and weight only support nch w and nh wc layout for now n c h w stand for batch channel height and width dimension respectively convolution be apply on the h and w dimension in channel int default 0 the number of input channel to this layer if not specify initialization will be defer to the first time forward be call and in channel will be infer from the shape of input data activation str activation function to use see fun c ~mxnet nd array activation if you don t specify anything no activation be apply ie linear activation a x x use bias boo l whether the layer use a bias vector weight initializer str or initializer initializer for the weight weight matrix bias initializer str or initializer initializer for the bias vector input data 4 d input tensor with shape batch size in channel height width when layout be nch w for other layouts shape be permute accordingly output out 4 d output tensor with shape batch size channel out height out width when layout be nch w out height and out width be calculate as out height floor height 2 pad 0 dilation 0 kernel size 0 1 1 /stride 0 1 out width floor width 2 pad 1 dilation 1 kernel size 1 1 1 /stride 1 1 def in it self channel kernel size stride number number pad number number dilation number number group number layout string activation none use bias true weight initializer none bias initializer string in channel number k war gs assert layout in string string string if be instance kernel size numeric type kernel size kernel size number assert len kernel size number string op name k war gs pop string string if be np array op name string super conv 2 d self in it channel kernel size stride pad dilation group layout in channel activation use bias weight initializer bias initializer op name k war gs
class conv 2 d transpose conv transpose 2 d convolution layer sometimes call de convolution the need for transpose convolutions generally arise from the desire to use a transformation go in the opposite direction of a normal convolution i e from something that have the shape of the output of some convolution to something that have the shape of its input while maintain a connectivity pattern that be compatible with say convolution if in channel be not specify parameter initialization will be defer to the first time forward be call and in channel will be infer from the shape of input data parameters channel int the dimensionality of the output space i e the number of output channel filter in the convolution kernel size int or tuple/list of 2 int specify the dimension of the convolution window stride int or tuple/list of 2 int specify the stride of the convolution pad int or a tuple/list of 2 int if pad be non zero then the input be implicitly zero pad on both side for pad number of point output pad int or a tuple/list of 2 int control the amount of implicit zero pad s on both side of the output for output pad number of point for each dimension dilation int or tuple/list of 2 int control the space between the kernel point also know as the a trou s algorithm group int control the connections between input and output at group 1 all input be conv ol ved to all output at group 2 the operation become equivalent to have two conv layer side by side each see half the input channel and produce half the output channel and both subsequently concatenate layout str default nch w dimension order of data and weight only support nch w and nh wc layout for now n c h w stand for batch channel height and width dimension respectively convolution be apply on the h and w dimension in channel int default 0 the number of input channel to this layer if not specify initialization will be defer to the first time forward be call and in channel will be infer from the shape of input data activation str activation function to use see fun c ~mxnet nd array activation if you don t specify anything no activation be apply ie linear activation a x x use bias boo l whether the layer use a bias vector weight initializer str or initializer initializer for the weight weight matrix bias initializer str or initializer initializer for the bias vector input data 4 d input tensor with shape batch size in channel height width when layout be nch w for other layouts shape be permute accordingly output out 4 d output tensor with shape batch size channel out height out width when layout be nch w out height and out width be calculate as out height height 1 stride 0 2 pad 0 kernel size 0 output pad 0 out width width 1 stride 1 2 pad 1 kernel size 1 output pad 1 def in it self channel kernel size stride number number pad number number output pad number number dilation number number group number layout string activation none use bias true weight initializer none bias initializer string in channel number k war gs assert layout in string string string if be instance kernel size numeric type kernel size kernel size number if be instance output pad numeric type output pad output pad number assert len kernel size number string assert len output pad number string op name k war gs pop string string if be np array op name string super conv 2 d transpose self in it channel kernel size stride pad dilation group layout in channel activation use bias weight initializer bias initializer op name op name adj output pad k war gs self out pad output pad
class conv 3 d conv 3 d convolution layer e g spatial convolution over volumes this layer create a convolution kernel that be conv ol ved with the layer input to produce a tensor of output if use bias be true a bias vector be create and add to the output finally if activation be not none it be apply to the output as well if in channel be not specify parameter initialization will be defer to the first time forward be call and in channel will be infer from the shape of input data parameters channel int the dimensionality of the output space i e the number of output channel filter in the convolution kernel size int or tuple/list of 3 int specify the dimension of the convolution window stride int or tuple/list of 3 int specify the stride of the convolution pad int or a tuple/list of 3 int if pad be non zero then the input be implicitly zero pad on both side for pad number of point dilation int or tuple/list of 3 int specify the dilation rate to use for dilate convolution group int control the connections between input and output at group 1 all input be conv ol ved to all output at group 2 the operation become equivalent to have two conv layer side by side each see half the input channel and produce half the output channel and both subsequently concatenate layout str default ncd h w dimension order of data and weight only support ncd h w and ndh wc layout for now n c h w d stand for batch channel height width and depth dimension respectively convolution be apply on the d h and w dimension in channel int default 0 the number of input channel to this layer if not specify initialization will be defer to the first time forward be call and in channel will be infer from the shape of input data activation str activation function to use see fun c ~mxnet nd array activation if you don t specify anything no activation be apply ie linear activation a x x use bias boo l whether the layer use a bias vector weight initializer str or initializer initializer for the weight weight matrix bias initializer str or initializer initializer for the bias vector input data 5 d input tensor with shape batch size in channel depth height width when layout be ncd h w for other layouts shape be permute accordingly output out 5 d output tensor with shape batch size channel out depth out height out width when layout be ncd h w out depth out height and out width be calculate as out depth floor depth 2 pad 0 dilation 0 kernel size 0 1 1 /stride 0 1 out height floor height 2 pad 1 dilation 1 kernel size 1 1 1 /stride 1 1 out width floor width 2 pad 2 dilation 2 kernel size 2 1 1 /stride 2 1 def in it self channel kernel size stride number number number pad number number number dilation number number number group number layout string activation none use bias true weight initializer none bias initializer string in channel number k war gs assert layout in string string string if be instance kernel size numeric type kernel size kernel size number assert len kernel size number string op name k war gs pop string string if be np array op name string super conv 3 d self in it channel kernel size stride pad dilation group layout in channel activation use bias weight initializer bias initializer op name k war gs
class conv 3 d transpose conv transpose 3 d convolution layer sometimes call de convolution the need for transpose convolutions generally arise from the desire to use a transformation go in the opposite direction of a normal convolution i e from something that have the shape of the output of some convolution to something that have the shape of its input while maintain a connectivity pattern that be compatible with say convolution if in channel be not specify parameter initialization will be defer to the first time forward be call and in channel will be infer from the shape of input data parameters channel int the dimensionality of the output space i e the number of output channel filter in the convolution kernel size int or tuple/list of 3 int specify the dimension of the convolution window stride int or tuple/list of 3 int specify the stride of the convolution pad int or a tuple/list of 3 int if pad be non zero then the input be implicitly zero pad on both side for pad number of point output pad int or a tuple/list of 3 int control the amount of implicit zero pad s on both side of the output for output pad number of point for each dimension dilation int or tuple/list of 3 int control the space between the kernel point also know as the a trou s algorithm group int control the connections between input and output at group 1 all input be conv ol ved to all output at group 2 the operation become equivalent to have two conv layer side by side each see half the input channel and produce half the output channel and both subsequently concatenate layout str default ncd h w dimension order of data and weight only support ncd h w and ndh wc layout for now n c h w d stand for batch channel height width and depth dimension respectively convolution be apply on the d h and w dimension in channel int default 0 the number of input channel to this layer if not specify initialization will be defer to the first time forward be call and in channel will be infer from the shape of input data activation str activation function to use see fun c ~mxnet nd array activation if you don t specify anything no activation be apply ie linear activation a x x use bias boo l whether the layer use a bias vector weight initializer str or initializer initializer for the weight weight matrix bias initializer str or initializer initializer for the bias vector input data 5 d input tensor with shape batch size in channel depth height width when layout be ncd h w for other layouts shape be permute accordingly output out 5 d output tensor with shape batch size channel out depth out height out width when layout be ncd h w out depth out height and out width be calculate as out depth depth 1 stride 0 2 pad 0 kernel size 0 output pad 0 out height height 1 stride 1 2 pad 1 kernel size 1 output pad 1 out width width 1 stride 2 2 pad 2 kernel size 2 output pad 2 def in it self channel kernel size stride number number number pad number number number output pad number number number dilation number number number group number layout string activation none use bias true weight initializer none bias initializer string in channel number k war gs assert layout in string string string if be instance kernel size numeric type kernel size kernel size number if be instance output pad numeric type output pad output pad number assert len kernel size number string assert len output pad number string op name k war gs pop string string if be np array op name string super conv 3 d transpose self in it channel kernel size stride pad dilation group layout in channel activation use bias weight initializer bias initializer op name op name adj output pad k war gs self out pad output pad
class dense hybrid block r just your regular densely connect nn layer dense implement the operation output activation dot input weight bias where activation be the element wise activation function pass as the activation argument weight be a weight matrix create by the layer and bias be a bias vector create by the layer only applicable if use bias be true note the input must be a tensor with rank 2 use flatten to convert it to rank 2 manually if necessary parameters units int dimensionality of the output space activation str activation function to use see help on activation layer if you don t specify anything no activation be apply ie linear activation a x x use bias boo l default true whether the layer use a bias vector flatten boo l default true whether the input tensor should be flatten if true all but the first axis of input data be collapse together if false all but the last axis of input data be keep the same and the transformation apply on the last axis d type str or np d type default float 32 data type of output embed s weight initializer str or initializer initializer for the kernel weight matrix bias initializer str or initializer initializer for the bias vector in units int optional size of the input data if not specify initialization will be defer to the first time forward be call and in units will be infer from the shape of input data prefix str or none see document of block params parameter dic t or none see document of block input data if flatten be true data should be a tensor with shape batch size x 1 x 2 xn where x 1 x 2 xn be equal to in units if flatten be false data should have shape x 1 x 2 xn in units output out if flatten be true out will be a tensor with shape batch size units if flatten be false out will have shape x 1 x 2 xn units def in it self units activation none use bias true flatten true d type string weight initializer none bias initializer string in units number k war gs super dense self in it k war gs self flatten flatten with self name scope self units units self in units in units self weight self params get string shape units in units in it weight initializer d type d type allow defer in it true if use bias self bias self params get string shape units in it bias initializer d type d type allow defer in it true else self bias none if activation be not none self act activation activation prefix activation string else self act none def hybrid forward self f x weight bias none fc f n px fully connect if be np array else f fully connect act fc x weight bias no bias bias be none num hide self units flatten self flatten name string if self act be not none act self act act return act def re pr self s string shape self weight shape return s format name self class name act self act if self act else string layout string format shape number if shape number else none shape number
class dropout hybrid block apply dropout to the input dropout consist in randomly set a fraction rate of input units to 0 at each update during train time which help prevent over fit parameters rate float fraction of the input units to drop must be a number between 0 and 1 ax tuple of int default the ax on which dropout mask be share if empty regular dropout be apply input data input tensor with arbitrary shape output out output tensor with the same shape as data reference dropout a simple way to prevent neural network from over fit http //www cs toronto edu/~rsalakhu/papers/srivastava14a pdf def in it self rate ax k war gs super dropout self in it k war gs self rate rate self ax ax def hybrid forward self f x if self rate number dropout f n px dropout if be np array else f dropout return dropout x p self rate ax self ax name string cudnn off false else copy f np copy if be np array else f identity return copy x def re pr self s string return s format name self class name self dic t
class flatten hybrid block r flatten the input to two dimensional input data input tensor with arbitrary shape n x 1 x 2 xn output out 2 d tensor with shape n x 1 \cdot x 2 \cdot \cdot xn def in it self k war gs super flatten self in it k war gs def hybrid forward self f x flatten f n px batch flatten if be np array else f flatten return flatten x def re pr self return self class name
class max pool 1 d pool max pool operation for one dimensional data parameters pool size int size of the max pool windows stride int or none factor by which to downscale e g 2 will halve the input size if none it will default to pool size pad int if pad be non zero then the input be implicitly zero pad on both side for pad number of point layout str default n cw dimension order of data and out n cw or nw c n c w stand for batch channel and width time dimension respectively pool be apply on the w dimension ce il mode boo l default false when true will use ce il instead of floor to compute the output shape input data 3 d input tensor with shape batch size in channel width when layout be n cw for other layouts shape be permute accordingly output out 3 d output tensor with shape batch size channel out width when layout be n cw out width be calculate as out width floor width 2 pad pool size /strides 1 when ce il mode be true ce il will be use instead of floor in this equation def in it self pool size number stride none pad number layout string ce il mode false k war gs assert layout in string string \ string if be instance pool size numeric type pool size pool size assert len pool size number string super max pool 1 d self in it pool size stride pad ce il mode false string layout k war gs
class max pool 2 d pool max pool operation for two dimensional spatial data parameters pool size int or list/tuple of 2 in ts size of the max pool windows stride int list/tuple of 2 in ts or none factor by which to downscale e g 2 will halve the input size if none it will default to pool size pad int or list/tuple of 2 in ts if pad be non zero then the input be implicitly zero pad on both side for pad number of point layout str default nch w dimension order of data and out nch w or nh wc n c h w stand for batch channel height and width dimension respectively pad be apply on h and w dimension ce il mode boo l default false when true will use ce il instead of floor to compute the output shape input data 4 d input tensor with shape batch size in channel height width when layout be nch w for other layouts shape be permute accordingly output out 4 d output tensor with shape batch size channel out height out width when layout be nch w out height and out width be calculate as out height floor height 2 pad 0 pool size 0 /strides 0 1 out width floor width 2 pad 1 pool size 1 /strides 1 1 when ce il mode be true ce il will be use instead of floor in this equation def in it self pool size number number stride none pad number layout string ce il mode false k war gs assert layout in string string \ string if be instance pool size numeric type pool size pool size number assert len pool size number string super max pool 2 d self in it pool size stride pad ce il mode false string layout k war gs
class max pool 3 d pool max pool operation for 3 d data spatial or s patio temporal parameters pool size int or list/tuple of 3 in ts size of the max pool windows stride int list/tuple of 3 in ts or none factor by which to downscale e g 2 will halve the input size if none it will default to pool size pad int or list/tuple of 3 in ts if pad be non zero then the input be implicitly zero pad on both side for pad number of point layout str default ncd h w dimension order of data and out ncd h w or ndh wc n c h w d stand for batch channel height width and depth dimension respectively pad be apply on d h and w dimension ce il mode boo l default false when true will use ce il instead of floor to compute the output shape input data 5 d input tensor with shape batch size in channel depth height width when layout be n cw for other layouts shape be permute accordingly output out 5 d output tensor with shape batch size channel out depth out height out width when layout be ncd h w out depth out height and out width be calculate as out depth floor depth 2 pad 0 pool size 0 /strides 0 1 out height floor height 2 pad 1 pool size 1 /strides 1 1 out width floor width 2 pad 2 pool size 2 /strides 2 1 when ce il mode be true ce il will be use instead of floor in this equation def in it self pool size number number number stride none pad number ce il mode false layout string k war gs assert layout in string string \ string if be instance pool size numeric type pool size pool size number assert len pool size number string super max pool 3 d self in it pool size stride pad ce il mode false string layout k war gs
class hinge loss loss r calculate the hinge loss function often use in s vms math l \sum i max 0 margin p red i \cdot label i where p red be the class i fier prediction and label be the target tensor contain value 1 or 1 label and p red must have the same number of elements parameters margin float the margin in hinge loss default to 1 0 weight float or none global scalar weight for loss batch axis int default 0 the axis that represent mini batch input p red prediction tensor with arbitrary shape label truth tensor with value 1 or 1 must have the same size as p red sample weight element wise weight tensor must be broadcast able to the same shape as p red for example if p red have shape 64 10 and you want to weigh each sample in the batch separately sample weight should have shape 64 1 output loss loss tensor with shape batch size di men ions other than batch axis be average out def in it self margin number weight none batch axis number k war gs super hinge loss self in it weight batch axis k war gs self margin margin def hybrid forward self f p red label sample weight none label reshape like f label p red loss f relu self margin p red label loss apply weight f loss self weight sample weight return f mean loss axis self batch axis exclude true
class huber loss loss r calculate smooth l 1 loss that be equal to l 1 loss if absolute error exceed rho but be equal to l 2 loss otherwise also call smooth l 1 loss math l \sum i \begin case \frac 1 2 rho label i p red i 2 \text if label i p red i rho \\ label i p red i \frac rho 2 \text otherwise \end case label and p red can have arbitrary shape as long as they have the same number of elements parameters rho float default 1 threshold for trim mean estimator weight float or none global scalar weight for loss batch axis int default 0 the axis that represent mini batch input p red prediction tensor with arbitrary shape label target tensor with the same size as p red sample weight element wise weight tensor must be broadcast able to the same shape as p red for example if p red have shape 64 10 and you want to weigh each sample in the batch separately sample weight should have shape 64 1 output loss loss tensor with shape batch size di men ions other than batch axis be average out def in it self rho number weight none batch axis number k war gs super huber loss self in it weight batch axis k war gs self rho rho def hybrid forward self f p red label sample weight none label reshape like f label p red loss f abs label p red loss f where loss self rho loss number self rho number / self rho f square loss loss apply weight f loss self weight sample weight return f mean loss axis self batch axis exclude true
class l 2 loss loss r calculate the mean square error between label and p red math l \frac 1 2 \sum i \vert label i p red i \vert 2 label and p red can have arbitrary shape as long as they have the same number of elements parameters weight float or none global scalar weight for loss batch axis int default 0 the axis that represent mini batch input p red prediction tensor with arbitrary shape label target tensor with the same size as p red sample weight element wise weight tensor must be broadcast able to the same shape as p red for example if p red have shape 64 10 and you want to weigh each sample in the batch separately sample weight should have shape 64 1 output loss loss tensor with shape batch size di men ions other than batch axis be average out def in it self weight number batch axis number k war gs super l 2 loss self in it weight batch axis k war gs def hybrid forward self f p red label sample weight none label reshape like f label p red loss f np square label p red if be np array else f square label p red loss apply weight f loss self weight / number sample weight if be np array if f be nd array return f np mean loss axis tuple range number loss n dim else return f n px batch flatten loss mean axis number else return f mean loss axis self batch axis exclude true
def log soft max self arg s k war gs convenience fluent method for py fun c log soft max the arguments be the same as for py fun c log soft max with this array as data return op log soft max self arg s k war gs
def soft max activation data none mode null out none name none k war gs r apply soft max activation to input this be intend for internal layer note this operator have be deprecate please use soft max if mode instance this operator will compute a soft max for each instance in the batch this be the default mode if mode channel this operator will compute a k class soft max at each position of each instance where k num channel this mode can only be use when the input array have at least 3 dimension this can be use for fully convolutional network image segmentation etc example input array mx nd array 3 0 5 0 5 2 7 2 4 7 3 0 2 soft max act mx nd soft max activation input array print soft max act as num py 1 78322066 e 02 1 46375655 e 03 5 38485940 e 04 6 56010211 e 03 9 73605454 e 01 6 56221947 e 03 5 95310994 e 04 9 73919690 e 01 1 78379621 e 02 1 08472735 e 03 define in /src/operator/nn/softmax activation cc l 58 parameters data nd array the input array mode channel instance optional default instance specify how to compute the soft max if set to instance it compute soft max for each instance if set to channel it compute cross channel soft max for each position of each instance out nd array optional the output nd array to hold the result return out nd array or list of nd array the output of this function return number
class ct c loss loss r connection ist temporal classification loss parameters layout str default nt c layout of prediction tensor n t c stand for batch size sequence length and alphabet size respectively label layout str default nt layout of the label n t stand for batch size and sequence length respectively weight float or none global scalar weight for loss input p red un normalize prediction tensor before soft max its shape depend on layout if layout be tnc p red should have shape sequence length batch size alphabet size note that in the last dimension index alphabet size 1 be reserve for internal use as blank label so alphabet size be one plus the actual alphabet size label zero base label tensor its shape depend on label layout if label layout be tn label should have shape label sequence length batch size p red lengths optional default none use for specify the length of each entry when different p red entries in the same batch have different lengths p red lengths should have shape batch size label lengths optional default none use for specify the length of each entry when different label entries in the same batch have different lengths label lengths should have shape batch size output loss output loss have shape batch size example suppose the vocabulary be a b c and in one batch we have three sequence ba cbb and a bac we can index the label as a 0 b 1 c 2 blank 3 then alphabet size should be 4 where label 3 be reserve for internal use by ct c loss we then need to pad each sequence with 1 to make a rectangular label tensor 1 0 1 1 2 1 1 1 0 1 0 2 reference connection ist temporal classification label un segment sequence data with recurrent neural network http //www cs toronto edu/~graves/icml 2006 pdf def in it self layout string label layout string weight none k war gs assert layout in string string \ string layout assert label layout in string string \ string label layout self layout layout self label layout label layout batch axis label layout find string super ct c loss self in it weight batch axis k war gs def hybrid forward self f p red label p red lengths none label lengths none sample weight none if self layout string p red f swap ax p red number number if self batch axis number label f swap ax label number number loss f ct c loss p red label p red lengths label lengths use data lengths p red lengths be not none use label lengths label lengths be not none blank label string return apply weight f loss self weight sample weight
class activation hybrid block r apply an activation function to input parameters activation str name of activation function to use see fun c ~mxnet nd array activation for available choices input data input tensor with arbitrary shape output out output tensor with the same shape as data def in it self activation k war gs self act type activation super activation self in it k war gs def alias self return self act type def hybrid forward self f x act f n px activation if be np array else f activation return act x act type self act type name string def re pr self s string return s format name self class name self dic t
class gru cell hybrid recurrent cell r gate rectify unit gru network cell note this be an implementation of the cudnn version of grus slight modification compare to cho et al 2014 the reset gate math r t be apply after matrix multiplication each call compute the follow function math \begin array ll r t s igm oid w ir x t b ir w hr h t 1 b hr \\ i t s igm oid w ii x t b ii w hi h t 1 b hi \\ n t \tanh w in x t b in r t w hn h t 1 b hn \\ h t 1 i t n t i t h t 1 \\ \end array where math h t be the hide state at time t math x t be the hide state of the previous layer at time t or math input t for the first layer and math r t math i t math n t be the reset input and new gate respectively parameters hide size int number of units in output symbol i 2 h weight initializer str or initializer initializer for the input weight matrix use for the linear transformation of the input h 2 h weight initializer str or initializer initializer for the recurrent weight matrix use for the linear transformation of the recurrent state i 2 h bias initializer str or initializer default zero initializer for the bias vector h 2 h bias initializer str or initializer default zero initializer for the bias vector prefix str default gru prefix for name of block s and name of weight if params be none params parameter or none default none container for weight share between cells create if none input data input tensor with shape batch size input size state a list of one initial recurrent state tensor with shape batch size num hide output out output tensor with shape batch size num hide next state a list of one output recurrent state tensor with the same shape as state def in it self hide size i 2 h weight initializer none h 2 h weight initializer none i 2 h bias initializer string h 2 h bias initializer string input size number prefix none params none super gru cell self in it prefix prefix params params self hide size hide size self input size input size self i 2 h weight self params get string shape number hide size input size in it i 2 h weight initializer allow defer in it true self h 2 h weight self params get string shape number hide size hide size in it h 2 h weight initializer allow defer in it true self i 2 h bias self params get string shape number hide size in it i 2 h bias initializer allow defer in it true self h 2 h bias self params get string shape number hide size in it h 2 h bias initializer allow defer in it true def state info self batch size number return string batch size self hide size string string def alias self return string def re pr self s string shape self i 2 h weight shape map string format shape number if shape number else none shape number return s format name self class name map map self dic t def hybrid forward self f input state i 2 h weight h 2 h weight i 2 h bias h 2 h bias py lint disable too many locals prefix string self counter prev state h state number i 2 h f fully connect data input weight i 2 h weight bias i 2 h bias num hide self hide size number name prefix string h 2 h f fully connect data prev state h weight h 2 h weight bias h 2 h bias num hide self hide size number name prefix string i 2 h r i 2 h z i 2 h f slice channel i 2 h num output number name prefix string h 2 h r h 2 h z h 2 h f slice channel h 2 h num output number name prefix string reset gate f activation f el em wise add i 2 h r h 2 h r name prefix string act type string name prefix string update gate f activation f el em wise add i 2 h z h 2 h z name prefix string act type string name prefix string next h t mp f activation f el em wise add i 2 h f el em wise mul reset gate h 2 h name prefix string name prefix string act type string name prefix string ones f ones like update gate name prefix string next h f el em wise add f el em wise mul f el em wise sub ones update gate name prefix string next h t mp name prefix string f el em wise mul update gate prev state h name prefix string name prefix string return next h next h
def norm data none ord null axis null out d type null keep dim null out none name none k war gs r compute the norm on an nd array this operator compute the norm on an nd array with the specify axis depend on the value of the ord parameter by default it compute the l 2 norm on the entire array currently only ord 2 support sparse nd array examples x 1 2 3 4 2 2 5 6 norm x ord 2 axis 1 3 1622777 4 472136 5 3851647 6 3245554 norm x ord 1 axis 1 4 6 7 8 r sp x cast storage row sparse norm r sp 5 47722578 csr x cast storage csr norm csr 5 47722578 define in /src/operator/tensor/broadcast reduce norm value cc l 88 parameters data nd array the input ord int optional default 2 order of the norm currently ord 1 and ord 2 be support axis shape or none optional default none the axis or ax along which to perform the reduction the default axis will compute over all elements into a scalar array with shape 1 if axis be int a reduction be perform on a particular axis if axis be a 2 tuple it specify the ax that hold 2 d matrices and the matrix norms of these matrices be compute out d type none float 16 float 32 float 64 int 32 int 64 int 8 optional default none the data type of the output keep dim boolean optional default 0 if this be set to true the reduce axis be leave in the result as dimension with size one out nd array optional the output nd array to hold the result return out nd array or list of nd array the output of this function return number
def pad data none mode null pad width null constant value null out none name none k war gs r pad an input array with a constant or edge value of the array note pad be deprecate use pad instead note current implementation only support 4 d and 5 d input array with pad apply only on ax 1 2 and 3 expect ax 4 and 5 in pad width to be zero this operation pad an input array with either a constant value or edge value along each axis of the input array the amount of pad be specify by pad width pad width be a tuple of integer pad widths for each axis of the format before 1 after 1 before n after n the pad width should be of length 2 n where n be the number of dimension of the array for dimension n of the input array before n and after n indicate how many value to add before and after the elements of the array along dimension n the widths of the higher two dimension before 1 after 1 before 2 after 2 must be 0 example x 1 2 3 4 5 6 7 8 9 10 11 12 11 12 13 14 15 16 17 18 19 20 21 22 pad x mode edge pad width 0 0 0 0 1 1 1 1 1 1 2 3 3 1 1 2 3 3 4 4 5 6 6 4 4 5 6 6 7 7 8 9 9 7 7 8 9 9 10 10 11 12 12 10 10 11 12 12 11 11 12 13 13 11 11 12 13 13 14 14 15 16 16 14 14 15 16 16 17 17 18 19 19 17 17 18 19 19 20 20 21 22 22 20 20 21 22 22 pad x mode constant constant value 0 pad width 0 0 0 0 1 1 1 1 0 0 0 0 0 0 1 2 3 0 0 4 5 6 0 0 0 0 0 0 0 0 0 0 0 0 7 8 9 0 0 10 11 12 0 0 0 0 0 0 0 0 0 0 0 0 11 12 13 0 0 14 15 16 0 0 0 0 0 0 0 0 0 0 0 0 17 18 19 0 0 20 21 22 0 0 0 0 0 0 define in /src/operator/pad cc l 765 parameters data nd array an n dimensional input array mode constant edge reflect require pad type to use constant pad with constant value edge pad use the edge value of the input array reflect pad by reflect value with respect to the edge pad width shape tuple require widths of the pad regions apply to the edge of each axis it be a tuple of integer pad widths for each axis of the format before 1 after 1 before n after n it should be of length 2 n where n be the number of dimension of the array this be equivalent to pad width in num py pad but flatten constant value double optional default 0 the value use for pad when mode be constant out nd array optional the output nd array to hold the result return out nd array or list of nd array the output of this function return number
def load self filename c tx none allow miss false ignore extra false restore prefix string cast d type false d type source string load parameters from file parameters filename str path to parameter file c tx context or list of context context s initialize load parameters on allow miss boo l default false whether to silently skip load parameters not represent in the file ignore extra boo l default false whether to silently ignore parameters from the file that be not present in this parameter dic t restore prefix str default prep end prefix to name of store parameters before load cast d type boo l default false cast the data type of the parameter d type source str default current must be in current save only valid if cast d type true specify the source of the d type for cast the parameters if restore prefix for name in self key assert name start with restore prefix \ string \ string \ string restore prefix name restore prefix nd array load nd array load filename self load dic t nd array load c tx allow miss ignore extra restore prefix filename cast d type d type source
def space to depth data none block size null out none name none k war gs r rearrange permute block of spatial data into depth similar to on nx space to depth operator https //github com/onnx/onnx/blob/master/docs/operators md space to depth the output be a new tensor where the value from height and width dimension be move to the depth dimension the reverse of this operation be depth to space math \begin gather x \prime reshape x n c h / block\ size block\ size w / block\ size block\ size \\ x \prime \prime transpose x \prime 0 3 5 1 2 4 \\ y reshape x \prime \prime n c block\ size 2 h / block\ size w / block\ size \end gather where math x be an input tensor with default layout as math n c h w batch channel height width and math y be the output tensor of layout math n c block\ size 2 h / block\ size w / block\ size example x 0 6 1 7 2 8 12 18 13 19 14 20 3 9 4 10 5 11 15 21 16 22 17 23 space to depth x 2 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 define in /src/operator/tensor/matrix op cc l 1018 parameters data nd array input nd array block size int require block of block size block size be move out nd array optional the output nd array to hold the result return out nd array or list of nd array the output of this function return number
class ada delta optimizer the ada delta optimizer this class implement ada delta an optimizer describe in ada delta an adaptive learn rate method available at https //arxiv org/abs/1212 5701 this optimizer update each weight by grad clip grad re scale grad wd weight clip gradient acc grad rho acc grad 1 rho grad grad delta sqr t acc delta epsilon / sqr t acc grad epsilon grad acc delta rho acc delta 1 rho delta delta weight delta wd weight this optimizer accept the follow parameters in addition to those accept by class optimizer parameters rho float decay rate for both square gradients and delta epsilon float small value to avoid division by 0 def in it self rho number epsilon number k war gs super ada delta self in it k war gs self rho rho self epsilon epsilon def create state self index weight return zero weight shape weight context accumulate g zero weight shape weight context accumulate delta def update self index weight grad state assert be instance weight nd array assert be instance grad nd array wd self get wd index self update count index pre process grad grad self re scale grad if self clip gradient be not none grad clip grad self clip gradient self clip gradient accumulate g and delta in it liz ation acc g acc delta state update g delta acc g self rho acc g number self rho grad grad current delta sqr t acc delta self epsilon / sqr t acc g self epsilon grad acc delta self rho acc delta number self rho current delta current delta update weight weight current delta wd weight
class ada grad optimizer ada grad optimizer this class implement the ada grad optimizer describe in adaptive sub gradient methods for online learn and stochastic optimization and available at http //www j ml r org/papers/volume12/duchi11a/duchi11a pdf this optimizer update each weight by grad clip grad re scale grad clip gradient history square grad div grad / sqr t history float stable eps weight div weight wd lr this optimizer accept the follow parameters in addition to those accept by class optimizer see also meth mx net nd array sparse ada grad update parameters eps float optional initial value of the history accumulator avoid division by 0 def in it self eps number k war gs super ada grad self in it k war gs self float stable eps eps def create state self index weight return zero weight shape weight context s type weight s type history def update self index weight grad state assert be instance weight nd array assert be instance grad nd array self update count index lr self get lr index wd self get wd index be sparse grad s type string history state if be sparse k war gs string self float stable eps string self re scale grad if self clip gradient k war gs string self clip gradient sparse ada grad update weight grad history out weight lr lr wd wd k war gs else grad grad self re scale grad if self clip gradient be not none grad clip grad self clip gradient self clip gradient history square grad div grad / sqr t history self float stable eps weight div weight wd lr
class adam optimizer the adam optimizer this class implement the optimizer describe in adam a method for stochastic optimization available at http //arxiv org/abs/1412 6980 if the storage type of grad be row sparse and lazy update be true \ lazy update at step t be apply by for row in grad indices re scale grad row clip grad row re scale grad wd weight row clip gradient m row beta 1 m row 1 beta 1 re scale grad row v row beta 2 v row 1 beta 2 re scale grad row 2 lr learn rate sqr t 1 beta 1 t / 1 beta 2 t w row w row lr m row / sqr t v row epsilon the lazy update only update the mean and var for the weight whose row sparse gradient indices appear in the current batch rather than update it for all indices compare with the original update it can provide large improvements in model train throughput for some applications however it provide slightly different semantics than the original update and may lead to different empirical result otherwise standard update at step t be apply by re scale grad clip grad re scale grad wd weight clip gradient m beta 1 m 1 beta 1 re scale grad v beta 2 v 1 beta 2 re scale grad 2 lr learn rate sqr t 1 beta 1 t / 1 beta 2 t w w lr m / sqr t v epsilon this optimizer accept the follow parameters in addition to those accept by class optimizer for detail of the update algorithm see class ~mxnet nd array adam update parameters beta 1 float optional exponential decay rate for the first moment estimate beta 2 float optional exponential decay rate for the second moment estimate epsilon float optional small value to avoid division by 0 lazy update boo l optional default be true if true lazy update be apply \ if the storage type of weight and grad be both row sparse def in it self learn rate number beta 1 number beta 2 number epsilon number lazy update true k war gs super adam self in it learn rate learn rate k war gs self beta 1 beta 1 self beta 2 beta 2 self epsilon epsilon self lazy update lazy update def create state self index weight s type weight s type if self lazy update else string return zero weight shape weight context d type weight d type s type s type mean zero weight shape weight context d type weight d type s type s type variance def update self index weight grad state assert be instance weight nd array assert be instance grad nd array self update count index lr self get lr index wd self get wd index t self index update count index coe f 1 number self beta 1 t coe f 2 number self beta 2 t lr math sqr t coe f 2 /coef1 k war gs string self beta 1 string self beta 2 string self epsilon string self re scale grad if self clip gradient k war gs string self clip gradient mean var state adam update weight grad mean var out weight lazy update self lazy update lr lr wd wd k war gs
class rms prop optimizer the rms prop optimizer two versions of rms prop be implement if center false we follow http //www cs toronto edu/~tijmen/csc321/slides/lecture slide lec 6 pdf by tie le man hinton 2012 for detail of the update algorithm see class ~mxnet nd array rms prop update if center true we follow http //arxiv org/pdf/1308 0850 v 5 pdf 38 45 by alex grave 2013 for detail of the update algorithm see class ~mxnet nd array rms prop alex update this optimizer accept the follow parameters in addition to those accept by class optimizer parameters gamma 1 float optional a decay factor of move average over past square gradient gamma 2 float optional a momentum factor only use if center true epsilon float optional small value to avoid division by 0 center boo l optional flag to control which version of rms prop to use true will use grave s version of rms prop false will use tie le man hinton s version of rms prop clip weight float optional clip weight into range clip weight clip weight def in it self learn rate number gamma 1 number gamma 2 number epsilon number center false clip weight none k war gs super rms prop self in it learn rate learn rate k war gs self gamma 1 gamma 1 self gamma 2 gamma 2 self center center self epsilon epsilon self clip weight clip weight def create state self index weight if self center return zero weight shape weight context s type weight s type n zero weight shape weight context s type weight s type g zero weight shape weight context s type weight s type delta else return zero weight shape weight context s type weight s type n def update self index weight grad state assert be instance weight nd array assert be instance grad nd array self update count index lr self get lr index wd self get wd index k war gs string self gamma 1 string self epsilon string self re scale grad if self center k war gs string self gamma 2 if self clip gradient k war gs string self clip gradient if self clip weight k war gs string self clip weight if not self center n state rms prop update weight grad n out weight lr lr wd wd k war gs else n g delta state rms prop alex update weight grad n g delta out weight lr lr wd wd k war gs
def grad head variables head grads none retain graph none create graph false train mode true py lint disable redefine outer name compute the gradients of head w r t variables gradients will be return as new nd array instead of store into variable grad support record gradient graph for compute higher order gradients note currently only a very limit set of operators support higher order \ gradients parameters head nd array or list of nd array output nd array s variables nd array or list of nd array input variables to compute gradients for head grads nd array or list of nd array or none gradients with respect to head retain graph boo l whether to keep computation graph to differentiate again instead of clear history and release memory default to the same value as create graph create graph boo l whether to record gradient graph for compute higher order train mode boo l optional whether to do backward for train or prediction return nd array or list of nd array gradients with respect to variables examples x mx nd ones 1 x attach grad with mx auto grad record z mx nd el em wise add mx nd exp x x dx mx auto grad grad z x create graph true print dx 3 71828175 nd array 1 cpu 0 head handle h grad handle parse head head head grads if be instance variables nd array variables variables else assert len variables string var handle c handle array variables retain graph retain graph if retain graph be not none else create graph grad var s c type pointer nd array handle grad s type c type pointer c type c int check call lib mx auto grad backward ex len head handle head handle h grad handle len var handle var handle c type c int retain graph c type c int create graph c type c int train mode c type by ref grad var s c type by ref grad s type ret nd array cls c type cast grad var s i nd array handle s type grad s type i for i in range len var handle if be instance variables nd array return ret number return ret
class identity hybrid block block that pass through the input directly this block can be use in conjunction with hybrid concurrent block for residual connection example net hybrid concurrent use net s name scope to give child block appropriate name with net name scope net add nn dense 10 activation relu net add nn dense 20 net add identity def in it self prefix none params none super identity self in it prefix prefix params params def hybrid forward self f x return x
class elu hybrid block r exponential linear unit elu fast and accurate deep network learn by exponential linear units clever t et al 2016 https //arxiv org/abs/1511 07289 publish as a conference paper at icl r 2016 parameters alpha float the alpha parameter as describe by clever t et al 2016 input data input tensor with arbitrary shape output out output tensor with the same shape as data def in it self alpha number k war gs super elu self in it k war gs self alpha alpha def hybrid forward self f x leaky relu f n px leaky relu if be np array else f leaky relu return leaky relu x act type string slope self alpha
def hard s igm oid data none alpha null beta null out none name none k war gs r compute hard s igm oid of x element wise math y max 0 min 1 alpha x beta define in /src/operator/tensor/elemwise u nary op basic cc l 161 parameters data nd array the input array alpha float optional default 0 200000003 slope of hard s igm oid beta float optional default 0 5 bias of hard s igm oid out nd array optional the output nd array to hold the result return out nd array or list of nd array the output of this function return number
class s elu hybrid block r scale exponential linear unit s elu self normalize neural network kl be bauer et al 2017 https //arxiv org/abs/1706 02515 input data input tensor with arbitrary shape output out output tensor with the same shape as data def in it self k war gs super s elu self in it k war gs def hybrid forward self f x leaky relu f n px leaky relu if be np array else f leaky relu return leaky relu x act type string name string
def dot lhs none rh s none transpose a null transpose b null forward s type null out none name none k war gs r dot product of two array dot s behavior depend on the input array dimension 1 d array inner product of vectors 2 d array matrix multiplication n d array a sum product over the last axis of the first input and the first axis of the second input for example give 3 d x with shape n m k and y with shape k r s the result array will have shape n m r s it be compute by dot x y i j a b sum x i j y a b example x reshape 0 1 2 3 4 5 6 7 shape 2 2 2 y reshape 7 6 5 4 3 2 1 0 shape 2 2 2 dot x y 0 0 1 1 0 sum x 0 0 y 1 1 0 the storage type of dot output depend on storage type of input transpose option and forward s type option for output storage type implement sparse operations include dot default default transpose a true/false transpose b true/false default dot csr default transpose a true default dot csr default transpose a true row sparse dot csr default default dot csr row sparse default dot default csr csr cpu only dot default csr forward s type default default dot default csr transpose b true forward s type default default if the combination of input storage type and forward s type do not match any of the above pattern dot will fallback and generate output with default storage note if the storage type of the lhs be csr the storage type of gradient w r t rh s will be row sparse only a subset of optimizer s support sparse gradients include sgd ada grad and adam note that by default lazy update be turn on which may perform differently from standard update for more detail please check the optimization api at https //mxnet incubator apache org/api/python/optimization/optimization html define in /src/operator/tensor/dot cc l 77 parameters lhs nd array the first input rh s nd array the second input transpose a boolean optional default 0 if true then transpose the first input before dot transpose b boolean optional default 0 if true then transpose the second input before dot forward s type none csr default row sparse optional default none the desire storage type of the forward output give by user if the combination of input storage type and this hint do not match any implement ones the dot operator will perform fallback operation and still produce an output of the desire storage type out nd array optional the output nd array to hold the result return out nd array or list of nd array the output of this function return number
class embed hybrid block r turn non negative integers indexes/tokens into dense vectors of fix size eg 4 20 0 25 0 1 0 6 0 2 note if sparse grad be set to true the gradient w r t weight will be sparse only a subset of optimizer s support sparse gradients include sgd ada grad and adam by default lazy update be turn on which may perform differently from standard update for more detail please check the optimization api at https //mxnet incubator apache org/api/python/optimization/optimization html parameters input dim int size of the vocabulary i e maximum integer index 1 output dim int dimension of the dense embed d type str or np d type default float 32 data type of output embed s weight initializer initializer initializer for the embed s matrix sparse grad boo l if true gradient w r t weight will be a row sparse nd array input data n 1 d tensor with shape x 1 x 2 xn 1 output out n d tensor with shape x 1 x 2 xn 1 output dim def in it self input dim output dim d type string weight initializer none sparse grad false k war gs super embed self in it k war gs grad s type string if sparse grad else string self k war gs string input dim string output dim string d type string sparse grad self weight self params get string shape input dim output dim in it weight initializer d type d type allow defer in it true grad s type grad s type def hybrid forward self f x weight embed f n px embed if be np array else f embed return embed x weight name string self k war gs def re pr self s string return s format block name self class name self k war gs
class layer norm hybrid block r apply layer normalization to the n dimensional input array this operator take an n dimensional input array and normalize the input use the give axis math out \frac x mean data axis \sqrt var data axis \epsilon gamma beta parameters axis int default 1 the axis that should be normalize this be typically the axis of the channel epsilon float default 1 e 5 small float add to variance to avoid divide by zero center boo l default true if true add offset of beta to normalize tensor if false beta be ignore scale boo l default true if true multiply by gamma if false gamma be not use beta initializer str or initializer default zero initializer for the beta weight gamma initializer str or initializer default ones initializer for the gamma weight in channel int default 0 number of channel feature map in input data if not specify initialization will be defer to the first time forward be call and in channel will be infer from the shape of input data input data input tensor with arbitrary shape output out output tensor with the same shape as data reference layer normalization https //arxiv org/pdf/1607 06450 pdf examples input of shape 2 5 x mx nd array 1 2 3 4 5 1 1 2 2 2 layer normalization be calculate with the above formula layer layer norm layer initialize c tx mx cpu 0 layer x 1 41421 0 707105 0 0 707105 1 41421 1 2247195 1 2247195 0 81647956 0 81647956 0 81647956 nd array 2 x 5 cpu 0 def in it self axis number epsilon number center true scale true beta initializer string gamma initializer string in channel number prefix none params none super layer norm self in it prefix prefix params params self k war gs string epsilon string axis string center string scale self axis axis self epsilon epsilon self center center self scale scale self gamma self params get string grad re q string if scale else string shape in channel in it gamma initializer allow defer in it true self beta self params get string grad re q string if center else string shape in channel in it beta initializer allow defer in it true def hybrid forward self f data gamma beta layer norm f n px layer norm if be np array else f layer norm return layer norm data gamma gamma beta beta axis self axis eps self epsilon def re pr self s string in channel self gamma shape number s string format in channel s string return s format name self class name content string join string join k v re pr for k v in self k war gs items
class leaky relu hybrid block r leaky version of a rectify linear unit it allow a small gradient when the unit be not active math f\left x\right \left\ \begin array lr \alpha x x \lt 0 \\ x x \geq 0 \\ \end array \right \\ parameters alpha float slope coefficient for the negative half axis must be 0 input data input tensor with arbitrary shape output out output tensor with the same shape as data def in it self alpha k war gs assert alpha number string super leaky relu self in it k war gs self alpha alpha def hybrid forward self f x leaky relu f n px leaky relu if be np array else f leaky relu return leaky relu x act type string slope self alpha name string def re pr self s string return s format name self class name alpha self alpha
class p relu hybrid block r parametric leaky version of a rectify linear unit https //arxiv org/abs/1502 01852 paper it learn a gradient when the unit be not active math f\left x\right \left\ \begin array lr \alpha x x \lt 0 \\ x x \geq 0 \\ \end array \right \\ where alpha be a learn parameter parameters alpha initializer initializer initializer for the embed s matrix in channel int default 1 number of channel alpha parameters to learn can either be 1 or n where n be the size of the second dimension of the input tensor input data input tensor with arbitrary shape output out output tensor with the same shape as data def in it self alpha initializer initializer constant number in channel number k war gs super p relu self in it k war gs with self name scope self alpha self params get string shape in channel in it alpha initializer def hybrid forward self f x alpha leaky relu f n px leaky relu if be np array else f leaky relu return leaky relu x gamma alpha act type string name string
class rnn rnn layer r apply a multi layer el man rnn with tan h or relu non linearity to an input sequence for each element in the input sequence each layer compute the follow function math h t \tanh w ih x t b ih w hh h t 1 b hh where math h t be the hide state at time t and math x t be the output of the previous layer at time t or math input t for the first layer if non linearity relu then relu be use instead of tan h parameters hide size int the number of feature in the hide state h num layer int default 1 number of recurrent layer activation relu or tan h default relu the activation function to use layout str default tnc the format of input and output tensors t n and c stand for sequence length batch size and feature dimension respectively dropout float default 0 if non zero introduce a dropout layer on the output of each rnn layer except the last layer bidirectional boo l default false if true become a bidirectional rnn i 2 h weight initializer str or initializer initializer for the input weight matrix use for the linear transformation of the input h 2 h weight initializer str or initializer initializer for the recurrent weight matrix use for the linear transformation of the recurrent state i 2 h bias initializer str or initializer initializer for the bias vector h 2 h bias initializer str or initializer initializer for the bias vector input size int default 0 the number of expect feature in the input x if not specify it will be infer from input d type str default float 32 type to initialize the parameters and default state to prefix str or none prefix of this block params parameter dic t or none share parameters for this block input data input tensor with shape sequence length batch size input size when layout be tnc for other layouts dimension be permute accordingly use transpose operator which add performance overhead consider create batch in tnc layout during data batch step state initial recurrent state tensor with shape num layer batch size num hide if bidirectional be true shape will instead be 2 num layer batch size num hide if state be none zero will be use as default begin state output out output tensor with shape sequence length batch size num hide when layout be tnc if bidirectional be true output shape will instead be sequence length batch size 2 num hide out state output recurrent state tensor with the same shape as state if state be none out state will not be return examples layer mx gluon rnn rnn 100 3 layer initialize input mx nd random uniform shape 5 3 10 by default zero be use as begin state output layer input manually specify begin state h 0 mx nd random uniform shape 3 3 100 output hn layer input h 0 def in it self hide size num layer number activation string layout string dropout number bidirectional false i 2 h weight initializer none h 2 h weight initializer none i 2 h bias initializer string h 2 h bias initializer string input size number d type string k war gs super rnn self in it hide size num layer layout dropout bidirectional input size i 2 h weight initializer h 2 h weight initializer i 2 h bias initializer h 2 h bias initializer string activation none none none none false d type k war gs def state info self batch size number return string self num layer self dir batch size self hide size string string string self d type
def up sample data k war gs r up sample the give input data two algorithms sample type be available for up sample nearest neighbor bi linear nearest neighbor up sample input data be expect to be nch w example x 1 1 1 1 1 1 1 1 1 up sample x scale 2 sample type nearest 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 bi linear up sample use de convolution algorithm under the hood you need provide both input data and the kernel input data be expect to be nch w num filter be expect to be same as the number of channel example x 1 1 1 1 1 1 1 1 1 w 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 up sample x w scale 2 sample type bi linear num filter 1 1 2 2 2 2 1 2 4 4 4 4 2 2 4 4 4 4 2 2 4 4 4 4 2 2 4 4 4 4 2 1 2 2 2 2 1 define in /src/operator/nn/upsampling cc l 172 this function support variable length of positional input parameters data symbol array of tensors to up sample for bi linear up sample there should be 2 input 1 data and 1 weight scale int require up sample scale num filter int optional default 0 input filter only use by bi linear sample type since bi linear up sample use de convolution num filter be set to the number of channel sample type bi linear nearest require up sample method multi input mode c on cat sum optional default c on cat how to handle multiple input c on cat mean concatenate up sample image along the channel dimension sum mean add all image together only available for nearest neighbor up sample work space long non negative optional default 512 t mp work space for de convolution mb name string optional name of the result symbol return symbol the result symbol return number
class s igm oid binary cross entropy loss loss r the cross entropy loss for binary classification alias s igm oid bce loss bce loss be useful when train logistic regression if from s igm oid be false default this loss compute math pro b \frac 1 1 \exp p red l \sum i label i \log pro b i pos\ weight 1 label i \log 1 pro b i if from s igm oid be true this loss compute math l \sum i label i \log p red i pos\ weight 1 label i \log 1 p red i a tensor pos weight 1 decrease the false negative count hence increase the recall conversely set pos weight 1 decrease the false positive count and increase the precision p red and label can have arbitrary shape as long as they have the same number of elements parameters from s igm oid boo l default be false whether the input be from the output of s igm oid set this to false will make the loss calculate s igm oid and bce together which be more numerically stable through log sum exp trick weight float or none global scalar weight for loss batch axis int default 0 the axis that represent mini batch input p red prediction tensor with arbitrary shape label target tensor with value in range 0 1 must have the same size as p red sample weight element wise weight tensor must be broadcast able to the same shape as p red for example if p red have shape 64 10 and you want to weigh each sample in the batch separately sample weight should have shape 64 1 pos weight a weight tensor of positive examples must be a vector with length equal to the number of class for example if p red have shape 64 10 pos weight should have shape 1 10 output loss loss tensor with shape batch size di men ions other than batch axis be average out def in it self from s igm oid false weight none batch axis number k war gs super s igm oid binary cross entropy loss self in it weight batch axis k war gs self from s igm oid from s igm oid def hybrid forward self f p red label sample weight none pos weight none label reshape like f label p red if be np array relu fn f n px relu act fn f n px activation abs fn f np abs mul fn f np multiply log fn f np log else relu fn f relu act fn f activation abs fn f abs mul fn f broadcast mul log fn f log if not self from s igm oid if pos weight be none we use the stable formula max x 0 x z log 1 exp abs x loss relu fn p red p red label \ act fn abs fn p red act type string else we use the stable formula x x z 1 z pos weight z \ log 1 exp abs x max x 0 log weight number mul fn pos weight number label loss p red p red label log weight \ act fn abs fn p red act type string relu fn p red else eps number if pos weight be none loss log fn p red eps label log fn number p red eps number label else loss mul fn log fn p red eps label pos weight log fn number p red eps number label loss apply weight f loss self weight sample weight if be np array if f be nd array return f np mean loss axis tuple range number loss n dim else return f n px batch flatten loss mean axis number else return f mean loss axis self batch axis exclude true
class soft max cross entropy loss loss r compute the soft max cross entropy loss alias soft max ce loss if sparse label be true default label should contain integer category indicators math \declaremathoperator soft max soft max p \softmax p red l \sum i \log p i label i label s shape should be p red s shape with the axis dimension remove i e for p red with shape 1 2 3 4 and axis 2 label s shape should be 1 2 4 if sparse label be false label should contain probability distribution and label s shape should be the same with p red math p \softmax p red l \sum i \sum j label j \log p ij parameters axis int default 1 the axis to sum over when compute soft max and entropy sparse label boo l default true whether label be an integer array instead of probability distribution from log its boo l default false whether input be a log probability usually from log soft max instead of un normalize number weight float or none global scalar weight for loss batch axis int default 0 the axis that represent mini batch input p red the prediction tensor where the batch axis dimension range over batch size and axis dimension range over the number of class label the truth tensor when sparse label be true label s shape should be p red s shape with the axis dimension remove i e for p red with shape 1 2 3 4 and axis 2 label s shape should be 1 2 4 and value should be integers between 0 and 2 if sparse label be false label s shape must be the same as p red and value should be float in the range 0 1 sample weight element wise weight tensor must be broadcast able to the same shape as label for example if label have shape 64 10 and you want to weigh each sample in the batch separately sample weight should have shape 64 1 output loss loss tensor with shape batch size di men ions other than batch axis be average out def in it self axis number sparse label true from log its false weight none batch axis number k war gs super soft max cross entropy loss self in it weight batch axis k war gs self axis axis self sparse label sparse label self from log its from log its def hybrid forward self f p red label sample weight none if be np array log soft max f n px log soft max pick f n px pick else log soft max f log soft max pick f pick if not self from log its p red log soft max p red self axis if self sparse label loss pick p red label axis self axis keep dim true else label reshape like f label p red loss p red label sum axis self axis keep dim true loss apply weight f loss self weight sample weight if be np array if f be nd array return loss mean axis tuple range number loss n dim else return f n px batch flatten loss mean axis number else return loss mean axis self batch axis exclude true
class cosine embed loss loss r for a target label 1 or 1 vectors input 1 and input 2 the function compute the cosine distance between the vectors this can be interpret as how similar/dissimilar two input vectors be math l \sum i \begin case 1 cos\ sim input 1 i input 2 i \text if label i 1\\ cos\ sim input 1 i input 2 i \text if label i 1 \end case \\ cos\ sim input 1 input 2 \frac input 1 i input 2 i input 1 i input 2 i input 1 input 2 can have arbitrary shape as long as they have the same number of elements parameters weight float or none global scalar weight for loss batch axis int default 0 the axis that represent mini batch margin float margin of separation between correct and incorrect pair input input 1 a tensor with arbitrary shape input 2 another tensor with same shape as p red to which input 1 be compare for similarity and loss calculation label a 1 d tensor indicate for each pair input 1 and input 2 target label be 1 or 1 sample weight element wise weight tensor must be broadcast able to the same shape as input 1 for example if input 1 have shape 64 10 and you want to weigh each sample in the batch separately sample weight should have shape 64 1 output loss the loss tensor with shape batch size def in it self weight none batch axis number margin number k war gs super cosine embed loss self in it weight batch axis k war gs self margin margin def hybrid forward self f input 1 input 2 label sample weight none input 1 reshape like f input 1 input 2 label label reshape number number cos sim self cosine similarity f input 1 input 2 y 1 label number y minus 1 label number cos sim a number cos sim y 1 if f be nd array z array f array number else z array f zero number number cos sim b f broadcast maximum z array y minus 1 cos sim self margin axis number loss cos sim a cos sim b loss apply weight f loss self weight sample weight return loss def cosine similarity self f x y axis number calculate the cosine similarity between 2 vectors x norm f norm x axis axis reshape number number y norm f norm y axis axis reshape number number x dot y f sum x y axis axis reshape number number if f be nd array eps arr f array number else eps arr f full number number number return x dot y / f broadcast maximum x norm y norm eps arr
class poisson nll loss loss r for a target random variable in a poisson distribution the function calculate the negative log likelihood loss poisson nll loss measure the loss accrue from a poisson regression prediction make by the model math l \text p red \text target \log \text p red \log \text target target p red can have arbitrary shape as long as they have the same number of elements parameters from log its boolean default true indicate whether log predict value have already be compute if true the loss be compute as math \exp \text p red \text target \text p red and if false then loss be compute as math \text p red \text target \log \text p red \text epsilon the default value weight float or none global scalar weight for loss batch axis int default 0 the axis that represent mini batch compute full boolean default false indicate whether to add an approximation stirling factor for the factorial term in the formula for the loss the stirling factor be math \text target \log \text target \text target 0 5 \log 2 \pi \text target epsilon float default 1 e 08 this be to avoid calculate log 0 which be not define input p red predict value target random variable count or number which belong to a poisson distribution sample weight element wise weight tensor must be broadcast able to the same shape as p red for example if p red have shape 64 10 and you want to weigh each sample in the batch separately sample weight should have shape 64 1 output loss average loss shape 1 1 of the loss tensor with shape batch size def in it self weight none from log its true batch axis number compute full false k war gs super poisson nll loss self in it weight batch axis k war gs self from log its from log its self compute full compute full def hybrid forward self f p red target sample weight none epsilon number target reshape like f target p red if self from log its loss f exp p red target p red else loss p red target f log p red epsilon if self compute full use num py s pi value stirling factor target \ f log target target number f log number target np pi target gt 1 target number stirling factor target gt 1 loss stirling factor loss apply weight f loss self weight sample weight return f mean loss
def save checkpoint prefix epoch symbol arg params aux params remove amp cast true checkpoint the model data into file parameters prefix str prefix of model name epoch int the epoch number of the model symbol symbol the input symbol arg params dic t of str to nd array model parameter dic t of name to nd array of net s weight aux params dic t of str to nd array model parameter dic t of name to nd array of net s auxiliary state remove amp cast boo l optional whether to remove the amp cast and amp multi cast operators before save the model note prefix symbol json will be save for symbol prefix epoch params will be save for parameters if symbol be not none symbol save string prefix remove amp cast remove amp cast save dic t string k v as in context cpu for k v in arg params items save dic t update string k v as in context cpu for k v in aux params items param name string prefix epoch nd save param name save dic t log info string param name
class factor scheduler lr scheduler reduce the learn rate by a factor for every n step it return a new learn rate by base lr pow factor floor num update/step parameters step int change the learn rate for every n update factor float optional the factor to change the learn rate stop factor lr float optional stop update the learn rate if it be less than this value def in it self step factor number stop factor lr number base lr number warm up step number warm up begin lr number warm up mode string super factor scheduler self in it base lr warm up step warm up begin lr warm up mode if step number raise value error string if factor number raise value error string self step step self factor factor self stop factor lr stop factor lr self count number def call self num update if num update self warm up step return self get warm up lr num update note use while rather than if for continue train via load epoch while num update self count self step self count self step self base lr self factor if self base lr self stop factor lr self base lr self stop factor lr log info string string num update self base lr else log info string num update self base lr return self base lr
class sgd optimizer the sgd optimizer with momentum and weight decay if the storage type of grad be row sparse and lazy update be true \ lazy update be apply by for row in grad indices re scale grad row lr re scale grad clip grad row clip gradient wd weight row state row momentum row state row re scale grad row weight row weight row state row the sparse update only update the momentum for the weight whose row sparse gradient indices appear in the current batch rather than update it for all indices compare with the original update it can provide large improvements in model train throughput for some applications however it provide slightly different semantics than the original update and may lead to different empirical result in the case when update on kv store be set to false either globally via mx net update on kv store 0 environment variable or as a parameter in class ~mxnet gluon trainer sgd optimizer can perform aggregate update of parameters which may lead to improve performance the aggregation size be control by mx net optimizer aggregation size environment variable and default to 4 otherwise standard update be apply by re scale grad lr re scale grad clip grad clip gradient wd weight state momentum state re scale grad weight weight state for detail of the update algorithm see class ~mxnet nd array sgd update and class ~mxnet nd array sgd mom update this optimizer accept the follow parameters in addition to those accept by class optimizer parameters momentum float optional the momentum value lazy update boo l optional default be true if true lazy update be apply \ if the storage type of weight and grad be both row sparse multi precision boo l optional flag to control the internal precision of the optimizer false result in use the same precision as the weight default true make internal 32 bite copy of the weight and apply gradients in 32 bite precision even if actual weight use in the model have lower precision turn this on can improve convergence and accuracy when train with float 16 def in it self momentum number lazy update true k war gs super sgd self in it k war gs self momentum momentum self lazy update lazy update self aggregate num int os get en v string string def create state multi precision self index weight weight master copy none if self multi precision and weight d type num py float 16 weight master copy weight as type num py float 32 return self create state index weight master copy weight master copy if weight d type num py float 16 and not self multi precision warn warn string string string string return self create state index weight def create state self index weight momentum none if self momentum number s type weight s type if self lazy update else string momentum zero weight shape weight context d type weight d type s type s type return momentum def update i mpl self indices weight grads state multi precision false aggregate true if not be instance indices tuple list indices indices weight weight grads grads state state for weight grad in zip weight grads assert be instance weight nd array assert be instance grad nd array aggregate aggregate and weight s type string and grad s type string self update count indices lr s self get lr s indices wds self get wds indices k war gs string self re scale grad if self momentum number k war gs string self momentum if self clip gradient k war gs string self clip gradient if aggregate if not multi precision if self momentum number multi sgd mom update flatten list zip weight grads state out weight num weight len weight lr s lr s wds wds k war gs else multi sgd update flatten list zip weight grads out weight num weight len weight lr s lr s wds wds k war gs else if self momentum number multi mp sgd mom update flatten list zip weight grads zip state out weight num weight len weight lr s lr s wds wds k war gs else multi mp sgd update flatten list zip weight grads list zip state number out weight num weight len weight lr s lr s wds wds k war gs else for weight grad state lr wd in zip weight grads state lr s wds if not multi precision if state be not none sgd mom update weight grad state out weight lazy update self lazy update lr lr wd wd k war gs else sgd update weight grad out weight lazy update self lazy update lr lr wd wd k war gs else if state number be not none mp sgd mom update weight grad state number state number out weight lr lr wd wd k war gs else mp sgd update weight grad state number out weight lr lr wd wd k war gs def update self index weight grad state self update i mpl index weight grad state multi precision false def update multi precision self index weight grad state if not be instance index tuple list use multi precision self multi precision and weight d type num py float 16 else use multi precision self multi precision and weight number d type num py float 16 self update i mpl index weight grad state multi precision use multi precision
class sequential block stack block sequentially example net nn sequential use net s name scope to give child block appropriate name with net name scope net add nn dense 10 activation relu net add nn dense 20 def in it self prefix none params none super sequential self in it prefix prefix params params def add self block add block on top of the stack for block in block self register child block def forward self x for block in self children value x block x return x def re pr self s string mods tr string join string format key key block indent block re pr number for key block in self children items return s format name self class name mods tr mods tr def get item self key layer list self children value key if be instance layer list net type self prefix self prefix with net name scope net add layer return net else return layer def len self return len self children def hybridize self active true k war gs activate or deactivate hybrid block s recursively have no effect on non hybrid children parameters active boo l default true whether to turn hybrid on or off k war gs string additional flag for hybridize operator if self children and all be instance c hybrid block for c in self children value warn warn string string self prefix stack level number super sequential self hybridize active k war gs
def pause train mode false py lint disable redefine outer name return a scope context to be use in with statement for cod that do not need gradients to be calculate example with auto grad record y model x backward y with auto grad pause test io gradient update parameters train mode boo l default false whether to do forward for train or predict return record state cope false train mode
def split data none num output null axis null squeeze axis null out none name none k war gs r split an array along a particular axis into multiple sub array note slice channel be deprecate use split instead note that num output should evenly divide the length of the axis along which to split the array example x 1 2 3 4 5 6 x shape 3 2 1 y split x axis 1 num output 2 // a list of 2 array with shape 3 1 1 y 1 3 5 2 4 6 y 0 shape 3 1 1 z split x axis 0 num output 3 // a list of 3 array with shape 1 2 1 z 1 2 3 4 5 6 z 0 shape 1 2 1 squeeze axis 1 remove the axis with length 1 from the shape of the output array note that set squeeze axis to 1 remove axis with length 1 only along the axis which it be split also squeeze axis can be set to true only if input shape axis num output example z split x axis 0 num output 3 squeeze axis 1 // a list of 3 array with shape 2 1 z 1 2 3 4 5 6 z 0 shape 2 1 define in /src/operator/slice channel cc l 106 parameters data nd array the input num output int require number of split note that this should evenly divide the length of the axis axis int optional default 1 axis along which to split squeeze axis boolean optional default 0 if true remove the axis with length 1 from the shape of the output array note that set squeeze axis to true remove axis with length 1 only along the axis which it be split also squeeze axis can be set to true only if input shape axis num output out nd array optional the output nd array to hold the result return out nd array or list of nd array the output of this function return number
def unique ar return index false return inverse false return count false axis none find the unique elements of an array return the sort unique elements of an array there be three optional output in addition to the unique elements the indices of the input array that give the unique value the indices of the unique array that reconstruct the input array the number of time each unique value come up in the input array parameters ar nd array input array unless axis be specify this will be flatten if it be not already 1 d return index boo l optional if true also return the indices of ar along the specify axis if provide or in the flatten array that result in the unique array return inverse boo l optional if true also return the indices of the unique array for the specify axis if provide that can be use to reconstruct ar return count boo l optional if true also return the number of time each unique item appear in ar axis int or none optional the axis to operate on if none ar will be flatten if an integer the sub array index by the give axis will be flatten and treat as the elements of a 1 d array with the dimension of the give axis see the note for more detail the default be none return unique nd array the sort unique value unique indices nd array optional the indices of the first occurrences of the unique value in the original array only provide if return index be true unique inverse nd array optional the indices to reconstruct the original array from the unique array only provide if return inverse be true unique count nd array optional the number of time each of the unique value come up in the original array only provide if return count be true note when an axis be specify the sub array index by the axis be sort this be do by make the specify axis the first dimension of the array and then flatten the sub array in c order the flatten sub array be then view as a structure type with each element give a label with the effect that we end up with a 1 d array of structure type that can be treat in the same way as any other 1 d array the result be that the flatten sub array be sort in le xico graphic order start with the first element this function differ from the original num py unique https //docs sci py org/doc/numpy/reference/generated/numpy unique html in the follow aspects only support nd array as input object array or structure array be not support examples np unique np array 1 1 2 2 3 3 array 1 2 3 a np array 1 1 2 3 np unique a array 1 2 3 return the unique row of a 2 d array a np array 1 0 0 1 0 0 2 3 4 np unique a axis 0 array 1 0 0 2 3 4 return the indices of the original array that give the unique value a np array 1 2 6 4 2 3 2 u indices np unique a return index true u array 1 2 3 4 6 indices array 0 1 5 3 2 d type int 64 a indices array 1 2 3 4 6 reconstruct the input array from the unique value a np array 1 2 6 4 2 3 2 u indices np unique a return inverse true u array 1 2 3 4 6 indices array 0 1 4 3 1 2 1 d type int 64 u indices array 1 2 6 4 2 3 2 return mx nd np unique ar return index return inverse return count axis
def mesh grid xi k war gs return coordinate matrices from coordinate vectors make n d coordinate array for vector i zed evaluations of n d scalar/vector field over n d grids give one dimensional coordinate array x 1 x 2 xn parameters x 1 x 2 xn nd array 1 d array represent the coordinate of a grid index xy ij optional cartesian xy default or matrix ij index of output see note for more detail sparse boo l optional if true a sparse grid be return in order to conserve memory default be false please note that sparse true be currently not support copy boo l optional if false a view into the original array be return in order to conserve memory default be true please note that copy false be currently not support return x 1 x 2 xn nd array for vectors x 1 x 2 xn with lengths ni len xi return n 1 n 2 n 3 nn shape array if index ij or n 2 n 1 n 3 nn shape array if index xy with the elements of xi repeat to fill the matrix along the first dimension for x 1 the second for x 2 and so on note this function support both index conventions through the index keyword argument give the string ij return a mesh grid with matrix index while xy return a mesh grid with cartesian index in the 2 d case with input of length m and n the output be of shape n m for xy index and m n for ij index in the 3 d case with input of length m n and p output be of shape n m p for xy index and m n p for ij index the difference be illustrate by the follow code snippet xv y v np mesh grid x y sparse false index ij for i in range nx for j in range ny treat xv i j y v i j xv y v np mesh grid x y sparse false index xy for i in range nx for j in range ny treat xv j i y v j i in the 1 d and 0 d case the index and sparse keywords have no effect n dim len xi copy k war gs pop string true if not copy raise not implement error string sparse k war gs pop string false if sparse raise not implement error string index k war gs pop string string if k war gs raise type error string list k war gs number if index not in string string raise value error string s 0 number n dim output x reshape s 0 i number s 0 i number for i x in enumerate xi if index string and n dim number switch first and second axis output number output number reshape number number s 0 number output number output number reshape number number s 0 number if not sparse return the full n d matrix not only the 1 d vector output broadcast array output return output
def eye n m none k number d type np float 32 k war gs return a 2 d array with ones on the diagonal and zero elsewhere parameters n int number of row in the output m int optional number of columns in the output if none default to n k int optional index of the diagonal 0 the default refer to the main diagonal a positive value refer to an upper diagonal and a negative value to a lower diagonal d type data type optional data type of the return array return i nd array of shape n m an array where all elements be equal to zero except for the k th diagonal whose value be equal to one examples np eye 2 d type int array 1 0 0 1 d type int 64 np eye 3 k 1 array 0 1 0 0 0 1 0 0 0 return mx nd np eye n m k d type k war gs
def ein sum operands k war gs r ein sum subscripts operands out none optimize false evaluate the einstein summation convention on the operands use the einstein summation convention many common multi dimensional linear algebraic array operations can be represent in a simple fashion in implicit mode ein sum compute these value in explicit mode ein sum provide further flexibility to compute other array operations that might not be consider classical einstein summation operations by disable or force summation over specify subscript label see the note and examples for clarification parameters subscripts str specify the subscripts for summation as comma separate list of subscript label an implicit classical einstein summation calculation be perform unless the explicit indicator be include as well as subscript label of the precise output form operands list of nd array these be the array for the operation out nd array optional if provide the calculation be do into this array optimize false true optional control if intermediate optimization should occur no optimization will occur if false default to false return output nd array the calculation base on the einstein summation convention note the einstein summation convention can be use to compute many multi dimensional linear algebraic array operations ein sum provide a succinct way of represent these a non exhaustive list of these operations which can be compute by ein sum be show below along with examples trace of an array py fun c np trace return a diagonal py fun c np dia g array axis summations py fun c np sum transpositions and permutations py fun c np transpose matrix multiplication and dot product py fun c np mat mul py fun c np dot vector inner and outer products py fun c np inner py fun c np outer broadcast element wise and scalar multiplication py fun c np multiply tensor contractions py fun c np tensor dot the subscripts string be a comma separate list of subscript label where each label refer to a dimension of the correspond operand whenever a label be repeat it be sum so np ein sum i i a b be equivalent to py fun c np inner a b np inner if a label appear only once it be not sum so np ein sum i a produce a view of a with no change a further example np ein sum ij jk a b describe traditional matrix multiplication and be equivalent to py fun c np mat mul a b np mat mul repeat subscript label in one operand take the diagonal for example np ein sum ii a be equivalent to py fun c np trace a np trace in implicit mode the choose subscripts be important since the ax of the output be reorder alphabetically this mean that np ein sum ij a doesn t affect a 2 d array while np ein sum ji a take its transpose additionally np ein sum ij jk a b return a matrix multiplication while np ein sum ij j h a b return the transpose of the multiplication since subscript h precede subscript i in explicit mode the output can be directly control by specify output subscript label this require the identifier as well as the list of output subscript label this feature increase the flexibility of the function since sum can be disable or force when require the call np ein sum i a be like py fun c np sum a axis 1 np sum and np ein sum ii i a be like py fun c np dia g a np dia g the difference be that ein sum do not allow broadcast by default additionally np ein sum ij j h ih a b directly specify the order of the output subscript label and therefore return matrix multiplication unlike the example above in implicit mode to enable and control broadcast use an ellipsis default num py style broadcast be do by add an ellipsis to the leave of each term like np ein sum ii i a to take the trace along the first and last ax you can do np ein sum i i a or to do a matrix matrix product with the leave most indices instead of rightmost one can do np ein sum ij jk ik a b when there be only one operand no ax be sum and no output parameter be provide a view into the operand be return instead of a new array thus take the diagonal as np ein sum ii i a produce a view the optimize argument which will optimize the contraction order of an ein sum expression for a contraction with three or more operands this can greatly increase the computational efficiency at the cost of a larger memory footprint during computation typically a greedy algorithm be apply which empirical test have show return the optim al path in the majority of case optim al be not support for now this function differ from the original num py ein sum https //docs sci py org/doc/numpy/reference/generated/numpy ein sum html in the follow way s do not support optim al strategy do not support the alternative subscript like ein sum op 0 sublist 0 op 1 sublist 1 sublist out do not produce view in any case examples a np a range 25 reshape 5 5 b np a range 5 c np a range 6 reshape 2 3 trace of a matrix np ein sum ii a array 60 extract the diagonal require explicit form np ein sum ii i a array 0 6 12 18 24 sum over an axis require explicit form np ein sum ij i a array 10 35 60 85 110 np sum a axis 1 array 10 35 60 85 110 for higher dimensional array sum a single axis can be do with ellipsis np ein sum j a array 10 35 60 85 110 compute a matrix transpose or reorder any number of ax np ein sum ji c array 0 3 1 4 2 5 np ein sum ij ji c array 0 3 1 4 2 5 np transpose c array 0 3 1 4 2 5 vector inner products np ein sum i i b b array 30 matrix vector multiplication np ein sum ij j a b array 30 80 130 180 230 np dot a b array 30 80 130 180 230 np ein sum j j a b array 30 80 130 180 230 broadcast and scalar multiplication np ein sum np array 3 c array 0 3 6 9 12 15 np ein sum ij np array 3 c array 0 3 6 9 12 15 np multiply 3 c array 0 3 6 9 12 15 vector outer product np ein sum i j np a range 2 1 b array 0 1 2 3 4 0 2 4 6 8 tensor contraction a np a range 60 reshape 3 4 5 b np a range 24 reshape 4 3 2 np ein sum i jk jil kl a b array 4400 4730 4532 4874 4664 5018 4796 5162 4928 5306 example of ellipsis use a np a range 6 reshape 3 2 b np a range 12 reshape 4 3 np ein sum ki jk ij a b array 10 28 46 64 13 40 67 94 np ein sum ki k i a b array 10 28 46 64 13 40 67 94 np ein sum k jk a b array 10 28 46 64 13 40 67 94 chain array operations for more complicate contractions speed up might be achieve by repeatedly compute a greedy path performance improvements can be particularly significant with larger array a np ones 64 reshape 2 4 8 basic ein sum ~42 22 ms benchmarked on 3 4 ghz intel xeon for iteration in range 500 np ein sum i jk ilm nj m n lk abc a a a a a greedy ein sum faster optim al path approximation ~0 117 ms for iteration in range 500 np ein sum i jk ilm nj m n lk abc a a a a a optimize true return mx nd np ein sum operands k war gs
