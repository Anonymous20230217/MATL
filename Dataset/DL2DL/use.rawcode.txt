class HingeEmbeddingLoss(_Loss):     r"""Measures the loss given an input tensor :math:`x` and a labels tensor :math:`y`     (containing 1 or -1).     This is usually used for measuring whether two inputs are similar or     dissimilar, e.g. using the L1 pairwise distance as :math:`x`, and is typically     used for learning nonlinear embeddings or semi-supervised learning.      The loss function for :math:`n`-th sample in the mini-batch is      .. math::         l_n = \begin{cases}             x_n, & \text{if}\; y_n = 1,\\             \max \{0, \Delta - x_n\}, & \text{if}\; y_n = -1,         \end{cases}      and the total loss functions is      .. math::         \ell(x, y) = \begin{cases}             \operatorname{mean}(L), & \text{if reduction} = \text{`mean';}\\             \operatorname{sum}(L),  & \text{if reduction} = \text{`sum'.}         \end{cases}      where :math:`L = \{l_1,\dots,l_N\}^\top`.      Args:         margin (float, optional): Has a default value of `1`.         size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,             the losses are averaged over each loss element in the batch. Note that for             some losses, there are multiple elements per sample. If the field :attr:`size_average`             is set to ``False``, the losses are instead summed for each minibatch. Ignored             when :attr:`reduce` is ``False``. Default: ``True``         reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the             losses are averaged or summed over observations for each minibatch depending             on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per             batch element instead and ignores :attr:`size_average`. Default: ``True``         reduction (string, optional): Specifies the reduction to apply to the output:             ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,             ``'mean'``: the sum of the output will be divided by the number of             elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`             and :attr:`reduce` are in the process of being deprecated, and in the meantime,             specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``      Shape:         - Input: :math:`(*)` where :math:`*` means, any number of dimensions. The sum operation           operates over all the elements.         - Target: :math:`(*)`, same shape as the input         - Output: scalar. If :attr:`reduction` is ``'none'``, then same shape as the input     """     __constants__ = ['margin', 'reduction']     margin: float      def __init__(self, margin: float = 1.0, size_average=None, reduce=None, reduction: str = 'mean') -> None:         super(HingeEmbeddingLoss, self).__init__(size_average, reduce, reduction)         self.margin = margin      def forward(self, input: Tensor, target: Tensor) -> Tensor:         return F.hinge_embedding_loss(input, target, margin=self.margin, reduction=self.reduction) 
def binary_cross_entropy(     input: Tensor,     target: Tensor,     weight: Optional[Tensor] = None,     size_average: Optional[bool] = None,     reduce: Optional[bool] = None,     reduction: str = "mean", ) -> Tensor:     r"""Function that measures the Binary Cross Entropy     between the target and the output.      See :class:`~torch.nn.BCELoss` for details.      Args:         input: Tensor of arbitrary shape         target: Tensor of the same shape as input         weight (Tensor, optional): a manual rescaling weight                 if provided it's repeated to match input tensor shape         size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,             the losses are averaged over each loss element in the batch. Note that for             some losses, there multiple elements per sample. If the field :attr:`size_average`             is set to ``False``, the losses are instead summed for each minibatch. Ignored             when reduce is ``False``. Default: ``True``         reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the             losses are averaged or summed over observations for each minibatch depending             on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per             batch element instead and ignores :attr:`size_average`. Default: ``True``         reduction (string, optional): Specifies the reduction to apply to the output:             ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,             ``'mean'``: the sum of the output will be divided by the number of             elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`             and :attr:`reduce` are in the process of being deprecated, and in the meantime,             specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``      Examples::          >>> input = torch.randn((3, 2), requires_grad=True)         >>> target = torch.rand((3, 2), requires_grad=False)         >>> loss = F.binary_cross_entropy(F.sigmoid(input), target)         >>> loss.backward()     """     if has_torch_function_variadic(input, target):         return handle_torch_function(             binary_cross_entropy,             (input, target),             input,             target,             weight=weight,             size_average=size_average,             reduce=reduce,             reduction=reduction,         )     if size_average is not None or reduce is not None:         reduction_enum = _Reduction.legacy_get_enum(size_average, reduce)     else:         reduction_enum = _Reduction.get_enum(reduction)     if target.size() != input.size():         raise ValueError(             "Using a target size ({}) that is different to the input size ({}) is deprecated. "             "Please ensure they have the same size.".format(target.size(), input.size())         )      if weight is not None:         new_size = _infer_size(target.size(), weight.size())         weight = weight.expand(new_size)      return torch._C._nn.binary_cross_entropy(input, target, weight, reduction_enum) 
class RNN(RNNBase):     r"""Applies a multi-layer Elman RNN with :math:`\tanh` or :math:`\text{ReLU}` non-linearity to an     input sequence.       For each element in the input sequence, each layer computes the following     function:      .. math::         h_t = \tanh(W_{ih} x_t + b_{ih} + W_{hh} h_{(t-1)} + b_{hh})      where :math:`h_t` is the hidden state at time `t`, :math:`x_t` is     the input at time `t`, and :math:`h_{(t-1)}` is the hidden state of the     previous layer at time `t-1` or the initial hidden state at time `0`.     If :attr:`nonlinearity` is ``'relu'``, then :math:`\text{ReLU}` is used instead of :math:`\tanh`.      Args:         input_size: The number of expected features in the input `x`         hidden_size: The number of features in the hidden state `h`         num_layers: Number of recurrent layers. E.g., setting ``num_layers=2``             would mean stacking two RNNs together to form a `stacked RNN`,             with the second RNN taking in outputs of the first RNN and             computing the final results. Default: 1         nonlinearity: The non-linearity to use. Can be either ``'tanh'`` or ``'relu'``. Default: ``'tanh'``         bias: If ``False``, then the layer does not use bias weights `b_ih` and `b_hh`.             Default: ``True``         batch_first: If ``True``, then the input and output tensors are provided             as `(batch, seq, feature)` instead of `(seq, batch, feature)`.             Note that this does not apply to hidden or cell states. See the             Inputs/Outputs sections below for details.  Default: ``False``         dropout: If non-zero, introduces a `Dropout` layer on the outputs of each             RNN layer except the last layer, with dropout probability equal to             :attr:`dropout`. Default: 0         bidirectional: If ``True``, becomes a bidirectional RNN. Default: ``False``      Inputs: input, h_0         * **input**: tensor of shape :math:`(L, N, H_{in})` when ``batch_first=False`` or           :math:`(N, L, H_{in})` when ``batch_first=True`` containing the features of           the input sequence.  The input can also be a packed variable length sequence.           See :func:`torch.nn.utils.rnn.pack_padded_sequence` or           :func:`torch.nn.utils.rnn.pack_sequence` for details.         * **h_0**: tensor of shape :math:`(D * \text{num\_layers}, N, H_{out})` containing the initial hidden           state for each element in the batch. Defaults to zeros if not provided.          where:          .. math::             \begin{aligned}                 N ={} & \text{batch size} \\                 L ={} & \text{sequence length} \\                 D ={} & 2 \text{ if bidirectional=True otherwise } 1 \\                 H_{in} ={} & \text{input\_size} \\                 H_{out} ={} & \text{hidden\_size}             \end{aligned}      Outputs: output, h_n         * **output**: tensor of shape :math:`(L, N, D * H_{out})` when ``batch_first=False`` or           :math:`(N, L, D * H_{out})` when ``batch_first=True`` containing the output features           `(h_t)` from the last layer of the RNN, for each `t`. If a           :class:`torch.nn.utils.rnn.PackedSequence` has been given as the input, the output           will also be a packed sequence.         * **h_n**: tensor of shape :math:`(D * \text{num\_layers}, N, H_{out})` containing the final hidden state           for each element in the batch.      Attributes:         weight_ih_l[k]: the learnable input-hidden weights of the k-th layer,             of shape `(hidden_size, input_size)` for `k = 0`. Otherwise, the shape is             `(hidden_size, num_directions * hidden_size)`         weight_hh_l[k]: the learnable hidden-hidden weights of the k-th layer,             of shape `(hidden_size, hidden_size)`         bias_ih_l[k]: the learnable input-hidden bias of the k-th layer,             of shape `(hidden_size)`         bias_hh_l[k]: the learnable hidden-hidden bias of the k-th layer,             of shape `(hidden_size)`      .. note::         All the weights and biases are initialized from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})`         where :math:`k = \frac{1}{\text{hidden\_size}}`      .. note::         For bidirectional RNNs, forward and backward are directions 0 and 1 respectively.         Example of splitting the output layers when ``batch_first=False``:         ``output.view(seq_len, batch, num_directions, hidden_size)``.      .. include:: ../cudnn_rnn_determinism.rst      .. include:: ../cudnn_persistent_rnn.rst      Examples::          >>> rnn = nn.RNN(10, 20, 2)         >>> input = torch.randn(5, 3, 10)         >>> h0 = torch.randn(2, 3, 20)         >>> output, hn = rnn(input, h0)     """      def __init__(self, *args, **kwargs):         if 'proj_size' in kwargs:             raise ValueError("proj_size argument is only supported for LSTM, not RNN or GRU")         self.nonlinearity = kwargs.pop('nonlinearity', 'tanh')         if self.nonlinearity == 'tanh':             mode = 'RNN_TANH'         elif self.nonlinearity == 'relu':             mode = 'RNN_RELU'         else:             raise ValueError("Unknown nonlinearity '{}'".format(self.nonlinearity))         super(RNN, self).__init__(mode, *args, **kwargs) 
class GRUCell(RNNCellBase):     r"""A gated recurrent unit (GRU) cell      .. math::          \begin{array}{ll}         r = \sigma(W_{ir} x + b_{ir} + W_{hr} h + b_{hr}) \\         z = \sigma(W_{iz} x + b_{iz} + W_{hz} h + b_{hz}) \\         n = \tanh(W_{in} x + b_{in} + r * (W_{hn} h + b_{hn})) \\         h' = (1 - z) * n + z * h         \end{array}      where :math:`\sigma` is the sigmoid function, and :math:`*` is the Hadamard product.      Args:         input_size: The number of expected features in the input `x`         hidden_size: The number of features in the hidden state `h`         bias: If ``False``, then the layer does not use bias weights `b_ih` and             `b_hh`. Default: ``True``      Inputs: input, hidden         - **input** of shape `(batch, input_size)`: tensor containing input features         - **hidden** of shape `(batch, hidden_size)`: tensor containing the initial hidden           state for each element in the batch.           Defaults to zero if not provided.      Outputs: h'         - **h'** of shape `(batch, hidden_size)`: tensor containing the next hidden state           for each element in the batch      Shape:         - Input1: :math:`(N, H_{in})` tensor containing input features where           :math:`H_{in}` = `input_size`         - Input2: :math:`(N, H_{out})` tensor containing the initial hidden           state for each element in the batch where :math:`H_{out}` = `hidden_size`           Defaults to zero if not provided.         - Output: :math:`(N, H_{out})` tensor containing the next hidden state           for each element in the batch      Attributes:         weight_ih: the learnable input-hidden weights, of shape             `(3*hidden_size, input_size)`         weight_hh: the learnable hidden-hidden weights, of shape             `(3*hidden_size, hidden_size)`         bias_ih: the learnable input-hidden bias, of shape `(3*hidden_size)`         bias_hh: the learnable hidden-hidden bias, of shape `(3*hidden_size)`      .. note::         All the weights and biases are initialized from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})`         where :math:`k = \frac{1}{\text{hidden\_size}}`      Examples::          >>> rnn = nn.GRUCell(10, 20)         >>> input = torch.randn(6, 3, 10)         >>> hx = torch.randn(3, 20)         >>> output = []         >>> for i in range(6):                 hx = rnn(input[i], hx)                 output.append(hx)     """      def __init__(self, input_size: int, hidden_size: int, bias: bool = True,                  device=None, dtype=None) -> None:         factory_kwargs = {'device': device, 'dtype': dtype}         super(GRUCell, self).__init__(input_size, hidden_size, bias, num_chunks=3, **factory_kwargs)      def forward(self, input: Tensor, hx: Optional[Tensor] = None) -> Tensor:         if hx is None:             hx = torch.zeros(input.size(0), self.hidden_size, dtype=input.dtype, device=input.device)         return _VF.gru_cell(             input, hx,             self.weight_ih, self.weight_hh,             self.bias_ih, self.bias_hh,         ) 
class Embedding(Module):     r"""A simple lookup table that stores embeddings of a fixed dictionary and size.      This module is often used to store word embeddings and retrieve them using indices.     The input to the module is a list of indices, and the output is the corresponding     word embeddings.      Args:         num_embeddings (int): size of the dictionary of embeddings         embedding_dim (int): the size of each embedding vector         padding_idx (int, optional): If specified, the entries at :attr:`padding_idx` do not contribute to the gradient;                                      therefore, the embedding vector at :attr:`padding_idx` is not updated during training,                                      i.e. it remains as a fixed "pad". For a newly constructed Embedding,                                      the embedding vector at :attr:`padding_idx` will default to all zeros,                                      but can be updated to another value to be used as the padding vector.         max_norm (float, optional): If given, each embedding vector with norm larger than :attr:`max_norm`                                     is renormalized to have norm :attr:`max_norm`.         norm_type (float, optional): The p of the p-norm to compute for the :attr:`max_norm` option. Default ``2``.         scale_grad_by_freq (boolean, optional): If given, this will scale gradients by the inverse of frequency of                                                 the words in the mini-batch. Default ``False``.         sparse (bool, optional): If ``True``, gradient w.r.t. :attr:`weight` matrix will be a sparse tensor.                                  See Notes for more details regarding sparse gradients.      Attributes:         weight (Tensor): the learnable weights of the module of shape (num_embeddings, embedding_dim)                          initialized from :math:`\mathcal{N}(0, 1)`      Shape:         - Input: :math:`(*)`, IntTensor or LongTensor of arbitrary shape containing the indices to extract         - Output: :math:`(*, H)`, where `*` is the input shape and :math:`H=\text{embedding\_dim}`      .. note::         Keep in mind that only a limited number of optimizers support         sparse gradients: currently it's :class:`optim.SGD` (`CUDA` and `CPU`),         :class:`optim.SparseAdam` (`CUDA` and `CPU`) and :class:`optim.Adagrad` (`CPU`)      .. note::         When :attr:`max_norm` is not ``None``, :class:`Embedding`'s forward method will modify the         :attr:`weight` tensor in-place. Since tensors needed for gradient computations cannot be         modified in-place, performing a differentiable operation on ``Embedding.weight`` before         calling :class:`Embedding`'s forward method requires cloning ``Embedding.weight`` when         :attr:`max_norm` is not ``None``. For example::              n, d, m = 3, 5, 7             embedding = nn.Embedding(n, d, max_norm=True)             W = torch.randn((m, d), requires_grad=True)             idx = torch.tensor([1, 2])             a = embedding.weight.clone() @ W.t()  # weight must be cloned for this to be differentiable             b = embedding(idx) @ W.t()  # modifies weight in-place             out = (a.unsqueeze(0) + b.unsqueeze(1))             loss = out.sigmoid().prod()             loss.backward()      Examples::          >>> # an Embedding module containing 10 tensors of size 3         >>> embedding = nn.Embedding(10, 3)         >>> # a batch of 2 samples of 4 indices each         >>> input = torch.LongTensor([[1,2,4,5],[4,3,2,9]])         >>> embedding(input)         tensor([[[-0.0251, -1.6902,  0.7172],                  [-0.6431,  0.0748,  0.6969],                  [ 1.4970,  1.3448, -0.9685],                  [-0.3677, -2.7265, -0.1685]],                  [[ 1.4970,  1.3448, -0.9685],                  [ 0.4362, -0.4004,  0.9400],                  [-0.6431,  0.0748,  0.6969],                  [ 0.9124, -2.3616,  1.1151]]])           >>> # example with padding_idx         >>> embedding = nn.Embedding(10, 3, padding_idx=0)         >>> input = torch.LongTensor([[0,2,0,5]])         >>> embedding(input)         tensor([[[ 0.0000,  0.0000,  0.0000],                  [ 0.1535, -2.0309,  0.9315],                  [ 0.0000,  0.0000,  0.0000],                  [-0.1655,  0.9897,  0.0635]]])          >>> # example of changing `pad` vector         >>> padding_idx = 0         >>> embedding = nn.Embedding(3, 3, padding_idx=padding_idx)         >>> embedding.weight         Parameter containing:         tensor([[ 0.0000,  0.0000,  0.0000],                 [-0.7895, -0.7089, -0.0364],                 [ 0.6778,  0.5803,  0.2678]], requires_grad=True)         >>> with torch.no_grad():         ...     embedding.weight[padding_idx] = torch.ones(3)         >>> embedding.weight         Parameter containing:         tensor([[ 1.0000,  1.0000,  1.0000],                 [-0.7895, -0.7089, -0.0364],                 [ 0.6778,  0.5803,  0.2678]], requires_grad=True)     """     __constants__ = ['num_embeddings', 'embedding_dim', 'padding_idx', 'max_norm',                      'norm_type', 'scale_grad_by_freq', 'sparse']      num_embeddings: int     embedding_dim: int     padding_idx: Optional[int]     max_norm: Optional[float]     norm_type: float     scale_grad_by_freq: bool     weight: Tensor     sparse: bool      def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: Optional[int] = None,                  max_norm: Optional[float] = None, norm_type: float = 2., scale_grad_by_freq: bool = False,                  sparse: bool = False, _weight: Optional[Tensor] = None,                  device=None, dtype=None) -> None:         factory_kwargs = {'device': device, 'dtype': dtype}         super(Embedding, self).__init__()         self.num_embeddings = num_embeddings         self.embedding_dim = embedding_dim         if padding_idx is not None:             if padding_idx > 0:                 assert padding_idx < self.num_embeddings, 'Padding_idx must be within num_embeddings'             elif padding_idx < 0:                 assert padding_idx >= -self.num_embeddings, 'Padding_idx must be within num_embeddings'                 padding_idx = self.num_embeddings + padding_idx         self.padding_idx = padding_idx         self.max_norm = max_norm         self.norm_type = norm_type         self.scale_grad_by_freq = scale_grad_by_freq         if _weight is None:             self.weight = Parameter(torch.empty((num_embeddings, embedding_dim), **factory_kwargs))             self.reset_parameters()         else:             assert list(_weight.shape) == [num_embeddings, embedding_dim], \                 'Shape of weight does not match num_embeddings and embedding_dim'             self.weight = Parameter(_weight)          self.sparse = sparse      def reset_parameters(self) -> None:         init.normal_(self.weight)         self._fill_padding_idx_with_zero()      def _fill_padding_idx_with_zero(self) -> None:         if self.padding_idx is not None:             with torch.no_grad():                 self.weight[self.padding_idx].fill_(0)      def forward(self, input: Tensor) -> Tensor:         return F.embedding(             input, self.weight, self.padding_idx, self.max_norm,             self.norm_type, self.scale_grad_by_freq, self.sparse)      def extra_repr(self) -> str:         s = '{num_embeddings}, {embedding_dim}'         if self.padding_idx is not None:             s += ', padding_idx={padding_idx}'         if self.max_norm is not None:             s += ', max_norm={max_norm}'         if self.norm_type != 2:             s += ', norm_type={norm_type}'         if self.scale_grad_by_freq is not False:             s += ', scale_grad_by_freq={scale_grad_by_freq}'         if self.sparse is not False:             s += ', sparse=True'         return s.format(**self.__dict__)      @classmethod     def from_pretrained(cls, embeddings, freeze=True, padding_idx=None,                         max_norm=None, norm_type=2., scale_grad_by_freq=False,                         sparse=False):         r"""Creates Embedding instance from given 2-dimensional FloatTensor.          Args:             embeddings (Tensor): FloatTensor containing weights for the Embedding.                 First dimension is being passed to Embedding as ``num_embeddings``, second as ``embedding_dim``.             freeze (boolean, optional): If ``True``, the tensor does not get updated in the learning process.                 Equivalent to ``embedding.weight.requires_grad = False``. Default: ``True``             padding_idx (int, optional): If specified, the entries at :attr:`padding_idx` do not contribute to the gradient;                                          therefore, the embedding vector at :attr:`padding_idx` is not updated during training,                                          i.e. it remains as a fixed "pad".             max_norm (float, optional): See module initialization documentation.             norm_type (float, optional): See module initialization documentation. Default ``2``.             scale_grad_by_freq (boolean, optional): See module initialization documentation. Default ``False``.             sparse (bool, optional): See module initialization documentation.          Examples::              >>> # FloatTensor containing pretrained weights             >>> weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])             >>> embedding = nn.Embedding.from_pretrained(weight)             >>> # Get embeddings for index 1             >>> input = torch.LongTensor([1])             >>> embedding(input)             tensor([[ 4.0000,  5.1000,  6.3000]])         """         assert embeddings.dim() == 2, \             'Embeddings parameter is expected to be 2-dimensional'         rows, cols = embeddings.shape         embedding = cls(             num_embeddings=rows,             embedding_dim=cols,             _weight=embeddings,             padding_idx=padding_idx,             max_norm=max_norm,             norm_type=norm_type,             scale_grad_by_freq=scale_grad_by_freq,             sparse=sparse)         embedding.weight.requires_grad = not freeze         return embedding 
class ELU(Module):     r"""Applies the element-wise function:      .. math::         \text{ELU}(x) = \begin{cases}         x, & \text{ if } x > 0\\         \alpha * (\exp(x) - 1), & \text{ if } x \leq 0         \end{cases}      Args:         alpha: the :math:`\alpha` value for the ELU formulation. Default: 1.0         inplace: can optionally do the operation in-place. Default: ``False``      Shape:         - Input: :math:`(N, *)` where `*` means, any number of additional           dimensions         - Output: :math:`(N, *)`, same shape as the input      .. image:: ../scripts/activation_images/ELU.png      Examples::          >>> m = nn.ELU()         >>> input = torch.randn(2)         >>> output = m(input)     """     __constants__ = ['alpha', 'inplace']     alpha: float     inplace: bool      def __init__(self, alpha: float = 1., inplace: bool = False) -> None:         super(ELU, self).__init__()         self.alpha = alpha         self.inplace = inplace      def forward(self, input: Tensor) -> Tensor:         return F.elu(input, self.alpha, self.inplace)      def extra_repr(self) -> str:         inplace_str = ', inplace=True' if self.inplace else ''         return 'alpha={}{}'.format(self.alpha, inplace_str) 
class Dropout(_DropoutNd):     r"""During training, randomly zeroes some of the elements of the input     tensor with probability :attr:`p` using samples from a Bernoulli     distribution. Each channel will be zeroed out independently on every forward     call.      This has proven to be an effective technique for regularization and     preventing the co-adaptation of neurons as described in the paper     `Improving neural networks by preventing co-adaptation of feature     detectors`_ .      Furthermore, the outputs are scaled by a factor of :math:`\frac{1}{1-p}` during     training. This means that during evaluation the module simply computes an     identity function.      Args:         p: probability of an element to be zeroed. Default: 0.5         inplace: If set to ``True``, will do this operation in-place. Default: ``False``      Shape:         - Input: :math:`(*)`. Input can be of any shape         - Output: :math:`(*)`. Output is of the same shape as input      Examples::          >>> m = nn.Dropout(p=0.2)         >>> input = torch.randn(20, 16)         >>> output = m(input)      .. _Improving neural networks by preventing co-adaptation of feature         detectors: https://arxiv.org/abs/1207.0580     """      def forward(self, input: Tensor) -> Tensor:         return F.dropout(input, self.p, self.training, self.inplace) 
def tensordot(a, b, dims=2, out: Optional[torch.Tensor] = None):  # noqa: F811     r"""Returns a contraction of a and b over multiple dimensions.      :attr:`tensordot` implements a generalized matrix product.      Args:       a (Tensor): Left tensor to contract       b (Tensor): Right tensor to contract       dims (int or Tuple[List[int], List[int]] or List[List[int]] containing two lists or Tensor): number of dimensions to          contract or explicit lists of dimensions for :attr:`a` and          :attr:`b` respectively      When called with a non-negative integer argument :attr:`dims` = :math:`d`, and     the number of dimensions of :attr:`a` and :attr:`b` is :math:`m` and :math:`n`,     respectively, :func:`~torch.tensordot` computes      .. math::         r_{i_0,...,i_{m-d}, i_d,...,i_n}           = \sum_{k_0,...,k_{d-1}} a_{i_0,...,i_{m-d},k_0,...,k_{d-1}} \times b_{k_0,...,k_{d-1}, i_d,...,i_n}.      When called with :attr:`dims` of the list form, the given dimensions will be contracted     in place of the last :math:`d` of :attr:`a` and the first :math:`d` of :math:`b`. The sizes     in these dimensions must match, but :func:`~torch.tensordot` will deal with broadcasted     dimensions.      Examples::          >>> a = torch.arange(60.).reshape(3, 4, 5)         >>> b = torch.arange(24.).reshape(4, 3, 2)         >>> torch.tensordot(a, b, dims=([1, 0], [0, 1]))         tensor([[4400., 4730.],                 [4532., 4874.],                 [4664., 5018.],                 [4796., 5162.],                 [4928., 5306.]])          >>> a = torch.randn(3, 4, 5, device='cuda')         >>> b = torch.randn(4, 5, 6, device='cuda')         >>> c = torch.tensordot(a, b, dims=2).cpu()         tensor([[ 8.3504, -2.5436,  6.2922,  2.7556, -1.0732,  3.2741],                 [ 3.3161,  0.0704,  5.0187, -0.4079, -4.3126,  4.8744],                 [ 0.8223,  3.9445,  3.2168, -0.2400,  3.4117,  1.7780]])          >>> a = torch.randn(3, 5, 4, 6)         >>> b = torch.randn(6, 4, 5, 3)         >>> torch.tensordot(a, b, dims=([2, 1, 3], [1, 2, 0]))         tensor([[  7.7193,  -2.4867, -10.3204],                 [  1.5513, -14.4737,  -6.5113],                 [ -0.2850,   4.2573,  -3.5997]])     """     if has_torch_function_variadic(a, b):         return handle_torch_function(tensordot, (a, b), a, b, dims=dims)      dims_a: List[int] = []     dims_b: List[int] = []      if isinstance(dims, (tuple, list)):         dims_a, dims_b = dims      if isinstance(dims, torch.Tensor):         num_elements = dims.numel()         if num_elements > 1:             assert dims.size()[0] == 2             dims_a = torch.jit.annotate(List[int], dims[0].tolist())             dims_b = torch.jit.annotate(List[int], dims[1].tolist())         else:             dims_val = int(dims.item())             if dims_val < 0:                 raise RuntimeError(f"tensordot expects dims >= 0, but got dims={dims}")             dims_a = list(range(-dims_val, 0))             dims_b = list(range(dims_val))      if isinstance(dims, int):         if dims < 0:             raise RuntimeError(f"tensordot expects dims >= 0, but got dims={dims}")         dims_a = list(range(-dims, 0))         dims_b = list(range(dims))      if len(dims_a) == 0 or len(dims_b) == 0:         raise RuntimeError(f"unsupported input to tensordot, got dims={dims}")      if out is None:         return _VF.tensordot(a, b, dims_a, dims_b)  # type: ignore[attr-defined]     else:         return _VF.tensordot(a, b, dims_a, dims_b, out=out)  # type: ignore[attr-defined] 
def tensordot(a, b, dims=2, out: Optional[torch.Tensor] = None):  # noqa: F811     r"""Returns a contraction of a and b over multiple dimensions.      :attr:`tensordot` implements a generalized matrix product.      Args:       a (Tensor): Left tensor to contract       b (Tensor): Right tensor to contract       dims (int or Tuple[List[int], List[int]] or List[List[int]] containing two lists or Tensor): number of dimensions to          contract or explicit lists of dimensions for :attr:`a` and          :attr:`b` respectively      When called with a non-negative integer argument :attr:`dims` = :math:`d`, and     the number of dimensions of :attr:`a` and :attr:`b` is :math:`m` and :math:`n`,     respectively, :func:`~torch.tensordot` computes      .. math::         r_{i_0,...,i_{m-d}, i_d,...,i_n}           = \sum_{k_0,...,k_{d-1}} a_{i_0,...,i_{m-d},k_0,...,k_{d-1}} \times b_{k_0,...,k_{d-1}, i_d,...,i_n}.      When called with :attr:`dims` of the list form, the given dimensions will be contracted     in place of the last :math:`d` of :attr:`a` and the first :math:`d` of :math:`b`. The sizes     in these dimensions must match, but :func:`~torch.tensordot` will deal with broadcasted     dimensions.      Examples::          >>> a = torch.arange(60.).reshape(3, 4, 5)         >>> b = torch.arange(24.).reshape(4, 3, 2)         >>> torch.tensordot(a, b, dims=([1, 0], [0, 1]))         tensor([[4400., 4730.],                 [4532., 4874.],                 [4664., 5018.],                 [4796., 5162.],                 [4928., 5306.]])          >>> a = torch.randn(3, 4, 5, device='cuda')         >>> b = torch.randn(4, 5, 6, device='cuda')         >>> c = torch.tensordot(a, b, dims=2).cpu()         tensor([[ 8.3504, -2.5436,  6.2922,  2.7556, -1.0732,  3.2741],                 [ 3.3161,  0.0704,  5.0187, -0.4079, -4.3126,  4.8744],                 [ 0.8223,  3.9445,  3.2168, -0.2400,  3.4117,  1.7780]])          >>> a = torch.randn(3, 5, 4, 6)         >>> b = torch.randn(6, 4, 5, 3)         >>> torch.tensordot(a, b, dims=([2, 1, 3], [1, 2, 0]))         tensor([[  7.7193,  -2.4867, -10.3204],                 [  1.5513, -14.4737,  -6.5113],                 [ -0.2850,   4.2573,  -3.5997]])     """     if has_torch_function_variadic(a, b):         return handle_torch_function(tensordot, (a, b), a, b, dims=dims)      dims_a: List[int] = []     dims_b: List[int] = []      if isinstance(dims, (tuple, list)):         dims_a, dims_b = dims      if isinstance(dims, torch.Tensor):         num_elements = dims.numel()         if num_elements > 1:             assert dims.size()[0] == 2             dims_a = torch.jit.annotate(List[int], dims[0].tolist())             dims_b = torch.jit.annotate(List[int], dims[1].tolist())         else:             dims_val = int(dims.item())             if dims_val < 0:                 raise RuntimeError(f"tensordot expects dims >= 0, but got dims={dims}")             dims_a = list(range(-dims_val, 0))             dims_b = list(range(dims_val))      if isinstance(dims, int):         if dims < 0:             raise RuntimeError(f"tensordot expects dims >= 0, but got dims={dims}")         dims_a = list(range(-dims, 0))         dims_b = list(range(dims))      if len(dims_a) == 0 or len(dims_b) == 0:         raise RuntimeError(f"unsupported input to tensordot, got dims={dims}")      if out is None:         return _VF.tensordot(a, b, dims_a, dims_b)  # type: ignore[attr-defined]     else:         return _VF.tensordot(a, b, dims_a, dims_b, out=out)  # type: ignore[attr-defined] 
class ConvTranspose3d(_ConvTransposeNd):     __doc__ = r"""Applies a 3D transposed convolution operator over an input image composed of several input     planes.     The transposed convolution operator multiplies each input value element-wise by a learnable kernel,     and sums over the outputs from all input feature planes.      This module can be seen as the gradient of Conv3d with respect to its input.     It is also known as a fractionally-strided convolution or     a deconvolution (although it is not an actual deconvolution operation).      This module supports :ref:`TensorFloat32<tf32_on_ampere>`.      * :attr:`stride` controls the stride for the cross-correlation.      * :attr:`padding` controls the amount of implicit zero padding on both       sides for ``dilation * (kernel_size - 1) - padding`` number of points. See note       below for details.      * :attr:`output_padding` controls the additional size added to one side       of the output shape. See note below for details.      * :attr:`dilation` controls the spacing between the kernel points; also known as the ид trous algorithm.       It is harder to describe, but this `link`_ has a nice visualization of what :attr:`dilation` does.      {groups_note}      The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`output_padding`     can either be:          - a single ``int`` -- in which case the same value is used for the depth, height and width dimensions         - a ``tuple`` of three ints -- in which case, the first `int` is used for the depth dimension,           the second `int` for the height dimension and the third `int` for the width dimension      Note:         The :attr:`padding` argument effectively adds ``dilation * (kernel_size - 1) - padding``         amount of zero padding to both sizes of the input. This is set so that         when a :class:`~torch.nn.Conv3d` and a :class:`~torch.nn.ConvTranspose3d`         are initialized with same parameters, they are inverses of each other in         regard to the input and output shapes. However, when ``stride > 1``,         :class:`~torch.nn.Conv3d` maps multiple input shapes to the same output         shape. :attr:`output_padding` is provided to resolve this ambiguity by         effectively increasing the calculated output shape on one side. Note         that :attr:`output_padding` is only used to find output shape, but does         not actually add zero-padding to output.      Note:         {cudnn_reproducibility_note}      Args:         in_channels (int): Number of channels in the input image         out_channels (int): Number of channels produced by the convolution         kernel_size (int or tuple): Size of the convolving kernel         stride (int or tuple, optional): Stride of the convolution. Default: 1         padding (int or tuple, optional): ``dilation * (kernel_size - 1) - padding`` zero-padding             will be added to both sides of each dimension in the input. Default: 0         output_padding (int or tuple, optional): Additional size added to one side             of each dimension in the output shape. Default: 0         groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1         bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``         dilation (int or tuple, optional): Spacing between kernel elements. Default: 1     """.format(**reproducibility_notes, **convolution_notes) + r"""      Shape:         - Input: :math:`(N, C_{in}, D_{in}, H_{in}, W_{in})`         - Output: :math:`(N, C_{out}, D_{out}, H_{out}, W_{out})` where          .. math::               D_{out} = (D_{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0] + \text{dilation}[0]                         \times (\text{kernel\_size}[0] - 1) + \text{output\_padding}[0] + 1         .. math::               H_{out} = (H_{in} - 1) \times \text{stride}[1] - 2 \times \text{padding}[1] + \text{dilation}[1]                         \times (\text{kernel\_size}[1] - 1) + \text{output\_padding}[1] + 1         .. math::               W_{out} = (W_{in} - 1) \times \text{stride}[2] - 2 \times \text{padding}[2] + \text{dilation}[2]                         \times (\text{kernel\_size}[2] - 1) + \text{output\_padding}[2] + 1       Attributes:         weight (Tensor): the learnable weights of the module of shape                          :math:`(\text{in\_channels}, \frac{\text{out\_channels}}{\text{groups}},`                          :math:`\text{kernel\_size[0]}, \text{kernel\_size[1]}, \text{kernel\_size[2]})`.                          The values of these weights are sampled from                          :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where                          :math:`k = \frac{groups}{C_\text{out} * \prod_{i=0}^{2}\text{kernel\_size}[i]}`         bias (Tensor):   the learnable bias of the module of shape (out_channels)                          If :attr:`bias` is ``True``, then the values of these weights are                          sampled from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where                          :math:`k = \frac{groups}{C_\text{out} * \prod_{i=0}^{2}\text{kernel\_size}[i]}`      Examples::          >>> # With square kernels and equal stride         >>> m = nn.ConvTranspose3d(16, 33, 3, stride=2)         >>> # non-square kernels and unequal stride and with padding         >>> m = nn.ConvTranspose3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(0, 4, 2))         >>> input = torch.randn(20, 16, 10, 50, 100)         >>> output = m(input)      .. _cross-correlation:         https://en.wikipedia.org/wiki/Cross-correlation      .. _link:         https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md     """      def __init__(         self,         in_channels: int,         out_channels: int,         kernel_size: _size_3_t,         stride: _size_3_t = 1,         padding: _size_3_t = 0,         output_padding: _size_3_t = 0,         groups: int = 1,         bias: bool = True,         dilation: _size_3_t = 1,         padding_mode: str = 'zeros',         device=None,         dtype=None     ) -> None:         factory_kwargs = {'device': device, 'dtype': dtype}         kernel_size = _triple(kernel_size)         stride = _triple(stride)         padding = _triple(padding)         dilation = _triple(dilation)         output_padding = _triple(output_padding)         super(ConvTranspose3d, self).__init__(             in_channels, out_channels, kernel_size, stride, padding, dilation,             True, output_padding, groups, bias, padding_mode, **factory_kwargs)      def forward(self, input: Tensor, output_size: Optional[List[int]] = None) -> Tensor:         if self.padding_mode != 'zeros':             raise ValueError('Only `zeros` padding mode is supported for ConvTranspose3d')          assert isinstance(self.padding, tuple)         # One cannot replace List by Tuple or Sequence in "_output_padding" because         # TorchScript does not support `Sequence[T]` or `Tuple[T, ...]`.         output_padding = self._output_padding(             input, output_size, self.stride, self.padding, self.kernel_size, self.dilation)  # type: ignore[arg-type]          return F.conv_transpose3d(             input, self.weight, self.bias, self.stride, self.padding,             output_padding, self.groups, self.dilation) 
class Conv3d(_ConvNd):     __doc__ = r"""Applies a 3D convolution over an input signal composed of several input     planes.      In the simplest case, the output value of the layer with input size :math:`(N, C_{in}, D, H, W)`     and output :math:`(N, C_{out}, D_{out}, H_{out}, W_{out})` can be precisely described as:      .. math::         out(N_i, C_{out_j}) = bias(C_{out_j}) +                                 \sum_{k = 0}^{C_{in} - 1} weight(C_{out_j}, k) \star input(N_i, k)      where :math:`\star` is the valid 3D `cross-correlation`_ operator     """ + r"""      This module supports :ref:`TensorFloat32<tf32_on_ampere>`.      * :attr:`stride` controls the stride for the cross-correlation.      * :attr:`padding` controls the amount of padding applied to the input. It       can be either a string {{'valid', 'same'}} or a tuple of ints giving the       amount of implicit padding applied on both sides.      * :attr:`dilation` controls the spacing between the kernel points; also known as the ид trous algorithm.       It is harder to describe, but this `link`_ has a nice visualization of what :attr:`dilation` does.      {groups_note}      The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`dilation` can either be:          - a single ``int`` -- in which case the same value is used for the depth, height and width dimension         - a ``tuple`` of three ints -- in which case, the first `int` is used for the depth dimension,           the second `int` for the height dimension and the third `int` for the width dimension      Note:         {depthwise_separable_note}      Note:         {cudnn_reproducibility_note}      Note:         ``padding='valid'`` is the same as no padding. ``padding='same'`` pads         the input so the output has the shape as the input. However, this mode         doesn't support any stride values other than 1.      Args:         in_channels (int): Number of channels in the input image         out_channels (int): Number of channels produced by the convolution         kernel_size (int or tuple): Size of the convolving kernel         stride (int or tuple, optional): Stride of the convolution. Default: 1         padding (int, tuple or str, optional): Padding added to all six sides of             the input. Default: 0         padding_mode (string, optional): ``'zeros'``, ``'reflect'``, ``'replicate'`` or ``'circular'``. Default: ``'zeros'``         dilation (int or tuple, optional): Spacing between kernel elements. Default: 1         groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1         bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``     """.format(**reproducibility_notes, **convolution_notes) + r"""      Shape:         - Input: :math:`(N, C_{in}, D_{in}, H_{in}, W_{in})`         - Output: :math:`(N, C_{out}, D_{out}, H_{out}, W_{out})` where            .. math::               D_{out} = \left\lfloor\frac{D_{in} + 2 \times \text{padding}[0] - \text{dilation}[0]                     \times (\text{kernel\_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor            .. math::               H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[1] - \text{dilation}[1]                     \times (\text{kernel\_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor            .. math::               W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[2] - \text{dilation}[2]                     \times (\text{kernel\_size}[2] - 1) - 1}{\text{stride}[2]} + 1\right\rfloor      Attributes:         weight (Tensor): the learnable weights of the module of shape                          :math:`(\text{out\_channels}, \frac{\text{in\_channels}}{\text{groups}},`                          :math:`\text{kernel\_size[0]}, \text{kernel\_size[1]}, \text{kernel\_size[2]})`.                          The values of these weights are sampled from                          :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where                          :math:`k = \frac{groups}{C_\text{in} * \prod_{i=0}^{2}\text{kernel\_size}[i]}`         bias (Tensor):   the learnable bias of the module of shape (out_channels). If :attr:`bias` is ``True``,                          then the values of these weights are                          sampled from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where                          :math:`k = \frac{groups}{C_\text{in} * \prod_{i=0}^{2}\text{kernel\_size}[i]}`      Examples::          >>> # With square kernels and equal stride         >>> m = nn.Conv3d(16, 33, 3, stride=2)         >>> # non-square kernels and unequal stride and with padding         >>> m = nn.Conv3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(4, 2, 0))         >>> input = torch.randn(20, 16, 10, 50, 100)         >>> output = m(input)      .. _cross-correlation:         https://en.wikipedia.org/wiki/Cross-correlation      .. _link:         https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md     """      def __init__(         self,         in_channels: int,         out_channels: int,         kernel_size: _size_3_t,         stride: _size_3_t = 1,         padding: Union[str, _size_3_t] = 0,         dilation: _size_3_t = 1,         groups: int = 1,         bias: bool = True,         padding_mode: str = 'zeros',         device=None,         dtype=None     ) -> None:         factory_kwargs = {'device': device, 'dtype': dtype}         kernel_size_ = _triple(kernel_size)         stride_ = _triple(stride)         padding_ = padding if isinstance(padding, str) else _triple(padding)         dilation_ = _triple(dilation)         super(Conv3d, self).__init__(             in_channels, out_channels, kernel_size_, stride_, padding_, dilation_,             False, _triple(0), groups, bias, padding_mode, **factory_kwargs)      def _conv_forward(self, input: Tensor, weight: Tensor, bias: Optional[Tensor]):         if self.padding_mode != "zeros":             return F.conv3d(                 F.pad(                     input, self._reversed_padding_repeated_twice, mode=self.padding_mode                 ),                 weight,                 bias,                 self.stride,                 _triple(0),                 self.dilation,                 self.groups,             )         return F.conv3d(             input, weight, bias, self.stride, self.padding, self.dilation, self.groups         )      def forward(self, input: Tensor) -> Tensor:         return self._conv_forward(input, self.weight, self.bias) 
class ConvTranspose2d(_ConvTransposeNd):     __doc__ = r"""Applies a 2D transposed convolution operator over an input image     composed of several input planes.      This module can be seen as the gradient of Conv2d with respect to its input.     It is also known as a fractionally-strided convolution or     a deconvolution (although it is not an actual deconvolution operation).      This module supports :ref:`TensorFloat32<tf32_on_ampere>`.      * :attr:`stride` controls the stride for the cross-correlation.      * :attr:`padding` controls the amount of implicit zero padding on both       sides for ``dilation * (kernel_size - 1) - padding`` number of points. See note       below for details.      * :attr:`output_padding` controls the additional size added to one side       of the output shape. See note below for details.      * :attr:`dilation` controls the spacing between the kernel points; also known as the ид trous algorithm.       It is harder to describe, but this `link`_ has a nice visualization of what :attr:`dilation` does.      {groups_note}      The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`output_padding`     can either be:          - a single ``int`` -- in which case the same value is used for the height and width dimensions         - a ``tuple`` of two ints -- in which case, the first `int` is used for the height dimension,           and the second `int` for the width dimension      Note:         The :attr:`padding` argument effectively adds ``dilation * (kernel_size - 1) - padding``         amount of zero padding to both sizes of the input. This is set so that         when a :class:`~torch.nn.Conv2d` and a :class:`~torch.nn.ConvTranspose2d`         are initialized with same parameters, they are inverses of each other in         regard to the input and output shapes. However, when ``stride > 1``,         :class:`~torch.nn.Conv2d` maps multiple input shapes to the same output         shape. :attr:`output_padding` is provided to resolve this ambiguity by         effectively increasing the calculated output shape on one side. Note         that :attr:`output_padding` is only used to find output shape, but does         not actually add zero-padding to output.      Note:         {cudnn_reproducibility_note}      Args:         in_channels (int): Number of channels in the input image         out_channels (int): Number of channels produced by the convolution         kernel_size (int or tuple): Size of the convolving kernel         stride (int or tuple, optional): Stride of the convolution. Default: 1         padding (int or tuple, optional): ``dilation * (kernel_size - 1) - padding`` zero-padding             will be added to both sides of each dimension in the input. Default: 0         output_padding (int or tuple, optional): Additional size added to one side             of each dimension in the output shape. Default: 0         groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1         bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``         dilation (int or tuple, optional): Spacing between kernel elements. Default: 1     """.format(**reproducibility_notes, **convolution_notes) + r"""      Shape:         - Input: :math:`(N, C_{in}, H_{in}, W_{in})`         - Output: :math:`(N, C_{out}, H_{out}, W_{out})` where          .. math::               H_{out} = (H_{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0] + \text{dilation}[0]                         \times (\text{kernel\_size}[0] - 1) + \text{output\_padding}[0] + 1         .. math::               W_{out} = (W_{in} - 1) \times \text{stride}[1] - 2 \times \text{padding}[1] + \text{dilation}[1]                         \times (\text{kernel\_size}[1] - 1) + \text{output\_padding}[1] + 1      Attributes:         weight (Tensor): the learnable weights of the module of shape                          :math:`(\text{in\_channels}, \frac{\text{out\_channels}}{\text{groups}},`                          :math:`\text{kernel\_size[0]}, \text{kernel\_size[1]})`.                          The values of these weights are sampled from                          :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where                          :math:`k = \frac{groups}{C_\text{out} * \prod_{i=0}^{1}\text{kernel\_size}[i]}`         bias (Tensor):   the learnable bias of the module of shape (out_channels)                          If :attr:`bias` is ``True``, then the values of these weights are                          sampled from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where                          :math:`k = \frac{groups}{C_\text{out} * \prod_{i=0}^{1}\text{kernel\_size}[i]}`      Examples::          >>> # With square kernels and equal stride         >>> m = nn.ConvTranspose2d(16, 33, 3, stride=2)         >>> # non-square kernels and unequal stride and with padding         >>> m = nn.ConvTranspose2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))         >>> input = torch.randn(20, 16, 50, 100)         >>> output = m(input)         >>> # exact output size can be also specified as an argument         >>> input = torch.randn(1, 16, 12, 12)         >>> downsample = nn.Conv2d(16, 16, 3, stride=2, padding=1)         >>> upsample = nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1)         >>> h = downsample(input)         >>> h.size()         torch.Size([1, 16, 6, 6])         >>> output = upsample(h, output_size=input.size())         >>> output.size()         torch.Size([1, 16, 12, 12])      .. _cross-correlation:         https://en.wikipedia.org/wiki/Cross-correlation      .. _link:         https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md     """      def __init__(         self,         in_channels: int,         out_channels: int,         kernel_size: _size_2_t,         stride: _size_2_t = 1,         padding: _size_2_t = 0,         output_padding: _size_2_t = 0,         groups: int = 1,         bias: bool = True,         dilation: int = 1,         padding_mode: str = 'zeros',         device=None,         dtype=None     ) -> None:         factory_kwargs = {'device': device, 'dtype': dtype}         kernel_size = _pair(kernel_size)         stride = _pair(stride)         padding = _pair(padding)         dilation = _pair(dilation)         output_padding = _pair(output_padding)         super(ConvTranspose2d, self).__init__(             in_channels, out_channels, kernel_size, stride, padding, dilation,             True, output_padding, groups, bias, padding_mode, **factory_kwargs)      def forward(self, input: Tensor, output_size: Optional[List[int]] = None) -> Tensor:         if self.padding_mode != 'zeros':             raise ValueError('Only `zeros` padding mode is supported for ConvTranspose2d')          assert isinstance(self.padding, tuple)         # One cannot replace List by Tuple or Sequence in "_output_padding" because         # TorchScript does not support `Sequence[T]` or `Tuple[T, ...]`.         output_padding = self._output_padding(             input, output_size, self.stride, self.padding, self.kernel_size, self.dilation)  # type: ignore[arg-type]          return F.conv_transpose2d(             input, self.weight, self.bias, self.stride, self.padding,             output_padding, self.groups, self.dilation) 
class BatchNorm1d(_BatchNorm):     r"""Applies Batch Normalization over a 2D or 3D input (a mini-batch of 1D     inputs with optional additional channel dimension) as described in the paper     `Batch Normalization: Accelerating Deep Network Training by Reducing     Internal Covariate Shift <https://arxiv.org/abs/1502.03167>`__ .      .. math::          y = \frac{x - \mathrm{E}[x]}{\sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta      The mean and standard-deviation are calculated per-dimension over     the mini-batches and :math:`\gamma` and :math:`\beta` are learnable parameter vectors     of size `C` (where `C` is the input size). By default, the elements of :math:`\gamma` are set     to 1 and the elements of :math:`\beta` are set to 0. The standard-deviation is calculated     via the biased estimator, equivalent to `torch.var(input, unbiased=False)`.      Also by default, during training this layer keeps running estimates of its     computed mean and variance, which are then used for normalization during     evaluation. The running estimates are kept with a default :attr:`momentum`     of 0.1.      If :attr:`track_running_stats` is set to ``False``, this layer then does not     keep running estimates, and batch statistics are instead used during     evaluation time as well.      .. note::         This :attr:`momentum` argument is different from one used in optimizer         classes and the conventional notion of momentum. Mathematically, the         update rule for running statistics here is         :math:`\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t`,         where :math:`\hat{x}` is the estimated statistic and :math:`x_t` is the         new observed value.      Because the Batch Normalization is done over the `C` dimension, computing statistics     on `(N, L)` slices, it's common terminology to call this Temporal Batch Normalization.      Args:         num_features: :math:`C` from an expected input of size             :math:`(N, C, L)` or :math:`L` from input of size :math:`(N, L)`         eps: a value added to the denominator for numerical stability.             Default: 1e-5         momentum: the value used for the running_mean and running_var             computation. Can be set to ``None`` for cumulative moving average             (i.e. simple average). Default: 0.1         affine: a boolean value that when set to ``True``, this module has             learnable affine parameters. Default: ``True``         track_running_stats: a boolean value that when set to ``True``, this             module tracks the running mean and variance, and when set to ``False``,             this module does not track such statistics, and initializes statistics             buffers :attr:`running_mean` and :attr:`running_var` as ``None``.             When these buffers are ``None``, this module always uses batch statistics.             in both training and eval modes. Default: ``True``      Shape:         - Input: :math:`(N, C)` or :math:`(N, C, L)`         - Output: :math:`(N, C)` or :math:`(N, C, L)` (same shape as input)      Examples::          >>> # With Learnable Parameters         >>> m = nn.BatchNorm1d(100)         >>> # Without Learnable Parameters         >>> m = nn.BatchNorm1d(100, affine=False)         >>> input = torch.randn(20, 100)         >>> output = m(input)     """      def _check_input_dim(self, input):         if input.dim() != 2 and input.dim() != 3:             raise ValueError(                 "expected 2D or 3D input (got {}D input)".format(input.dim())             ) 
class AvgPool3d(_AvgPoolNd):     r"""Applies a 3D average pooling over an input signal composed of several input     planes.      In the simplest case, the output value of the layer with input size :math:`(N, C, D, H, W)`,     output :math:`(N, C, D_{out}, H_{out}, W_{out})` and :attr:`kernel_size` :math:`(kD, kH, kW)`     can be precisely described as:      .. math::         \begin{aligned}             \text{out}(N_i, C_j, d, h, w) ={} & \sum_{k=0}^{kD-1} \sum_{m=0}^{kH-1} \sum_{n=0}^{kW-1} \\                                               & \frac{\text{input}(N_i, C_j, \text{stride}[0] \times d + k,                                                       \text{stride}[1] \times h + m, \text{stride}[2] \times w + n)}                                                      {kD \times kH \times kW}         \end{aligned}      If :attr:`padding` is non-zero, then the input is implicitly zero-padded on all three sides     for :attr:`padding` number of points.      Note:         When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding         or the input. Sliding windows that would start in the right padded region are ignored.      The parameters :attr:`kernel_size`, :attr:`stride` can either be:          - a single ``int`` -- in which case the same value is used for the depth, height and width dimension         - a ``tuple`` of three ints -- in which case, the first `int` is used for the depth dimension,           the second `int` for the height dimension and the third `int` for the width dimension      Args:         kernel_size: the size of the window         stride: the stride of the window. Default value is :attr:`kernel_size`         padding: implicit zero padding to be added on all three sides         ceil_mode: when True, will use `ceil` instead of `floor` to compute the output shape         count_include_pad: when True, will include the zero-padding in the averaging calculation         divisor_override: if specified, it will be used as divisor, otherwise :attr:`kernel_size` will be used      Shape:         - Input: :math:`(N, C, D_{in}, H_{in}, W_{in})`         - Output: :math:`(N, C, D_{out}, H_{out}, W_{out})`, where            .. math::               D_{out} = \left\lfloor\frac{D_{in} + 2 \times \text{padding}[0] -                     \text{kernel\_size}[0]}{\text{stride}[0]} + 1\right\rfloor            .. math::               H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[1] -                     \text{kernel\_size}[1]}{\text{stride}[1]} + 1\right\rfloor            .. math::               W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[2] -                     \text{kernel\_size}[2]}{\text{stride}[2]} + 1\right\rfloor      Examples::          >>> # pool of square window of size=3, stride=2         >>> m = nn.AvgPool3d(3, stride=2)         >>> # pool of non-square window         >>> m = nn.AvgPool3d((3, 2, 2), stride=(2, 1, 2))         >>> input = torch.randn(20, 16, 50,44, 31)         >>> output = m(input)     """     __constants__ = ['kernel_size', 'stride', 'padding', 'ceil_mode', 'count_include_pad', 'divisor_override']      kernel_size: _size_3_t     stride: _size_3_t     padding: _size_3_t     ceil_mode: bool     count_include_pad: bool      def __init__(self, kernel_size: _size_3_t, stride: Optional[_size_3_t] = None, padding: _size_3_t = 0,                  ceil_mode: bool = False, count_include_pad: bool = True, divisor_override: Optional[int] = None) -> None:         super(AvgPool3d, self).__init__()         self.kernel_size = kernel_size         self.stride = stride if (stride is not None) else kernel_size         self.padding = padding         self.ceil_mode = ceil_mode         self.count_include_pad = count_include_pad         self.divisor_override = divisor_override      def forward(self, input: Tensor) -> Tensor:         return F.avg_pool3d(input, self.kernel_size, self.stride,                             self.padding, self.ceil_mode, self.count_include_pad, self.divisor_override)      def __setstate__(self, d):         super(AvgPool3d, self).__setstate__(d)         self.__dict__.setdefault('padding', 0)         self.__dict__.setdefault('ceil_mode', False)         self.__dict__.setdefault('count_include_pad', True) 
class AvgPool2d(_AvgPoolNd):     r"""Applies a 2D average pooling over an input signal composed of several input     planes.      In the simplest case, the output value of the layer with input size :math:`(N, C, H, W)`,     output :math:`(N, C, H_{out}, W_{out})` and :attr:`kernel_size` :math:`(kH, kW)`     can be precisely described as:      .. math::          out(N_i, C_j, h, w)  = \frac{1}{kH * kW} \sum_{m=0}^{kH-1} \sum_{n=0}^{kW-1}                                input(N_i, C_j, stride[0] \times h + m, stride[1] \times w + n)      If :attr:`padding` is non-zero, then the input is implicitly zero-padded on both sides     for :attr:`padding` number of points.      Note:         When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding         or the input. Sliding windows that would start in the right padded region are ignored.      The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding` can either be:          - a single ``int`` -- in which case the same value is used for the height and width dimension         - a ``tuple`` of two ints -- in which case, the first `int` is used for the height dimension,           and the second `int` for the width dimension      Args:         kernel_size: the size of the window         stride: the stride of the window. Default value is :attr:`kernel_size`         padding: implicit zero padding to be added on both sides         ceil_mode: when True, will use `ceil` instead of `floor` to compute the output shape         count_include_pad: when True, will include the zero-padding in the averaging calculation         divisor_override: if specified, it will be used as divisor, otherwise :attr:`kernel_size` will be used      Shape:         - Input: :math:`(N, C, H_{in}, W_{in})`         - Output: :math:`(N, C, H_{out}, W_{out})`, where            .. math::               H_{out} = \left\lfloor\frac{H_{in}  + 2 \times \text{padding}[0] -                 \text{kernel\_size}[0]}{\text{stride}[0]} + 1\right\rfloor            .. math::               W_{out} = \left\lfloor\frac{W_{in}  + 2 \times \text{padding}[1] -                 \text{kernel\_size}[1]}{\text{stride}[1]} + 1\right\rfloor      Examples::          >>> # pool of square window of size=3, stride=2         >>> m = nn.AvgPool2d(3, stride=2)         >>> # pool of non-square window         >>> m = nn.AvgPool2d((3, 2), stride=(2, 1))         >>> input = torch.randn(20, 16, 50, 32)         >>> output = m(input)     """     __constants__ = ['kernel_size', 'stride', 'padding', 'ceil_mode', 'count_include_pad', 'divisor_override']      kernel_size: _size_2_t     stride: _size_2_t     padding: _size_2_t     ceil_mode: bool     count_include_pad: bool      def __init__(self, kernel_size: _size_2_t, stride: Optional[_size_2_t] = None, padding: _size_2_t = 0,                  ceil_mode: bool = False, count_include_pad: bool = True, divisor_override: Optional[int] = None) -> None:         super(AvgPool2d, self).__init__()         self.kernel_size = kernel_size         self.stride = stride if (stride is not None) else kernel_size         self.padding = padding         self.ceil_mode = ceil_mode         self.count_include_pad = count_include_pad         self.divisor_override = divisor_override      def forward(self, input: Tensor) -> Tensor:         return F.avg_pool2d(input, self.kernel_size, self.stride,                             self.padding, self.ceil_mode, self.count_include_pad, self.divisor_override) 
class AvgPool1d(_AvgPoolNd):     r"""Applies a 1D average pooling over an input signal composed of several     input planes.      In the simplest case, the output value of the layer with input size :math:`(N, C, L)`,     output :math:`(N, C, L_{out})` and :attr:`kernel_size` :math:`k`     can be precisely described as:      .. math::          \text{out}(N_i, C_j, l) = \frac{1}{k} \sum_{m=0}^{k-1}                                \text{input}(N_i, C_j, \text{stride} \times l + m)      If :attr:`padding` is non-zero, then the input is implicitly zero-padded on both sides     for :attr:`padding` number of points.      Note:         When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding         or the input. Sliding windows that would start in the right padded region are ignored.      The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding` can each be     an ``int`` or a one-element tuple.      Args:         kernel_size: the size of the window         stride: the stride of the window. Default value is :attr:`kernel_size`         padding: implicit zero padding to be added on both sides         ceil_mode: when True, will use `ceil` instead of `floor` to compute the output shape         count_include_pad: when True, will include the zero-padding in the averaging calculation      Shape:         - Input: :math:`(N, C, L_{in})`         - Output: :math:`(N, C, L_{out})`, where            .. math::               L_{out} = \left\lfloor \frac{L_{in} +               2 \times \text{padding} - \text{kernel\_size}}{\text{stride}} + 1\right\rfloor      Examples::          >>> # pool with window of size=3, stride=2         >>> m = nn.AvgPool1d(3, stride=2)         >>> m(torch.tensor([[[1.,2,3,4,5,6,7]]]))         tensor([[[ 2.,  4.,  6.]]])     """      kernel_size: _size_1_t     stride: _size_1_t     padding: _size_1_t     ceil_mode: bool     count_include_pad: bool      def __init__(self, kernel_size: _size_1_t, stride: _size_1_t = None, padding: _size_1_t = 0, ceil_mode: bool = False,                  count_include_pad: bool = True) -> None:         super(AvgPool1d, self).__init__()         self.kernel_size = _single(kernel_size)         self.stride = _single(stride if stride is not None else kernel_size)         self.padding = _single(padding)         self.ceil_mode = ceil_mode         self.count_include_pad = count_include_pad      def forward(self, input: Tensor) -> Tensor:         return F.avg_pool1d(             input, self.kernel_size, self.stride, self.padding, self.ceil_mode,             self.count_include_pad) 
def selu(input: Tensor, inplace: bool = False) -> Tensor:     r"""selu(input, inplace=False) -> Tensor      Applies element-wise,     :math:`\text{SELU}(x) = scale * (\max(0,x) + \min(0, \alpha * (\exp(x) - 1)))`,     with :math:`\alpha=1.6732632423543772848170429916717` and     :math:`scale=1.0507009873554804934193349852946`.      See :class:`~torch.nn.SELU` for more details.     """     if has_torch_function_unary(input):         return handle_torch_function(selu, (input,), input, inplace=inplace)     if inplace:         result = torch.selu_(input)     else:         result = torch.selu(input)     return result 
def relu(input: Tensor, inplace: bool = False) -> Tensor:     r"""relu(input, inplace=False) -> Tensor      Applies the rectified linear unit function element-wise. See     :class:`~torch.nn.ReLU` for more details.     """     if has_torch_function_unary(input):         return handle_torch_function(relu, (input,), input, inplace=inplace)     if inplace:         result = torch.relu_(input)     else:         result = torch.relu(input)     return result 
def elu(input: Tensor, alpha: float = 1.0, inplace: bool = False) -> Tensor:     r"""Applies element-wise,     :math:`\text{ELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x) - 1))`.      See :class:`~torch.nn.ELU` for more details.     """     if has_torch_function_unary(input):         return handle_torch_function(elu, (input,), input, alpha=alpha, inplace=inplace)     if inplace:         result = torch._C._nn.elu_(input, alpha)     else:         result = torch._C._nn.elu(input, alpha)     return result 
def load(f, map_location=None, pickle_module=pickle, **pickle_load_args):     # Reference: https://github.com/pytorch/pytorch/issues/54354     # The first line of this docstring overrides the one Sphinx generates for the     # documentation. We need it so that Sphinx doesn't leak `pickle`s path from     # the build environment (e.g. `<module 'pickle' from '/leaked/path').      """load(f, map_location=None, pickle_module=pickle, **pickle_load_args)      Loads an object saved with :func:`torch.save` from a file.      :func:`torch.load` uses Python's unpickling facilities but treats storages,     which underlie tensors, specially. They are first deserialized on the     CPU and are then moved to the device they were saved from. If this fails     (e.g. because the run time system doesn't have certain devices), an exception     is raised. However, storages can be dynamically remapped to an alternative     set of devices using the :attr:`map_location` argument.      If :attr:`map_location` is a callable, it will be called once for each serialized     storage with two arguments: storage and location. The storage argument     will be the initial deserialization of the storage, residing on the CPU.     Each serialized storage has a location tag associated with it which     identifies the device it was saved from, and this tag is the second     argument passed to :attr:`map_location`. The builtin location tags are ``'cpu'``     for CPU tensors and ``'cuda:device_id'`` (e.g. ``'cuda:2'``) for CUDA tensors.     :attr:`map_location` should return either ``None`` or a storage. If     :attr:`map_location` returns a storage, it will be used as the final deserialized     object, already moved to the right device. Otherwise, :func:`torch.load` will     fall back to the default behavior, as if :attr:`map_location` wasn't specified.      If :attr:`map_location` is a :class:`torch.device` object or a string containing     a device tag, it indicates the location where all tensors should be loaded.      Otherwise, if :attr:`map_location` is a dict, it will be used to remap location tags     appearing in the file (keys), to ones that specify where to put the     storages (values).      User extensions can register their own location tags and tagging and     deserialization methods using :func:`torch.serialization.register_package`.      Args:         f: a file-like object (has to implement :meth:`read`, :meth:`readline`, :meth:`tell`, and :meth:`seek`),             or a string or os.PathLike object containing a file name         map_location: a function, :class:`torch.device`, string or a dict specifying how to remap storage             locations         pickle_module: module used for unpickling metadata and objects (has to             match the :attr:`pickle_module` used to serialize file)         pickle_load_args: (Python 3 only) optional keyword arguments passed over to             :func:`pickle_module.load` and :func:`pickle_module.Unpickler`, e.g.,             :attr:`errors=...`.      .. warning::         :func:`torch.load()` uses ``pickle`` module implicitly, which is known to be insecure.         It is possible to construct malicious pickle data which will execute arbitrary code         during unpickling. Never load data that could have come from an untrusted         source, or that could have been tampered with. **Only load data you trust**.      .. note::         When you call :func:`torch.load()` on a file which contains GPU tensors, those tensors         will be loaded to GPU by default. You can call ``torch.load(.., map_location='cpu')``         and then :meth:`load_state_dict` to avoid GPU RAM surge when loading a model checkpoint.      .. note::         By default, we decode byte strings as ``utf-8``.  This is to avoid a common error         case ``UnicodeDecodeError: 'ascii' codec can't decode byte 0x...``         when loading files saved by Python 2 in Python 3.  If this default         is incorrect, you may use an extra :attr:`encoding` keyword argument to specify how         these objects should be loaded, e.g., :attr:`encoding='latin1'` decodes them         to strings using ``latin1`` encoding, and :attr:`encoding='bytes'` keeps them         as byte arrays which can be decoded later with ``byte_array.decode(...)``.      Example:         >>> torch.load('tensors.pt')         # Load all tensors onto the CPU         >>> torch.load('tensors.pt', map_location=torch.device('cpu'))         # Load all tensors onto the CPU, using a function         >>> torch.load('tensors.pt', map_location=lambda storage, loc: storage)         # Load all tensors onto GPU 1         >>> torch.load('tensors.pt', map_location=lambda storage, loc: storage.cuda(1))         # Map tensors from GPU 1 to GPU 0         >>> torch.load('tensors.pt', map_location={'cuda:1':'cuda:0'})         # Load tensor from io.BytesIO object         >>> with open('tensor.pt', 'rb') as f:         ...     buffer = io.BytesIO(f.read())         >>> torch.load(buffer)         # Load a module with 'ascii' encoding for unpickling         >>> torch.load('module.pt', encoding='ascii')     """     _check_dill_version(pickle_module)      if 'encoding' not in pickle_load_args.keys():         pickle_load_args['encoding'] = 'utf-8'      with _open_file_like(f, 'rb') as opened_file:         if _is_zipfile(opened_file):             # The zipfile reader is going to advance the current file position.             # If we want to actually tail call to torch.jit.load, we need to             # reset back to the original position.             orig_position = opened_file.tell()             with _open_zipfile_reader(opened_file) as opened_zipfile:                 if _is_torchscript_zip(opened_zipfile):                     warnings.warn("'torch.load' received a zip file that looks like a TorchScript archive"                                   " dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to"                                   " silence this warning)", UserWarning)                     opened_file.seek(orig_position)                     return torch.jit.load(opened_file)                 return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)         return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args) 