{
 "cntk.ops.depth_to_space": {
  "code": "@typemap\ndef depth_to_space(operand, block_size, name=''):\n    '''\n    Rearranges elements in the input tensor from the depth dimension into spatial blocks.\n\n    This operation is useful for implementing sub-pixel convolution that is part of models  \n    for image super-resolution (see [1]). It rearranges elements of an input tensor of shape \n    (Cxbxb, H, W) to a tensor of shape (C, bxH, bxW), where b is the `block_size`.\n    \n    Example:\n        >>> x = np.array(np.reshape(range(8), (8, 1, 1)), dtype=np.float32)\n        >>> x = np.tile(x, (1, 2, 3))\n        >>> a = C.input_variable((8, 2, 3))\n        >>> d2s_op = C.depth_to_space(a, block_size=2)\n        >>> d2s_op.eval({a:x})\n        array([[[[ 0.,  2.,  0.,  2.,  0.,  2.],\n                 [ 4.,  6.,  4.,  6.,  4.,  6.],\n                 [ 0.,  2.,  0.,  2.,  0.,  2.],\n                 [ 4.,  6.,  4.,  6.,  4.,  6.]],\n        <BLANKLINE>\n                [[ 1.,  3.,  1.,  3.,  1.,  3.],\n                 [ 5.,  7.,  5.,  7.,  5.,  7.],\n                 [ 1.,  3.,  1.,  3.,  1.,  3.],\n                 [ 5.,  7.,  5.,  7.,  5.,  7.]]]], dtype=float32)\n\n    Args:\n        operand: Input tensor, with dimensions :math:`[C \\\\times H \\\\times W]`.\n        block_size (int): Integer value. This defines the size of the spatial block where the \n         depth elements move to. Number of channels, C, in the input tensor must be divisible \n         by math:`(block_size \\\\times block_size)`\n        name (str, optional): the name of the Function instance in the network\n    Returns:\n        :class:`~cntk.ops.functions.Function`\n\n    See also:\n        [1] W. Shi et. al. `: Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network <https://arxiv.org/abs/1609.05158>`_.\n    '''\n    from cntk.cntk_py import depth_to_space\n    operand = sanitize_input(operand, get_data_type(operand))\n    if not float(block_size).is_integer():\n        raise ValueError('block_size must be an integer.')\n    return depth_to_space(operand, block_size, name)\n",
  "sig": "depth_to_space(operand|operand: Input tensor, with dimensions :math:`[C \\\\times H \\\\times W]`)(block_size|block_size (int): Integer value. This defines the size of the spatial block where the         depth elements move to. Number of channels, C, in the input tensor must be divisible          by math:`(block_size \\\\times block_size)`)(name|name (str, optional): the name of the Function instance in the network)",
  "dst": "    Rearranges elements in the input tensor from the depth dimension into spatial blocks.    This operation is useful for implementing sub-pixel convolution that is part of models      for image super-resolution (see [1]). It rearranges elements of an input tensor of shape     (Cxbxb, H, W) to a tensor of shape (C, bxH, bxW), where b is the `block_size`."
 },
 "cntk.random.bernoulli": {
  "code": "@typemap\ndef bernoulli(shape, dtype=default_override_or(np.float32), mean=0.5, seed=auto_select, name=''):\n    \"\"\"bernoulli(shape, dtype=default_override_or(np.float32), mean=0.5, seed=auto_select, name='')\n    Generates samples from the Bernoulli distribution with success probability `mean`.\n\n    Args:\n        shape (tuple): shape of the output (entries are independent random draws)\n        dtype (np.float32 or np.float64 or np.float16): data type. Default is np.float32.\n        mean (float): success probability\n        seed (int): pseudo random number generator seed (default: automatically select a unique seed)\n        name (str, optional): the name of the Function instance in the network\n\n    Returns:\n        :class:`~cntk.ops.functions.Function`\n\n    Examples:\n        >>> b = C.random.bernoulli((2,3), seed=98052)\n        >>> b.eval(device=C.cpu()) # explicitly setting cpu because this is tested on multiple platforms; leave it unspecified in your code\n        array([[ 1.,  1.,  0.],\n               [ 1.,  0.,  0.]], dtype=float32)\n    \"\"\"\n    from cntk.cntk_py import bernoulli_random\n    shape, dtype = sanitize_random_args(shape, dtype)\n    return bernoulli_random(shape, dtype, mean, seed, name)\n",
  "sig": "bernoulli(shape|shape (tuple): shape of the output (entries are independent random draws))(dtype|dtype (np.float32 or np.float64 or np.float16): data type. Default is np.float32.)(mean|mean (float): success probability)(seed|seed (int): pseudo random number generator seed (default: automatically select a unique seed))(name|name (str, optional): the name of the Function instance in the network)",
  "dst": "    Generates samples from the Bernoulli distribution with success probability `mean`."
 },
 "cntk.random.normal": {
  "code": "@typemap\ndef normal(shape, dtype=default_override_or(np.float32), mean=0.0, scale=1.0, seed=auto_select, name=''):\n    \"\"\"normal(shape, dtype=default_override_or(np.float32), mean=0.0, scale=1.0, seed=auto_select, name='')\n    Generates samples from the normal distribution with mean `mean` and standard deviation `scale`.\n\n    Args:\n        shape (tuple): shape of the output (entries are independent random draws)\n        dtype (np.float32 or np.float64 or np.float16): data type. Default is np.float32.\n        mean (float): mean of the distribution\n        scale (float): scale (standard deviation) of the distribution\n        seed (int): pseudo random number generator seed (default: automatically select a unique seed)\n        name (str, optional): the name of the Function instance in the network\n\n    Returns:\n        :class:`~cntk.ops.functions.Function`\n\n    Examples:\n        >>> z = C.random.normal((2,3), seed=98052)\n        >>> z.eval(device=C.cpu()) # explicitly setting cpu because this is tested on multiple platforms; leave it unspecified in your code\n        array([[ 1.803254,  0.995395, -0.631974],\n               [-1.736721,  0.005615, -0.340025]], dtype=float32)\n    \"\"\"\n    from cntk.cntk_py import normal_random\n    shape, dtype = sanitize_random_args(shape, dtype)\n    return normal_random(shape, dtype, mean, scale, seed, name)\n",
  "sig": "normal(shape|shape (tuple): shape of the output (entries are independent random draws))(dtype|dtype (np.float32 or np.float64 or np.float16): data type. Default is np.float32.)(mean|mean (float): mean of the distribution)(scale|scale (float): scale (standard deviation) of the distribution)(seed|seed (int): pseudo random number generator seed (default: automatically select a unique seed))(name|name (str, optional): the name of the Function instance in the network)",
  "dst": "    Generates samples from the normal distribution with mean `mean` and standard deviation `scale`."
 },
 "cntk.random.uniform": {
  "code": "@typemap\ndef uniform(shape, dtype=default_override_or(np.float32), low=0.0, high=1.0, seed=auto_select, name=''):\n    \"\"\"uniform(shape, dtype=default_override_or(np.float32), low=0.0, high=1.0, seed=auto_select, name='')\n    Generates samples from the uniform distribution in the interval [`low`,`high`).\n\n    Args:\n        shape (tuple): shape of the output (entries are independent random draws)\n        dtype (np.float32 or np.float64 or np.float16): data type. Default is np.float32.\n        low (float): lower end of the range of the random numbers\n        high (float): upper end of the range of the random numbers\n        seed (int): pseudo random number generator seed (default: automatically select a unique seed)\n        name (str, optional): the name of the Function instance in the network\n\n    Returns:\n        :class:`~cntk.ops.functions.Function`\n\n    Examples:\n        >>> u = C.random.uniform((2,3), seed=98052)\n        >>> u.eval(device=C.cpu()) # explicitly setting cpu because this is tested on multiple platforms; leave it unspecified in your code\n        array([[ 0.931785,  0.814722,  0.479606],\n               [ 0.937468,  0.004351,  0.185131]], dtype=float32)\n\n    \"\"\"\n    from cntk.cntk_py import uniform_random\n    shape, dtype = sanitize_random_args(shape, dtype)\n    return uniform_random(shape, dtype, low, high, seed, name)\n",
  "sig": "uniform(shape|shape (tuple): shape of the output (entries are independent random draws))(dtype|dtype (np.float32 or np.float64 or np.float16): data type. Default is np.float32.)(low|low (float): lower end of the range of the random numbers)(high|high (float): upper end of the range of the random numbers)(seed|seed (int): pseudo random number generator seed (default: automatically select a unique seed))(name|name (str, optional): the name of the Function instance in the network)",
  "dst": "    Generates samples from the uniform distribution in the interval [`low`,`high`)."
 },
 "cntk.initializer.he_normal": {
  "code": "def he_normal(scale=DefaultParamInitScale, output_rank=SentinelValueForInferParamInitRank, filter_rank=SentinelValueForInferParamInitRank, seed=None):\n    '''\n    initializer\n\n    Args:\n        scale (float): scale\n        output_rank (int): output rank\n        filter_rank (int): filter rank\n        seed (int): random seed\n\n    Returns:\n        initializer for :class:`~cntk.variables.Parameter`\n        initialized to Gaussian distribution with mean `0` and standard\n        deviation $$scale*sqrt(2.0/fanIn)$$\n    '''\n    if seed is None:\n        seed = SentinelValueForAutoSelectRandomSeed\n\n    return cntk_py.he_normal_initializer(scale, output_rank, filter_rank, seed)\n",
  "sig": "he_normal(scale|scale (float): scale)(output_rank|output_rank (int): output rank)(filter_rank|filter_rank (int): filter rank)(seed|seed (int): random seed)",
  "dst": "    initializer"
 },
 "cntk.initializer.he_uniform": {
  "code": "def he_uniform(scale=DefaultParamInitScale, output_rank=SentinelValueForInferParamInitRank, filter_rank=SentinelValueForInferParamInitRank, seed=None):\n    '''\n    initializer\n\n    Args:\n        scale (float): scale\n        output_rank (int): output rank\n        filter_rank (int): filter rank\n        seed (int): random seed\n\n    Returns:\n        initializer for :class:`~cntk.variables.Parameter`\n        initialized to uniform distribution between\n        $$scale*sqrt(6.0/fanIn)*[-1,1]$$\n    '''\n    if seed is None:\n        seed = SentinelValueForAutoSelectRandomSeed\n\n    return cntk_py.he_uniform_initializer(scale, output_rank, filter_rank, seed)\n",
  "sig": "he_uniform(scale|scale (float): scale)(output_rank|output_rank (int): output rank)(filter_rank|filter_rank (int): filter rank)(seed|seed (int): random seed)",
  "dst": "    initializer"
 },
 "cntk.initializer.glorot_normal": {
  "code": "def glorot_normal(scale=DefaultParamInitScale, output_rank=SentinelValueForInferParamInitRank, filter_rank=SentinelValueForInferParamInitRank, seed=None):\n    '''\n    initializer\n\n    Args:\n        scale (float): scale\n        output_rank (int): output rank\n        filter_rank (int): filter rank\n        seed (int): random seed\n\n    Returns:\n        initializer for :class:`~cntk.variables.Parameter`\n        initialized to Gaussian distribution with mean `0` and standard\n        deviation $$scale*sqrt(2.0/(fanIn+fanOut))$$\n    '''\n    if seed is None:\n        seed = SentinelValueForAutoSelectRandomSeed\n\n    return cntk_py.glorot_normal_initializer(scale, output_rank, filter_rank, seed)\n",
  "sig": "glorot_normal(scale|scale (float): scale)(output_rank|output_rank (int): output rank)(filter_rank|filter_rank (int): filter rank)(seed|seed (int): random seed)",
  "dst": "    initializer"
 },
 "cntk.initializer.glorot_uniform": {
  "code": "def glorot_uniform(scale=DefaultParamInitScale, output_rank=SentinelValueForInferParamInitRank, filter_rank=SentinelValueForInferParamInitRank, seed=None):\n    '''\n    Glorot initializer\n\n    Args:\n        scale (float): scale\n        output_rank (int): output rank\n        filter_rank (int): filter rank\n        seed (int): random seed\n\n    Returns:\n        initializer for :class:`~cntk.variables.Parameter`\n        initialized to uniform distribution between\n        $$scale*sqrt(6.0/(fanIn+fanOut))*[-1,1]$$\n    '''\n    if seed is None:\n        seed = SentinelValueForAutoSelectRandomSeed\n\n    return cntk_py.glorot_uniform_initializer(scale, output_rank, filter_rank, seed)\n",
  "sig": "glorot_uniform(scale|scale (float): scale)(output_rank|output_rank (int): output rank)(filter_rank|filter_rank (int): filter rank)(seed|seed (int): random seed)",
  "dst": "    Glorot initializer"
 },
 "cntk.initializer.normal": {
  "code": "def normal(scale, output_rank=SentinelValueForInferParamInitRank, filter_rank=SentinelValueForInferParamInitRank, seed=None):\n    '''\n    Normal initializer\n\n    Args:\n        scale (float): scale\n        output_rank (int): output rank\n        filter_rank (int): filter rank\n        seed (int): random seed\n\n    Returns:\n        initializer for :class:`~cntk.variables.Parameter`\n        initialized to normal distribution with mean `0` and standard deviation `scale`.\n    '''\n    if seed is None:\n        seed = SentinelValueForAutoSelectRandomSeed\n\n    return cntk_py.normal_initializer(scale, output_rank, filter_rank, seed)\n",
  "sig": "normal(scale|scale (float): scale)(output_rank|output_rank (int): output rank)(filter_rank|filter_rank (int): filter rank)(seed|seed (int): random seed)",
  "dst": "    Normal initializer"
 },
 "cntk.initializer.uniform": {
  "code": "def uniform(scale, seed=None):\n    '''\n    Uniform initializer\n\n    Args:\n        scale (float): scale\n        seed (int): random seed\n\n    Returns:\n        initializer for :class:`~cntk.variables.Parameter`\n        initialized to uniform distribution between `scale*[-1.0, 1.0]`.\n        Note: this maps to the \"uniform1\" distribution in BrainScript.\n    '''\n    if seed is None:\n        seed = SentinelValueForAutoSelectRandomSeed\n\n    return cntk_py.uniform_initializer(scale, seed)\n",
  "sig": "uniform(scale|scale (float): scale)(seed|seed (int): random seed)",
  "dst": "    Uniform initializer"
 },
 "cntk.layers.layers.batch_normalization": {
  "code": "@typemap\ndef batch_normalization(operand, scale, bias, running_mean, running_inv_std, spatial,\n                        normalization_time_constant=5000, blend_time_constant=0,\n                        epsilon=0.00001, use_cudnn_engine=False, disable_regularization=False, name='', running_count=None):\n    # TODO: running_count should be right after running_inv_std; no need for upwards compat\n    '''\n    Normalizes layer outputs for every minibatch for each output (feature) independently\n    and applies affine transformation to preserve representation of the layer.\n\n    Args:\n        operand: input of the batch normalization operation\n        scale: parameter tensor that holds the learned componentwise-scaling factors\n        bias: parameter tensor that holds the learned bias. ``scale`` and ``bias`` must have the same\n         dimensions which must be equal to the input dimensions in case of ``spatial`` = False or\n         number of output convolution feature maps in case of ``spatial`` = True\n        running_mean: running mean which is used during evaluation phase and might be used during\n         training as well. You must pass a constant tensor with initial value 0 and the same dimensions\n         as ``scale`` and ``bias``\n        running_inv_std: running variance. Represented as ``running_mean``\n        running_count: Denotes the total number of samples that have been used so far to compute\n         the ``running_mean`` and ``running_inv_std`` parameters. You must pass a scalar (either rank-0 ``constant(val)``).\n        spatial(bool): flag that indicates whether to compute mean/var for each feature in a minibatch\n         independently or, in case of convolutional layers, per future map\n        normalization_time_constant(float, default 5000): time constant for computing running average of\n         mean and variance as a low-pass filtered version of the batch statistics.\n        blend_time_constant(float, default 0): constant for smoothing batch estimates with the running\n         statistics\n        epsilon: conditioner constant added to the variance when computing the inverse standard deviation\n        use_cudnn_engine(bool, default False):\n        name (str, optional): the name of the Function instance in the network\n        disable_regularization(bool, default False): turn off regularization in batch normalization\n    Returns:\n        :class:`~cntk.ops.functions.Function`\n    '''\n    if running_count is None:\n        running_count = constant(0)\n        import warnings\n        warnings.warn(\"batch_normalization requires an additional \"\n            \"'running_count' parameter, which can be \"\n            \"instantiated as 'constant(0)'\", Warning)\n\n    from cntk.cntk_py import batch_normalization\n    operand = sanitize_input(operand)\n    return batch_normalization(operand, scale, bias, running_mean, running_inv_std, running_count, spatial,\n                               normalization_time_constant, blend_time_constant,\n                               epsilon, use_cudnn_engine, disable_regularization, name)\n",
  "sig": "batch_normalization(operand|operand: input of the batch normalization operation)(scale|scale: parameter tensor that holds the learned componentwise-scaling factors)(bias|bias: parameter tensor that holds the learned bias. ``scale`` and ``bias`` must have the same         dimensions which must be equal to the input dimensions in case of ``spatial`` = False or         number of output convolution feature maps in case of ``spatial`` = True)(running_mean|running_mean: running mean which is used during evaluation phase and might be used during         training as well. You must pass a constant tensor with initial value 0 and the same dimensions         as ``scale`` and ``bias``)(running_inv_std|running_inv_std: running variance. Represented as ``running_mean``)(spatial|spatial(bool): flag that indicates whether to compute mean/var for each feature in a minibatch         independently or, in case of convolutional layers, per future map)(normalization_time_constant|normalization_time_constant(float, default 5000): time constant for computing running average of         mean and variance as a low-pass filtered version of the batch statistics.)(blend_time_constant|blend_time_constant(float, default 0): constant for smoothing batch estimates with the running         statistics)(epsilon|epsilon: conditioner constant added to the variance when computing the inverse standard deviation)(use_cudnn_engine|use_cudnn_engine(bool, default False):)(disable_regularization|disable_regularization(bool, default False): turn off regularization in batch normalization)(name|name (str, optional): the name of the Function instance in the network)(running_count|running_count: Denotes the total number of samples that have been used so far to compute         the ``running_mean`` and ``running_inv_std`` parameters. You must pass a scalar (either rank-0 ``constant(val)``).)",
  "dst": "    Normalizes layer outputs for every minibatch for each output (feature) independently    and applies affine transformation to preserve representation of the layer."
 },
 "cntk.cntk_py.batch_normalization": {
  "code": "@typemap\ndef batch_normalization(operand, scale, bias, running_mean, running_inv_std, spatial,\n                        normalization_time_constant=5000, blend_time_constant=0,\n                        epsilon=0.00001, use_cudnn_engine=False, disable_regularization=False, name='', running_count=None):\n    # TODO: running_count should be right after running_inv_std; no need for upwards compat\n    '''\n    Normalizes layer outputs for every minibatch for each output (feature) independently\n    and applies affine transformation to preserve representation of the layer.\n\n    Args:\n        operand: input of the batch normalization operation\n        scale: parameter tensor that holds the learned componentwise-scaling factors\n        bias: parameter tensor that holds the learned bias. ``scale`` and ``bias`` must have the same\n         dimensions which must be equal to the input dimensions in case of ``spatial`` = False or\n         number of output convolution feature maps in case of ``spatial`` = True\n        running_mean: running mean which is used during evaluation phase and might be used during\n         training as well. You must pass a constant tensor with initial value 0 and the same dimensions\n         as ``scale`` and ``bias``\n        running_inv_std: running variance. Represented as ``running_mean``\n        running_count: Denotes the total number of samples that have been used so far to compute\n         the ``running_mean`` and ``running_inv_std`` parameters. You must pass a scalar (either rank-0 ``constant(val)``).\n        spatial(bool): flag that indicates whether to compute mean/var for each feature in a minibatch\n         independently or, in case of convolutional layers, per future map\n        normalization_time_constant(float, default 5000): time constant for computing running average of\n         mean and variance as a low-pass filtered version of the batch statistics.\n        blend_time_constant(float, default 0): constant for smoothing batch estimates with the running\n         statistics\n        epsilon: conditioner constant added to the variance when computing the inverse standard deviation\n        use_cudnn_engine(bool, default False):\n        name (str, optional): the name of the Function instance in the network\n        disable_regularization(bool, default False): turn off regularization in batch normalization\n    Returns:\n        :class:`~cntk.ops.functions.Function`\n    '''\n    if running_count is None:\n        running_count = constant(0)\n        import warnings\n        warnings.warn(\"batch_normalization requires an additional \"\n            \"'running_count' parameter, which can be \"\n            \"instantiated as 'constant(0)'\", Warning)\n\n    from cntk.cntk_py import batch_normalization\n    operand = sanitize_input(operand)\n    return batch_normalization(operand, scale, bias, running_mean, running_inv_std, running_count, spatial,\n                               normalization_time_constant, blend_time_constant,\n                               epsilon, use_cudnn_engine, disable_regularization, name)\n",
  "sig": "batch_normalization(operand|operand: input of the batch normalization operation)(scale|scale: parameter tensor that holds the learned componentwise-scaling factors)(bias|bias: parameter tensor that holds the learned bias. ``scale`` and ``bias`` must have the same         dimensions which must be equal to the input dimensions in case of ``spatial`` = False or         number of output convolution feature maps in case of ``spatial`` = True)(running_mean|running_mean: running mean which is used during evaluation phase and might be used during         training as well. You must pass a constant tensor with initial value 0 and the same dimensions         as ``scale`` and ``bias``)(running_inv_std|running_inv_std: running variance. Represented as ``running_mean``)(spatial|spatial(bool): flag that indicates whether to compute mean/var for each feature in a minibatch         independently or, in case of convolutional layers, per future map)(normalization_time_constant|normalization_time_constant(float, default 5000): time constant for computing running average of         mean and variance as a low-pass filtered version of the batch statistics.)(blend_time_constant|blend_time_constant(float, default 0): constant for smoothing batch estimates with the running         statistics)(epsilon|epsilon: conditioner constant added to the variance when computing the inverse standard deviation)(use_cudnn_engine|use_cudnn_engine(bool, default False):)(disable_regularization|disable_regularization(bool, default False): turn off regularization in batch normalization)(name|name (str, optional): the name of the Function instance in the network)(running_count|running_count: Denotes the total number of samples that have been used so far to compute         the ``running_mean`` and ``running_inv_std`` parameters. You must pass a scalar (either rank-0 ``constant(val)``).)",
  "dst": "    Normalizes layer outputs for every minibatch for each output (feature) independently    and applies affine transformation to preserve representation of the layer."
 },
 "cntk.layers.blocks.GRU": {
  "code": "def GRU(shape, cell_shape=None, activation=default_override_or(tanh),\n        init=default_override_or(glorot_uniform()), init_bias=default_override_or(0),\n        enable_self_stabilization=default_override_or(False),\n        name=''):\n    '''\n    GRU(shape, cell_shape=None, activation=tanh, init=glorot_uniform(), init_bias=0, enable_self_stabilization=False, name='')\n\n    Layer factory function to create a GRU block for use inside a recurrence.\n    The GRU block implements one step of the recurrence and is stateless. It accepts the previous state as its first argument,\n    and outputs its new state.\n\n    Example:\n     >>> # a gated recurrent layer\n     >>> from cntk.layers import *\n     >>> gru_layer = Recurrence(GRU(500))\n\n    Args:\n        shape (`int` or `tuple` of `ints`): vector or tensor dimension of the output of this layer\n        cell_shape (tuple, defaults to `None`): if given, then the output state is first computed at `cell_shape`\n         and linearly projected to `shape`\n        activation (:class:`~cntk.ops.functions.Function`, defaults to :func:`~cntk.ops.tanh`): function to apply at the end, e.g. `relu`\n        init (scalar or NumPy array or :mod:`cntk.initializer`, defaults to `glorot_uniform`): initial value of weights `W`\n        init_bias (scalar or NumPy array or :mod:`cntk.initializer`, defaults to 0): initial value of weights `b`\n        enable_self_stabilization (bool, defaults to `False`): if `True` then add a :func:`~cntk.layers.blocks.Stabilizer`\n         to all state-related projections (but not the data input)\n        name (str, defaults to ''): the name of the Function instance in the network\n\n    Returns:\n        :class:`~cntk.ops.functions.Function`:\n        A function ``(prev_h, input) -> h`` that implements one step of a recurrent GRU layer.\n    '''\n\n    activation                = get_default_override(GRU, activation=activation)\n    init                      = get_default_override(GRU, init=init)\n    init_bias                 = get_default_override(GRU, init_bias=init_bias)\n    enable_self_stabilization = get_default_override(GRU, enable_self_stabilization=enable_self_stabilization)\n\n    return _RecurrentBlock('GRU', shape, cell_shape, activation=activation, use_peepholes=False,\n                           init=init, init_bias=init_bias,\n                           enable_self_stabilization=enable_self_stabilization, name=name)\n",
  "sig": "GRU(shape|shape (`int` or `tuple` of `ints`): vector or tensor dimension of the output of this layer)(cell_shape|cell_shape (tuple, defaults to `None`): if given, then the output state is first computed at `cell_shape`         and linearly projected to `shape`)(activation|activation (:class:`~cntk.ops.functions.Function`, defaults to :func:`~cntk.ops.tanh`): function to apply at the end, e.g. `relu`)(init|init_bias (scalar or NumPy array or :mod:`cntk.initializer`, defaults to 0): initial value of weights `b`)(init_bias|init_bias (scalar or NumPy array or :mod:`cntk.initializer`, defaults to 0): initial value of weights `b`)(enable_self_stabilization|enable_self_stabilization (bool, defaults to `False`): if `True` then add a :func:`~cntk.layers.blocks.Stabilizer`         to all state-related projections (but not the data input))(name|name (str, defaults to ''): the name of the Function instance in the network)",
  "dst": "    Layer factory function to create a GRU block for use inside a recurrence.    The GRU block implements one step of the recurrence and is stateless. It accepts the previous state as its first argument,    and outputs its new state."
 },
 "cntk.layers.blocks.LSTM": {
  "code": "def LSTM(shape, cell_shape=None, activation=default_override_or(tanh), use_peepholes=default_override_or(False),\n         init=default_override_or(glorot_uniform()), init_bias=default_override_or(0),\n         enable_self_stabilization=default_override_or(False),\n         name=''):\n    '''\n    LSTM(shape, cell_shape=None, activation=tanh, use_peepholes=False, init=glorot_uniform(), init_bias=0, enable_self_stabilization=False, name='')\n\n    Layer factory function to create an LSTM block for use inside a recurrence.\n    The LSTM block implements one step of the recurrence and is stateless. It accepts the previous state as its first two arguments,\n    and outputs its new state as a two-valued tuple ``(h,c)``.\n\n    Example:\n     >>> # a typical recurrent LSTM layer\n     >>> from cntk.layers import *\n     >>> lstm_layer = Recurrence(LSTM(500))\n\n    Args:\n        shape (`int` or `tuple` of `ints`): vector or tensor dimension of the output of this layer\n        cell_shape (tuple, defaults to `None`): if given, then the output state is first computed at `cell_shape`\n         and linearly projected to `shape`\n        activation (:class:`~cntk.ops.functions.Function`, defaults to :func:`~cntk.ops.tanh`): function to apply at the end, e.g. `relu`\n        use_peepholes (bool, defaults to `False`):\n        init (scalar or NumPy array or :mod:`cntk.initializer`, defaults to `glorot_uniform`): initial value of weights `W`\n        init_bias (scalar or NumPy array or :mod:`cntk.initializer`, defaults to 0): initial value of weights `b`\n        enable_self_stabilization (bool, defaults to `False`): if `True` then add a :func:`~cntk.layers.blocks.Stabilizer`\n         to all state-related projections (but not the data input)\n        name (str, defaults to ''): the name of the Function instance in the network\n\n    Returns:\n        :class:`~cntk.ops.functions.Function`:\n        A function ``(prev_h, prev_c, input) -> (h, c)`` that implements one step of a recurrent LSTM layer.\n    '''\n\n    activation                = get_default_override(LSTM, activation=activation)\n    use_peepholes             = get_default_override(LSTM, use_peepholes=use_peepholes)\n    init                      = get_default_override(LSTM, init=init)\n    init_bias                 = get_default_override(LSTM, init_bias=init_bias)\n    enable_self_stabilization = get_default_override(LSTM, enable_self_stabilization=enable_self_stabilization)\n\n    return _RecurrentBlock('LSTM', shape, cell_shape, activation=activation, use_peepholes=use_peepholes,\n                           init=init, init_bias=init_bias,\n                           enable_self_stabilization=enable_self_stabilization, name=name)\n",
  "sig": "LSTM(shape|shape (`int` or `tuple` of `ints`): vector or tensor dimension of the output of this layer)(cell_shape|cell_shape (tuple, defaults to `None`): if given, then the output state is first computed at `cell_shape`         and linearly projected to `shape`)(activation|activation (:class:`~cntk.ops.functions.Function`, defaults to :func:`~cntk.ops.tanh`): function to apply at the end, e.g. `relu`)(use_peepholes|use_peepholes (bool, defaults to `False`):)(init|init_bias (scalar or NumPy array or :mod:`cntk.initializer`, defaults to 0): initial value of weights `b`)(init_bias|init_bias (scalar or NumPy array or :mod:`cntk.initializer`, defaults to 0): initial value of weights `b`)(enable_self_stabilization|enable_self_stabilization (bool, defaults to `False`): if `True` then add a :func:`~cntk.layers.blocks.Stabilizer`         to all state-related projections (but not the data input))(name|name (str, defaults to ''): the name of the Function instance in the network)",
  "dst": "    Layer factory function to create an LSTM block for use inside a recurrence.    The LSTM block implements one step of the recurrence and is stateless. It accepts the previous state as its first two arguments,    and outputs its new state as a two-valued tuple ``(h,c)``."
 },
 "cntk.layers.layers.AveragePooling": {
  "code": "def AveragePooling(filter_shape,  # shape of receptive field, e.g. (3,3)\n                   strides=1,\n                   pad=default_override_or(False),\n                   name=''):\n    '''\n    AveragePooling(filter_shape, strides=1, pad=False, name='')\n\n    Layer factory function to create an average-pooling layer.\n\n    Like ``Convolution()``, ``AveragePooling()`` processes items arranged on an N-dimensional grid, such as an image.\n    Typically, each item is a vector.\n    For each item, average-pooling computes the element-wise mean over a window (\"receptive field\") of items surrounding the item's position on the grid.\n\n    The size (spatial extent) of the receptive field is given by ``filter_shape``.\n    E.g. for 2D pooling, ``filter_shape`` should be a tuple of two integers, such as `(5,5)`.\n\n    Example:\n     >>> f = AveragePooling((3,3), strides=2)  # reduce dimensionality by 2, pooling over windows of 3x3\n     >>> h = C.input_variable((32,240,320))  # e.g. 32-dim feature map\n     >>> hp = f(h)\n     >>> hp.shape  # spatial dimension has been halved due to stride, and lost one due to 3x3 window without padding\n         (32, 119, 159)\n\n     >>> f = AveragePooling((2,2), strides=2)\n     >>> f.update_signature((1,4,4))\n     >>> im = np.array([[[3, 5, 2, 6], [4, 2, 8, 3], [1, 6, 4, 7], [7, 3, 5, 9]]])  # a 4x4 image (feature-map depth 1 for simplicity)\n     >>> im\n         array([[[3, 5, 2, 6],\n                 [4, 2, 8, 3],\n                 [1, 6, 4, 7],\n                 [7, 3, 5, 9]]])\n     >>> f([[im]])  # due to strides=2, this computes the averages of each 2x2 sub-block\n         array([[[[ 3.5 ,  4.75],\n                  [ 4.25,  6.25]]]], dtype=float32)\n\n    Args:\n     filter_shape (`int` or `tuple` of `ints`): shape (spatial extent) of the receptive field, *not* including the input feature-map depth. E.g. (3,3) for a 2D convolution.\n     strides (`int` or `tuple` of `ints`, defaults to 1): stride (increment when sliding over the input). Use a `tuple` to specify a per-axis value.\n     pad (`bool` or `tuple` of `bools`, defaults to `False`): if `False`, then the pooling operation will be shifted over the \"valid\"\n      area of input, that is, no value outside the area is used. If ``pad=True`` on the other hand,\n      pooling will be applied to all input positions, and positions outside the valid region will be excluded from the averaging.\n      Use a `tuple` to specify a per-axis value.\n     name (str, defaults to ''): the name of the function instance in the network\n\n    Returns:\n        cntk.ops.functions.Function:\n        A function that accepts one argument and applies the average-pooling operation to it\n    '''\n    pad = get_default_override(AveragePooling, pad=pad)\n    return _Pooling(PoolingType_Average, filter_shape, strides=strides, pad=pad, op_name='AveragePooling', name=name)\n",
  "sig": "AveragePooling(filter_shape|filter_shape (`int` or `tuple` of `ints`): shape (spatial extent) of the receptive field, *not* including the input feature-map depth. E.g. (3,3) for a 2D convolution.)(strides|strides (`int` or `tuple` of `ints`, defaults to 1): stride (increment when sliding over the input). Use a `tuple` to specify a per-axis value.)(pad|pad (`bool` or `tuple` of `bools`, defaults to `False`): if `False`, then the pooling operation will be shifted over the \"valid\"      area of input, that is, no value outside the area is used. If ``pad=True`` on the other hand,      pooling will be applied to all input positions, and positions outside the valid region will be excluded from the averaging.      Use a `tuple` to specify a per-axis value.)(name|name (str, defaults to ''): the name of the function instance in the network)",
  "dst": "    Layer factory function to create an average-pooling layer.    Like ``Convolution()``, ``AveragePooling()`` processes items arranged on an N-dimensional grid, such as an image.    Typically, each item is a vector.    For each item, average-pooling computes the element-wise mean over a window (\"receptive field\") of items surrounding the item's position on the grid.    The size (spatial extent) of the receptive field is given by ``filter_shape``.    E.g. for 2D pooling, ``filter_shape`` should be a tuple of two integers, such as `(5,5)`."
 },
 "cntk.layers.layers.Convolution1D": {
  "code": "def Convolution1D(filter_shape,     # shape of receptive field, e.g. (3)\n                  num_filters=None, # e.g. 64 or None (which means 1 channel and don't add a dimension)\n                  activation=default_override_or(identity),\n                  init=default_override_or(C.glorot_uniform()),\n                  pad=default_override_or(False),\n                  strides=1,\n                  bias=default_override_or(True),\n                  init_bias=default_override_or(0),\n                  reduction_rank=1, # (0 means input has no depth dimension, e.g. audio signal or B&W image)\n                  dilation=1,\n                  name=''):\n    '''\n    Convolution1D(filter_shape, num_filters=None, activation=identity, init=glorot_uniform(), pad=False, strides=1, bias=True, init_bias=0, reduction_rank=1, name='')\n\n    Layer factory function to create a 1D convolution layer with optional non-linearity.\n    Same as `Convolution()` except that filter_shape is verified to be 1-dimensional.\n    See `Convolution()` for extensive documentation.\n\n    Args:\n     filter_shape (`int` or `tuple` of `ints`): shape (spatial extent) of the receptive field, *not* including the input feature-map depth. E.g. (3,3) for a 2D convolution.\n     num_filters (int, defaults to `None`): number of filters (output feature-map depth), or ``()`` to denote scalar output items (output shape will have no depth axis).\n     activation (:class:`~cntk.ops.functions.Function`, defaults to `identity`): optional function to apply at the end, e.g. `relu`\n     init (scalar or NumPy array or :mod:`cntk.initializer`, defaults to :func:`~cntk.initializer.glorot_uniform` ): initial value of weights `W`\n     pad (`bool` or `tuple` of `bools`, defaults to `False`): if `False`, then the filter will be shifted over the \"valid\"\n      area of input, that is, no value outside the area is used. If ``pad=True`` on the other hand,\n      the filter will be applied to all input positions, and positions outside the valid region will be considered containing zero.\n      Use a `tuple` to specify a per-axis value.\n     strides (`int` or `tuple` of `ints`, defaults to 1): stride of the convolution (increment when sliding the filter over the input). Use a `tuple` to specify a per-axis value.\n     bias (bool, defaults to `True`): the layer will have no bias if `False` is passed here\n     init_bias (scalar or NumPy array or :mod:`cntk.initializer`, defaults to 0): initial value of weights `b`\n     reduction_rank (`int`, defaults to 1): set to 0 if input items are scalars (input has no depth axis), e.g. an audio signal or a black-and-white image\n      that is stored with tensor shape (H,W) instead of (1,H,W)\n     dilation (tuple, optional): the dilation value along each axis, default 1 mean no dilation.\n     name (str, defaults to ''): the name of the function instance in the network\n\n    Returns:\n        cntk.ops.functions.Function:\n        A function that accepts one argument and applies the convolution operation to it\n\n    '''\n\n    activation = get_default_override(Convolution1D, activation=activation)\n    init       = get_default_override(Convolution1D, init=init)\n    pad        = get_default_override(Convolution1D, pad=pad)\n    bias       = get_default_override(Convolution1D, bias=bias)\n    init_bias  = get_default_override(Convolution1D, init_bias=init_bias)\n    if len(_as_tuple(filter_shape)) != 1:\n         raise ValueError('Convolution1D: filter_shape must be a scalar')\n    return Convolution(filter_shape, num_filters=num_filters, activation=activation, init=init, pad=pad, sequential=False, strides=strides, sharing=True, bias=bias, init_bias=init_bias, reduction_rank=reduction_rank, dilation=dilation, op_name='Convolution1D', name=name)\n",
  "sig": "Convolution1D(filter_shape|filter_shape (`int` or `tuple` of `ints`): shape (spatial extent) of the receptive field, *not* including the input feature-map depth. E.g. (3,3) for a 2D convolution.)(num_filters|num_filters (int, defaults to `None`): number of filters (output feature-map depth), or ``()`` to denote scalar output items (output shape will have no depth axis).)(activation|activation (:class:`~cntk.ops.functions.Function`, defaults to `identity`): optional function to apply at the end, e.g. `relu`)(init|init_bias (scalar or NumPy array or :mod:`cntk.initializer`, defaults to 0): initial value of weights `b`)(pad|pad (`bool` or `tuple` of `bools`, defaults to `False`): if `False`, then the filter will be shifted over the \"valid\"      area of input, that is, no value outside the area is used. If ``pad=True`` on the other hand,      the filter will be applied to all input positions, and positions outside the valid region will be considered containing zero.      Use a `tuple` to specify a per-axis value.)(strides|strides (`int` or `tuple` of `ints`, defaults to 1): stride of the convolution (increment when sliding the filter over the input). Use a `tuple` to specify a per-axis value.)(bias|bias (bool, defaults to `True`): the layer will have no bias if `False` is passed here)(init_bias|init_bias (scalar or NumPy array or :mod:`cntk.initializer`, defaults to 0): initial value of weights `b`)(reduction_rank|reduction_rank (`int`, defaults to 1): set to 0 if input items are scalars (input has no depth axis), e.g. an audio signal or a black-and-white image      that is stored with tensor shape (H,W) instead of (1,H,W))(dilation|dilation (tuple, optional): the dilation value along each axis, default 1 mean no dilation.)(name|name (str, defaults to ''): the name of the function instance in the network)",
  "dst": "    Layer factory function to create a 1D convolution layer with optional non-linearity.    Same as `Convolution()` except that filter_shape is verified to be 1-dimensional.    See `Convolution()` for extensive documentation."
 },
 "cntk.layers.layers.Convolution2D": {
  "code": "def Convolution2D(filter_shape,     # shape of receptive field, e.g. (3,3). Must be a 2-element tuple.\n                  num_filters=None, # e.g. 64 or None (which means 1 channel and don't add a dimension)\n                  activation=default_override_or(identity),\n                  init=default_override_or(C.glorot_uniform()),\n                  pad=default_override_or(False),\n                  strides=1,\n                  bias=default_override_or(True),\n                  init_bias=default_override_or(0),\n                  reduction_rank=1, # (0 means input has no depth dimension, e.g. audio signal or B&W image)\n                  dilation=1,\n                  groups=1,\n                  name=''):\n    '''\n    Convolution2D(filter_shape, num_filters=None, activation=identity, init=glorot_uniform(), pad=False, strides=1, bias=True, init_bias=0, reduction_rank=1, name='')\n\n    Layer factory function to create a 2D convolution layer with optional non-linearity.\n    Same as `Convolution()` except that filter_shape is verified to be 2-dimensional.\n    See `Convolution()` for extensive documentation.\n\n    Args:\n     filter_shape (`int` or `tuple` of `ints`): shape (spatial extent) of the receptive field, *not* including the input feature-map depth. E.g. (3,3) for a 2D convolution.\n     num_filters (int, defaults to `None`): number of filters (output feature-map depth), or ``()`` to denote scalar output items (output shape will have no depth axis).\n     activation (:class:`~cntk.ops.functions.Function`, defaults to `identity`): optional function to apply at the end, e.g. `relu`\n     init (scalar or NumPy array or :mod:`cntk.initializer`, defaults to :func:`~cntk.initializer.glorot_uniform` ): initial value of weights `W`\n     pad (`bool` or `tuple` of `bools`, defaults to `False`): if `False`, then the filter will be shifted over the \"valid\"\n      area of input, that is, no value outside the area is used. If ``pad=True`` on the other hand,\n      the filter will be applied to all input positions, and positions outside the valid region will be considered containing zero.\n      Use a `tuple` to specify a per-axis value.\n     strides (`int` or `tuple` of `ints`, defaults to 1): stride of the convolution (increment when sliding the filter over the input). Use a `tuple` to specify a per-axis value.\n     bias (bool, defaults to `True`): the layer will have no bias if `False` is passed here\n     init_bias (scalar or NumPy array or :mod:`cntk.initializer`, defaults to 0): initial value of weights `b`\n     reduction_rank (`int`, defaults to 1): set to 0 if input items are scalars (input has no depth axis), e.g. an audio signal or a black-and-white image\n      that is stored with tensor shape (H,W) instead of (1,H,W)\n     dilation (tuple, optional): the dilation value along each axis, default 1 mean no dilation.\n     groups (`int`, default 1): number of groups during convolution, that controls the connections between input and output channels. Deafult value is 1,\n      which means that all input channels are convolved to produce all output channels. A value of N would mean that the input (and output) channels are\n      divided into N groups with the input channels in one group (say i-th input group) contributing to output channels in only one group (i-th output group).\n      Number of input and output channels must be divisble by value of groups argument. Also, value of this argument must be strictly positive, i.e. groups > 0.\n     name (str, defaults to ''): the name of the function instance in the network\n\n    Returns:\n        cntk.ops.functions.Function:\n        A function that accepts one argument and applies the convolution operation to it\n\n    '''\n\n    activation = get_default_override(Convolution2D, activation=activation)\n    init       = get_default_override(Convolution2D, init=init)\n    pad        = get_default_override(Convolution2D, pad=pad)\n    bias       = get_default_override(Convolution2D, bias=bias)\n    init_bias  = get_default_override(Convolution2D, init_bias=init_bias)\n    if len(_as_tuple(filter_shape)) > 2:\n         raise ValueError('Convolution2D: filter_shape must be a scalar or a 2D tuple, e.g. 3 or (3,3)')\n    filter_shape = _pad_to_shape((0,0), filter_shape, 'filter_shape')\n    return Convolution(filter_shape, num_filters=num_filters, activation=activation, init=init, pad=pad, sequential=False,\n                       strides=strides, sharing=True, bias=bias, init_bias=init_bias, reduction_rank=reduction_rank,\n                       dilation=dilation, groups=groups, op_name='Convolution2D', name=name)\n",
  "sig": "Convolution2D(filter_shape|filter_shape (`int` or `tuple` of `ints`): shape (spatial extent) of the receptive field, *not* including the input feature-map depth. E.g. (3,3) for a 2D convolution.)(num_filters|num_filters (int, defaults to `None`): number of filters (output feature-map depth), or ``()`` to denote scalar output items (output shape will have no depth axis).)(activation|activation (:class:`~cntk.ops.functions.Function`, defaults to `identity`): optional function to apply at the end, e.g. `relu`)(init|init_bias (scalar or NumPy array or :mod:`cntk.initializer`, defaults to 0): initial value of weights `b`)(pad|pad (`bool` or `tuple` of `bools`, defaults to `False`): if `False`, then the filter will be shifted over the \"valid\"      area of input, that is, no value outside the area is used. If ``pad=True`` on the other hand,      the filter will be applied to all input positions, and positions outside the valid region will be considered containing zero.      Use a `tuple` to specify a per-axis value.)(strides|strides (`int` or `tuple` of `ints`, defaults to 1): stride of the convolution (increment when sliding the filter over the input). Use a `tuple` to specify a per-axis value.)(bias|bias (bool, defaults to `True`): the layer will have no bias if `False` is passed here)(init_bias|init_bias (scalar or NumPy array or :mod:`cntk.initializer`, defaults to 0): initial value of weights `b`)(reduction_rank|reduction_rank (`int`, defaults to 1): set to 0 if input items are scalars (input has no depth axis), e.g. an audio signal or a black-and-white image      that is stored with tensor shape (H,W) instead of (1,H,W))(dilation|dilation (tuple, optional): the dilation value along each axis, default 1 mean no dilation.)(groups|groups (`int`, default 1): number of groups during convolution, that controls the connections between input and output channels. Deafult value is 1,      which means that all input channels are convolved to produce all output channels. A value of N would mean that the input (and output) channels are      divided into N groups with the input channels in one group (say i-th input group) contributing to output channels in only one group (i-th output group).      Number of input and output channels must be divisble by value of groups argument. Also, value of this argument must be strictly positive, i.e. groups > 0.)(name|name (str, defaults to ''): the name of the function instance in the network)",
  "dst": "    Layer factory function to create a 2D convolution layer with optional non-linearity.    Same as `Convolution()` except that filter_shape is verified to be 2-dimensional.    See `Convolution()` for extensive documentation."
 },
 "cntk.layers.layers.ConvolutionTranspose2D": {
  "code": "def ConvolutionTranspose2D(filter_shape,        # a 2D tuple, e.g., (3,3)\n                           num_filters,\n                           activation=default_override_or(identity),\n                           init=default_override_or(C.glorot_uniform()),\n                           pad=default_override_or(False),\n                           strides=1,\n                           bias=default_override_or(True),\n                           init_bias=default_override_or(0),\n                           output_shape=None,\n                           reduction_rank=1, # (0 means input has no depth dimension, e.g. audio signal or B&W image)\n                           dilation=1,\n                           name=''):\n    '''\n    ConvolutionTranspose2D(filter_shape, num_filters, activation=identity, init=glorot_uniform(), pad=False, strides=1, bias=True, init_bias=0, output_shape=None, name='')\n\n    Layer factory function to create a 2D convolution transpose layer with optional non-linearity.\n    Same as `ConvolutionTranspose()` except that filter_shape is verified to be 2-dimensional.\n    See `ConvolutionTranspose()` for extensive documentation.\n    '''\n    activation = get_default_override(ConvolutionTranspose2D, activation=activation)\n    init       = get_default_override(ConvolutionTranspose2D, init=init)\n    pad        = get_default_override(ConvolutionTranspose2D, pad=pad)\n    bias       = get_default_override(ConvolutionTranspose2D, bias=bias)\n    init_bias  = get_default_override(ConvolutionTranspose2D, init_bias=init_bias)\n    output_shape = get_default_override(ConvolutionTranspose2D, output_shape=output_shape)\n    if len(_as_tuple(filter_shape)) > 2:\n         raise ValueError('ConvolutionTranspose2D: filter_shape must be a scalar or a 2D tuple, e.g. 3 or (3,3)')\n    filter_shape = _pad_to_shape((0,0), filter_shape, 'filter_shape')\n    return ConvolutionTranspose(filter_shape, num_filters, activation, init, pad, strides, True, bias, init_bias, output_shape, reduction_rank=reduction_rank, dilation=dilation, name=name)\n",
  "sig": "ConvolutionTranspose2D(filter_shape|shape (spatial extent) of the receptive field, not including the input feature-map depth. E.g. (3,3) for a 2D convolution)(num_filters|number of filters (output feature-map depth), or () to denote scalar output items (output shape will have no depth axis).)(activation|optional function to apply at the end, e.g. relu)(init|initial value of weights W)(pad|if False, then the filter will be shifted over the 'valid' area of input, that is, no value outside the area is used. If pad=True on the other hand, the filter will be applied to all input positions, and positions outside the valid region will be considered containing zero. Use a tuple to specify a per-axis value)(strides|stride of the convolution (increment when sliding the filter over the input). Use a tuple to specify a per-axis value)(bias|the layer will have no bias if False is passed here)(init_bias|initial value of weights b)(output_shape|output shape. When strides > 2, the output shape is non-deterministic. User can specify the wanted output shape. Note the specified shape must satisify the condition that if a convolution is perform from the output with the same setting, the result must have same shape as the input. that is stored with tensor shape (H,W) instead of (1,H,W))(reduction_rank|set to 0 if input items are scalars (input has no depth axis), e.g. an audio signal or a black-and-white image)(dilation|the dilation value along each axis, default 1 mean no dilation)(name|the name of the Function instance in the network)",
  "dst": "    Layer factory function to create a 2D convolution transpose layer with optional non-linearity.    Same as `ConvolutionTranspose()` except that filter_shape is verified to be 2-dimensional.    See `ConvolutionTranspose()` for extensive documentation."
 },
 "cntk.layers.layers.Convolution3D": {
  "code": "def Convolution3D(filter_shape,     # shape of receptive field, e.g. (3,3,3). Must be a 3-element tuple.\n                  num_filters=None, # e.g. 64 or None (which means 1 channel and don't add a dimension)\n                  activation=default_override_or(identity),\n                  init=default_override_or(C.glorot_uniform()),\n                  pad=default_override_or(False),\n                  strides=1,\n                  bias=default_override_or(True),\n                  init_bias=default_override_or(0),\n                  reduction_rank=1, # (0 means input has no depth dimension, e.g. audio signal or B&W image)\n                  dilation=1,\n                  groups=1,\n                  name=''):\n    '''\n    Convolution3D(filter_shape, num_filters=None, activation=identity, init=glorot_uniform(), pad=False, strides=1, bias=True, init_bias=0, reduction_rank=1, name='')\n\n    Layer factory function to create a 3D convolution layer with optional non-linearity.\n    Same as `Convolution()` except that filter_shape is verified to be 3-dimensional.\n    See `Convolution()` for extensive documentation.\n\n    Args:\n     filter_shape (`int` or `tuple` of `ints`): shape (spatial extent) of the receptive field, *not* including the input feature-map depth. E.g. (3,3) for a 2D convolution.\n     num_filters (int, defaults to `None`): number of filters (output feature-map depth), or ``()`` to denote scalar output items (output shape will have no depth axis).\n     activation (:class:`~cntk.ops.functions.Function`, defaults to `identity`): optional function to apply at the end, e.g. `relu`\n     init (scalar or NumPy array or :mod:`cntk.initializer`, defaults to :func:`~cntk.initializer.glorot_uniform` ): initial value of weights `W`\n     pad (`bool` or `tuple` of `bools`, defaults to `False`): if `False`, then the filter will be shifted over the \"valid\"\n      area of input, that is, no value outside the area is used. If ``pad=True`` on the other hand,\n      the filter will be applied to all input positions, and positions outside the valid region will be considered containing zero.\n      Use a `tuple` to specify a per-axis value.\n     strides (`int` or `tuple` of `ints`, defaults to 1): stride of the convolution (increment when sliding the filter over the input). Use a `tuple` to specify a per-axis value.\n     bias (bool, defaults to `True`): the layer will have no bias if `False` is passed here\n     init_bias (scalar or NumPy array or :mod:`cntk.initializer`, defaults to 0): initial value of weights `b`\n     reduction_rank (`int`, defaults to 1): set to 0 if input items are scalars (input has no depth axis), e.g. an audio signal or a black-and-white image\n      that is stored with tensor shape (H,W) instead of (1,H,W)\n     dilation (tuple, optional): the dilation value along each axis, default 1 mean no dilation.\n     groups (`int`, default 1): number of groups during convolution, that controls the connections between input and output channels. Deafult value is 1,\n      which means that all input channels are convolved to produce all output channels. A value of N would mean that the input (and output) channels are\n      divided into N groups with the input channels in one group (say i-th input group) contributing to output channels in only one group (i-th output group).\n      Number of input and output channels must be divisble by value of groups argument. Also, value of this argument must be strictly positive, i.e. groups > 0.\n     name (str, defaults to ''): the name of the function instance in the network\n\n    Returns:\n        cntk.ops.functions.Function:\n        A function that accepts one argument and applies the convolution operation to it\n\n    '''\n\n    activation = get_default_override(Convolution3D, activation=activation)\n    init       = get_default_override(Convolution3D, init=init)\n    pad        = get_default_override(Convolution3D, pad=pad)\n    bias       = get_default_override(Convolution3D, bias=bias)\n    init_bias  = get_default_override(Convolution3D, init_bias=init_bias)\n    if len(_as_tuple(filter_shape)) > 3:\n         raise ValueError('Convolution3D: filter_shape must be a scalar or a 3D tuple, e.g. 3 or (3,3,3)')\n    filter_shape = _pad_to_shape((0,0,0), filter_shape, 'filter_shape')\n    return Convolution(filter_shape, num_filters=num_filters, activation=activation, init=init, pad=pad, sequential=False,\n                       strides=strides, sharing=True, bias=bias, init_bias=init_bias, reduction_rank=reduction_rank,\n                       dilation=dilation, groups=groups, op_name='Convolution3D', name=name)\n",
  "sig": "Convolution3D(filter_shape|filter_shape (`int` or `tuple` of `ints`): shape (spatial extent) of the receptive field, *not* including the input feature-map depth. E.g. (3,3) for a 2D convolution.)(num_filters|num_filters (int, defaults to `None`): number of filters (output feature-map depth), or ``()`` to denote scalar output items (output shape will have no depth axis).)(activation|activation (:class:`~cntk.ops.functions.Function`, defaults to `identity`): optional function to apply at the end, e.g. `relu`)(init|init_bias (scalar or NumPy array or :mod:`cntk.initializer`, defaults to 0): initial value of weights `b`)(pad|pad (`bool` or `tuple` of `bools`, defaults to `False`): if `False`, then the filter will be shifted over the \"valid\"      area of input, that is, no value outside the area is used. If ``pad=True`` on the other hand,      the filter will be applied to all input positions, and positions outside the valid region will be considered containing zero.      Use a `tuple` to specify a per-axis value.)(strides|strides (`int` or `tuple` of `ints`, defaults to 1): stride of the convolution (increment when sliding the filter over the input). Use a `tuple` to specify a per-axis value.)(bias|bias (bool, defaults to `True`): the layer will have no bias if `False` is passed here)(init_bias|init_bias (scalar or NumPy array or :mod:`cntk.initializer`, defaults to 0): initial value of weights `b`)(reduction_rank|reduction_rank (`int`, defaults to 1): set to 0 if input items are scalars (input has no depth axis), e.g. an audio signal or a black-and-white image      that is stored with tensor shape (H,W) instead of (1,H,W))(dilation|dilation (tuple, optional): the dilation value along each axis, default 1 mean no dilation.)(groups|groups (`int`, default 1): number of groups during convolution, that controls the connections between input and output channels. Deafult value is 1,      which means that all input channels are convolved to produce all output channels. A value of N would mean that the input (and output) channels are      divided into N groups with the input channels in one group (say i-th input group) contributing to output channels in only one group (i-th output group).      Number of input and output channels must be divisble by value of groups argument. Also, value of this argument must be strictly positive, i.e. groups > 0.)(name|name (str, defaults to ''): the name of the function instance in the network)",
  "dst": "    Layer factory function to create a 3D convolution layer with optional non-linearity.    Same as `Convolution()` except that filter_shape is verified to be 3-dimensional.    See `Convolution()` for extensive documentation."
 },
 "cntk.layers.layers.ConvolutionTranspose3D": {
  "code": "def ConvolutionTranspose3D(filter_shape,        # a 3D tuple, e.g., (3,3,3)\n                           num_filters,\n                           activation=default_override_or(identity),\n                           init=default_override_or(C.glorot_uniform()),\n                           pad=default_override_or(False),\n                           strides=1,\n                           bias=default_override_or(True),\n                           init_bias=default_override_or(0),\n                           output_shape=None,\n                           reduction_rank=1, # (0 means input has no depth dimension, e.g. audio signal or B&W image)\n                           dilation=1,\n                           name=''):\n    '''\n    ConvolutionTranspose3D(filter_shape, num_filters, activation=identity, init=glorot_uniform(), pad=False, strides=1, bias=True, init_bias=0, output_shape=None, name='')\n\n    Layer factory function to create a 3D convolution transpose layer with optional non-linearity.\n    Same as `ConvolutionTranspose()` except that filter_shape is verified to be 3-dimensional.\n    See `ConvolutionTranspose()` for extensive documentation.\n    '''\n    activation = get_default_override(ConvolutionTranspose3D, activation=activation)\n    init       = get_default_override(ConvolutionTranspose3D, init=init)\n    pad        = get_default_override(ConvolutionTranspose3D, pad=pad)\n    bias       = get_default_override(ConvolutionTranspose3D, bias=bias)\n    init_bias  = get_default_override(ConvolutionTranspose3D, init_bias=init_bias)\n    output_shape = get_default_override(ConvolutionTranspose3D, output_shape=output_shape)\n    if len(_as_tuple(filter_shape)) > 3:\n         raise ValueError('ConvolutionTranspose3D: filter_shape must be a scalar or a 3D tuple, e.g. 3 or (3,3,3)')\n    filter_shape = _pad_to_shape((0,0,0), filter_shape, 'filter_shape')\n    return ConvolutionTranspose(filter_shape, num_filters, activation, init, pad, strides, True, bias, init_bias, output_shape, reduction_rank=reduction_rank, dilation=dilation, name=name)\n",
  "sig": "ConvolutionTranspose3D(filter_shape|shape (spatial extent) of the receptive field, not including the input feature-map depth. E.g. (3,3) for a 2D convolution)(num_filters|number of filters (output feature-map depth), or () to denote scalar output items (output shape will have no depth axis).)(activation|optional function to apply at the end, e.g. relu)(init|initial value of weights W)(pad|if False, then the filter will be shifted over the 'valid' area of input, that is, no value outside the area is used. If pad=True on the other hand, the filter will be applied to all input positions, and positions outside the valid region will be considered containing zero. Use a tuple to specify a per-axis value)(strides|stride of the convolution (increment when sliding the filter over the input). Use a tuple to specify a per-axis value)(bias|the layer will have no bias if False is passed here)(init_bias|initial value of weights b)(output_shape|output shape. When strides > 2, the output shape is non-deterministic. User can specify the wanted output shape. Note the specified shape must satisify the condition that if a convolution is perform from the output with the same setting, the result must have same shape as the input. that is stored with tensor shape (H,W) instead of (1,H,W))(reduction_rank|set to 0 if input items are scalars (input has no depth axis), e.g. an audio signal or a black-and-white image)(dilation|the dilation value along each axis, default 1 mean no dilation)(name|the name of the Function instance in the network)",
  "dst": "    Layer factory function to create a 3D convolution transpose layer with optional non-linearity.    Same as `ConvolutionTranspose()` except that filter_shape is verified to be 3-dimensional.    See `ConvolutionTranspose()` for extensive documentation."
 },
 "cntk.layers.layers.Dense": {
  "code": "def Dense(shape, activation=default_override_or(identity), init=default_override_or(C.glorot_uniform()),\n          input_rank=None, map_rank=None,\n          bias=default_override_or(True), init_bias=default_override_or(0),\n          name=''):\n    '''\n    Dense(shape, activation=identity, init=glorot_uniform(), input_rank=None, map_rank=None, bias=True, init_bias=0, name='')\n\n    Layer factory function to create an instance of a fully-connected linear layer of the form\n    `activation(input @ W + b)` with weights `W` and bias `b`, and `activation` and `b` being optional.\n    `shape` may describe a tensor as well.\n\n    A ``Dense`` layer instance owns its parameter tensors `W` and `b`, and exposes them as attributes ``.W`` and ``.b``.\n\n    Example:\n     >>> f = Dense(5, activation=C.relu)\n     >>> x = C.input_variable(3)\n     >>> h = f(x)\n     >>> h.shape\n         (5,)\n     >>> f.W.shape\n         (3, 5)\n     >>> f.b.value\n         array([ 0.,  0.,  0.,  0.,  0.], dtype=float32)\n\n     >>> # activation through default options\n     >>> with C.default_options(activation=C.relu):\n     ...     f = Dense(500)\n\n    The ``Dense`` layer can be applied to inputs that are tensors, not just vectors.\n    This is useful, e.g., at the top of a image-processing cascade, where after many\n    convolutions with padding and strides it is difficult to know the precise dimensions.\n    For this case, CNTK has an extended definition of matrix product, in which\n    the input tensor will be treated as if it had been automatically flattened.\n    The weight matrix will be a tensor that reflects the \"flattened\" dimensions in its axes.\n\n    Example:\n     >>> f = Dense(5, activation=C.softmax) # a 5-class classifier\n     >>> x = C.input_variable((64,16,16)) # e.g. an image reduced by a convolution stack\n     >>> y = f(x)\n     >>> y.shape\n     (5,)\n     >>> f.W.shape  # \"row\" dimension of \"matrix\" consists of 3 axes that match the input\n     (64, 16, 16, 5)\n\n    This behavior can be modified by telling CNTK either the number of axes that should not be projected (``map_rank``)\n    or the rank of the input (``input_rank``). If neither is specified, all input dimensions are\n    projected, as in the example above.\n\n    Example:\n     >>> f = Dense(5, activation=C.softmax, input_rank=2) # a 5-class classifier\n     >>> x = C.input_variable((10, 3, 3)) # e.g. 10 parallel 3x3 objects. Input has input_rank=2 axes\n     >>> y = f(x)\n     >>> y.shape  # the 10 parallel objects are classified separately, the \"10\" dimension is retained\n     (10, 5)\n     >>> f.W.shape  # \"row\" dimension of \"matrix\" consists of (3,3) matching the input axes to project\n     (3, 3, 5)\n\n     >>> f = Dense(5, activation=C.softmax, map_rank=2)\n     >>> x = C.input_variable((4, 6, 3, 3, 3)) # e.g. 24 parallel 3x3x3 objects arranged in a 4x6 grid. The grid is to be retained\n     >>> y = f(x)\n     >>> y.shape  # the 4x6 elements are classified separately, the grid structure is retained\n     (4, 6, 5)\n     >>> f.W.shape  # \"row\" dimension of \"matrix\" consists of (3,3) matching the input axes to project\n     (3, 3, 3, 5)\n     >>> z = y([np.zeros(x.shape)])\n     >>> assert z.shape == (1, 4, 6, 5)\n\n    Args:\n     shape (`int` or `tuple` of `ints`): vector or tensor dimension of the output of this layer\n     activation (:class:`~cntk.ops.functions.Function`, defaults to identity): optional function to apply at the end, e.g. `relu`\n     init (scalar or NumPy array or :mod:`cntk.initializer`, defaults to :func:`~cntk.initializer.glorot_uniform` ): initial value of weights `W`\n     input_rank (int, defaults to `None`): number of inferred axes to add to W (`map_rank` must not be given)\n     map_rank (int, defaults to `None`): expand W to leave exactly `map_rank` axes (`input_rank` must not be given)\n     bias (bool, optional, defaults to `True`): the layer will have no bias if `False` is passed here\n     init_bias (scalar or NumPy array or :mod:`cntk.initializer`, defaults to 0): initial value of weights `b`\n     name (str, defaults to ''): the name of the function instance in the network\n\n    Returns:\n        cntk.ops.functions.Function:\n        A function that accepts one argument and applies the operation to it\n    '''\n\n    activation = get_default_override(Dense, activation=activation)\n    init       = get_default_override(Dense, init=init)\n    bias       = get_default_override(Dense, bias=bias)\n    init_bias  = get_default_override(Dense, init_bias=init_bias)\n\n    output_shape = _as_tuple(shape)\n\n    if input_rank is not None and map_rank is not None:\n        raise ValueError(\"Dense: input_rank and map_rank cannot be specified at the same time.\")\n\n    # determine meaning of axes\n    # W gets dimension (input_shape + shape)\n    # where input_shape is determined as:\n    #  - by default, equal to the dimensions of the input passed to Dense()\n    #  - if input_rank is given, then the last 'input_rank' dimensions of the input (all others are not reduced over)\n    #  - if map_rank is given, then the all but the first 'map_rank' dimensions of the input (those are not reduced over)\n    # where input_rank and map_rank are mutually exclusive.\n\n    output_rank = len(output_shape)   # support outputs with tensor layouts\n\n    # If input_rank not given then pass a single _INFERRED; map_rank if given will determine the input_rank.\n    # The dimension inference may still create multiple axes.\n    input_shape = _INFERRED * (input_rank if input_rank is not None else 1)\n\n    if input_rank is not None:\n        infer_input_rank_to_map = -1 # means map_rank is not specified; input_rank rules\n    elif map_rank is None:\n        infer_input_rank_to_map = 0  # neither given: default to 'infer W to use all input dims'\n    else:\n        infer_input_rank_to_map = map_rank  # infer W to use all input dims except the first static 'map_rank' ones\n\n    # parameters bound to this Function\n    init_weights = _initializer_for(init, Record(output_rank=output_rank))\n    W = Parameter(input_shape + output_shape, init=init_weights, name='W')\n    b = Parameter(              output_shape, init=init_bias,    name='b') if bias else None\n\n    # expression of this function\n    @BlockFunction('Dense', name)\n    def dense(x):\n        r = times(x, W, output_rank=output_rank, infer_input_rank_to_map=infer_input_rank_to_map)\n        if b:\n            r = r + b\n        if activation is not None:\n            r = activation(r)\n        return r\n    return dense\n",
  "sig": "Dense(shape|shape (`int` or `tuple` of `ints`): vector or tensor dimension of the output of this layer)(activation|activation (:class:`~cntk.ops.functions.Function`, defaults to identity): optional function to apply at the end, e.g. `relu`)(init|init_bias (scalar or NumPy array or :mod:`cntk.initializer`, defaults to 0): initial value of weights `b`)(input_rank|input_rank (int, defaults to `None`): number of inferred axes to add to W (`map_rank` must not be given))(map_rank|map_rank (int, defaults to `None`): expand W to leave exactly `map_rank` axes (`input_rank` must not be given))(bias|bias (bool, optional, defaults to `True`): the layer will have no bias if `False` is passed here)(init_bias|init_bias (scalar or NumPy array or :mod:`cntk.initializer`, defaults to 0): initial value of weights `b`)(name|name (str, defaults to ''): the name of the function instance in the network)",
  "dst": "    Layer factory function to create an instance of a fully-connected linear layer of the form    `activation(input @ W + b)` with weights `W` and bias `b`, and `activation` and `b` being optional.    `shape` may describe a tensor as well.    A ``Dense`` layer instance owns its parameter tensors `W` and `b`, and exposes them as attributes ``.W`` and ``.b``."
 },
 "cntk.layers.layers.Dropout": {
  "code": "def Dropout(dropout_rate=None,\n            keep_prob=None,\n            seed = SentinelValueForAutoSelectRandomSeed,\n            name=''):\n    '''\n    Layer factory function to create a drop-out layer.\n\n    The dropout rate can be specified as the probability of *dropping* a value (``dropout_rate``).\n    E.g. ``Dropout(0.3)`` means \"drop 30% of the activation values.\"\n    Alternatively, it can also be specified as the probability of *keeping* a value (``keep_prob``).\n\n    The dropout operation is only applied during training. During testing, this is a no-op.\n    To make sure that this leads to correct results, the dropout operation in training\n    multiplies the result by (1/(1-``dropout_rate``)).\n\n    Example:\n     >>> f = Dropout(0.2)   # \"drop 20% of activations\"\n     >>> h = C.input_variable(3)\n     >>> hd = f(h)\n\n     >>> f = Dropout(keep_prob=0.8)   # \"keep 80%\"\n     >>> h = C.input_variable(3)\n     >>> hd = f(h)\n\n    Args:\n     dropout_rate (float): probability of dropping out an element, mutually exclusive with ``keep_prob``\n     keep_prob (float): probability of keeping an element, mutually exclusive with ``dropout_rate``\n     seed (int): random seed.\n     name (str, defaults to ''): the name of the function instance in the network\n\n    Returns:\n        cntk.ops.functions.Function:\n        A function that accepts one argument and applies the operation to it\n    '''\n    if dropout_rate is None and keep_prob is None:\n        raise ValueError(\"Dropout: either dropout_rate or keep_prob must be specified.\")\n    elif dropout_rate is not None and keep_prob is not None:\n        raise ValueError(\"Dropout: dropout_rate and keep_prob cannot be specified at the same time.\")\n    elif keep_prob is not None:\n        if keep_prob < 0.0 or keep_prob >= 1.0:\n            raise ValueError(\"Dropout: keep_prob must be in the interval [0,1)\")\n        dropout_rate = 1-keep_prob\n\n    @BlockFunction('Dropout', name)\n    def dropout_f(x):\n        return dropout(x, dropout_rate=dropout_rate, seed=seed)\n    return dropout_f\n",
  "sig": "Dropout(dropout_rate|dropout_rate (float): probability of dropping out an element, mutually exclusive with ``keep_prob``)(keep_prob|keep_prob (float): probability of keeping an element, mutually exclusive with ``dropout_rate``)(seed|seed (int): random seed.)(name|name (str, defaults to ''): the name of the function instance in the network)",
  "dst": "    Layer factory function to create a drop-out layer.    The dropout rate can be specified as the probability of *dropping* a value (``dropout_rate``).    E.g. ``Dropout(0.3)`` means \"drop 30% of the activation values.\"    Alternatively, it can also be specified as the probability of *keeping* a value (``keep_prob``).    The dropout operation is only applied during training. During testing, this is a no-op.    To make sure that this leads to correct results, the dropout operation in training    multiplies the result by (1/(1-``dropout_rate``))."
 },
 "cntk.cntk_py.dropout": {
  "code": "@typemap\ndef dropout(x, dropout_rate=0.0, seed = SentinelValueForAutoSelectRandomSeed, name=''):\n    '''\n    Each element of the input is independently set to 0 with probability ``dropout_rate``\n    or to 1 / (1 - ``dropout_rate``) times its original value (with probability 1-``dropout_rate``).\n    Dropout is a good way to reduce overfitting.\n\n    This behavior only happens during training. During inference dropout is a no-op.\n    In the paper that introduced dropout it was suggested to scale the weights during inference.\n    In CNTK's implementation, because the values that are not set to 0 are multiplied\n    with (1 / (1 - ``dropout_rate``)), this is not necessary.\n\n    Example:\n        >>> data = [[10, 20],[30, 40],[50, 60]]\n        >>> C.dropout(data, 0.5).eval() # doctest: +SKIP\n        array([[  0.,  40.],\n               [  0.,  80.],\n               [  0.,   0.]], dtype=float32)\n\n        >>> C.dropout(data, 0.75).eval() # doctest: +SKIP\n        array([[   0.,    0.],\n               [   0.,  160.],\n               [   0.,  240.]], dtype=float32)\n\n    Args:\n        x: input tensor\n        dropout_rate (float, [0,1)): probability that an element of ``x`` will be set to zero\n        seed (int): random seed.\n        name (:class:`str`, optional): the name of the Function instance in the network\n\n    Returns:\n        :class:`~cntk.ops.functions.Function`\n    '''\n    if dropout_rate < 0.0 or dropout_rate >= 1.0:\n        raise ValueError('dropout_rate must be in the interval [0,1)')\n\n    from cntk.cntk_py import dropout\n    x = sanitize_input(x)\n\n    return dropout(x, dropout_rate, seed, name)\n",
  "sig": "dropout(x|x: input tensor)(dropout_rate|dropout_rate (float): probability of dropping out an element, mutually exclusive with ``keep_prob``)(seed|seed (int): random seed.)(name|name (:class:`str`, optional): the name of the Function instance in the network)",
  "dst": "    Each element of the input is independently set to 0 with probability ``dropout_rate``    or to 1 / (1 - ``dropout_rate``) times its original value (with probability 1-``dropout_rate``).    Dropout is a good way to reduce overfitting.    This behavior only happens during training. During inference dropout is a no-op.    In the paper that introduced dropout it was suggested to scale the weights during inference.    In CNTK's implementation, because the values that are not set to 0 are multiplied    with (1 / (1 - ``dropout_rate``)), this is not necessary."
 },
 "cntk.layers.layers.MaxPooling": {
  "code": "def MaxPooling(filter_shape,  # shape of receptive field, e.g. (3,3)\n               strides=1,\n               pad=default_override_or(False),\n               name=''):\n    '''\n    MaxPooling(filter_shape, strides=1, pad=False, name='')\n\n    Layer factory function to create a max-pooling layer.\n\n    Like ``Convolution()``, ``MaxPooling()`` processes items arranged on an N-dimensional grid, such as an image.\n    Typically, each item is a vector.\n    For each item, max-pooling computes the element-wise maximum over a window (\"receptive field\") of items surrounding the item's position on the grid.\n\n    The size (spatial extent) of the receptive field is given by ``filter_shape``.\n    E.g. for 2D pooling, ``filter_shape`` should be a tuple of two integers, such as `(5,5)`.\n\n    Example:\n     >>> f = MaxPooling((3,3), strides=2)  # reduce dimensionality by 2, pooling over windows of 3x3\n     >>> h = C.input_variable((32,240,320))  # e.g. 32-dim feature map\n     >>> hp = f(h)\n     >>> hp.shape  # spatial dimension has been halved due to stride, and lost one due to 3x3 window without padding\n         (32, 119, 159)\n\n     >>> f = MaxPooling((2,2), strides=2)\n     >>> f.update_signature((1,4,4))\n     >>> im = np.array([[[3, 5, 2, 6], [4, 2, 8, 3], [1, 6, 4, 7], [7, 3, 5, 9]]])  # a 4x4 image (feature-map depth 1 for simplicity)\n     >>> im\n         array([[[3, 5, 2, 6],\n                 [4, 2, 8, 3],\n                 [1, 6, 4, 7],\n                 [7, 3, 5, 9]]])\n     >>> f([[im]])  # due to strides=2, this picks the max out of each 2x2 sub-block\n         array([[[[ 5.,  8.],\n                  [ 7.,  9.]]]], dtype=float32)\n\n    Args:\n     filter_shape (`int` or `tuple` of `ints`): shape (spatial extent) of the receptive field, *not* including the input feature-map depth. E.g. (3,3) for a 2D convolution.\n     strides (`int` or `tuple` of `ints`, defaults to 1): stride (increment when sliding over the input). Use a `tuple` to specify a per-axis value.\n     pad (`bool` or `tuple` of `bools`, defaults to `False`): if `False`, then the pooling operation will be shifted over the \"valid\"\n      area of input, that is, no value outside the area is used. If ``pad=True`` on the other hand,\n      pooling will be applied to all input positions, and positions outside the valid region will be considered containing zero.\n      Use a `tuple` to specify a per-axis value.\n     name (str, defaults to ''): the name of the function instance in the network\n\n    Returns:\n        cntk.ops.functions.Function:\n        A function that accepts one argument and applies the max-pooling operation to it\n    '''\n    pad = get_default_override(MaxPooling, pad=pad)\n    return _Pooling(PoolingType_Max, filter_shape, strides=strides, pad=pad, op_name='MaxPooling', name=name)\n",
  "sig": "MaxPooling(filter_shape|filter_shape (`int` or `tuple` of `ints`): shape (spatial extent) of the receptive field, *not* including the input feature-map depth. E.g. (3,3) for a 2D convolution.)(strides|strides (`int` or `tuple` of `ints`, defaults to 1): stride (increment when sliding over the input). Use a `tuple` to specify a per-axis value.)(pad|pad (`bool` or `tuple` of `bools`, defaults to `False`): if `False`, then the pooling operation will be shifted over the \"valid\"      area of input, that is, no value outside the area is used. If ``pad=True`` on the other hand,      pooling will be applied to all input positions, and positions outside the valid region will be considered containing zero.      Use a `tuple` to specify a per-axis value.)(name|name (str, defaults to ''): the name of the function instance in the network)",
  "dst": "    Layer factory function to create a max-pooling layer.    Like ``Convolution()``, ``MaxPooling()`` processes items arranged on an N-dimensional grid, such as an image.    Typically, each item is a vector.    For each item, max-pooling computes the element-wise maximum over a window (\"receptive field\") of items surrounding the item's position on the grid.    The size (spatial extent) of the receptive field is given by ``filter_shape``.    E.g. for 2D pooling, ``filter_shape`` should be a tuple of two integers, such as `(5,5)`."
 },
 "cntk.squared_error": {
  "code": "@typemap\ndef squared_error(output, target, name=''):\n    '''\n    This operation computes the sum of the squared difference between elements\n    in the two input matrices. The result is a scalar (i.e., one by one matrix).\n    This is often used as a training criterion.\n\n    Example:\n        >>> i1 = C.input_variable((1,2))\n        >>> i2 = C.input_variable((1,2))\n        >>> C.squared_error(i1,i2).eval({i1:np.asarray([[[2., 1.]]], dtype=np.float32), i2:np.asarray([[[4., 6.]]], dtype=np.float32)})\n        array([ 29.], dtype=float32)\n\n        >>> C.squared_error(i1,i2).eval({i1:np.asarray([[[1., 2.]]], dtype=np.float32), i2:np.asarray([[[1., 2.]]], dtype=np.float32)})\n        array([ 0.], dtype=float32)\n\n    Args:\n        output: the output values from the network\n        target: it is usually a one-hot vector where the hot bit\n         corresponds to the label index\n        name (str, optional): the name of the Function instance in the network\n    Returns:\n        :class:`~cntk.ops.functions.Function`\n    '''\n    from cntk.cntk_py import squared_error\n    dtype = get_data_type(output, target)\n    output = sanitize_input(output, dtype)\n    target = sanitize_input(target, dtype)\n    return squared_error(output, target, name)\n",
  "sig": "squared_error(output|output: the output values from the network)(target|target: it is usually a one-hot vector where the hot bit         corresponds to the label index)(name|name (str, optional): the name of the Function instance in the network)",
  "dst": "    This operation computes the sum of the squared difference between elements    in the two input matrices. The result is a scalar (i.e., one by one matrix).    This is often used as a training criterion."
 },
 "cntk.ops.log_softmax": {
  "code": "@typemap\ndef log_softmax(x, axis = None, name = ''):\n    '''\n    Computes the logsoftmax normalized values of x. That is, y = x - log(reduce_sum(exp(x), axis))\n    (the implementation uses an equivalent formula for numerical stability).\n\n    It is also possible to use `x - reduce_log_sum_exp(x, axis)` instead of log_softmax:\n    this can be faster (one reduce pass instead of two), but can behave slightly differently numerically.\n\n    Args:\n        x: numpy array or any :class:`~cntk.ops.functions.Function` that outputs a tensor\n        axis (int): axis along which the logsoftmax operation will be performed (the default is the last axis)\n        name (str, optional): the name of the Function instance in the network\n    Returns:\n        :class:`~cntk.ops.functions.Function`\n    '''\n    from cntk.cntk_py import log_softmax\n    x = sanitize_input(x)\n    if axis is not None:\n        axis = sanitize_axis(axis)\n        return log_softmax(x, axis, name)\n    else:\n        return log_softmax(x, name)\n",
  "sig": "log_softmax(x|x: numpy array or any :class:`~cntk.ops.functions.Function` that outputs a tensor)(axis|axis (int): axis along which the logsoftmax operation will be performed (the default is the last axis))(name|name (str, optional): the name of the Function instance in the network)",
  "dst": "    Computes the logsoftmax normalized values of x. That is, y = x - log(reduce_sum(exp(x), axis))    (the implementation uses an equivalent formula for numerical stability).    It is also possible to use `x - reduce_log_sum_exp(x, axis)` instead of log_softmax:    this can be faster (one reduce pass instead of two), but can behave slightly differently numerically."
 },
 "cntk.ops.softmax": {
  "code": "@typemap\ndef softmax(x, axis=None, name=''):\n    r'''\n    Computes the gradient of :math:`f(z)=\\log\\sum_i\\exp(z_i)` at ``z = x``. Concretely,\n\n    :math:`\\mathrm{softmax}(x)=\\left[\\frac{\\exp(x_1)}{\\sum_i\\exp(x_i)}\\quad\\frac{\\exp(x_1)}{\\sum_i\\exp(x_i)}\\quad\\ldots\\quad\\frac{\\exp(x_1)}{\\sum_i\\exp(x_i)}\\right]`\n\n    with the understanding that the implementation can use equivalent formulas\n    for efficiency and numerical stability.\n\n    The output is a vector of non-negative numbers that sum to 1 and can\n    therefore be interpreted as probabilities for mutually exclusive outcomes\n    as in the case of multiclass classification.\n\n    If ``axis`` is given as integer, then the softmax will be computed along that axis.\n    If the provided ``axis`` is -1, it will be computed along the last axis. Otherwise,\n    softmax will be applied to all axes.\n\n    Example:\n        >>> C.softmax([[1, 1, 2, 3]]).eval()\n        array([[ 0.082595,  0.082595,  0.224515,  0.610296]], dtype=float32)\n\n        >>> C.softmax([1, 1]).eval()\n        array([ 0.5,  0.5], dtype=float32)\n\n        >>> C.softmax([[[1, 1], [3, 5]]], axis=-1).eval()\n        array([[[ 0.5     ,  0.5     ],\n                [ 0.119203,  0.880797]]], dtype=float32)\n\n        >>> C.softmax([[[1, 1], [3, 5]]], axis=1).eval()\n        array([[[ 0.119203,  0.017986],\n                [ 0.880797,  0.982014]]], dtype=float32)\n\n    Args:\n        x: numpy array or any :class:`~cntk.ops.functions.Function` that outputs a tensor\n        axis (int or :class:`~cntk.axis.Axis`): axis along which the softmax operation will be performed\n        name (str, optional): the name of the Function instance in the network\n    Returns:\n        :class:`~cntk.ops.functions.Function`\n    '''\n    from cntk.cntk_py import softmax\n    x = sanitize_input(x)\n    if axis is not None:\n        axis = sanitize_axis(axis)\n        return softmax(x, axis, name)\n    else:\n        return softmax(x, name)\n",
  "sig": "softmax(x|x: numpy array or any :class:`~cntk.ops.functions.Function` that outputs a tensor)(axis|axis (int or :class:`~cntk.axis.Axis`): axis along which the softmax operation will be performed)(name|name (str, optional): the name of the Function instance in the network)",
  "dst": "    Computes the gradient of :math:`f(z)=\\log\\sum_i\\exp(z_i)` at ``z = x``. Concretely,    :math:`\\mathrm{softmax}(x)=\\left[\\frac{\\exp(x_1)}{\\sum_i\\exp(x_i)}\\quad\\frac{\\exp(x_1)}{\\sum_i\\exp(x_i)}\\quad\\ldots\\quad\\frac{\\exp(x_1)}{\\sum_i\\exp(x_i)}\\right]`    with the understanding that the implementation can use equivalent formulas    for efficiency and numerical stability.    The output is a vector of non-negative numbers that sum to 1 and can    therefore be interpreted as probabilities for mutually exclusive outcomes    as in the case of multiclass classification.    If ``axis`` is given as integer, then the softmax will be computed along that axis.    If the provided ``axis`` is -1, it will be computed along the last axis. Otherwise,"
 },
 "cntk.layers.MaxPooling": {
  "code": "def MaxPooling(filter_shape,  # shape of receptive field, e.g. (3,3)\n               strides=1,\n               pad=default_override_or(False),\n               name=''):\n    '''\n    MaxPooling(filter_shape, strides=1, pad=False, name='')\n\n    Layer factory function to create a max-pooling layer.\n\n    Like ``Convolution()``, ``MaxPooling()`` processes items arranged on an N-dimensional grid, such as an image.\n    Typically, each item is a vector.\n    For each item, max-pooling computes the element-wise maximum over a window (\"receptive field\") of items surrounding the item's position on the grid.\n\n    The size (spatial extent) of the receptive field is given by ``filter_shape``.\n    E.g. for 2D pooling, ``filter_shape`` should be a tuple of two integers, such as `(5,5)`.\n\n    Example:\n     >>> f = MaxPooling((3,3), strides=2)  # reduce dimensionality by 2, pooling over windows of 3x3\n     >>> h = C.input_variable((32,240,320))  # e.g. 32-dim feature map\n     >>> hp = f(h)\n     >>> hp.shape  # spatial dimension has been halved due to stride, and lost one due to 3x3 window without padding\n         (32, 119, 159)\n\n     >>> f = MaxPooling((2,2), strides=2)\n     >>> f.update_signature((1,4,4))\n     >>> im = np.array([[[3, 5, 2, 6], [4, 2, 8, 3], [1, 6, 4, 7], [7, 3, 5, 9]]])  # a 4x4 image (feature-map depth 1 for simplicity)\n     >>> im\n         array([[[3, 5, 2, 6],\n                 [4, 2, 8, 3],\n                 [1, 6, 4, 7],\n                 [7, 3, 5, 9]]])\n     >>> f([[im]])  # due to strides=2, this picks the max out of each 2x2 sub-block\n         array([[[[ 5.,  8.],\n                  [ 7.,  9.]]]], dtype=float32)\n\n    Args:\n     filter_shape (`int` or `tuple` of `ints`): shape (spatial extent) of the receptive field, *not* including the input feature-map depth. E.g. (3,3) for a 2D convolution.\n     strides (`int` or `tuple` of `ints`, defaults to 1): stride (increment when sliding over the input). Use a `tuple` to specify a per-axis value.\n     pad (`bool` or `tuple` of `bools`, defaults to `False`): if `False`, then the pooling operation will be shifted over the \"valid\"\n      area of input, that is, no value outside the area is used. If ``pad=True`` on the other hand,\n      pooling will be applied to all input positions, and positions outside the valid region will be considered containing zero.\n      Use a `tuple` to specify a per-axis value.\n     name (str, defaults to ''): the name of the function instance in the network\n\n    Returns:\n        cntk.ops.functions.Function:\n        A function that accepts one argument and applies the max-pooling operation to it\n    '''\n    pad = get_default_override(MaxPooling, pad=pad)\n    return _Pooling(PoolingType_Max, filter_shape, strides=strides, pad=pad, op_name='MaxPooling', name=name)\n",
  "sig": "MaxPooling(filter_shape|filter_shape (`int` or `tuple` of `ints`): shape (spatial extent) of the receptive field, *not* including the input feature-map depth. E.g. (3,3) for a 2D convolution.)(strides|strides (`int` or `tuple` of `ints`, defaults to 1): stride (increment when sliding over the input). Use a `tuple` to specify a per-axis value.)(pad|pad (`bool` or `tuple` of `bools`, defaults to `False`): if `False`, then the pooling operation will be shifted over the \"valid\"      area of input, that is, no value outside the area is used. If ``pad=True`` on the other hand,      pooling will be applied to all input positions, and positions outside the valid region will be considered containing zero.      Use a `tuple` to specify a per-axis value.)(name|name (str, defaults to ''): the name of the function instance in the network)",
  "dst": "    Layer factory function to create a max-pooling layer.    Like ``Convolution()``, ``MaxPooling()`` processes items arranged on an N-dimensional grid, such as an image.    Typically, each item is a vector.    For each item, max-pooling computes the element-wise maximum over a window (\"receptive field\") of items surrounding the item's position on the grid.    The size (spatial extent) of the receptive field is given by ``filter_shape``.    E.g. for 2D pooling, ``filter_shape`` should be a tuple of two integers, such as `(5,5)`."
 },
 "cntk.ops.relu": {
  "code": "@typemap\ndef relu(x, name=''):\n    '''\n    Rectified linear operation. Computes the element-wise rectified linear\n    of ``x``: ``max(x, 0)``\n\n    The output tensor has the same shape as ``x``.\n\n    Example:\n        >>> C.relu([[-1, -0.5, 0, 1, 2]]).eval()\n        array([[ 0.,  0.,  0.,  1.,  2.]], dtype=float32)\n\n    Args:\n        x: numpy array or any :class:`~cntk.ops.functions.Function` that outputs a tensor\n        name (str, optional): the name of the Function instance in the network\n    Returns:\n        :class:`~cntk.ops.functions.Function`\n    '''\n    from cntk.cntk_py import re_lu\n    x = sanitize_input(x)\n    return re_lu(x, name)\n",
  "sig": "relu(x|x: numpy array or any :class:`~cntk.ops.functions.Function` that outputs a tensor)(name|name (str, optional): the name of the Function instance in the network)",
  "dst": "    Rectified linear operation. Computes the element-wise rectified linear    of ``x``: ``max(x, 0)``    The output tensor has the same shape as ``x``."
 },
 "cntk.ops.pad": {
  "code": "@typemap\ndef pad(x, pattern, mode=CONSTANT_PAD, constant_value = 0, name=''):\n    '''\n    Pads a tensor according to the specified patterns.\n    Three padding modes are supported: CONSTANT / REFLECT / SYMMETRIC.\n\n    Example:\n        >>> data = np.arange(6, dtype=np.float32).reshape((2,3))\n        >>> x = C.constant(value=data)\n        >>> C.pad(x, pattern=[(1,1),(2,2)], mode=C.ops.CONSTANT_PAD, constant_value=1).eval()\n        array([[ 1.,  1.,  1.,  1.,  1.,  1.,  1.],\n               [ 1.,  1.,  0.,  1.,  2.,  1.,  1.],\n               [ 1.,  1.,  3.,  4.,  5.,  1.,  1.],\n               [ 1.,  1.,  1.,  1.,  1.,  1.,  1.]], dtype=float32)\n        >>> C.pad(x, pattern=[(1,1),(2,2)], mode=C.ops.REFLECT_PAD).eval()\n        array([[ 5.,  4.,  3.,  4.,  5.,  4.,  3.],\n               [ 2.,  1.,  0.,  1.,  2.,  1.,  0.],\n               [ 5.,  4.,  3.,  4.,  5.,  4.,  3.],\n               [ 2.,  1.,  0.,  1.,  2.,  1.,  0.]], dtype=float32)\n        >>> C.pad(x, pattern=[(1,1),(2,2)], mode=C.ops.SYMMETRIC_PAD).eval()\n        array([[ 1.,  0.,  0.,  1.,  2.,  2.,  1.],\n               [ 1.,  0.,  0.,  1.,  2.,  2.,  1.],\n               [ 4.,  3.,  3.,  4.,  5.,  5.,  4.],\n               [ 4.,  3.,  3.,  4.,  5.,  5.,  4.]], dtype=float32)\n\n    Args:\n        x: tensor to be padded.\n        pattern (list of tuple with 2 integers): how many values to add before and after the contents of the tensor in each dimension.\n        mode (int): padding mode: C.ops.CONSTANT_PAD, C.ops.REFLECT_PAD and C.ops.SYMMETRIC_PAD\n        constant_value: the value used to fill the padding cells, only meaningful under CONSTANT mode.\n        name (str, optional): the name of the Function instance in the network\n    Returns:\n        :class:`~cntk.ops.functions.Function`\n    '''\n    from cntk.cntk_py import pad\n    if (any([len(_) != 2 for _  in pattern])):\n        raise ValueError(\"padding pattern should be a integer list with shape [n, 2]\")\n    x = sanitize_input(x)\n    head = [p[0] for p in reversed(pattern)]\n    foot = [p[1] for p in reversed(pattern)]\n    return pad(x, mode, head, foot, constant_value, name)\n",
  "sig": "pad(x|x: tensor to be padded.)(pattern|pattern (list of tuple with 2 integers): how many values to add before and after the contents of the tensor in each dimension.)(mode|mode (int): padding mode: C.ops.CONSTANT_PAD, C.ops.REFLECT_PAD and C.ops.SYMMETRIC_PAD)(constant_value|constant_value: the value used to fill the padding cells, only meaningful under CONSTANT mode.)(name|name (str, optional): the name of the Function instance in the network)",
  "dst": "    Pads a tensor according to the specified patterns.    Three padding modes are supported: CONSTANT / REFLECT / SYMMETRIC."
 },
 "cntk.ops.functions.load_model": {
  "code": "@typemap\ndef load_model(model, device=None, format=ModelFormat.CNTKv2):\n    '''\n    Alias for :func:`~cntk.ops.functions.Function.load`.\n    '''\n    return Function.load(model, device, format)\n",
  "sig": "load_model(model|either a file path of a model file or a byte buffer containing the binary representation of a model)(device|specifies the device to allocate the model on)(format|specifies the format of the file to load. if the specified format is ONNX, then model must be a filename)",
  "dst": "    Alias for :func:`~cntk.ops.functions.Function.load`."
 },
 "cntk.ops.space_to_depth": {
  "code": "@typemap\ndef space_to_depth(operand, block_size, name=''):\n    '''\n    Rearranges elements in the input tensor from the spatial dimensions to the depth dimension.\n\n    This is the reverse transformation of depth_to_space. This operation is useful for implementing \n    and testing sub-pixel convolution that is part of models for image super-resolution (see [1]).\n    It rearranges elements of an input tensor of shape (C, H, W) to a tensor of shape (C*b*b, H/b, W/b),\n    where b is the `block_size`, by rearranging non-overlapping spatial blocks of size `block_size` x `block_size`\n    into the depth/channel dimension at each location.\n    \n    Example:\n        >>> np.random.seed(3)\n        >>> x = np.random.randint(low=0, high=100, size=(1, 4, 6)).astype(np.float32)\n        >>> a = C.input_variable((1, 4, 6))\n        >>> s2d_op = C.space_to_depth(a, block_size=2)\n        >>> s2d_op.eval({a:x})\n        array([[[[ 24.,  56.,   0.],\n                 [ 96.,  44.,  39.]],\n        <BLANKLINE>\n                [[  3.,  72.,  21.],\n                 [ 20.,  93.,  14.]],\n        <BLANKLINE>\n                [[ 19.,  41.,  21.],\n                 [ 26.,  90.,  66.]],\n        <BLANKLINE>\n                [[ 74.,  10.,  38.],\n                 [ 81.,  22.,   2.]]]], dtype=float32)\n\n    Args:\n        operand: Input tensor, with dimensions :math:`[C \\\\times H \\\\times W]`.\n        block_size (int): Integer value. This defines the size of the spatial block whose elements\n         are moved to the depth dimension. Size of spatial dimensions (H, W) in the input tensor\n         must be divisible by math:`block_size`\n        name (str, optional): the name of the Function instance in the network\n    Returns:\n        :class:`~cntk.ops.functions.Function`\n\n    See also:\n        [1] W. Shi et. al. `: Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network <https://arxiv.org/abs/1609.05158>`_.\n    '''\n    from cntk.cntk_py import space_to_depth\n    operand = sanitize_input(operand, get_data_type(operand))\n    if not float(block_size).is_integer():\n        raise ValueError('block_size must be an integer.')\n    return space_to_depth(operand, block_size, name)\n",
  "sig": "space_to_depth(operand|operand: Input tensor, with dimensions :math:`[C \\\\times H \\\\times W]`.)(block_size|block_size (int): Integer value. This defines the size of the spatial block whose elements         are moved to the depth dimension. Size of spatial dimensions (H, W) in the input tensor         must be divisible by math:`block_size`)(name|name (str, optional): the name of the Function instance in the network)",
  "dst": "    Rearranges elements in the input tensor from the spatial dimensions to the depth dimension.    This is the reverse transformation of depth_to_space. This operation is useful for implementing     and testing sub-pixel convolution that is part of models for image super-resolution (see [1]).    It rearranges elements of an input tensor of shape (C, H, W) to a tensor of shape (C*b*b, H/b, W/b),    where b is the `block_size`, by rearranging non-overlapping spatial blocks of size `block_size` x `block_size`    into the depth/channel dimension at each location."
 },
 "cntk.learners.adadelta": {
  "code": "@typemap\ndef adadelta(parameters, lr=learning_parameter_schedule_per_sample(1), rho=0.95, epsilon=1e-8,\n             l1_regularization_weight=0.0, l2_regularization_weight=0.0,\n             gaussian_noise_injection_std_dev=0.0, gradient_clipping_threshold_per_sample=np.inf,\n             gradient_clipping_with_truncation=True, use_mean_gradient=None,\n             minibatch_size=None, epoch_size=None):\n    '''adadelta(parameters, lr, rho, epsilon, l1_regularization_weight=0, l2_regularization_weight=0, gaussian_noise_injection_std_dev=0, gradient_clipping_threshold_per_sample=np.inf, gradient_clipping_with_truncation=True)\n    Creates an AdaDelta learner instance to learn the parameters. See [1] for\n    more information.\n\n    Args:\n        parameters (list of parameters): list of network parameters to tune.\n         These can be obtained by the root operator's ``parameters``.\n        lr (float, list, output of :func:`learning_parameter_schedule`): a learning rate in float, or a learning rate schedule.\n         See also:  :func:`learning_parameter_schedule`\n        rho (float): exponential smooth factor for each minibatch.\n        epsilon (float): epsilon for sqrt.\n        l1_regularization_weight (float, optional): the L1 regularization weight per sample,\n         defaults to 0.0\n        l2_regularization_weight (float, optional): the L2 regularization weight per sample,\n         defaults to 0.0\n        gaussian_noise_injection_std_dev (float, optional): the standard deviation\n         of the Gaussian noise added to parameters post update, defaults to 0.0\n        gradient_clipping_threshold_per_sample (float, optional): clipping threshold\n         per sample, defaults to infinity\n        gradient_clipping_with_truncation (bool, default ``True``): use gradient clipping\n         with truncation\n        use_mean_gradient (bool, optional): use averaged gradient as input to learner.\n\n            deprecated:: 2.2\n                Use minibatch_size parameter to specify the reference minibatch size.\n        minibatch_size (int, default ``None``): The minibatch size that the learner's parameters are designed or pre-tuned for. This\n         size is usually set to the same as the minibatch data source's size. CNTK will perform automatic scaling of the parameters\n         to enable efficient model parameter update implementation while approximate the behavior of pre-designed and pre-tuned parameters.\n         In case that minibatch_size is not specified, CNTK will inherit the minibatch size from the learning rate schedule;\n         if the learning rate schedule does not specify the minibatch_size, CNTK will set it to :attr:`IGNORE`. Setting minibatch_size to :attr:`IGNORE`\n         will have the learner apply as it is preventing CNTK performing any hyper-parameter scaling. See also:  :func:`learning_parameter_schedule`\n        epoch_size (optional, int): number of samples as a scheduling unit for learning rate. See also:  :func:`learning_parameter_schedule`\n\n    Returns:\n        :class:`~cntk.learners.Learner`: learner instance that can be passed to\n        the :class:`~cntk.train.trainer.Trainer`\n\n    See also\n        [1]  Matthew D. Zeiler, `ADADELTA: An Adaptive Learning Rate Method\n        <https://arxiv.org/pdf/1212.5701.pdf>`_.\n    '''\n    gaussian_noise_injection_std_dev = \\\n        training_parameter_schedule(\n            gaussian_noise_injection_std_dev)\n    lr, minibatch_size = _infer_learning_rate_schedule_and_ref_minibatch_size(use_mean_gradient, minibatch_size, lr, epoch_size)\n\n    additional_options = cntk_py.AdditionalLearningOptions()\n    additional_options.l1_regularization_weight = l1_regularization_weight\n    additional_options.l2_regularization_weight = l2_regularization_weight\n    additional_options.gaussian_noise_injection_std_dev = gaussian_noise_injection_std_dev\n    additional_options.gradient_clipping_threshold_per_sample = gradient_clipping_threshold_per_sample\n    additional_options.gradient_clipping_with_truncation = gradient_clipping_with_truncation\n    minibatch_size = _infer_ref_minibatch_size_from_legacy_use_mean_gradient(minibatch_size, use_mean_gradient)\n    if minibatch_size is not None:\n        additional_options.dict_options[cntk_py.Learner._MINIBATCH_SIZE] = cntk_py.SizeTWrapper(minibatch_size) #need this to make proper typed DictionaryValue\n\n    opt = cntk_py.ada_delta_learner(parameters, lr, rho, epsilon,\n                                    additional_options)\n    opt.is_minibatch_size_explicitly_specified = minibatch_size is not None\n    return opt\n",
  "sig": "adadelta(parameters|parameters (list of parameters): list of network parameters to tune.         These can be obtained by the root operator's ``parameters``.)(lr|lr (float, list, output of :func:`learning_parameter_schedule`): a learning rate in float, or a learning rate schedule.         See also:  :func:`learning_parameter_schedule`)(rho|rho (float): exponential smooth factor for each minibatch.)(epsilon|epsilon (float): epsilon for sqrt.)(l1_regularization_weight|l1_regularization_weight (float, optional): the L1 regularization weight per sample,         defaults to 0.0)(l2_regularization_weight|l2_regularization_weight (float, optional): the L2 regularization weight per sample,         defaults to 0.0)(gaussian_noise_injection_std_dev|gaussian_noise_injection_std_dev (float, optional): the standard deviation         of the Gaussian noise added to parameters post update, defaults to 0.0)(gradient_clipping_threshold_per_sample|gradient_clipping_threshold_per_sample (float, optional): clipping threshold         per sample, defaults to infinity)(gradient_clipping_with_truncation|gradient_clipping_with_truncation (bool, default ``True``): use gradient clipping         with truncation)(use_mean_gradient|use_mean_gradient (bool, optional): use averaged gradient as input to learner.            deprecated:: 2.2                Use minibatch_size parameter to specify the reference minibatch size.)(minibatch_size|minibatch_size (int, default ``None``): The minibatch size that the learner's parameters are designed or pre-tuned for. This         size is usually set to the same as the minibatch data source's size. CNTK will perform automatic scaling of the parameters         to enable efficient model parameter update implementation while approximate the behavior of pre-designed and pre-tuned parameters.         In case that minibatch_size is not specified, CNTK will inherit the minibatch size from the learning rate schedule;         if the learning rate schedule does not specify the minibatch_size, CNTK will set it to :attr:`IGNORE`. Setting minibatch_size to :attr:`IGNORE`         will have the learner apply as it is preventing CNTK performing any hyper-parameter scaling. See also:  :func:`learning_parameter_schedule`)(epoch_size|epoch_size (optional, int): number of samples as a scheduling unit for learning rate. See also:  :func:`learning_parameter_schedule`)",
  "dst": "    Creates an AdaDelta learner instance to learn the parameters. See [1] for    more information."
 },
 "cntk.learners.adagrad": {
  "code": "@typemap\ndef adagrad(parameters, lr, need_ave_multiplier=True,\n            l1_regularization_weight=0.0, l2_regularization_weight=0.0,\n            gaussian_noise_injection_std_dev=0.0, gradient_clipping_threshold_per_sample=np.inf,\n            gradient_clipping_with_truncation=True, use_mean_gradient=None,\n            minibatch_size=None, epoch_size=None):\n    '''adagrad(parameters, lr, need_ave_multiplier=True, l1_regularization_weight=0, l2_regularization_weight=0, gaussian_noise_injection_std_dev=0, gradient_clipping_threshold_per_sample=np.inf, gradient_clipping_with_truncation=True)\n    Creates an AdaGrad learner instance to learn the parameters. See [1] for\n    more information.\n\n    Args:\n        parameters (list of parameters): list of network parameters to tune.\n         These can be obtained by the root operator's ``parameters``.\n        lr (float, list, output of :func:`learning_parameter_schedule`): a learning rate in float, or a learning rate schedule.\n         See also:  :func:`learning_parameter_schedule`\n        need_ave_multiplier (bool, default):\n        l1_regularization_weight (float, optional): the L1 regularization weight per sample,\n         defaults to 0.0\n        l2_regularization_weight (float, optional): the L2 regularization weight per sample,\n         defaults to 0.0\n        gaussian_noise_injection_std_dev (float, optional): the standard deviation\n         of the Gaussian noise added to parameters post update, defaults to 0.0\n        gradient_clipping_threshold_per_sample (float, optional): clipping threshold\n         per sample, defaults to infinity\n        gradient_clipping_with_truncation (bool, default ``True``): use gradient clipping\n         with truncation\n        use_mean_gradient (bool, optional): use averaged gradient as input to learner.\n\n            deprecated:: 2.2\n                Use minibatch_size parameter to specify the reference minibatch size.\n        minibatch_size (int, default ``None``): The minibatch size that the learner's parameters are designed or pre-tuned for. This\n         size is usually set to the same as the minibatch data source's size. CNTK will perform automatic scaling of the parameters\n         to enable efficient model parameter update implementation while approximate the behavior of pre-designed and pre-tuned parameters.\n         In case that minibatch_size is not specified, CNTK will inherit the minibatch size from the learning rate schedule;\n         if the learning rate schedule does not specify the minibatch_size, CNTK will set it to :attr:`IGNORE`. Setting minibatch_size to :attr:`IGNORE`\n         will have the learner apply as it is preventing CNTK performing any hyper-parameter scaling. See also:  :func:`learning_parameter_schedule`\n        epoch_size (optional, int): number of samples as a scheduling unit for learning rate. See also:  :func:`learning_parameter_schedule`\n\n    Returns:\n        :class:`~cntk.learners.Learner`: learner instance that can be passed to\n        the :class:`~cntk.train.trainer.Trainer`\n\n    See also:\n        [1]  J. Duchi, E. Hazan, and Y. Singer. `Adaptive Subgradient Methods\n        for Online Learning and Stochastic Optimization\n        <http://www.magicbroom.info/Papers/DuchiHaSi10.pdf>`_. The Journal of\n        Machine Learning Research, 2011.\n    '''\n    lr, minibatch_size = _infer_learning_rate_schedule_and_ref_minibatch_size(use_mean_gradient, minibatch_size, lr, epoch_size)\n    gaussian_noise_injection_std_dev = \\\n        training_parameter_schedule(\n            gaussian_noise_injection_std_dev)\n\n    additional_options = cntk_py.AdditionalLearningOptions()\n    additional_options.l1_regularization_weight = l1_regularization_weight\n    additional_options.l2_regularization_weight = l2_regularization_weight\n    additional_options.gaussian_noise_injection_std_dev = gaussian_noise_injection_std_dev\n    additional_options.gradient_clipping_threshold_per_sample = gradient_clipping_threshold_per_sample\n    additional_options.gradient_clipping_with_truncation = gradient_clipping_with_truncation\n    minibatch_size = _infer_ref_minibatch_size_from_legacy_use_mean_gradient(minibatch_size, use_mean_gradient)\n    if minibatch_size is not None:\n        additional_options.dict_options[cntk_py.Learner._MINIBATCH_SIZE] = cntk_py.SizeTWrapper(minibatch_size) #need this to make proper typed DictionaryValue\n\n    opt = cntk_py.ada_grad_learner(parameters, lr, need_ave_multiplier,\n                                    additional_options)\n    opt.is_minibatch_size_explicitly_specified = minibatch_size is not None\n    return opt\n",
  "sig": "adagrad(parameters|parameters (list of parameters): list of network parameters to tune.         These can be obtained by the root operator's ``parameters``.)(lr|lr (float, list, output of :func:`learning_parameter_schedule`): a learning rate in float, or a learning rate schedule.         See also:  :func:`learning_parameter_schedule`)(need_ave_multiplier|need_ave_multiplier (bool, default):)(l1_regularization_weight|l1_regularization_weight (float, optional): the L1 regularization weight per sample,         defaults to 0.0)(l2_regularization_weight|l2_regularization_weight (float, optional): the L2 regularization weight per sample,         defaults to 0.0)(gaussian_noise_injection_std_dev|gaussian_noise_injection_std_dev (float, optional): the standard deviation         of the Gaussian noise added to parameters post update, defaults to 0.0)(gradient_clipping_threshold_per_sample|gradient_clipping_threshold_per_sample (float, optional): clipping threshold         per sample, defaults to infinity)(gradient_clipping_with_truncation|gradient_clipping_with_truncation (bool, default ``True``): use gradient clipping         with truncation)(use_mean_gradient|use_mean_gradient (bool, optional): use averaged gradient as input to learner.            deprecated:: 2.2                Use minibatch_size parameter to specify the reference minibatch size.)(minibatch_size|minibatch_size (int, default ``None``): The minibatch size that the learner's parameters are designed or pre-tuned for. This         size is usually set to the same as the minibatch data source's size. CNTK will perform automatic scaling of the parameters         to enable efficient model parameter update implementation while approximate the behavior of pre-designed and pre-tuned parameters.         In case that minibatch_size is not specified, CNTK will inherit the minibatch size from the learning rate schedule;         if the learning rate schedule does not specify the minibatch_size, CNTK will set it to :attr:`IGNORE`. Setting minibatch_size to :attr:`IGNORE`         will have the learner apply as it is preventing CNTK performing any hyper-parameter scaling. See also:  :func:`learning_parameter_schedule`)(epoch_size|epoch_size (optional, int): number of samples as a scheduling unit for learning rate. See also:  :func:`learning_parameter_schedule`)",
  "dst": "    Creates an AdaGrad learner instance to learn the parameters. See [1] for    more information."
 },
 "cntk.learners.adam": {
  "code": "@typemap\ndef adam(parameters, lr, momentum, unit_gain=default_unit_gain_value(),\n         variance_momentum=momentum_schedule_per_sample(0.9999986111120757),\n         l1_regularization_weight=0.0, l2_regularization_weight=0.0,\n         gaussian_noise_injection_std_dev=0.0, gradient_clipping_threshold_per_sample=np.inf,\n         gradient_clipping_with_truncation=True, use_mean_gradient=None, epsilon=1e-8, adamax=False,\n         minibatch_size=None, epoch_size=None):\n    '''adam(parameters, lr, momentum, unit_gain=default_unit_gain_value(), variance_momentum=momentum_schedule_per_sample(0.9999986111120757), l1_regularization_weight=0, l2_regularization_weight=0, gaussian_noise_injection_std_dev=0, gradient_clipping_threshold_per_sample=np.inf, gradient_clipping_with_truncation=True, epsilon=1e-8, adamax=False)\n    Creates an Adam learner instance to learn the parameters. See [1] for more\n    information.\n\n    Args:\n        parameters (list of parameters): list of network parameters to tune.\n         These can be obtained by the root operator's ``parameters``.\n        lr (float, list, output of :func:`learning_parameter_schedule`): a learning rate in float, or a learning rate schedule.\n         See also:  :func:`learning_parameter_schedule`\n        momentum (float, list, output of :func:`momentum_schedule`): momentum schedule. Note that this is the beta1 parameter in the Adam paper [1]. \n         For additional information, please refer to the :cntkwiki:`this CNTK Wiki article <BrainScript-SGD-Block#converting-learning-rate-and-momentum-parameters-from-other-toolkits>`.\n        unit_gain: when ``True``, momentum is interpreted as a unit-gain filter. Defaults\n         to the value returned by :func:`default_unit_gain_value`.\n        variance_momentum (float, list, output of :func:`momentum_schedule`): variance momentum schedule. \n         Note that this is the beta2 parameter in the Adam paper [1]. Defaults to ``momentum_schedule_per_sample(0.9999986111120757)``. \n        l1_regularization_weight (float, optional): the L1 regularization weight per sample,\n         defaults to 0.0\n        l2_regularization_weight (float, optional): the L2 regularization weight per sample,\n         defaults to 0.0\n        gaussian_noise_injection_std_dev (float, optional): the standard deviation\n         of the Gaussian noise added to parameters post update, defaults to 0.0\n        gradient_clipping_threshold_per_sample (float, optional): clipping threshold\n         per sample, defaults to infinity\n        gradient_clipping_with_truncation (bool, default ``True``): use gradient clipping\n         with truncation\n        use_mean_gradient (bool, optional): use averaged gradient as input to learner.\n\n            deprecated:: 2.2\n                Use minibatch_size parameter to specify the reference minibatch size.\n        epsilon (float, optional): numerical stability constant,\n         defaults to 1e-8\n        adamax: when ``True``, use infinity-norm variance momentum update instead of L2. Defaults\n         to False\n        minibatch_size (int, default ``None``): The minibatch size that the learner's parameters are designed or pre-tuned for. This\n         size is usually set to the same as the minibatch data source's size. CNTK will perform automatic scaling of the parameters\n         to enable efficient model parameter update implementation while approximate the behavior of pre-designed and pre-tuned parameters.\n         In case that minibatch_size is not specified, CNTK will inherit the minibatch size from the learning rate schedule;\n         if the learning rate schedule does not specify the minibatch_size, CNTK will set it to :attr:`IGNORE`. Setting minibatch_size to :attr:`IGNORE`\n         will have the learner apply as it is preventing CNTK performing any hyper-parameter scaling. See also:  :func:`learning_parameter_schedule`\n        epoch_size (optional, int): number of samples as a scheduling unit for learning rate, momentum and variance_momentum. See also:  :func:`learning_parameter_schedule`\n\n    Returns:\n        :class:`~cntk.learners.Learner`: learner instance that can be passed to\n        the :class:`~cntk.train.trainer.Trainer`\n\n    See also:\n        [1] D. Kingma, J. Ba. `Adam: A Method for Stochastic Optimization\n        <https://arxiv.org/abs/1412.6980>`_. International Conference for\n        Learning Representations, 2015.\n    '''\n    lr, minibatch_size = _infer_learning_rate_schedule_and_ref_minibatch_size(use_mean_gradient, minibatch_size, lr, epoch_size)\n\n    momentum = _infer_learning_parameter_schedule(momentum, minibatch_size, epoch_size)\n    _verify_momentum_type(momentum)\n    variance_momentum = _infer_learning_parameter_schedule(variance_momentum, minibatch_size, epoch_size)\n    _verify_momentum_type(variance_momentum)\n    gaussian_noise_injection_std_dev = \\\n        training_parameter_schedule(\n            gaussian_noise_injection_std_dev)\n\n    additional_options = cntk_py.AdditionalLearningOptions()\n    additional_options.l1_regularization_weight = l1_regularization_weight\n    additional_options.l2_regularization_weight = l2_regularization_weight\n    additional_options.gaussian_noise_injection_std_dev = gaussian_noise_injection_std_dev\n    additional_options.gradient_clipping_threshold_per_sample = gradient_clipping_threshold_per_sample\n    additional_options.gradient_clipping_with_truncation = gradient_clipping_with_truncation\n    if minibatch_size is not None:\n        additional_options.dict_options[cntk_py.Learner._MINIBATCH_SIZE] = cntk_py.SizeTWrapper(minibatch_size) #need this to make proper typed DictionaryValue\n\n    opt = cntk_py.adam_learner(parameters, lr, momentum, unit_gain,\n                                variance_momentum, epsilon, adamax, additional_options)\n    opt.is_minibatch_size_explicitly_specified = minibatch_size is not None\n    return opt\n",
  "sig": "adam(parameters|parameters (list of parameters): list of network parameters to tune.         These can be obtained by the root operator's ``parameters``.)(lr|lr (float, list, output of :func:`learning_parameter_schedule`): a learning rate in float, or a learning rate schedule.         See also:  :func:`learning_parameter_schedule`)(momentum|momentum (float, list, output of :func:`momentum_schedule`): momentum schedule. Note that this is the beta1 parameter in the Adam paper [1].         For additional information, please refer to the :cntkwiki:`this CNTK Wiki article <BrainScript-SGD-Block#converting-learning-rate-and-momentum-parameters-from-other-toolkits>`.)(unit_gain|unit_gain: when ``True``, momentum is interpreted as a unit-gain filter. Defaults         to the value returned by :func:`default_unit_gain_value`.)(variance_momentum|variance_momentum (float, list, output of :func:`momentum_schedule`): variance momentum schedule.         Note that this is the beta2 parameter in the Adam paper [1]. Defaults to ``momentum_schedule_per_sample(0.9999986111120757)``. )(l1_regularization_weight|l1_regularization_weight (float, optional): the L1 regularization weight per sample,         defaults to 0.0)(l2_regularization_weight|l2_regularization_weight (float, optional): the L2 regularization weight per sample,         defaults to 0.0)(gaussian_noise_injection_std_dev|gaussian_noise_injection_std_dev (float, optional): the standard deviation         of the Gaussian noise added to parameters post update, defaults to 0.0)(gradient_clipping_threshold_per_sample|gradient_clipping_threshold_per_sample (float, optional): clipping threshold         per sample, defaults to infinity)(gradient_clipping_with_truncation|gradient_clipping_with_truncation (bool, default ``True``): use gradient clipping         with truncation)(use_mean_gradient|use_mean_gradient (bool, optional): use averaged gradient as input to learner.            deprecated:: 2.2                Use minibatch_size parameter to specify the reference minibatch size.)(epsilon|epsilon (float, optional): numerical stability constant,         defaults to 1e-8         to False)(adamax|NULL)(minibatch_size|minibatch_size (int, default ``None``): The minibatch size that the learner's parameters are designed or pre-tuned for. This         size is usually set to the same as the minibatch data source's size. CNTK will perform automatic scaling of the parameters         to enable efficient model parameter update implementation while approximate the behavior of pre-designed and pre-tuned parameters.         In case that minibatch_size is not specified, CNTK will inherit the minibatch size from the learning rate schedule;         if the learning rate schedule does not specify the minibatch_size, CNTK will set it to :attr:`IGNORE`. Setting minibatch_size to :attr:`IGNORE`         will have the learner apply as it is preventing CNTK performing any hyper-parameter scaling. See also:  :func:`learning_parameter_schedule`)(epoch_size|epoch_size (optional, int): number of samples as a scheduling unit for learning rate, momentum and variance_momentum. See also:  :func:`learning_parameter_schedule`)",
  "dst": "    Creates an Adam learner instance to learn the parameters. See [1] for more    information."
 },
 "cntk.learners.rmsprop": {
  "code": "@typemap\ndef rmsprop(parameters, lr,\n            gamma, inc, dec, max, min,\n            need_ave_multiplier=True,\n            l1_regularization_weight=0.0, l2_regularization_weight=0.0,\n            gaussian_noise_injection_std_dev=0.0, gradient_clipping_threshold_per_sample=np.inf,\n            gradient_clipping_with_truncation=True, use_mean_gradient=None,\n            minibatch_size=None, epoch_size=None):\n    '''rmsprop(parameters, lr, gamma, inc, dec, max, min, need_ave_multiplier=True, l1_regularization_weight=0, l2_regularization_weight=0, gaussian_noise_injection_std_dev=0, gradient_clipping_threshold_per_sample=np.inf, gradient_clipping_with_truncation=True)\n    Creates an RMSProp learner instance to learn the parameters.\n\n    Args:\n        parameters (list of parameters): list of network parameters to tune.\n         These can be obtained by the root operator's ``parameters``.\n        lr (float, list, output of :func:`learning_parameter_schedule`): a learning rate in float, or a learning rate schedule.\n         See also:  :func:`learning_parameter_schedule`\n        gamma (float): Trade-off factor for current and previous gradients. Common value is 0.95. Should be in range (0.0, 1.0)\n        inc (float): Increasing factor when trying to adjust current learning_rate. Should be greater than 1\n        dec (float): Decreasing factor when trying to adjust current learning_rate. Should be in range (0.0, 1.0)\n        max (float): Maximum scale allowed for the initial learning_rate. Should be greater than zero and min\n        min (float): Minimum scale allowed for the initial learning_rate. Should be greater than zero\n        need_ave_multiplier (bool, default ``True``):\n        l1_regularization_weight (float, optional): the L1 regularization weight per sample,\n         defaults to 0.0\n        l2_regularization_weight (float, optional): the L2 regularization weight per sample,\n         defaults to 0.0\n        gaussian_noise_injection_std_dev (float, optional): the standard deviation\n         of the Gaussian noise added to parameters post update, defaults to 0.0\n        gradient_clipping_threshold_per_sample (float, optional): clipping threshold\n         per sample, defaults to infinity\n        gradient_clipping_with_truncation (bool, default ``True``): use gradient clipping\n         with truncation\n        use_mean_gradient (bool, optional): use averaged gradient as input to learner.\n\n            deprecated:: 2.2\n                Use minibatch_size parameter to specify the reference minibatch size.\n        minibatch_size (int, default ``None``): The minibatch size that the learner's parameters are designed or pre-tuned for. This\n         size is usually set to the same as the minibatch data source's size. CNTK will perform automatic scaling of the parameters\n         to enable efficient model parameter update implementation while approximate the behavior of pre-designed and pre-tuned parameters.\n         In case that minibatch_size is not specified, CNTK will inherit the minibatch size from the learning rate schedule;\n         if the learning rate schedule does not specify the minibatch_size, CNTK will set it to :attr:`IGNORE`. Setting minibatch_size to :attr:`IGNORE`\n         will have the learner apply as it is preventing CNTK performing any hyper-parameter scaling. See also:  :func:`learning_parameter_schedule`\n        epoch_size (optional, int): number of samples as a scheduling unit for learning rate. See also:  :func:`learning_parameter_schedule`\n\n    Returns:\n        :class:`~cntk.learners.Learner`: learner instance that can be passed to\n        the :class:`~cntk.train.trainer.Trainer`\n    '''\n    lr, minibatch_size = _infer_learning_rate_schedule_and_ref_minibatch_size(use_mean_gradient, minibatch_size, lr, epoch_size)\n\n    gaussian_noise_injection_std_dev = \\\n        training_parameter_schedule(\n            gaussian_noise_injection_std_dev)\n\n    additional_options = cntk_py.AdditionalLearningOptions()\n    additional_options.l1_regularization_weight = l1_regularization_weight\n    additional_options.l2_regularization_weight = l2_regularization_weight\n    additional_options.gaussian_noise_injection_std_dev = gaussian_noise_injection_std_dev\n    additional_options.gradient_clipping_threshold_per_sample = gradient_clipping_threshold_per_sample\n    additional_options.gradient_clipping_with_truncation = gradient_clipping_with_truncation\n    minibatch_size = _infer_ref_minibatch_size_from_legacy_use_mean_gradient(minibatch_size, use_mean_gradient)\n    if minibatch_size is not None:\n        additional_options.dict_options[cntk_py.Learner._MINIBATCH_SIZE] = cntk_py.SizeTWrapper(minibatch_size) #need this to make proper typed DictionaryValue\n\n    opt = cntk_py.rmsprop_learner(parameters, lr, gamma, inc, dec, max, min,\n                                   need_ave_multiplier, additional_options)\n    opt.is_minibatch_size_explicitly_specified = minibatch_size is not None\n    return opt\n",
  "sig": "rmsprop(parameters|parameters (list of parameters): list of network parameters to tune.         These can be obtained by the root operator's ``parameters``.)(lr|lr (float, list, output of :func:`learning_parameter_schedule`): a learning rate in float, or a learning rate schedule.         See also:  :func:`learning_parameter_schedule`)(gamma|gamma (float): Trade-off factor for current and previous gradients. Common value is 0.95. Should be in range (0.0, 1.0))(inc|inc (float): Increasing factor when trying to adjust current learning_rate. Should be greater than 1)(dec|dec (float): Decreasing factor when trying to adjust current learning_rate. Should be in range (0.0, 1.0))(max|max (float): Maximum scale allowed for the initial learning_rate. Should be greater than zero and min)(min|minibatch_size (int, default ``None``): The minibatch size that the learner's parameters are designed or pre-tuned for. This)(need_ave_multiplier|need_ave_multiplier (bool, default ``True``):)(l1_regularization_weight|l1_regularization_weight (float, optional): the L1 regularization weight per sample,         defaults to 0.0)(l2_regularization_weight|l2_regularization_weight (float, optional): the L2 regularization weight per sample,         defaults to 0.0)(gaussian_noise_injection_std_dev|gaussian_noise_injection_std_dev (float, optional): the standard deviation         of the Gaussian noise added to parameters post update, defaults to 0.0)(gradient_clipping_threshold_per_sample|gradient_clipping_threshold_per_sample (float, optional): clipping threshold         per sample, defaults to infinity)(gradient_clipping_with_truncation|gradient_clipping_with_truncation (bool, default ``True``): use gradient clipping         with truncation)(use_mean_gradient|use_mean_gradient (bool, optional): use averaged gradient as input to learner.            deprecated:: 2.2                Use minibatch_size parameter to specify the reference minibatch size.)(minibatch_size|minibatch_size (int, default ``None``): The minibatch size that the learner's parameters are designed or pre-tuned for. This         size is usually set to the same as the minibatch data source's size. CNTK will perform automatic scaling of the parameters         to enable efficient model parameter update implementation while approximate the behavior of pre-designed and pre-tuned parameters.         In case that minibatch_size is not specified, CNTK will inherit the minibatch size from the learning rate schedule;         if the learning rate schedule does not specify the minibatch_size, CNTK will set it to :attr:`IGNORE`. Setting minibatch_size to :attr:`IGNORE`         will have the learner apply as it is preventing CNTK performing any hyper-parameter scaling. See also:  :func:`learning_parameter_schedule`)(epoch_size|epoch_size (optional, int): number of samples as a scheduling unit for learning rate. See also:  :func:`learning_parameter_schedule`)",
  "dst": "    Creates an RMSProp learner instance to learn the parameters."
 },
 "cntk.ops.functions.Function.grad": {
  "code": "    @typemap\n    def grad(self, at, wrt=None, outputs=None, device=None, as_numpy=True, grad_root=None):\n        '''\n        Computes the gradient of this Function at location ``at`` with respect to ``wrt``.\n        The Function must have a single output.\n\n        Example:\n            >>> x = C.input_variable(shape=(1,), needs_gradient=True)\n            >>> y = C.sqrt(x)\n            >>> a = np.asarray([1,4,16],dtype=np.float32).reshape(3,1)\n            >>> y.grad({x:a})\n            array([[ 0.5  ],\n            <BLANKLINE>\n                   [ 0.25 ],\n            <BLANKLINE>\n                   [ 0.125]], dtype=float32)\n\n        Args:\n            at (dict) : mapping of the Function's arguments to values\n            wrt (list, default `None`): list of Variables with respect to which the\n             gradient will be computed. If omitted, the gradients with\n             respect to all arguments of this Function that need gradient will be computed.\n            outputs (iterable, optional): outputs (including intermediate outputs in the graph)\n             to fetch values for. If not specified, values for none of the outputs are fetched.\n            device (:class:`~cntk.device.DeviceDescriptor`, default `None`): the device\n             descriptor that contains the type and id of the device on which the\n             computation is performed. If `None`, the default device is used.\n            as_numpy (bool, default `True`): whether to return the gradients as a NumPy array. Default True.\n             Specifying this as False returns a CNTK Value which avoids a\n             costly conversion but returns a somewhat opaque object. Also, the Value objects\n             are temporary and only guaranteed to be valid until the next forward/eval/backward/grad call.\n             You must explicitly clone the temporay Value objects if they need to be accessed later.\n            grad_root (:class:`~cntk.variables.Variable`, optional): specify the root of gradients calculation.\n             If not specified, the output of this function will be used as gradient root.\n\n        Returns:\n            dict or NumPy Array or a tuple of these: Dict with keys of ``wrt`` variables and gradient values of\n            ``wrt`` variables. A single NumPy array if there is only one gradient value.\n            If ``outputs`` were specified (to fetch values for), this method returns a tuple where the 2nd element\n            of the tuple is the ``outputs`` values; a dict with keys of specified ``outputs`` variables and\n            values of computed ``outputs``, or a single NumPy array if there is only one output value.\n            Each element has the same shape as the ``wrt`` or ``outputs`` variables including dynamic axes\n            (such as the batch axis).\n        '''\n        if device is None:\n            device = DeviceDescriptor.use_default_device()\n\n        in_var_map = sanitize_var_map(self.arguments, at, None, device)\n\n        if outputs is None:\n            outputs = []\n\n        if wrt is None:\n            wrt = [arg for arg in self.arguments if arg.needs_gradient]\n            if len(wrt) == 0:\n                raise ValueError(\"None of the Function '%s' arguments have 'needs_gradient == True'\" % str(self))\n\n        output_map = {v: None for v in outputs}\n        wrt_map = {v: None for v in wrt}\n\n        if grad_root is None:\n            super(Function, self).gradients(in_var_map, wrt_map, output_map, device)\n        else:\n            super(Function, self).gradients(in_var_map, grad_root, wrt_map, output_map, device)\n\n        if as_numpy:\n            for k in output_map:\n                output_map[k] = _value_as_sequence_or_array(output_map[k], k)\n            for k in wrt_map:\n                wrt_map[k] = _value_as_sequence_or_array(wrt_map[k], k)\n\n        if len(output_map) == 0:\n            return sanitize_variable_value_dict(wrt_map)\n        else:\n            return sanitize_variable_value_dict(wrt_map), sanitize_variable_value_dict(output_map)\n",
  "sig": "grad(self|NULL)(at|at (dict) : mapping of the Function's arguments to values)(wrt|wrt (list, default `None`): list of Variables with respect to which the             respect to all arguments of this Function that need gradient will be computed.)(outputs|outputs (iterable, optional): outputs (including intermediate outputs in the graph)             to fetch values for. If not specified, values for none of the outputs are fetched.)(device|device (:class:`~cntk.device.DeviceDescriptor`, default `None`): the device             descriptor that contains the type and id of the device on which the             computation is performed. If `None`, the default device is used.)(as_numpy|as_numpy (bool, default `True`): whether to return the gradients as a NumPy array. Default True.             Specifying this as False returns a CNTK Value which avoids a             costly conversion but returns a somewhat opaque object. Also, the Value objects             are temporary and only guaranteed to be valid until the next forward/eval/backward/grad call.             You must explicitly clone the temporay Value objects if they need to be accessed later.             If not specified, the output of this function will be used as gradient root.)(grad_root|specify the root of gradients calculation. If not specified, the output of this function will be used as gradient root)",
  "dst": "        Computes the gradient of this Function at location ``at`` with respect to ``wrt``.        The Function must have a single output."
 },
 "cntk.ops.elu": {
  "code": "@typemap\ndef elu(x, alpha=1.0, name=''):\n    '''\n    Exponential linear unit operation. Computes the element-wise exponential linear\n    of ``x``: ``max(x, 0)`` for ``x >= 0`` and ``x``: ``alpha * (exp(x)-1)`` otherwise.\n\n    The output tensor has the same shape as ``x``.\n\n    Example:\n        >>> C.elu([[-1, -0.5, 0, 1, 2]]).eval()\n        array([[-0.632121, -0.393469,  0.      ,  1.      ,  2.      ]], dtype=float32)\n\n    Args:\n        x (`numpy.array` or :class:`~cntk.ops.functions.Function`): any :class:`~cntk.ops.functions.Function` that outputs a tensor.\n        name (`str`, default to ''): the name of the Function instance in the network\n\n    Returns:\n        cntk.ops.functions.Function:\n        An instance of :class:`~cntk.ops.functions.Function`\n    '''\n    from cntk.cntk_py import elu\n    x = sanitize_input(x)\n    return elu(x, alpha, name)\n",
  "sig": "elu(x|x (`numpy.array` or :class:`~cntk.ops.functions.Function`): any :class:`~cntk.ops.functions.Function` that outputs a tensor.)(alpha|NULL)(name|name (`str`, default to ''): the name of the Function instance in the network)",
  "dst": "    Exponential linear unit operation. Computes the element-wise exponential linear    of ``x``: ``max(x, 0)`` for ``x >= 0`` and ``x``: ``alpha * (exp(x)-1)`` otherwise.    The output tensor has the same shape as ``x``."
 },
 "cntk.ops.hard_sigmoid": {
  "code": "@typemap\ndef hard_sigmoid(x, alpha, beta, name = ''):\n    '''\n    Computes the element-wise HardSigmoid function, y = max(0, min(1, alpha * x + beta)).\n\n    Example:\n        >>> alpha = 1 \n        >>> beta = 2\n        >>> C.hard_sigmoid([-2.5, -1.5, 1], alpha, beta).eval()\n        array([ 0. ,  0.5,  1. ], dtype=float32)\n\n    Args:\n        x: numpy array or any :class:`~cntk.ops.functions.Function` that outputs a tensor\n        alpha (float): the alpha term of the above equation.\n        beta (float): the beta term of the above equation.\n        name (str): the name of the Function instance in the network\n    Returns:\n        :class:`~cntk.ops.functions.Function`\n    '''\n    from cntk.cntk_py import hard_sigmoid\n    x = sanitize_input(x)\n    return hard_sigmoid(x, alpha, beta, name)\n",
  "sig": "hard_sigmoid(x|x: numpy array or any :class:`~cntk.ops.functions.Function` that outputs a tensor)(alpha|alpha (float): the alpha term of the above equation.)(beta|beta (float): the beta term of the above equation.)(name|name (str): the name of the Function instance in the network)",
  "dst": "    Computes the element-wise HardSigmoid function, y = max(0, min(1, alpha * x + beta))."
 },
 "cntk.ops.selu": {
  "code": "@typemap\ndef selu(x, scale=1.0507009873554804934193349852946, alpha=1.6732632423543772848170429916717, name=''):\n    '''\n    Scaled exponential linear unit operation. Computes the element-wise exponential linear\n    of ``x``: ``scale * x`` for ``x >= 0`` and ``x``: ``scale * alpha * (exp(x)-1)`` otherwise.\n\n    The output tensor has the same shape as ``x``.\n\n    Example:\n        >>> C.selu([[-1, -0.5, 0, 1, 2]]).eval()\n        array([[-1.111331, -0.691758,  0.      ,  1.050701,  2.101402]], dtype=float32)\n\n    Args:\n        x (`numpy.array` or :class:`~cntk.ops.functions.Function`): any :class:`~cntk.ops.functions.Function` that outputs a tensor.\n        name (`str`, default to ''): the name of the Function instance in the network\n\n    Returns:\n        cntk.ops.functions.Function:\n        An instance of :class:`~cntk.ops.functions.Function`\n    '''\n    from cntk.cntk_py import selu\n    x = sanitize_input(x)\n    return selu(x, scale, alpha, name)\n",
  "sig": "selu(x|x (`numpy.array` or :class:`~cntk.ops.functions.Function`): any :class:`~cntk.ops.functions.Function` that outputs a tensor.)(scale|NULL)(alpha|NULL)(name|name (`str`, default to ''): the name of the Function instance in the network)",
  "dst": "    Scaled exponential linear unit operation. Computes the element-wise exponential linear    of ``x``: ``scale * x`` for ``x >= 0`` and ``x``: ``scale * alpha * (exp(x)-1)`` otherwise.    The output tensor has the same shape as ``x``."
 },
 "cntk.layers.sigmoid": {
  "code": "@typemap\ndef sigmoid(x, name=''):\n    '''\n    Computes the element-wise sigmoid of ``x``:\n\n    :math:`sigmoid(x) = {1 \\\\over {1+\\\\exp(-x)}}`\n\n    The output tensor has the same shape as ``x``.\n\n    Example:\n        >>> C.sigmoid([-2, -1., 0., 1., 2.]).eval()\n        array([ 0.119203,  0.268941,  0.5     ,  0.731059,  0.880797], dtype=float32)\n\n    Args:\n        x: numpy array or any :class:`~cntk.ops.functions.Function` that outputs a tensor\n        name (str, optional): the name of the Function instance in the network\n    Returns:\n        :class:`~cntk.ops.functions.Function`\n    '''\n    from cntk.cntk_py import sigmoid\n    x = sanitize_input(x)\n    return sigmoid(x, name)\n",
  "sig": "sigmoid(x|x: numpy array or any :class:`~cntk.ops.functions.Function` that outputs a tensor)(name|name (str, optional): the name of the Function instance in the network)",
  "dst": "    Computes the element-wise sigmoid of ``x``:    :math:`sigmoid(x) = {1 \\\\over {1+\\\\exp(-x)}}`    The output tensor has the same shape as ``x``."
 },
 "cntk.layers.softplus": {
  "code": "@typemap\ndef softplus(x, steepness=1, name=''):\n    '''\n    Softplus operation. Computes the element-wise softplus of ``x``:\n\n    :math:`\\\\mathrm{softplus}(x) = {\\\\log(1+\\\\exp(x))}`\n\n    The optional ``steepness`` allows to make the knee sharper (``steepness>1``) or softer, by computing\n    ``softplus(x * steepness) / steepness``.\n    (For very large steepness, this approaches a linear rectifier).\n\n    The output tensor has the same shape as ``x``.\n\n    Example:\n        >>> C.softplus([[-1, -0.5, 0, 1, 2]]).eval()\n        array([[ 0.313262,  0.474077,  0.693147,  1.313262,  2.126928]], dtype=float32)\n\n        >>> C.softplus([[-1, -0.5, 0, 1, 2]], steepness=4).eval()\n        array([[ 0.004537,  0.031732,  0.173287,  1.004537,  2.000084]], dtype=float32)\n\n    Args:\n        x (`numpy.array` or :class:`~cntk.ops.functions.Function`): any :class:`~cntk.ops.functions.Function` that outputs a tensor.\n        steepness (float, optional): optional steepness factor\n        name (`str`, default to ''): the name of the Function instance in the network\n    Returns:\n        cntk.ops.functions.Function:\n        An instance of :class:`~cntk.ops.functions.Function`\n    '''\n    from cntk.cntk_py import softplus\n    x = sanitize_input(x)\n    if steepness == 1:\n        return softplus(x, name)\n    xp = placeholder()\n    f = typemap(softplus)(steepness * xp) / steepness\n    return as_block(f, [(xp, x)], 'softplus', name)\n",
  "sig": "softplus(x|x (`numpy.array` or :class:`~cntk.ops.functions.Function`): any :class:`~cntk.ops.functions.Function` that outputs a tensor.)(steepness|steepness (float, optional): optional steepness factor)(name|name (`str`, default to ''): the name of the Function instance in the network)",
  "dst": "    Softplus operation. Computes the element-wise softplus of ``x``:    :math:`\\\\mathrm{softplus}(x) = {\\\\log(1+\\\\exp(x))}`    The optional ``steepness`` allows to make the knee sharper (``steepness>1``) or softer, by computing    ``softplus(x * steepness) / steepness``.    (For very large steepness, this approaches a linear rectifier).    The output tensor has the same shape as ``x``."
 },
 "cntk.ops.softsign": {
  "code": "@typemap\ndef softsign(x, steepness=1, name=''):\n    '''\n    Computes the element-wise softsign of ``x``:\n\n    :math:`sigmoid(x) = {x \\\\over {1+\\\\abs(x)}}`\n\n    The output tensor has the same shape as ``x``.\n\n    Example:\n        >>> C.softsign([[-1, 0, 1]]).eval()\n        array([[-0.5,  0. ,  0.5]], dtype=float32)\n\n    Args:\n        x: numpy array or any :class:`~cntk.ops.functions.Function` that outputs a tensor\n        name (str, optional): the name of the Function instance in the network\n    Returns:\n        :class:`~cntk.ops.functions.Function`\n    '''\n    from cntk.cntk_py import softsign\n    x = sanitize_input(x)\n    return softsign(x, name)\n",
  "sig": "softsign(x|x: numpy array or any :class:`~cntk.ops.functions.Function` that outputs a tensor)(steepness|NULL)(name|name (str, optional): the name of the Function instance in the network)",
  "dst": "    Computes the element-wise softsign of ``x``:    :math:`sigmoid(x) = {x \\\\over {1+\\\\abs(x)}}`    The output tensor has the same shape as ``x``."
 },
 "cntk.ops.tanh": {
  "code": "@typemap\ndef tanh(x, name=''):\n    '''\n    Computes the element-wise tanh of ``x``:\n\n    The output tensor has the same shape as ``x``.\n\n    Example:\n        >>> C.tanh([[1,2],[3,4]]).eval()\n        array([[ 0.761594,  0.964028],\n               [ 0.995055,  0.999329]], dtype=float32)\n\n    Args:\n        x: numpy array or any :class:`~cntk.ops.functions.Function` that outputs a tensor\n        name (str, optional): the name of the Function instance in the network\n    Returns:\n        :class:`~cntk.ops.functions.Function`\n    '''\n    from cntk.cntk_py import tanh\n    x = sanitize_input(x)\n    return tanh(x, name)\n",
  "sig": "tanh(x|x: numpy array or any :class:`~cntk.ops.functions.Function` that outputs a tensor)(name|name (str, optional): the name of the Function instance in the network)",
  "dst": "    Computes the element-wise tanh of ``x``:    The output tensor has the same shape as ``x``."
 },
 "cntk.ops.element_times": {
  "code": "@associative_multi_arg\n@typemap\ndef element_times(left, right, name=''):\n    '''\n    The output of this operation is the element-wise product of the two or more input\n    tensors. It supports broadcasting.\n\n    Example:\n        >>> C.element_times([1., 1., 1., 1.], [0.5, 0.25, 0.125, 0.]).eval()\n        array([ 0.5  ,  0.25 ,  0.125,  0.   ], dtype=float32)\n\n        >>> C.element_times([5., 10., 15., 30.], [2.]).eval()\n        array([ 10.,  20.,  30.,  60.], dtype=float32)\n\n        >>> C.element_times([5., 10., 15., 30.], [2.], [1., 2., 1., 2.]).eval()\n        array([  10.,   40.,   30.,  120.], dtype=float32)\n\n    Args:\n        arg1: left side tensor\n        arg2: right side tensor\n        *more_args: additional inputs\n        name (str, optional): the name of the Function instance in the network\n    Returns:\n        :class:`~cntk.ops.functions.Function`\n    '''\n    from cntk.cntk_py import element_times as cntk_py_element_times\n    dtype = get_data_type(left, right)\n    left = sanitize_input(left, dtype)\n    right = sanitize_input(right, dtype)\n    return cntk_py_element_times(left, right, name)\n",
  "sig": "element_times(left|left side tensor)(right|right side tensor)(name|name (str, optional): the name of the Function instance in the network)",
  "dst": "    The output of this operation is the element-wise product of the two or more input    tensors. It supports broadcasting."
 },
 "cntk.layers.layers.Embedding": {
  "code": "def Embedding(shape=None, init=default_override_or(C.glorot_uniform()), weights=None, name=''):\n    '''\n    Embedding(shape=None, init=glorot_uniform(), weights=None, name='')\n\n    Layer factory function to create a embedding layer.\n\n    An embedding is conceptually a lookup table. For every input token (e.g. a word or any category label), the corresponding\n    entry in the lookup table is returned.\n\n    In CNTK, discrete items such as words are represented as one-hot vectors.\n    The table lookup is realized as a matrix product, with a matrix\n    whose rows are the embedding vectors.\n    Note that multiplying a matrix from the left with a one-hot vector is the same as copying\n    out the row for which the input vector is 1.\n    CNTK has special optimizations to make this operation as efficient as an actual table lookup if the input is sparse.\n\n    The lookup table in this layer is learnable,\n    unless a user-specified one is supplied through the ``weights`` parameter.\n    For example, to use an existing embedding table from a file in numpy format, use this::\n\n      Embedding(weights=np.load('PATH.npy'))\n\n    To initialize a learnable lookup table with a given numpy array that is to be used as\n    the initial value, pass that array to the ``init`` parameter (not ``weights``).\n\n    An ``Embedding`` instance owns its weight parameter tensor `E`, and exposes it as an attribute ``.E``.\n\n    Example:\n     >>> # learnable embedding\n     >>> f = Embedding(5)\n     >>> x = C.input_variable(3)\n     >>> e = f(x)\n     >>> e.shape\n         (5,)\n     >>> f.E.shape\n         (3, 5)\n\n     >>> # user-supplied embedding\n     >>> f = Embedding(weights=[[.5, .3, .1, .4, .2], [.7, .6, .3, .2, .9]])\n     >>> f.E.value\n         array([[ 0.5,  0.3,  0.1,  0.4,  0.2],\n                [ 0.7,  0.6,  0.3,  0.2,  0.9]], dtype=float32)\n     >>> x = C.input_variable(2, is_sparse=True)\n     >>> e = f(x)\n     >>> e.shape\n         (5,)\n     >>> e(C.Value.one_hot([[1], [0], [0], [1]], num_classes=2))\n     array([[ 0.7,  0.6,  0.3,  0.2,  0.9],\n            [ 0.5,  0.3,  0.1,  0.4,  0.2],\n            [ 0.5,  0.3,  0.1,  0.4,  0.2],\n            [ 0.7,  0.6,  0.3,  0.2,  0.9]], dtype=float32)\n\n    Args:\n     shape (`int` or `tuple` of `ints`): vector or tensor dimension of the output of this layer\n     init (scalar or NumPy array or :mod:`cntk.initializer`, defaults to :func:`~cntk.initializer.glorot_uniform` ): (learnable embedding only) initial value of weights `E`\n     weights (NumPy array, mutually exclusive with ``init``, defuats to `None`): (user-supplied embedding only) the lookup table.\n      The matrix rows are the embedding vectors, ``weights[i,:]`` being the embedding that corresponds to input category `i`.\n     name (str, defaults to ''): the name of the function instance in the network\n\n    Returns:\n        cntk.ops.functions.Function:\n        A function that accepts one argument and applies the embedding operation to it\n    '''\n\n    if not is_default_override(init) and weights is not None:\n        raise ValueError('Embedding: init and weights options are mutually exclusive')\n\n    # parameters bound to this Function:\n    # no weights given: learn the embedding\n    if weights is None:\n        if shape is None:\n            raise ValueError('Embedding: output shape must be specified')\n        init = get_default_override(Embedding, init=init)\n        shape = _as_tuple(shape)\n        weight_shape = _INFERRED + shape\n        E = Parameter(weight_shape, init=init, name='E')\n    # weights given: use them as constant\n    else:\n        import numpy as np\n        weights = np.array(weights)\n        weight_shape = np.shape(weights)\n        if shape is not None: # user may give shape, then it must match\n            raise ValueError('Embedding: output shape must not be specified when weights are given')\n        E = Constant(weights, name='E')\n\n    # expression\n    @BlockFunction('Embedding', name)\n    def embed(x):\n        return times(x,E)\n    return embed\n",
  "sig": "Embedding(shape|shape (`int` or `tuple` of `ints`): vector or tensor dimension of the output of this layer)(init|init (scalar or NumPy array or :mod:`cntk.initializer`, defaults to :func:`~cntk.initializer.glorot_uniform` ): (learnable embedding only) initial value of weights `E`)(weights|weights (NumPy array, mutually exclusive with ``init``, defuats to `None`): (user-supplied embedding only) the lookup table.      The matrix rows are the embedding vectors, ``weights[i,:]`` being the embedding that corresponds to input category `i`.)(name|name (str, defaults to ''): the name of the function instance in the network)",
  "dst": "    Layer factory function to create a embedding layer.    An embedding is conceptually a lookup table. For every input token (e.g. a word or any category label), the corresponding    entry in the lookup table is returned.    In CNTK, discrete items such as words are represented as one-hot vectors.    The table lookup is realized as a matrix product, with a matrix    whose rows are the embedding vectors.    Note that multiplying a matrix from the left with a one-hot vector is the same as copying    out the row for which the input vector is 1.    CNTK has special optimizations to make this operation as efficient as an actual table lookup if the input is sparse.    The lookup table in this layer is learnable,    unless a user-specified one is supplied through the ``weights`` parameter.    For example, to use an existing embedding table from a file in numpy format, use this::    To initialize a learnable lookup table with a given numpy array that is to be used as    the initial value, pass that array to the ``init`` parameter (not ``weights``).    An ``Embedding`` instance owns its weight parameter tensor `E`, and exposes it as an attribute ``.E``."
 },
 "cntk.ops.flatten": {
  "code": "@typemap\ndef flatten(x, axis = None, name = ''):\n    '''\n    Flattens the input tensor into a 2D matrix.\n    If the input tensor has shape (d_0, d_1, ... d_n) then the output will have shape (d_0 X d_1 ... d_(axis-1), d_axis X d_(axis+1) ... X dn).\n\n    Example:\n        >>> # create 2x3x4 matrix, flatten the matrix at axis = 1\n        >>> shape = (2, 3, 4) \n        >>> data = np.reshape(np.arange(np.prod(shape), dtype = np.float32), shape)\n        >>> C.flatten(data, 1).eval()\n        array([[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n                 11.],\n               [ 12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,  22.,\n                 23.]], dtype=float32)\n\n    Args:\n        x: Input tensor.\n        axis (int): (Default to 0) Indicates up to which input dimensions (exclusive) should be flattened to the outer dimension of the output\n        name (str, optional): the name of the Function instance in the network\n\n    Returns:\n        :class:`~cntk.ops.functions.Function`\n    '''\n    from cntk.cntk_py import flatten\n    x = sanitize_input(x)\n    if axis is not None:\n        axis = sanitize_axis(axis)\n        return flatten(x, axis, name)\n    else:\n        return flatten(x, name)\n",
  "sig": "flatten(x|x: Input tensor.)(axis|axis (int): (Default to 0) Indicates up to which input dimensions (exclusive) should be flattened to the outer dimension of the output)(name|name (str, optional): the name of the Function instance in the network)",
  "dst": "    Flattens the input tensor into a 2D matrix.    If the input tensor has shape (d_0, d_1, ... d_n) then the output will have shape (d_0 X d_1 ... d_(axis-1), d_axis X d_(axis+1) ... X dn)."
 },
 "cntk.layers.layers.LayerNormalization": {
  "code": "def LayerNormalization(initial_scale=1, initial_bias=0, epsilon=default_override_or(0.00001), name=''):\n    '''\n    LayerNormalization(initial_scale=1, initial_bias=0, epsilon=0.00001, name='')\n\n    Layer factory function to create a function that implements layer normalization.\n\n    Layer normalization applies this formula to every input element (element-wise):\n    ``y = (x - mean(x)) / (stddev(x) + epsilon) * scale + bias``\n    where ``scale`` and ``bias`` are learned parameters of the same dimention as the input/output.\n\n    Example:\n     >>> f = LayerNormalization(initial_scale=2, initial_bias=1)\n     >>> f.update_signature(4)\n     >>> f([np.array([4,0,0,4])])  # result has mean 1 and standard deviation 2, reflecting the initial values for scale and bias\n         array([[ 2.99999, -0.99999, -0.99999,  2.99999]], dtype=float32)\n\n    Args:\n     initial_scale (float, default 1): initial value for the ``scale`` parameter\n     initial_bias (float, default 0): initial value for the ``bias`` parameter\n     epsilon (float, default 0.00001): epsilon added to the standard deviation to avoid division by 0\n     name (str, optional): the name of the Function instance in the network\n\n    Returns:\n        cntk.ops.functions.Function:\n        A function that accepts one argument and applies the operation to it\n\n    Todo:\n       Add paper reference.\n    '''\n    epsilon = get_default_override(LayerNormalization, epsilon=epsilon)\n\n    dtype = get_default_override(None, dtype=default_override_or(np.float32))\n\n    # parameters bound to this Function\n    scale = Parameter(_INFERRED, init=initial_scale, name='scale')  # TODO: if this gets usage then offer a Softplus version like Stabilizer() for stability?\n    bias  = Parameter(_INFERRED, init=initial_bias, name='bias')\n    # cast to specified data type. The default for number is float32 which might be different than desired. \n    epsilon = np.asarray(epsilon, dtype=dtype)\n\n    # expression\n    @BlockFunction('LayerNormalization', name)\n    def layer_normalize(x):\n        mean = reduce_mean(x) # normalize w.r.t. actual sample statistics\n        x0 = x - mean;\n        std = sqrt (reduce_mean (x0 * x0))\n        if (epsilon != 0):\n            std += epsilon\n        x_hat = x0 / std\n        return x_hat * scale + bias    # denormalize with learned parameters\n    return layer_normalize\n",
  "sig": "LayerNormalization(initial_scale|initial_scale (float, default 1): initial value for the ``scale`` parameter)(initial_bias|initial_bias (float, default 0): initial value for the ``bias`` parameter)(epsilon|epsilon (float, default 0.00001): epsilon added to the standard deviation to avoid division by 0)(name|name (str, optional): the name of the Function instance in the network)",
  "dst": "    Layer factory function to create a function that implements layer normalization.    Layer normalization applies this formula to every input element (element-wise):    ``y = (x - mean(x)) / (stddev(x) + epsilon) * scale + bias``    where ``scale`` and ``bias`` are learned parameters of the same dimention as the input/output."
 },
 "cntk.ops.leaky_relu": {
  "code": "@typemap\ndef leaky_relu(x, alpha = 0.01, name=''):\n    '''\n    Leaky Rectified linear operation. Computes the element-wise leaky rectified linear\n    of ``x``: ``max(x, 0)`` for ``x >= 0`` and ``x``: ``alpha*x`` otherwise.\n\n    The output tensor has the same shape as ``x``.\n\n    Example:\n        >>> C.leaky_relu([[-1, -0.5, 0, 1, 2]]).eval()\n        array([[-0.01 , -0.005,  0.   ,  1.   ,  2.   ]], dtype=float32)\n\n    Args:\n        x (`numpy.array` or :class:`~cntk.ops.functions.Function`): any :class:`~cntk.ops.functions.Function` that outputs a tensor.\n        alpha (float): the alpha term of the above equation.\n        name (`str`, default to ''): the name of the Function instance in the network\n\n    Returns:\n        cntk.ops.functions.Function:\n        An instance of :class:`~cntk.ops.functions.Function`\n    '''\n    from cntk.cntk_py import leaky_re_lu\n    x = sanitize_input(x)\n    return leaky_re_lu(x, alpha, name)\n",
  "sig": "leaky_relu(x|x (`numpy.array` or :class:`~cntk.ops.functions.Function`): any :class:`~cntk.ops.functions.Function` that outputs a tensor.)(alpha|alpha (float): the alpha term of the above equation.)(name|name (`str`, default to ''): the name of the Function instance in the network)",
  "dst": "    Leaky Rectified linear operation. Computes the element-wise leaky rectified linear    of ``x``: ``max(x, 0)`` for ``x >= 0`` and ``x``: ``alpha*x`` otherwise.    The output tensor has the same shape as ``x``."
 },
 "cntk.ops.param_relu": {
  "code": "@typemap\ndef param_relu(alpha, x, name=''):\n    '''\n    Parametric rectified linear operation. Computes the element-wise parameteric rectified linear\n    of ``x``: ``max(x, 0)`` for ``x >= 0`` and ``x``: ``alpha*x`` otherwise.\n\n    The output tensor has the same shape as ``x``.\n\n    Example:\n        >>> alpha = C.constant(value=[[0.5, 0.5, 0.5, 0.5, 0.5]])\n        >>> C.param_relu(alpha, [[-1, -0.5, 0, 1, 2]]).eval()\n        array([[-0.5 , -0.25,  0.  ,  1.  ,  2.  ]], dtype=float32)\n\n    Args:\n        alpha (:class:`~cntk.variables.Parameter`): same shape as x\n        x (`numpy.array` or :class:`~cntk.ops.functions.Function`): any :class:`~cntk.ops.functions.Function` that outputs a tensor.\n        name (`str`, default to ''): the name of the Function instance in the network\n\n    Returns:\n        cntk.ops.functions.Function:\n        An instance of :class:`~cntk.ops.functions.Function`\n    '''\n    from cntk.cntk_py import pre_lu\n    x = sanitize_input(x)\n    return pre_lu(alpha, x, name)\n",
  "sig": "param_relu(alpha|alpha (:class:`~cntk.variables.Parameter`): same shape as x)(x|x (`numpy.array` or :class:`~cntk.ops.functions.Function`): any :class:`~cntk.ops.functions.Function` that outputs a tensor.)(name|name (`str`, default to ''): the name of the Function instance in the network)",
  "dst": "    Parametric rectified linear operation. Computes the element-wise parameteric rectified linear    of ``x``: ``max(x, 0)`` for ``x >= 0`` and ``x``: ``alpha*x`` otherwise.    The output tensor has the same shape as ``x``."
 },
 "cntk.layers.blocks.RNNStep": {
  "code": "def RNNStep(shape, cell_shape=None, activation=default_override_or(sigmoid),\n            init=default_override_or(glorot_uniform()), init_bias=default_override_or(0),\n            enable_self_stabilization=default_override_or(False),\n            name=''):\n    '''\n    RNNStep(shape, cell_shape=None, activation=sigmoid, init=glorot_uniform(), init_bias=0, enable_self_stabilization=False, name='')\n\n    Layer factory function to create a plain RNN block for use inside a recurrence.\n    The RNN block implements one step of the recurrence and is stateless. It accepts the previous state as its first argument,\n    and outputs its new state.\n\n    Example:\n     >>> # a plain relu RNN layer\n     >>> from cntk.layers import *\n     >>> relu_rnn_layer = Recurrence(RNNStep(500, activation=C.relu))\n\n    Args:\n        shape (`int` or `tuple` of `ints`): vector or tensor dimension of the output of this layer\n        cell_shape (tuple, defaults to `None`): if given, then the output state is first computed at `cell_shape`\n         and linearly projected to `shape`\n        activation (:class:`~cntk.ops.functions.Function`, defaults to signmoid): function to apply at the end, e.g. `relu`\n        init (scalar or NumPy array or :mod:`cntk.initializer`, defaults to `glorot_uniform`): initial value of weights `W`\n        init_bias (scalar or NumPy array or :mod:`cntk.initializer`, defaults to 0): initial value of weights `b`\n        enable_self_stabilization (bool, defaults to `False`): if `True` then add a :func:`~cntk.layers.blocks.Stabilizer`\n         to all state-related projections (but not the data input)\n        name (str, defaults to ''): the name of the Function instance in the network\n\n    Returns:\n        :class:`~cntk.ops.functions.Function`:\n        A function ``(prev_h, input) -> h`` where ``h = activation(input @ W + prev_h @ R + b)``\n    '''\n\n    activation                = get_default_override(RNNStep, activation=activation)\n    init                      = get_default_override(RNNStep, init=init)\n    init_bias                 = get_default_override(RNNStep, init_bias=init_bias)\n    enable_self_stabilization = get_default_override(RNNStep, enable_self_stabilization=enable_self_stabilization)\n\n    return _RecurrentBlock('RNNStep', shape, cell_shape, activation=activation, use_peepholes=False,\n                           init=init, init_bias=init_bias,\n                           enable_self_stabilization=enable_self_stabilization, name=name)\n",
  "sig": "RNNStep(shape|shape (`int` or `tuple` of `ints`): vector or tensor dimension of the output of this layer)(cell_shape|cell_shape (tuple, defaults to `None`): if given, then the output state is first computed at `cell_shape`         and linearly projected to `shape`)(activation|activation (:class:`~cntk.ops.functions.Function`, defaults to signmoid): function to apply at the end, e.g. `relu`)(init|init_bias (scalar or NumPy array or :mod:`cntk.initializer`, defaults to 0): initial value of weights `b`)(init_bias|init_bias (scalar or NumPy array or :mod:`cntk.initializer`, defaults to 0): initial value of weights `b`)(enable_self_stabilization|enable_self_stabilization (bool, defaults to `False`): if `True` then add a :func:`~cntk.layers.blocks.Stabilizer`         to all state-related projections (but not the data input))(name|name (str, defaults to ''): the name of the Function instance in the network)",
  "dst": "    Layer factory function to create a plain RNN block for use inside a recurrence.    The RNN block implements one step of the recurrence and is stateless. It accepts the previous state as its first argument,    and outputs its new state."
 },
 "cntk.losses.binary_cross_entropy": {
  "code": "@typemap\ndef binary_cross_entropy(output, target, name=''):\n    r'''\n    Computes the binary cross entropy (aka logistic loss) between the ``output`` and ``target``.\n\n    Args:\n        output: the computed posterior probability for a variable to be 1 from the network (typ. a ``sigmoid``)\n        target: ground-truth label, 0 or 1\n        name (str, optional): the name of the Function instance in the network\n\n    Returns:\n        :class:`~cntk.ops.functions.Function`\n\n    Todo:\n        add an example\n    '''\n    from cntk.cntk_py import binary_cross_entropy\n    dtype = get_data_type(output, target)\n    output = sanitize_input(output, dtype)\n    target = sanitize_input(target, dtype)\n    return binary_cross_entropy(output, target, name)\n",
  "sig": "binary_cross_entropy(output|output: the computed posterior probability for a variable to be 1 from the network (typ. a ``sigmoid``))(target|target: ground-truth label, 0 or 1)(name|name (str, optional): the name of the Function instance in the network)",
  "dst": "    Computes the binary cross entropy (aka logistic loss) between the ``output`` and ``target``."
 },
 "cntk.losses.cross_entropy_with_softmax": {
  "code": "@typemap\ndef cross_entropy_with_softmax(output_vector, target_vector, axis=-1, name=''):\n    r'''\n    This operation computes the cross entropy between the ``target_vector`` and\n    the softmax of the ``output_vector``. The elements of ``target_vector``\n    have to be non-negative and should sum to 1. The ``output_vector`` can\n    contain any values. The function will internally compute the softmax of\n    the ``output_vector``. Concretely,\n\n    :math:`\\mathrm{softmax}(x)=\\left[\\frac{\\exp(x_1)}{\\sum_i\\exp(x_i)}\\quad\\frac{\\exp(x_1)}{\\sum_i\\exp(x_i)}\\quad\\ldots\\quad\\frac{\\exp(x_1)}{\\sum_i\\exp(x_i)}\\right]`\n\n    :math:`\\mathrm{cross\\_entropy\\_with\\_softmax}(o, t) = -\\sum_{i} t_i \\log(\\mathrm{softmax}(o)_i)`\n\n    with the understanding that the implementation can use equivalent formulas\n    for efficiency and numerical stability.\n\n    Example:\n        >>> C.cross_entropy_with_softmax([[1., 1., 1., 50.]], [[0., 0., 0., 1.]]).eval()\n        array([[ 0.]], dtype=float32)\n\n        >>> C.cross_entropy_with_softmax([[1., 2., 3., 4.]], [[0.35, 0.15, 0.05, 0.45]]).eval()\n        array([[ 1.84019]], dtype=float32)\n\n    Args:\n        output_vector: the unscaled computed output values from the network\n        target_vector: usually it is one-hot vector where the hot bit\n         corresponds to the label index. But it can be any probability\n         distribution over the labels.\n        axis (int or :class:`~cntk.axis.Axis`, optional): if given, cross entropy will be computed\n                along this axis\n        name (str, optional): the name of the Function instance in the network\n    Returns:\n        :class:`~cntk.ops.functions.Function`\n    '''\n    from cntk.cntk_py import cross_entropy_with_softmax\n    dtype = get_data_type(output_vector, target_vector)\n    output_vector = sanitize_input(output_vector, dtype)\n    target_vector = sanitize_input(target_vector, dtype)\n    axis = sanitize_axis(axis)\n    return cross_entropy_with_softmax(output_vector, target_vector, axis, name)\n",
  "sig": "cross_entropy_with_softmax(output_vector|output_vector: the unscaled computed output values from the network)(target_vector|target_vector: usually it is one-hot vector where the hot bit         corresponds to the label index. But it can be any probability         distribution over the labels.)(axis|axis (int or :class:`~cntk.axis.Axis`, optional): if given, cross entropy will be computed                along this axis)(name|name (str, optional): the name of the Function instance in the network)",
  "dst": "    This operation computes the cross entropy between the ``target_vector`` and    the softmax of the ``output_vector``. The elements of ``target_vector``    have to be non-negative and should sum to 1. The ``output_vector`` can    contain any values. The function will internally compute the softmax of    the ``output_vector``. Concretely,    :math:`\\mathrm{softmax}(x)=\\left[\\frac{\\exp(x_1)}{\\sum_i\\exp(x_i)}\\quad\\frac{\\exp(x_1)}{\\sum_i\\exp(x_i)}\\quad\\ldots\\quad\\frac{\\exp(x_1)}{\\sum_i\\exp(x_i)}\\right]`    :math:`\\mathrm{cross\\_entropy\\_with\\_softmax}(o, t) = -\\sum_{i} t_i \\log(\\mathrm{softmax}(o)_i)`    with the understanding that the implementation can use equivalent formulas    for efficiency and numerical stability."
 },
 "cntk.losses.cosine_distance": {
  "code": "@typemap\ndef cosine_distance(x, y, name=''):\n    '''\n    Computes the cosine distance between ``x`` and ``y``:\n\n    Example:\n        >>> a = np.asarray([-1, -1, -1, 1, 1, -1, 1, 1, -1, 1, 1, -1]).reshape(3,2,2)\n        >>> b = np.asarray([1, 1, -1, 1, 1, -1, 1, -1, -1, -1, -1, 1]).reshape(3,2,2)\n        >>> x = C.sequence.input_variable(shape=(2,))\n        >>> y = C.sequence.input_variable(shape=(2,))\n        >>> np.round(C.cosine_distance(x,y).eval({x:a,y:b}),5)\n        array([[-1.,  1.],\n               [ 1.,  0.],\n               [ 0., -1.]], dtype=float32)\n\n    Args:\n        x: numpy array or any :class:`~cntk.ops.functions.Function` that outputs a tensor\n        name (str, optional): the name of the Function instance in the network\n    Returns:\n        :class:`~cntk.ops.functions.Function`\n    '''\n    from cntk.cntk_py import cosine_distance\n    dtype = get_data_type(x, y)\n    x = sanitize_input(x, dtype)\n    y = sanitize_input(y, dtype)\n    return cosine_distance(x, y, name)\n",
  "sig": "cosine_distance(x|x: numpy array or any :class:`~cntk.ops.functions.Function` that outputs a tensor)(y|NULL)(name|name (str, optional): the name of the Function instance in the network)",
  "dst": "    Computes the cosine distance between ``x`` and ``y``:"
 },
 "cntk.train.Trainer.save_checkpoint": {
  "code": "    def save_checkpoint(self, filename, external_state={}):\n        '''\n        Saves a checkpoint of the model and other Trainer state at the\n        specified file location.\n\n        In distributed environment the checkpointing is done by \n        the main worker.\n\n        Args:\n            filename (str): filename to store the checkpoint.\n            external_state (dict): additional external state, default is empty.\n        '''\n\n        super(Trainer, self).save_checkpoint(filename, _py_dict_to_cntk_dict(external_state))\n",
  "sig": "save_checkpoint(self|NULL)(filename|filename (str): filename to store the checkpoint.)(external_state|external_state (dict): additional external state, default is empty.)",
  "dst": "        Saves a checkpoint of the model and other Trainer state at the        specified file location.        In distributed environment the checkpointing is done by         the main worker."
 },
 "cntk.learners.sgd": {
  "code": "@typemap\ndef sgd(parameters, lr,\n        l1_regularization_weight=0.0, l2_regularization_weight=0.0,\n        gaussian_noise_injection_std_dev=0.0, gradient_clipping_threshold_per_sample=np.inf,\n        gradient_clipping_with_truncation=True, use_mean_gradient=None,\n        minibatch_size=None, epoch_size=None):\n    '''sgd(parameters, lr, l1_regularization_weight=0, l2_regularization_weight=0, gaussian_noise_injection_std_dev=0, gradient_clipping_threshold_per_sample=np.inf, gradient_clipping_with_truncation=True)\n    Creates an SGD learner instance to learn the parameters. See [1] for more\n    information on how to set the parameters.\n\n    Args:\n        parameters (list of parameters): list of network parameters to tune.\n         These can be obtained by the '.parameters()' method of the root\n         operator.\n        lr (float, list, output of :func:`learning_parameter_schedule`): a learning rate in float, or a learning rate schedule.\n         See also:  :func:`learning_parameter_schedule`\n        l1_regularization_weight (float, optional): the L1 regularization weight per sample,\n         defaults to 0.0\n        l2_regularization_weight (float, optional): the L2 regularization weight per sample,\n         defaults to 0.0\n        gaussian_noise_injection_std_dev (float, optional): the standard deviation\n         of the Gaussian noise added to parameters post update, defaults to 0.0\n        gradient_clipping_threshold_per_sample (float, optional): clipping threshold\n         per sample, defaults to infinity\n        gradient_clipping_with_truncation (bool, default ``True``): use gradient clipping\n         with truncation\n        use_mean_gradient (bool, optional): use averaged gradient as input to learner.\n\n            deprecated:: 2.2\n                Use minibatch_size parameter to specify the reference minibatch size.\n        minibatch_size (int, default ``None``): The minibatch size that the learner's parameters are designed or pre-tuned for. This\n         size is usually set to the same as the minibatch data source's size. CNTK will perform automatic scaling of the parameters\n         to enable efficient model parameter update implementation while approximate the behavior of pre-designed and pre-tuned parameters.\n         In case that minibatch_size is not specified, CNTK will inherit the minibatch size from the learning rate schedule;\n         if the learning rate schedule does not specify the minibatch_size, CNTK will set it to :attr:`IGNORE`. Setting minibatch_size to :attr:`IGNORE`\n         will have the learner apply as it is preventing CNTK performing any hyper-parameter scaling. See also:  :func:`learning_parameter_schedule`\n        epoch_size (optional, int): number of samples as a scheduling unit for learning rate. See also:  :func:`learning_parameter_schedule`\n\n\n    Returns:\n        :class:`~cntk.learners.Learner`: learner instance that can be passed to\n        the :class:`~cntk.train.trainer.Trainer`\n\n    See also:\n        [1] L. Bottou. `Stochastic Gradient Descent Tricks\n        <https://www.microsoft.com/en-us/research/publication/stochastic-gradient-tricks>`_. Neural\n        Networks: Tricks of the Trade: Springer, 2012.\n    '''\n    lr, minibatch_size = _infer_learning_rate_schedule_and_ref_minibatch_size(use_mean_gradient, minibatch_size, lr, epoch_size)\n    gaussian_noise_injection_std_dev = \\\n        training_parameter_schedule(\n            gaussian_noise_injection_std_dev)\n\n    additional_options = cntk_py.AdditionalLearningOptions()\n    additional_options.l1_regularization_weight = l1_regularization_weight\n    additional_options.l2_regularization_weight = l2_regularization_weight\n    additional_options.gaussian_noise_injection_std_dev = gaussian_noise_injection_std_dev\n    additional_options.gradient_clipping_threshold_per_sample = gradient_clipping_threshold_per_sample\n    additional_options.gradient_clipping_with_truncation = gradient_clipping_with_truncation\n    if minibatch_size is not None:\n        additional_options.dict_options[cntk_py.Learner._MINIBATCH_SIZE] = cntk_py.SizeTWrapper(minibatch_size) #need this to make proper typed DictionaryValue\n\n    opt = cntk_py.sgd_learner(parameters, lr, additional_options)\n    opt.is_minibatch_size_explicitly_specified = minibatch_size is not None\n    return opt\n",
  "sig": "sgd(parameters|parameters (list of parameters): list of network parameters to tune.         These can be obtained by the '.parameters()' method of the root         operator.)(lr|lr (float, list, output of :func:`learning_parameter_schedule`): a learning rate in float, or a learning rate schedule.         See also:  :func:`learning_parameter_schedule`)(l1_regularization_weight|l1_regularization_weight (float, optional): the L1 regularization weight per sample,         defaults to 0.0)(l2_regularization_weight|l2_regularization_weight (float, optional): the L2 regularization weight per sample,         defaults to 0.0)(gaussian_noise_injection_std_dev|gaussian_noise_injection_std_dev (float, optional): the standard deviation         of the Gaussian noise added to parameters post update, defaults to 0.0)(gradient_clipping_threshold_per_sample|gradient_clipping_threshold_per_sample (float, optional): clipping threshold         per sample, defaults to infinity)(gradient_clipping_with_truncation|gradient_clipping_with_truncation (bool, default ``True``): use gradient clipping         with truncation)(use_mean_gradient|use_mean_gradient (bool, optional): use averaged gradient as input to learner.            deprecated:: 2.2                Use minibatch_size parameter to specify the reference minibatch size.)(minibatch_size|minibatch_size (int, default ``None``): The minibatch size that the learner's parameters are designed or pre-tuned for. This         size is usually set to the same as the minibatch data source's size. CNTK will perform automatic scaling of the parameters         to enable efficient model parameter update implementation while approximate the behavior of pre-designed and pre-tuned parameters.         In case that minibatch_size is not specified, CNTK will inherit the minibatch size from the learning rate schedule;         if the learning rate schedule does not specify the minibatch_size, CNTK will set it to :attr:`IGNORE`. Setting minibatch_size to :attr:`IGNORE`         will have the learner apply as it is preventing CNTK performing any hyper-parameter scaling. See also:  :func:`learning_parameter_schedule`)(epoch_size|epoch_size (optional, int): number of samples as a scheduling unit for learning rate. See also:  :func:`learning_parameter_schedule`)",
  "dst": "    Creates an SGD learner instance to learn the parameters. See [1] for more    information on how to set the parameters."
 },
 "cntk.layers.Sequential": {
  "code": "def Sequential(layers, name=''):\n    '''\n    Sequential(layers, name='')\n\n    Layer factory function to create a composite that applies a sequence of layers (or any functions) onto an input.\n    ``Sequential ([F, G, H])(x)`` means the same as ``H(G(F(x)))``.\n\n    The list of functions may also include tuples of functions. In that case, each function\n    in a tuple is applied to the input, and the result is a tuple containing the results of\n    these function applications. If followed by another function (typ. ``plus`` or ``splice``),\n    the tuple items form the arguments to that function.\n\n    Intermediate values in the chain can be accessed by name by inserting a ``Label(name=...)`` layer.\n\n    Note: An equivalent way of writing ``Sequential ([F, G, H])(x)`` is ``F >> G >> H``.\n\n    Example:\n     >>> from cntk.layers import *\n\n     >>> # sequence classifier. Maps a one-hot word sequence to a scalar probability value.\n     >>> # The recurrence is a Fold(), meaning only the final hidden state is produced.\n     >>> # The Label() layer allows to access the final hidden layer by name.\n     >>> model = Sequential([Embedding(300), Fold(LSTM(500)), Label('hidden'), Dense(1, activation=sigmoid)])\n     >>> model.update_signature(Sequence[Tensor[30000]])\n     >>> model.hidden.shape\n         (500,)\n\n     >>> # simple example that squares an input value\n     >>> f = Sequential([C.log, lambda x: 2 * x, C.exp])  # the second function is a Python lambda\n     >>> f.update_signature(1)\n     >>> f([np.array([2])])     # log, times 2, exp is the same as computing the square\n         array([[ 4.]], dtype=float32)\n\n     >>> # using function tuples to implement a bidirectional LSTM\n     >>> bi_lstm = Sequential([(Recurrence(LSTM(250)),                      # first tuple entry: forward pass\n     ...                        Recurrence(LSTM(250), go_backwards=True)),  # second: backward pass\n     ...                       splice])                                     # splice both on top of each other\n\n     >>> # using function tuple to implement a ResNet block\n     >>> # The function tuple applies all items to the input, and emits a tuple with the results\n     >>> # that then act as the arguments to the next one.\n     >>> # Here we say (Convolution(), identity), which generates two arguments to the next function,\n     >>> # the first being the convolution, the second being the input passed through.\n     >>> # Following that with plus() implements the ResNet formula.\n     >>> from cntk.ops import plus, relu\n     >>> resnet_layer = Sequential([(Convolution((3,3), 64, activation=None), # first tuple entry\n     ...                             identity),                               # second tuple entry is a pass-through\n     ...                            plus,                                     # this sums both\n     ...                            relu])                                    # activation applied afterwards\n\n     >>> # simple function-tuples example with values\n     >>> f = Sequential([(lambda x: x * x, identity), splice])  # computes tuple (x^2, x) and splices both values\n     >>> f.update_signature(1)\n     >>> f([np.array([2])])\n         array([[ 4.,  2.]], dtype=float32)\n\n    Args:\n      layers (list of :class:`~cntk.ops.functions.Function`, equivalent Python functions, tuples of functions, or lists thereof): the list of functions to apply in sequence.\n        A tuple aplies each of its items to the input and results in a tuple value.\n        An item that is a list will be flattened.\n\n    Returns:\n        cntk.ops.functions.Function:\n        A function that accepts one argument and applies the given ``functions`` one after another.\n    '''\n    if not isinstance(layers, list):  # to support nested lists, run every item recursively through Sequential()\n        # TODO: Is this confusing w.r.t. tuple which is parallel and list which is sequential?\n        return layers\n    from functools import reduce\n    layers = [Sequential(layer) for layer in layers]  # expand all layers recursively\n    composed_function = reduce(lambda f, g: f >> g, layers, identity)\n\n    return _inject_name(composed_function, name)\n",
  "sig": "Sequential(layers|layers (list of :class:`~cntk.ops.functions.Function`, equivalent Python functions, tuples of functions, or lists thereof): the list of functions to apply in sequence.        A tuple aplies each of its items to the input and results in a tuple value.        An item that is a list will be flattened.)(name|NULL)",
  "dst": "    Layer factory function to create a composite that applies a sequence of layers (or any functions) onto an input.    ``Sequential ([F, G, H])(x)`` means the same as ``H(G(F(x)))``.    The list of functions may also include tuples of functions. In that case, each function    in a tuple is applied to the input, and the result is a tuple containing the results of    these function applications. If followed by another function (typ. ``plus`` or ``splice``),    the tuple items form the arguments to that function.    Intermediate values in the chain can be accessed by name by inserting a ``Label(name=...)`` layer.    Note: An equivalent way of writing ``Sequential ([F, G, H])(x)`` is ``F >> G >> H``."
 },
 "cntk.ops.per_dim_mean_variance_normalize": {
  "code": "@typemap\ndef per_dim_mean_variance_normalize(operand, mean, inv_stddev, name=''):\n    '''\n    Computes per dimension mean-variance normalization of the specified input operand.\n\n    Args:\n        operand: the variable to be normalized\n        mean (NumPy array): per dimension mean to use for the normalization\n        inv_stddev (NumPy array): per dimension standard deviation to use for the normalization\n        name (str, optional): the name of the Function instance in the network\n    Returns:\n        :class:`~cntk.ops.functions.Function`\n    '''\n    from cntk.cntk_py import per_dim_mean_variance_normalize\n    mean = sanitize_input(mean, get_data_type(mean))\n    inv_stddev = sanitize_input(inv_stddev, get_data_type(inv_stddev))\n    return per_dim_mean_variance_normalize(operand, mean, inv_stddev, name)\n",
  "sig": "per_dim_mean_variance_normalize(operand|operand: the variable to be normalized)(mean|mean (NumPy array): per dimension mean to use for the normalization)(inv_stddev|inv_stddev (NumPy array): per dimension standard deviation to use for the normalization)(name|name (str, optional): the name of the Function instance in the network)",
  "dst": "    Computes per dimension mean-variance normalization of the specified input operand."
 },
 "cntk.ops.stop_gradient": {
  "code": "@typemap\ndef stop_gradient(input, name=''):\n    '''\n    Outputs its input as it is and prevents any gradient contribution from its output to its input.\n\n    Args:\n        input: class:`~cntk.ops.functions.Function` that outputs a tensor\n        name (str, optional): the name of the Function instance in the network\n    Returns:\n        :class:`~cntk.ops.functions.Function`\n    '''\n    from cntk.cntk_py import stop_gradient\n    dtype = get_data_type(input)\n    op = sanitize_input(input, dtype)\n    return stop_gradient(op, name)\n",
  "sig": "stop_gradient(input|input: class:`~cntk.ops.functions.Function` that outputs a tensor)(name|name (str, optional): the name of the Function instance in the network)",
  "dst": "    Outputs its input as it is and prevents any gradient contribution from its output to its input."
 },
 "cntk.ops.crop_manual": {
  "code": "@typemap\ndef crop_manual(node_input, node_referent, offset_x, offset_y, name = ''):\n    '''\n    Crops input along spatial dimensions so that it matches spatial size of reference input.\n    Crop offsets are given in pixels.\n\n    Args:\n        node_input: class:`~cntk.ops.functions.Function` that outputs the tensor to be cropped\n        node_referent: class:`~cntk.ops.functions.Function` that outputs the reference tensor\n        offset_x (int): horizontal crop offset\n        offset_y (int): vertical crop offset\n        name (str, optional): the name of the Function instance in the network\n    Returns:\n        :class:`~cntk.ops.functions.Function`\n    '''\n    from cntk.cntk_py import crop\n    arg_input = sanitize_input(node_input, get_data_type(node_input))\n    arg_ref = sanitize_input(node_referent,  get_data_type(node_referent))\n    return crop(arg_input, arg_ref, offset_x, offset_y, name)\n",
  "sig": "crop_manual(node_input|node_input: class:`~cntk.ops.functions.Function` that outputs the tensor to be cropped)(node_referent|node_referent: class:`~cntk.ops.functions.Function` that outputs the reference tensor)(offset_x|offset_x (int): horizontal crop offset)(offset_y|offset_y (int): vertical crop offset)(name|name (str, optional): the name of the Function instance in the network)",
  "dst": "    Crops input along spatial dimensions so that it matches spatial size of reference input.    Crop offsets are given in pixels."
 },
 "cntk.ops.functions.unique": {
  "code": "def unique(enumeration):\n    \"\"\"Class decorator for enumerations ensuring unique member values.\"\"\"\n    duplicates = []\n    for name, member in enumeration.__members__.items():\n        if name != member.name:\n            duplicates.append((name, member.name))\n    if duplicates:\n        alias_details = ', '.join(\n                [\"%s -> %s\" % (alias, name) for (alias, name) in duplicates])\n        raise ValueError('duplicate values found in %r: %s' %\n                (enumeration, alias_details))\n    return enumeration\n",
  "sig": "unique(enumeration|NULL)",
  "dst": "Class decorator for enumerations ensuring unique member values."
 }
}