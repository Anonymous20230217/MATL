{
 "mxnet.ndarray.NDArray.clip": {
  "code": "    def clip(self, *args, **kwargs):\n        \"\"\"Convenience fluent method for :py:func:`clip`.\n\n        The arguments are the same as for :py:func:`clip`, with\n        this array as data.\n        \"\"\"\n        return op.clip(self, *args, **kwargs)\n",
  "sig": "clip(data|Input array)(a_min|Minimum value)(a_max|Maximum value)(out|The output NDArray to hold the result)",
  "dst": "Convenience fluent method for :py:func:`clip`.        The arguments are the same as for :py:func:`clip`, with        this array as data."
 },
 "mxnet.gluon.block.NDArray.depth_to_space": {
  "code": "    def depth_to_space(self, *args, **kwargs):\n        \"\"\"Convenience fluent method for :py:func:`depth_to_space`.\n\n        The arguments are the same as for :py:func:`depth_to_space`, with\n        this array as data.\n        \"\"\"\n        return op.depth_to_space(self, *args, **kwargs)\n",
  "sig": "depth_to_space(data|Input ndarray)(block_size|Blocks of [block_size. block_size] are moved)(out|The output NDArray to hold the result)",
  "dst": "Convenience fluent method for :py:func:`depth_to_space`.        The arguments are the same as for :py:func:`depth_to_space`, with        this array as data."
 },
 "mxnet.ndarray.numpy_extension.random.bernoulli": {
  "code": "def bernoulli(prob, logit, size, dtype, ctx, out):\n    \"\"\"Creates a Bernoulli distribution parameterized by :attr:`prob`\n    or :attr:`logit` (but not both).\n\n    Samples are binary (0 or 1). They take the value `1` with probability `p`\n    and `0` with probability `1 - p`.\n\n    Parameters\n    ----------\n    prob : float, ndarray\n        The probability of sampling '1'.\n        Only one of prob or logit should be passed in.\n    logit : float, ndarray\n        The log-odds of sampling '1'.\n        Only one of prob or logit should be passed in.\n    size : int or tuple of ints, optional\n        Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n        ``m * n * k`` samples are drawn.  Default is None, in which case a\n        single value is returned.\n    dtype : dtype, optional\n        Desired dtype of the result. All dtypes are determined by their\n        name, i.e., 'int64', 'int', etc, so byteorder is not available\n        and a specific precision may have different C types depending\n        on the platform. The default value is 'np.float32'.\n    ctx : Context, optional\n        Device context of output. Default is current context.\n    out : symbol, optional\n        The output symbol (default is `None`).\n\n    Returns\n    -------\n    out : ndarray\n        Drawn samples from the parameterized bernoulli distribution.\n\n    Examples\n    --------\n    >>> prob = np.random.uniform(size=(4,4))\n    >>> logit = np.log(prob) - np.log(1 - prob)\n    >>> npx.random.bernoulli(logit=logit)\n    array([[0., 1., 1., 1.],\n        [0., 1., 1., 1.],\n        [0., 1., 0., 0.],\n        [1., 0., 1., 0.]])\n\n    >>> npx.random.bernoulli(prob=prob)\n    array([[0., 1., 0., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 0.],\n        [1., 0., 1., 0.]])\n    \"\"\"\n    from ...numpy import ndarray as np_ndarray\n    tensor_type_name = np_ndarray\n    if (prob is None) == (logit is None):\n        raise ValueError(\n            \"Either `prob` or `logit` must be specified, but not both. \" +\n            \"Received prob={}, logit={}\".format(prob, logit))\n    if dtype is None:\n        dtype = 'float32'\n    if ctx is None:\n        ctx = current_context()\n    if size == ():\n        size = None\n    if prob is not None:\n        is_tensor = isinstance(prob, tensor_type_name)\n        if is_tensor:\n            return _npi.bernoulli(prob, prob=None, logit=None, is_logit=False,\n                                  size=size, ctx=ctx, dtype=dtype, out=out)\n        else:\n            return _npi.bernoulli(prob=prob, logit=None, is_logit=False,\n                                  size=size, ctx=ctx, dtype=dtype, out=out)\n    else:\n        is_tensor = isinstance(logit, tensor_type_name)\n        if is_tensor:\n            return _npi.bernoulli(logit, prob=None, logit=None, is_logit=True,\n                                  size=size, ctx=ctx, dtype=dtype, out=out)\n        else:\n            return _npi.bernoulli(prob=None, logit=logit, is_logit=True,\n                                  size=size, ctx=ctx, dtype=dtype, out=out)\n",
  "sig": "bernoulli(prob|The probability of sampling '1'.Only one of prob or logit should be passed in)(logit|The log-odds of sampling '1'.Only one of prob or logit should be passed in)(size|Output shape)(dtype|Desired dtype of the result)(ctx|Device context of output)(out|The output symbol (default is `None`))",
  "dst": "Creates a Bernoulli distribution parameterized by :attr:`prob`    or :attr:`logit` (but not both).    Samples are binary (0 or 1). They take the value `1` with probability `p`    and `0` with probability `1 - p`.    Parameters    ----------    prob : float, ndarray        The probability of sampling '1'.        Only one of prob or logit should be passed in.    logit : float, ndarray        The log-odds of sampling '1'.        Only one of prob or logit should be passed in.    size : int or tuple of ints, optional        Output shape.  If the given shape is, e.g., ``(m, n, k)``, then        ``m * n * k`` samples are drawn.  Default is None, in which case a        single value is returned.    dtype : dtype, optional        Desired dtype of the result. All dtypes are determined by their        name, i.e., 'int64', 'int', etc, so byteorder is not available        and a specific precision may have different C types depending        on the platform. The default value is 'np.float32'.    ctx : Context, optional        Device context of output. Default is current context.    out : symbol, optional        The output symbol (default is `None`).    Returns    -------    out : ndarray        Drawn samples from the parameterized bernoulli distribution.    Examples    --------    >>> prob = np.random.uniform(size=(4,4))    >>> logit = np.log(prob) - np.log(1 - prob)    >>> npx.random.bernoulli(logit=logit)    array([[0., 1., 1., 1.],        [0., 1., 1., 1.],        [0., 1., 0., 0.],        [1., 0., 1., 0.]])    >>> npx.random.bernoulli(prob=prob)    array([[0., 1., 0., 1.],        [1., 1., 1., 1.],        [1., 1., 1., 0.],        [1., 0., 1., 0.]])"
 },
 "mxnet.ndarray.random.exponential": {
  "code": "def exponential(scale=1, shape=_Null, dtype=_Null, ctx=None, out=None, **kwargs):\n    r\"\"\"Draw samples from an exponential distribution.\n\n    Its probability density function is\n\n    .. math:: f(x; \\frac{1}{\\beta}) = \\frac{1}{\\beta} \\exp(-\\frac{x}{\\beta}),\n\n    for x > 0 and 0 elsewhere. \\beta is the scale parameter, which is the\n    inverse of the rate parameter \\lambda = 1/\\beta.\n\n    Parameters\n    ----------\n    scale : float or NDArray, optional\n        The scale parameter, \\beta = 1/\\lambda.\n    shape : int or tuple of ints, optional\n        The number of samples to draw. If shape is, e.g., `(m, n)` and `scale` is\n        a scalar, output shape will be `(m, n)`. If `scale`\n        is an NDArray with shape, e.g., `(x, y)`, then output will have shape\n        `(x, y, m, n)`, where `m*n` samples are drawn for each entry in `scale`.\n    dtype : {'float16', 'float32', 'float64'}, optional\n        Data type of output samples. Default is 'float32'\n    ctx : Context, optional\n        Device context of output. Default is current context. Overridden by\n        `scale.context` when `scale` is an NDArray.\n    out : NDArray, optional\n        Store output to an existing NDArray.\n\n    Returns\n    -------\n    NDArray\n        If input `shape` has shape, e.g., `(m, n)` and `scale` is a scalar, output shape will\n        be `(m, n)`. If `scale` is an NDArray with shape, e.g., `(x, y)`, then `output`\n        will have shape `(x, y, m, n)`, where `m*n` samples are drawn for each entry in scale.\n\n    Examples\n    --------\n    >>> mx.nd.random.exponential(1)\n    [ 0.79587454]\n    <NDArray 1 @cpu(0)>\n    >>> mx.nd.random.exponential(1, shape=(2,))\n    [ 0.89856035  1.25593066]\n    <NDArray 2 @cpu(0)>\n    >>> scale = mx.nd.array([1,2,3])\n    >>> mx.nd.random.exponential(scale, shape=2)\n    [[  0.41063145   0.42140478]\n     [  2.59407091  10.12439728]\n     [  2.42544937   1.14260709]]\n    <NDArray 3x2 @cpu(0)>\n    \"\"\"\n    return _random_helper(_internal._random_exponential, _internal._sample_exponential,\n                          [1.0/scale], shape, dtype, ctx, out, kwargs)\n",
  "sig": "exponential(scale|The scale parameter, beta = 1/lambda)(shape|The number of samples to draw. If shape is, e.g., (m, n) and scale is a scalar, output shape will be (m, n). If scale is an NDArray with shape, e.g., (x, y), then output will have shape (x, y, m, n), where m*n samples are drawn for each entry in scale.)(dtype|Data type of output samples. Default is 'float32')(ctx|Device context of output. Default is current context. Overridden by scale.context when scale is an NDArray)(out|Store output to an existing NDArray)",
  "dst": "Draw samples from an exponential distribution.    Its probability density function is    .. math:: f(x; \\frac{1}{\\beta}) = \\frac{1}{\\beta} \\exp(-\\frac{x}{\\beta}),    for x > 0 and 0 elsewhere. \\beta is the scale parameter, which is the    inverse of the rate parameter \\lambda = 1/\\beta.    Parameters    ----------    scale : float or NDArray, optional        The scale parameter, \\beta = 1/\\lambda.    shape : int or tuple of ints, optional        The number of samples to draw. If shape is, e.g., `(m, n)` and `scale` is        a scalar, output shape will be `(m, n)`. If `scale`        is an NDArray with shape, e.g., `(x, y)`, then output will have shape        `(x, y, m, n)`, where `m*n` samples are drawn for each entry in `scale`.    dtype : {'float16', 'float32', 'float64'}, optional        Data type of output samples. Default is 'float32'    ctx : Context, optional        Device context of output. Default is current context. Overridden by        `scale.context` when `scale` is an NDArray.    out : NDArray, optional        Store output to an existing NDArray.    Returns    -------    NDArray        If input `shape` has shape, e.g., `(m, n)` and `scale` is a scalar, output shape will        be `(m, n)`. If `scale` is an NDArray with shape, e.g., `(x, y)`, then `output`        will have shape `(x, y, m, n)`, where `m*n` samples are drawn for each entry in scale.    Examples    --------    >>> mx.nd.random.exponential(1)    [ 0.79587454]    <NDArray 1 @cpu(0)>    >>> mx.nd.random.exponential(1, shape=(2,))    [ 0.89856035  1.25593066]    <NDArray 2 @cpu(0)>    >>> scale = mx.nd.array([1,2,3])    >>> mx.nd.random.exponential(scale, shape=2)    [[  0.41063145   0.42140478]     [  2.59407091  10.12439728]     [  2.42544937   1.14260709]]    <NDArray 3x2 @cpu(0)>"
 },
 "mxnet.ndarray.random.gamma": {
  "code": "def gamma(alpha=1, beta=1, shape=_Null, dtype=_Null, ctx=None, out=None, **kwargs):\n    \"\"\"Draw random samples from a gamma distribution.\n\n    Samples are distributed according to a gamma distribution parametrized\n    by *alpha* (shape) and *beta* (scale).\n\n    Parameters\n    ----------\n    alpha : float or NDArray, optional\n        The shape of the gamma distribution. Should be greater than zero.\n    beta : float or NDArray, optional\n        The scale of the gamma distribution. Should be greater than zero.\n        Default is equal to 1.\n    shape : int or tuple of ints, optional\n        The number of samples to draw. If shape is, e.g., `(m, n)` and `alpha` and\n        `beta` are scalars, output shape will be `(m, n)`. If `alpha` and `beta`\n        are NDArrays with shape, e.g., `(x, y)`, then output will have shape\n        `(x, y, m, n)`, where `m*n` samples are drawn for each `[alpha, beta)` pair.\n    dtype : {'float16', 'float32', 'float64'}, optional\n        Data type of output samples. Default is 'float32'\n    ctx : Context, optional\n        Device context of output. Default is current context. Overridden by\n        `alpha.context` when `alpha` is an NDArray.\n    out : NDArray, optional\n        Store output to an existing NDArray.\n\n    Returns\n    -------\n    NDArray\n        If input `shape` has shape, e.g., `(m, n)` and `alpha` and `beta` are scalars, output\n        shape will be `(m, n)`. If `alpha` and `beta` are NDArrays with shape, e.g.,\n        `(x, y)`, then output will have shape `(x, y, m, n)`, where `m*n` samples are\n        drawn for each `[alpha, beta)` pair.\n\n    Examples\n    --------\n    >>> mx.nd.random.gamma(1, 1)\n    [ 1.93308783]\n    <NDArray 1 @cpu(0)>\n    >>> mx.nd.random.gamma(1, 1, shape=(2,))\n    [ 0.48216391  2.09890771]\n    <NDArray 2 @cpu(0)>\n    >>> alpha = mx.nd.array([1,2,3])\n    >>> beta = mx.nd.array([2,3,4])\n    >>> mx.nd.random.gamma(alpha, beta, shape=2)\n    [[  3.24343276   0.94137681]\n     [  3.52734375   0.45568955]\n     [ 14.26264095  14.0170126 ]]\n    <NDArray 3x2 @cpu(0)>\n    \"\"\"\n    return _random_helper(_internal._random_gamma, _internal._sample_gamma,\n                          [alpha, beta], shape, dtype, ctx, out, kwargs)\n",
  "sig": "gamma(alpha|The shape of the gamma distribution. Should be greater than zero)(beta|The scale of the gamma distribution. Should be greater than zero. Default is equal to 1)(shape|The number of samples to draw. If shape is, e.g., (m, n) and alpha and beta are scalars, output shape will be (m, n). If alpha and beta are NDArrays with shape, e.g., (x, y), then output will have shape (x, y, m, n), where m*n samples are drawn for each [alpha, beta) pair)(dtype|Data type of output samples. Default is 'float32')(ctx|Device context of output. Default is current context. Overridden by alpha.context when alpha is an NDArray)(out|Store output to an existing NDArray)",
  "dst": "Draw random samples from a gamma distribution.    Samples are distributed according to a gamma distribution parametrized    by *alpha* (shape) and *beta* (scale).    Parameters    ----------    alpha : float or NDArray, optional        The shape of the gamma distribution. Should be greater than zero.    beta : float or NDArray, optional        The scale of the gamma distribution. Should be greater than zero.        Default is equal to 1.    shape : int or tuple of ints, optional        The number of samples to draw. If shape is, e.g., `(m, n)` and `alpha` and        `beta` are scalars, output shape will be `(m, n)`. If `alpha` and `beta`        are NDArrays with shape, e.g., `(x, y)`, then output will have shape        `(x, y, m, n)`, where `m*n` samples are drawn for each `[alpha, beta)` pair.    dtype : {'float16', 'float32', 'float64'}, optional        Data type of output samples. Default is 'float32'    ctx : Context, optional        Device context of output. Default is current context. Overridden by        `alpha.context` when `alpha` is an NDArray.    out : NDArray, optional        Store output to an existing NDArray.    Returns    -------    NDArray        If input `shape` has shape, e.g., `(m, n)` and `alpha` and `beta` are scalars, output        shape will be `(m, n)`. If `alpha` and `beta` are NDArrays with shape, e.g.,        `(x, y)`, then output will have shape `(x, y, m, n)`, where `m*n` samples are        drawn for each `[alpha, beta)` pair.    Examples    --------    >>> mx.nd.random.gamma(1, 1)    [ 1.93308783]    <NDArray 1 @cpu(0)>    >>> mx.nd.random.gamma(1, 1, shape=(2,))    [ 0.48216391  2.09890771]    <NDArray 2 @cpu(0)>    >>> alpha = mx.nd.array([1,2,3])    >>> beta = mx.nd.array([2,3,4])    >>> mx.nd.random.gamma(alpha, beta, shape=2)    [[  3.24343276   0.94137681]     [  3.52734375   0.45568955]     [ 14.26264095  14.0170126 ]]    <NDArray 3x2 @cpu(0)>"
 },
 "mxnet.ndarray.random.normal": {
  "code": "def normal(loc=0, scale=1, shape=_Null, dtype=_Null, ctx=None, out=None, **kwargs):\n    \"\"\"Draw random samples from a normal (Gaussian) distribution.\n\n    Samples are distributed according to a normal distribution parametrized\n    by *loc* (mean) and *scale* (standard deviation).\n\n\n    Parameters\n    ----------\n    loc : float or NDArray, optional\n        Mean (centre) of the distribution.\n    scale : float or NDArray, optional\n        Standard deviation (spread or width) of the distribution.\n    shape : int or tuple of ints, optional\n        The number of samples to draw. If shape is, e.g., `(m, n)` and `loc` and\n        `scale` are scalars, output shape will be `(m, n)`. If `loc` and `scale`\n        are NDArrays with shape, e.g., `(x, y)`, then output will have shape\n        `(x, y, m, n)`, where `m*n` samples are drawn for each `[loc, scale)` pair.\n    dtype : {'float16', 'float32', 'float64'}, optional\n        Data type of output samples. Default is 'float32'\n    ctx : Context, optional\n        Device context of output. Default is current context. Overridden by\n        `loc.context` when `loc` is an NDArray.\n    out : NDArray, optional\n        Store output to an existing NDArray.\n\n    Returns\n    -------\n    NDArray\n        An NDArray of type `dtype`. If input `shape` has shape, e.g., `(m, n)` and\n        `loc` and `scale` are scalars, output shape will be `(m, n)`. If `loc` and\n        `scale` are NDArrays with shape, e.g., `(x, y)`, then output will have shape\n        `(x, y, m, n)`, where `m*n` samples are drawn for each `[loc, scale)` pair.\n\n    Examples\n    --------\n    >>> mx.nd.random.normal(0, 1)\n    [ 2.21220636]\n    <NDArray 1 @cpu(0)>\n    >>> mx.nd.random.normal(0, 1, ctx=mx.gpu(0))\n    [ 0.29253659]\n    <NDArray 1 @gpu(0)>\n    >>> mx.nd.random.normal(-1, 1, shape=(2,))\n    [-0.2259962  -0.51619542]\n    <NDArray 2 @cpu(0)>\n    >>> loc = mx.nd.array([1,2,3])\n    >>> scale = mx.nd.array([2,3,4])\n    >>> mx.nd.random.normal(loc, scale, shape=2)\n    [[ 0.55912292  3.19566321]\n     [ 1.91728961  2.47706747]\n     [ 2.79666662  5.44254589]]\n    <NDArray 3x2 @cpu(0)>\n    \"\"\"\n    return _random_helper(_internal._random_normal, _internal._sample_normal,\n                          [loc, scale], shape, dtype, ctx, out, kwargs)\n",
  "sig": "normal(loc| Mean (centre) of the distribution)(scale| Standard deviation (spread or width) of the distribution)(shape|The number of samples to draw. If shape is, e.g., (m, n) and loc and scale are scalars, output shape will be (m, n). If loc and scale are NDArrays with shape, e.g., (x, y), then output will have shape (x, y, m, n), where m*n samples are drawn for each [loc, scale) pair)(dtype|Data type of output samples. Default is 'float32')(ctx|Device context of output. Default is current context. Overridden by loc.context when loc is an NDArray)(out|Store output to an existing NDArray)",
  "dst": "Draw random samples from a normal (Gaussian) distribution.    Samples are distributed according to a normal distribution parametrized    by *loc* (mean) and *scale* (standard deviation).    Parameters    ----------    loc : float or NDArray, optional        Mean (centre) of the distribution.    scale : float or NDArray, optional        Standard deviation (spread or width) of the distribution.    shape : int or tuple of ints, optional        The number of samples to draw. If shape is, e.g., `(m, n)` and `loc` and        `scale` are scalars, output shape will be `(m, n)`. If `loc` and `scale`        are NDArrays with shape, e.g., `(x, y)`, then output will have shape        `(x, y, m, n)`, where `m*n` samples are drawn for each `[loc, scale)` pair.    dtype : {'float16', 'float32', 'float64'}, optional        Data type of output samples. Default is 'float32'    ctx : Context, optional        Device context of output. Default is current context. Overridden by        `loc.context` when `loc` is an NDArray.    out : NDArray, optional        Store output to an existing NDArray.    Returns    -------    NDArray        An NDArray of type `dtype`. If input `shape` has shape, e.g., `(m, n)` and        `loc` and `scale` are scalars, output shape will be `(m, n)`. If `loc` and        `scale` are NDArrays with shape, e.g., `(x, y)`, then output will have shape        `(x, y, m, n)`, where `m*n` samples are drawn for each `[loc, scale)` pair.    Examples    --------    >>> mx.nd.random.normal(0, 1)    [ 2.21220636]    <NDArray 1 @cpu(0)>    >>> mx.nd.random.normal(0, 1, ctx=mx.gpu(0))    [ 0.29253659]    <NDArray 1 @gpu(0)>    >>> mx.nd.random.normal(-1, 1, shape=(2,))    [-0.2259962  -0.51619542]    <NDArray 2 @cpu(0)>    >>> loc = mx.nd.array([1,2,3])    >>> scale = mx.nd.array([2,3,4])    >>> mx.nd.random.normal(loc, scale, shape=2)    [[ 0.55912292  3.19566321]     [ 1.91728961  2.47706747]     [ 2.79666662  5.44254589]]    <NDArray 3x2 @cpu(0)>"
 },
 "mxnet.ndarray.random.uniform": {
  "code": "def uniform(low=0, high=1, shape=_Null, dtype=_Null, ctx=None, out=None, **kwargs):\n    \"\"\"Draw random samples from a uniform distribution.\n\n    Samples are uniformly distributed over the half-open interval *[low, high)*\n    (includes *low*, but excludes *high*).\n\n    Parameters\n    ----------\n    low : float or NDArray, optional\n        Lower boundary of the output interval. All values generated will be\n        greater than or equal to low. The default value is 0.\n    high : float or NDArray, optional\n        Upper boundary of the output interval. All values generated will be\n        less than high. The default value is 1.0.\n    shape : int or tuple of ints, optional\n        The number of samples to draw. If shape is, e.g., `(m, n)` and `low` and\n        `high` are scalars, output shape will be `(m, n)`. If `low` and `high`\n        are NDArrays with shape, e.g., `(x, y)`, then output will have shape\n        `(x, y, m, n)`, where `m*n` samples are drawn for each `[low, high)` pair.\n    dtype : {'float16', 'float32', 'float64'}, optional\n        Data type of output samples. Default is 'float32'\n    ctx : Context, optional\n        Device context of output. Default is current context. Overridden by\n        `low.context` when `low` is an NDArray.\n    out : NDArray, optional\n        Store output to an existing NDArray.\n\n    Returns\n    -------\n    NDArray\n        An NDArray of type `dtype`. If input `shape` has shape, e.g.,\n        `(m, n)` and `low` and `high` are scalars, output shape will be `(m, n)`.\n        If `low` and `high` are NDArrays with shape, e.g., `(x, y)`, then the\n        return NDArray will have shape `(x, y, m, n)`, where `m*n` uniformly distributed\n        samples are drawn for each `[low, high)` pair.\n\n    Examples\n    --------\n    >>> mx.nd.random.uniform(0, 1)\n    [ 0.54881352]\n    <NDArray 1 @cpu(0)\n    >>> mx.nd.random.uniform(0, 1, ctx=mx.gpu(0))\n    [ 0.92514056]\n    <NDArray 1 @gpu(0)>\n    >>> mx.nd.random.uniform(-1, 1, shape=(2,))\n    [ 0.71589124  0.08976638]\n    <NDArray 2 @cpu(0)>\n    >>> low = mx.nd.array([1,2,3])\n    >>> high = mx.nd.array([2,3,4])\n    >>> mx.nd.random.uniform(low, high, shape=2)\n    [[ 1.78653979  1.93707538]\n     [ 2.01311183  2.37081361]\n     [ 3.30491424  3.69977832]]\n    <NDArray 3x2 @cpu(0)>\n    \"\"\"\n    return _random_helper(_internal._random_uniform, _internal._sample_uniform,\n                          [low, high], shape, dtype, ctx, out, kwargs)\n",
  "sig": "uniform(low|Lower boundary of the output interval. All values generated will be greater than or equal to low. The default value is 0)(high|Upper boundary of the output interval. All values generated will be less than high. The default value is 1.0)(shape|The number of samples to draw. If shape is, e.g., (m, n) and low and high are scalars, output shape will be (m, n). If low and high are NDArrays with shape, e.g., (x, y), then output will have shape (x, y, m, n), where m*n samples are drawn for each [low, high) pair)(dtype|Data type of output samples. Default is 'float32')(ctx|Device context of output. Default is current context. Overridden by low.context when low is an NDArray)(out|Store output to an existing NDArray)",
  "dst": "Draw random samples from a uniform distribution.    Samples are uniformly distributed over the half-open interval *[low, high)*    (includes *low*, but excludes *high*).    Parameters    ----------    low : float or NDArray, optional        Lower boundary of the output interval. All values generated will be        greater than or equal to low. The default value is 0.    high : float or NDArray, optional        Upper boundary of the output interval. All values generated will be        less than high. The default value is 1.0.    shape : int or tuple of ints, optional        The number of samples to draw. If shape is, e.g., `(m, n)` and `low` and        `high` are scalars, output shape will be `(m, n)`. If `low` and `high`        are NDArrays with shape, e.g., `(x, y)`, then output will have shape        `(x, y, m, n)`, where `m*n` samples are drawn for each `[low, high)` pair.    dtype : {'float16', 'float32', 'float64'}, optional        Data type of output samples. Default is 'float32'    ctx : Context, optional        Device context of output. Default is current context. Overridden by        `low.context` when `low` is an NDArray.    out : NDArray, optional        Store output to an existing NDArray.    Returns    -------    NDArray        An NDArray of type `dtype`. If input `shape` has shape, e.g.,        `(m, n)` and `low` and `high` are scalars, output shape will be `(m, n)`.        If `low` and `high` are NDArrays with shape, e.g., `(x, y)`, then the        return NDArray will have shape `(x, y, m, n)`, where `m*n` uniformly distributed        samples are drawn for each `[low, high)` pair.    Examples    --------    >>> mx.nd.random.uniform(0, 1)    [ 0.54881352]    <NDArray 1 @cpu(0)    >>> mx.nd.random.uniform(0, 1, ctx=mx.gpu(0))    [ 0.92514056]    <NDArray 1 @gpu(0)>    >>> mx.nd.random.uniform(-1, 1, shape=(2,))    [ 0.71589124  0.08976638]    <NDArray 2 @cpu(0)>    >>> low = mx.nd.array([1,2,3])    >>> high = mx.nd.array([2,3,4])    >>> mx.nd.random.uniform(low, high, shape=2)    [[ 1.78653979  1.93707538]     [ 2.01311183  2.37081361]     [ 3.30491424  3.69977832]]    <NDArray 3x2 @cpu(0)>"
 },
 "mxnet.initializer.Constant": {
  "code": "class Constant(Initializer):\n    \"\"\"Initializes the weights to a given value.\n    The value passed in can be a scalar or a NDarray that matches the shape\n    of the parameter to be set.\n\n    Parameters\n    ----------\n    value : float, NDArray\n        Value to set.\n    \"\"\"\n    def __init__(self, value):\n        super(Constant, self).__init__(value=value)\n        self.value = value\n\n    def _init_weight(self, _, arr):\n        arr[:] = self.value\n\n    def dumps(self):\n        val = self._kwargs['value']\n        if not np.isscalar(val):\n            self._kwargs['value'] = val.tolist() if isinstance(val, np.ndarray) else val.asnumpy().tolist()\n        return json.dumps([self.__class__.__name__.lower(), self._kwargs])\n",
  "sig": "Constant(value|Value to set)",
  "dst": "Initializes the weights to a given value.    The value passed in can be a scalar or a NDarray that matches the shape    of the parameter to be set.    Parameters    ----------    value : float, NDArray        Value to set."
 },
 "mxnet.initializer.Xavier": {
  "code": "class Xavier(Initializer):\n    \"\"\"Returns an initializer performing \"Xavier\" initialization for weights.\n\n    This initializer is designed to keep the scale of gradients roughly the same\n    in all layers.\n\n    By default, `rnd_type` is ``'uniform'`` and `factor_type` is ``'avg'``,\n    the initializer fills the weights with random numbers in the range\n    of :math:`[-c, c]`, where :math:`c = \\\\sqrt{\\\\frac{3.}{0.5 * (n_{in} + n_{out})}}`.\n    :math:`n_{in}` is the number of neurons feeding into weights, and :math:`n_{out}` is\n    the number of neurons the result is fed to.\n\n    If `rnd_type` is ``'uniform'`` and `factor_type` is ``'in'``,\n    the :math:`c = \\\\sqrt{\\\\frac{3.}{n_{in}}}`.\n    Similarly when `factor_type` is ``'out'``, the :math:`c = \\\\sqrt{\\\\frac{3.}{n_{out}}}`.\n\n    If `rnd_type` is ``'gaussian'`` and `factor_type` is ``'avg'``,\n    the initializer fills the weights with numbers from normal distribution with\n    a standard deviation of :math:`\\\\sqrt{\\\\frac{3.}{0.5 * (n_{in} + n_{out})}}`.\n\n    Parameters\n    ----------\n    rnd_type: str, optional\n        Random generator type, can be ``'gaussian'`` or ``'uniform'``.\n\n    factor_type: str, optional\n        Can be ``'avg'``, ``'in'``, or ``'out'``.\n\n    magnitude: float, optional\n        Scale of random number.\n    \"\"\"\n    def __init__(self, rnd_type=\"uniform\", factor_type=\"avg\", magnitude=3):\n        super(Xavier, self).__init__(rnd_type=rnd_type, factor_type=factor_type,\n                                     magnitude=magnitude)\n        self.rnd_type = rnd_type\n        self.factor_type = factor_type\n        self.magnitude = float(magnitude)\n\n\n    def _init_weight(self, name, arr):\n        shape = arr.shape\n        hw_scale = 1.\n        if len(shape) < 2:\n            raise ValueError('Xavier initializer cannot be applied to vector {0}. It requires at'\n                             ' least 2D.'.format(name))\n        if len(shape) > 2:\n            hw_scale = np.prod(shape[2:])\n        fan_in, fan_out = shape[1] * hw_scale, shape[0] * hw_scale\n        factor = 1.\n        if self.factor_type == \"avg\":\n            factor = (fan_in + fan_out) / 2.0\n        elif self.factor_type == \"in\":\n            factor = fan_in\n        elif self.factor_type == \"out\":\n            factor = fan_out\n        else:\n            raise ValueError(\"Incorrect factor type\")\n        scale = np.sqrt(self.magnitude / factor)\n        if self.rnd_type == \"uniform\":\n            uniform_fn = _mx_np.random.uniform if is_np_array() else random.uniform\n            uniform_fn(-scale, scale, arr.shape, out=arr)\n        elif self.rnd_type == \"gaussian\":\n            normal_fn = _mx_np.random.normal if is_np_array() else random.normal\n            normal_fn(0, scale, arr.shape, out=arr)\n        else:\n            raise ValueError(\"Unknown random type\")\n",
  "sig": "Xavier(rnd_type|Random generator type, can be 'gaussian' or 'uniform')(factor_type|Can be 'avg', 'in', or 'out')(magnitude|Scale of random number)",
  "dst": "Returns an initializer performing \"Xavier\" initialization for weights.    This initializer is designed to keep the scale of gradients roughly the same    in all layers.    By default, `rnd_type` is ``'uniform'`` and `factor_type` is ``'avg'``,    the initializer fills the weights with random numbers in the range    of :math:`[-c, c]`, where :math:`c = \\\\sqrt{\\\\frac{3.}{0.5 * (n_{in} + n_{out})}}`.    :math:`n_{in}` is the number of neurons feeding into weights, and :math:`n_{out}` is    the number of neurons the result is fed to.    If `rnd_type` is ``'uniform'`` and `factor_type` is ``'in'``,    the :math:`c = \\\\sqrt{\\\\frac{3.}{n_{in}}}`.    Similarly when `factor_type` is ``'out'``, the :math:`c = \\\\sqrt{\\\\frac{3.}{n_{out}}}`.    If `rnd_type` is ``'gaussian'`` and `factor_type` is ``'avg'``,    the initializer fills the weights with numbers from normal distribution with    a standard deviation of :math:`\\\\sqrt{\\\\frac{3.}{0.5 * (n_{in} + n_{out})}}`.    Parameters    ----------    rnd_type: str, optional        Random generator type, can be ``'gaussian'`` or ``'uniform'``.    factor_type: str, optional        Can be ``'avg'``, ``'in'``, or ``'out'``.    magnitude: float, optional        Scale of random number."
 },
 "mxnet.initializer.One": {
  "code": "class One(Initializer):\n    \"\"\"Initializes weights to one.\n\n    Example\n    -------\n    >>> # Given 'module', an instance of 'mxnet.module.Module', initialize weights to one.\n    ...\n    >>> init = mx.initializer.One()\n    >>> module.init_params(init)\n    >>> for dictionary in module.get_params():\n    ...     for key in dictionary:\n    ...         print(key)\n    ...         print(dictionary[key].asnumpy())\n    ...\n    fullyconnected0_weight\n    [[ 1.  1.  1.]]\n    \"\"\"\n    def __init__(self):\n        super(One, self).__init__()\n\n    def _init_weight(self, _, arr):\n        arr[:] = 1\n",
  "sig": "One()",
  "dst": "Initializes weights to one.    Example    -------    >>> # Given 'module', an instance of 'mxnet.module.Module', initialize weights to one.    ...    >>> init = mx.initializer.One()    >>> module.init_params(init)    >>> for dictionary in module.get_params():    ...     for key in dictionary:    ...         print(key)    ...         print(dictionary[key].asnumpy())    ...    fullyconnected0_weight    [[ 1.  1.  1.]]"
 },
 "mxnet.initializer.Orthogonal": {
  "code": "class Orthogonal(Initializer):\n    \"\"\"Initialize weight as orthogonal matrix.\n\n    This initializer implements *Exact solutions to the nonlinear dynamics of\n    learning in deep linear neural networks*, available at\n    https://arxiv.org/abs/1312.6120.\n\n    Parameters\n    ----------\n    scale : float optional\n        Scaling factor of weight.\n\n    rand_type: string optional\n        Use \"uniform\" or \"normal\" random number to initialize weight.\n\n    \"\"\"\n    def __init__(self, scale=1.414, rand_type=\"uniform\"):\n        super(Orthogonal, self).__init__(scale=scale, rand_type=rand_type)\n        self.scale = scale\n        self.rand_type = rand_type\n\n    def _init_weight(self, _, arr):\n        nout = arr.shape[0]\n        nin = np.prod(arr.shape[1:])\n        if self.rand_type == \"uniform\":\n            tmp = random.uniform(-1.0, 1.0, shape=(nout, nin)).asnumpy()\n        elif self.rand_type == \"normal\":\n            tmp = random.normal(0.0, 1.0, shape=(nout, nin)).asnumpy()\n        u, _, v = np.linalg.svd(tmp, full_matrices=False) # pylint: disable=invalid-name\n        if u.shape == tmp.shape:\n            res = u\n        else:\n            res = v\n        res = self.scale * res.reshape(arr.shape)\n        arr[:] = res\n",
  "sig": "Orthogonal(scale|Scaling factor of weight)(rand_type|Use “uniform” or “normal” random number to initialize weight)",
  "dst": "Initialize weight as orthogonal matrix.    This initializer implements *Exact solutions to the nonlinear dynamics of    learning in deep linear neural networks*, available at    https://arxiv.org/abs/1312.6120.    Parameters    ----------    scale : float optional        Scaling factor of weight.    rand_type: string optional        Use \"uniform\" or \"normal\" random number to initialize weight."
 },
 "mxnet.initializer.Normal": {
  "code": "class Normal(Initializer):\n    \"\"\"Initializes weights with random values sampled from a normal distribution\n    with a mean of zero and standard deviation of `sigma`.\n\n    Parameters\n    ----------\n    sigma : float, optional\n        Standard deviation of the normal distribution.\n        Default standard deviation is 0.01.\n\n    Example\n    -------\n    >>> # Given 'module', an instance of 'mxnet.module.Module', initialize weights\n    >>> # to random values sampled from a normal distribution.\n    ...\n    >>> init = mx.init.Normal(0.5)\n    >>> module.init_params(init)\n    >>> for dictionary in module.get_params():\n    ...     for key in dictionary:\n    ...         print(key)\n    ...         print(dictionary[key].asnumpy())\n    ...\n    fullyconnected0_weight\n    [[-0.3214761  -0.12660924  0.53789419]]\n    \"\"\"\n    def __init__(self, sigma=0.01):\n        super(Normal, self).__init__(sigma=sigma)\n        self.sigma = sigma\n\n    def _init_weight(self, _, arr):\n        normal_fn = _mx_np.random.normal if is_np_array() else random.normal\n        normal_fn(0, self.sigma, arr.shape, out=arr)\n",
  "sig": "Normal(sigma|Standard deviation of the normal distribution. Default standard deviation is 0.01)",
  "dst": "Initializes weights with random values sampled from a normal distribution    with a mean of zero and standard deviation of `sigma`.    Parameters    ----------    sigma : float, optional        Standard deviation of the normal distribution.        Default standard deviation is 0.01.    Example    -------    >>> # Given 'module', an instance of 'mxnet.module.Module', initialize weights    >>> # to random values sampled from a normal distribution.    ...    >>> init = mx.init.Normal(0.5)    >>> module.init_params(init)    >>> for dictionary in module.get_params():    ...     for key in dictionary:    ...         print(key)    ...         print(dictionary[key].asnumpy())    ...    fullyconnected0_weight    [[-0.3214761  -0.12660924  0.53789419]]"
 },
 "mxnet.initializer.Uniform": {
  "code": "class Uniform(Initializer):\n    \"\"\"Initializes weights with random values uniformly sampled from a given range.\n\n    Parameters\n    ----------\n    scale : float, optional\n        The bound on the range of the generated random values.\n        Values are generated from the range [-`scale`, `scale`].\n        Default scale is 0.07.\n\n    Example\n    -------\n    >>> # Given 'module', an instance of 'mxnet.module.Module', initialize weights\n    >>> # to random values uniformly sampled between -0.1 and 0.1.\n    ...\n    >>> init = mx.init.Uniform(0.1)\n    >>> module.init_params(init)\n    >>> for dictionary in module.get_params():\n    ...     for key in dictionary:\n    ...         print(key)\n    ...         print(dictionary[key].asnumpy())\n    ...\n    fullyconnected0_weight\n    [[ 0.01360891 -0.02144304  0.08511933]]\n    \"\"\"\n    def __init__(self, scale=0.07):\n        super(Uniform, self).__init__(scale=scale)\n        self.scale = scale\n\n    def _init_weight(self, _, arr):\n        uniform_fn = _mx_np.random.uniform if is_np_array() else random.uniform\n        uniform_fn(-self.scale, self.scale, arr.shape, out=arr)\n",
  "sig": "Uniform(scale|The bound on the range of the generated random values. Values are generated from the range [-scale, scale]. Default scale is 0.07)",
  "dst": "Initializes weights with random values uniformly sampled from a given range.    Parameters    ----------    scale : float, optional        The bound on the range of the generated random values.        Values are generated from the range [-`scale`, `scale`].        Default scale is 0.07.    Example    -------    >>> # Given 'module', an instance of 'mxnet.module.Module', initialize weights    >>> # to random values uniformly sampled between -0.1 and 0.1.    ...    >>> init = mx.init.Uniform(0.1)    >>> module.init_params(init)    >>> for dictionary in module.get_params():    ...     for key in dictionary:    ...         print(key)    ...         print(dictionary[key].asnumpy())    ...    fullyconnected0_weight    [[ 0.01360891 -0.02144304  0.08511933]]"
 },
 "mxnet.initializer.Zero": {
  "code": "class Zero(Initializer):\n    \"\"\"Initializes weights to zero.\n\n    Example\n    -------\n    >>> # Given 'module', an instance of 'mxnet.module.Module', initialize weights to zero.\n    ...\n    >>> init = mx.initializer.Zero()\n    >>> module.init_params(init)\n    >>> for dictionary in module.get_params():\n    ...     for key in dictionary:\n    ...         print(key)\n    ...         print(dictionary[key].asnumpy())\n    ...\n    fullyconnected0_weight\n    [[ 0.  0.  0.]]\n    \"\"\"\n    def __init__(self):\n        super(Zero, self).__init__()\n\n    def _init_weight(self, _, arr):\n        arr[:] = 0\n",
  "sig": "Zero()",
  "dst": "Initializes weights to zero.    Example    -------    >>> # Given 'module', an instance of 'mxnet.module.Module', initialize weights to zero.    ...    >>> init = mx.initializer.Zero()    >>> module.init_params(init)    >>> for dictionary in module.get_params():    ...     for key in dictionary:    ...         print(key)    ...         print(dictionary[key].asnumpy())    ...    fullyconnected0_weight    [[ 0.  0.  0.]]"
 },
 "mxnet.gluon.nn.BatchNorm": {
  "code": "class BatchNorm(_BatchNorm):\n    \"\"\"Batch normalization layer (Ioffe and Szegedy, 2014).\n    Normalizes the input at each batch, i.e. applies a transformation\n    that maintains the mean activation close to 0 and the activation\n    standard deviation close to 1.\n\n    Parameters\n    ----------\n    axis : int, default 1\n        The axis that should be normalized. This is typically the channels\n        (C) axis. For instance, after a `Conv2D` layer with `layout='NCHW'`,\n        set `axis=1` in `BatchNorm`. If `layout='NHWC'`, then set `axis=3`.\n    momentum: float, default 0.9\n        Momentum for the moving average.\n    epsilon: float, default 1e-5\n        Small float added to variance to avoid dividing by zero.\n    center: bool, default True\n        If True, add offset of `beta` to normalized tensor.\n        If False, `beta` is ignored.\n    scale: bool, default True\n        If True, multiply by `gamma`. If False, `gamma` is not used.\n        When the next layer is linear (also e.g. `nn.relu`),\n        this can be disabled since the scaling\n        will be done by the next layer.\n    use_global_stats: bool, default False\n        If True, use global moving statistics instead of local batch-norm. This will force\n        change batch-norm into a scale shift operator.\n        If False, use local batch-norm.\n    beta_initializer: str or `Initializer`, default 'zeros'\n        Initializer for the beta weight.\n    gamma_initializer: str or `Initializer`, default 'ones'\n        Initializer for the gamma weight.\n    running_mean_initializer: str or `Initializer`, default 'zeros'\n        Initializer for the running mean.\n    running_variance_initializer: str or `Initializer`, default 'ones'\n        Initializer for the running variance.\n    in_channels : int, default 0\n        Number of channels (feature maps) in input data. If not specified,\n        initialization will be deferred to the first time `forward` is called\n        and `in_channels` will be inferred from the shape of input data.\n\n\n    Inputs:\n        - **data**: input tensor with arbitrary shape.\n\n    Outputs:\n        - **out**: output tensor with the same shape as `data`.\n    \"\"\"\n    def __init__(self, axis=1, momentum=0.9, epsilon=1e-5, center=True, scale=True,\n                 use_global_stats=False,\n                 beta_initializer='zeros', gamma_initializer='ones',\n                 running_mean_initializer='zeros', running_variance_initializer='ones',\n                 in_channels=0, **kwargs):\n        super(BatchNorm, self).__init__(\n            axis=axis, momentum=momentum, epsilon=epsilon, center=center,\n            scale=scale,\n            use_global_stats=use_global_stats, fuse_relu=False,\n            beta_initializer=beta_initializer,\n            gamma_initializer=gamma_initializer,\n            running_mean_initializer=running_mean_initializer,\n            running_variance_initializer=running_variance_initializer,\n            in_channels=in_channels, **kwargs)\n",
  "sig": "BatchNorm(axis|The axis that should be normalized. This is typically the channels (C) axis. For instance, after a Conv2D layer with layout='NCHW', set axis=1 in BatchNorm. If layout='NHWC', then set axis=3)(momentum|Momentum for the moving average)(epsilon|Small float added to variance to avoid dividing by zero)(center|If True, add offset of beta to normalized tensor. If False, beta is ignored)(scale|If True, multiply by gamma. If False, gamma is not used. When the next layer is linear (also e.g. nn.relu), this can be disabled since the scaling will be done by the next layer)(use_global_stats|If True, use global moving statistics instead of local batch-norm. This will force change batch-norm into a scale shift operator. If False, use local batch-norm)(beta_initializer|Initializer for the beta weight)(gamma_initializer|Initializer for the gamma weight)(running_mean_initializer|Initializer for the running mean)(running_variance_initializer|Initializer for the running variance)(in_channels|Number of channels (feature maps) in input data. If not specified, initialization will be deferred to the first time forward is called and in_channels will be inferred from the shape of input data)",
  "dst": "Batch normalization layer (Ioffe and Szegedy, 2014).    Normalizes the input at each batch, i.e. applies a transformation    that maintains the mean activation close to 0 and the activation    standard deviation close to 1.    Parameters    ----------    axis : int, default 1        The axis that should be normalized. This is typically the channels        (C) axis. For instance, after a `Conv2D` layer with `layout='NCHW'`,        set `axis=1` in `BatchNorm`. If `layout='NHWC'`, then set `axis=3`.    momentum: float, default 0.9        Momentum for the moving average.    epsilon: float, default 1e-5        Small float added to variance to avoid dividing by zero.    center: bool, default True        If True, add offset of `beta` to normalized tensor.        If False, `beta` is ignored.    scale: bool, default True        If True, multiply by `gamma`. If False, `gamma` is not used.        When the next layer is linear (also e.g. `nn.relu`),        this can be disabled since the scaling        will be done by the next layer.    use_global_stats: bool, default False        If True, use global moving statistics instead of local batch-norm. This will force        change batch-norm into a scale shift operator.        If False, use local batch-norm.    beta_initializer: str or `Initializer`, default 'zeros'        Initializer for the beta weight.    gamma_initializer: str or `Initializer`, default 'ones'        Initializer for the gamma weight.    running_mean_initializer: str or `Initializer`, default 'zeros'        Initializer for the running mean.    running_variance_initializer: str or `Initializer`, default 'ones'        Initializer for the running variance.    in_channels : int, default 0        Number of channels (feature maps) in input data. If not specified,        initialization will be deferred to the first time `forward` is called        and `in_channels` will be inferred from the shape of input data.    Inputs:        - **data**: input tensor with arbitrary shape.    Outputs:        - **out**: output tensor with the same shape as `data`."
 },
 "mxnet.gluon.rnn.GRU": {
  "code": "class GRU(_RNNLayer):\n    r\"\"\"Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.\n    Note: this is an implementation of the cuDNN version of GRUs\n    (slight modification compared to Cho et al. 2014; the reset gate :math:`r_t`\n    is applied after matrix multiplication).\n\n    For each element in the input sequence, each layer computes the following\n    function:\n\n    .. math::\n        \\begin{array}{ll}\n        r_t = sigmoid(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\\\\n        i_t = sigmoid(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\\\\n        n_t = \\tanh(W_{in} x_t + b_{in} + r_t * (W_{hn} h_{(t-1)} + b_{hn})) \\\\\n        h_t = (1 - i_t) * n_t + i_t * h_{(t-1)} \\\\\n        \\end{array}\n\n    where :math:`h_t` is the hidden state at time `t`, :math:`x_t` is the hidden\n    state of the previous layer at time `t` or :math:`input_t` for the first layer,\n    and :math:`r_t`, :math:`i_t`, :math:`n_t` are the reset, input, and new gates, respectively.\n\n    Parameters\n    ----------\n    hidden_size: int\n        The number of features in the hidden state h\n    num_layers: int, default 1\n        Number of recurrent layers.\n    layout : str, default 'TNC'\n        The format of input and output tensors. T, N and C stand for\n        sequence length, batch size, and feature dimensions respectively.\n    dropout: float, default 0\n        If non-zero, introduces a dropout layer on the outputs of each\n        RNN layer except the last layer\n    bidirectional: bool, default False\n        If True, becomes a bidirectional RNN.\n    i2h_weight_initializer : str or Initializer\n        Initializer for the input weights matrix, used for the linear\n        transformation of the inputs.\n    h2h_weight_initializer : str or Initializer\n        Initializer for the recurrent weights matrix, used for the linear\n        transformation of the recurrent state.\n    i2h_bias_initializer : str or Initializer\n        Initializer for the bias vector.\n    h2h_bias_initializer : str or Initializer\n        Initializer for the bias vector.\n    dtype : str, default 'float32'\n        Type to initialize the parameters and default states to\n    input_size: int, default 0\n        The number of expected features in the input x.\n        If not specified, it will be inferred from input.\n    prefix : str or None\n        Prefix of this `Block`.\n    params : ParameterDict or None\n        Shared Parameters for this `Block`.\n\n\n    Inputs:\n        - **data**: input tensor with shape `(sequence_length, batch_size, input_size)`\n          when `layout` is \"TNC\". For other layouts, dimensions are permuted accordingly\n          using transpose() operator which adds performance overhead. Consider creating\n          batches in TNC layout during data batching step.\n        - **states**: initial recurrent state tensor with shape\n          `(num_layers, batch_size, num_hidden)`. If `bidirectional` is True,\n          shape will instead be `(2*num_layers, batch_size, num_hidden)`. If\n          `states` is None, zeros will be used as default begin states.\n\n    Outputs:\n        - **out**: output tensor with shape `(sequence_length, batch_size, num_hidden)`\n          when `layout` is \"TNC\". If `bidirectional` is True, output shape will instead\n          be `(sequence_length, batch_size, 2*num_hidden)`\n        - **out_states**: output recurrent state tensor with the same shape as `states`.\n          If `states` is None `out_states` will not be returned.\n\n\n    Examples\n    --------\n    >>> layer = mx.gluon.rnn.GRU(100, 3)\n    >>> layer.initialize()\n    >>> input = mx.nd.random.uniform(shape=(5, 3, 10))\n    >>> # by default zeros are used as begin state\n    >>> output = layer(input)\n    >>> # manually specify begin state.\n    >>> h0 = mx.nd.random.uniform(shape=(3, 3, 100))\n    >>> output, hn = layer(input, h0)\n    \"\"\"\n    def __init__(self, hidden_size, num_layers=1, layout='TNC',\n                 dropout=0, bidirectional=False, input_size=0,\n                 i2h_weight_initializer=None, h2h_weight_initializer=None,\n                 i2h_bias_initializer='zeros', h2h_bias_initializer='zeros',\n                 dtype='float32', **kwargs):\n        super(GRU, self).__init__(hidden_size, num_layers, layout,\n                                  dropout, bidirectional, input_size,\n                                  i2h_weight_initializer, h2h_weight_initializer,\n                                  i2h_bias_initializer, h2h_bias_initializer,\n                                  'gru', None, None, None, None, False,\n                                  dtype, **kwargs)\n\n    def state_info(self, batch_size=0):\n        return [{'shape': (self._num_layers * self._dir, batch_size, self._hidden_size),\n                 '__layout__': 'LNC', 'dtype': self._dtype}]\n",
  "sig": "GRU(hidden_size|The number of features in the hidden state h)(num_layers|Number of recurrent layers)(layout|The format of input and output tensors. T, N and C stand for sequence length, batch size, and feature dimensions respectively)(dropout|If non-zero, introduces a dropout layer on the outputs of each RNN layer except the last layer)(bidirectional|If True, becomes a bidirectional RNN)(i2h_weight_initializer|Initializer for the input weights matrix, used for the linear transformation of the inputs)(h2h_weight_initializer|Initializer for the recurrent weights matrix, used for the linear transformation of the recurrent state)(i2h_bias_initializer|Initializer for the bias vector)(h2h_bias_initializer|Initializer for the bias vector)(dtype|Type to initialize the parameters and default states to)(input_size|The number of expected features in the input x. If not specified, it will be inferred from input)(prefix|Prefix of this Block)(params|Shared Parameters for this Block)",
  "dst": "Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.    Note: this is an implementation of the cuDNN version of GRUs    (slight modification compared to Cho et al. 2014; the reset gate :math:`r_t`    is applied after matrix multiplication).    For each element in the input sequence, each layer computes the following    function:    .. math::        \\begin{array}{ll}        r_t = sigmoid(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\\\        i_t = sigmoid(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\\\        n_t = \\tanh(W_{in} x_t + b_{in} + r_t * (W_{hn} h_{(t-1)} + b_{hn})) \\\\        h_t = (1 - i_t) * n_t + i_t * h_{(t-1)} \\\\        \\end{array}    where :math:`h_t` is the hidden state at time `t`, :math:`x_t` is the hidden    state of the previous layer at time `t` or :math:`input_t` for the first layer,    and :math:`r_t`, :math:`i_t`, :math:`n_t` are the reset, input, and new gates, respectively.    Parameters    ----------    hidden_size: int        The number of features in the hidden state h    num_layers: int, default 1        Number of recurrent layers.    layout : str, default 'TNC'        The format of input and output tensors. T, N and C stand for        sequence length, batch size, and feature dimensions respectively.    dropout: float, default 0        If non-zero, introduces a dropout layer on the outputs of each        RNN layer except the last layer    bidirectional: bool, default False        If True, becomes a bidirectional RNN.    i2h_weight_initializer : str or Initializer        Initializer for the input weights matrix, used for the linear        transformation of the inputs.    h2h_weight_initializer : str or Initializer        Initializer for the recurrent weights matrix, used for the linear        transformation of the recurrent state.    i2h_bias_initializer : str or Initializer        Initializer for the bias vector.    h2h_bias_initializer : str or Initializer        Initializer for the bias vector.    dtype : str, default 'float32'        Type to initialize the parameters and default states to    input_size: int, default 0        The number of expected features in the input x.        If not specified, it will be inferred from input.    prefix : str or None        Prefix of this `Block`.    params : ParameterDict or None        Shared Parameters for this `Block`.    Inputs:        - **data**: input tensor with shape `(sequence_length, batch_size, input_size)`          when `layout` is \"TNC\". For other layouts, dimensions are permuted accordingly          using transpose() operator which adds performance overhead. Consider creating          batches in TNC layout during data batching step.        - **states**: initial recurrent state tensor with shape          `(num_layers, batch_size, num_hidden)`. If `bidirectional` is True,          shape will instead be `(2*num_layers, batch_size, num_hidden)`. If          `states` is None, zeros will be used as default begin states.    Outputs:        - **out**: output tensor with shape `(sequence_length, batch_size, num_hidden)`          when `layout` is \"TNC\". If `bidirectional` is True, output shape will instead          be `(sequence_length, batch_size, 2*num_hidden)`        - **out_states**: output recurrent state tensor with the same shape as `states`.          If `states` is None `out_states` will not be returned.    Examples    --------    >>> layer = mx.gluon.rnn.GRU(100, 3)    >>> layer.initialize()    >>> input = mx.nd.random.uniform(shape=(5, 3, 10))    >>> # by default zeros are used as begin state    >>> output = layer(input)    >>> # manually specify begin state.    >>> h0 = mx.nd.random.uniform(shape=(3, 3, 100))    >>> output, hn = layer(input, h0)"
 },
 "mxnet.gluon.rnn.RNNCell": {
  "code": "class RNNCell(HybridRecurrentCell):\n    r\"\"\"Elman RNN recurrent neural network cell.\n\n    Each call computes the following function:\n\n    .. math::\n\n        h_t = \\tanh(w_{ih} * x_t + b_{ih}  +  w_{hh} * h_{(t-1)} + b_{hh})\n\n    where :math:`h_t` is the hidden state at time `t`, and :math:`x_t` is the hidden\n    state of the previous layer at time `t` or :math:`input_t` for the first layer.\n    If nonlinearity='relu', then `ReLU` is used instead of `tanh`.\n\n    Parameters\n    ----------\n    hidden_size : int\n        Number of units in output symbol\n    activation : str or Symbol, default 'tanh'\n        Type of activation function.\n    i2h_weight_initializer : str or Initializer\n        Initializer for the input weights matrix, used for the linear\n        transformation of the inputs.\n    h2h_weight_initializer : str or Initializer\n        Initializer for the recurrent weights matrix, used for the linear\n        transformation of the recurrent state.\n    i2h_bias_initializer : str or Initializer, default 'zeros'\n        Initializer for the bias vector.\n    h2h_bias_initializer : str or Initializer, default 'zeros'\n        Initializer for the bias vector.\n    prefix : str, default ``'rnn_'``\n        Prefix for name of `Block`s\n        (and name of weight if params is `None`).\n    params : Parameter or None\n        Container for weight sharing between cells.\n        Created if `None`.\n\n\n    Inputs:\n        - **data**: input tensor with shape `(batch_size, input_size)`.\n        - **states**: a list of one initial recurrent state tensor with shape\n          `(batch_size, num_hidden)`.\n\n    Outputs:\n        - **out**: output tensor with shape `(batch_size, num_hidden)`.\n        - **next_states**: a list of one output recurrent state tensor with the\n          same shape as `states`.\n    \"\"\"\n    def __init__(self, hidden_size, activation='tanh',\n                 i2h_weight_initializer=None, h2h_weight_initializer=None,\n                 i2h_bias_initializer='zeros', h2h_bias_initializer='zeros',\n                 input_size=0, prefix=None, params=None):\n        super(RNNCell, self).__init__(prefix=prefix, params=params)\n        self._hidden_size = hidden_size\n        self._activation = activation\n        self._input_size = input_size\n        self.i2h_weight = self.params.get('i2h_weight', shape=(hidden_size, input_size),\n                                          init=i2h_weight_initializer,\n                                          allow_deferred_init=True)\n        self.h2h_weight = self.params.get('h2h_weight', shape=(hidden_size, hidden_size),\n                                          init=h2h_weight_initializer,\n                                          allow_deferred_init=True)\n        self.i2h_bias = self.params.get('i2h_bias', shape=(hidden_size,),\n                                        init=i2h_bias_initializer,\n                                        allow_deferred_init=True)\n        self.h2h_bias = self.params.get('h2h_bias', shape=(hidden_size,),\n                                        init=h2h_bias_initializer,\n                                        allow_deferred_init=True)\n\n    def state_info(self, batch_size=0):\n        return [{'shape': (batch_size, self._hidden_size), '__layout__': 'NC'}]\n\n    def _alias(self):\n        return 'rnn'\n\n    def __repr__(self):\n        s = '{name}({mapping}'\n        if hasattr(self, '_activation'):\n            s += ', {_activation}'\n        s += ')'\n        shape = self.i2h_weight.shape\n        mapping = '{0} -> {1}'.format(shape[1] if shape[1] else None, shape[0])\n        return s.format(name=self.__class__.__name__,\n                        mapping=mapping,\n                        **self.__dict__)\n\n    def hybrid_forward(self, F, inputs, states, i2h_weight,\n                       h2h_weight, i2h_bias, h2h_bias):\n        prefix = 't%d_'%self._counter\n        i2h = F.FullyConnected(data=inputs, weight=i2h_weight, bias=i2h_bias,\n                               num_hidden=self._hidden_size,\n                               name=prefix+'i2h')\n        h2h = F.FullyConnected(data=states[0], weight=h2h_weight, bias=h2h_bias,\n                               num_hidden=self._hidden_size,\n                               name=prefix+'h2h')\n        i2h_plus_h2h = F.elemwise_add(i2h, h2h, name=prefix+'plus0')\n        output = self._get_activation(F, i2h_plus_h2h, self._activation,\n                                      name=prefix+'out')\n\n        return output, [output]\n",
  "sig": "RNNCell(hidden_size|Number of units in output symbol)(activation|Type of activation function)(i2h_weight_initializer|Initializer for the input weights matrix, used for the linear transformation of the inputs)(h2h_weight_initializer|Initializer for the recurrent weights matrix, used for the linear transformation of the recurrent state)(i2h_bias_initializer|Initializer for the bias vector)(h2h_bias_initializer|Initializer for the bias vector)(prefix|Prefix for name of Block`s (and name of weight if params is `None))(params|Container for weight sharing between cells. Created if None)",
  "dst": "Elman RNN recurrent neural network cell.    Each call computes the following function:    .. math::        h_t = \\tanh(w_{ih} * x_t + b_{ih}  +  w_{hh} * h_{(t-1)} + b_{hh})    where :math:`h_t` is the hidden state at time `t`, and :math:`x_t` is the hidden    state of the previous layer at time `t` or :math:`input_t` for the first layer.    If nonlinearity='relu', then `ReLU` is used instead of `tanh`.    Parameters    ----------    hidden_size : int        Number of units in output symbol    activation : str or Symbol, default 'tanh'        Type of activation function.    i2h_weight_initializer : str or Initializer        Initializer for the input weights matrix, used for the linear        transformation of the inputs.    h2h_weight_initializer : str or Initializer        Initializer for the recurrent weights matrix, used for the linear        transformation of the recurrent state.    i2h_bias_initializer : str or Initializer, default 'zeros'        Initializer for the bias vector.    h2h_bias_initializer : str or Initializer, default 'zeros'        Initializer for the bias vector.    prefix : str, default ``'rnn_'``        Prefix for name of `Block`s        (and name of weight if params is `None`).    params : Parameter or None        Container for weight sharing between cells.        Created if `None`.    Inputs:        - **data**: input tensor with shape `(batch_size, input_size)`.        - **states**: a list of one initial recurrent state tensor with shape          `(batch_size, num_hidden)`.    Outputs:        - **out**: output tensor with shape `(batch_size, num_hidden)`.        - **next_states**: a list of one output recurrent state tensor with the          same shape as `states`."
 },
 "mxnet.gluon.rnn.LSTM": {
  "code": "class LSTM(_RNNLayer):\n    r\"\"\"Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence.\n\n    For each element in the input sequence, each layer computes the following\n    function:\n\n    .. math::\n        \\begin{array}{ll}\n        i_t = sigmoid(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\\\\n        f_t = sigmoid(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\\\\n        g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hc} h_{(t-1)} + b_{hg}) \\\\\n        o_t = sigmoid(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\\\\n        c_t = f_t * c_{(t-1)} + i_t * g_t \\\\\n        h_t = o_t * \\tanh(c_t)\n        \\end{array}\n\n    where :math:`h_t` is the hidden state at time `t`, :math:`c_t` is the\n    cell state at time `t`, :math:`x_t` is the hidden state of the previous\n    layer at time `t` or :math:`input_t` for the first layer, and :math:`i_t`,\n    :math:`f_t`, :math:`g_t`, :math:`o_t` are the input, forget, cell, and\n    out gates, respectively.\n\n    Parameters\n    ----------\n    hidden_size: int\n        The number of features in the hidden state h.\n    num_layers: int, default 1\n        Number of recurrent layers.\n    layout : str, default 'TNC'\n        The format of input and output tensors. T, N and C stand for\n        sequence length, batch size, and feature dimensions respectively.\n    dropout: float, default 0\n        If non-zero, introduces a dropout layer on the outputs of each\n        RNN layer except the last layer.\n    bidirectional: bool, default False\n        If `True`, becomes a bidirectional RNN.\n    i2h_weight_initializer : str or Initializer\n        Initializer for the input weights matrix, used for the linear\n        transformation of the inputs.\n    h2h_weight_initializer : str or Initializer\n        Initializer for the recurrent weights matrix, used for the linear\n        transformation of the recurrent state.\n    i2h_bias_initializer : str or Initializer, default 'lstmbias'\n        Initializer for the bias vector. By default, bias for the forget\n        gate is initialized to 1 while all other biases are initialized\n        to zero.\n    h2h_bias_initializer : str or Initializer\n        Initializer for the bias vector.\n    projection_size: int, default None\n        The number of features after projection.\n    h2r_weight_initializer : str or Initializer, default None\n        Initializer for the projected recurrent weights matrix, used for the linear\n        transformation of the recurrent state to the projected space.\n    state_clip_min : float or None, default None\n        Minimum clip value of LSTM states. This option must be used together with\n        state_clip_max. If None, clipping is not applied.\n    state_clip_max : float or None, default None\n        Maximum clip value of LSTM states. This option must be used together with\n        state_clip_min. If None, clipping is not applied.\n    state_clip_nan : boolean, default False\n        Whether to stop NaN from propagating in state by clipping it to min/max.\n        If the clipping range is not specified, this option is ignored.\n    dtype : str, default 'float32'\n        Type to initialize the parameters and default states to\n    input_size: int, default 0\n        The number of expected features in the input x.\n        If not specified, it will be inferred from input.\n    prefix : str or None\n        Prefix of this `Block`.\n    params : `ParameterDict` or `None`\n        Shared Parameters for this `Block`.\n\n\n    Inputs:\n        - **data**: input tensor with shape `(sequence_length, batch_size, input_size)`\n          when `layout` is \"TNC\". For other layouts, dimensions are permuted accordingly\n          using transpose() operator which adds performance overhead. Consider creating\n          batches in TNC layout during data batching step.\n        - **states**: a list of two initial recurrent state tensors. Each has shape\n          `(num_layers, batch_size, num_hidden)`. If `bidirectional` is True,\n          shape will instead be `(2*num_layers, batch_size, num_hidden)`. If\n          `states` is None, zeros will be used as default begin states.\n\n    Outputs:\n        - **out**: output tensor with shape `(sequence_length, batch_size, num_hidden)`\n          when `layout` is \"TNC\". If `bidirectional` is True, output shape will instead\n          be `(sequence_length, batch_size, 2*num_hidden)`\n        - **out_states**: a list of two output recurrent state tensors with the same\n          shape as in `states`. If `states` is None `out_states` will not be returned.\n\n\n    Examples\n    --------\n    >>> layer = mx.gluon.rnn.LSTM(100, 3)\n    >>> layer.initialize()\n    >>> input = mx.nd.random.uniform(shape=(5, 3, 10))\n    >>> # by default zeros are used as begin state\n    >>> output = layer(input)\n    >>> # manually specify begin state.\n    >>> h0 = mx.nd.random.uniform(shape=(3, 3, 100))\n    >>> c0 = mx.nd.random.uniform(shape=(3, 3, 100))\n    >>> output, hn = layer(input, [h0, c0])\n    \"\"\"\n    def __init__(self, hidden_size, num_layers=1, layout='TNC',\n                 dropout=0, bidirectional=False, input_size=0,\n                 i2h_weight_initializer=None, h2h_weight_initializer=None,\n                 i2h_bias_initializer='zeros', h2h_bias_initializer='zeros',\n                 projection_size=None, h2r_weight_initializer=None,\n                 state_clip_min=None, state_clip_max=None, state_clip_nan=False,\n                 dtype='float32', **kwargs):\n        super(LSTM, self).__init__(hidden_size, num_layers, layout,\n                                   dropout, bidirectional, input_size,\n                                   i2h_weight_initializer, h2h_weight_initializer,\n                                   i2h_bias_initializer, h2h_bias_initializer,\n                                   'lstm', projection_size, h2r_weight_initializer,\n                                   state_clip_min, state_clip_max, state_clip_nan,\n                                   dtype, **kwargs)\n\n    def state_info(self, batch_size=0):\n        if self._projection_size is None:\n            return [{'shape': (self._num_layers * self._dir, batch_size, self._hidden_size),\n                     '__layout__': 'LNC', 'dtype': self._dtype},\n                    {'shape': (self._num_layers * self._dir, batch_size, self._hidden_size),\n                     '__layout__': 'LNC', 'dtype': self._dtype}]\n        else:\n            return [{'shape': (self._num_layers * self._dir, batch_size, self._projection_size),\n                     '__layout__': 'LNC', 'dtype': self._dtype},\n                    {'shape': (self._num_layers * self._dir, batch_size, self._hidden_size),\n                     '__layout__': 'LNC', 'dtype': self._dtype}]\n",
  "sig": "LSTM(hidden_size|The number of features in the hidden state h)(num_layers|Number of recurrent layers)(layout|The format of input and output tensors. T, N and C stand for sequence length, batch size, and feature dimensions respectively)(dropout|If non-zero, introduces a dropout layer on the outputs of each RNN layer except the last layer)(bidirectional|If True, becomes a bidirectional RNN)(i2h_weight_initializer|Initializer for the input weights matrix, used for the linear transformation of the inputs)(h2h_weight_initializer|Initializer for the input weights matrix, used for the linear transformation of the inputs)(i2h_bias_initializer|Initializer for the bias vector. By default, bias for the forget gate is initialized to 1 while all other biases are initialized to zero)(h2h_bias_initializer| Initializer for the bias vector)(projection_size|The number of features after projection)(h2r_weight_initializer|Initializer for the projected recurrent weights matrix, used for the linear transformation of the recurrent state to the projected space)(state_clip_min|Minimum clip value of LSTM states. This option must be used together with state_clip_max. If None, clipping is not applied)(state_clip_max|Maximum clip value of LSTM states. This option must be used together with state_clip_min. If None, clipping is not applied)(state_clip_nan|Whether to stop NaN from propagating in state by clipping it to min/max. If the clipping range is not specified, this option is ignored)(dtype|Type to initialize the parameters and default states to)(input_size|The number of expected features in the input x. If not specified, it will be inferred from input)(prefix|Prefix of this Block)(params|Shared Parameters for this Block)", 
  "dst": "Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence.    For each element in the input sequence, each layer computes the following    function:    .. math::        \\begin{array}{ll}        i_t = sigmoid(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\\\        f_t = sigmoid(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\\\        g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hc} h_{(t-1)} + b_{hg}) \\\\        o_t = sigmoid(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\\\        c_t = f_t * c_{(t-1)} + i_t * g_t \\\\        h_t = o_t * \\tanh(c_t)        \\end{array}    where :math:`h_t` is the hidden state at time `t`, :math:`c_t` is the    cell state at time `t`, :math:`x_t` is the hidden state of the previous    layer at time `t` or :math:`input_t` for the first layer, and :math:`i_t`,    :math:`f_t`, :math:`g_t`, :math:`o_t` are the input, forget, cell, and    out gates, respectively.    Parameters    ----------    hidden_size: int        The number of features in the hidden state h.    num_layers: int, default 1        Number of recurrent layers.    layout : str, default 'TNC'        The format of input and output tensors. T, N and C stand for        sequence length, batch size, and feature dimensions respectively.    dropout: float, default 0        If non-zero, introduces a dropout layer on the outputs of each        RNN layer except the last layer.    bidirectional: bool, default False        If `True`, becomes a bidirectional RNN.    i2h_weight_initializer : str or Initializer        Initializer for the input weights matrix, used for the linear        transformation of the inputs.    h2h_weight_initializer : str or Initializer        Initializer for the recurrent weights matrix, used for the linear        transformation of the recurrent state.    i2h_bias_initializer : str or Initializer, default 'lstmbias'        Initializer for the bias vector. By default, bias for the forget        gate is initialized to 1 while all other biases are initialized        to zero.    h2h_bias_initializer : str or Initializer        Initializer for the bias vector.    projection_size: int, default None        The number of features after projection.    h2r_weight_initializer : str or Initializer, default None        Initializer for the projected recurrent weights matrix, used for the linear        transformation of the recurrent state to the projected space.    state_clip_min : float or None, default None        Minimum clip value of LSTM states. This option must be used together with        state_clip_max. If None, clipping is not applied.    state_clip_max : float or None, default None        Maximum clip value of LSTM states. This option must be used together with        state_clip_min. If None, clipping is not applied.    state_clip_nan : boolean, default False        Whether to stop NaN from propagating in state by clipping it to min/max.        If the clipping range is not specified, this option is ignored.    dtype : str, default 'float32'        Type to initialize the parameters and default states to    input_size: int, default 0        The number of expected features in the input x.        If not specified, it will be inferred from input.    prefix : str or None        Prefix of this `Block`.    params : `ParameterDict` or `None`        Shared Parameters for this `Block`.    Inputs:        - **data**: input tensor with shape `(sequence_length, batch_size, input_size)`          when `layout` is \"TNC\". For other layouts, dimensions are permuted accordingly          using transpose() operator which adds performance overhead. Consider creating          batches in TNC layout during data batching step.        - **states**: a list of two initial recurrent state tensors. Each has shape          `(num_layers, batch_size, num_hidden)`. If `bidirectional` is True,          shape will instead be `(2*num_layers, batch_size, num_hidden)`. If          `states` is None, zeros will be used as default begin states.    Outputs:        - **out**: output tensor with shape `(sequence_length, batch_size, num_hidden)`          when `layout` is \"TNC\". If `bidirectional` is True, output shape will instead          be `(sequence_length, batch_size, 2*num_hidden)`        - **out_states**: a list of two output recurrent state tensors with the same          shape as in `states`. If `states` is None `out_states` will not be returned.    Examples    --------    >>> layer = mx.gluon.rnn.LSTM(100, 3)    >>> layer.initialize()    >>> input = mx.nd.random.uniform(shape=(5, 3, 10))    >>> # by default zeros are used as begin state    >>> output = layer(input)    >>> # manually specify begin state.    >>> h0 = mx.nd.random.uniform(shape=(3, 3, 100))    >>> c0 = mx.nd.random.uniform(shape=(3, 3, 100))    >>> output, hn = layer(input, [h0, c0])"
 },
 "mxnet.gluon.rnn.LSTMCell": {
  "code": "class LSTMCell(HybridRecurrentCell):\n    r\"\"\"Long-Short Term Memory (LSTM) network cell.\n\n    Each call computes the following function:\n\n    .. math::\n        \\begin{array}{ll}\n        i_t = sigmoid(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\\\\n        f_t = sigmoid(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\\\\n        g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hc} h_{(t-1)} + b_{hg}) \\\\\n        o_t = sigmoid(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\\\\n        c_t = f_t * c_{(t-1)} + i_t * g_t \\\\\n        h_t = o_t * \\tanh(c_t)\n        \\end{array}\n\n    where :math:`h_t` is the hidden state at time `t`, :math:`c_t` is the\n    cell state at time `t`, :math:`x_t` is the hidden state of the previous\n    layer at time `t` or :math:`input_t` for the first layer, and :math:`i_t`,\n    :math:`f_t`, :math:`g_t`, :math:`o_t` are the input, forget, cell, and\n    out gates, respectively.\n\n    Parameters\n    ----------\n    hidden_size : int\n        Number of units in output symbol.\n    i2h_weight_initializer : str or Initializer\n        Initializer for the input weights matrix, used for the linear\n        transformation of the inputs.\n    h2h_weight_initializer : str or Initializer\n        Initializer for the recurrent weights matrix, used for the linear\n        transformation of the recurrent state.\n    i2h_bias_initializer : str or Initializer, default 'zeros'\n        Initializer for the bias vector.\n    h2h_bias_initializer : str or Initializer, default 'zeros'\n        Initializer for the bias vector.\n    prefix : str, default ``'lstm_'``\n        Prefix for name of `Block`s\n        (and name of weight if params is `None`).\n    params : Parameter or None, default None\n        Container for weight sharing between cells.\n        Created if `None`.\n    activation : str, default 'tanh'\n        Activation type to use. See nd/symbol Activation\n        for supported types.\n    recurrent_activation : str, default 'sigmoid'\n        Activation type to use for the recurrent step. See nd/symbol Activation\n        for supported types.\n\n    Inputs:\n        - **data**: input tensor with shape `(batch_size, input_size)`.\n        - **states**: a list of two initial recurrent state tensors. Each has shape\n          `(batch_size, num_hidden)`.\n\n    Outputs:\n        - **out**: output tensor with shape `(batch_size, num_hidden)`.\n        - **next_states**: a list of two output recurrent state tensors. Each has\n          the same shape as `states`.\n    \"\"\"\n    # pylint: disable=too-many-instance-attributes\n    def __init__(self, hidden_size,\n                 i2h_weight_initializer=None, h2h_weight_initializer=None,\n                 i2h_bias_initializer='zeros', h2h_bias_initializer='zeros',\n                 input_size=0, prefix=None, params=None, activation='tanh',\n                 recurrent_activation='sigmoid'):\n        super(LSTMCell, self).__init__(prefix=prefix, params=params)\n\n        self._hidden_size = hidden_size\n        self._input_size = input_size\n        self.i2h_weight = self.params.get('i2h_weight', shape=(4*hidden_size, input_size),\n                                          init=i2h_weight_initializer,\n                                          allow_deferred_init=True)\n        self.h2h_weight = self.params.get('h2h_weight', shape=(4*hidden_size, hidden_size),\n                                          init=h2h_weight_initializer,\n                                          allow_deferred_init=True)\n        self.i2h_bias = self.params.get('i2h_bias', shape=(4*hidden_size,),\n                                        init=i2h_bias_initializer,\n                                        allow_deferred_init=True)\n        self.h2h_bias = self.params.get('h2h_bias', shape=(4*hidden_size,),\n                                        init=h2h_bias_initializer,\n                                        allow_deferred_init=True)\n        self._activation = activation\n        self._recurrent_activation = recurrent_activation\n\n\n    def state_info(self, batch_size=0):\n        return [{'shape': (batch_size, self._hidden_size), '__layout__': 'NC'},\n                {'shape': (batch_size, self._hidden_size), '__layout__': 'NC'}]\n\n    def _alias(self):\n        return 'lstm'\n\n    def __repr__(self):\n        s = '{name}({mapping})'\n        shape = self.i2h_weight.shape\n        mapping = '{0} -> {1}'.format(shape[1] if shape[1] else None, shape[0])\n        return s.format(name=self.__class__.__name__,\n                        mapping=mapping,\n                        **self.__dict__)\n\n    def hybrid_forward(self, F, inputs, states, i2h_weight,\n                       h2h_weight, i2h_bias, h2h_bias):\n        # pylint: disable=too-many-locals\n        prefix = 't%d_'%self._counter\n        i2h = F.FullyConnected(data=inputs, weight=i2h_weight, bias=i2h_bias,\n                               num_hidden=self._hidden_size*4, name=prefix+'i2h')\n        h2h = F.FullyConnected(data=states[0], weight=h2h_weight, bias=h2h_bias,\n                               num_hidden=self._hidden_size*4, name=prefix+'h2h')\n        gates = F.elemwise_add(i2h, h2h, name=prefix+'plus0')\n        slice_gates = F.SliceChannel(gates, num_outputs=4, name=prefix+'slice')\n        in_gate = self._get_activation(\n            F, slice_gates[0], self._recurrent_activation, name=prefix+'i')\n        forget_gate = self._get_activation(\n            F, slice_gates[1], self._recurrent_activation, name=prefix+'f')\n        in_transform = self._get_activation(\n            F, slice_gates[2], self._activation, name=prefix+'c')\n        out_gate = self._get_activation(\n            F, slice_gates[3], self._recurrent_activation, name=prefix+'o')\n        next_c = F.elemwise_add(F.elemwise_mul(forget_gate, states[1], name=prefix+'mul0'),\n                                F.elemwise_mul(in_gate, in_transform, name=prefix+'mul1'),\n                                name=prefix+'state')\n        next_h = F.elemwise_mul(out_gate, F.Activation(next_c, act_type=self._activation, name=prefix+'activation0'),\n                                name=prefix+'out')\n\n        return next_h, [next_h, next_c]\n",
  "sig": "LSTMCell(hidden_size|Number of units in output symbol)(i2h_weight_initializer|Initializer for the input weights matrix, used for the linear transformation of the inputs)(h2h_weight_initializer| Initializer for the recurrent weights matrix, used for the linear transformation of the recurrent state)(i2h_bias_initializer|Initializer for the bias vector)(h2h_bias_initializer|Initializer for the bias vector)(prefix|Prefix for name of Block`s (and name of weight if params is `None))(params|Container for weight sharing between cells. Created if None)(activation|Activation type to use. See nd/symbol Activation for supported types)(recurrent_activation|Activation type to use for the recurrent step. See nd/symbol Activation for supported types)(Inputs|data: input tensor with shape (batch_size, input_size)states: a list of two initial recurrent state tensors. Each has shape (batch_size, num_hidden))(Outputs|out: output tensor with shape (batch_size, num_hidden)next_states: a list of two output recurrent state tensors. Each has the same shape as states)",
  "dst": "Long-Short Term Memory (LSTM) network cell.    Each call computes the following function:    .. math::        \\begin{array}{ll}        i_t = sigmoid(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\\\        f_t = sigmoid(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\\\        g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hc} h_{(t-1)} + b_{hg}) \\\\        o_t = sigmoid(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\\\        c_t = f_t * c_{(t-1)} + i_t * g_t \\\\        h_t = o_t * \\tanh(c_t)        \\end{array}    where :math:`h_t` is the hidden state at time `t`, :math:`c_t` is the    cell state at time `t`, :math:`x_t` is the hidden state of the previous    layer at time `t` or :math:`input_t` for the first layer, and :math:`i_t`,    :math:`f_t`, :math:`g_t`, :math:`o_t` are the input, forget, cell, and    out gates, respectively.    Parameters    ----------    hidden_size : int        Number of units in output symbol.    i2h_weight_initializer : str or Initializer        Initializer for the input weights matrix, used for the linear        transformation of the inputs.    h2h_weight_initializer : str or Initializer        Initializer for the recurrent weights matrix, used for the linear        transformation of the recurrent state.    i2h_bias_initializer : str or Initializer, default 'zeros'        Initializer for the bias vector.    h2h_bias_initializer : str or Initializer, default 'zeros'        Initializer for the bias vector.    prefix : str, default ``'lstm_'``        Prefix for name of `Block`s        (and name of weight if params is `None`).    params : Parameter or None, default None        Container for weight sharing between cells.        Created if `None`.    activation : str, default 'tanh'        Activation type to use. See nd/symbol Activation        for supported types.    recurrent_activation : str, default 'sigmoid'        Activation type to use for the recurrent step. See nd/symbol Activation        for supported types.    Inputs:        - **data**: input tensor with shape `(batch_size, input_size)`.        - **states**: a list of two initial recurrent state tensors. Each has shape          `(batch_size, num_hidden)`.    Outputs:        - **out**: output tensor with shape `(batch_size, num_hidden)`.        - **next_states**: a list of two output recurrent state tensors. Each has          the same shape as `states`."
 },
 "mxnet.gluon.nn.AvgPool1D": {
  "code": "class AvgPool1D(_Pooling):\n    \"\"\"Average pooling operation for temporal data.\n\n    Parameters\n    ----------\n    pool_size: int\n        Size of the average pooling windows.\n    strides: int, or None\n        Factor by which to downscale. E.g. 2 will halve the input size.\n        If `None`, it will default to `pool_size`.\n    padding: int\n        If padding is non-zero, then the input is implicitly\n        zero-padded on both sides for padding number of points.\n    layout : str, default 'NCW'\n        Dimension ordering of data and out ('NCW' or 'NWC').\n        'N', 'C', 'W' stands for batch, channel, and width (time) dimensions\n        respectively. padding is applied on 'W' dimension.\n    ceil_mode : bool, default False\n        When `True`, will use ceil instead of floor to compute the output shape.\n    count_include_pad : bool, default True\n        When 'False', will exclude padding elements when computing the average value.\n\n\n    Inputs:\n        - **data**: 3D input tensor with shape `(batch_size, in_channels, width)`\n          when `layout` is `NCW`. For other layouts shape is permuted accordingly.\n\n    Outputs:\n        - **out**: 3D output tensor with shape `(batch_size, channels, out_width)`\n          when `layout` is `NCW`. out_width is calculated as::\n\n              out_width = floor((width+2*padding-pool_size)/strides)+1\n\n          When `ceil_mode` is `True`, ceil will be used instead of floor in this\n          equation.\n    \"\"\"\n    def __init__(self, pool_size=2, strides=None, padding=0, layout='NCW',\n                 ceil_mode=False, count_include_pad=True, **kwargs):\n        assert layout in ('NCW', 'NWC'),\\\n            \"Only NCW and NWC layouts are valid for 1D Pooling\"\n        if isinstance(pool_size, numeric_types):\n            pool_size = (pool_size,)\n        assert len(pool_size) == 1, \"pool_size must be a number or a list of 1 ints\"\n        super(AvgPool1D, self).__init__(\n            pool_size, strides, padding, ceil_mode, False, 'avg', layout, count_include_pad,\n            **kwargs)\n",
  "sig": "AvgPool1D(pool_size|Size of the average pooling windows)(strides|Factor by which to downscale. E.g. 2 will halve the input size. If None, it will default to pool_size)(padding|If padding is non-zero, then the input is implicitly zero-padded on both sides for padding number of points)(layout|Dimension ordering of data and out (‘NCW’ or ‘NWC’). ‘N’, ‘C’, ‘W’ stands for batch, channel, and width (time) dimensions respectively. padding is applied on ‘W’ dimension)(ceil_mode|When True, will use ceil instead of floor to compute the output shape)(count_include_pad|When ‘False’, will exclude padding elements when computing the average value)",
  "dst": "Average pooling operation for temporal data.    Parameters    ----------    pool_size: int        Size of the average pooling windows.    strides: int, or None        Factor by which to downscale. E.g. 2 will halve the input size.        If `None`, it will default to `pool_size`.    padding: int        If padding is non-zero, then the input is implicitly        zero-padded on both sides for padding number of points.    layout : str, default 'NCW'        Dimension ordering of data and out ('NCW' or 'NWC').        'N', 'C', 'W' stands for batch, channel, and width (time) dimensions        respectively. padding is applied on 'W' dimension.    ceil_mode : bool, default False        When `True`, will use ceil instead of floor to compute the output shape.    count_include_pad : bool, default True        When 'False', will exclude padding elements when computing the average value.    Inputs:        - **data**: 3D input tensor with shape `(batch_size, in_channels, width)`          when `layout` is `NCW`. For other layouts shape is permuted accordingly.    Outputs:        - **out**: 3D output tensor with shape `(batch_size, channels, out_width)`          when `layout` is `NCW`. out_width is calculated as::              out_width = floor((width+2*padding-pool_size)/strides)+1          When `ceil_mode` is `True`, ceil will be used instead of floor in this          equation."
 },
 "mxnet.gluon.nn.AvgPool2D": {
  "code": "class AvgPool2D(_Pooling):\n    \"\"\"Average pooling operation for spatial data.\n\n    Parameters\n    ----------\n    pool_size: int or list/tuple of 2 ints,\n        Size of the average pooling windows.\n    strides: int, list/tuple of 2 ints, or None.\n        Factor by which to downscale. E.g. 2 will halve the input size.\n        If `None`, it will default to `pool_size`.\n    padding: int or list/tuple of 2 ints,\n        If padding is non-zero, then the input is implicitly\n        zero-padded on both sides for padding number of points.\n    layout : str, default 'NCHW'\n        Dimension ordering of data and out ('NCHW' or 'NHWC').\n        'N', 'C', 'H', 'W' stands for batch, channel, height, and width\n        dimensions respectively. padding is applied on 'H' and 'W' dimension.\n    ceil_mode : bool, default False\n        When True, will use ceil instead of floor to compute the output shape.\n    count_include_pad : bool, default True\n        When 'False', will exclude padding elements when computing the average value.\n\n\n    Inputs:\n        - **data**: 4D input tensor with shape\n          `(batch_size, in_channels, height, width)` when `layout` is `NCHW`.\n          For other layouts shape is permuted accordingly.\n\n    Outputs:\n        - **out**: 4D output tensor with shape\n          `(batch_size, channels, out_height, out_width)` when `layout` is `NCHW`.\n          out_height and out_width are calculated as::\n\n              out_height = floor((height+2*padding[0]-pool_size[0])/strides[0])+1\n              out_width = floor((width+2*padding[1]-pool_size[1])/strides[1])+1\n\n          When `ceil_mode` is `True`, ceil will be used instead of floor in this\n          equation.\n    \"\"\"\n    def __init__(self, pool_size=(2, 2), strides=None, padding=0,\n                 ceil_mode=False, layout='NCHW', count_include_pad=True, **kwargs):\n        assert layout in ('NCHW', 'NHWC'),\\\n            \"Only NCHW and NHWC layouts are valid for 2D Pooling\"\n        if isinstance(pool_size, numeric_types):\n            pool_size = (pool_size,)*2\n        assert len(pool_size) == 2, \"pool_size must be a number or a list of 2 ints\"\n        super(AvgPool2D, self).__init__(\n            pool_size, strides, padding, ceil_mode, False, 'avg', layout, count_include_pad,\n            **kwargs)\n",
  "sig": "AvgPool2D(pool_size|Size of the average pooling windows)(strides|Factor by which to downscale. E.g. 2 will halve the input size. If None, it will default to pool_size)(padding|If padding is non-zero, then the input is implicitly zero-padded on both sides for padding number of points)(layout|Dimension ordering of data and out (‘NCW’ or ‘NWC’). ‘N’, ‘C’, ‘W’ stands for batch, channel, and width (time) dimensions respectively. padding is applied on ‘W’ dimension)(ceil_mode|When True, will use ceil instead of floor to compute the output shape)(count_include_pad|When ‘False’, will exclude padding elements when computing the average value)",
  "dst": "Average pooling operation for spatial data.    Parameters    ----------    pool_size: int or list/tuple of 2 ints,        Size of the average pooling windows.    strides: int, list/tuple of 2 ints, or None.        Factor by which to downscale. E.g. 2 will halve the input size.        If `None`, it will default to `pool_size`.    padding: int or list/tuple of 2 ints,        If padding is non-zero, then the input is implicitly        zero-padded on both sides for padding number of points.    layout : str, default 'NCHW'        Dimension ordering of data and out ('NCHW' or 'NHWC').        'N', 'C', 'H', 'W' stands for batch, channel, height, and width        dimensions respectively. padding is applied on 'H' and 'W' dimension.    ceil_mode : bool, default False        When True, will use ceil instead of floor to compute the output shape.    count_include_pad : bool, default True        When 'False', will exclude padding elements when computing the average value.    Inputs:        - **data**: 4D input tensor with shape          `(batch_size, in_channels, height, width)` when `layout` is `NCHW`.          For other layouts shape is permuted accordingly.    Outputs:        - **out**: 4D output tensor with shape          `(batch_size, channels, out_height, out_width)` when `layout` is `NCHW`.          out_height and out_width are calculated as::              out_height = floor((height+2*padding[0]-pool_size[0])/strides[0])+1              out_width = floor((width+2*padding[1]-pool_size[1])/strides[1])+1          When `ceil_mode` is `True`, ceil will be used instead of floor in this          equation."
 },
 "mxnet.gluon.nn.AvgPool3D": {
  "code": "class AvgPool3D(_Pooling):\n    \"\"\"Average pooling operation for 3D data (spatial or spatio-temporal).\n\n    Parameters\n    ----------\n    pool_size: int or list/tuple of 3 ints,\n        Size of the average pooling windows.\n    strides: int, list/tuple of 3 ints, or None.\n        Factor by which to downscale. E.g. 2 will halve the input size.\n        If `None`, it will default to `pool_size`.\n    padding: int or list/tuple of 3 ints,\n        If padding is non-zero, then the input is implicitly\n        zero-padded on both sides for padding number of points.\n    layout : str, default 'NCDHW'\n        Dimension ordering of data and out ('NCDHW' or 'NDHWC').\n        'N', 'C', 'H', 'W', 'D' stands for batch, channel, height, width and\n        depth dimensions respectively. padding is applied on 'D', 'H' and 'W'\n        dimension.\n    ceil_mode : bool, default False\n        When True, will use ceil instead of floor to compute the output shape.\n    count_include_pad : bool, default True\n        When 'False', will exclude padding elements when computing the average value.\n\n\n    Inputs:\n        - **data**: 5D input tensor with shape\n          `(batch_size, in_channels, depth, height, width)` when `layout` is `NCDHW`.\n          For other layouts shape is permuted accordingly.\n\n    Outputs:\n        - **out**: 5D output tensor with shape\n          `(batch_size, channels, out_depth, out_height, out_width)` when `layout` is `NCDHW`.\n          out_depth, out_height and out_width are calculated as::\n\n              out_depth = floor((depth+2*padding[0]-pool_size[0])/strides[0])+1\n              out_height = floor((height+2*padding[1]-pool_size[1])/strides[1])+1\n              out_width = floor((width+2*padding[2]-pool_size[2])/strides[2])+1\n\n          When `ceil_mode` is `True,` ceil will be used instead of floor in this\n          equation.\n    \"\"\"\n    def __init__(self, pool_size=(2, 2, 2), strides=None, padding=0,\n                 ceil_mode=False, layout='NCDHW', count_include_pad=True, **kwargs):\n        assert layout in ('NCDHW', 'NDHWC'),\\\n            \"Only NCDHW and NDHWC layouts are valid for 3D Pooling\"\n        if isinstance(pool_size, numeric_types):\n            pool_size = (pool_size,)*3\n        assert len(pool_size) == 3, \"pool_size must be a number or a list of 3 ints\"\n        super(AvgPool3D, self).__init__(\n            pool_size, strides, padding, ceil_mode, False, 'avg', layout, count_include_pad,\n            **kwargs)\n",
  "sig": "AvgPool3D(pool_size|Size of the average pooling windows)(strides|Factor by which to downscale. E.g. 2 will halve the input size. If None, it will default to pool_size)(padding|If padding is non-zero, then the input is implicitly zero-padded on both sides for padding number of points)(layout|Dimension ordering of data and out (‘NCW’ or ‘NWC’). ‘N’, ‘C’, ‘W’ stands for batch, channel, and width (time) dimensions respectively. padding is applied on ‘W’ dimension)(ceil_mode|When True, will use ceil instead of floor to compute the output shape)(count_include_pad|When ‘False’, will exclude padding elements when computing the average value)",
  "dst": "Average pooling operation for 3D data (spatial or spatio-temporal).    Parameters    ----------    pool_size: int or list/tuple of 3 ints,        Size of the average pooling windows.    strides: int, list/tuple of 3 ints, or None.        Factor by which to downscale. E.g. 2 will halve the input size.        If `None`, it will default to `pool_size`.    padding: int or list/tuple of 3 ints,        If padding is non-zero, then the input is implicitly        zero-padded on both sides for padding number of points.    layout : str, default 'NCDHW'        Dimension ordering of data and out ('NCDHW' or 'NDHWC').        'N', 'C', 'H', 'W', 'D' stands for batch, channel, height, width and        depth dimensions respectively. padding is applied on 'D', 'H' and 'W'        dimension.    ceil_mode : bool, default False        When True, will use ceil instead of floor to compute the output shape.    count_include_pad : bool, default True        When 'False', will exclude padding elements when computing the average value.    Inputs:        - **data**: 5D input tensor with shape          `(batch_size, in_channels, depth, height, width)` when `layout` is `NCDHW`.          For other layouts shape is permuted accordingly.    Outputs:        - **out**: 5D output tensor with shape          `(batch_size, channels, out_depth, out_height, out_width)` when `layout` is `NCDHW`.          out_depth, out_height and out_width are calculated as::              out_depth = floor((depth+2*padding[0]-pool_size[0])/strides[0])+1              out_height = floor((height+2*padding[1]-pool_size[1])/strides[1])+1              out_width = floor((width+2*padding[2]-pool_size[2])/strides[2])+1          When `ceil_mode` is `True,` ceil will be used instead of floor in this          equation."
 },
 "mxnet.gluon.nn.Conv1D": {
  "code": "class Conv1D(_Conv):\n    r\"\"\"1D convolution layer (e.g. temporal convolution).\n\n    This layer creates a convolution kernel that is convolved\n    with the layer input over a single spatial (or temporal) dimension\n    to produce a tensor of outputs.\n    If `use_bias` is True, a bias vector is created and added to the outputs.\n    Finally, if `activation` is not `None`,\n    it is applied to the outputs as well.\n\n    If `in_channels` is not specified, `Parameter` initialization will be\n    deferred to the first time `forward` is called and `in_channels` will be\n    inferred from the shape of input data.\n\n\n    Parameters\n    ----------\n    channels : int\n        The dimensionality of the output space, i.e. the number of output\n        channels (filters) in the convolution.\n    kernel_size :int or tuple/list of 1 int\n        Specifies the dimensions of the convolution window.\n    strides : int or tuple/list of 1 int,\n        Specify the strides of the convolution.\n    padding : int or a tuple/list of 1 int,\n        If padding is non-zero, then the input is implicitly zero-padded\n        on both sides for padding number of points\n    dilation : int or tuple/list of 1 int\n        Specifies the dilation rate to use for dilated convolution.\n    groups : int\n        Controls the connections between inputs and outputs.\n        At groups=1, all inputs are convolved to all outputs.\n        At groups=2, the operation becomes equivalent to having two conv\n        layers side by side, each seeing half the input channels, and producing\n        half the output channels, and both subsequently concatenated.\n    layout: str, default 'NCW'\n        Dimension ordering of data and weight. Only supports 'NCW' layout for now.\n        'N', 'C', 'W' stands for batch, channel, and width (time) dimensions\n        respectively. Convolution is applied on the 'W' dimension.\n    in_channels : int, default 0\n        The number of input channels to this layer. If not specified,\n        initialization will be deferred to the first time `forward` is called\n        and `in_channels` will be inferred from the shape of input data.\n    activation : str\n        Activation function to use. See :func:`~mxnet.ndarray.Activation`.\n        If you don't specify anything, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).\n    use_bias : bool\n        Whether the layer uses a bias vector.\n    weight_initializer : str or `Initializer`\n        Initializer for the `weight` weights matrix.\n    bias_initializer : str or `Initializer`\n        Initializer for the bias vector.\n\n\n    Inputs:\n        - **data**: 3D input tensor with shape `(batch_size, in_channels, width)`\n          when `layout` is `NCW`. For other layouts shape is permuted accordingly.\n\n    Outputs:\n        - **out**: 3D output tensor with shape `(batch_size, channels, out_width)`\n          when `layout` is `NCW`. out_width is calculated as::\n\n              out_width = floor((width+2*padding-dilation*(kernel_size-1)-1)/stride)+1\n    \"\"\"\n    def __init__(self, channels, kernel_size, strides=1, padding=0, dilation=1,\n                 groups=1, layout='NCW', activation=None, use_bias=True,\n                 weight_initializer=None, bias_initializer='zeros',\n                 in_channels=0, **kwargs):\n        assert layout == 'NCW', \"Only supports 'NCW' layout for now\"\n        if isinstance(kernel_size, numeric_types):\n            kernel_size = (kernel_size,)\n        assert len(kernel_size) == 1, \"kernel_size must be a number or a list of 1 ints\"\n        op_name = kwargs.pop('op_name', 'Convolution')\n        if is_np_array():\n            op_name = 'convolution'\n        super(Conv1D, self).__init__(\n            channels, kernel_size, strides, padding, dilation, groups, layout,\n            in_channels, activation, use_bias, weight_initializer, bias_initializer,\n            op_name, **kwargs)\n",
  "sig": "Conv1D(channels|The dimensionality of the output space, i.e. the number of output channels (filters) in the convolution)(kernel_size|Specifies the dimensions of the convolution window)(strides|Specify the strides of the convolution)(padding|If padding is non-zero, then the input is implicitly zero-padded on both sides for padding number of points)(dilation| Specifies the dilation rate to use for dilated convolution)(groups|Controls the connections between inputs and outputs. At groups=1, all inputs are convolved to all outputs. At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels, and producing half the output channels, and both subsequently concatenated)(layout|Dimension ordering of data and weight. Only supports ‘NCW’ layout for now. ‘N’, ‘C’, ‘W’ stands for batch, channel, and width (time) dimensions respectively. Convolution is applied on the ‘W’ dimension)(in_channels|The number of input channels to this layer. If not specified, initialization will be deferred to the first time forward is called and in_channels will be inferred from the shape of input data)(activation|Activation function to use. See Activation(). If you don’t specify anything, no activation is applied (ie. “linear” activation: a(x) = x))(use_bias|Whether the layer uses a bias vector)(weight_initializer|Initializer for the weight weights matrix)(bias_initializer|Initializer for the bias vector)",
  "dst": "1D convolution layer (e.g. temporal convolution).    This layer creates a convolution kernel that is convolved    with the layer input over a single spatial (or temporal) dimension    to produce a tensor of outputs.    If `use_bias` is True, a bias vector is created and added to the outputs.    Finally, if `activation` is not `None`,    it is applied to the outputs as well.    If `in_channels` is not specified, `Parameter` initialization will be    deferred to the first time `forward` is called and `in_channels` will be    inferred from the shape of input data.    Parameters    ----------    channels : int        The dimensionality of the output space, i.e. the number of output        channels (filters) in the convolution.    kernel_size :int or tuple/list of 1 int        Specifies the dimensions of the convolution window.    strides : int or tuple/list of 1 int,        Specify the strides of the convolution.    padding : int or a tuple/list of 1 int,        If padding is non-zero, then the input is implicitly zero-padded        on both sides for padding number of points    dilation : int or tuple/list of 1 int        Specifies the dilation rate to use for dilated convolution.    groups : int        Controls the connections between inputs and outputs.        At groups=1, all inputs are convolved to all outputs.        At groups=2, the operation becomes equivalent to having two conv        layers side by side, each seeing half the input channels, and producing        half the output channels, and both subsequently concatenated.    layout: str, default 'NCW'        Dimension ordering of data and weight. Only supports 'NCW' layout for now.        'N', 'C', 'W' stands for batch, channel, and width (time) dimensions        respectively. Convolution is applied on the 'W' dimension.    in_channels : int, default 0        The number of input channels to this layer. If not specified,        initialization will be deferred to the first time `forward` is called        and `in_channels` will be inferred from the shape of input data.    activation : str        Activation function to use. See :func:`~mxnet.ndarray.Activation`.        If you don't specify anything, no activation is applied        (ie. \"linear\" activation: `a(x) = x`).    use_bias : bool        Whether the layer uses a bias vector.    weight_initializer : str or `Initializer`        Initializer for the `weight` weights matrix.    bias_initializer : str or `Initializer`        Initializer for the bias vector.    Inputs:        - **data**: 3D input tensor with shape `(batch_size, in_channels, width)`          when `layout` is `NCW`. For other layouts shape is permuted accordingly.    Outputs:        - **out**: 3D output tensor with shape `(batch_size, channels, out_width)`          when `layout` is `NCW`. out_width is calculated as::              out_width = floor((width+2*padding-dilation*(kernel_size-1)-1)/stride)+1"
 },
 "mxnet.gluon.nn.Conv2D": {
  "code": "class Conv2D(_Conv):\n    r\"\"\"2D convolution layer (e.g. spatial convolution over images).\n\n    This layer creates a convolution kernel that is convolved\n    with the layer input to produce a tensor of\n    outputs. If `use_bias` is True,\n    a bias vector is created and added to the outputs. Finally, if\n    `activation` is not `None`, it is applied to the outputs as well.\n\n    If `in_channels` is not specified, `Parameter` initialization will be\n    deferred to the first time `forward` is called and `in_channels` will be\n    inferred from the shape of input data.\n\n    Parameters\n    ----------\n    channels : int\n        The dimensionality of the output space, i.e. the number of output\n        channels (filters) in the convolution.\n    kernel_size :int or tuple/list of 2 int\n        Specifies the dimensions of the convolution window.\n    strides : int or tuple/list of 2 int,\n        Specify the strides of the convolution.\n    padding : int or a tuple/list of 2 int,\n        If padding is non-zero, then the input is implicitly zero-padded\n        on both sides for padding number of points\n    dilation : int or tuple/list of 2 int\n        Specifies the dilation rate to use for dilated convolution.\n    groups : int\n        Controls the connections between inputs and outputs.\n        At groups=1, all inputs are convolved to all outputs.\n        At groups=2, the operation becomes equivalent to having two conv\n        layers side by side, each seeing half the input channels, and producing\n        half the output channels, and both subsequently concatenated.\n    layout : str, default 'NCHW'\n        Dimension ordering of data and weight. Only supports 'NCHW' and 'NHWC'\n        layout for now. 'N', 'C', 'H', 'W' stands for batch, channel, height,\n        and width dimensions respectively. Convolution is applied on the 'H' and\n        'W' dimensions.\n    in_channels : int, default 0\n        The number of input channels to this layer. If not specified,\n        initialization will be deferred to the first time `forward` is called\n        and `in_channels` will be inferred from the shape of input data.\n    activation : str\n        Activation function to use. See :func:`~mxnet.ndarray.Activation`.\n        If you don't specify anything, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).\n    use_bias : bool\n        Whether the layer uses a bias vector.\n    weight_initializer : str or `Initializer`\n        Initializer for the `weight` weights matrix.\n    bias_initializer : str or `Initializer`\n        Initializer for the bias vector.\n\n\n    Inputs:\n        - **data**: 4D input tensor with shape\n          `(batch_size, in_channels, height, width)` when `layout` is `NCHW`.\n          For other layouts shape is permuted accordingly.\n\n    Outputs:\n        - **out**: 4D output tensor with shape\n          `(batch_size, channels, out_height, out_width)` when `layout` is `NCHW`.\n          out_height and out_width are calculated as::\n\n              out_height = floor((height+2*padding[0]-dilation[0]*(kernel_size[0]-1)-1)/stride[0])+1\n              out_width = floor((width+2*padding[1]-dilation[1]*(kernel_size[1]-1)-1)/stride[1])+1\n    \"\"\"\n    def __init__(self, channels, kernel_size, strides=(1, 1), padding=(0, 0),\n                 dilation=(1, 1), groups=1, layout='NCHW',\n                 activation=None, use_bias=True, weight_initializer=None,\n                 bias_initializer='zeros', in_channels=0, **kwargs):\n        assert layout in ('NCHW', 'NHWC'), \"Only supports 'NCHW' and 'NHWC' layout for now\"\n        if isinstance(kernel_size, numeric_types):\n            kernel_size = (kernel_size,)*2\n        assert len(kernel_size) == 2, \"kernel_size must be a number or a list of 2 ints\"\n        op_name = kwargs.pop('op_name', 'Convolution')\n        if is_np_array():\n            op_name = 'convolution'\n        super(Conv2D, self).__init__(\n            channels, kernel_size, strides, padding, dilation, groups, layout,\n            in_channels, activation, use_bias, weight_initializer, bias_initializer,\n            op_name, **kwargs)\n",
  "sig": "Conv2D(channels|The dimensionality of the output space, i.e. the number of output channels (filters) in the convolution)(kernel_size|Specifies the dimensions of the convolution window)(strides|Specify the strides of the convolution)(padding|If padding is non-zero, then the input is implicitly zero-padded on both sides for padding number of points)(dilation| Specifies the dilation rate to use for dilated convolution)(groups|Controls the connections between inputs and outputs. At groups=1, all inputs are convolved to all outputs. At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels, and producing half the output channels, and both subsequently concatenated)(layout|Dimension ordering of data and weight. Only supports ‘NCW’ layout for now. ‘N’, ‘C’, ‘W’ stands for batch, channel, and width (time) dimensions respectively. Convolution is applied on the ‘W’ dimension)(in_channels|The number of input channels to this layer. If not specified, initialization will be deferred to the first time forward is called and in_channels will be inferred from the shape of input data)(activation|Activation function to use. See Activation(). If you don’t specify anything, no activation is applied (ie. “linear” activation: a(x) = x))(use_bias|Whether the layer uses a bias vector)(weight_initializer|Initializer for the weight weights matrix)(bias_initializer|Initializer for the bias vector)",
  "dst": "2D convolution layer (e.g. spatial convolution over images).    This layer creates a convolution kernel that is convolved    with the layer input to produce a tensor of    outputs. If `use_bias` is True,    a bias vector is created and added to the outputs. Finally, if    `activation` is not `None`, it is applied to the outputs as well.    If `in_channels` is not specified, `Parameter` initialization will be    deferred to the first time `forward` is called and `in_channels` will be    inferred from the shape of input data.    Parameters    ----------    channels : int        The dimensionality of the output space, i.e. the number of output        channels (filters) in the convolution.    kernel_size :int or tuple/list of 2 int        Specifies the dimensions of the convolution window.    strides : int or tuple/list of 2 int,        Specify the strides of the convolution.    padding : int or a tuple/list of 2 int,        If padding is non-zero, then the input is implicitly zero-padded        on both sides for padding number of points    dilation : int or tuple/list of 2 int        Specifies the dilation rate to use for dilated convolution.    groups : int        Controls the connections between inputs and outputs.        At groups=1, all inputs are convolved to all outputs.        At groups=2, the operation becomes equivalent to having two conv        layers side by side, each seeing half the input channels, and producing        half the output channels, and both subsequently concatenated.    layout : str, default 'NCHW'        Dimension ordering of data and weight. Only supports 'NCHW' and 'NHWC'        layout for now. 'N', 'C', 'H', 'W' stands for batch, channel, height,        and width dimensions respectively. Convolution is applied on the 'H' and        'W' dimensions.    in_channels : int, default 0        The number of input channels to this layer. If not specified,        initialization will be deferred to the first time `forward` is called        and `in_channels` will be inferred from the shape of input data.    activation : str        Activation function to use. See :func:`~mxnet.ndarray.Activation`.        If you don't specify anything, no activation is applied        (ie. \"linear\" activation: `a(x) = x`).    use_bias : bool        Whether the layer uses a bias vector.    weight_initializer : str or `Initializer`        Initializer for the `weight` weights matrix.    bias_initializer : str or `Initializer`        Initializer for the bias vector.    Inputs:        - **data**: 4D input tensor with shape          `(batch_size, in_channels, height, width)` when `layout` is `NCHW`.          For other layouts shape is permuted accordingly.    Outputs:        - **out**: 4D output tensor with shape          `(batch_size, channels, out_height, out_width)` when `layout` is `NCHW`.          out_height and out_width are calculated as::              out_height = floor((height+2*padding[0]-dilation[0]*(kernel_size[0]-1)-1)/stride[0])+1              out_width = floor((width+2*padding[1]-dilation[1]*(kernel_size[1]-1)-1)/stride[1])+1"
 },
 "mxnet.gluon.nn.Conv2DTranspose": {
  "code": "class Conv2DTranspose(_Conv):\n    \"\"\"Transposed 2D convolution layer (sometimes called Deconvolution).\n\n    The need for transposed convolutions generally arises\n    from the desire to use a transformation going in the opposite direction\n    of a normal convolution, i.e., from something that has the shape of the\n    output of some convolution to something that has the shape of its input\n    while maintaining a connectivity pattern that is compatible with\n    said convolution.\n\n    If `in_channels` is not specified, `Parameter` initialization will be\n    deferred to the first time `forward` is called and `in_channels` will be\n    inferred from the shape of input data.\n\n\n    Parameters\n    ----------\n    channels : int\n        The dimensionality of the output space, i.e. the number of output\n        channels (filters) in the convolution.\n    kernel_size :int or tuple/list of 2 int\n        Specifies the dimensions of the convolution window.\n    strides : int or tuple/list of 2 int\n        Specify the strides of the convolution.\n    padding : int or a tuple/list of 2 int,\n        If padding is non-zero, then the input is implicitly zero-padded\n        on both sides for padding number of points\n    output_padding: int or a tuple/list of 2 int\n        Controls the amount of implicit zero-paddings on both sides of the\n        output for output_padding number of points for each dimension.\n    dilation : int or tuple/list of 2 int\n        Controls the spacing between the kernel points; also known as the\n        a trous algorithm\n    groups : int\n        Controls the connections between inputs and outputs.\n        At groups=1, all inputs are convolved to all outputs.\n        At groups=2, the operation becomes equivalent to having two conv\n        layers side by side, each seeing half the input channels, and producing\n        half the output channels, and both subsequently concatenated.\n    layout : str, default 'NCHW'\n        Dimension ordering of data and weight. Only supports 'NCHW' and 'NHWC'\n        layout for now. 'N', 'C', 'H', 'W' stands for batch, channel, height,\n        and width dimensions respectively. Convolution is applied on the 'H' and\n        'W' dimensions.\n    in_channels : int, default 0\n        The number of input channels to this layer. If not specified,\n        initialization will be deferred to the first time `forward` is called\n        and `in_channels` will be inferred from the shape of input data.\n    activation : str\n        Activation function to use. See :func:`~mxnet.ndarray.Activation`.\n        If you don't specify anything, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).\n    use_bias : bool\n        Whether the layer uses a bias vector.\n    weight_initializer : str or `Initializer`\n        Initializer for the `weight` weights matrix.\n    bias_initializer : str or `Initializer`\n        Initializer for the bias vector.\n\n\n    Inputs:\n        - **data**: 4D input tensor with shape\n          `(batch_size, in_channels, height, width)` when `layout` is `NCHW`.\n          For other layouts shape is permuted accordingly.\n\n    Outputs:\n        - **out**: 4D output tensor with shape\n          `(batch_size, channels, out_height, out_width)` when `layout` is `NCHW`.\n          out_height and out_width are calculated as::\n\n              out_height = (height-1)*strides[0]-2*padding[0]+kernel_size[0]+output_padding[0]\n              out_width = (width-1)*strides[1]-2*padding[1]+kernel_size[1]+output_padding[1]\n    \"\"\"\n    def __init__(self, channels, kernel_size, strides=(1, 1), padding=(0, 0),\n                 output_padding=(0, 0), dilation=(1, 1), groups=1, layout='NCHW',\n                 activation=None, use_bias=True, weight_initializer=None,\n                 bias_initializer='zeros', in_channels=0, **kwargs):\n        assert layout in ('NCHW', 'NHWC'), \"Only supports 'NCHW' and 'NHWC' layout for now\"\n        if isinstance(kernel_size, numeric_types):\n            kernel_size = (kernel_size,)*2\n        if isinstance(output_padding, numeric_types):\n            output_padding = (output_padding,)*2\n        assert len(kernel_size) == 2, \"kernel_size must be a number or a list of 2 ints\"\n        assert len(output_padding) == 2, \"output_padding must be a number or a list of 2 ints\"\n        op_name = kwargs.pop('op_name', 'Deconvolution')\n        if is_np_array():\n            op_name = 'deconvolution'\n        super(Conv2DTranspose, self).__init__(\n            channels, kernel_size, strides, padding, dilation, groups, layout,\n            in_channels, activation, use_bias, weight_initializer,\n            bias_initializer, op_name=op_name, adj=output_padding, **kwargs)\n        self.outpad = output_padding\n",
  "sig": "Conv2DTranspose(channels|The dimensionality of the output space, i.e. the number of output channels (filters) in the convolution)(kernel_size|Specifies the dimensions of the convolution window)(strides|Specify the strides of the convolution)(padding|If padding is non-zero, then the input is implicitly zero-padded on both sides for padding number of points)(output_padding|Controls the amount of implicit zero-paddings on both sides of the output for output_padding number of points for each dimension)(dilation| Specifies the dilation rate to use for dilated convolution)(groups|Controls the connections between inputs and outputs. At groups=1, all inputs are convolved to all outputs. At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels, and producing half the output channels, and both subsequently concatenated)(layout|Dimension ordering of data and weight. Only supports ‘NCW’ layout for now. ‘N’, ‘C’, ‘W’ stands for batch, channel, and width (time) dimensions respectively. Convolution is applied on the ‘W’ dimension)(in_channels|The number of input channels to this layer. If not specified, initialization will be deferred to the first time forward is called and in_channels will be inferred from the shape of input data)(activation|Activation function to use. See Activation(). If you don’t specify anything, no activation is applied (ie. “linear” activation: a(x) = x))(use_bias|Whether the layer uses a bias vector)(weight_initializer|Initializer for the weight weights matrix)(bias_initializer|Initializer for the bias vector)",
  "dst": "Transposed 2D convolution layer (sometimes called Deconvolution).    The need for transposed convolutions generally arises    from the desire to use a transformation going in the opposite direction    of a normal convolution, i.e., from something that has the shape of the    output of some convolution to something that has the shape of its input    while maintaining a connectivity pattern that is compatible with    said convolution.    If `in_channels` is not specified, `Parameter` initialization will be    deferred to the first time `forward` is called and `in_channels` will be    inferred from the shape of input data.    Parameters    ----------    channels : int        The dimensionality of the output space, i.e. the number of output        channels (filters) in the convolution.    kernel_size :int or tuple/list of 2 int        Specifies the dimensions of the convolution window.    strides : int or tuple/list of 2 int        Specify the strides of the convolution.    padding : int or a tuple/list of 2 int,        If padding is non-zero, then the input is implicitly zero-padded        on both sides for padding number of points    output_padding: int or a tuple/list of 2 int        Controls the amount of implicit zero-paddings on both sides of the        output for output_padding number of points for each dimension.    dilation : int or tuple/list of 2 int        Controls the spacing between the kernel points; also known as the        a trous algorithm    groups : int        Controls the connections between inputs and outputs.        At groups=1, all inputs are convolved to all outputs.        At groups=2, the operation becomes equivalent to having two conv        layers side by side, each seeing half the input channels, and producing        half the output channels, and both subsequently concatenated.    layout : str, default 'NCHW'        Dimension ordering of data and weight. Only supports 'NCHW' and 'NHWC'        layout for now. 'N', 'C', 'H', 'W' stands for batch, channel, height,        and width dimensions respectively. Convolution is applied on the 'H' and        'W' dimensions.    in_channels : int, default 0        The number of input channels to this layer. If not specified,        initialization will be deferred to the first time `forward` is called        and `in_channels` will be inferred from the shape of input data.    activation : str        Activation function to use. See :func:`~mxnet.ndarray.Activation`.        If you don't specify anything, no activation is applied        (ie. \"linear\" activation: `a(x) = x`).    use_bias : bool        Whether the layer uses a bias vector.    weight_initializer : str or `Initializer`        Initializer for the `weight` weights matrix.    bias_initializer : str or `Initializer`        Initializer for the bias vector.    Inputs:        - **data**: 4D input tensor with shape          `(batch_size, in_channels, height, width)` when `layout` is `NCHW`.          For other layouts shape is permuted accordingly.    Outputs:        - **out**: 4D output tensor with shape          `(batch_size, channels, out_height, out_width)` when `layout` is `NCHW`.          out_height and out_width are calculated as::              out_height = (height-1)*strides[0]-2*padding[0]+kernel_size[0]+output_padding[0]              out_width = (width-1)*strides[1]-2*padding[1]+kernel_size[1]+output_padding[1]"
 },
 "mxnet.gluon.nn.Conv3D": {
  "code": "class Conv3D(_Conv):\n    \"\"\"3D convolution layer (e.g. spatial convolution over volumes).\n\n    This layer creates a convolution kernel that is convolved\n    with the layer input to produce a tensor of\n    outputs. If `use_bias` is `True`,\n    a bias vector is created and added to the outputs. Finally, if\n    `activation` is not `None`, it is applied to the outputs as well.\n\n    If `in_channels` is not specified, `Parameter` initialization will be\n    deferred to the first time `forward` is called and `in_channels` will be\n    inferred from the shape of input data.\n\n    Parameters\n    ----------\n    channels : int\n        The dimensionality of the output space, i.e. the number of output\n        channels (filters) in the convolution.\n    kernel_size :int or tuple/list of 3 int\n        Specifies the dimensions of the convolution window.\n    strides : int or tuple/list of 3 int,\n        Specify the strides of the convolution.\n    padding : int or a tuple/list of 3 int,\n        If padding is non-zero, then the input is implicitly zero-padded\n        on both sides for padding number of points\n    dilation : int or tuple/list of 3 int\n        Specifies the dilation rate to use for dilated convolution.\n    groups : int\n        Controls the connections between inputs and outputs.\n        At groups=1, all inputs are convolved to all outputs.\n        At groups=2, the operation becomes equivalent to having two conv\n        layers side by side, each seeing half the input channels, and producing\n        half the output channels, and both subsequently concatenated.\n    layout : str, default 'NCDHW'\n        Dimension ordering of data and weight. Only supports 'NCDHW' and 'NDHWC'\n        layout for now. 'N', 'C', 'H', 'W', 'D' stands for batch, channel, height,\n        width and depth dimensions respectively. Convolution is applied on the 'D',\n        'H' and 'W' dimensions.\n    in_channels : int, default 0\n        The number of input channels to this layer. If not specified,\n        initialization will be deferred to the first time `forward` is called\n        and `in_channels` will be inferred from the shape of input data.\n    activation : str\n        Activation function to use. See :func:`~mxnet.ndarray.Activation`.\n        If you don't specify anything, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).\n    use_bias : bool\n        Whether the layer uses a bias vector.\n    weight_initializer : str or `Initializer`\n        Initializer for the `weight` weights matrix.\n    bias_initializer : str or `Initializer`\n        Initializer for the bias vector.\n\n\n    Inputs:\n        - **data**: 5D input tensor with shape\n          `(batch_size, in_channels, depth, height, width)` when `layout` is `NCDHW`.\n          For other layouts shape is permuted accordingly.\n\n    Outputs:\n        - **out**: 5D output tensor with shape\n          `(batch_size, channels, out_depth, out_height, out_width)` when `layout` is `NCDHW`.\n          out_depth, out_height and out_width are calculated as::\n\n              out_depth = floor((depth+2*padding[0]-dilation[0]*(kernel_size[0]-1)-1)/stride[0])+1\n              out_height = floor((height+2*padding[1]-dilation[1]*(kernel_size[1]-1)-1)/stride[1])+1\n              out_width = floor((width+2*padding[2]-dilation[2]*(kernel_size[2]-1)-1)/stride[2])+1\n    \"\"\"\n    def __init__(self, channels, kernel_size, strides=(1, 1, 1), padding=(0, 0, 0),\n                 dilation=(1, 1, 1), groups=1, layout='NCDHW', activation=None,\n                 use_bias=True, weight_initializer=None, bias_initializer='zeros',\n                 in_channels=0, **kwargs):\n        assert layout in ('NCDHW', 'NDHWC'), \"Only supports 'NCDHW' and 'NDHWC' layout for now\"\n        if isinstance(kernel_size, numeric_types):\n            kernel_size = (kernel_size,)*3\n        assert len(kernel_size) == 3, \"kernel_size must be a number or a list of 3 ints\"\n        op_name = kwargs.pop('op_name', 'Convolution')\n        if is_np_array():\n            op_name = 'convolution'\n        super(Conv3D, self).__init__(\n            channels, kernel_size, strides, padding, dilation, groups, layout,\n            in_channels, activation, use_bias, weight_initializer, bias_initializer,\n            op_name, **kwargs)\n",
  "sig": "Conv3D(channels|The dimensionality of the output space, i.e. the number of output channels (filters) in the convolution)(kernel_size|Specifies the dimensions of the convolution window)(strides|Specify the strides of the convolution)(padding|If padding is non-zero, then the input is implicitly zero-padded on both sides for padding number of points)(dilation| Specifies the dilation rate to use for dilated convolution)(groups|Controls the connections between inputs and outputs. At groups=1, all inputs are convolved to all outputs. At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels, and producing half the output channels, and both subsequently concatenated)(layout|Dimension ordering of data and weight. Only supports ‘NCW’ layout for now. ‘N’, ‘C’, ‘W’ stands for batch, channel, and width (time) dimensions respectively. Convolution is applied on the ‘W’ dimension)(in_channels|The number of input channels to this layer. If not specified, initialization will be deferred to the first time forward is called and in_channels will be inferred from the shape of input data)(activation|Activation function to use. See Activation(). If you don’t specify anything, no activation is applied (ie. “linear” activation: a(x) = x))(use_bias|Whether the layer uses a bias vector)(weight_initializer|Initializer for the weight weights matrix)(bias_initializer|Initializer for the bias vector)",
  "dst": "3D convolution layer (e.g. spatial convolution over volumes).    This layer creates a convolution kernel that is convolved    with the layer input to produce a tensor of    outputs. If `use_bias` is `True`,    a bias vector is created and added to the outputs. Finally, if    `activation` is not `None`, it is applied to the outputs as well.    If `in_channels` is not specified, `Parameter` initialization will be    deferred to the first time `forward` is called and `in_channels` will be    inferred from the shape of input data.    Parameters    ----------    channels : int        The dimensionality of the output space, i.e. the number of output        channels (filters) in the convolution.    kernel_size :int or tuple/list of 3 int        Specifies the dimensions of the convolution window.    strides : int or tuple/list of 3 int,        Specify the strides of the convolution.    padding : int or a tuple/list of 3 int,        If padding is non-zero, then the input is implicitly zero-padded        on both sides for padding number of points    dilation : int or tuple/list of 3 int        Specifies the dilation rate to use for dilated convolution.    groups : int        Controls the connections between inputs and outputs.        At groups=1, all inputs are convolved to all outputs.        At groups=2, the operation becomes equivalent to having two conv        layers side by side, each seeing half the input channels, and producing        half the output channels, and both subsequently concatenated.    layout : str, default 'NCDHW'        Dimension ordering of data and weight. Only supports 'NCDHW' and 'NDHWC'        layout for now. 'N', 'C', 'H', 'W', 'D' stands for batch, channel, height,        width and depth dimensions respectively. Convolution is applied on the 'D',        'H' and 'W' dimensions.    in_channels : int, default 0        The number of input channels to this layer. If not specified,        initialization will be deferred to the first time `forward` is called        and `in_channels` will be inferred from the shape of input data.    activation : str        Activation function to use. See :func:`~mxnet.ndarray.Activation`.        If you don't specify anything, no activation is applied        (ie. \"linear\" activation: `a(x) = x`).    use_bias : bool        Whether the layer uses a bias vector.    weight_initializer : str or `Initializer`        Initializer for the `weight` weights matrix.    bias_initializer : str or `Initializer`        Initializer for the bias vector.    Inputs:        - **data**: 5D input tensor with shape          `(batch_size, in_channels, depth, height, width)` when `layout` is `NCDHW`.          For other layouts shape is permuted accordingly.    Outputs:        - **out**: 5D output tensor with shape          `(batch_size, channels, out_depth, out_height, out_width)` when `layout` is `NCDHW`.          out_depth, out_height and out_width are calculated as::              out_depth = floor((depth+2*padding[0]-dilation[0]*(kernel_size[0]-1)-1)/stride[0])+1              out_height = floor((height+2*padding[1]-dilation[1]*(kernel_size[1]-1)-1)/stride[1])+1              out_width = floor((width+2*padding[2]-dilation[2]*(kernel_size[2]-1)-1)/stride[2])+1"
 },
 "mxnet.gluon.nn.Conv3DTranspose": {
  "code": "class Conv3DTranspose(_Conv):\n    \"\"\"Transposed 3D convolution layer (sometimes called Deconvolution).\n\n    The need for transposed convolutions generally arises\n    from the desire to use a transformation going in the opposite direction\n    of a normal convolution, i.e., from something that has the shape of the\n    output of some convolution to something that has the shape of its input\n    while maintaining a connectivity pattern that is compatible with\n    said convolution.\n\n    If `in_channels` is not specified, `Parameter` initialization will be\n    deferred to the first time `forward` is called and `in_channels` will be\n    inferred from the shape of input data.\n\n\n    Parameters\n    ----------\n    channels : int\n        The dimensionality of the output space, i.e. the number of output\n        channels (filters) in the convolution.\n    kernel_size :int or tuple/list of 3 int\n        Specifies the dimensions of the convolution window.\n    strides : int or tuple/list of 3 int\n        Specify the strides of the convolution.\n    padding : int or a tuple/list of 3 int,\n        If padding is non-zero, then the input is implicitly zero-padded\n        on both sides for padding number of points\n    output_padding: int or a tuple/list of 3 int\n        Controls the amount of implicit zero-paddings on both sides of the\n        output for output_padding number of points for each dimension.\n    dilation : int or tuple/list of 3 int\n        Controls the spacing between the kernel points; also known as the\n        a trous algorithm.\n    groups : int\n        Controls the connections between inputs and outputs.\n        At groups=1, all inputs are convolved to all outputs.\n        At groups=2, the operation becomes equivalent to having two conv\n        layers side by side, each seeing half the input channels, and producing\n        half the output channels, and both subsequently concatenated.\n    layout : str, default 'NCDHW'\n        Dimension ordering of data and weight. Only supports 'NCDHW' and 'NDHWC'\n        layout for now. 'N', 'C', 'H', 'W', 'D' stands for batch, channel, height,\n        width and depth dimensions respectively. Convolution is applied on the 'D',\n        'H' and 'W' dimensions.\n    in_channels : int, default 0\n        The number of input channels to this layer. If not specified,\n        initialization will be deferred to the first time `forward` is called\n        and `in_channels` will be inferred from the shape of input data.\n    activation : str\n        Activation function to use. See :func:`~mxnet.ndarray.Activation`.\n        If you don't specify anything, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).\n    use_bias : bool\n        Whether the layer uses a bias vector.\n    weight_initializer : str or `Initializer`\n        Initializer for the `weight` weights matrix.\n    bias_initializer : str or `Initializer`\n        Initializer for the bias vector.\n\n\n    Inputs:\n        - **data**: 5D input tensor with shape\n          `(batch_size, in_channels, depth, height, width)` when `layout` is `NCDHW`.\n          For other layouts shape is permuted accordingly.\n\n    Outputs:\n        - **out**: 5D output tensor with shape\n          `(batch_size, channels, out_depth, out_height, out_width)` when `layout` is `NCDHW`.\n          out_depth, out_height and out_width are calculated as::\n\n            out_depth = (depth-1)*strides[0]-2*padding[0]+kernel_size[0]+output_padding[0]\n            out_height = (height-1)*strides[1]-2*padding[1]+kernel_size[1]+output_padding[1]\n            out_width = (width-1)*strides[2]-2*padding[2]+kernel_size[2]+output_padding[2]\n    \"\"\"\n    def __init__(self, channels, kernel_size, strides=(1, 1, 1), padding=(0, 0, 0),\n                 output_padding=(0, 0, 0), dilation=(1, 1, 1), groups=1, layout='NCDHW',\n                 activation=None, use_bias=True, weight_initializer=None,\n                 bias_initializer='zeros', in_channels=0, **kwargs):\n        assert layout in ('NCDHW', 'NDHWC'), \"Only supports 'NCDHW' and 'NDHWC' layout for now\"\n        if isinstance(kernel_size, numeric_types):\n            kernel_size = (kernel_size,)*3\n        if isinstance(output_padding, numeric_types):\n            output_padding = (output_padding,)*3\n        assert len(kernel_size) == 3, \"kernel_size must be a number or a list of 3 ints\"\n        assert len(output_padding) == 3, \"output_padding must be a number or a list of 3 ints\"\n        op_name = kwargs.pop('op_name', 'Deconvolution')\n        if is_np_array():\n            op_name = 'deconvolution'\n        super(Conv3DTranspose, self).__init__(\n            channels, kernel_size, strides, padding, dilation, groups, layout,\n            in_channels, activation, use_bias, weight_initializer, bias_initializer,\n            op_name=op_name, adj=output_padding, **kwargs)\n        self.outpad = output_padding\n",
  "sig": "Conv3DTranspose(channels|The dimensionality of the output space, i.e. the number of output channels (filters) in the convolution)(kernel_size|Specifies the dimensions of the convolution window)(strides|Specify the strides of the convolution)(padding|If padding is non-zero, then the input is implicitly zero-padded on both sides for padding number of points)(output_padding|Controls the amount of implicit zero-paddings on both sides of the output for output_padding number of points for each dimension)(dilation| Specifies the dilation rate to use for dilated convolution)(groups|Controls the connections between inputs and outputs. At groups=1, all inputs are convolved to all outputs. At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels, and producing half the output channels, and both subsequently concatenated)(layout|Dimension ordering of data and weight. Only supports ‘NCW’ layout for now. ‘N’, ‘C’, ‘W’ stands for batch, channel, and width (time) dimensions respectively. Convolution is applied on the ‘W’ dimension)(in_channels|The number of input channels to this layer. If not specified, initialization will be deferred to the first time forward is called and in_channels will be inferred from the shape of input data)(activation|Activation function to use. See Activation(). If you don’t specify anything, no activation is applied (ie. “linear” activation: a(x) = x))(use_bias|Whether the layer uses a bias vector)(weight_initializer|Initializer for the weight weights matrix)(bias_initializer|Initializer for the bias vector)",
  "dst": "Transposed 3D convolution layer (sometimes called Deconvolution).    The need for transposed convolutions generally arises    from the desire to use a transformation going in the opposite direction    of a normal convolution, i.e., from something that has the shape of the    output of some convolution to something that has the shape of its input    while maintaining a connectivity pattern that is compatible with    said convolution.    If `in_channels` is not specified, `Parameter` initialization will be    deferred to the first time `forward` is called and `in_channels` will be    inferred from the shape of input data.    Parameters    ----------    channels : int        The dimensionality of the output space, i.e. the number of output        channels (filters) in the convolution.    kernel_size :int or tuple/list of 3 int        Specifies the dimensions of the convolution window.    strides : int or tuple/list of 3 int        Specify the strides of the convolution.    padding : int or a tuple/list of 3 int,        If padding is non-zero, then the input is implicitly zero-padded        on both sides for padding number of points    output_padding: int or a tuple/list of 3 int        Controls the amount of implicit zero-paddings on both sides of the        output for output_padding number of points for each dimension.    dilation : int or tuple/list of 3 int        Controls the spacing between the kernel points; also known as the        a trous algorithm.    groups : int        Controls the connections between inputs and outputs.        At groups=1, all inputs are convolved to all outputs.        At groups=2, the operation becomes equivalent to having two conv        layers side by side, each seeing half the input channels, and producing        half the output channels, and both subsequently concatenated.    layout : str, default 'NCDHW'        Dimension ordering of data and weight. Only supports 'NCDHW' and 'NDHWC'        layout for now. 'N', 'C', 'H', 'W', 'D' stands for batch, channel, height,        width and depth dimensions respectively. Convolution is applied on the 'D',        'H' and 'W' dimensions.    in_channels : int, default 0        The number of input channels to this layer. If not specified,        initialization will be deferred to the first time `forward` is called        and `in_channels` will be inferred from the shape of input data.    activation : str        Activation function to use. See :func:`~mxnet.ndarray.Activation`.        If you don't specify anything, no activation is applied        (ie. \"linear\" activation: `a(x) = x`).    use_bias : bool        Whether the layer uses a bias vector.    weight_initializer : str or `Initializer`        Initializer for the `weight` weights matrix.    bias_initializer : str or `Initializer`        Initializer for the bias vector.    Inputs:        - **data**: 5D input tensor with shape          `(batch_size, in_channels, depth, height, width)` when `layout` is `NCDHW`.          For other layouts shape is permuted accordingly.    Outputs:        - **out**: 5D output tensor with shape          `(batch_size, channels, out_depth, out_height, out_width)` when `layout` is `NCDHW`.          out_depth, out_height and out_width are calculated as::            out_depth = (depth-1)*strides[0]-2*padding[0]+kernel_size[0]+output_padding[0]            out_height = (height-1)*strides[1]-2*padding[1]+kernel_size[1]+output_padding[1]            out_width = (width-1)*strides[2]-2*padding[2]+kernel_size[2]+output_padding[2]"
 },
 "mxnet.gluon.nn.Dense": {
  "code": "class Dense(HybridBlock):\n    r\"\"\"Just your regular densely-connected NN layer.\n\n    `Dense` implements the operation:\n    `output = activation(dot(input, weight) + bias)`\n    where `activation` is the element-wise activation function\n    passed as the `activation` argument, `weight` is a weights matrix\n    created by the layer, and `bias` is a bias vector created by the layer\n    (only applicable if `use_bias` is `True`).\n\n    .. note::\n        the input must be a tensor with rank 2. Use `flatten` to convert it\n        to rank 2 manually if necessary.\n\n    Parameters\n    ----------\n    units : int\n        Dimensionality of the output space.\n    activation : str\n        Activation function to use. See help on `Activation` layer.\n        If you don't specify anything, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).\n    use_bias : bool, default True\n        Whether the layer uses a bias vector.\n    flatten: bool, default True\n        Whether the input tensor should be flattened.\n        If true, all but the first axis of input data are collapsed together.\n        If false, all but the last axis of input data are kept the same, and the transformation\n        applies on the last axis.\n    dtype : str or np.dtype, default 'float32'\n        Data type of output embeddings.\n    weight_initializer : str or `Initializer`\n        Initializer for the `kernel` weights matrix.\n    bias_initializer: str or `Initializer`\n        Initializer for the bias vector.\n    in_units : int, optional\n        Size of the input data. If not specified, initialization will be\n        deferred to the first time `forward` is called and `in_units`\n        will be inferred from the shape of input data.\n    prefix : str or None\n        See document of `Block`.\n    params : ParameterDict or None\n        See document of `Block`.\n\n\n    Inputs:\n        - **data**: if `flatten` is True, `data` should be a tensor with shape\n          `(batch_size, x1, x2, ..., xn)`, where x1 * x2 * ... * xn is equal to\n          `in_units`. If `flatten` is False, `data` should have shape\n          `(x1, x2, ..., xn, in_units)`.\n\n    Outputs:\n        - **out**: if `flatten` is True, `out` will be a tensor with shape\n          `(batch_size, units)`. If `flatten` is False, `out` will have shape\n          `(x1, x2, ..., xn, units)`.\n    \"\"\"\n    def __init__(self, units, activation=None, use_bias=True, flatten=True,\n                 dtype='float32', weight_initializer=None, bias_initializer='zeros',\n                 in_units=0, **kwargs):\n        super(Dense, self).__init__(**kwargs)\n        self._flatten = flatten\n        with self.name_scope():\n            self._units = units\n            self._in_units = in_units\n            self.weight = self.params.get('weight', shape=(units, in_units),\n                                          init=weight_initializer, dtype=dtype,\n                                          allow_deferred_init=True)\n            if use_bias:\n                self.bias = self.params.get('bias', shape=(units,),\n                                            init=bias_initializer, dtype=dtype,\n                                            allow_deferred_init=True)\n            else:\n                self.bias = None\n            if activation is not None:\n                self.act = Activation(activation, prefix=activation+'_')\n            else:\n                self.act = None\n\n    def hybrid_forward(self, F, x, weight, bias=None):\n        fc = F.npx.fully_connected if is_np_array() else F.FullyConnected\n        act = fc(x, weight, bias, no_bias=bias is None, num_hidden=self._units,\n                 flatten=self._flatten, name='fwd')\n        if self.act is not None:\n            act = self.act(act)\n        return act\n\n    def __repr__(self):\n        s = '{name}({layout}, {act})'\n        shape = self.weight.shape\n        return s.format(name=self.__class__.__name__,\n                        act=self.act if self.act else 'linear',\n                        layout='{0} -> {1}'.format(shape[1] if shape[1] else None, shape[0]))\n",
  "sig": "Dense(units|Dimensionality of the output space)(activation|Activation function to use. See help on Activation layer. If you don’t specify anything, no activation is applied (ie. “linear” activation: a(x) = x))(use_bias|Whether the layer uses a bias vector)(flatten|Whether the input tensor should be flattened. If true, all but the first axis of input data are collapsed together. If false, all but the last axis of input data are kept the same, and the transformation applies on the last axis)(dtype|Data type of output embeddings)(weight_initializer| Initializer for the kernel weights matrix)(bias_initializer|Initializer for the bias vector)(in_units|Size of the input data. If not specified, initialization will be deferred to the first time forward is called and in_units will be inferred from the shape of input data)(prefix|See document of Block)(params|See document of Block)",
  "dst": "Just your regular densely-connected NN layer.    `Dense` implements the operation:    `output = activation(dot(input, weight) + bias)`    where `activation` is the element-wise activation function    passed as the `activation` argument, `weight` is a weights matrix    created by the layer, and `bias` is a bias vector created by the layer    (only applicable if `use_bias` is `True`).    .. note::        the input must be a tensor with rank 2. Use `flatten` to convert it        to rank 2 manually if necessary.    Parameters    ----------    units : int        Dimensionality of the output space.    activation : str        Activation function to use. See help on `Activation` layer.        If you don't specify anything, no activation is applied        (ie. \"linear\" activation: `a(x) = x`).    use_bias : bool, default True        Whether the layer uses a bias vector.    flatten: bool, default True        Whether the input tensor should be flattened.        If true, all but the first axis of input data are collapsed together.        If false, all but the last axis of input data are kept the same, and the transformation        applies on the last axis.    dtype : str or np.dtype, default 'float32'        Data type of output embeddings.    weight_initializer : str or `Initializer`        Initializer for the `kernel` weights matrix.    bias_initializer: str or `Initializer`        Initializer for the bias vector.    in_units : int, optional        Size of the input data. If not specified, initialization will be        deferred to the first time `forward` is called and `in_units`        will be inferred from the shape of input data.    prefix : str or None        See document of `Block`.    params : ParameterDict or None        See document of `Block`.    Inputs:        - **data**: if `flatten` is True, `data` should be a tensor with shape          `(batch_size, x1, x2, ..., xn)`, where x1 * x2 * ... * xn is equal to          `in_units`. If `flatten` is False, `data` should have shape          `(x1, x2, ..., xn, in_units)`.    Outputs:        - **out**: if `flatten` is True, `out` will be a tensor with shape          `(batch_size, units)`. If `flatten` is False, `out` will have shape          `(x1, x2, ..., xn, units)`."
 },
 "mxnet.gluon.nn.Dropout": {
  "code": "class Dropout(HybridBlock):\n    \"\"\"Applies Dropout to the input.\n\n    Dropout consists in randomly setting a fraction `rate` of input units\n    to 0 at each update during training time, which helps prevent overfitting.\n\n    Parameters\n    ----------\n    rate : float\n        Fraction of the input units to drop. Must be a number between 0 and 1.\n    axes : tuple of int, default ()\n        The axes on which dropout mask is shared. If empty, regular dropout is applied.\n\n\n    Inputs:\n        - **data**: input tensor with arbitrary shape.\n\n    Outputs:\n        - **out**: output tensor with the same shape as `data`.\n\n    References\n    ----------\n        `Dropout: A Simple Way to Prevent Neural Networks from Overfitting\n        <http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf>`_\n    \"\"\"\n    def __init__(self, rate, axes=(), **kwargs):\n        super(Dropout, self).__init__(**kwargs)\n        self._rate = rate\n        self._axes = axes\n\n    def hybrid_forward(self, F, x):\n        if self._rate > 0:\n            dropout = F.npx.dropout if is_np_array() else F.Dropout\n            return dropout(x, p=self._rate, axes=self._axes, name='fwd', cudnn_off=False)\n        else:\n            copy = F.np.copy if is_np_array() else F.identity\n            return copy(x)\n\n    def __repr__(self):\n        s = '{name}(p = {_rate}, axes={_axes})'\n        return s.format(name=self.__class__.__name__,\n                        **self.__dict__)\n",
  "sig": "Dropout(rate|Fraction of the input units to drop. Must be a number between 0 and 1)(axes|The axes on which dropout mask is shared. If empty, regular dropout is applied)",
  "dst": "Applies Dropout to the input.    to 0 at each update during training time, which helps prevent overfitting.    Parameters    ----------    rate : float        Fraction of the input units to drop. Must be a number between 0 and 1.    axes : tuple of int, default ()        The axes on which dropout mask is shared. If empty, regular dropout is applied.    Inputs:        - **data**: input tensor with arbitrary shape.    Outputs:        - **out**: output tensor with the same shape as `data`.    References    ----------        `Dropout: A Simple Way to Prevent Neural Networks from Overfitting        <http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf>`_"
 },
 "mxnet.gluon.nn.Flatten": {
  "code": "class Flatten(HybridBlock):\n    r\"\"\"Flattens the input to two dimensional.\n\n    Inputs:\n        - **data**: input tensor with arbitrary shape `(N, x1, x2, ..., xn)`\n\n    Output:\n        - **out**: 2D tensor with shape: `(N, x1 \\cdot x2 \\cdot ... \\cdot xn)`\n    \"\"\"\n    def __init__(self, **kwargs):\n        super(Flatten, self).__init__(**kwargs)\n\n    def hybrid_forward(self, F, x):\n        flatten = F.npx.batch_flatten if is_np_array() else F.flatten\n        return flatten(x)\n\n    def __repr__(self):\n        return self.__class__.__name__\n",
  "sig": "Flatten(**kwargs|)",
  "dst": "    Inputs:        - **data**: input tensor with arbitrary shape `(N, x1, x2, ..., xn)`    Output:        - **out**: 2D tensor with shape: `(N, x1 \\cdot x2 \\cdot ... \\cdot xn)`"
 },
 "mxnet.gluon.nn.MaxPool1D": {
  "code": "class MaxPool1D(_Pooling):\n    \"\"\"Max pooling operation for one dimensional data.\n\n\n    Parameters\n    ----------\n    pool_size: int\n        Size of the max pooling windows.\n    strides: int, or None\n        Factor by which to downscale. E.g. 2 will halve the input size.\n        If `None`, it will default to `pool_size`.\n    padding: int\n        If padding is non-zero, then the input is implicitly\n        zero-padded on both sides for padding number of points.\n    layout : str, default 'NCW'\n        Dimension ordering of data and out ('NCW' or 'NWC').\n        'N', 'C', 'W' stands for batch, channel, and width (time) dimensions\n        respectively. Pooling is applied on the W dimension.\n    ceil_mode : bool, default False\n        When `True`, will use ceil instead of floor to compute the output shape.\n\n\n    Inputs:\n        - **data**: 3D input tensor with shape `(batch_size, in_channels, width)`\n          when `layout` is `NCW`. For other layouts shape is permuted accordingly.\n\n    Outputs:\n        - **out**: 3D output tensor with shape `(batch_size, channels, out_width)`\n          when `layout` is `NCW`. out_width is calculated as::\n\n              out_width = floor((width+2*padding-pool_size)/strides)+1\n\n          When `ceil_mode` is `True`, ceil will be used instead of floor in this\n          equation.\n    \"\"\"\n    def __init__(self, pool_size=2, strides=None, padding=0, layout='NCW',\n                 ceil_mode=False, **kwargs):\n        assert layout in ('NCW', 'NWC'),\\\n            \"Only NCW and NWC layouts are valid for 1D Pooling\"\n        if isinstance(pool_size, numeric_types):\n            pool_size = (pool_size,)\n        assert len(pool_size) == 1, \"pool_size must be a number or a list of 1 ints\"\n        super(MaxPool1D, self).__init__(\n            pool_size, strides, padding, ceil_mode, False, 'max', layout, **kwargs)\n",
  "sig": "MaxPool1D(pool_size| Size of the max pooling windows)(strides|Factor by which to downscale. E.g. 2 will halve the input size. If None, it will default to pool_size)(padding| If padding is non-zero, then the input is implicitly zero-padded on both sides for padding number of points)(layout|Dimension ordering of data and out (‘NCW’ or ‘NWC’). ‘N’, ‘C’, ‘W’ stands for batch, channel, and width (time) dimensions respectively. Pooling is applied on the W dimension)(ceil_mode|When True, will use ceil instead of floor to compute the output shape)",
  "dst": "Max pooling operation for one dimensional data.    Parameters    ----------    pool_size: int        Size of the max pooling windows.    strides: int, or None        Factor by which to downscale. E.g. 2 will halve the input size.        If `None`, it will default to `pool_size`.    padding: int        If padding is non-zero, then the input is implicitly        zero-padded on both sides for padding number of points.    layout : str, default 'NCW'        Dimension ordering of data and out ('NCW' or 'NWC').        'N', 'C', 'W' stands for batch, channel, and width (time) dimensions        respectively. Pooling is applied on the W dimension.    ceil_mode : bool, default False        When `True`, will use ceil instead of floor to compute the output shape.    Inputs:        - **data**: 3D input tensor with shape `(batch_size, in_channels, width)`          when `layout` is `NCW`. For other layouts shape is permuted accordingly.    Outputs:        - **out**: 3D output tensor with shape `(batch_size, channels, out_width)`          when `layout` is `NCW`. out_width is calculated as::              out_width = floor((width+2*padding-pool_size)/strides)+1          When `ceil_mode` is `True`, ceil will be used instead of floor in this          equation."
 },
 "mxnet.gluon.nn.MaxPool2D": {
  "code": "class MaxPool2D(_Pooling):\n    \"\"\"Max pooling operation for two dimensional (spatial) data.\n\n\n    Parameters\n    ----------\n    pool_size: int or list/tuple of 2 ints,\n        Size of the max pooling windows.\n    strides: int, list/tuple of 2 ints, or None.\n        Factor by which to downscale. E.g. 2 will halve the input size.\n        If `None`, it will default to `pool_size`.\n    padding: int or list/tuple of 2 ints,\n        If padding is non-zero, then the input is implicitly\n        zero-padded on both sides for padding number of points.\n    layout : str, default 'NCHW'\n        Dimension ordering of data and out ('NCHW' or 'NHWC').\n        'N', 'C', 'H', 'W' stands for batch, channel, height, and width\n        dimensions respectively. padding is applied on 'H' and 'W' dimension.\n    ceil_mode : bool, default False\n        When `True`, will use ceil instead of floor to compute the output shape.\n\n\n    Inputs:\n        - **data**: 4D input tensor with shape\n          `(batch_size, in_channels, height, width)` when `layout` is `NCHW`.\n          For other layouts shape is permuted accordingly.\n\n    Outputs:\n        - **out**: 4D output tensor with shape\n          `(batch_size, channels, out_height, out_width)` when `layout` is `NCHW`.\n          out_height and out_width are calculated as::\n\n              out_height = floor((height+2*padding[0]-pool_size[0])/strides[0])+1\n              out_width = floor((width+2*padding[1]-pool_size[1])/strides[1])+1\n\n          When `ceil_mode` is `True`, ceil will be used instead of floor in this\n          equation.\n    \"\"\"\n    def __init__(self, pool_size=(2, 2), strides=None, padding=0, layout='NCHW',\n                 ceil_mode=False, **kwargs):\n        assert layout in ('NCHW', 'NHWC'),\\\n            \"Only NCHW and NHWC layouts are valid for 2D Pooling\"\n        if isinstance(pool_size, numeric_types):\n            pool_size = (pool_size,)*2\n        assert len(pool_size) == 2, \"pool_size must be a number or a list of 2 ints\"\n        super(MaxPool2D, self).__init__(\n            pool_size, strides, padding, ceil_mode, False, 'max', layout, **kwargs)\n",
  "sig": "MaxPool2D(pool_size| Size of the max pooling windows)(strides|Factor by which to downscale. E.g. 2 will halve the input size. If None, it will default to pool_size)(padding| If padding is non-zero, then the input is implicitly zero-padded on both sides for padding number of points)(layout|Dimension ordering of data and out (‘NCW’ or ‘NWC’). ‘N’, ‘C’, ‘W’ stands for batch, channel, and width (time) dimensions respectively. Pooling is applied on the W dimension)(ceil_mode|When True, will use ceil instead of floor to compute the output shape)",
  "dst": "Max pooling operation for two dimensional (spatial) data.    Parameters    ----------    pool_size: int or list/tuple of 2 ints,        Size of the max pooling windows.    strides: int, list/tuple of 2 ints, or None.        Factor by which to downscale. E.g. 2 will halve the input size.        If `None`, it will default to `pool_size`.    padding: int or list/tuple of 2 ints,        If padding is non-zero, then the input is implicitly        zero-padded on both sides for padding number of points.    layout : str, default 'NCHW'        Dimension ordering of data and out ('NCHW' or 'NHWC').        'N', 'C', 'H', 'W' stands for batch, channel, height, and width        dimensions respectively. padding is applied on 'H' and 'W' dimension.    ceil_mode : bool, default False        When `True`, will use ceil instead of floor to compute the output shape.    Inputs:        - **data**: 4D input tensor with shape          `(batch_size, in_channels, height, width)` when `layout` is `NCHW`.          For other layouts shape is permuted accordingly.    Outputs:        - **out**: 4D output tensor with shape          `(batch_size, channels, out_height, out_width)` when `layout` is `NCHW`.          out_height and out_width are calculated as::              out_height = floor((height+2*padding[0]-pool_size[0])/strides[0])+1              out_width = floor((width+2*padding[1]-pool_size[1])/strides[1])+1          When `ceil_mode` is `True`, ceil will be used instead of floor in this          equation."
 },
 "mxnet.gluon.nn.MaxPool3D": {
  "code": "class MaxPool3D(_Pooling):\n    \"\"\"Max pooling operation for 3D data (spatial or spatio-temporal).\n\n\n    Parameters\n    ----------\n    pool_size: int or list/tuple of 3 ints,\n        Size of the max pooling windows.\n    strides: int, list/tuple of 3 ints, or None.\n        Factor by which to downscale. E.g. 2 will halve the input size.\n        If `None`, it will default to `pool_size`.\n    padding: int or list/tuple of 3 ints,\n        If padding is non-zero, then the input is implicitly\n        zero-padded on both sides for padding number of points.\n    layout : str, default 'NCDHW'\n        Dimension ordering of data and out ('NCDHW' or 'NDHWC').\n        'N', 'C', 'H', 'W', 'D' stands for batch, channel, height, width and\n        depth dimensions respectively. padding is applied on 'D', 'H' and 'W'\n        dimension.\n    ceil_mode : bool, default False\n        When `True`, will use ceil instead of floor to compute the output shape.\n\n\n    Inputs:\n        - **data**: 5D input tensor with shape\n          `(batch_size, in_channels, depth, height, width)` when `layout` is `NCW`.\n          For other layouts shape is permuted accordingly.\n\n    Outputs:\n        - **out**: 5D output tensor with shape\n          `(batch_size, channels, out_depth, out_height, out_width)` when `layout` is `NCDHW`.\n          out_depth, out_height and out_width are calculated as::\n\n              out_depth = floor((depth+2*padding[0]-pool_size[0])/strides[0])+1\n              out_height = floor((height+2*padding[1]-pool_size[1])/strides[1])+1\n              out_width = floor((width+2*padding[2]-pool_size[2])/strides[2])+1\n\n          When `ceil_mode` is `True`, ceil will be used instead of floor in this\n          equation.\n    \"\"\"\n    def __init__(self, pool_size=(2, 2, 2), strides=None, padding=0,\n                 ceil_mode=False, layout='NCDHW', **kwargs):\n        assert layout in ('NCDHW', 'NDHWC'),\\\n            \"Only NCDHW and NDHWC layouts are valid for 3D Pooling\"\n        if isinstance(pool_size, numeric_types):\n            pool_size = (pool_size,)*3\n        assert len(pool_size) == 3, \"pool_size must be a number or a list of 3 ints\"\n        super(MaxPool3D, self).__init__(\n            pool_size, strides, padding, ceil_mode, False, 'max', layout, **kwargs)\n",
  "sig": "MaxPool3D(pool_size| Size of the max pooling windows)(strides|Factor by which to downscale. E.g. 2 will halve the input size. If None, it will default to pool_size)(padding| If padding is non-zero, then the input is implicitly zero-padded on both sides for padding number of points)(layout|Dimension ordering of data and out (‘NCW’ or ‘NWC’). ‘N’, ‘C’, ‘W’ stands for batch, channel, and width (time) dimensions respectively. Pooling is applied on the W dimension)(ceil_mode|When True, will use ceil instead of floor to compute the output shape)",
  "dst": "Max pooling operation for 3D data (spatial or spatio-temporal).    Parameters    ----------    pool_size: int or list/tuple of 3 ints,        Size of the max pooling windows.    strides: int, list/tuple of 3 ints, or None.        Factor by which to downscale. E.g. 2 will halve the input size.        If `None`, it will default to `pool_size`.    padding: int or list/tuple of 3 ints,        If padding is non-zero, then the input is implicitly        zero-padded on both sides for padding number of points.    layout : str, default 'NCDHW'        Dimension ordering of data and out ('NCDHW' or 'NDHWC').        'N', 'C', 'H', 'W', 'D' stands for batch, channel, height, width and        depth dimensions respectively. padding is applied on 'D', 'H' and 'W'        dimension.    ceil_mode : bool, default False        When `True`, will use ceil instead of floor to compute the output shape.    Inputs:        - **data**: 5D input tensor with shape          `(batch_size, in_channels, depth, height, width)` when `layout` is `NCW`.          For other layouts shape is permuted accordingly.    Outputs:        - **out**: 5D output tensor with shape          `(batch_size, channels, out_depth, out_height, out_width)` when `layout` is `NCDHW`.          out_depth, out_height and out_width are calculated as::              out_depth = floor((depth+2*padding[0]-pool_size[0])/strides[0])+1              out_height = floor((height+2*padding[1]-pool_size[1])/strides[1])+1              out_width = floor((width+2*padding[2]-pool_size[2])/strides[2])+1          When `ceil_mode` is `True`, ceil will be used instead of floor in this          equation."
 },
 "mxnet.gluon.loss.HingeLoss": {
  "code": "class HingeLoss(Loss):\n    r\"\"\"Calculates the hinge loss function often used in SVMs:\n\n    .. math::\n        L = \\sum_i max(0, {margin} - {pred}_i \\cdot {label}_i)\n\n    where `pred` is the classifier prediction and `label` is the target tensor\n    containing values -1 or 1. `label` and `pred` must have the same number of\n    elements.\n\n    Parameters\n    ----------\n    margin : float\n        The margin in hinge loss. Defaults to 1.0\n    weight : float or None\n        Global scalar weight for loss.\n    batch_axis : int, default 0\n        The axis that represents mini-batch.\n\n\n    Inputs:\n        - **pred**: prediction tensor with arbitrary shape.\n        - **label**: truth tensor with values -1 or 1. Must have the same size\n          as pred.\n        - **sample_weight**: element-wise weighting tensor. Must be broadcastable\n          to the same shape as pred. For example, if pred has shape (64, 10)\n          and you want to weigh each sample in the batch separately,\n          sample_weight should have shape (64, 1).\n\n    Outputs:\n        - **loss**: loss tensor with shape (batch_size,). Dimenions other than\n          batch_axis are averaged out.\n    \"\"\"\n\n    def __init__(self, margin=1, weight=None, batch_axis=0, **kwargs):\n        super(HingeLoss, self).__init__(weight, batch_axis, **kwargs)\n        self._margin = margin\n\n    def hybrid_forward(self, F, pred, label, sample_weight=None):\n        label = _reshape_like(F, label, pred)\n        loss = F.relu(self._margin - pred * label)\n        loss = _apply_weighting(F, loss, self._weight, sample_weight)\n        return F.mean(loss, axis=self._batch_axis, exclude=True)\n",
  "sig": "HingeLoss()",
  "dst": "Calculates the hinge loss function often used in SVMs:    .. math::        L = \\sum_i max(0, {margin} - {pred}_i \\cdot {label}_i)    where `pred` is the classifier prediction and `label` is the target tensor    containing values -1 or 1. `label` and `pred` must have the same number of    elements.    Parameters    ----------    margin : float        The margin in hinge loss. Defaults to 1.0    weight : float or None        Global scalar weight for loss.    batch_axis : int, default 0        The axis that represents mini-batch.    Inputs:        - **pred**: prediction tensor with arbitrary shape.        - **label**: truth tensor with values -1 or 1. Must have the same size          as pred.        - **sample_weight**: element-wise weighting tensor. Must be broadcastable          to the same shape as pred. For example, if pred has shape (64, 10)          and you want to weigh each sample in the batch separately,          sample_weight should have shape (64, 1).    Outputs:        - **loss**: loss tensor with shape (batch_size,). Dimenions other than          batch_axis are averaged out."
 },
 "mxnet.gluon.loss.HuberLoss": {
  "code": "class HuberLoss(Loss):\n    r\"\"\"Calculates smoothed L1 loss that is equal to L1 loss if absolute error\n    exceeds rho but is equal to L2 loss otherwise. Also called SmoothedL1 loss.\n\n    .. math::\n        L = \\sum_i \\begin{cases} \\frac{1}{2 {rho}} ({label}_i - {pred}_i)^2 &\n                           \\text{ if } |{label}_i - {pred}_i| < {rho} \\\\\n                           |{label}_i - {pred}_i| - \\frac{{rho}}{2} &\n                           \\text{ otherwise }\n            \\end{cases}\n\n    `label` and `pred` can have arbitrary shape as long as they have the same\n    number of elements.\n\n    Parameters\n    ----------\n    rho : float, default 1\n        Threshold for trimmed mean estimator.\n    weight : float or None\n        Global scalar weight for loss.\n    batch_axis : int, default 0\n        The axis that represents mini-batch.\n\n\n    Inputs:\n        - **pred**: prediction tensor with arbitrary shape\n        - **label**: target tensor with the same size as pred.\n        - **sample_weight**: element-wise weighting tensor. Must be broadcastable\n          to the same shape as pred. For example, if pred has shape (64, 10)\n          and you want to weigh each sample in the batch separately,\n          sample_weight should have shape (64, 1).\n\n    Outputs:\n        - **loss**: loss tensor with shape (batch_size,). Dimenions other than\n          batch_axis are averaged out.\n    \"\"\"\n\n    def __init__(self, rho=1, weight=None, batch_axis=0, **kwargs):\n        super(HuberLoss, self).__init__(weight, batch_axis, **kwargs)\n        self._rho = rho\n\n    def hybrid_forward(self, F, pred, label, sample_weight=None):\n        label = _reshape_like(F, label, pred)\n        loss = F.abs(label - pred)\n        loss = F.where(loss > self._rho, loss - 0.5 * self._rho,\n                       (0.5 / self._rho) * F.square(loss))\n        loss = _apply_weighting(F, loss, self._weight, sample_weight)\n        return F.mean(loss, axis=self._batch_axis, exclude=True)\n",
  "sig": "HuberLoss()",
  "dst": "Calculates smoothed L1 loss that is equal to L1 loss if absolute error    exceeds rho but is equal to L2 loss otherwise. Also called SmoothedL1 loss.    .. math::        L = \\sum_i \\begin{cases} \\frac{1}{2 {rho}} ({label}_i - {pred}_i)^2 &                           \\text{ if } |{label}_i - {pred}_i| < {rho} \\\\                           |{label}_i - {pred}_i| - \\frac{{rho}}{2} &                           \\text{ otherwise }            \\end{cases}    `label` and `pred` can have arbitrary shape as long as they have the same    number of elements.    Parameters    ----------    rho : float, default 1        Threshold for trimmed mean estimator.    weight : float or None        Global scalar weight for loss.    batch_axis : int, default 0        The axis that represents mini-batch.    Inputs:        - **pred**: prediction tensor with arbitrary shape        - **label**: target tensor with the same size as pred.        - **sample_weight**: element-wise weighting tensor. Must be broadcastable          to the same shape as pred. For example, if pred has shape (64, 10)          and you want to weigh each sample in the batch separately,          sample_weight should have shape (64, 1).    Outputs:        - **loss**: loss tensor with shape (batch_size,). Dimenions other than          batch_axis are averaged out."
 },
 "mxnet.gluon.loss.L2Loss": {
  "code": "class L2Loss(Loss):\n    r\"\"\"Calculates the mean squared error between `label` and `pred`.\n\n    .. math:: L = \\frac{1}{2} \\sum_i \\vert {label}_i - {pred}_i \\vert^2.\n\n    `label` and `pred` can have arbitrary shape as long as they have the same\n    number of elements.\n\n    Parameters\n    ----------\n    weight : float or None\n        Global scalar weight for loss.\n    batch_axis : int, default 0\n        The axis that represents mini-batch.\n\n\n    Inputs:\n        - **pred**: prediction tensor with arbitrary shape\n        - **label**: target tensor with the same size as pred.\n        - **sample_weight**: element-wise weighting tensor. Must be broadcastable\n          to the same shape as pred. For example, if pred has shape (64, 10)\n          and you want to weigh each sample in the batch separately,\n          sample_weight should have shape (64, 1).\n\n    Outputs:\n        - **loss**: loss tensor with shape (batch_size,). Dimenions other than\n          batch_axis are averaged out.\n    \"\"\"\n\n    def __init__(self, weight=1., batch_axis=0, **kwargs):\n        super(L2Loss, self).__init__(weight, batch_axis, **kwargs)\n\n    def hybrid_forward(self, F, pred, label, sample_weight=None):\n        label = _reshape_like(F, label, pred)\n        loss = F.np.square(label - pred) if is_np_array() else F.square(label - pred)\n        loss = _apply_weighting(F, loss, self._weight / 2, sample_weight)\n        if is_np_array():\n            if F is ndarray:\n                return F.np.mean(loss, axis=tuple(range(1, loss.ndim)))\n            else:\n                return F.npx.batch_flatten(loss).mean(axis=1)\n        else:\n            return F.mean(loss, axis=self._batch_axis, exclude=True)\n",
  "sig": "L2Loss()",
  "dst": "Calculates the mean squared error between `label` and `pred`.    .. math:: L = \\frac{1}{2} \\sum_i \\vert {label}_i - {pred}_i \\vert^2.    `label` and `pred` can have arbitrary shape as long as they have the same    number of elements.    Parameters    ----------    weight : float or None        Global scalar weight for loss.    batch_axis : int, default 0        The axis that represents mini-batch.    Inputs:        - **pred**: prediction tensor with arbitrary shape        - **label**: target tensor with the same size as pred.        - **sample_weight**: element-wise weighting tensor. Must be broadcastable          to the same shape as pred. For example, if pred has shape (64, 10)          and you want to weigh each sample in the batch separately,          sample_weight should have shape (64, 1).    Outputs:        - **loss**: loss tensor with shape (batch_size,). Dimenions other than          batch_axis are averaged out."
 },
 "mxnet.ndarray.NDArray.log_softmax": {
  "code": "    def log_softmax(self, *args, **kwargs):\n        \"\"\"Convenience fluent method for :py:func:`log_softmax`.\n\n        The arguments are the same as for :py:func:`log_softmax`, with\n        this array as data.\n        \"\"\"\n        return op.log_softmax(self, *args, **kwargs)\n",
  "sig": "log_softmax(data|The input array)(axis|The axis along which to compute softmax)(temperature|Temperature parameter in softmax)(dtype|DType of the output in case this can’t be inferred. Defaults to the same as input’s dtype if not defined (dtype=None))(use_length| Whether to use the length input as a mask over the data input)(out| The output NDArray to hold the result)",
  "dst": "Convenience fluent method for :py:func:`log_softmax`.        The arguments are the same as for :py:func:`log_softmax`, with        this array as data."
 },
 "mxnet.ndarray.gen_op.SoftmaxActivation": {
  "code": "def SoftmaxActivation(data=None, mode=_Null, out=None, name=None, **kwargs):\n    r\"\"\"Applies softmax activation to input. This is intended for internal layers.\n\n    .. note::\n\n      This operator has been deprecated, please use `softmax`.\n\n    If `mode` = ``instance``, this operator will compute a softmax for each instance in the batch.\n    This is the default mode.\n\n    If `mode` = ``channel``, this operator will compute a k-class softmax at each position\n    of each instance, where `k` = ``num_channel``. This mode can only be used when the input array\n    has at least 3 dimensions.\n    This can be used for `fully convolutional network`, `image segmentation`, etc.\n\n    Example::\n\n      >>> input_array = mx.nd.array([[3., 0.5, -0.5, 2., 7.],\n      >>>                            [2., -.4, 7.,   3., 0.2]])\n      >>> softmax_act = mx.nd.SoftmaxActivation(input_array)\n      >>> print softmax_act.asnumpy()\n      [[  1.78322066e-02   1.46375655e-03   5.38485940e-04   6.56010211e-03   9.73605454e-01]\n       [  6.56221947e-03   5.95310994e-04   9.73919690e-01   1.78379621e-02   1.08472735e-03]]\n\n\n\n    Defined in ../src/operator/nn/softmax_activation.cc:L58\n\n    Parameters\n    ----------\n    data : NDArray\n        The input array.\n    mode : {'channel', 'instance'},optional, default='instance'\n        Specifies how to compute the softmax. If set to ``instance``, it computes softmax for each instance. If set to ``channel``, It computes cross channel softmax for each position of each instance.\n\n    out : NDArray, optional\n        The output NDArray to hold the result.\n\n    Returns\n    -------\n    out : NDArray or list of NDArrays\n        The output of this function.\n    \"\"\"\n    return (0,)\n",
  "sig": "SoftmaxActivation(data|The input array)(mode|Specifies how to compute the softmax. If set to instance, it computes softmax for each instance. If set to channel, It computes cross channel softmax for each position of each instance)(out|The output NDArray to hold the result)(name|NULL)",
  "dst": "Applies softmax activation to input. This is intended for internal layers.    .. note::      This operator has been deprecated, please use `softmax`.    If `mode` = ``instance``, this operator will compute a softmax for each instance in the batch.    This is the default mode.    If `mode` = ``channel``, this operator will compute a k-class softmax at each position    of each instance, where `k` = ``num_channel``. This mode can only be used when the input array    has at least 3 dimensions.    This can be used for `fully convolutional network`, `image segmentation`, etc.    Example::      >>> input_array = mx.nd.array([[3., 0.5, -0.5, 2., 7.],      >>>                            [2., -.4, 7.,   3., 0.2]])      >>> softmax_act = mx.nd.SoftmaxActivation(input_array)      >>> print softmax_act.asnumpy()      [[  1.78322066e-02   1.46375655e-03   5.38485940e-04   6.56010211e-03   9.73605454e-01]       [  6.56221947e-03   5.95310994e-04   9.73919690e-01   1.78379621e-02   1.08472735e-03]]    Defined in ../src/operator/nn/softmax_activation.cc:L58    Parameters    ----------    data : NDArray        The input array.    mode : {'channel', 'instance'},optional, default='instance'        Specifies how to compute the softmax. If set to ``instance``, it computes softmax for each instance. If set to ``channel``, It computes cross channel softmax for each position of each instance.    out : NDArray, optional        The output NDArray to hold the result.    Returns    -------    out : NDArray or list of NDArrays        The output of this function."
 },
 "mxnet.gluon.loss.CTCLoss": {
  "code": "class CTCLoss(Loss):\n    r\"\"\"Connectionist Temporal Classification Loss.\n\n\n    Parameters\n    ----------\n    layout : str, default 'NTC'\n        Layout of prediction tensor. 'N', 'T', 'C' stands for batch size,\n        sequence length, and alphabet_size respectively.\n    label_layout : str, default 'NT'\n        Layout of the labels. 'N', 'T' stands for batch size, and sequence\n        length respectively.\n    weight : float or None\n        Global scalar weight for loss.\n\n\n    Inputs:\n        - **pred**: unnormalized prediction tensor (before softmax).\n          Its shape depends on `layout`. If `layout` is 'TNC', pred\n          should have shape `(sequence_length, batch_size, alphabet_size)`.\n          Note that in the last dimension, index `alphabet_size-1` is reserved\n          for internal use as blank label. So `alphabet_size` is one plus the\n          actual alphabet size.\n\n        - **label**: zero-based label tensor. Its shape depends on `label_layout`.\n          If `label_layout` is 'TN', `label` should have shape\n          `(label_sequence_length, batch_size)`.\n\n        - **pred_lengths**: optional (default None), used for specifying the\n          length of each entry when different `pred` entries in the same batch\n          have different lengths. `pred_lengths` should have shape `(batch_size,)`.\n\n        - **label_lengths**: optional (default None), used for specifying the\n          length of each entry when different `label` entries in the same batch\n          have different lengths. `label_lengths` should have shape `(batch_size,)`.\n\n    Outputs:\n        - **loss**: output loss has shape `(batch_size,)`.\n\n\n    **Example**: suppose the vocabulary is `[a, b, c]`, and in one batch we\n    have three sequences 'ba', 'cbb', and 'abac'. We can index the labels as\n    `{'a': 0, 'b': 1, 'c': 2, blank: 3}`. Then `alphabet_size` should be 4,\n    where label 3 is reserved for internal use by `CTCLoss`. We then need to\n    pad each sequence with `-1` to make a rectangular `label` tensor::\n\n        [[1, 0, -1, -1],\n         [2, 1,  1, -1],\n         [0, 1,  0,  2]]\n\n\n    References\n    ----------\n        `Connectionist Temporal Classification: Labelling Unsegmented\n        Sequence Data with Recurrent Neural Networks\n        <http://www.cs.toronto.edu/~graves/icml_2006.pdf>`_\n    \"\"\"\n\n    def __init__(self, layout='NTC', label_layout='NT', weight=None, **kwargs):\n        assert layout in ['NTC', 'TNC'],\\\n            \"Only 'NTC' and 'TNC' layouts for pred are supported. Got: %s\" % layout\n        assert label_layout in ['NT', 'TN'],\\\n            \"Only 'NT' and 'TN' layouts for label are supported. Got: %s\" % label_layout\n        self._layout = layout\n        self._label_layout = label_layout\n        batch_axis = label_layout.find('N')\n        super(CTCLoss, self).__init__(weight, batch_axis, **kwargs)\n\n    def hybrid_forward(self, F, pred, label,\n                       pred_lengths=None, label_lengths=None, sample_weight=None):\n        if self._layout == 'NTC':\n            pred = F.swapaxes(pred, 0, 1)\n        if self._batch_axis == 1:\n            label = F.swapaxes(label, 0, 1)\n        loss = F.CTCLoss(pred, label, pred_lengths, label_lengths,\n                         use_data_lengths=pred_lengths is not None,\n                         use_label_lengths=label_lengths is not None,\n                         blank_label='last')\n        return _apply_weighting(F, loss, self._weight, sample_weight)\n",
  "sig": "CTCLoss(layout|Layout of prediction tensor. ‘N’, ‘T’, ‘C’ stands for batch size, sequence length, and alphabet_size respectively)(label_layout|Layout of the labels. ‘N’, ‘T’ stands for batch size, and sequence length respectively)(weight|Global scalar weight for loss)",
  "dst": "Connectionist Temporal Classification Loss.    Parameters    ----------    layout : str, default 'NTC'        Layout of prediction tensor. 'N', 'T', 'C' stands for batch size,        sequence length, and alphabet_size respectively.    label_layout : str, default 'NT'        Layout of the labels. 'N', 'T' stands for batch size, and sequence        length respectively.    weight : float or None        Global scalar weight for loss.    Inputs:        - **pred**: unnormalized prediction tensor (before softmax).          Its shape depends on `layout`. If `layout` is 'TNC', pred          should have shape `(sequence_length, batch_size, alphabet_size)`.          Note that in the last dimension, index `alphabet_size-1` is reserved          for internal use as blank label. So `alphabet_size` is one plus the          actual alphabet size.        - **label**: zero-based label tensor. Its shape depends on `label_layout`.          If `label_layout` is 'TN', `label` should have shape          `(label_sequence_length, batch_size)`.        - **pred_lengths**: optional (default None), used for specifying the          length of each entry when different `pred` entries in the same batch          have different lengths. `pred_lengths` should have shape `(batch_size,)`.        - **label_lengths**: optional (default None), used for specifying the          length of each entry when different `label` entries in the same batch          have different lengths. `label_lengths` should have shape `(batch_size,)`.    Outputs:        - **loss**: output loss has shape `(batch_size,)`.    **Example**: suppose the vocabulary is `[a, b, c]`, and in one batch we    have three sequences 'ba', 'cbb', and 'abac'. We can index the labels as    `{'a': 0, 'b': 1, 'c': 2, blank: 3}`. Then `alphabet_size` should be 4,    where label 3 is reserved for internal use by `CTCLoss`. We then need to    pad each sequence with `-1` to make a rectangular `label` tensor::        [[1, 0, -1, -1],         [2, 1,  1, -1],         [0, 1,  0,  2]]    References    ----------        `Connectionist Temporal Classification: Labelling Unsegmented        Sequence Data with Recurrent Neural Networks        <http://www.cs.toronto.edu/~graves/icml_2006.pdf>`_"
 },
 "mxnet.gluon.nn.Activation": {
  "code": "class Activation(HybridBlock):\n    r\"\"\"Applies an activation function to input.\n\n    Parameters\n    ----------\n    activation : str\n        Name of activation function to use.\n        See :func:`~mxnet.ndarray.Activation` for available choices.\n\n\n    Inputs:\n        - **data**: input tensor with arbitrary shape.\n\n    Outputs:\n        - **out**: output tensor with the same shape as `data`.\n    \"\"\"\n    def __init__(self, activation, **kwargs):\n        self._act_type = activation\n        super(Activation, self).__init__(**kwargs)\n\n    def _alias(self):\n        return self._act_type\n\n    def hybrid_forward(self, F, x):\n        act = F.npx.activation if is_np_array() else F.Activation\n        return act(x, act_type=self._act_type, name='fwd')\n\n    def __repr__(self):\n        s = '{name}({_act_type})'\n        return s.format(name=self.__class__.__name__,\n                        **self.__dict__)\n",
  "sig": "Activation(activation| Name of activation function to use. See Activation() for available choices.)",
  "dst": "Applies an activation function to input.    Parameters    ----------    activation : str        Name of activation function to use.        See :func:`~mxnet.ndarray.Activation` for available choices.    Inputs:        - **data**: input tensor with arbitrary shape.    Outputs:        - **out**: output tensor with the same shape as `data`."
 },
 "mxnet.gluon.rnn.GRUCell": {
  "code": "class GRUCell(HybridRecurrentCell):\n    r\"\"\"Gated Rectified Unit (GRU) network cell.\n    Note: this is an implementation of the cuDNN version of GRUs\n    (slight modification compared to Cho et al. 2014; the reset gate :math:`r_t`\n    is applied after matrix multiplication).\n\n    Each call computes the following function:\n\n    .. math::\n        \\begin{array}{ll}\n        r_t = sigmoid(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\\\\n        i_t = sigmoid(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\\\\n        n_t = \\tanh(W_{in} x_t + b_{in} + r_t * (W_{hn} h_{(t-1)} + b_{hn})) \\\\\n        h_t = (1 - i_t) * n_t + i_t * h_{(t-1)} \\\\\n        \\end{array}\n\n    where :math:`h_t` is the hidden state at time `t`, :math:`x_t` is the hidden\n    state of the previous layer at time `t` or :math:`input_t` for the first layer,\n    and :math:`r_t`, :math:`i_t`, :math:`n_t` are the reset, input, and new gates, respectively.\n\n    Parameters\n    ----------\n    hidden_size : int\n        Number of units in output symbol.\n    i2h_weight_initializer : str or Initializer\n        Initializer for the input weights matrix, used for the linear\n        transformation of the inputs.\n    h2h_weight_initializer : str or Initializer\n        Initializer for the recurrent weights matrix, used for the linear\n        transformation of the recurrent state.\n    i2h_bias_initializer : str or Initializer, default 'zeros'\n        Initializer for the bias vector.\n    h2h_bias_initializer : str or Initializer, default 'zeros'\n        Initializer for the bias vector.\n    prefix : str, default ``'gru_'``\n        prefix for name of `Block`s\n        (and name of weight if params is `None`).\n    params : Parameter or None, default None\n        Container for weight sharing between cells.\n        Created if `None`.\n\n\n    Inputs:\n        - **data**: input tensor with shape `(batch_size, input_size)`.\n        - **states**: a list of one initial recurrent state tensor with shape\n          `(batch_size, num_hidden)`.\n\n    Outputs:\n        - **out**: output tensor with shape `(batch_size, num_hidden)`.\n        - **next_states**: a list of one output recurrent state tensor with the\n          same shape as `states`.\n    \"\"\"\n    def __init__(self, hidden_size,\n                 i2h_weight_initializer=None, h2h_weight_initializer=None,\n                 i2h_bias_initializer='zeros', h2h_bias_initializer='zeros',\n                 input_size=0, prefix=None, params=None):\n        super(GRUCell, self).__init__(prefix=prefix, params=params)\n        self._hidden_size = hidden_size\n        self._input_size = input_size\n        self.i2h_weight = self.params.get('i2h_weight', shape=(3*hidden_size, input_size),\n                                          init=i2h_weight_initializer,\n                                          allow_deferred_init=True)\n        self.h2h_weight = self.params.get('h2h_weight', shape=(3*hidden_size, hidden_size),\n                                          init=h2h_weight_initializer,\n                                          allow_deferred_init=True)\n        self.i2h_bias = self.params.get('i2h_bias', shape=(3*hidden_size,),\n                                        init=i2h_bias_initializer,\n                                        allow_deferred_init=True)\n        self.h2h_bias = self.params.get('h2h_bias', shape=(3*hidden_size,),\n                                        init=h2h_bias_initializer,\n                                        allow_deferred_init=True)\n\n    def state_info(self, batch_size=0):\n        return [{'shape': (batch_size, self._hidden_size), '__layout__': 'NC'}]\n\n    def _alias(self):\n        return 'gru'\n\n    def __repr__(self):\n        s = '{name}({mapping})'\n        shape = self.i2h_weight.shape\n        mapping = '{0} -> {1}'.format(shape[1] if shape[1] else None, shape[0])\n        return s.format(name=self.__class__.__name__,\n                        mapping=mapping,\n                        **self.__dict__)\n\n    def hybrid_forward(self, F, inputs, states, i2h_weight,\n                       h2h_weight, i2h_bias, h2h_bias):\n        # pylint: disable=too-many-locals\n        prefix = 't%d_'%self._counter\n        prev_state_h = states[0]\n        i2h = F.FullyConnected(data=inputs,\n                               weight=i2h_weight,\n                               bias=i2h_bias,\n                               num_hidden=self._hidden_size * 3,\n                               name=prefix+'i2h')\n        h2h = F.FullyConnected(data=prev_state_h,\n                               weight=h2h_weight,\n                               bias=h2h_bias,\n                               num_hidden=self._hidden_size * 3,\n                               name=prefix+'h2h')\n\n        i2h_r, i2h_z, i2h = F.SliceChannel(i2h, num_outputs=3,\n                                           name=prefix+'i2h_slice')\n        h2h_r, h2h_z, h2h = F.SliceChannel(h2h, num_outputs=3,\n                                           name=prefix+'h2h_slice')\n\n        reset_gate = F.Activation(F.elemwise_add(i2h_r, h2h_r, name=prefix+'plus0'), act_type=\"sigmoid\",\n                                  name=prefix+'r_act')\n        update_gate = F.Activation(F.elemwise_add(i2h_z, h2h_z, name=prefix+'plus1'), act_type=\"sigmoid\",\n                                   name=prefix+'z_act')\n\n        next_h_tmp = F.Activation(F.elemwise_add(i2h,\n                                                 F.elemwise_mul(reset_gate, h2h, name=prefix+'mul0'),\n                                                 name=prefix+'plus2'),\n                                  act_type=\"tanh\",\n                                  name=prefix+'h_act')\n\n        ones = F.ones_like(update_gate, name=prefix+\"ones_like0\")\n        next_h = F.elemwise_add(F.elemwise_mul(F.elemwise_sub(ones, update_gate, name=prefix+'minus0'),\n                                               next_h_tmp,\n                                               name=prefix+'mul1'),\n                                F.elemwise_mul(update_gate, prev_state_h, name=prefix+'mul20'),\n                                name=prefix+'out')\n\n        return next_h, [next_h]\n",
  "sig": "GRUCell(hidden_size|Number of units in output symbol)(i2h_weight_initializer|Initializer for the input weights matrix, used for the linear transformation of the inputs)(h2h_weight_initializer| Initializer for the recurrent weights matrix, used for the linear transformation of the recurrent state)(i2h_bias_initializer|Initializer for the bias vector)(h2h_bias_initializer|Initializer for the bias vector)(prefix|prefix for name of Block`s (and name of weight if params is `None))(params|Container for weight sharing between cells. Created if None)",
  "dst": "Gated Rectified Unit (GRU) network cell.    Note: this is an implementation of the cuDNN version of GRUs    (slight modification compared to Cho et al. 2014; the reset gate :math:`r_t`    is applied after matrix multiplication).    Each call computes the following function:    .. math::        \\begin{array}{ll}        r_t = sigmoid(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\\\        i_t = sigmoid(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\\\        n_t = \\tanh(W_{in} x_t + b_{in} + r_t * (W_{hn} h_{(t-1)} + b_{hn})) \\\\        h_t = (1 - i_t) * n_t + i_t * h_{(t-1)} \\\\        \\end{array}    where :math:`h_t` is the hidden state at time `t`, :math:`x_t` is the hidden    state of the previous layer at time `t` or :math:`input_t` for the first layer,    and :math:`r_t`, :math:`i_t`, :math:`n_t` are the reset, input, and new gates, respectively.    Parameters    ----------    hidden_size : int        Number of units in output symbol.    i2h_weight_initializer : str or Initializer        Initializer for the input weights matrix, used for the linear        transformation of the inputs.    h2h_weight_initializer : str or Initializer        Initializer for the recurrent weights matrix, used for the linear        transformation of the recurrent state.    i2h_bias_initializer : str or Initializer, default 'zeros'        Initializer for the bias vector.    h2h_bias_initializer : str or Initializer, default 'zeros'        Initializer for the bias vector.    prefix : str, default ``'gru_'``        prefix for name of `Block`s        (and name of weight if params is `None`).    params : Parameter or None, default None        Container for weight sharing between cells.        Created if `None`.    Inputs:        - **data**: input tensor with shape `(batch_size, input_size)`.        - **states**: a list of one initial recurrent state tensor with shape          `(batch_size, num_hidden)`.    Outputs:        - **out**: output tensor with shape `(batch_size, num_hidden)`.        - **next_states**: a list of one output recurrent state tensor with the          same shape as `states`."
 },
 "mxnet.ndarray.gen_op.norm": {
  "code": "def norm(data=None, ord=_Null, axis=_Null, out_dtype=_Null, keepdims=_Null, out=None, name=None, **kwargs):\n    r\"\"\"Computes the norm on an NDArray.\n\n    This operator computes the norm on an NDArray with the specified axis, depending\n    on the value of the ord parameter. By default, it computes the L2 norm on the entire\n    array. Currently only ord=2 supports sparse ndarrays.\n\n    Examples::\n\n      x = [[[1, 2],\n            [3, 4]],\n           [[2, 2],\n            [5, 6]]]\n\n      norm(x, ord=2, axis=1) = [[3.1622777 4.472136 ]\n                                [5.3851647 6.3245554]]\n\n      norm(x, ord=1, axis=1) = [[4., 6.],\n                                [7., 8.]]\n\n      rsp = x.cast_storage('row_sparse')\n\n      norm(rsp) = [5.47722578]\n\n      csr = x.cast_storage('csr')\n\n      norm(csr) = [5.47722578]\n\n\n\n    Defined in ../src/operator/tensor/broadcast_reduce_norm_value.cc:L88\n\n    Parameters\n    ----------\n    data : NDArray\n        The input\n    ord : int, optional, default='2'\n        Order of the norm. Currently ord=1 and ord=2 is supported.\n    axis : Shape or None, optional, default=None\n        The axis or axes along which to perform the reduction.\n          The default, `axis=()`, will compute over all elements into a\n          scalar array with shape `(1,)`.\n          If `axis` is int, a reduction is performed on a particular axis.\n          If `axis` is a 2-tuple, it specifies the axes that hold 2-D matrices,\n          and the matrix norms of these matrices are computed.\n    out_dtype : {None, 'float16', 'float32', 'float64', 'int32', 'int64', 'int8'},optional, default='None'\n        The data type of the output.\n    keepdims : boolean, optional, default=0\n        If this is set to `True`, the reduced axis is left in the result as dimension with size one.\n\n    out : NDArray, optional\n        The output NDArray to hold the result.\n\n    Returns\n    -------\n    out : NDArray or list of NDArrays\n        The output of this function.\n    \"\"\"\n    return (0,)\n",
  "sig": "norm(data| The input)(ord|Order of the norm. Currently ord=1 and ord=2 is supported)(axis|The axis or axes along which to perform the reduction.The default, axis=(), will compute over all elements into a scalar array with shape (1,). If axis is int, a reduction is performed on a particular axis. If axis is a 2-tuple, it specifies the axes that hold 2-D matrices, and the matrix norms of these matrices are computed.)(out_dtype|The data type of the output)(keepdims|If this is set to True, the reduced axis is left in the result as dimension with size one)(out|The output NDArray to hold the result)(name|NULL)",
  "dst": "Computes the norm on an NDArray.    This operator computes the norm on an NDArray with the specified axis, depending    on the value of the ord parameter. By default, it computes the L2 norm on the entire    array. Currently only ord=2 supports sparse ndarrays.    Examples::      x = [[[1, 2],            [3, 4]],           [[2, 2],            [5, 6]]]                                [5.3851647 6.3245554]]                                [7., 8.]]      rsp = x.cast_storage('row_sparse')      csr = x.cast_storage('csr')    Defined in ../src/operator/tensor/broadcast_reduce_norm_value.cc:L88    Parameters    ----------    data : NDArray        The input    ord : int, optional, default='2'        Order of the norm. Currently ord=1 and ord=2 is supported.    axis : Shape or None, optional, default=None        The axis or axes along which to perform the reduction.          The default, `axis=()`, will compute over all elements into a          scalar array with shape `(1,)`.          If `axis` is int, a reduction is performed on a particular axis.          If `axis` is a 2-tuple, it specifies the axes that hold 2-D matrices,          and the matrix norms of these matrices are computed.    out_dtype : {None, 'float16', 'float32', 'float64', 'int32', 'int64', 'int8'},optional, default='None'        The data type of the output.    keepdims : boolean, optional, default=0        If this is set to `True`, the reduced axis is left in the result as dimension with size one.    out : NDArray, optional        The output NDArray to hold the result.    Returns    -------    out : NDArray or list of NDArrays        The output of this function."
 },
 "mxnet.ndarray.gen_op.pad": {
  "code": "def pad(data=None, mode=_Null, pad_width=_Null, constant_value=_Null, out=None, name=None, **kwargs):\n    r\"\"\"Pads an input array with a constant or edge values of the array.\n\n    .. note:: `Pad` is deprecated. Use `pad` instead.\n\n    .. note:: Current implementation only supports 4D and 5D input arrays with padding applied\n       only on axes 1, 2 and 3. Expects axes 4 and 5 in `pad_width` to be zero.\n\n    This operation pads an input array with either a `constant_value` or edge values\n    along each axis of the input array. The amount of padding is specified by `pad_width`.\n\n    `pad_width` is a tuple of integer padding widths for each axis of the format\n    ``(before_1, after_1, ... , before_N, after_N)``. The `pad_width` should be of length ``2*N``\n    where ``N`` is the number of dimensions of the array.\n\n    For dimension ``N`` of the input array, ``before_N`` and ``after_N`` indicates how many values\n    to add before and after the elements of the array along dimension ``N``.\n    The widths of the higher two dimensions ``before_1``, ``after_1``, ``before_2``,\n    ``after_2`` must be 0.\n\n    Example::\n\n       x = [[[[  1.   2.   3.]\n              [  4.   5.   6.]]\n\n             [[  7.   8.   9.]\n              [ 10.  11.  12.]]]\n\n\n            [[[ 11.  12.  13.]\n              [ 14.  15.  16.]]\n\n             [[ 17.  18.  19.]\n              [ 20.  21.  22.]]]]\n\n       pad(x,mode=\"edge\", pad_width=(0,0,0,0,1,1,1,1)) =\n\n             [[[[  1.   1.   2.   3.   3.]\n                [  1.   1.   2.   3.   3.]\n                [  4.   4.   5.   6.   6.]\n                [  4.   4.   5.   6.   6.]]\n\n               [[  7.   7.   8.   9.   9.]\n                [  7.   7.   8.   9.   9.]\n                [ 10.  10.  11.  12.  12.]\n                [ 10.  10.  11.  12.  12.]]]\n\n\n              [[[ 11.  11.  12.  13.  13.]\n                [ 11.  11.  12.  13.  13.]\n                [ 14.  14.  15.  16.  16.]\n                [ 14.  14.  15.  16.  16.]]\n\n               [[ 17.  17.  18.  19.  19.]\n                [ 17.  17.  18.  19.  19.]\n                [ 20.  20.  21.  22.  22.]\n                [ 20.  20.  21.  22.  22.]]]]\n\n       pad(x, mode=\"constant\", constant_value=0, pad_width=(0,0,0,0,1,1,1,1)) =\n\n             [[[[  0.   0.   0.   0.   0.]\n                [  0.   1.   2.   3.   0.]\n                [  0.   4.   5.   6.   0.]\n                [  0.   0.   0.   0.   0.]]\n\n               [[  0.   0.   0.   0.   0.]\n                [  0.   7.   8.   9.   0.]\n                [  0.  10.  11.  12.   0.]\n                [  0.   0.   0.   0.   0.]]]\n\n\n              [[[  0.   0.   0.   0.   0.]\n                [  0.  11.  12.  13.   0.]\n                [  0.  14.  15.  16.   0.]\n                [  0.   0.   0.   0.   0.]]\n\n               [[  0.   0.   0.   0.   0.]\n                [  0.  17.  18.  19.   0.]\n                [  0.  20.  21.  22.   0.]\n                [  0.   0.   0.   0.   0.]]]]\n\n\n\n\n    Defined in ../src/operator/pad.cc:L765\n\n    Parameters\n    ----------\n    data : NDArray\n        An n-dimensional input array.\n    mode : {'constant', 'edge', 'reflect'}, required\n        Padding type to use. \"constant\" pads with `constant_value` \"edge\" pads using the edge values of the input array \"reflect\" pads by reflecting values with respect to the edges.\n    pad_width : Shape(tuple), required\n        Widths of the padding regions applied to the edges of each axis. It is a tuple of integer padding widths for each axis of the format ``(before_1, after_1, ... , before_N, after_N)``. It should be of length ``2*N`` where ``N`` is the number of dimensions of the array.This is equivalent to pad_width in numpy.pad, but flattened.\n    constant_value : double, optional, default=0\n        The value used for padding when `mode` is \"constant\".\n\n    out : NDArray, optional\n        The output NDArray to hold the result.\n\n    Returns\n    -------\n    out : NDArray or list of NDArrays\n        The output of this function.\n    \"\"\"\n    return (0,)\n",
  "sig": "pad(data|An n-dimensional input array)(mode|Padding type to use. “constant” pads with constant_value “edge” pads using the edge values of the input array “reflect” pads by reflecting values with respect to the edges.)(pad_width|Widths of the padding regions applied to the edges of each axis. It is a tuple of integer padding widths for each axis of the format (before_1, after_1, ... , before_N, after_N). It should be of length 2*N where N is the number of dimensions of the array.This is equivalent to pad_width in numpy.pad, but flattened)(constant_value|The value used for padding when mode is “constant”)(out|The output NDArray to hold the result)(name|NULL)",
  "dst": "Pads an input array with a constant or edge values of the array.    .. note:: `Pad` is deprecated. Use `pad` instead.    .. note:: Current implementation only supports 4D and 5D input arrays with padding applied       only on axes 1, 2 and 3. Expects axes 4 and 5 in `pad_width` to be zero.    This operation pads an input array with either a `constant_value` or edge values    along each axis of the input array. The amount of padding is specified by `pad_width`.    `pad_width` is a tuple of integer padding widths for each axis of the format    ``(before_1, after_1, ... , before_N, after_N)``. The `pad_width` should be of length ``2*N``    where ``N`` is the number of dimensions of the array.    For dimension ``N`` of the input array, ``before_N`` and ``after_N`` indicates how many values    to add before and after the elements of the array along dimension ``N``.    The widths of the higher two dimensions ``before_1``, ``after_1``, ``before_2``,    ``after_2`` must be 0.    Example::       x = [[[[  1.   2.   3.]              [  4.   5.   6.]]             [[  7.   8.   9.]              [ 10.  11.  12.]]]            [[[ 11.  12.  13.]              [ 14.  15.  16.]]             [[ 17.  18.  19.]              [ 20.  21.  22.]]]]             [[[[  1.   1.   2.   3.   3.]                [  1.   1.   2.   3.   3.]                [  4.   4.   5.   6.   6.]                [  4.   4.   5.   6.   6.]]               [[  7.   7.   8.   9.   9.]                [  7.   7.   8.   9.   9.]                [ 10.  10.  11.  12.  12.]                [ 10.  10.  11.  12.  12.]]]              [[[ 11.  11.  12.  13.  13.]                [ 11.  11.  12.  13.  13.]                [ 14.  14.  15.  16.  16.]                [ 14.  14.  15.  16.  16.]]               [[ 17.  17.  18.  19.  19.]                [ 17.  17.  18.  19.  19.]                [ 20.  20.  21.  22.  22.]                [ 20.  20.  21.  22.  22.]]]]             [[[[  0.   0.   0.   0.   0.]                [  0.   1.   2.   3.   0.]                [  0.   4.   5.   6.   0.]                [  0.   0.   0.   0.   0.]]               [[  0.   0.   0.   0.   0.]                [  0.   7.   8.   9.   0.]                [  0.  10.  11.  12.   0.]                [  0.   0.   0.   0.   0.]]]              [[[  0.   0.   0.   0.   0.]                [  0.  11.  12.  13.   0.]                [  0.  14.  15.  16.   0.]                [  0.   0.   0.   0.   0.]]               [[  0.   0.   0.   0.   0.]                [  0.  17.  18.  19.   0.]                [  0.  20.  21.  22.   0.]                [  0.   0.   0.   0.   0.]]]]    Defined in ../src/operator/pad.cc:L765    Parameters    ----------    data : NDArray        An n-dimensional input array.    mode : {'constant', 'edge', 'reflect'}, required        Padding type to use. \"constant\" pads with `constant_value` \"edge\" pads using the edge values of the input array \"reflect\" pads by reflecting values with respect to the edges.        Widths of the padding regions applied to the edges of each axis. It is a tuple of integer padding widths for each axis of the format ``(before_1, after_1, ... , before_N, after_N)``. It should be of length ``2*N`` where ``N`` is the number of dimensions of the array.This is equivalent to pad_width in numpy.pad, but flattened.    constant_value : double, optional, default=0        The value used for padding when `mode` is \"constant\".    out : NDArray, optional        The output NDArray to hold the result.    Returns    -------    out : NDArray or list of NDArrays        The output of this function."
 },
 "mxnet.gluon.ParameterDict.load": {
  "code": "    def load(self, filename, ctx=None, allow_missing=False,\n             ignore_extra=False, restore_prefix='', cast_dtype=False,\n             dtype_source=\"current\"):\n        \"\"\"Load parameters from file.\n\n        Parameters\n        ----------\n        filename : str\n            Path to parameter file.\n        ctx : Context or list of Context\n            Context(s) initialize loaded parameters on.\n        allow_missing : bool, default False\n            Whether to silently skip loading parameters not represents in the file.\n        ignore_extra : bool, default False\n            Whether to silently ignore parameters from the file that are not\n            present in this ParameterDict.\n        restore_prefix : str, default ''\n            prepend prefix to names of stored parameters before loading.\n        cast_dtype : bool, default False\n            Cast the data type of the parameter\n        dtype_source : str, default 'current'\n            must be in {'current', 'saved'}\n            Only valid if cast_dtype=True, specify the source of the dtype for casting\n            the parameters\n        \"\"\"\n        if restore_prefix:\n            for name in self.keys():\n                assert name.startswith(restore_prefix), \\\n                    \"restore_prefix is '%s' but Parameters name '%s' does not start \" \\\n                    \"with '%s'. For more info on naming, please see \" \\\n                    \"https://mxnet.io/api/python/docs/tutorials/packages/gluon/blocks/naming.html\"%(\n                        restore_prefix, name, restore_prefix)\n        ndarray_load = ndarray.load(filename)\n        self.load_dict(ndarray_load, ctx, allow_missing,\n                       ignore_extra, restore_prefix, filename, cast_dtype, dtype_source)\n",
  "sig": "load(self|NULL)(filename|Path to parameter file)(ctx|Context(s) initialize loaded parameters on)(allow_missing|Whether to silently skip loading parameters not represents in the file)(ignore_extra|Whether to silently ignore parameters from the file that are not present in this ParameterDict)(restore_prefix|prepend prefix to names of stored parameters before loading)(cast_dtype|Cast the data type of the parameter)(dtype_source|must be in {‘current’, ‘saved’} Only valid if cast_dtype=True, specify the source of the dtype for casting the parameters)",
  "dst": "Load parameters from file.        Parameters        ----------        filename : str            Path to parameter file.        ctx : Context or list of Context            Context(s) initialize loaded parameters on.        allow_missing : bool, default False            Whether to silently skip loading parameters not represents in the file.        ignore_extra : bool, default False            Whether to silently ignore parameters from the file that are not            present in this ParameterDict.        restore_prefix : str, default ''            prepend prefix to names of stored parameters before loading.        cast_dtype : bool, default False            Cast the data type of the parameter        dtype_source : str, default 'current'            must be in {'current', 'saved'}            Only valid if cast_dtype=True, specify the source of the dtype for casting            the parameters"
 },
 "mxnet.ndarray.gen_op.space_to_depth": {
  "code": "def space_to_depth(data=None, block_size=_Null, out=None, name=None, **kwargs):\n    r\"\"\"Rearranges(permutes) blocks of spatial data into depth.\n    Similar to ONNX SpaceToDepth operator:\n    https://github.com/onnx/onnx/blob/master/docs/Operators.md#SpaceToDepth\n    The output is a new tensor where the values from height and width dimension are\n    moved to the depth dimension. The reverse of this operation is ``depth_to_space``.\n    .. math::\n        \\begin{gather*}\n        x \\prime = reshape(x, [N, C, H / block\\_size, block\\_size, W / block\\_size, block\\_size]) \\\\\n        x \\prime \\prime = transpose(x \\prime, [0, 3, 5, 1, 2, 4]) \\\\\n        y = reshape(x \\prime \\prime, [N, C * (block\\_size ^ 2), H / block\\_size, W / block\\_size])\n        \\end{gather*}\n    where :math:`x` is an input tensor with default layout as :math:`[N, C, H, W]`: [batch, channels, height, width]\n    and :math:`y` is the output tensor of layout :math:`[N, C * (block\\_size ^ 2), H / block\\_size, W / block\\_size]`\n    Example::\n      x = [[[[0, 6, 1, 7, 2, 8],\n             [12, 18, 13, 19, 14, 20],\n             [3, 9, 4, 10, 5, 11],\n             [15, 21, 16, 22, 17, 23]]]]\n      space_to_depth(x, 2) = [[[[0, 1, 2],\n                                [3, 4, 5]],\n                               [[6, 7, 8],\n                                [9, 10, 11]],\n                               [[12, 13, 14],\n                                [15, 16, 17]],\n                               [[18, 19, 20],\n                                [21, 22, 23]]]]\n\n\n    Defined in ../src/operator/tensor/matrix_op.cc:L1018\n\n    Parameters\n    ----------\n    data : NDArray\n        Input ndarray\n    block_size : int, required\n        Blocks of [block_size. block_size] are moved\n\n    out : NDArray, optional\n        The output NDArray to hold the result.\n\n    Returns\n    -------\n    out : NDArray or list of NDArrays\n        The output of this function.\n    \"\"\"\n    return (0,)\n",
  "sig": "space_to_depth(data|Input ndarray)(block_size|Blocks of [block_size. block_size] are moved)(out|The output NDArray to hold the result)(name|NULL)",
  "dst": "Rearranges(permutes) blocks of spatial data into depth.    Similar to ONNX SpaceToDepth operator:    https://github.com/onnx/onnx/blob/master/docs/Operators.md#SpaceToDepth    The output is a new tensor where the values from height and width dimension are    moved to the depth dimension. The reverse of this operation is ``depth_to_space``.    .. math::        \\begin{gather*}        x \\prime = reshape(x, [N, C, H / block\\_size, block\\_size, W / block\\_size, block\\_size]) \\\\        x \\prime \\prime = transpose(x \\prime, [0, 3, 5, 1, 2, 4]) \\\\        y = reshape(x \\prime \\prime, [N, C * (block\\_size ^ 2), H / block\\_size, W / block\\_size])        \\end{gather*}    where :math:`x` is an input tensor with default layout as :math:`[N, C, H, W]`: [batch, channels, height, width]    and :math:`y` is the output tensor of layout :math:`[N, C * (block\\_size ^ 2), H / block\\_size, W / block\\_size]`    Example::      x = [[[[0, 6, 1, 7, 2, 8],             [12, 18, 13, 19, 14, 20],             [3, 9, 4, 10, 5, 11],             [15, 21, 16, 22, 17, 23]]]]                                [3, 4, 5]],                               [[6, 7, 8],                                [9, 10, 11]],                               [[12, 13, 14],                                [15, 16, 17]],                               [[18, 19, 20],                                [21, 22, 23]]]]    Defined in ../src/operator/tensor/matrix_op.cc:L1018    Parameters    ----------    data : NDArray        Input ndarray    block_size : int, required        Blocks of [block_size. block_size] are moved    out : NDArray, optional        The output NDArray to hold the result.    Returns    -------    out : NDArray or list of NDArrays        The output of this function."
 },
 "mxnet.optimizer.AdaDelta": {
  "code": "class AdaDelta(Optimizer):\n    \"\"\"The AdaDelta optimizer.\n\n    This class implements AdaDelta, an optimizer described in  *ADADELTA: An adaptive\n    learning rate method*, available at https://arxiv.org/abs/1212.5701.\n\n    This optimizer updates each weight by::\n\n        grad = clip(grad * rescale_grad + wd * weight, clip_gradient)\n        acc_grad = rho * acc_grad + (1. - rho) * grad * grad\n        delta = sqrt(acc_delta + epsilon) / sqrt(acc_grad + epsilon) * grad\n        acc_delta = rho * acc_delta + (1. - rho) * delta * delta\n        weight -= (delta + wd * weight)\n\n    This optimizer accepts the following parameters in addition to those accepted\n    by :class:`.Optimizer`.\n\n    Parameters\n    ----------\n    rho: float\n        Decay rate for both squared gradients and delta.\n    epsilon : float\n        Small value to avoid division by 0.\n    \"\"\"\n    def __init__(self, rho=0.90, epsilon=1e-5, **kwargs):\n        super(AdaDelta, self).__init__(**kwargs)\n        self.rho = rho\n        self.epsilon = epsilon\n\n    def create_state(self, index, weight):\n        return (zeros(weight.shape, weight.context),  # accumulated g\n                zeros(weight.shape, weight.context))  # accumulated delta\n\n    def update(self, index, weight, grad, state):\n        assert(isinstance(weight, NDArray))\n        assert(isinstance(grad, NDArray))\n        wd = self._get_wd(index)\n        self._update_count(index)\n\n        # preprocess grad\n        grad *= self.rescale_grad\n        if self.clip_gradient is not None:\n            grad = clip(grad, - self.clip_gradient, self.clip_gradient)\n\n        # accumulated g and delta initlization\n        acc_g, acc_delta = state\n\n        # update g, delta\n        acc_g[:] *= self.rho\n        acc_g[:] += (1. - self.rho) * grad * grad\n        current_delta = sqrt(acc_delta + self.epsilon) / sqrt(acc_g + self.epsilon) * grad\n        acc_delta[:] *= self.rho\n        acc_delta[:] += (1. - self.rho) * current_delta * current_delta\n\n        # update weight\n        weight[:] -= current_delta + wd * weight\n",
  "sig": "AdaDelta(rho|Decay rate for both squared gradients and delta)(epsilon|Small value to avoid division by 0)",
  "dst": "The AdaDelta optimizer.    This class implements AdaDelta, an optimizer described in  *ADADELTA: An adaptive    learning rate method*, available at https://arxiv.org/abs/1212.5701.    This optimizer updates each weight by::        grad = clip(grad * rescale_grad + wd * weight, clip_gradient)        acc_grad = rho * acc_grad + (1. - rho) * grad * grad        delta = sqrt(acc_delta + epsilon) / sqrt(acc_grad + epsilon) * grad        acc_delta = rho * acc_delta + (1. - rho) * delta * delta        weight -= (delta + wd * weight)    This optimizer accepts the following parameters in addition to those accepted    by :class:`.Optimizer`.    Parameters    ----------    rho: float        Decay rate for both squared gradients and delta.    epsilon : float        Small value to avoid division by 0."
 },
 "mxnet.optimizer.AdaGrad": {
  "code": "class AdaGrad(Optimizer):\n    \"\"\"AdaGrad optimizer.\n\n    This class implements the AdaGrad optimizer described in *Adaptive Subgradient\n    Methods for Online Learning and Stochastic Optimization*, and available at\n    http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf.\n\n    This optimizer updates each weight by::\n\n        grad = clip(grad * rescale_grad, clip_gradient)\n        history += square(grad)\n        div = grad / sqrt(history + float_stable_eps)\n        weight += (div + weight * wd) * -lr\n\n    This optimizer accepts the following parameters in addition to those accepted\n    by :class:`.Optimizer`.\n\n    See Also\n    ----------\n    :meth:`mxnet.ndarray.sparse.adagrad_update`.\n\n    Parameters\n    ----------\n    eps: float, optional\n        Initial value of the history accumulator. Avoids division by 0.\n\n    \"\"\"\n    def __init__(self, eps=1e-7, **kwargs):\n        super(AdaGrad, self).__init__(**kwargs)\n        self.float_stable_eps = eps\n\n    def create_state(self, index, weight):\n        return zeros(weight.shape, weight.context, stype=weight.stype)  # history\n\n    def update(self, index, weight, grad, state):\n        assert(isinstance(weight, NDArray))\n        assert(isinstance(grad, NDArray))\n        self._update_count(index)\n        lr = self._get_lr(index)\n        wd = self._get_wd(index)\n\n        is_sparse = grad.stype == 'row_sparse'\n        history = state\n\n        if is_sparse:\n            kwargs = {'epsilon': self.float_stable_eps,\n                      'rescale_grad': self.rescale_grad}\n            if self.clip_gradient:\n                kwargs['clip_gradient'] = self.clip_gradient\n            sparse.adagrad_update(weight, grad, history, out=weight, lr=lr, wd=wd, **kwargs)\n        else:\n            grad = grad * self.rescale_grad\n            if self.clip_gradient is not None:\n                grad = clip(grad, -self.clip_gradient, self.clip_gradient)\n            history[:] += square(grad)\n            div = grad / sqrt(history + self.float_stable_eps)\n            weight[:] += (div + weight * wd) * -lr\n",
  "sig": "AdaGrad(eps|Initial value of the history accumulator. Avoids division by 0)",
  "dst": "    This class implements the AdaGrad optimizer described in *Adaptive Subgradient    Methods for Online Learning and Stochastic Optimization*, and available at    http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf.    This optimizer updates each weight by::        grad = clip(grad * rescale_grad, clip_gradient)        history += square(grad)        div = grad / sqrt(history + float_stable_eps)        weight += (div + weight * wd) * -lr    This optimizer accepts the following parameters in addition to those accepted    by :class:`.Optimizer`.    See Also    ----------    :meth:`mxnet.ndarray.sparse.adagrad_update`.    Parameters    ----------    eps: float, optional        Initial value of the history accumulator. Avoids division by 0."
 },
 "mxnet.optimizer.Adam": {
  "code": "class Adam(Optimizer):\n    \"\"\"The Adam optimizer.\n\n    This class implements the optimizer described in *Adam: A Method for\n    Stochastic Optimization*, available at http://arxiv.org/abs/1412.6980.\n\n    If the storage types of grad is ``row_sparse``, and ``lazy_update`` is True, \\\n    **lazy updates** at step t are applied by::\n\n        for row in grad.indices:\n            rescaled_grad[row] = clip(grad[row] * rescale_grad + wd * weight[row], clip_gradient)\n            m[row] = beta1 * m[row] + (1 - beta1) * rescaled_grad[row]\n            v[row] = beta2 * v[row] + (1 - beta2) * (rescaled_grad[row]**2)\n            lr = learning_rate * sqrt(1 - beta1**t) / (1 - beta2**t)\n            w[row] = w[row] - lr * m[row] / (sqrt(v[row]) + epsilon)\n\n    The lazy update only updates the mean and var for the weights whose row_sparse\n    gradient indices appear in the current batch, rather than updating it for all indices.\n    Compared with the original update, it can provide large improvements in model training\n    throughput for some applications. However, it provides slightly different semantics than\n    the original update, and may lead to different empirical results.\n\n    Otherwise, **standard updates** at step t are applied by::\n\n        rescaled_grad = clip(grad * rescale_grad + wd * weight, clip_gradient)\n        m = beta1 * m + (1 - beta1) * rescaled_grad\n        v = beta2 * v + (1 - beta2) * (rescaled_grad**2)\n        lr = learning_rate * sqrt(1 - beta1**t) / (1 - beta2**t)\n        w = w - lr * m / (sqrt(v) + epsilon)\n\n    This optimizer accepts the following parameters in addition to those accepted\n    by :class:`.Optimizer`.\n\n    For details of the update algorithm, see :class:`~mxnet.ndarray.adam_update`.\n\n    Parameters\n    ----------\n    beta1 : float, optional\n        Exponential decay rate for the first moment estimates.\n    beta2 : float, optional\n        Exponential decay rate for the second moment estimates.\n    epsilon : float, optional\n        Small value to avoid division by 0.\n    lazy_update : bool, optional\n       Default is True. If True, lazy updates are applied \\\n       if the storage types of weight and grad are both ``row_sparse``.\n    \"\"\"\n    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8,\n                 lazy_update=True, **kwargs):\n        super(Adam, self).__init__(learning_rate=learning_rate, **kwargs)\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.epsilon = epsilon\n        self.lazy_update = lazy_update\n\n    def create_state(self, index, weight):\n        stype = weight.stype if self.lazy_update else 'default'\n        return (zeros(weight.shape, weight.context, dtype=weight.dtype,\n                      stype=stype),  # mean\n                zeros(weight.shape, weight.context, dtype=weight.dtype,\n                      stype=stype))  # variance\n\n    def update(self, index, weight, grad, state):\n        assert(isinstance(weight, NDArray))\n        assert(isinstance(grad, NDArray))\n        self._update_count(index)\n        lr = self._get_lr(index)\n        wd = self._get_wd(index)\n\n        t = self._index_update_count[index]\n        coef1 = 1. - self.beta1**t\n        coef2 = 1. - self.beta2**t\n        lr *= math.sqrt(coef2)/coef1\n\n        kwargs = {'beta1': self.beta1, 'beta2': self.beta2, 'epsilon': self.epsilon,\n                  'rescale_grad': self.rescale_grad}\n        if self.clip_gradient:\n            kwargs['clip_gradient'] = self.clip_gradient\n\n        mean, var = state\n        adam_update(weight, grad, mean, var, out=weight,\n                    lazy_update=self.lazy_update, lr=lr, wd=wd, **kwargs)\n",
  "sig": "Adam(beta1|Exponential decay rate for the first moment estimates)(beta2|Exponential decay rate for the second moment estimates)(epsilon|Small value to avoid division by 0)(lazy_update|Default is True. If True, lazy updates are applied if the storage types of weight and grad are both row_sparse)",
  "dst": "The Adam optimizer.    This class implements the optimizer described in *Adam: A Method for    Stochastic Optimization*, available at http://arxiv.org/abs/1412.6980.    If the storage types of grad is ``row_sparse``, and ``lazy_update`` is True, \\    **lazy updates** at step t are applied by::        for row in grad.indices:            rescaled_grad[row] = clip(grad[row] * rescale_grad + wd * weight[row], clip_gradient)            m[row] = beta1 * m[row] + (1 - beta1) * rescaled_grad[row]            v[row] = beta2 * v[row] + (1 - beta2) * (rescaled_grad[row]**2)            lr = learning_rate * sqrt(1 - beta1**t) / (1 - beta2**t)            w[row] = w[row] - lr * m[row] / (sqrt(v[row]) + epsilon)    The lazy update only updates the mean and var for the weights whose row_sparse    gradient indices appear in the current batch, rather than updating it for all indices.    Compared with the original update, it can provide large improvements in model training    throughput for some applications. However, it provides slightly different semantics than    the original update, and may lead to different empirical results.    Otherwise, **standard updates** at step t are applied by::        rescaled_grad = clip(grad * rescale_grad + wd * weight, clip_gradient)        m = beta1 * m + (1 - beta1) * rescaled_grad        v = beta2 * v + (1 - beta2) * (rescaled_grad**2)        lr = learning_rate * sqrt(1 - beta1**t) / (1 - beta2**t)        w = w - lr * m / (sqrt(v) + epsilon)    This optimizer accepts the following parameters in addition to those accepted    by :class:`.Optimizer`.    For details of the update algorithm, see :class:`~mxnet.ndarray.adam_update`.    Parameters    ----------    beta1 : float, optional        Exponential decay rate for the first moment estimates.    beta2 : float, optional        Exponential decay rate for the second moment estimates.    epsilon : float, optional        Small value to avoid division by 0.    lazy_update : bool, optional       Default is True. If True, lazy updates are applied \\       if the storage types of weight and grad are both ``row_sparse``."
 },
 "mxnet.optimizer.RMSProp": {
  "code": "class RMSProp(Optimizer):\n    \"\"\"The RMSProp optimizer.\n\n    Two versions of RMSProp are implemented:\n\n    If ``centered=False``, we follow\n    http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf by\n    Tieleman & Hinton, 2012.\n    For details of the update algorithm see :class:`~mxnet.ndarray.rmsprop_update`.\n\n    If ``centered=True``, we follow http://arxiv.org/pdf/1308.0850v5.pdf (38)-(45)\n    by Alex Graves, 2013.\n    For details of the update algorithm see :class:`~mxnet.ndarray.rmspropalex_update`.\n\n    This optimizer accepts the following parameters in addition to those accepted\n    by :class:`.Optimizer`.\n\n    Parameters\n    ----------\n    gamma1: float, optional\n        A decay factor of moving average over past squared gradient.\n    gamma2: float, optional\n        A \"momentum\" factor. Only used if `centered`=``True``.\n    epsilon : float, optional\n        Small value to avoid division by 0.\n    centered : bool, optional\n        Flag to control which version of RMSProp to use.::\n\n            True: will use Graves's version of `RMSProp`,\n            False: will use Tieleman & Hinton's version of `RMSProp`.\n\n    clip_weights : float, optional\n        Clips weights into range ``[-clip_weights, clip_weights]``.\n    \"\"\"\n    def __init__(self, learning_rate=0.001, gamma1=0.9, gamma2=0.9,\n                 epsilon=1e-8, centered=False, clip_weights=None, **kwargs):\n        super(RMSProp, self).__init__(learning_rate=learning_rate, **kwargs)\n        self.gamma1 = gamma1\n        self.gamma2 = gamma2\n        self.centered = centered\n        self.epsilon = epsilon\n        self.clip_weights = clip_weights\n\n    def create_state(self, index, weight):\n        if self.centered:\n            return (\n                zeros(weight.shape, weight.context, stype=weight.stype),  # n\n                zeros(weight.shape, weight.context, stype=weight.stype),  # g\n                zeros(weight.shape, weight.context, stype=weight.stype))  # delta\n        else:\n            return (zeros(weight.shape, weight.context, stype=weight.stype),)  # n\n\n    def update(self, index, weight, grad, state):\n        assert(isinstance(weight, NDArray))\n        assert(isinstance(grad, NDArray))\n        self._update_count(index)\n        lr = self._get_lr(index)\n        wd = self._get_wd(index)\n\n        kwargs = {'gamma1': self.gamma1, 'epsilon': self.epsilon,\n                  'rescale_grad': self.rescale_grad}\n        if self.centered:\n            kwargs['gamma2'] = self.gamma2\n        if self.clip_gradient:\n            kwargs['clip_gradient'] = self.clip_gradient\n        if self.clip_weights:\n            kwargs['clip_weights'] = self.clip_weights\n\n        if not self.centered:\n            (n, ) = state\n            rmsprop_update(\n                weight, grad, n, out=weight, lr=lr, wd=wd, **kwargs)\n        else:\n            n, g, delta = state\n            rmspropalex_update(weight, grad, n, g, delta, out=weight,\n                               lr=lr, wd=wd, **kwargs)\n",
  "sig": "RMSProp(gamma1|A decay factor of moving average over past squared gradient)(gamma2|A “momentum” factor. Only used if centered`=``True`)(epsilon|Small value to avoid division by 0)(centered|Flag to control which version of RMSProp to use)(clip_weights|Clips weights into range [-clip_weights, clip_weights])",
  "dst": "The RMSProp optimizer.    Two versions of RMSProp are implemented:    If ``centered=False``, we follow    http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf by    Tieleman & Hinton, 2012.    For details of the update algorithm see :class:`~mxnet.ndarray.rmsprop_update`.    If ``centered=True``, we follow http://arxiv.org/pdf/1308.0850v5.pdf (38)-(45)    by Alex Graves, 2013.    For details of the update algorithm see :class:`~mxnet.ndarray.rmspropalex_update`.    This optimizer accepts the following parameters in addition to those accepted    by :class:`.Optimizer`.    Parameters    ----------    gamma1: float, optional        A decay factor of moving average over past squared gradient.    gamma2: float, optional        A \"momentum\" factor. Only used if `centered`=``True``.    epsilon : float, optional        Small value to avoid division by 0.    centered : bool, optional        Flag to control which version of RMSProp to use.::            True: will use Graves's version of `RMSProp`,            False: will use Tieleman & Hinton's version of `RMSProp`.    clip_weights : float, optional        Clips weights into range ``[-clip_weights, clip_weights]``."
 },
 "mxnet.autograd.grad": {
  "code": "def grad(heads, variables, head_grads=None, retain_graph=None, create_graph=False,\n         train_mode=True):  #pylint: disable=redefined-outer-name\n    \"\"\"Compute the gradients of heads w.r.t variables. Gradients will be\n    returned as new NDArrays instead of stored into `variable.grad`.\n    Supports recording gradient graph for computing higher order gradients.\n\n    .. note::\n\n      Currently only a very limited set of operators support higher order \\\n      gradients.\n\n    Parameters\n    ----------\n    heads: NDArray or list of NDArray\n        Output NDArray(s)\n    variables: NDArray or list of NDArray\n        Input variables to compute gradients for.\n    head_grads: NDArray or list of NDArray or None\n        Gradients with respect to heads.\n    retain_graph: bool\n        Whether to keep computation graph to differentiate again, instead\n        of clearing history and release memory. Defaults to the same value\n        as create_graph.\n    create_graph: bool\n        Whether to record gradient graph for computing higher order\n    train_mode: bool, optional\n        Whether to do backward for training or prediction.\n\n    Returns\n    -------\n    NDArray or list of NDArray:\n        Gradients with respect to variables.\n\n    Examples\n    --------\n    >>> x = mx.nd.ones((1,))\n    >>> x.attach_grad()\n    >>> with mx.autograd.record():\n    ...     z = mx.nd.elemwise_add(mx.nd.exp(x), x)\n    >>> dx = mx.autograd.grad(z, [x], create_graph=True)\n    >>> print(dx)\n    [\n    [ 3.71828175]\n    <NDArray 1 @cpu(0)>]\n    \"\"\"\n    head_handles, hgrad_handles = _parse_head(heads, head_grads)\n\n    if isinstance(variables, NDArray):\n        variables = [variables]\n    else:\n        assert len(variables), \"variables cannot be an empty list.\"\n    var_handles = c_handle_array(variables)\n\n    retain_graph = retain_graph if retain_graph is not None else create_graph\n    grad_vars = ctypes.POINTER(NDArrayHandle)()\n    grad_stypes = ctypes.POINTER(ctypes.c_int)()\n\n    check_call(_LIB.MXAutogradBackwardEx(\n        len(head_handles),\n        head_handles,\n        hgrad_handles,\n        len(var_handles),\n        var_handles,\n        ctypes.c_int(retain_graph),\n        ctypes.c_int(create_graph),\n        ctypes.c_int(train_mode),\n        ctypes.byref(grad_vars),\n        ctypes.byref(grad_stypes)))\n\n    ret = [_ndarray_cls(ctypes.cast(grad_vars[i], NDArrayHandle),\n                        stype=grad_stypes[i])\n           for i in range(len(var_handles))]\n    if isinstance(variables, NDArray):\n        return ret[0]\n    return ret\n",
  "sig": "grad(heads|Output NDArray(s))(variables|Input variables to compute gradients for)(head_grads|Gradients with respect to heads)(retain_graph|Whether to keep computation graph to differentiate again, instead of clearing history and release memory. Defaults to the same value as create_graph)(create_graph|Whether to record gradient graph for computing higher order)(train_mode|Whether to do backward for training or prediction)",
  "dst": "Compute the gradients of heads w.r.t variables. Gradients will be    returned as new NDArrays instead of stored into `variable.grad`.    Supports recording gradient graph for computing higher order gradients.    .. note::      Currently only a very limited set of operators support higher order \\    Parameters    ----------    heads: NDArray or list of NDArray        Output NDArray(s)    variables: NDArray or list of NDArray        Input variables to compute gradients for.    head_grads: NDArray or list of NDArray or None        Gradients with respect to heads.    retain_graph: bool        Whether to keep computation graph to differentiate again, instead        of clearing history and release memory. Defaults to the same value        as create_graph.    create_graph: bool        Whether to record gradient graph for computing higher order    train_mode: bool, optional        Whether to do backward for training or prediction.    Returns    -------    NDArray or list of NDArray:        Gradients with respect to variables.    Examples    --------    >>> x = mx.nd.ones((1,))    >>> x.attach_grad()    >>> with mx.autograd.record():    ...     z = mx.nd.elemwise_add(mx.nd.exp(x), x)    >>> dx = mx.autograd.grad(z, [x], create_graph=True)    >>> print(dx)    [    [ 3.71828175]    <NDArray 1 @cpu(0)>]"
 },
 "mxnet.gluon.contrib.nn.Identity": {
  "code": "class Identity(HybridBlock):\n    \"\"\"Block that passes through the input directly.\n\n    This block can be used in conjunction with HybridConcurrent\n    block for residual connection.\n\n    Example::\n\n        net = HybridConcurrent()\n        # use net's name_scope to give child Blocks appropriate names.\n        with net.name_scope():\n            net.add(nn.Dense(10, activation='relu'))\n            net.add(nn.Dense(20))\n            net.add(Identity())\n    \"\"\"\n    def __init__(self, prefix=None, params=None):\n        super(Identity, self).__init__(prefix=prefix, params=params)\n\n    def hybrid_forward(self, F, x):\n        return x\n",
  "sig": "Identity()",
  "dst": "Block that passes through the input directly.    This block can be used in conjunction with HybridConcurrent    block for residual connection.    Example::        net = HybridConcurrent()        # use net's name_scope to give child Blocks appropriate names.        with net.name_scope():            net.add(nn.Dense(10, activation='relu'))            net.add(nn.Dense(20))            net.add(Identity())"
 },
 "mxnet.gluon.nn.ELU": {
  "code": "class ELU(HybridBlock):\n    r\"\"\"\n    Exponential Linear Unit (ELU)\n        \"Fast and Accurate Deep Network Learning by Exponential Linear Units\", Clevert et al, 2016\n        https://arxiv.org/abs/1511.07289\n        Published as a conference paper at ICLR 2016\n\n    Parameters\n    ----------\n    alpha : float\n        The alpha parameter as described by Clevert et al, 2016\n\n\n    Inputs:\n        - **data**: input tensor with arbitrary shape.\n\n    Outputs:\n        - **out**: output tensor with the same shape as `data`.\n    \"\"\"\n\n    def __init__(self, alpha=1.0, **kwargs):\n        super(ELU, self).__init__(**kwargs)\n        self._alpha = alpha\n\n    def hybrid_forward(self, F, x):\n        leaky_relu = F.npx.leaky_relu if is_np_array() else F.LeakyReLU\n        return leaky_relu(x, act_type='elu', slope=self._alpha)\n",
  "sig": "ELU(alpha|The alpha parameter as described by Clevert et al, 2016)",
  "dst": "    Exponential Linear Unit (ELU)        \"Fast and Accurate Deep Network Learning by Exponential Linear Units\", Clevert et al, 2016        https://arxiv.org/abs/1511.07289        Published as a conference paper at ICLR 2016    Parameters    ----------    alpha : float        The alpha parameter as described by Clevert et al, 2016    Inputs:        - **data**: input tensor with arbitrary shape.    Outputs:        - **out**: output tensor with the same shape as `data`."
 },
 "mxnet.ndarray.gen_op.hard_sigmoid": {
  "code": "def hard_sigmoid(data=None, alpha=_Null, beta=_Null, out=None, name=None, **kwargs):\n    r\"\"\"Computes hard sigmoid of x element-wise.\n\n    .. math::\n       y = max(0, min(1, alpha * x + beta))\n\n\n\n    Defined in ../src/operator/tensor/elemwise_unary_op_basic.cc:L161\n\n    Parameters\n    ----------\n    data : NDArray\n        The input array.\n    alpha : float, optional, default=0.200000003\n        Slope of hard sigmoid\n    beta : float, optional, default=0.5\n        Bias of hard sigmoid.\n\n    out : NDArray, optional\n        The output NDArray to hold the result.\n\n    Returns\n    -------\n    out : NDArray or list of NDArrays\n        The output of this function.\n    \"\"\"\n    return (0,)\n",
  "sig": "hard_sigmoid(data|The input array)(alpha| Slope of hard sigmoid)(beta|Bias of hard sigmoid)(out|The output NDArray to hold the result)(name|NULL)",
  "dst": "Computes hard sigmoid of x element-wise.    .. math::       y = max(0, min(1, alpha * x + beta))    Defined in ../src/operator/tensor/elemwise_unary_op_basic.cc:L161    Parameters    ----------    data : NDArray        The input array.    alpha : float, optional, default=0.200000003        Slope of hard sigmoid    beta : float, optional, default=0.5        Bias of hard sigmoid.    out : NDArray, optional        The output NDArray to hold the result.    Returns    -------    out : NDArray or list of NDArrays        The output of this function."
 },
 "mxnet.gluon.nn.SELU": {
  "code": "class SELU(HybridBlock):\n    r\"\"\"\n    Scaled Exponential Linear Unit (SELU)\n        \"Self-Normalizing Neural Networks\", Klambauer et al, 2017\n        https://arxiv.org/abs/1706.02515\n\n\n    Inputs:\n        - **data**: input tensor with arbitrary shape.\n\n    Outputs:\n        - **out**: output tensor with the same shape as `data`.\n    \"\"\"\n    def __init__(self, **kwargs):\n        super(SELU, self).__init__(**kwargs)\n\n    def hybrid_forward(self, F, x):\n        leaky_relu = F.npx.leaky_relu if is_np_array() else F.LeakyReLU\n        return leaky_relu(x, act_type='selu', name='fwd')\n",
  "sig": "SELU()",
  "dst": "    Scaled Exponential Linear Unit (SELU)        \"Self-Normalizing Neural Networks\", Klambauer et al, 2017        https://arxiv.org/abs/1706.02515    Inputs:        - **data**: input tensor with arbitrary shape.    Outputs:        - **out**: output tensor with the same shape as `data`."
 },
 "mxnet.ndarray.gen_op.dot": {
  "code": "def dot(lhs=None, rhs=None, transpose_a=_Null, transpose_b=_Null, forward_stype=_Null, out=None, name=None, **kwargs):\n    r\"\"\"Dot product of two arrays.\n\n    ``dot``'s behavior depends on the input array dimensions:\n\n    - 1-D arrays: inner product of vectors\n    - 2-D arrays: matrix multiplication\n    - N-D arrays: a sum product over the last axis of the first input and the first\n      axis of the second input\n\n      For example, given 3-D ``x`` with shape `(n,m,k)` and ``y`` with shape `(k,r,s)`, the\n      result array will have shape `(n,m,r,s)`. It is computed by::\n\n        dot(x,y)[i,j,a,b] = sum(x[i,j,:]*y[:,a,b])\n\n      Example::\n\n        x = reshape([0,1,2,3,4,5,6,7], shape=(2,2,2))\n        y = reshape([7,6,5,4,3,2,1,0], shape=(2,2,2))\n        dot(x,y)[0,0,1,1] = 0\n        sum(x[0,0,:]*y[:,1,1]) = 0\n\n    The storage type of ``dot`` output depends on storage types of inputs, transpose option and\n    forward_stype option for output storage type. Implemented sparse operations include:\n\n    - dot(default, default, transpose_a=True/False, transpose_b=True/False) = default\n    - dot(csr, default, transpose_a=True) = default\n    - dot(csr, default, transpose_a=True) = row_sparse\n    - dot(csr, default) = default\n    - dot(csr, row_sparse) = default\n    - dot(default, csr) = csr (CPU only)\n    - dot(default, csr, forward_stype='default') = default\n    - dot(default, csr, transpose_b=True, forward_stype='default') = default\n\n    If the combination of input storage types and forward_stype does not match any of the\n    above patterns, ``dot`` will fallback and generate output with default storage.\n\n    .. Note::\n\n        If the storage type of the lhs is \"csr\", the storage type of gradient w.r.t rhs will be\n        \"row_sparse\". Only a subset of optimizers support sparse gradients, including SGD, AdaGrad\n        and Adam. Note that by default lazy updates is turned on, which may perform differently\n        from standard updates. For more details, please check the Optimization API at:\n        https://mxnet.incubator.apache.org/api/python/optimization/optimization.html\n\n\n\n    Defined in ../src/operator/tensor/dot.cc:L77\n\n    Parameters\n    ----------\n    lhs : NDArray\n        The first input\n    rhs : NDArray\n        The second input\n    transpose_a : boolean, optional, default=0\n        If true then transpose the first input before dot.\n    transpose_b : boolean, optional, default=0\n        If true then transpose the second input before dot.\n    forward_stype : {None, 'csr', 'default', 'row_sparse'},optional, default='None'\n        The desired storage type of the forward output given by user, if thecombination of input storage types and this hint does not matchany implemented ones, the dot operator will perform fallback operationand still produce an output of the desired storage type.\n\n    out : NDArray, optional\n        The output NDArray to hold the result.\n\n    Returns\n    -------\n    out : NDArray or list of NDArrays\n        The output of this function.\n    \"\"\"\n    return (0,)\n",
  "sig": "dot(lhs|The first input)(rhs|The second input)(transpose_a|If true then transpose the first input before dot)(transpose_b| If true then transpose the second input before dot)(forward_stype|The desired storage type of the forward output given by user, if thecombination of input storage types and this hint does not matchany implemented ones, the dot operator will perform fallback operationand still produce an output of the desired storage type)(out|The output NDArray to hold the result)(name|NULL)",
  "dst": "Dot product of two arrays.    ``dot``'s behavior depends on the input array dimensions:    - 1-D arrays: inner product of vectors    - 2-D arrays: matrix multiplication    - N-D arrays: a sum product over the last axis of the first input and the first      axis of the second input      For example, given 3-D ``x`` with shape `(n,m,k)` and ``y`` with shape `(k,r,s)`, the      result array will have shape `(n,m,r,s)`. It is computed by::      Example::        x = reshape([0,1,2,3,4,5,6,7], shape=(2,2,2))        y = reshape([7,6,5,4,3,2,1,0], shape=(2,2,2))        sum(x[0,0,:]*y[:,1,1]) = 0    The storage type of ``dot`` output depends on storage types of inputs, transpose option and    forward_stype option for output storage type. Implemented sparse operations include:    - dot(default, default, transpose_a=True/False, transpose_b=True/False) = default    - dot(csr, default, transpose_a=True) = default    - dot(csr, default, transpose_a=True) = row_sparse    - dot(csr, default) = default    - dot(csr, row_sparse) = default    - dot(default, csr) = csr (CPU only)    - dot(default, csr, forward_stype='default') = default    - dot(default, csr, transpose_b=True, forward_stype='default') = default    If the combination of input storage types and forward_stype does not match any of the    above patterns, ``dot`` will fallback and generate output with default storage.    .. Note::        If the storage type of the lhs is \"csr\", the storage type of gradient w.r.t rhs will be        \"row_sparse\". Only a subset of optimizers support sparse gradients, including SGD, AdaGrad        and Adam. Note that by default lazy updates is turned on, which may perform differently        from standard updates. For more details, please check the Optimization API at:        https://mxnet.incubator.apache.org/api/python/optimization/optimization.html    Defined in ../src/operator/tensor/dot.cc:L77    Parameters    ----------    lhs : NDArray        The first input    rhs : NDArray        The second input    transpose_a : boolean, optional, default=0        If true then transpose the first input before dot.    transpose_b : boolean, optional, default=0        If true then transpose the second input before dot.    forward_stype : {None, 'csr', 'default', 'row_sparse'},optional, default='None'        The desired storage type of the forward output given by user, if thecombination of input storage types and this hint does not matchany implemented ones, the dot operator will perform fallback operationand still produce an output of the desired storage type.    out : NDArray, optional        The output NDArray to hold the result.    Returns    -------    out : NDArray or list of NDArrays        The output of this function."
 },
 "mxnet.gluon.nn.Embedding": {
  "code": "class Embedding(HybridBlock):\n    r\"\"\"Turns non-negative integers (indexes/tokens) into dense vectors\n    of fixed size. eg. [4, 20] -> [[0.25, 0.1], [0.6, -0.2]]\n\n    .. note::\n        if `sparse_grad` is set to True, the gradient w.r.t weight will be\n        sparse. Only a subset of optimizers support sparse gradients, including SGD,\n        AdaGrad and Adam. By default lazy updates is turned on, which may perform\n        differently from standard updates. For more details, please check the\n        Optimization API at:\n        https://mxnet.incubator.apache.org/api/python/optimization/optimization.html\n\n    Parameters\n    ----------\n    input_dim : int\n        Size of the vocabulary, i.e. maximum integer index + 1.\n    output_dim : int\n        Dimension of the dense embedding.\n    dtype : str or np.dtype, default 'float32'\n        Data type of output embeddings.\n    weight_initializer : Initializer\n        Initializer for the `embeddings` matrix.\n    sparse_grad: bool\n        If True, gradient w.r.t. weight will be a 'row_sparse' NDArray.\n\n    Inputs:\n        - **data**: (N-1)-D tensor with shape: `(x1, x2, ..., xN-1)`.\n\n    Output:\n        - **out**: N-D tensor with shape: `(x1, x2, ..., xN-1, output_dim)`.\n    \"\"\"\n    def __init__(self, input_dim, output_dim, dtype='float32',\n                 weight_initializer=None, sparse_grad=False, **kwargs):\n        super(Embedding, self).__init__(**kwargs)\n        grad_stype = 'row_sparse' if sparse_grad else 'default'\n        self._kwargs = {'input_dim': input_dim, 'output_dim': output_dim,\n                        'dtype': dtype, 'sparse_grad': sparse_grad}\n        self.weight = self.params.get('weight', shape=(input_dim, output_dim),\n                                      init=weight_initializer, dtype=dtype,\n                                      allow_deferred_init=True, grad_stype=grad_stype)\n\n    def hybrid_forward(self, F, x, weight):\n        embedding = F.npx.embedding if is_np_array() else F.Embedding\n        return embedding(x, weight, name='fwd', **self._kwargs)\n\n    def __repr__(self):\n        s = '{block_name}({input_dim} -> {output_dim}, {dtype})'\n        return s.format(block_name=self.__class__.__name__,\n                        **self._kwargs)\n",
  "sig": "Embedding(input_dim|Size of the vocabulary, i.e. maximum integer index + 1)(output_dim|Dimension of the dense embedding)(dtype|Data type of output embeddings)(weight_initializer|Initializer for the embeddings matrix)(sparse_grad|If True, gradient w.r.t. weight will be a ‘row_sparse’ NDArray)(Inputs|data: (N-1)-D tensor with shape: (x1, x2, …, xN-1))(Output|out: N-D tensor with shape: (x1, x2, …, xN-1, output_dim))",
  "dst": "Turns non-negative integers (indexes/tokens) into dense vectors    of fixed size. eg. [4, 20] -> [[0.25, 0.1], [0.6, -0.2]]    .. note::        if `sparse_grad` is set to True, the gradient w.r.t weight will be        sparse. Only a subset of optimizers support sparse gradients, including SGD,        AdaGrad and Adam. By default lazy updates is turned on, which may perform        differently from standard updates. For more details, please check the        Optimization API at:        https://mxnet.incubator.apache.org/api/python/optimization/optimization.html    Parameters    ----------    input_dim : int        Size of the vocabulary, i.e. maximum integer index + 1.    output_dim : int        Dimension of the dense embedding.    dtype : str or np.dtype, default 'float32'        Data type of output embeddings.    weight_initializer : Initializer        Initializer for the `embeddings` matrix.    sparse_grad: bool        If True, gradient w.r.t. weight will be a 'row_sparse' NDArray.    Inputs:        - **data**: (N-1)-D tensor with shape: `(x1, x2, ..., xN-1)`.    Output:        - **out**: N-D tensor with shape: `(x1, x2, ..., xN-1, output_dim)`."
 },
 "mxnet.gluon.nn.LayerNorm": {
  "code": "class LayerNorm(HybridBlock):\n    r\"\"\"\n    Applies layer normalization to the n-dimensional input array.\n    This operator takes an n-dimensional input array and normalizes\n    the input using the given axis:\n\n    .. math::\n\n      out = \\frac{x - mean[data, axis]}{ \\sqrt{Var[data, axis] + \\epsilon}} * gamma + beta\n\n    Parameters\n    ----------\n    axis : int, default -1\n        The axis that should be normalized. This is typically the axis of the channels.\n    epsilon: float, default 1e-5\n        Small float added to variance to avoid dividing by zero.\n    center: bool, default True\n        If True, add offset of `beta` to normalized tensor.\n        If False, `beta` is ignored.\n    scale: bool, default True\n        If True, multiply by `gamma`. If False, `gamma` is not used.\n    beta_initializer: str or `Initializer`, default 'zeros'\n        Initializer for the beta weight.\n    gamma_initializer: str or `Initializer`, default 'ones'\n        Initializer for the gamma weight.\n    in_channels : int, default 0\n        Number of channels (feature maps) in input data. If not specified,\n        initialization will be deferred to the first time `forward` is called\n        and `in_channels` will be inferred from the shape of input data.\n\n\n    Inputs:\n        - **data**: input tensor with arbitrary shape.\n\n    Outputs:\n        - **out**: output tensor with the same shape as `data`.\n\n    References\n    ----------\n        `Layer Normalization\n        <https://arxiv.org/pdf/1607.06450.pdf>`_\n\n    Examples\n    --------\n    >>> # Input of shape (2, 5)\n    >>> x = mx.nd.array([[1, 2, 3, 4, 5], [1, 1, 2, 2, 2]])\n    >>> # Layer normalization is calculated with the above formula\n    >>> layer = LayerNorm()\n    >>> layer.initialize(ctx=mx.cpu(0))\n    >>> layer(x)\n    [[-1.41421    -0.707105    0.          0.707105    1.41421   ]\n     [-1.2247195  -1.2247195   0.81647956  0.81647956  0.81647956]]\n    <NDArray 2x5 @cpu(0)>\n    \"\"\"\n    def __init__(self, axis=-1, epsilon=1e-5, center=True, scale=True,\n                 beta_initializer='zeros', gamma_initializer='ones',\n                 in_channels=0, prefix=None, params=None):\n        super(LayerNorm, self).__init__(prefix=prefix, params=params)\n        self._kwargs = {'eps': epsilon, 'axis': axis, 'center': center, 'scale': scale}\n        self._axis = axis\n        self._epsilon = epsilon\n        self._center = center\n        self._scale = scale\n        self.gamma = self.params.get('gamma', grad_req='write' if scale else 'null',\n                                     shape=(in_channels,), init=gamma_initializer,\n                                     allow_deferred_init=True)\n        self.beta = self.params.get('beta', grad_req='write' if center else 'null',\n                                    shape=(in_channels,), init=beta_initializer,\n                                    allow_deferred_init=True)\n\n    def hybrid_forward(self, F, data, gamma, beta):\n        layer_norm = F.npx.layer_norm if is_np_array() else F.LayerNorm\n        return layer_norm(data, gamma=gamma, beta=beta, axis=self._axis, eps=self._epsilon)\n\n    def __repr__(self):\n        s = '{name}({content}'\n        in_channels = self.gamma.shape[0]\n        s += ', in_channels={0}'.format(in_channels)\n        s += ')'\n        return s.format(name=self.__class__.__name__,\n                        content=', '.join(['='.join([k, v.__repr__()])\n                                           for k, v in self._kwargs.items()]))\n",
  "sig": "LayerNorm(axis|The axis that should be normalized. This is typically the axis of the channels)(epsilon|Small float added to variance to avoid dividing by zero)(center|If True, add offset of beta to normalized tensor. If False, beta is ignored)(scale| If True, multiply by gamma. If False, gamma is not used)(beta_initializer|Initializer for the beta weight)(gamma_initializer|Initializer for the gamma weight)(in_channels|Number of channels (feature maps) in input data. If not specified, initialization will be deferred to the first time forward is called and in_channels will be inferred from the shape of input data)",
  "dst": "    Applies layer normalization to the n-dimensional input array.    This operator takes an n-dimensional input array and normalizes    the input using the given axis:    .. math::      out = \\frac{x - mean[data, axis]}{ \\sqrt{Var[data, axis] + \\epsilon}} * gamma + beta    Parameters    ----------    axis : int, default -1        The axis that should be normalized. This is typically the axis of the channels.    epsilon: float, default 1e-5        Small float added to variance to avoid dividing by zero.    center: bool, default True        If True, add offset of `beta` to normalized tensor.        If False, `beta` is ignored.    scale: bool, default True        If True, multiply by `gamma`. If False, `gamma` is not used.    beta_initializer: str or `Initializer`, default 'zeros'        Initializer for the beta weight.    gamma_initializer: str or `Initializer`, default 'ones'        Initializer for the gamma weight.    in_channels : int, default 0        Number of channels (feature maps) in input data. If not specified,        initialization will be deferred to the first time `forward` is called        and `in_channels` will be inferred from the shape of input data.    Inputs:        - **data**: input tensor with arbitrary shape.    Outputs:        - **out**: output tensor with the same shape as `data`.    References    ----------        `Layer Normalization        <https://arxiv.org/pdf/1607.06450.pdf>`_    Examples    --------    >>> # Input of shape (2, 5)    >>> x = mx.nd.array([[1, 2, 3, 4, 5], [1, 1, 2, 2, 2]])    >>> # Layer normalization is calculated with the above formula    >>> layer = LayerNorm()    >>> layer.initialize(ctx=mx.cpu(0))    >>> layer(x)    [[-1.41421    -0.707105    0.          0.707105    1.41421   ]     [-1.2247195  -1.2247195   0.81647956  0.81647956  0.81647956]]    <NDArray 2x5 @cpu(0)>"
 },
 "mxnet.gluon.nn.LeakyReLU": {
  "code": "class LeakyReLU(HybridBlock):\n    r\"\"\"Leaky version of a Rectified Linear Unit.\n\n    It allows a small gradient when the unit is not active\n\n    .. math::\n\n        f\\left(x\\right) = \\left\\{\n            \\begin{array}{lr}\n               \\alpha x & : x \\lt 0 \\\\\n                      x & : x \\geq 0 \\\\\n            \\end{array}\n        \\right.\\\\\n\n    Parameters\n    ----------\n    alpha : float\n        slope coefficient for the negative half axis. Must be >= 0.\n\n\n    Inputs:\n        - **data**: input tensor with arbitrary shape.\n\n    Outputs:\n        - **out**: output tensor with the same shape as `data`.\n    \"\"\"\n    def __init__(self, alpha, **kwargs):\n        assert alpha >= 0, \"Slope coefficient for LeakyReLU must be no less than 0.\"\n        super(LeakyReLU, self).__init__(**kwargs)\n        self._alpha = alpha\n\n    def hybrid_forward(self, F, x):\n        leaky_relu = F.npx.leaky_relu if is_np_array() else F.LeakyReLU\n        return leaky_relu(x, act_type='leaky', slope=self._alpha, name='fwd')\n\n    def __repr__(self):\n        s = '{name}({alpha})'\n        return s.format(name=self.__class__.__name__,\n                        alpha=self._alpha)\n",
  "sig": "LeakyReLU(alpha|slope coefficient for the negative half axis. Must be >= 0)",
  "dst": "Leaky version of a Rectified Linear Unit.    It allows a small gradient when the unit is not active    .. math::        f\\left(x\\right) = \\left\\{            \\begin{array}{lr}               \\alpha x & : x \\lt 0 \\\\                      x & : x \\geq 0 \\\\            \\end{array}        \\right.\\\\    Parameters    ----------    alpha : float        slope coefficient for the negative half axis. Must be >= 0.    Inputs:        - **data**: input tensor with arbitrary shape.    Outputs:        - **out**: output tensor with the same shape as `data`."
 },
 "mxnet.gluon.nn.PReLU": {
  "code": "class PReLU(HybridBlock):\n    r\"\"\"Parametric leaky version of a Rectified Linear Unit.\n    <https://arxiv.org/abs/1502.01852>`_ paper.\n\n    It learns a gradient when the unit is not active\n\n    .. math::\n\n        f\\left(x\\right) = \\left\\{\n            \\begin{array}{lr}\n               \\alpha x & : x \\lt 0 \\\\\n                      x & : x \\geq 0 \\\\\n            \\end{array}\n        \\right.\\\\\n\n    where alpha is a learned parameter.\n\n    Parameters\n    ----------\n    alpha_initializer : Initializer\n        Initializer for the `embeddings` matrix.\n    in_channels : int, default 1\n        Number of channels (alpha parameters) to learn. Can either be 1\n        or `n` where `n` is the size of the second dimension of the input\n        tensor.\n\n    Inputs:\n        - **data**: input tensor with arbitrary shape.\n\n    Outputs:\n        - **out**: output tensor with the same shape as `data`.\n    \"\"\"\n    def __init__(self, alpha_initializer=initializer.Constant(0.25),\n                 in_channels=1, **kwargs):\n        super(PReLU, self).__init__(**kwargs)\n        with self.name_scope():\n            self.alpha = self.params.get('alpha', shape=(in_channels,),\n                                         init=alpha_initializer)\n\n    def hybrid_forward(self, F, x, alpha):\n        leaky_relu = F.npx.leaky_relu if is_np_array() else F.LeakyReLU\n        return leaky_relu(x, gamma=alpha, act_type='prelu', name='fwd')\n",
  "sig": "PReLU(alpha_initializer|Initializer for the embeddings matrix)(in_channels|Number of channels (alpha parameters) to learn. Can either be 1 or n where n is the size of the second dimension of the input tensor)(Inputs|data: input tensor with arbitrary shape)(Outputs|out: output tensor with the same shape as data)",
  "dst": "Parametric leaky version of a Rectified Linear Unit.    <https://arxiv.org/abs/1502.01852>`_ paper.    It learns a gradient when the unit is not active    .. math::        f\\left(x\\right) = \\left\\{            \\begin{array}{lr}               \\alpha x & : x \\lt 0 \\\\                      x & : x \\geq 0 \\\\            \\end{array}        \\right.\\\\    where alpha is a learned parameter.    Parameters    ----------    alpha_initializer : Initializer        Initializer for the `embeddings` matrix.    in_channels : int, default 1        Number of channels (alpha parameters) to learn. Can either be 1        or `n` where `n` is the size of the second dimension of the input        tensor.    Inputs:        - **data**: input tensor with arbitrary shape.    Outputs:        - **out**: output tensor with the same shape as `data`."
 },
 "mxnet.gluon.rnn.RNN": {
  "code": "class RNN(_RNNLayer):\n    r\"\"\"Applies a multi-layer Elman RNN with `tanh` or `ReLU` non-linearity to an input sequence.\n\n    For each element in the input sequence, each layer computes the following\n    function:\n\n    .. math::\n        h_t = \\tanh(w_{ih} * x_t + b_{ih}  +  w_{hh} * h_{(t-1)} + b_{hh})\n\n    where :math:`h_t` is the hidden state at time `t`, and :math:`x_t` is the output\n    of the previous layer at time `t` or :math:`input_t` for the first layer.\n    If nonlinearity='relu', then `ReLU` is used instead of `tanh`.\n\n    Parameters\n    ----------\n    hidden_size: int\n        The number of features in the hidden state h.\n    num_layers: int, default 1\n        Number of recurrent layers.\n    activation: {'relu' or 'tanh'}, default 'relu'\n        The activation function to use.\n    layout : str, default 'TNC'\n        The format of input and output tensors. T, N and C stand for\n        sequence length, batch size, and feature dimensions respectively.\n    dropout: float, default 0\n        If non-zero, introduces a dropout layer on the outputs of each\n        RNN layer except the last layer.\n    bidirectional: bool, default False\n        If `True`, becomes a bidirectional RNN.\n    i2h_weight_initializer : str or Initializer\n        Initializer for the input weights matrix, used for the linear\n        transformation of the inputs.\n    h2h_weight_initializer : str or Initializer\n        Initializer for the recurrent weights matrix, used for the linear\n        transformation of the recurrent state.\n    i2h_bias_initializer : str or Initializer\n        Initializer for the bias vector.\n    h2h_bias_initializer : str or Initializer\n        Initializer for the bias vector.\n    input_size: int, default 0\n        The number of expected features in the input x.\n        If not specified, it will be inferred from input.\n    dtype : str, default 'float32'\n        Type to initialize the parameters and default states to\n    prefix : str or None\n        Prefix of this `Block`.\n    params : ParameterDict or None\n        Shared Parameters for this `Block`.\n\n\n    Inputs:\n        - **data**: input tensor with shape `(sequence_length, batch_size, input_size)`\n          when `layout` is \"TNC\". For other layouts, dimensions are permuted accordingly\n          using transpose() operator which adds performance overhead. Consider creating\n          batches in TNC layout during data batching step.\n\n        - **states**: initial recurrent state tensor with shape\n          `(num_layers, batch_size, num_hidden)`. If `bidirectional` is True,\n          shape will instead be `(2*num_layers, batch_size, num_hidden)`. If\n          `states` is None, zeros will be used as default begin states.\n\n    Outputs:\n        - **out**: output tensor with shape `(sequence_length, batch_size, num_hidden)`\n          when `layout` is \"TNC\". If `bidirectional` is True, output shape will instead\n          be `(sequence_length, batch_size, 2*num_hidden)`\n        - **out_states**: output recurrent state tensor with the same shape as `states`.\n          If `states` is None `out_states` will not be returned.\n\n\n    Examples\n    --------\n    >>> layer = mx.gluon.rnn.RNN(100, 3)\n    >>> layer.initialize()\n    >>> input = mx.nd.random.uniform(shape=(5, 3, 10))\n    >>> # by default zeros are used as begin state\n    >>> output = layer(input)\n    >>> # manually specify begin state.\n    >>> h0 = mx.nd.random.uniform(shape=(3, 3, 100))\n    >>> output, hn = layer(input, h0)\n    \"\"\"\n    def __init__(self, hidden_size, num_layers=1, activation='relu',\n                 layout='TNC', dropout=0, bidirectional=False,\n                 i2h_weight_initializer=None, h2h_weight_initializer=None,\n                 i2h_bias_initializer='zeros', h2h_bias_initializer='zeros',\n                 input_size=0, dtype='float32', **kwargs):\n        super(RNN, self).__init__(hidden_size, num_layers, layout,\n                                  dropout, bidirectional, input_size,\n                                  i2h_weight_initializer, h2h_weight_initializer,\n                                  i2h_bias_initializer, h2h_bias_initializer,\n                                  'rnn_'+activation, None, None, None, None, False,\n                                  dtype, **kwargs)\n\n    def state_info(self, batch_size=0):\n        return [{'shape': (self._num_layers * self._dir, batch_size, self._hidden_size),\n                 '__layout__': 'LNC', 'dtype': self._dtype}]\n",
  "sig": "RNN(hidden_size|The number of features in the hidden state h)(num_layers|Number of recurrent layers)(activation|The activation function to use)(layout|The format of input and output tensors. T, N and C stand for sequence length, batch size, and feature dimensions respectively)(dropout|If non-zero, introduces a dropout layer on the outputs of each RNN layer except the last layer)(bidirectional|If True, becomes a bidirectional RNN)(i2h_weight_initializer|Initializer for the input weights matrix, used for the linear transformation of the inputs)(h2h_weight_initializer| Initializer for the recurrent weights matrix, used for the linear transformation of the recurrent state)(i2h_bias_initializer|Initializer for the bias vector)(h2h_bias_initializer|Initializer for the bias vector)(input_size|The number of expected features in the input x. If not specified, it will be inferred from input)(dtype|Type to initialize the parameters and default states to)(prefix|Prefix of this Block)(params|Shared Parameters for this Block)",
  "dst": "Applies a multi-layer Elman RNN with `tanh` or `ReLU` non-linearity to an input sequence.    For each element in the input sequence, each layer computes the following    function:    .. math::        h_t = \\tanh(w_{ih} * x_t + b_{ih}  +  w_{hh} * h_{(t-1)} + b_{hh})    where :math:`h_t` is the hidden state at time `t`, and :math:`x_t` is the output    of the previous layer at time `t` or :math:`input_t` for the first layer.    If nonlinearity='relu', then `ReLU` is used instead of `tanh`.    Parameters    ----------    hidden_size: int        The number of features in the hidden state h.    num_layers: int, default 1        Number of recurrent layers.    activation: {'relu' or 'tanh'}, default 'relu'        The activation function to use.    layout : str, default 'TNC'        The format of input and output tensors. T, N and C stand for        sequence length, batch size, and feature dimensions respectively.    dropout: float, default 0        If non-zero, introduces a dropout layer on the outputs of each    bidirectional: bool, default False        If `True`, becomes a bidirectional RNN.    i2h_weight_initializer : str or Initializer        Initializer for the input weights matrix, used for the linear        transformation of the inputs.    h2h_weight_initializer : str or Initializer        Initializer for the recurrent weights matrix, used for the linear        transformation of the recurrent state.    i2h_bias_initializer : str or Initializer        Initializer for the bias vector.    h2h_bias_initializer : str or Initializer        Initializer for the bias vector.    input_size: int, default 0        The number of expected features in the input x.        If not specified, it will be inferred from input.    dtype : str, default 'float32'        Type to initialize the parameters and default states to    prefix : str or None        Prefix of this `Block`.    params : ParameterDict or None        Shared Parameters for this `Block`.    Inputs:        - **data**: input tensor with shape `(sequence_length, batch_size, input_size)`          when `layout` is \"TNC\". For other layouts, dimensions are permuted accordingly          using transpose() operator which adds performance overhead. Consider creating          batches in TNC layout during data batching step.        - **states**: initial recurrent state tensor with shape          `(num_layers, batch_size, num_hidden)`. If `bidirectional` is True,          shape will instead be `(2*num_layers, batch_size, num_hidden)`. If          `states` is None, zeros will be used as default begin states.    Outputs:        - **out**: output tensor with shape `(sequence_length, batch_size, num_hidden)`          when `layout` is \"TNC\". If `bidirectional` is True, output shape will instead          be `(sequence_length, batch_size, 2*num_hidden)`        - **out_states**: output recurrent state tensor with the same shape as `states`.          If `states` is None `out_states` will not be returned.    Examples    --------    >>> layer = mx.gluon.rnn.RNN(100, 3)    >>> layer.initialize()    >>> input = mx.nd.random.uniform(shape=(5, 3, 10))    >>> # by default zeros are used as begin state    >>> output = layer(input)    >>> # manually specify begin state.    >>> h0 = mx.nd.random.uniform(shape=(3, 3, 100))    >>> output, hn = layer(input, h0)"
 },
 "mxnet.symbol.gen_op.UpSampling": {
  "code": "def UpSampling(*data, **kwargs):\n    r\"\"\"Upsamples the given input data.\n\n    Two algorithms (``sample_type``) are available for upsampling:\n\n    - Nearest Neighbor\n    - Bilinear\n\n    **Nearest Neighbor Upsampling**\n\n    Input data is expected to be NCHW.\n\n    Example::\n\n      x = [[[[1. 1. 1.]\n             [1. 1. 1.]\n             [1. 1. 1.]]]]\n\n      UpSampling(x, scale=2, sample_type='nearest') = [[[[1. 1. 1. 1. 1. 1.]\n                                                         [1. 1. 1. 1. 1. 1.]\n                                                         [1. 1. 1. 1. 1. 1.]\n                                                         [1. 1. 1. 1. 1. 1.]\n                                                         [1. 1. 1. 1. 1. 1.]\n                                                         [1. 1. 1. 1. 1. 1.]]]]\n\n    **Bilinear Upsampling**\n\n    Uses `deconvolution` algorithm under the hood. You need provide both input data and the kernel.\n\n    Input data is expected to be NCHW.\n\n    `num_filter` is expected to be same as the number of channels.\n\n    Example::\n\n      x = [[[[1. 1. 1.]\n             [1. 1. 1.]\n             [1. 1. 1.]]]]\n\n      w = [[[[1. 1. 1. 1.]\n             [1. 1. 1. 1.]\n             [1. 1. 1. 1.]\n             [1. 1. 1. 1.]]]]\n  \n      UpSampling(x, w, scale=2, sample_type='bilinear', num_filter=1) = [[[[1. 2. 2. 2. 2. 1.]\n                                                                           [2. 4. 4. 4. 4. 2.]\n                                                                           [2. 4. 4. 4. 4. 2.]\n                                                                           [2. 4. 4. 4. 4. 2.]\n                                                                           [2. 4. 4. 4. 4. 2.]\n                                                                           [1. 2. 2. 2. 2. 1.]]]]\n\n\n    Defined in ../src/operator/nn/upsampling.cc:L172\n    This function support variable length of positional input.\n\n    Parameters\n    ----------\n    data : Symbol[]\n        Array of tensors to upsample. For bilinear upsampling, there should be 2 inputs - 1 data and 1 weight.\n    scale : int, required\n        Up sampling scale\n    num_filter : int, optional, default='0'\n        Input filter. Only used by bilinear sample_type.Since bilinear upsampling uses deconvolution, num_filters is set to the number of channels.\n    sample_type : {'bilinear', 'nearest'}, required\n        upsampling method\n    multi_input_mode : {'concat', 'sum'},optional, default='concat'\n        How to handle multiple input. concat means concatenate upsampled images along the channel dimension. sum means add all images together, only available for nearest neighbor upsampling.\n    workspace : long (non-negative), optional, default=512\n        Tmp workspace for deconvolution (MB)\n\n    name : string, optional.\n        Name of the resulting symbol.\n\n    Returns\n    -------\n    Symbol\n        The result symbol.\n    \"\"\"\n    return (0,)\n",
  "sig": "UpSampling(data| Array of tensors to upsample. For bilinear upsampling, there should be 2 inputs - 1 data and 1 weight)(scale|Up sampling scale)(num_filter|Input filter. Only used by bilinear sample_type.Since bilinear upsampling uses deconvolution, num_filters is set to the number of channels)(sample_type|upsampling method)(multi_input_mode|How to handle multiple input. concat means concatenate upsampled images along the channel dimension. sum means add all images together, only available for nearest neighbor upsampling)(workspace|Tmp workspace for deconvolution (MB))(out|The output NDArray to hold the result)",
  "dst": "Upsamples the given input data.    Two algorithms (``sample_type``) are available for upsampling:    - Nearest Neighbor    - Bilinear    **Nearest Neighbor Upsampling**    Input data is expected to be NCHW.    Example::      x = [[[[1. 1. 1.]             [1. 1. 1.]             [1. 1. 1.]]]]                                                         [1. 1. 1. 1. 1. 1.]                                                         [1. 1. 1. 1. 1. 1.]                                                         [1. 1. 1. 1. 1. 1.]                                                         [1. 1. 1. 1. 1. 1.]                                                         [1. 1. 1. 1. 1. 1.]]]]    **Bilinear Upsampling**    Uses `deconvolution` algorithm under the hood. You need provide both input data and the kernel.    Input data is expected to be NCHW.    `num_filter` is expected to be same as the number of channels.    Example::      x = [[[[1. 1. 1.]             [1. 1. 1.]             [1. 1. 1.]]]]      w = [[[[1. 1. 1. 1.]             [1. 1. 1. 1.]             [1. 1. 1. 1.]             [1. 1. 1. 1.]]]]                                                                           [2. 4. 4. 4. 4. 2.]                                                                           [2. 4. 4. 4. 4. 2.]                                                                           [2. 4. 4. 4. 4. 2.]                                                                           [2. 4. 4. 4. 4. 2.]                                                                           [1. 2. 2. 2. 2. 1.]]]]    Defined in ../src/operator/nn/upsampling.cc:L172    This function support variable length of positional input.    Parameters    ----------    data : Symbol[]        Array of tensors to upsample. For bilinear upsampling, there should be 2 inputs - 1 data and 1 weight.    scale : int, required        Up sampling scale    num_filter : int, optional, default='0'        Input filter. Only used by bilinear sample_type.Since bilinear upsampling uses deconvolution, num_filters is set to the number of channels.    sample_type : {'bilinear', 'nearest'}, required        upsampling method    multi_input_mode : {'concat', 'sum'},optional, default='concat'        How to handle multiple input. concat means concatenate upsampled images along the channel dimension. sum means add all images together, only available for nearest neighbor upsampling.    workspace : long (non-negative), optional, default=512        Tmp workspace for deconvolution (MB)    name : string, optional.        Name of the resulting symbol.    Returns    -------    Symbol        The result symbol."
 },
 "mxnet.gluon.loss.SigmoidBCELoss": {
  "code": "class SigmoidBinaryCrossEntropyLoss(Loss):\n    r\"\"\"The cross-entropy loss for binary classification. (alias: SigmoidBCELoss)\n\n    BCE loss is useful when training logistic regression. If `from_sigmoid`\n    is False (default), this loss computes:\n\n    .. math::\n\n        prob = \\frac{1}{1 + \\exp(-{pred})}\n\n        L = - \\sum_i {label}_i * \\log({prob}_i) * pos\\_weight +\n            (1 - {label}_i) * \\log(1 - {prob}_i)\n\n    If `from_sigmoid` is True, this loss computes:\n\n    .. math::\n\n        L = - \\sum_i {label}_i * \\log({pred}_i) * pos\\_weight +\n            (1 - {label}_i) * \\log(1 - {pred}_i)\n\n    A tensor `pos_weight > 1` decreases the false negative count, hence increasing\n    the recall.\n    Conversely setting `pos_weight < 1` decreases the false positive count and\n    increases the precision.\n\n    `pred` and `label` can have arbitrary shape as long as they have the same\n    number of elements.\n\n    Parameters\n    ----------\n    from_sigmoid : bool, default is `False`\n        Whether the input is from the output of sigmoid. Set this to false will make\n        the loss calculate sigmoid and BCE together, which is more numerically\n        stable through log-sum-exp trick.\n    weight : float or None\n        Global scalar weight for loss.\n    batch_axis : int, default 0\n        The axis that represents mini-batch.\n\n\n    Inputs:\n        - **pred**: prediction tensor with arbitrary shape\n        - **label**: target tensor with values in range `[0, 1]`. Must have the\n          same size as `pred`.\n        - **sample_weight**: element-wise weighting tensor. Must be broadcastable\n          to the same shape as pred. For example, if pred has shape (64, 10)\n          and you want to weigh each sample in the batch separately,\n          sample_weight should have shape (64, 1).\n        - **pos_weight**: a weighting tensor of positive examples. Must be a vector with length\n          equal to the number of classes.For example, if pred has shape (64, 10),\n          pos_weight should have shape (1, 10).\n\n    Outputs:\n        - **loss**: loss tensor with shape (batch_size,). Dimenions other than\n          batch_axis are averaged out.\n    \"\"\"\n\n    def __init__(self, from_sigmoid=False, weight=None, batch_axis=0, **kwargs):\n        super(SigmoidBinaryCrossEntropyLoss, self).__init__(\n            weight, batch_axis, **kwargs)\n        self._from_sigmoid = from_sigmoid\n\n    def hybrid_forward(self, F, pred, label, sample_weight=None, pos_weight=None):\n        label = _reshape_like(F, label, pred)\n        if is_np_array():\n            relu_fn = F.npx.relu\n            act_fn = F.npx.activation\n            abs_fn = F.np.abs\n            mul_fn = F.np.multiply\n            log_fn = F.np.log\n        else:\n            relu_fn = F.relu\n            act_fn = F.Activation\n            abs_fn = F.abs\n            mul_fn = F.broadcast_mul\n            log_fn = F.log\n        if not self._from_sigmoid:\n            if pos_weight is None:\n                # We use the stable formula: max(x, 0) - x * z + log(1 + exp(-abs(x)))\n                loss = relu_fn(pred) - pred * label + \\\n                    act_fn(-abs_fn(pred), act_type='softrelu')\n            else:\n                # We use the stable formula: x - x * z + (1 + z * pos_weight - z) * \\\n                #    (log(1 + exp(-abs(x))) + max(-x, 0))\n                log_weight = 1 + mul_fn(pos_weight - 1, label)\n                loss = pred - pred * label + log_weight * \\\n                       (act_fn(-abs_fn(pred), act_type='softrelu') + relu_fn(-pred))\n        else:\n            eps = 1e-12\n            if pos_weight is None:\n                loss = -(log_fn(pred + eps) * label\n                         + log_fn(1. - pred + eps) * (1. - label))\n            else:\n                loss = -(mul_fn(log_fn(pred + eps) * label, pos_weight)\n                         + log_fn(1. - pred + eps) * (1. - label))\n        loss = _apply_weighting(F, loss, self._weight, sample_weight)\n        if is_np_array():\n            if F is ndarray:\n                return F.np.mean(loss, axis=tuple(range(1, loss.ndim)))\n            else:\n                return F.npx.batch_flatten(loss).mean(axis=1)\n        else:\n            return F.mean(loss, axis=self._batch_axis, exclude=True)\n",
  "sig": "SigmoidBCELoss(axis|The axis to sum over when computing softmax and entropy)(sparse_label|Whether label is an integer array instead of probability distribution)(from_logits|Whether input is a log probability (usually from log_softmax) instead of unnormalized numbers)(weight|Global scalar weight for loss)(batch_axis|The axis that represents mini-batch)",
  "dst": "The cross-entropy loss for binary classification. (alias: SigmoidBCELoss)    BCE loss is useful when training logistic regression. If `from_sigmoid`    is False (default), this loss computes:    .. math::        prob = \\frac{1}{1 + \\exp(-{pred})}        L = - \\sum_i {label}_i * \\log({prob}_i) * pos\\_weight +            (1 - {label}_i) * \\log(1 - {prob}_i)    If `from_sigmoid` is True, this loss computes:    .. math::        L = - \\sum_i {label}_i * \\log({pred}_i) * pos\\_weight +            (1 - {label}_i) * \\log(1 - {pred}_i)    A tensor `pos_weight > 1` decreases the false negative count, hence increasing    the recall.    Conversely setting `pos_weight < 1` decreases the false positive count and    increases the precision.    `pred` and `label` can have arbitrary shape as long as they have the same    number of elements.    Parameters    ----------    from_sigmoid : bool, default is `False`        Whether the input is from the output of sigmoid. Set this to false will make        the loss calculate sigmoid and BCE together, which is more numerically        stable through log-sum-exp trick.    weight : float or None        Global scalar weight for loss.    batch_axis : int, default 0        The axis that represents mini-batch.    Inputs:        - **pred**: prediction tensor with arbitrary shape        - **label**: target tensor with values in range `[0, 1]`. Must have the          same size as `pred`.        - **sample_weight**: element-wise weighting tensor. Must be broadcastable          to the same shape as pred. For example, if pred has shape (64, 10)          and you want to weigh each sample in the batch separately,          sample_weight should have shape (64, 1).        - **pos_weight**: a weighting tensor of positive examples. Must be a vector with length          equal to the number of classes.For example, if pred has shape (64, 10),          pos_weight should have shape (1, 10).    Outputs:        - **loss**: loss tensor with shape (batch_size,). Dimenions other than          batch_axis are averaged out."
 },
 "mxnet.gluon.loss.SoftmaxCrossEntropyLoss": {
  "code": "class SoftmaxCrossEntropyLoss(Loss):\n    r\"\"\"Computes the softmax cross entropy loss. (alias: SoftmaxCELoss)\n\n    If `sparse_label` is `True` (default), label should contain integer\n    category indicators:\n\n    .. math::\n\n        \\DeclareMathOperator{softmax}{softmax}\n\n        p = \\softmax({pred})\n\n        L = -\\sum_i \\log p_{i,{label}_i}\n\n    `label`'s shape should be `pred`'s shape with the `axis` dimension removed.\n    i.e. for `pred` with shape (1,2,3,4) and `axis = 2`, `label`'s shape should\n    be (1,2,4).\n\n    If `sparse_label` is `False`, `label` should contain probability distribution\n    and `label`'s shape should be the same with `pred`:\n\n    .. math::\n\n        p = \\softmax({pred})\n\n        L = -\\sum_i \\sum_j {label}_j \\log p_{ij}\n\n    Parameters\n    ----------\n    axis : int, default -1\n        The axis to sum over when computing softmax and entropy.\n    sparse_label : bool, default True\n        Whether label is an integer array instead of probability distribution.\n    from_logits : bool, default False\n        Whether input is a log probability (usually from log_softmax) instead\n        of unnormalized numbers.\n    weight : float or None\n        Global scalar weight for loss.\n    batch_axis : int, default 0\n        The axis that represents mini-batch.\n\n\n    Inputs:\n        - **pred**: the prediction tensor, where the `batch_axis` dimension\n          ranges over batch size and `axis` dimension ranges over the number\n          of classes.\n        - **label**: the truth tensor. When `sparse_label` is True, `label`'s\n          shape should be `pred`'s shape with the `axis` dimension removed.\n          i.e. for `pred` with shape (1,2,3,4) and `axis = 2`, `label`'s shape\n          should be (1,2,4) and values should be integers between 0 and 2. If\n          `sparse_label` is False, `label`'s shape must be the same as `pred`\n          and values should be floats in the range `[0, 1]`.\n        - **sample_weight**: element-wise weighting tensor. Must be broadcastable\n          to the same shape as label. For example, if label has shape (64, 10)\n          and you want to weigh each sample in the batch separately,\n          sample_weight should have shape (64, 1).\n\n    Outputs:\n        - **loss**: loss tensor with shape (batch_size,). Dimenions other than\n          batch_axis are averaged out.\n    \"\"\"\n\n    def __init__(self, axis=-1, sparse_label=True, from_logits=False, weight=None,\n                 batch_axis=0, **kwargs):\n        super(SoftmaxCrossEntropyLoss, self).__init__(\n            weight, batch_axis, **kwargs)\n        self._axis = axis\n        self._sparse_label = sparse_label\n        self._from_logits = from_logits\n\n    def hybrid_forward(self, F, pred, label, sample_weight=None):\n        if is_np_array():\n            log_softmax = F.npx.log_softmax\n            pick = F.npx.pick\n        else:\n            log_softmax = F.log_softmax\n            pick = F.pick\n        if not self._from_logits:\n            pred = log_softmax(pred, self._axis)\n        if self._sparse_label:\n            loss = -pick(pred, label, axis=self._axis, keepdims=True)\n        else:\n            label = _reshape_like(F, label, pred)\n            loss = -(pred * label).sum(axis=self._axis, keepdims=True)\n        loss = _apply_weighting(F, loss, self._weight, sample_weight)\n        if is_np_array():\n            if F is ndarray:\n                return loss.mean(axis=tuple(range(1, loss.ndim)))\n            else:\n                return F.npx.batch_flatten(loss).mean(axis=1)\n        else:\n            return loss.mean(axis=self._batch_axis, exclude=True)\n",
  "sig": "SoftmaxCrossEntropyLoss(axis|The axis to sum over when computing softmax and entropy)(sparse_label|Whether label is an integer array instead of probability distribution)(from_logits|Whether input is a log probability (usually from log_softmax) instead of unnormalized numbers)(weight|Global scalar weight for loss)(batch_axis|The axis that represents mini-batch)",
  "dst": "Computes the softmax cross entropy loss. (alias: SoftmaxCELoss)    If `sparse_label` is `True` (default), label should contain integer    category indicators:    .. math::        \\DeclareMathOperator{softmax}{softmax}        p = \\softmax({pred})        L = -\\sum_i \\log p_{i,{label}_i}    `label`'s shape should be `pred`'s shape with the `axis` dimension removed.    i.e. for `pred` with shape (1,2,3,4) and `axis = 2`, `label`'s shape should    be (1,2,4).    If `sparse_label` is `False`, `label` should contain probability distribution    and `label`'s shape should be the same with `pred`:    .. math::        p = \\softmax({pred})        L = -\\sum_i \\sum_j {label}_j \\log p_{ij}    Parameters    ----------    axis : int, default -1        The axis to sum over when computing softmax and entropy.    sparse_label : bool, default True        Whether label is an integer array instead of probability distribution.    from_logits : bool, default False        Whether input is a log probability (usually from log_softmax) instead        of unnormalized numbers.    weight : float or None        Global scalar weight for loss.    batch_axis : int, default 0        The axis that represents mini-batch.    Inputs:        - **pred**: the prediction tensor, where the `batch_axis` dimension          ranges over batch size and `axis` dimension ranges over the number          of classes.        - **label**: the truth tensor. When `sparse_label` is True, `label`'s          shape should be `pred`'s shape with the `axis` dimension removed.          i.e. for `pred` with shape (1,2,3,4) and `axis = 2`, `label`'s shape          should be (1,2,4) and values should be integers between 0 and 2. If          `sparse_label` is False, `label`'s shape must be the same as `pred`          and values should be floats in the range `[0, 1]`.        - **sample_weight**: element-wise weighting tensor. Must be broadcastable          to the same shape as label. For example, if label has shape (64, 10)          and you want to weigh each sample in the batch separately,          sample_weight should have shape (64, 1).    Outputs:        - **loss**: loss tensor with shape (batch_size,). Dimenions other than          batch_axis are averaged out."
 },
 "mxnet.gluon.loss.CosineEmbeddingLoss": {
  "code": "class CosineEmbeddingLoss(Loss):\n    r\"\"\"For a target label 1 or -1, vectors input1 and input2, the function computes the cosine distance\n    between the vectors. This can be interpreted as how similar/dissimilar two input vectors are.\n\n    .. math::\n\n        L = \\sum_i \\begin{cases} 1 - {cos\\_sim({input1}_i, {input2}_i)} & \\text{ if } {label}_i = 1\\\\\n                         {cos\\_sim({input1}_i, {input2}_i)} & \\text{ if } {label}_i = -1 \\end{cases}\\\\\n        cos\\_sim(input1, input2) = \\frac{{input1}_i.{input2}_i}{||{input1}_i||.||{input2}_i||}\n\n    `input1`, `input2` can have arbitrary shape as long as they have the same number of elements.\n\n    Parameters\n    ----------\n    weight : float or None\n        Global scalar weight for loss.\n    batch_axis : int, default 0\n        The axis that represents mini-batch.\n    margin : float\n        Margin of separation between correct and incorrect pair.\n\n\n    Inputs:\n        - **input1**: a tensor with arbitrary shape\n        - **input2**: another tensor with same shape as pred to which input1 is\n          compared for similarity and loss calculation\n        - **label**: A 1-D tensor indicating for each pair input1 and input2, target label is 1 or -1\n        - **sample_weight**: element-wise weighting tensor. Must be broadcastable\n          to the same shape as input1. For example, if input1 has shape (64, 10)\n          and you want to weigh each sample in the batch separately,\n          sample_weight should have shape (64, 1).\n\n    Outputs:\n        - **loss**: The loss tensor with shape (batch_size,).\n    \"\"\"\n\n    def __init__(self, weight=None, batch_axis=0, margin=0, **kwargs):\n        super(CosineEmbeddingLoss, self).__init__(weight, batch_axis, **kwargs)\n        self._margin = margin\n\n    def hybrid_forward(self, F, input1, input2, label, sample_weight=None):\n        input1 = _reshape_like(F, input1, input2)\n        label = label.reshape((-1, 1))\n        cos_sim = self._cosine_similarity(F, input1, input2)\n        y_1 = label == 1\n        y_minus_1 = label == -1\n        cos_sim_a = (1 - cos_sim) * y_1\n\n        if F is ndarray:\n            z_array = F.array([0])\n        else:\n            z_array = F.zeros((1, 1))\n        cos_sim_b = F.broadcast_maximum(\n            z_array, y_minus_1 * (cos_sim - self._margin), axis=1)\n        loss = cos_sim_a + cos_sim_b\n        loss = _apply_weighting(F, loss, self._weight, sample_weight)\n        return loss\n\n    def _cosine_similarity(self, F, x, y, axis=-1):\n        # Calculates the cosine similarity between 2 vectors\n        x_norm = F.norm(x, axis=axis).reshape((-1, 1))\n        y_norm = F.norm(y, axis=axis).reshape((-1, 1))\n        x_dot_y = F.sum(x * y, axis=axis).reshape((-1, 1))\n        if F is ndarray:\n            eps_arr = F.array([1e-12])\n        else:\n            eps_arr = F.full((1, 1), 1e-12)\n        return (x_dot_y / F.broadcast_maximum(x_norm * y_norm, eps_arr))\n",
  "sig": "CosineEmbeddingLoss(weight|Global scalar weight for loss)(batch_axis|The axis that represents mini-batch)(margin|Margin of separation between correct and incorrect pair)",
  "dst": "For a target label 1 or -1, vectors input1 and input2, the function computes the cosine distance    between the vectors. This can be interpreted as how similar/dissimilar two input vectors are.    .. math::        L = \\sum_i \\begin{cases} 1 - {cos\\_sim({input1}_i, {input2}_i)} & \\text{ if } {label}_i = 1\\\\                         {cos\\_sim({input1}_i, {input2}_i)} & \\text{ if } {label}_i = -1 \\end{cases}\\\\        cos\\_sim(input1, input2) = \\frac{{input1}_i.{input2}_i}{||{input1}_i||.||{input2}_i||}    `input1`, `input2` can have arbitrary shape as long as they have the same number of elements.    Parameters    ----------    weight : float or None        Global scalar weight for loss.    batch_axis : int, default 0        The axis that represents mini-batch.    margin : float        Margin of separation between correct and incorrect pair.    Inputs:        - **input1**: a tensor with arbitrary shape        - **input2**: another tensor with same shape as pred to which input1 is          compared for similarity and loss calculation        - **label**: A 1-D tensor indicating for each pair input1 and input2, target label is 1 or -1        - **sample_weight**: element-wise weighting tensor. Must be broadcastable          to the same shape as input1. For example, if input1 has shape (64, 10)          and you want to weigh each sample in the batch separately,          sample_weight should have shape (64, 1).    Outputs:        - **loss**: The loss tensor with shape (batch_size,)."
 },
 "mxnet.gluon.loss.PoissonNLLLoss": {
  "code": "class PoissonNLLLoss(Loss):\n    r\"\"\"For a target (Random Variable) in a Poisson distribution, the function calculates the Negative\n    Log likelihood loss.\n    PoissonNLLLoss measures the loss accrued from a poisson regression prediction made by the model.\n\n    .. math::\n        L = \\text{pred} - \\text{target} * \\log(\\text{pred}) +\\log(\\text{target!})\n\n    `target`, 'pred' can have arbitrary shape as long as they have the same number of elements.\n\n    Parameters\n    ----------\n    from_logits : boolean, default True\n        indicating whether log(predicted) value has already been computed. If True, the loss is computed as\n        :math:`\\exp(\\text{pred}) - \\text{target} * \\text{pred}`, and if False, then loss is computed as\n        :math:`\\text{pred} - \\text{target} * \\log(\\text{pred}+\\text{epsilon})`.The default value\n    weight : float or None\n        Global scalar weight for loss.\n    batch_axis : int, default 0\n        The axis that represents mini-batch.\n    compute_full: boolean, default False\n        Indicates whether to add an approximation(Stirling factor) for the Factorial term in the formula for the loss.\n        The Stirling factor is:\n        :math:`\\text{target} * \\log(\\text{target}) - \\text{target} + 0.5 * \\log(2 * \\pi * \\text{target})`\n    epsilon: float, default 1e-08\n        This is to avoid calculating log(0) which is not defined.\n\n\n    Inputs:\n        - **pred**:   Predicted value\n        - **target**: Random variable(count or number) which belongs to a Poisson distribution.\n        - **sample_weight**: element-wise weighting tensor. Must be broadcastable\n          to the same shape as pred. For example, if pred has shape (64, 10)\n          and you want to weigh each sample in the batch separately,\n          sample_weight should have shape (64, 1).\n\n    Outputs:\n        - **loss**: Average loss (shape=(1,1)) of the loss tensor with shape (batch_size,).\n    \"\"\"\n\n    def __init__(self, weight=None, from_logits=True, batch_axis=0, compute_full=False, **kwargs):\n        super(PoissonNLLLoss, self).__init__(weight, batch_axis, **kwargs)\n        self._from_logits = from_logits\n        self._compute_full = compute_full\n\n    def hybrid_forward(self, F, pred, target, sample_weight=None, epsilon=1e-08):\n        target = _reshape_like(F, target, pred)\n        if self._from_logits:\n            loss = F.exp(pred) - target * pred\n        else:\n            loss = pred - target * F.log(pred + epsilon)\n        if self._compute_full:\n            # Using numpy's pi value\n            stirling_factor = target * \\\n                F.log(target) - target + 0.5 * F.log(2 * target * np.pi)\n            target_gt_1 = target > 1\n            stirling_factor *= target_gt_1\n            loss += stirling_factor\n        loss = _apply_weighting(F, loss, self._weight, sample_weight)\n        return F.mean(loss)\n",
  "sig": "PoissonNLLLoss(from_logits|indicating whether log(predicted) value has already been computed. If True, the loss is computed as  exp(pred)−target∗pred , and if False, then loss is computed as  pred−target∗log(pred+epsilon) .The default value)(weight|Global scalar weight for loss)(batch_axis|The axis that represents mini-batch)(compute_full|Indicates whether to add an approximation(Stirling factor) for the Factorial term in the formula for the loss. The Stirling factor is:  target∗log(target)−target+0.5∗log(2∗𝜋∗target))(epsilon|This is to avoid calculating log(0) which is not defined)",
  "dst": "For a target (Random Variable) in a Poisson distribution, the function calculates the Negative    Log likelihood loss.    .. math::        L = \\text{pred} - \\text{target} * \\log(\\text{pred}) +\\log(\\text{target!})    `target`, 'pred' can have arbitrary shape as long as they have the same number of elements.    Parameters    ----------    from_logits : boolean, default True        indicating whether log(predicted) value has already been computed. If True, the loss is computed as        :math:`\\exp(\\text{pred}) - \\text{target} * \\text{pred}`, and if False, then loss is computed as        :math:`\\text{pred} - \\text{target} * \\log(\\text{pred}+\\text{epsilon})`.The default value    weight : float or None        Global scalar weight for loss.    batch_axis : int, default 0        The axis that represents mini-batch.    compute_full: boolean, default False        Indicates whether to add an approximation(Stirling factor) for the Factorial term in the formula for the loss.        The Stirling factor is:        :math:`\\text{target} * \\log(\\text{target}) - \\text{target} + 0.5 * \\log(2 * \\pi * \\text{target})`    epsilon: float, default 1e-08        This is to avoid calculating log(0) which is not defined.    Inputs:        - **pred**:   Predicted value        - **target**: Random variable(count or number) which belongs to a Poisson distribution.        - **sample_weight**: element-wise weighting tensor. Must be broadcastable          to the same shape as pred. For example, if pred has shape (64, 10)          and you want to weigh each sample in the batch separately,          sample_weight should have shape (64, 1).    Outputs:        - **loss**: Average loss (shape=(1,1)) of the loss tensor with shape (batch_size,)."
 },
 "mxnet.model.save_checkpoint": {
  "code": "def save_checkpoint(prefix, epoch, symbol, arg_params, aux_params, remove_amp_cast=True):\n    \"\"\"Checkpoint the model data into file.\n\n    Parameters\n    ----------\n    prefix : str\n        Prefix of model name.\n    epoch : int\n        The epoch number of the model.\n    symbol : Symbol\n        The input Symbol.\n    arg_params : dict of str to NDArray\n        Model parameter, dict of name to NDArray of net's weights.\n    aux_params : dict of str to NDArray\n        Model parameter, dict of name to NDArray of net's auxiliary states.\n    remove_amp_cast : bool, optional\n        Whether to remove the amp_cast and amp_multicast operators, before saving the model.\n    Notes\n    -----\n    - ``prefix-symbol.json`` will be saved for symbol.\n    - ``prefix-epoch.params`` will be saved for parameters.\n    \"\"\"\n    if symbol is not None:\n        symbol.save('%s-symbol.json' % prefix, remove_amp_cast=remove_amp_cast)\n\n    save_dict = {('arg:%s' % k) : v.as_in_context(cpu()) for k, v in arg_params.items()}\n    save_dict.update({('aux:%s' % k) : v.as_in_context(cpu()) for k, v in aux_params.items()})\n    param_name = '%s-%04d.params' % (prefix, epoch)\n    nd.save(param_name, save_dict)\n    logging.info('Saved checkpoint to \\\"%s\\\"', param_name)\n",
  "sig": "save_checkpoint(prefix|The file prefix to checkpoint to)(epoch| The current epoch number)",
  "dst": "Checkpoint the model data into file.    Parameters    ----------    prefix : str        Prefix of model name.    epoch : int        The epoch number of the model.    symbol : Symbol        The input Symbol.    arg_params : dict of str to NDArray        Model parameter, dict of name to NDArray of net's weights.    aux_params : dict of str to NDArray        Model parameter, dict of name to NDArray of net's auxiliary states.    remove_amp_cast : bool, optional        Whether to remove the amp_cast and amp_multicast operators, before saving the model.    Notes    -----    - ``prefix-symbol.json`` will be saved for symbol.    - ``prefix-epoch.params`` will be saved for parameters."
 },
 "mxnet.lr_scheduler.FactorScheduler": {
  "code": "class FactorScheduler(LRScheduler):\n    \"\"\"Reduce the learning rate by a factor for every *n* steps.\n\n    It returns a new learning rate by::\n\n        base_lr * pow(factor, floor(num_update/step))\n\n    Parameters\n    ----------\n    step : int\n        Changes the learning rate for every n updates.\n    factor : float, optional\n        The factor to change the learning rate.\n    stop_factor_lr : float, optional\n        Stop updating the learning rate if it is less than this value.\n    \"\"\"\n    def __init__(self, step, factor=1, stop_factor_lr=1e-8, base_lr=0.01,\n                 warmup_steps=0, warmup_begin_lr=0, warmup_mode='linear'):\n        super(FactorScheduler, self).__init__(base_lr, warmup_steps, warmup_begin_lr, warmup_mode)\n        if step < 1:\n            raise ValueError(\"Schedule step must be greater or equal than 1 round\")\n        if factor > 1.0:\n            raise ValueError(\"Factor must be no more than 1 to make lr reduce\")\n        self.step = step\n        self.factor = factor\n        self.stop_factor_lr = stop_factor_lr\n        self.count = 0\n\n    def __call__(self, num_update):\n        if num_update < self.warmup_steps:\n            return self.get_warmup_lr(num_update)\n\n        # NOTE: use while rather than if  (for continuing training via load_epoch)\n        while num_update > self.count + self.step:\n            self.count += self.step\n            self.base_lr *= self.factor\n            if self.base_lr < self.stop_factor_lr:\n                self.base_lr = self.stop_factor_lr\n                logging.info(\"Update[%d]: now learning rate arrived at %0.5e, will not \"\n                             \"change in the future\", num_update, self.base_lr)\n            else:\n                logging.info(\"Update[%d]: Change learning rate to %0.5e\",\n                             num_update, self.base_lr)\n        return self.base_lr\n",
  "sig": "FactorScheduler(step|Changes the learning rate for every n updates)(factor|The factor to change the learning rate)(stop_factor_lr|Stop updating the learning rate if it is less than this value)",
  "dst": "Reduce the learning rate by a factor for every *n* steps.    It returns a new learning rate by::        base_lr * pow(factor, floor(num_update/step))    Parameters    ----------    step : int        Changes the learning rate for every n updates.    factor : float, optional        The factor to change the learning rate.    stop_factor_lr : float, optional        Stop updating the learning rate if it is less than this value."
 },
 "mxnet.optimizer.SGD": {
  "code": "class SGD(Optimizer):\n    \"\"\"The SGD optimizer with momentum and weight decay.\n\n    If the storage types of grad is ``row_sparse`` and ``lazy_update`` is True, \\\n    **lazy updates** are applied by::\n\n        for row in grad.indices:\n            rescaled_grad[row] = lr * (rescale_grad * clip(grad[row], clip_gradient) + wd * weight[row])\n            state[row] = momentum[row] * state[row] + rescaled_grad[row]\n            weight[row] = weight[row] - state[row]\n\n    The sparse update only updates the momentum for the weights whose row_sparse\n    gradient indices appear in the current batch, rather than updating it for all\n    indices. Compared with the original update, it can provide large\n    improvements in model training throughput for some applications. However, it\n    provides slightly different semantics than the original update, and\n    may lead to different empirical results.\n\n    In the case when ``update_on_kvstore`` is set to False (either globally via\n    MXNET_UPDATE_ON_KVSTORE=0 environment variable or as a parameter in\n    :class:`~mxnet.gluon.Trainer`) SGD optimizer can perform aggregated update\n    of parameters, which may lead to improved performance. The aggregation size\n    is controlled by MXNET_OPTIMIZER_AGGREGATION_SIZE environment variable and\n    defaults to 4.\n\n    Otherwise, **standard updates** are applied by::\n\n        rescaled_grad = lr * (rescale_grad * clip(grad, clip_gradient) + wd * weight)\n        state = momentum * state + rescaled_grad\n        weight = weight - state\n\n    For details of the update algorithm see\n    :class:`~mxnet.ndarray.sgd_update` and :class:`~mxnet.ndarray.sgd_mom_update`.\n\n    This optimizer accepts the following parameters in addition to those accepted\n    by :class:`.Optimizer`.\n\n    Parameters\n    ----------\n    momentum : float, optional\n        The momentum value.\n    lazy_update : bool, optional\n        Default is True. If True, lazy updates are applied \\\n        if the storage types of weight and grad are both ``row_sparse``.\n    multi_precision: bool, optional\n        Flag to control the internal precision of the optimizer.\n        False: results in using the same precision as the weights (default),\n        True: makes internal 32-bit copy of the weights and applies gradients\n        in 32-bit precision even if actual weights used in the model have lower precision.\n        Turning this on can improve convergence and accuracy when training with float16.\n    \"\"\"\n    def __init__(self, momentum=0.0, lazy_update=True, **kwargs):\n        super(SGD, self).__init__(**kwargs)\n        self.momentum = momentum\n        self.lazy_update = lazy_update\n        self.aggregate_num = int(os.getenv('MXNET_OPTIMIZER_AGGREGATION_SIZE', \"4\"))\n\n    def create_state_multi_precision(self, index, weight):\n        weight_master_copy = None\n        if self.multi_precision and weight.dtype == numpy.float16:\n            weight_master_copy = weight.astype(numpy.float32)\n            return (self.create_state(index, weight_master_copy), weight_master_copy)\n        if weight.dtype == numpy.float16 and not self.multi_precision:\n            warnings.warn(\"Accumulating with float16 in optimizer can lead to \"\n                          \"poor accuracy or slow convergence. \"\n                          \"Consider using multi_precision=True option of the \"\n                          \"SGD optimizer\")\n        return self.create_state(index, weight)\n\n    def create_state(self, index, weight):\n        momentum = None\n        if self.momentum != 0.0:\n            stype = weight.stype if self.lazy_update else 'default'\n            momentum = zeros(weight.shape, weight.context, dtype=weight.dtype, stype=stype)\n        return momentum\n\n    def _update_impl(self, indices, weights, grads, states, multi_precision=False):\n        aggregate = True\n        if not isinstance(indices, (tuple, list)):\n            indices = [indices]\n            weights = [weights]\n            grads = [grads]\n            states = [states]\n        for weight, grad in zip(weights, grads):\n            assert(isinstance(weight, NDArray))\n            assert(isinstance(grad, NDArray))\n            aggregate = (aggregate and\n                         weight.stype == 'default' and\n                         grad.stype == 'default')\n        self._update_count(indices)\n        lrs = self._get_lrs(indices)\n        wds = self._get_wds(indices)\n\n        kwargs = {'rescale_grad': self.rescale_grad}\n        if self.momentum > 0:\n            kwargs['momentum'] = self.momentum\n        if self.clip_gradient:\n            kwargs['clip_gradient'] = self.clip_gradient\n\n        if aggregate:\n            if not multi_precision:\n                if self.momentum > 0:\n                    multi_sgd_mom_update(*_flatten_list(zip(weights, grads, states)), out=weights,\n                                         num_weights=len(weights), lrs=lrs, wds=wds, **kwargs)\n                else:\n                    multi_sgd_update(*_flatten_list(zip(weights, grads)), out=weights,\n                                     num_weights=len(weights), lrs=lrs, wds=wds, **kwargs)\n            else:\n                if self.momentum > 0:\n                    multi_mp_sgd_mom_update(*_flatten_list(zip(weights, grads, *zip(*states))),\n                                            out=weights, num_weights=len(weights),\n                                            lrs=lrs, wds=wds, **kwargs)\n                else:\n                    multi_mp_sgd_update(*_flatten_list(zip(weights, grads,\n                                                           list(zip(*states))[1])),\n                                        out=weights, num_weights=len(weights),\n                                        lrs=lrs, wds=wds, **kwargs)\n        else:\n            for weight, grad, state, lr, wd in zip(weights, grads, states, lrs, wds):\n                if not multi_precision:\n                    if state is not None:\n                        sgd_mom_update(weight, grad, state, out=weight,\n                                       lazy_update=self.lazy_update, lr=lr, wd=wd, **kwargs)\n                    else:\n                        sgd_update(weight, grad, out=weight, lazy_update=self.lazy_update,\n                                   lr=lr, wd=wd, **kwargs)\n                else:\n                    if state[0] is not None:\n                        mp_sgd_mom_update(weight, grad, state[0], state[1], out=weight,\n                                          lr=lr, wd=wd, **kwargs)\n                    else:\n                        mp_sgd_update(weight, grad, state[1], out=weight,\n                                      lr=lr, wd=wd, **kwargs)\n\n    def update(self, index, weight, grad, state):\n        self._update_impl(index, weight, grad, state, multi_precision=False)\n\n    def update_multi_precision(self, index, weight, grad, state):\n        if not isinstance(index, (tuple, list)):\n            use_multi_precision = self.multi_precision and weight.dtype == numpy.float16\n        else:\n            use_multi_precision = self.multi_precision and weight[0].dtype == numpy.float16\n        self._update_impl(index, weight, grad, state,\n                          multi_precision=use_multi_precision)\n",
  "sig": "SGD(momentum|The momentum value)(lazy_update|Default is True. If True, lazy updates are applied if the storage types of weight and grad are both row_sparse)(multi_precision|Flag to control the internal precision of the optimizer. False: results in using the same precision as the weights (default), True: makes internal 32-bit copy of the weights and applies gradients in 32-bit precision even if actual weights used in the model have lower precision. Turning this on can improve convergence and accuracy when training with float16)",
  "dst": "The SGD optimizer with momentum and weight decay.    If the storage types of grad is ``row_sparse`` and ``lazy_update`` is True, \\    **lazy updates** are applied by::        for row in grad.indices:            rescaled_grad[row] = lr * (rescale_grad * clip(grad[row], clip_gradient) + wd * weight[row])            state[row] = momentum[row] * state[row] + rescaled_grad[row]            weight[row] = weight[row] - state[row]    The sparse update only updates the momentum for the weights whose row_sparse    gradient indices appear in the current batch, rather than updating it for all    indices. Compared with the original update, it can provide large    improvements in model training throughput for some applications. However, it    provides slightly different semantics than the original update, and    may lead to different empirical results.    In the case when ``update_on_kvstore`` is set to False (either globally via    MXNET_UPDATE_ON_KVSTORE=0 environment variable or as a parameter in    :class:`~mxnet.gluon.Trainer`) SGD optimizer can perform aggregated update    of parameters, which may lead to improved performance. The aggregation size    is controlled by MXNET_OPTIMIZER_AGGREGATION_SIZE environment variable and    defaults to 4.    Otherwise, **standard updates** are applied by::        rescaled_grad = lr * (rescale_grad * clip(grad, clip_gradient) + wd * weight)        state = momentum * state + rescaled_grad        weight = weight - state    For details of the update algorithm see    :class:`~mxnet.ndarray.sgd_update` and :class:`~mxnet.ndarray.sgd_mom_update`.    This optimizer accepts the following parameters in addition to those accepted    by :class:`.Optimizer`.    Parameters    ----------    momentum : float, optional        The momentum value.    lazy_update : bool, optional        Default is True. If True, lazy updates are applied \\        if the storage types of weight and grad are both ``row_sparse``.    multi_precision: bool, optional        Flag to control the internal precision of the optimizer.        False: results in using the same precision as the weights (default),        True: makes internal 32-bit copy of the weights and applies gradients        in 32-bit precision even if actual weights used in the model have lower precision.        Turning this on can improve convergence and accuracy when training with float16."
 },
 "mxnet.gluon.nn.Sequential": {
  "code": "class Sequential(Block):\n    \"\"\"Stacks Blocks sequentially.\n\n    Example::\n\n        net = nn.Sequential()\n        # use net's name_scope to give child Blocks appropriate names.\n        with net.name_scope():\n            net.add(nn.Dense(10, activation='relu'))\n            net.add(nn.Dense(20))\n    \"\"\"\n    def __init__(self, prefix=None, params=None):\n        super(Sequential, self).__init__(prefix=prefix, params=params)\n\n    def add(self, *blocks):\n        \"\"\"Adds block on top of the stack.\"\"\"\n        for block in blocks:\n            self.register_child(block)\n\n    def forward(self, x):\n        for block in self._children.values():\n            x = block(x)\n        return x\n\n    def __repr__(self):\n        s = '{name}(\\n{modstr}\\n)'\n        modstr = '\\n'.join(['  ({key}): {block}'.format(key=key,\n                                                        block=_indent(block.__repr__(), 2))\n                            for key, block in self._children.items()])\n        return s.format(name=self.__class__.__name__,\n                        modstr=modstr)\n\n    def __getitem__(self, key):\n        layers = list(self._children.values())[key]\n        if isinstance(layers, list):\n            net = type(self)(prefix=self._prefix)\n            with net.name_scope():\n                net.add(*layers)\n            return net\n        else:\n            return layers\n\n    def __len__(self):\n        return len(self._children)\n\n    def hybridize(self, active=True, **kwargs):\n        \"\"\"Activates or deactivates `HybridBlock` s recursively. Has no effect on\n        non-hybrid children.\n\n        Parameters\n        ----------\n        active : bool, default True\n            Whether to turn hybrid on or off.\n        **kwargs : string\n            Additional flags for hybridized operator.\n        \"\"\"\n        if self._children and all(isinstance(c, HybridBlock) for c in self._children.values()):\n            warnings.warn(\n                \"All children of this Sequential layer '%s' are HybridBlocks. Consider \"\n                \"using HybridSequential for the best performance.\"%self.prefix, stacklevel=2)\n        super(Sequential, self).hybridize(active, **kwargs)\n",
  "sig": "Sequential()",
  "dst": "Stacks Blocks sequentially.    Example::        net = nn.Sequential()        # use net's name_scope to give child Blocks appropriate names.        with net.name_scope():            net.add(nn.Dense(10, activation='relu'))            net.add(nn.Dense(20))"
 },
 "mxnet.autograd.pause": {
  "code": "def pause(train_mode=False): #pylint: disable=redefined-outer-name\n    \"\"\"Returns a scope context to be used in 'with' statement for codes that do not need\n    gradients to be calculated.\n\n    Example::\n\n        with autograd.record():\n            y = model(x)\n            backward([y])\n            with autograd.pause():\n                # testing, IO, gradient updates...\n\n    Parameters\n    ----------\n    train_mode: bool, default False\n        Whether to do forward for training or predicting.\n    \"\"\"\n    return _RecordingStateScope(False, train_mode)\n",
  "sig": "pause(train_mode|Whether to do forward for training or predicting)",
  "dst": "Returns a scope context to be used in 'with' statement for codes that do not need    gradients to be calculated.    Example::        with autograd.record():            y = model(x)            backward([y])            with autograd.pause():                # testing, IO, gradient updates...    Parameters    ----------    train_mode: bool, default False        Whether to do forward for training or predicting."
 },
 "mxnet.ndarray.gen_op.split": {
  "code": "def split(data=None, num_outputs=_Null, axis=_Null, squeeze_axis=_Null, out=None, name=None, **kwargs):\n    r\"\"\"Splits an array along a particular axis into multiple sub-arrays.\n\n    .. note:: ``SliceChannel`` is deprecated. Use ``split`` instead.\n\n    **Note** that `num_outputs` should evenly divide the length of the axis\n    along which to split the array.\n\n    Example::\n\n       x  = [[[ 1.]\n              [ 2.]]\n             [[ 3.]\n              [ 4.]]\n             [[ 5.]\n              [ 6.]]]\n       x.shape = (3, 2, 1)\n\n       y = split(x, axis=1, num_outputs=2) // a list of 2 arrays with shape (3, 1, 1)\n       y = [[[ 1.]]\n            [[ 3.]]\n            [[ 5.]]]\n\n           [[[ 2.]]\n            [[ 4.]]\n            [[ 6.]]]\n\n       y[0].shape = (3, 1, 1)\n\n       z = split(x, axis=0, num_outputs=3) // a list of 3 arrays with shape (1, 2, 1)\n       z = [[[ 1.]\n             [ 2.]]]\n\n           [[[ 3.]\n             [ 4.]]]\n\n           [[[ 5.]\n             [ 6.]]]\n\n       z[0].shape = (1, 2, 1)\n\n    `squeeze_axis=1` removes the axis with length 1 from the shapes of the output arrays.\n    **Note** that setting `squeeze_axis` to ``1`` removes axis with length 1 only\n    along the `axis` which it is split.\n    Also `squeeze_axis` can be set to true only if ``input.shape[axis] == num_outputs``.\n\n    Example::\n\n       z = split(x, axis=0, num_outputs=3, squeeze_axis=1) // a list of 3 arrays with shape (2, 1)\n       z = [[ 1.]\n            [ 2.]]\n\n           [[ 3.]\n            [ 4.]]\n\n           [[ 5.]\n            [ 6.]]\n       z[0].shape = (2 ,1 )\n\n\n\n    Defined in ../src/operator/slice_channel.cc:L106\n\n    Parameters\n    ----------\n    data : NDArray\n        The input\n    num_outputs : int, required\n        Number of splits. Note that this should evenly divide the length of the `axis`.\n    axis : int, optional, default='1'\n        Axis along which to split.\n    squeeze_axis : boolean, optional, default=0\n        If true, Removes the axis with length 1 from the shapes of the output arrays. **Note** that setting `squeeze_axis` to ``true`` removes axis with length 1 only along the `axis` which it is split. Also `squeeze_axis` can be set to ``true`` only if ``input.shape[axis] == num_outputs``.\n\n    out : NDArray, optional\n        The output NDArray to hold the result.\n\n    Returns\n    -------\n    out : NDArray or list of NDArrays\n        The output of this function.\n    \"\"\"\n    return (0,)\n",
  "sig": "split(data|The input)(num_outputs|Number of splits. Note that this should evenly divide the length of the axis)(axis|Axis along which to split)(squeeze_axis|If true, Removes the axis with length 1 from the shapes of the output arrays. Note that setting squeeze_axis to true removes axis with length 1 only along the axis which it is split. Also squeeze_axis can be set to true only if input.shape[axis] == num_outputs)(out|The output NDArray to hold the result)(name|NULL)",
  "dst": "Splits an array along a particular axis into multiple sub-arrays.    .. note:: ``SliceChannel`` is deprecated. Use ``split`` instead.    **Note** that `num_outputs` should evenly divide the length of the axis    along which to split the array.    Example::       x  = [[[ 1.]              [ 2.]]             [[ 3.]              [ 4.]]             [[ 5.]              [ 6.]]]       x.shape = (3, 2, 1)       y = split(x, axis=1, num_outputs=2) // a list of 2 arrays with shape (3, 1, 1)       y = [[[ 1.]]            [[ 3.]]            [[ 5.]]]           [[[ 2.]]            [[ 4.]]            [[ 6.]]]       y[0].shape = (3, 1, 1)       z = split(x, axis=0, num_outputs=3) // a list of 3 arrays with shape (1, 2, 1)       z = [[[ 1.]             [ 2.]]]           [[[ 3.]             [ 4.]]]           [[[ 5.]             [ 6.]]]       z[0].shape = (1, 2, 1)    `squeeze_axis=1` removes the axis with length 1 from the shapes of the output arrays.    **Note** that setting `squeeze_axis` to ``1`` removes axis with length 1 only    along the `axis` which it is split.    Also `squeeze_axis` can be set to true only if ``input.shape[axis] == num_outputs``.    Example::       z = split(x, axis=0, num_outputs=3, squeeze_axis=1) // a list of 3 arrays with shape (2, 1)       z = [[ 1.]            [ 2.]]           [[ 3.]            [ 4.]]           [[ 5.]            [ 6.]]       z[0].shape = (2 ,1 )    Defined in ../src/operator/slice_channel.cc:L106    Parameters    ----------    data : NDArray        The input    num_outputs : int, required        Number of splits. Note that this should evenly divide the length of the `axis`.    axis : int, optional, default='1'        Axis along which to split.    squeeze_axis : boolean, optional, default=0        If true, Removes the axis with length 1 from the shapes of the output arrays. **Note** that setting `squeeze_axis` to ``true`` removes axis with length 1 only along the `axis` which it is split. Also `squeeze_axis` can be set to ``true`` only if ``input.shape[axis] == num_outputs``.    out : NDArray, optional        The output NDArray to hold the result.    Returns    -------    out : NDArray or list of NDArrays        The output of this function."
 },
 "mxnet.np.unique":{
    "code": "@set_module('mxnet.ndarray.numpy')\ndef unique(ar, return_index=False, return_inverse=False, return_counts=False, axis=None):\n    \"\"\"\n    Find the unique elements of an array.\n    Returns the sorted unique elements of an array. There are three optional\n    outputs in addition to the unique elements:\n    * the indices of the input array that give the unique values\n    * the indices of the unique array that reconstruct the input array\n    * the number of times each unique value comes up in the input array\n    Parameters\n    ----------\n    ar : ndarray\n        Input array. Unless `axis` is specified, this will be flattened if it\n        is not already 1-D.\n    return_index : bool, optional\n        If True, also return the indices of `ar` (along the specified axis,\n        if provided, or in the flattened array) that result in the unique array.\n    return_inverse : bool, optional\n        If True, also return the indices of the unique array (for the specified\n        axis, if provided) that can be used to reconstruct `ar`.\n    return_counts : bool, optional\n        If True, also return the number of times each unique item appears\n        in `ar`.\n    axis : int or None, optional\n        The axis to operate on. If None, `ar` will be flattened. If an integer,\n        the subarrays indexed by the given axis will be flattened and treated\n        as the elements of a 1-D array with the dimension of the given axis,\n        see the notes for more details. The default is None.\n    Returns\n    -------\n    unique : ndarray\n        The sorted unique values.\n    unique_indices : ndarray, optional\n        The indices of the first occurrences of the unique values in the\n        original array. Only provided if `return_index` is True.\n    unique_inverse : ndarray, optional\n        The indices to reconstruct the original array from the\n        unique array. Only provided if `return_inverse` is True.\n    unique_counts : ndarray, optional\n        The number of times each of the unique values comes up in the\n        original array. Only provided if `return_counts` is True.\n    Notes\n    -----\n    When an axis is specified the subarrays indexed by the axis are sorted.\n    This is done by making the specified axis the first dimension of the array\n    and then flattening the subarrays in C order. The flattened subarrays are\n    then viewed as a structured type with each element given a label, with the\n    effect that we end up with a 1-D array of structured types that can be\n    treated in the same way as any other 1-D array. The result is that the\n    flattened subarrays are sorted in lexicographic order starting with the\n    first element.\n    This function differs from the original `numpy.unique\n    <https://docs.scipy.org/doc/numpy/reference/generated/numpy.unique.html>`_ in\n    the following aspects:\n    - Only support ndarray as input.\n    - Object arrays or structured arrays are not supported.\n    Examples\n    --------\n    >>> np.unique(np.array([1, 1, 2, 2, 3, 3]))\n    array([1., 2., 3.])\n    >>> a = np.array([[1, 1], [2, 3]])\n    >>> np.unique(a)\n    array([1., 2., 3.])\n    Return the unique rows of a 2D array\n    >>> a = np.array([[1, 0, 0], [1, 0, 0], [2, 3, 4]])\n    >>> np.unique(a, axis=0)\n    array([[1., 0., 0.],\n           [2., 3., 4.]])\n    Return the indices of the original array that give the unique values:\n    >>> a = np.array([1, 2, 6, 4, 2, 3, 2])\n    >>> u, indices = np.unique(a, return_index=True)\n    >>> u\n    array([1., 2., 3., 4., 6.])\n    >>> indices\n    array([0, 1, 5, 3, 2], dtype=int64)\n    >>> a[indices]\n    array([1., 2., 3., 4., 6.])\n    Reconstruct the input array from the unique values:\n    >>> a = np.array([1, 2, 6, 4, 2, 3, 2])\n    >>> u, indices = np.unique(a, return_inverse=True)\n    >>> u\n    array([1., 2., 3., 4., 6.])\n    >>> indices\n    array([0, 1, 4, 3, 1, 2, 1], dtype=int64)\n    >>> u[indices]\n    array([1., 2., 6., 4., 2., 3., 2.])\n    \"\"\"\n    ret = list(_api_internal.unique(ar, return_index, return_inverse, return_counts, axis))\n    return ret[0] if len(ret) == 1 else tuple(ret)",
    "sig": "unique(ar| Input array. Unless axis is specified, this will be flattened if it is not already 1-D.)(return_index|If True, also return the indices of ar (along the specified axis, if provided, or in the flattened array) that result in the unique array.)(return_inverse|If True, also return the indices of the unique array (for the specified axis, if provided) that can be used to reconstruct ar.)(return_counts|If True, also return the number of times each unique item appears in ar.)(axis|The axis to operate on. If None, ar will be flattened. If an integer, the subarrays indexed by the given axis will be flattened and treated as the elements of a 1-D array with the dimension of the given axis, see the notes for more details. The default is None.)",
    "dst": "Find the unique elements of an array.\n    Returns the sorted unique elements of an array. There are three optional\n    outputs in addition to the unique elements:\n    * the indices of the input array that give the unique values\n    * the indices of the unique array that reconstruct the input array\n    * the number of times each unique value comes up in the input array"
 },
 "mxnet.np.meshgrid":{
    "code": "def meshgrid(*xi, **kwargs):\n    \"\"\"\n    Return coordinate matrices from coordinate vectors.\n    Make N-D coordinate arrays for vectorized evaluations of\n    N-D scalar/vector fields over N-D grids, given\n    one-dimensional coordinate arrays x1, x2,..., xn.\n    Parameters\n    ----------\n    x1, x2,..., xn : ndarrays\n        1-D arrays representing the coordinates of a grid.\n    indexing : {'xy', 'ij'}, optional\n        Cartesian ('xy', default) or matrix ('ij') indexing of output.\n        See Notes for more details.\n    sparse : bool, optional\n        If True a sparse grid is returned in order to conserve memory.\n        Default is False. Please note that `sparse=True` is currently\n        not supported.\n    copy : bool, optional\n        If False, a view into the original arrays are returned in order to\n        conserve memory.  Default is True. Please note that `copy=False`\n        is currently not supported.\n    Returns\n    -------\n    X1, X2,..., XN : ndarray\n        For vectors `x1`, `x2`,..., 'xn' with lengths ``Ni=len(xi)`` ,\n        return ``(N1, N2, N3,...Nn)`` shaped arrays if indexing='ij'\n        or ``(N2, N1, N3,...Nn)`` shaped arrays if indexing='xy'\n        with the elements of `xi` repeated to fill the matrix along\n        the first dimension for `x1`, the second for `x2` and so on.\n    Notes\n    -----\n    This function supports both indexing conventions through the indexing\n    keyword argument.  Giving the string 'ij' returns a meshgrid with\n    matrix indexing, while 'xy' returns a meshgrid with Cartesian indexing.\n    In the 2-D case with inputs of length M and N, the outputs are of shape\n    (N, M) for 'xy' indexing and (M, N) for 'ij' indexing.  In the 3-D case\n    with inputs of length M, N and P, outputs are of shape (N, M, P) for\n    'xy' indexing and (M, N, P) for 'ij' indexing.  The difference is\n    illustrated by the following code snippet::\n        xv, yv = np.meshgrid(x, y, sparse=False, indexing='ij')\n        for i in range(nx):\n            for j in range(ny):\n                # treat xv[i,j], yv[i,j]\n        xv, yv = np.meshgrid(x, y, sparse=False, indexing='xy')\n        for i in range(nx):\n            for j in range(ny):\n                # treat xv[j,i], yv[j,i]\n    In the 1-D and 0-D case, the indexing and sparse keywords have no effect.\n    \"\"\"\n    ndim = len(xi)\n\n    copy_ = kwargs.pop('copy', True)\n    if not copy_:\n        raise NotImplementedError('copy=False is not implemented')\n    sparse = kwargs.pop('sparse', False)\n    if sparse:\n        raise NotImplementedError('sparse=False is not implemented')\n    indexing = kwargs.pop('indexing', 'xy')\n\n    if kwargs:\n        raise TypeError(f\"meshgrid() got an unexpected keyword argument '{list(kwargs)[0]}'\")\n\n    if indexing not in ['xy', 'ij']:\n        raise ValueError(\n            \"Valid values for `indexing` are 'xy' and 'ij'.\")\n\n    s0 = (1,) * ndim\n    output = [x.reshape(s0[:i] + (-1,) + s0[i + 1:])\n              for i, x in enumerate(xi)]\n\n    if indexing == 'xy' and ndim > 1:\n        # switch first and second axis\n        output[0] = output[0].reshape(1, -1, *s0[2:])\n        output[1] = output[1].reshape(-1, 1, *s0[2:])\n\n    if not sparse:\n        # Return the full N-D matrix (not only the 1-D vector)\n        output = broadcast_arrays(*output)\n\n    return output",
    "sig": "meshgrid(x2,.., xn|1-D arrays representing the coordinates of a grid)(indexing|Cartesian ('xy', default) or matrix ('ij') indexing of output. See Notes for more details.)(sparse|If True a sparse grid is returned in order to conserve memory. Default is False. Please note that sparse=True is currently not supported.)(copy|If False, a view into the original arrays are returned in order to conserve memory. Default is True. Please note that copy=False is currently not supported)",
    "dst": "Return coordinate matrices from coordinate vectors.\n    Make N-D coordinate arrays for vectorized evaluations of\n    N-D scalar/vector fields over N-D grids, given\n    one-dimensional coordinate arrays x1, x2,..., xn."
 },
 "mxnet.np.eye":{
    "code": "def eye(N, M=0, k=0, ctx=None, dtype=None, **kwargs):\n\"\"\"Return a 2-D array with ones on the diagonal and zeros elsewhere.\n    Parameters\n    ----------\n    N: int\n        Number of rows in the output.\n    M: int, optional\n        Number of columns in the output. If 0, defaults to N.\n    k: int, optional\n        Index of the diagonal: 0 (the default) refers to the main diagonal,\n        a positive value refers to an upper diagonal,\n        and a negative value to a lower diagonal.\n    ctx: Context, optional\n        An optional device context (default is the current default context)\n    dtype: str or numpy.dtype, optional\n        An optional value type (default is `float32`)\n    Returns\n    -------\n    NDArray\n        A created array\n    Examples\n    --------\n    >>> mx.nd.eye(2)\n    [[ 1.  0.]\n     [ 0.  1.]]\n    <NDArray 2x2 @cpu(0)>\n    >>> mx.nd.eye(2, 3, 1)\n    [[ 0.  1.  0.]\n     [ 0.  0.  1.]]\n    <NDArray 2x3 @cpu(0)>\n    \"\"\"\n    # pylint: disable= unused-argument\n    if ctx is None:\n        ctx = current_device()\n    dtype = mx_real_t if dtype is None else dtype\n    # pylint: disable= no-member, protected-access\n    return _internal._eye(N=N, M=M, k=k, ctx=ctx, dtype=dtype, **kwargs)\n    # pylint: enable= no-member, protected-access",
    "sig": "eye(N| Number of rows in the output)(M|Number of columns in the output. If None, defaults to N.)(k| Index of the diagonal: 0 (the default) refers to the main diagonal, a positive value refers to an upper diagonal, and a negative value to a lower diagonal)(dtype|Data-type of the returned array. When npx.is_np_default_dtype() returns False, default dtype is float32; When npx.is_np_default_dtype() returns True, default dtype is float64.)(device| Device context on which the memory is allocated. Default is mxnet.device.current_device().)",
    "dst": "Return a 2-D array with ones on the diagonal and zeros elsewhere"
 },
 "mxnet.np.einsum":{
    "code": "@set_module('mxnet.ndarray.numpy') def einsum(subscripts, *operands, out=None, optimize=False): r\"\"\" einsum(subscripts, *operands, out=None, optimize=False) Evaluates the Einstein summation convention on the operands. Using the Einstein summation convention, many common multi-dimensional, linear algebraic array operations can be represented in a simple fashion. In *implicit* mode `einsum` computes these values. In *explicit* mode, `einsum` provides further flexibility to compute other array operations that might not be considered classical Einstein summation operations, by disabling, or forcing summation over specified subscript labels. See the notes and examples for clarification. Parameters ---------- subscripts : str Specifies the subscripts for summation as comma separated list of subscript labels. An implicit (classical Einstein summation) calculation is performed unless the explicit indicator '->' is included as well as subscript labels of the precise output form. operands : list of ndarray These are the arrays for the operation. out : ndarray, optional If provided, the calculation is done into this array. optimize : {False, True}, optional Controls if intermediate optimization should occur. No optimization will occur if False. Defaults to False. Returns ------- output : ndarray The calculation based on the Einstein summation convention. Notes ----- The Einstein summation convention can be used to compute many multi-dimensional, linear algebraic array operations. `einsum` provides a succinct way of representing these. A non-exhaustive list of these operations, which can be computed by `einsum`, is shown below along with examples: * Trace of an array, :py:func:`np.trace`. * Return a diagonal, :py:func:`np.diag`. * Array axis summations, :py:func:`np.sum`. * Transpositions and permutations, :py:func:`np.transpose`. * Matrix multiplication and dot product, :py:func:`np.matmul` :py:func:`np.dot`. * Vector inner and outer products, :py:func:`np.inner` :py:func:`np.outer`. * Broadcasting, element-wise and scalar multiplication, :py:func:`np.multiply`. * Tensor contractions, :py:func:`np.tensordot`. The subscripts string is a comma-separated list of subscript labels, where each label refers to a dimension of the corresponding operand. Whenever a label is repeated it is summed, so ``np.einsum('i,i', a, b)`` is equivalent to :py:func:`np.inner(a,b) <np.inner>`. If a label appears only once, it is not summed, so ``np.einsum('i', a)`` produces a view of ``a`` with no changes. A further example ``np.einsum('ij,jk', a, b)`` describes traditional matrix multiplication and is equivalent to :py:func:`np.matmul(a,b) <np.matmul>`. Repeated subscript labels in one operand take the diagonal. For example, ``np.einsum('ii', a)`` is equivalent to :py:func:`np.trace(a) <np.trace>`. In *implicit mode*, the chosen subscripts are important since the axes of the output are reordered alphabetically.  This means that ``np.einsum('ij', a)`` doesn't affect a 2D array, while ``np.einsum('ji', a)`` takes its transpose. Additionally, ``np.einsum('ij,jk', a, b)`` returns a matrix multiplication, while, ``np.einsum('ij,jh', a, b)`` returns the transpose of the multiplication since subscript 'h' precedes subscript 'i'. In *explicit mode* the output can be directly controlled by specifying output subscript labels.  This requires the identifier '->' as well as the list of output subscript labels. This feature increases the flexibility of the function since summing can be disabled or forced when required. The call ``np.einsum('i->', a)`` is like :py:func:`np.sum(a, axis=-1) <np.sum>`, and ``np.einsum('ii->i', a)`` is like :py:func:`np.diag(a) <np.diag>`. The difference is that `einsum` does not allow broadcasting by default. Additionally ``np.einsum('ij,jh->ih', a, b)`` directly specifies the order of the output subscript labels and therefore returns matrix multiplication, unlike the example above in implicit mode. To enable and control broadcasting, use an ellipsis.  Default NumPy-style broadcasting is done by adding an ellipsis to the left of each term, like ``np.einsum('...ii->...i', a)``. To take the trace along the first and last axes, you can do ``np.einsum('i...i', a)``, or to do a matrix-matrix product with the left-most indices instead of rightmost, one can do ``np.einsum('ij...,jk...->ik...', a, b)``. When there is only one operand, no axes are summed, and no output parameter is provided, a view into the operand is returned instead of a new array.  Thus, taking the diagonal as ``np.einsum('ii->i', a)`` produces a view. The ``optimize`` argument which will optimize the contraction order of an einsum expression. For a contraction with three or more operands this can greatly increase the computational efficiency at the cost of a larger memory footprint during computation. Typically a 'greedy' algorithm is applied which empirical tests have shown returns the optimal path in the majority of cases. 'optimal' is not supported for now. This function differs from the original `numpy.einsum <https://docs.scipy.org/doc/numpy/reference/generated/numpy.einsum.html>`_ in the following way(s): - Does not support 'optimal' strategy - Does not support the alternative subscript like `einsum(op0, sublist0, op1, sublist1, ..., [sublistout])` - Does not produce view in any cases Examples -------- >>> a = np.arange(25).reshape(5,5) >>> b = np.arange(5) >>> c = np.arange(6).reshape(2,3) Trace of a matrix: >>> np.einsum('ii', a) array(60.) Extract the diagonal (requires explicit form): >>> np.einsum('ii->i', a) array([ 0.,  6., 12., 18., 24.]) Sum over an axis (requires explicit form): >>> np.einsum('ij->i', a) array([ 10.,  35.,  60.,  85., 110.]) >>> np.sum(a, axis=1) array([ 10.,  35.,  60.,  85., 110.]) For higher dimensional arrays summing a single axis can be done with ellipsis: >>> np.einsum('...j->...', a) array([ 10.,  35.,  60.,  85., 110.]) Compute a matrix transpose, or reorder any number of axes: >>> np.einsum('ji', c) array([[0., 3.], [1., 4.], [2., 5.]]) >>> np.einsum('ij->ji', c) array([[0., 3.], [1., 4.], [2., 5.]]) >>> np.transpose(c) array([[0., 3.], [1., 4.], [2., 5.]]) Vector inner products: >>> np.einsum('i,i', b, b) array(30.) Matrix vector multiplication: >>> np.einsum('ij,j', a, b) array([ 30.,  80., 130., 180., 230.]) >>> np.dot(a, b) array([ 30.,  80., 130., 180., 230.]) >>> np.einsum('...j,j', a, b) array([ 30.,  80., 130., 180., 230.]) Broadcasting and scalar multiplication: >>> np.einsum('..., ...', np.array(3), c) array([[ 0.,  3.,  6.], [ 9., 12., 15.]]) >>> np.einsum(',ij', np.array(3), c) array([[ 0.,  3.,  6.], [ 9., 12., 15.]]) >>> np.multiply(3, c) array([[ 0.,  3.,  6.], [ 9., 12., 15.]]) Vector outer product: >>> np.einsum('i,j', np.arange(2)+1, b) array([[0., 1., 2., 3., 4.], [0., 2., 4., 6., 8.]]) Tensor contraction: >>> a = np.arange(60.).reshape(3,4,5) >>> b = np.arange(24.).reshape(4,3,2) >>> np.einsum('ijk,jil->kl', a, b) array([[4400., 4730.], [4532., 4874.], [4664., 5018.], [4796., 5162.], [4928., 5306.]]) Example of ellipsis use: >>> a = np.arange(6).reshape((3,2)) >>> b = np.arange(12).reshape((4,3)) >>> np.einsum('ki,jk->ij', a, b) array([[10., 28., 46., 64.], [13., 40., 67., 94.]]) >>> np.einsum('ki,...k->i...', a, b) array([[10., 28., 46., 64.], [13., 40., 67., 94.]]) >>> np.einsum('k...,jk', a, b) array([[10., 28., 46., 64.], [13., 40., 67., 94.]]) Chained array operations. For more complicated contractions, speed ups might be achieved by repeatedly computing a 'greedy' path. Performance improvements can be particularly significant with larger arrays: >>> a = np.ones(64).reshape(2,4,8) # Basic `einsum`: ~42.22ms  (benchmarked on 3.4GHz Intel Xeon.) >>> for iteration in range(500): ...     np.einsum('ijk,ilm,njm,nlk,abc->',a,a,a,a,a) # Greedy `einsum` (faster optimal path approximation): ~0.117ms >>> for iteration in range(500): ...     np.einsum('ijk,ilm,njm,nlk,abc->',a,a,a,a,a, optimize=True) \"\"\" # Grab non-einsum kwargs; do not optimize by default. optimize_arg = kwargs.pop('optimize', False) out = kwargs.pop('out', None) subscripts = operands[0] operands = operands[1:] return _api_internal.einsum(*operands, subscripts, out, int(optimize_arg))",
    "sig": "einsum(subscripts|Specifies the subscripts for summation as comma separated list of subscript labels. An implicit (classical Einstein summation) calculation is performed unless the explicit indicator ‘->’ is included as well as subscript labels of the precise output form.)(operands| These are the arrays for the operation)(out|If provided, the calculation is done into this array)(optimize|Controls if intermediate optimization should occur. No optimization will occur if False. Defaults to False.)",
    "dst": "Evaluates the Einstein summation convention on the operands.\n    Using the Einstein summation convention, many common multi-dimensional,\n    linear algebraic array operations can be represented in a simple fashion.\n    In *implicit* mode `einsum` computes these values.\n    In *explicit* mode, `einsum` provides further flexibility to compute\n    other array operations that might not be considered classical Einstein\n    summation operations, by disabling, or forcing summation over specified\n    subscript labels.\n    See the notes and examples for clarification."
 }
 

}