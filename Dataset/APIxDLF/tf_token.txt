class aggregationmethod object     string   add n = 0   default = add n      experimental tree = 1   experimental accumulate n = 2   
class criticalsection object     string    def   init   self  name=none  share name=none                 critical section def=none  import scope=none       string     context ensure initialize       if critical section def and name be not none        raise valueerror string                        string      if critical section def        raise valueerror string      else        self  init from args name  share name     def  init from args self  name  share name         string     with ops name scope name  string      as name        with ops init scope                     container = ops get default graph    container                  if share name be none            share name = name         if container be none            container = string         self  handle = gen resource variable ops mutex v2              share name=shared name  container=container  name=name                   self  signature =               container                           share name or id self  handle                get device or colocation self  handle        if not context execute eagerly          ops add to collections critical section  self      property   def name self       return self  handle op name    def execute self  fn  exclusive resource access=true  name=none       string     with ops name scope name  string                           with  push critical section stack self  signature           lock = gen resource variable ops mutex lock self  handle           if not context execute eagerly                                    with ops get default graph    lock                exist ops = ops get default graph   get operations               with ops control dependencies  lock                  r = fn                                                      create ops =  set ops get default graph   get operations                                difference exist ops           else            with ops control dependencies  lock                r = fn          if not context execute eagerly            self  add control dependencies to lock create ops  lock op                                      capture resources = object identity objectidentityset               input  for op in create ops             for input  in op input             if input  dtype == dtypes resource                                                         if any self  be self handle x  for x in capture resources             raise valueerror                string               string           self  check multiple access to resources              capture resources  exclusive resource access         r flat =   identity x  for x in nest flatten r          with ops control dependencies r flat                    with ops colocate with self  handle                                              ensure lock exist = gen resource variable ops consume mutex lock                lock                             r = nest pack sequence as r  control flow ops tuple nest flatten r           with ops control dependencies  ensure lock exist            output = nest map structure  identity  r         if not context execute eagerly            signature =  executionsignature              op=lock op              handle=self  handle              resources=list capture resources               exclusive resource access=exclusive resource access          ops add to collections              critical section executions  signature         return output    def  add control dependencies to lock self  create ops  lock op       string          all args = set  input  op for op in create ops for input  in op input       all args update          input op for op in create ops for input op in op control input                           all args dict = dict  op  id  op  for op in all args                 for op in create ops        all args dict pop op  id  none      for op in lock op control input        all args dict pop op  id  none      for input  in lock op input        all args dict pop input  op  id  none      all args dict pop lock op  id  none       all args = all args dict value        if not all args               return                          all args = control flow ops group  all args       lock op  add control input all args          def  be self handle self  x       string     if isinstance x  ops eagertensor         return x be self  handle     return  x op type == string                          and x op get attr string              and  x op get attr string  ==                  self  handle op get attr string               and  x op device == self  handle op device                  or  get colocation x op  ==  get colocation self  handle op       def  check multiple access to resources        self  capture resources  exclusive resource access       string                    for sg in ops get collection critical section executions         if self  be self handle sg handle                    continue       if not  exclusive resource access or sg exclusive resource access                    continue       resource intersection = capture resources intersection sg resources        if resource intersection          raise valueerror              string             string             string             string             string                list resource intersection   self  handle  sg  sg handle   
class devicespecv2 object     string      slot   =  string  string  string  string  string                 string  string     def   init   self  job=none  replica=none  task=none  device type=none                 device index=none       string     self  job =  as str or none job      self  replica =  as int or none replica      self  task =  as int or none task      self  device type =  as device str or none device type      self  device index =  as int or none device index      self  as string = self  components to string          job=self  job  replica=self  replica  task=self  task          device type=self  device type  device index=self  device index      self  hash = hash self to string       def to string self       string     return self  as string     classmethod   def from string cls  spec       string     return cls  cls  string to components spec      def parse from string self  spec       string     return self from string spec     def make merge spec self  dev       string     return self   class    self  get combine properties dev      def replace self    kwargs       string     init kwargs = dict          job=self job  replica=self replica  task=self task          device type=self device type  device index=self device index            init kwargs update kwargs      return self   class     init kwargs      property   def job self       return self  job     property   def replica self       return self  replica     property   def task self       return self  task     property   def device type self       return self  device type     property   def device index self       return self  device index    def  get combine properties self  dev       string     return           dev job if dev job be not none else self job          dev replica if dev replica be not none else self replica          dev task if dev task be not none else self task          dev device type if dev device type be not none else self device type          dev device index if dev device index be not none else self device index            staticmethod   def  string to components spec=none       string     cache result =  string to components cache get spec      if cache result be not none        return cache result      raw spec = spec       job  replica  task  device type  device index = none  none  none  none  none      spec = spec or string     split =  x split string  for x in spec split string       for y in split        ly = len y        if y                   if ly == 2 and y 0  == string            job = y 1          elif ly == 2 and y 0  == string            replica = y 1          elif ly == 2 and y 0  == string            task = y 1          elif   ly == 1 or ly == 2  and                 y 0  upper   == string  or  y 0  upper   == string               if device type be not none              raise valueerror string   spec            device type = y 0  upper             if ly == 2 and y 1   = string              device index = int y 1           elif ly == 3 and y 0  == string            if device type be not none              raise valueerror string   spec            device type = y 1            if y 2   = string              device index = int y 2           elif ly and y 0   = string              raise valueerror string    y 0   spec        output =  job  replica  task  device type  device index       string to components cache raw spec  = output     return output     staticmethod   def  components to string job  replica  task  device type  device index       string     key =  job  replica  task  device type  device index      cache result =  components to string cache get key      if cache result be not none        return cache result      output =        if job be not none        output append string   job      if replica be not none        output append string   str replica       if task be not none        output append string   str task       if device type be not none        device index string = string       if device index be not none                   device index string = str device index        output append string    device type  device index string        output = string join output       components to string cache key  = output     return output    def   eq   self  other       string     return  isinstance other  self   class    and             self to string   == other to string       def   hash   self       return self  hash 
class gradienttape object     string    def   init   self  persistent=false  watch access variables=true       string     self  tape = none     self  persistent = persistent     self  watch access variables = watch access variables     self  watch variables =        self  record = false     self  create eagerly = context execute eagerly       if self  create eagerly        context ensure initialize         context context   start step      def   enter   self       string     self  push tape       return self    def   exit   self  typ  value  traceback       string     if self  record        self  pop tape      def  push tape self       string     if self  record        raise valueerror string                        string      if self  tape be none        self  tape = tape push new tape            persistent=self  persistent            watch access variables=self  watch access variables      else        tape push tape self  tape      self  record = true    def  pop tape self       if not self  record        raise valueerror string      tape pop tape self  tape      self  record = false    def   del   self       if self  create eagerly        try          context context   end step         except attributeerror          pass       except typeerror          pass    def watch self  tensor       string     for t in nest flatten tensor         if not   pywrap utils istensor t  or  pywrap utils isvariable t            raise valueerror string format              type t          if not backprop util istrainable t           log log first n              log warn  string             string  5  t dtype        if hasattr t  string                                      tape watch variable self  tape  t        else          tape watch self  tape  t      tf contextlib contextmanager   def stop record self       string     if self  tape be none        raise runtimeerror            string      self  pop tape       try        yield     finally        self  push tape      def reset self       string     self  pop tape       self  tape = none     self  push tape      def watch variables self       string     if self  tape be not none        self  watch variables = self  tape watch variables       return self  watch variables    def gradient self                 target                 source                 output gradients=none                 unconnected gradients=unconnectedgradients none       string     if self  tape be none        raise runtimeerror string                          string      if self  record        if not self  persistent          self  pop tape         else          log log first n              log warn  string             string             string             string             string             string             string             string             string  1       flat target =        for t in nest flatten target         if not backprop util istrainable t           log vlog              log warn  string             string             string  t dtype        if resource variable ops be resource variable t           with self            t = ops convert to tensor t        flat target append t       flat source = nest flatten source      flat source raw = flat source     flat source =   handle or self x  for x in flat source      for t in flat source raw        if not backprop util istrainable t           log vlog              log warn  string             string             string  t dtype       if output gradients be not none        output gradients =  none if x be none else ops convert to tensor x                            for x in nest flatten output gradients        flat grad = imperative grad imperative grad          self  tape          flat target          flat source          output gradients=output gradients          source raw=flat source raw          unconnected gradients=unconnected gradients       if not self  persistent               self  watch variables = self  tape watch variables         self  tape = none      grad = nest pack sequence as source  flat grad      return grad    def jacobian self                 target                 source                 unconnected gradients=unconnectedgradients none                 parallel iterations=none                 experimental use pfor=true       string     flat source = nest flatten source      target static shape = target shape     target shape = array ops shape target                self  push tape       target = array ops reshape target   -1       self  pop tape        def loop fn i         self  push tape         y = array ops gather target  i        self  pop tape         return self gradient y  flat source                             unconnected gradients=unconnected gradients       try        target size = int target shape 0       except typeerror        target size = array ops shape target  0       if experimental use pfor        try          output = pfor ops pfor loop fn  target size                                 parallel iterations=parallel iterations        except valueerror as err          six reraise              valueerror              valueerror                  str err    string                 string                 string               sys exc info   2       else        if context execute eagerly   and not self  persistent          raise runtimeerror              string             string             string        output = pfor ops for loop            loop fn   target dtype    len flat source   target size            parallel iterations=parallel iterations       for i  out in enumerate output         if out be not none          new shape = array ops concat               target shape  array ops shape out  1     axis=0          out = array ops reshape out  new shape          if context execute eagerly              out set shape target static shape concatenate flat source i  shape         output i  = out      return nest pack sequence as source  output     def batch jacobian self                       target                       source                       unconnected gradients=unconnectedgradients none                       parallel iterations=none                       experimental use pfor=true       string     target shape = target shape     if target shape rank be none        dim = tensor shape dimension none      else        dim = target shape dim 0      if not  target shape with rank at least 2  and             source shape with rank at least 2  and             dim be compatible with source shape 0           raise valueerror            string           string    target shape  source shape       if target shape be fully define          batch size = int target shape 0         target row size = target shape num elements   // batch size     else        target shape = array ops shape target        batch size = target shape 0        target row size = array ops size target  // batch size     source shape = array ops shape source                     self  push tape       with ops control dependencies           check ops assert equal batch size  source shape 0            target = array ops reshape target   batch size  target row size       self  pop tape        def loop fn i         self  push tape         y = array ops gather target  i  axis=1        self  pop tape         return self gradient y  source                             unconnected gradients=unconnected gradients       if experimental use pfor        try          output = pfor ops pfor loop fn  target row size                                 parallel iterations=parallel iterations        except valueerror as err          six reraise              valueerror              valueerror                  str err    string                 string                 string               sys exc info   2       else        if context execute eagerly   and not self  persistent          raise runtimeerror              string             string             string        output = pfor ops for loop loop fn  target dtype  target row size                                   parallel iterations=parallel iterations      new shape = array ops concat  target shape  source shape 1     axis=0      if output be none        return array ops zero new shape      else        output = array ops reshape output                                    target row size  batch size  -1         output = array ops transpose output   1  0  2         return array ops reshape output  new shape  
class graph object     string    def   init   self       string                                   self  lock = thread rlock                                          self  group lock = lock util grouplock num groups=2      self  nod by id =          self  next id counter = 0       self  nod by name =          self  version = 0            self  name in use =        self  stack state be thread local = false     self  thread local = thread local                      self  graph device function stack = traceable stack traceablestack            self  default original op = none               self  control flow context = none                    self  graph control dependencies stack =             self  collections =             self  seed = none          self  attr scope map =             self  op to kernel label map =                  self  gradient override map =             self  gradient function map =                  self  finalize = false          self  function = collections ordereddict            self  graph def versions = versions pb2 versiondef          producer=versions graph def version          min consumer=versions graph def version min consumer      self  build function = false               self  graph colocation stack = traceable stack traceablestack            self  unfeedable tensors = object identity objectidentityset            self  unfetchable ops = set            self  handle feeders =             self  handle readers =             self  handle movers =             self  handle deleters =                            self  graph key = string    uid                        self  last loss reduction = none                         self  be loss scale by optimizer = false     self  container = string               self  add control dependencies = false          self  op def cache =                       self  bcast grad args cache =                       self  reduce shape cache =                   self  scoped c graph = c api util scopedtfgraph                      c api setrequireshapeinferencefns self  c graph  false      if tf2 enable          self switch to thread local             tf contextlib contextmanager   def  variable creator scope self  creator  priority=100       string               old = self  variable creator stack     new = list old      new append  priority  creator                 new sort key=lambda item  item 0       self  thread local  variable creator stack = new       try        yield     finally        if self  thread local  variable creator stack be not new            raise runtimeerror              string        self  thread local  variable creator stack = old             property   def  variable creator stack self       if not hasattr self  thread local  string         self  thread local  variable creator stack =                                                                                                return self  thread local  variable creator stack        variable creator stack setter   def  variable creator stack self  variable creator stack       self  thread local  variable creator stack = variable creator stack      def  check not finalize self       string     if self  finalize        raise runtimeerror string     def  add op self  op  op name       string     self  check not finalize       with self  lock        self  next id counter  = 1       op id = self  next id counter       self  nod by id op id  = op       self  nod by name op name  = op       self  version = max self  version  op id        return op id     property   def  c graph self       if self  scoped c graph        return self  scoped c graph graph     return none     property   def version self       string     if self  finalize        return self  version      with self  lock        return self  version     property   def graph def versions self            string          with c api util tf buffer   as buf        c api tf graphversions self  c graph  buf        data = c api tf getbuffer buf      version def = versions pb2 versiondef       version def parsefromstring compat as bytes data       return version def     property   def seed self       string     return self  seed     seed setter   def seed self  seed       self  seed = seed     property   def finalize self       string     return self  finalize    def finalize self       string     self  finalize = true    def  unsafe unfinalize self       string     self  finalize = false    def  get control flow context self       string     return self  control flow context    def  set control flow context self  ctx       string     self  control flow context = ctx    def  copy function to graph def self  graph def  start bytesize       string     bytesize = start bytesize     for f in self  function value          bytesize  = f definition bytesize         if bytesize >=  1 << 31  or bytesize < 0          raise valueerror string        graph def library function extend  f definition         if f grad func name          grad def = function pb2 gradientdef           grad def function name = f name         grad def gradient func = f grad func name         graph def library gradient extend  grad def      def  as graph def self  from version=none  add shapes=false            string          with self  lock        with c api util tf buffer   as buf          c api tf graphtographdef self  c graph  buf          data = c api tf getbuffer buf        graph = graph pb2 graphdef         graph parsefromstring compat as bytes data                if not graph library function          graph clearfield string         if add shape          for node in graph node            op = self  nod by name node name            if op output              node attr string  list shape extend                   output get shape   as proto   for output in op output           for function def in graph library function            define function = self  function function def signature name            try              func graph = define function graph           except attributeerror                                                     continue           input shape = function def attr string            try              func graph input = func graph input           except attributeerror              continue                                 for input tensor    in zip func graph input                                       function def signature input arg               if input tensor dtype == dtypes resource                                                                            input shape list shape add   copyfrom                    tensor shape tensorshape none  as proto                else                input shape list shape add   copyfrom                    input tensor get shape   as proto              for node in function def node def              try                op = func graph get operation by name node name              except keyerror                continue             output = op output              if op type == string                                              num output = len node attr string  list type                output = output  num output               node attr string  list shape extend                   output get shape   as proto   for output in output        return graph  self  version    def as graph def self  from version=none  add shapes=false            string          result    = self  as graph def from version  add shape      return result    def  be function self  name       string     return compat as str name  in self  function    def  get function self  name       string     return self  function get compat as str name   none     def  add function self  function       string     name = function name          if  function grad func name be not none  and  function python grad func be                                                   not none         raise valueerror string   name                 gradient =           function  grad func  c func func if function  grad func else none      c api tf graphcopyfunction self  c graph  function  c func func  gradient            self  function compat as str name   = function           if self  graph def versions min consumer < 12        self  graph def versions min consumer = 12     property   def build function self       string     return self  build function        deprecate args none                     string                    string  string    def create op        self        op type        input        dtypes=none          input types=none        name=none        attrs=none        op def=none        compute shapes=true        compute device=true       string     del compute shape     for idx  a in enumerate input         if not isinstance a  tensor           raise typeerror string    idx  a       return self  create op internal op type  input  dtypes  input type  name                                      attrs  op def  compute device     def  create op internal        self        op type        input        dtypes=none          input types=none        name=none        attrs=none        op def=none        compute device=true       string     self  check not finalize       if name be none        name = op type               if name and name -1  == string        name = name from scope name name      else        name = self unique name name       node def =  nodedef op type  name  attrs       input ops = set  t op for t in input       control input = self  control dependencies for input input ops                with self  mutation lock          ret = operation            node def            self            inputs=inputs            output types=dtypes            control inputs=control input            input types=input type            original op=self  default original op            op def=op def        self  create op helper ret  compute device=compute device      return ret    def  create op from tf operation self  c op  compute device=true       string     self  check not finalize       ret = operation c op  self                          name key = ret name lower       if name key not in self  name in use        self  name in use name key  = 1     self  create op helper ret  compute device=compute device      return ret    def  create op helper self  op  compute device=true       string               for key  value in self  attr scope map items          try          op get attr key        except valueerror          if callable value             value = value op node def            if not isinstance value   type none   attr value pb2 attrvalue                raise typeerror                  string                 string                    key  value           if value            op  set attr key  value              try        kernel label = self  op to kernel label map op type        op  set attr string                       attr value pb2 attrvalue s=compat as bytes kernel label        except keyerror        pass      op  gradient function = self  gradient function map get op type                   try        map op type = self  gradient override map op type        op  set attr string                       attr value pb2 attrvalue s=compat as bytes map op type        except keyerror        pass      self  record op see by control dependencies op       if compute device        self  apply device function op                           op  colocation code locations = self  snapshot colocation stack metadata             if self  colocation stack        all colocation group =          for colocation op in self  colocation stack peek objs            all colocation group extend colocation op colocation group            if colocation op device                       op  set device colocation op device                    all colocation group = sort set all colocation group                op  set attr            string            attr value pb2 attrvalue                list=attr value pb2 attrvalue listvalue s=all colocation group                                         if self  container and op  be stateful          try          container attr = op get attr string        except valueerror                   pass       else          if not container attr            op  set attr string  attr value pb2 attrvalue                  s=compat as bytes self  container       def  add new tf operations self  compute devices=true       string               new ops =           self  create op from tf operation c op  compute device=compute devices          for c op in c api util new tf operations self                  for op in new ops        new control input = self  control dependencies for input op input        op  add control input new control input        op  control flow post process             return new ops    def as graph element self  obj  allow tensor=true  allow operation=true       string     if self  finalize        return self  as graph element lock obj  allow tensor  allow operation       with self  lock        return self  as graph element lock obj  allow tensor  allow operation     def  as graph element lock self  obj  allow tensor  allow operation       string                                    if allow tensor and allow operation        type str = string     elif allow tensor        type str = string     elif allow operation        type str = string     else        raise valueerror string       temp obj =  as graph element obj      if temp obj be not none        obj = temp obj           if isinstance obj  compat bytes or text type         name = compat as str obj         if string in name and allow tensor                   try            op name  out n = name split string            out n = int out n          except            raise valueerror string                            string                            string   repr name           if op name in self  nod by name            op = self  nod by name op name          else            raise keyerror string                          string                          string    repr name   repr op name            try            return op output out n          except            raise keyerror string                          string                          string                             repr name   repr op name   len op output           elif string in name and not allow tensor                   raise valueerror string                             repr name   type str          elif string not in name and allow operation                   if name not in self  nod by name            raise keyerror string                          string   repr name           return self  nod by name name         elif string not in name and not allow operation                   if name in self  nod by name                       err msg =  string                         repr name   type str           else            err msg =  string                      string    repr name   type str           err msg  =  string                     string          raise valueerror err msg       elif isinstance obj  tensor  and allow tensor               if obj graph be not self          raise valueerror string   obj        return obj     elif isinstance obj  operation  and allow operation               if obj graph be not self          raise valueerror string   obj        return obj     else               raise typeerror string                          type obj    name    type str      def get operations self       string     if self  finalize        return list self  nod by id value         with self  lock        return list self  nod by id value       def get operation by name self  name       string      if not isinstance name  six string type         raise typeerror string                         type name    name        return self as graph element name  allow tensor=false  allow operation=true     def  get operation by name unsafe self  name       string      if self  finalize        return self  nod by name name       with self  lock        return self  nod by name name     def  get operation by tf operation self  tf oper       op name = c api tf operationname tf oper      return self  get operation by name unsafe op name     def get tensor by name self  name       string          if not isinstance name  six string type         raise typeerror string                         type name    name        return self as graph element name  allow tensor=true  allow operation=false     def  get tensor by tf output self  tf output       string     op = self  get operation by tf operation tf output oper      return op output tf output index      property   def  last id self       return self  next id counter    def  get op def self  type         string               try        return self  op def cache type      except keyerror        with c api util tf buffer   as buf                   c api tf graphgetopdef self  c graph  compat as bytes type   buf                   data = c api tf getbuffer buf        op def = op def pb2 opdef         op def parsefromstring compat as bytes data         self  op def cache type  = op def       return op def    def as default self       string     return  default graph stack get controller self      property   def collections self       string     return list self  collections     def add to collection self  name  value       string       self  check not finalize       with self  lock        if name not in self  collections          self  collections name  =  value        else          self  collections name  append value     def add to collections self  name  value       string          name =  name   if isinstance name  six string type  else set name      for name in name        self add to collection name  value     def get collection ref self  name       string       with self  lock        coll list = self  collections get name  none        if coll list be none          coll list =            self  collections name  = coll list       return coll list    def get collection self  name  scope=none       string       with self  lock        collection = self  collections get name  none        if collection be none          return          if scope be none          return list collection        else          c =            regex = re compile scope          for item in collection            try              if regex match item name                 c append item            except attributeerror                           pass         return c    def get all collection key self       string     with self  lock        return  x for x in self  collections if isinstance x  six string type      def clear collection self  name       string     self  check not finalize       with self  lock        if name in self  collections          del self  collections name      tf contextlib contextmanager   def  original op self  op       string     old original op = self  default original op     self  default original op = op     try        yield     finally        self  default original op = old original op     property   def  name stack self            if not hasattr self  thread local  string         self  thread local  name stack = string     return self  thread local  name stack      name stack setter   def  name stack self  name stack       self  thread local  name stack = name stack        tf contextlib contextmanager   def name scope self  name       string     if name        if isinstance name  compat bytes or text type           name = compat as str name         if self  name stack                                     if not  valid scope name regex match name             raise valueerror string   name        else                            if not  valid op name regex match name             raise valueerror string   name      old stack = self  name stack     if not name          new stack = none     elif name -1  == string        new stack = name from scope name name      else        new stack = self unique name name      self  name stack = new stack     try        yield string if new stack be none else new stack   string     finally        self  name stack = old stack        def unique name self  name  mark as used=true       string     if self  name stack        name = self  name stack   string   name                name key = name lower       i = self  name in use get name key  0           if mark as use        self  name in use name key  = i   1     if i > 0        base name key = name key              while name key in self  name in use          name key = string    base name key  i          i  = 1                     if mark as use          self  name in use name key  = 1               name = string    name  i - 1      return name    def get name scope self       string     return self  name stack     tf contextlib contextmanager   def  colocate with for gradient self  op  gradient uid                                    ignore existing=false       with self colocate with op  ignore exist         if gradient uid be not none and self  control flow context be not none          self  control flow context entergradientcolocation op  gradient uid          try            yield         finally            self  control flow context exitgradientcolocation op  gradient uid        else          yield     tf contextlib contextmanager   def colocate with self  op  ignore existing=false       string     if op be none and not ignore exist        raise valueerror string                        string      op =  op to colocate with op  self                                          device fn tmp = self  device function stack     self  device function stack = traceable stack traceablestack        if ignore exist        current stack = self  colocation stack       self  colocation stack = traceable stack traceablestack        if op be not none                             self  colocation stack push obj op  offset=4       try        yield     finally               self  device function stack = device fn tmp       if op be not none          self  colocation stack pop obj                 if ignore exist          self  colocation stack = current stack    def  add device to stack self  device name or function  offset=0       string     total offset = 1   offset     spec =  userdevicespec device name or function      self  device function stack push obj spec  offset=total offset      return spec     tf contextlib contextmanager   def device self  device name or function            string     self  add device to stack device name or function  offset=2      old top of stack = self  device function stack peek top obj       try        yield     finally        new top of stack = self  device function stack peek top obj         if old top of stack be not new top of stack          raise runtimeerror string        self  device function stack pop obj      def  apply device function self  op       string                              prior device string = none     for device spec in self  device function stack peek objs          if device spec be null merge          continue        if device spec function be none          break        device string = device spec string merge op                       if device string be not prior device string          op  set device from string device string          prior device string = device string     op  device code locations = self  snapshot device function stack metadata               tf contextlib contextmanager   def container self  container name       string     original container = self  container     self  container = container name     try        yield self  container     finally        self  container = original container        class  controldependenciescontroller object       string      def   init   self  graph  control input         string       self  graph = graph       if control input be none          self  control input val =            self  new stack = true       else          self  control input val = control input         self  new stack = false       self  see nod = set         self  old stack = none       self  old control flow context = none        def   enter   self         if self  new stack                   self  old stack = self  graph  control dependencies stack         self  graph  control dependencies stack =                     self  old control flow context = self  graph  get control flow context           self  graph  set control flow context none        self  graph  push control dependencies controller self       def   exit   self  unused type  unused value  unused traceback         self  graph  pop control dependencies controller self        if self  new stack          self  graph  control dependencies stack = self  old stack         self  graph  set control flow context self  old control flow context          property     def control input self         return self  control input val      def add op self  op         if isinstance op  tensor           op = op experimental ref         self  see nod add op       def op in group self  op         if isinstance op  tensor           op = op experimental ref         return op in self  see nod    def  push control dependencies controller self  controller       self  control dependencies stack append controller     def  pop control dependencies controller self  controller       assert self  control dependencies stack -1  be controller     self  control dependencies stack pop      def  current control dependencies self       ret = set       for controller in self  control dependencies stack        for op in controller control input          ret add op      return ret    def  control dependencies for input self  input ops       string     ret =        for controller in self  control dependencies stack                             dominate = false       for op in input ops          if controller op in group op             dominate = true           break       if not dominate                                     ret extend  c for c in controller control input if c not in input ops       return ret    def  record op see by control dependencies self  op       string     for controller in self  control dependencies stack        controller add op op     def control dependencies self  control input       string     if control input be none        return self  controldependenciescontroller self  none                          control ops =        current = self  current control dependencies       for c in control input                             if  isinstance c  indexedslices  or            hasattr c  string  and hasattr c  string             c = c op       c = self as graph element c        if isinstance c  tensor           c = c op       elif not isinstance c  operation           raise typeerror string   c        if c not in current          control ops append c          current add c      return self  controldependenciescontroller self  control ops         tf contextlib contextmanager   def  attr scope self  attr map       string     if not isinstance attr map  dict         raise typeerror string                       string                save attrs =             for name  attr in attr map items          if not  isinstance name  six string type  and                isinstance attr   type none   attr value pb2 attrvalue   or                callable attr             raise typeerror string                         string                         string        try          save attrs name  = self  attr scope map name        except keyerror          pass       if attr be none          del self  attr scope map name        else          self  attr scope map name  = attr     try        yield       finally                      for name  attr in attr map items            try            self  attr scope map name  = save attrs name          except keyerror            del self  attr scope map name             tf contextlib contextmanager   def  kernel label map self  op to kernel label map       string     if not isinstance op to kernel label map  dict         raise typeerror string                       string                save label =             for op type  label in op to kernel label map items          if not  isinstance op type  six string type  and               isinstance label  six string type            raise typeerror string                         string        try          save label op type  = self  op to kernel label map op type        except keyerror          pass       self  op to kernel label map op type  = label     try        yield       finally               for op type  label in op to kernel label map items            try            self  op to kernel label map op type  = save label op type          except keyerror            del self  op to kernel label map op type          tf contextlib contextmanager   def  override gradient function self  gradient function map       string           assert not self  gradient function map     self  gradient function map = gradient function map     yield     self  gradient function map =           tf contextlib contextmanager   def gradient override map self  op type map       string     if not isinstance op type map  dict         raise typeerror string                       string                save mappings =             for op type  map op type in op type map items          if not  isinstance op type  six string type  and               isinstance map op type  six string type            raise typeerror string                         string        try          save mappings op type  = self  gradient override map op type        except keyerror          pass       self  gradient override map op type  = map op type     try        yield       finally               for op type  map op type in op type map items            try            self  gradient override map op type  = save mappings op type          except keyerror            del self  gradient override map op type         def prevent feed self  tensor       string     self  unfeedable tensors add tensor     def be feedable self  tensor       string     return tensor not in self  unfeedable tensors    def prevent fetch self  op       string     self  unfetchable ops add op     def be fetchable self  tensor or op       string     if isinstance tensor or op  tensor         return tensor or op op not in self  unfetchable ops     else        return tensor or op not in self  unfetchable ops    def switch to thread local self       string     if not self  stack state be thread local        self  stack state be thread local = true     property   def  device function stack self       if self  stack state be thread local                             if not hasattr self  thread local  string           stack copy for this thread = self  graph device function stack copy           self  thread local  device function stack = stack copy for this thread       return self  thread local  device function stack            else        return self  graph device function stack     property   def  device function outer to inner self       user device specs = self  device function stack peek objs       device function =  spec function for spec in user device specs      device function outer to inner = list reverse device function       return device function outer to inner    def  snapshot device function stack metadata self       string     snapshot =        for obj in self  device function stack peek traceable objs          obj copy = obj copy metadata         obj copy obj = obj obj display name       snapshot append obj copy      return snapshot      device function stack setter   def  device function stack self  device function stack       if self  stack state be thread local               self  thread local  device function stack = device function stack            else        self  graph device function stack = device function stack     property   def  colocation stack self       string     if self  stack state be thread local                             if not hasattr self  thread local  string           stack copy for this thread = self  graph colocation stack copy           self  thread local  colocation stack = stack copy for this thread       return self  thread local  colocation stack            else        return self  graph colocation stack    def  snapshot colocation stack metadata self       string     return           traceable obj obj name  traceable obj copy metadata           for traceable obj in self  colocation stack peek traceable objs              colocation stack setter   def  colocation stack self  colocation stack       if self  stack state be thread local               self  thread local  colocation stack = colocation stack            else        self  graph colocation stack = colocation stack     property   def  control dependencies stack self       if self  stack state be thread local                      if not hasattr self  thread local  string           self  thread local  control dependencies stack =               self  graph control dependencies stack           return self  thread local  control dependencies stack     else        return self  graph control dependencies stack      control dependencies stack setter   def  control dependencies stack self  control dependencies       if self  stack state be thread local        self  thread local  control dependencies stack = control dependencies     else        self  graph control dependencies stack = control dependencies     property   def  distribution strategy stack self       string     if not hasattr self  thread local  string         self  thread local  distribution strategy stack =          return self  thread local  distribution strategy stack        distribution strategy stack setter   def  distribution strategy stack self   distribution strategy stack       self  thread local  distribution strategy stack =              distribution strategy stack      property   def  global distribute strategy scope self       string     if not hasattr self  thread local  string         self  thread local distribute strategy scope = none     return self  thread local distribute strategy scope      global distribute strategy scope setter   def  global distribute strategy scope self  distribute strategy scope       self  thread local distribute strategy scope =  distribute strategy scope      property   def  auto cast variable read dtype self       string     if not hasattr self  thread local  string         self  thread local  auto cast variable read dtype = none       return self  thread local  auto cast variable read dtype        auto cast variable read dtype setter   def  auto cast variable read dtype self  dtype       if dtype        dtype = dtypes as dtype dtype      self  thread local  auto cast variable read dtype = dtype       tf contextlib contextmanager   def  enable auto cast variables self  dtype       string     prev read dtype = self  auto cast variable read dtype     try        self  auto cast variable read dtype = dtype       yield     finally        self  auto cast variable read dtype = prev read dtype    def  mutation lock self       string     return self  group lock group  mutation lock group     def  session run lock self       string     return self  group lock group  session run lock group  
class indexedslices  tensorlike  composite tensor compositetensor     string    def   init   self  value  indices  dense shape=none       string     self  value = value     self  indices = indices     self  dense shape = dense shape     property   def value self       string     return self  value     property   def indices self       string     return self  indices     property   def dense shape self       string     return self  dense shape     property   def shape self       string     if self  dense shape be none        return tensor shape tensorshape none       return tensor util constant value as shape self  dense shape      property   def name self       string     return self value name     property   def device self       string     return self value device     property   def op self       string     return self value op     property   def dtype self       string     return self value dtype     property   def graph self       string     return self  value graph    def   str   self       return string             self  indices  self  value           string            self  dense shape  if self  dense shape be not none else string     def   neg   self       return indexedslices -self value  self indices  self dense shape      property   def  type spec self       indices shape = self  indices shape merge with self  value shape  1       dense shape = tensor shape tensorshape  none   concatenate          self  value shape 1        if self  dense shape be not none        dense shape dtype = self  dense shape dtype       dense shape = dense shape merge with            tensor util constant value as shape self  dense shape       else        dense shape dtype = none     return indexedslicesspec dense shape  self dtype  self  indices dtype                               dense shape dtype  indices shape     def  shape invariant to type spec self  shape                           indices shape = shape  1      dense shape = tensor shape tensorshape  none   concatenate shape 1        if self  dense shape be none        dense shape dtype = none     else        dense shape dtype = self  dense shape dtype     return indexedslicesspec dense shape  self dtype  self  indices dtype                               dense shape dtype  indices shape     def consumers self       return self  consumers   
class indexedslicesspec type spec typespec     string      slot   =  string  string  string                 string  string     value type = property lambda self  indexedslices     def   init   self  shape=none  dtype=dtypes float32                 indices dtype=dtypes int64  dense shape dtype=none                 indices shape=none       string     self  shape = tensor shape as shape shape      self  value dtype = dtypes as dtype dtype      self  indices dtype = dtypes as dtype indices dtype      if dense shape dtype be none        self  dense shape dtype = none     else        self  dense shape dtype = dtypes as dtype dense shape dtype      self  indices shape = tensor shape as shape indices shape  with rank 1     def  serialize self       return  self  shape  self  value dtype  self  indices dtype              self  dense shape dtype  self  indices shape      property   def  component specs self       value shape = self  indices shape concatenate self  shape 1        specs =           tensor spec tensorspec value shape  self  value dtype           tensor spec tensorspec self  indices shape  self  indices dtype       if self  dense shape dtype be not none        specs append            tensor spec tensorspec  self  shape ndims   self  dense shape dtype       return tuple specs     def  to components self  value       if value dense shape be none        return  value value  value indices      else        return  value value  value indices  value dense shape     def  from components self  tensor list       if  all isinstance t  np ndarray  for t in tensor list  and         not tf2 enable           if len tensor list  == 2          return indexedslicesvalue tensor list 0   tensor list 1   none        else          return indexedslicesvalue  tensor list      else        return indexedslices  tensor list  
class module track autotrackable     string              tf module ignore properties = frozenset         string        string         def   init   self  name=none       if name be none        name = camel to snake type self    name        else        if not valid identifier name           raise valueerror              string             string   name       self  name = name     if tf2 enable          with ops name scope v2 name  as scope name          self  name scope = ops name scope v2 scope name      else        with ops name scope name  as scope name          self  scope name = scope name     property   def name self       string     return self  name     property   def name scope self       string     if tf2 enable          return self  name scope     else               return ops name scope self  scope name      property   def variables self       string     return tuple self  flatten predicate= be variable       property   def trainable variables self       string     return tuple self  flatten predicate= be trainable variable       property   def submodules self       string     return tuple self  flatten predicate= be module      def  flatten self                 recursive=true                 predicate=none                 attribute traversal key=none                 with path=false       string     if predicate be none        predicate = lambda    true      return  flatten module          self          recursive=recursive          predicate=predicate          attribute to ignore=self  tf module ignore properties          attribute traversal key=attribute traversal key          with path=with path      classmethod   def with name scope cls  method       string     def method with name scope self   args    kwargs         with self name scope          return method self   args    kwargs       return tf decorator make decorator method  method with name scope  
class operation object     string    def   init   self                 node def                 g                 inputs=none                 output types=none                 control inputs=none                 input types=none                 original op=none                 op def=none       r   create an `operation`       note  this constructor validate the name of the `operation`  pass     as `node def name`   valid `operation` name match the follow     regular expression            a-za-z0-9   a-za-z0-9  \\-/        args        node def  `node def pb2 nodedef`   `nodedef` for the `operation`  use for         attribute of `node def pb2 nodedef`  typically `name`  `op`  and         `device`   the `input` attribute be irrelevant here as it will be         compute when generate the model        g  `graph`  the parent graph        input  list of `tensor` object  the input to this `operation`        output type  list of `dtype` object   list of the type of the `tensors`         compute by this operation   the length of this list indicate the         number of output endpoints of the `operation`        control input  list of operations or tensors from which to have a control         dependency        input type  list of `dtype` object represent the type of the tensors         accept by the `operation`   by default use ` x dtype base dtype for x         in input `   operations that expect reference-typed input must specify         these explicitly        original op  optional  use to associate the new `operation` with an         exist `operation`  for example  a replica with the op that be         replicate         op def  optional  the `op def pb2 opdef` proto that describe the op type         that this `operation` represent       raise        typeerror  if control input be not operations or tensors          or if `node def` be not a `nodedef`          or if `g` be not a `graph`          or if `inputs` be not tensors          or if `inputs` and `input types` be incompatible        valueerror  if the `node def` name be not valid                                        if isinstance node def  node def pb2 nodedef         if node def bytesize   >=  1 << 31  or node def bytesize   < 0          raise valueerror              string        if not  valid op name regex match node def name           raise valueerror string   node def name        c op = none     elif type node def    name   == string        assert input be none       assert output type be none       assert control input be none       assert input type be none       assert original op be none       assert op def be none       c op = node def     else        raise typeerror string   node def       if not isinstance g  graph         raise typeerror string   g      self  graph = g      if input be none        input =        elif not isinstance input  list         raise typeerror string   input      for a in input        if not isinstance a  tensor           raise typeerror string   a      if input type be none        input type =  i dtype base dtype for i in input      else        if not all            x be compatible with i dtype  for i  x in zip input  input type            raise typeerror string                         string                            node def name   i dtype for i in input   input type             control input ops =        if control input        for c in control input          control op = none         if isinstance c  operation             control op = c         elif isinstance c   tensor  indexedslices              control op = c op         else            raise typeerror string                           string   c          control input ops append control op            self  input val = none           self  original op = original op     self  traceback = tf stack extract stack                  self  device code locations = none               self  colocation code locations = none     self  control flow context = self graph  get control flow context                                 self  gradient function = none           if c op        self  c op = c op       op def = g  get op def c api tf operationoptype c op         name = self name     else        if op def be none          op def = self  graph  get op def node def op                      group input = self  reconstruct sequence input            op def  input  node def attr        self  c op =  create c op self  graph  node def  group input                                  control input ops        name = compat as str node def name            self  be stateful = op def be stateful           num output = c api tf operationnumoutputs self  c op      self  output =        for i in range num output         tf output = c api util tf output self  c op  i        output type = c api tf operationoutputtype tf output        tensor = tensor  create with tf output self  i  output type  tf output          self  output append tensor       self  id value = self  graph  add op self  name         if not c op        self  control flow post process input tensors=inputs     def  control flow post process self  input tensors=none       string     if input tensors be none        input tensors = self input     for input tensor in input tensors        control flow util checkinputfromvalidcontext self  input tensor op      if self  control flow context be not none        self  control flow context addop self     def  reconstruct sequence input self  op def  input  attrs       string     group input =        i = 0     for input arg in op def input arg        if input arg number attr          input len = attrs input arg number attr  i         be sequence = true       elif input arg type list attr          input len = len attrs input arg type list attr  list type          be sequence = true       else          input len = 1         be sequence = false        if be sequence          group input append input i i   input len         else          group input append input i         i  = input len      assert i == len input      return group input    def colocation group self       string     default colocation group =  compat as bytes string   self name       try        class attr = self get attr string      except valueerror                      return default colocation group      attr group =           class name for class name in class attr         if class name startswith b loc                          return attr group if attr group else default colocation group    def value self       string     return tuple self output     def  get control flow context self       string     return self  control flow context    def  set control flow context self  ctx       string     self  control flow context = ctx     property   def name self       string     return c api tf operationname self  c op      property   def  id self       string     return self  id value     property   def device self       string     return c api tf operationdevice self  c op      property   def  device assignments self       string     return self  device code locations or        property   def  colocation dict self       string     locations dict = self  colocation code locations or        return locations dict copy       property   def  output type self       string     num output = c api tf operationnumoutputs self  c op      output type =           c api tf operationoutputtype self  tf output i           for i in xrange num output                                if output type        assert isinstance output type 0   int      return output type    def  tf output self  output idx       string     tf output = c api tf output       tf output oper = self  c op     tf output index = output idx     return tf output    def  tf input self  input idx       string     tf input = c api tf input       tf input oper = self  c op     tf input index = input idx     return tf input    def  set device self  device         string     self  set device from string compat as str  device string device       def  set device from string self  device str       string     c api setrequesteddevice          self  graph  c graph            self  c op            device str     def  update input self  index  tensor       string     if not isinstance tensor  tensor         raise typeerror string   tensor       assert same graph self  tensor            self  input val = none     c api updateedge          self  graph  c graph            tensor  as tf output              self  tf input index      def  add while input self  tensors       string     for tensor in tensors        if not isinstance tensor  tensor           raise typeerror string   tensor         assert same graph self  tensor                self  input val = none       c api addwhileinputhack            self  graph  c graph              tensor  as tf output                self  c op     def  add control input self  ops       string     for op in ops        if not isinstance op  operation           raise typeerror string   op        c api addcontrolinput self  graph  c graph  self  c op  op  c op       def  add control input self  op       string     if not isinstance op  operation         raise typeerror string   op      c api addcontrolinput self  graph  c graph  self  c op  op  c op       def  remove all control input self       string     c api removeallcontrolinputs self  graph  c graph  self  c op       def  add output self  type  shape       string     assert len type  == len shape      orig num output = len self output      for i in range len type          t = tensor self  orig num output   i  type i         self  output append t        t set shape shape i      def   str   self       return str self node def     def   repr   self       return string    self name  self type      property   def output self       string     return self  output     property   def input self       string     if self  input val be none               self  input val = tuple map self graph  get tensor by tf output                                     c api getoperationinputs self  c op               return self  input val     property   def  input type self       num input = c api tf operationnuminputs self  c op      input type =           dtypes as dtype c api tf operationinputtype self  tf input i            for i in xrange num input            return input type     property   def control input self       string     control c ops = c api tf operationgetcontrolinputs wrapper self  c op           return           self graph  get operation by name unsafe c api tf operationname c op           for c op in control c ops                property   def  control output self       string     control c ops = c api tf operationgetcontroloutputs wrapper self  c op           return           self graph  get operation by name unsafe c api tf operationname c op           for c op in control c ops                property   def type self       string     return c api tf operationoptype self  c op      property   def graph self       string     return self  graph     property   def node def self            string          with c api util tf buffer   as buf        c api tf operationtonodedef self  c op  buf        data = c api tf getbuffer buf      node def = node def pb2 nodedef       node def parsefromstring compat as bytes data       return node def     property   def op def self            string          return self  graph  get op def self type      property   def traceback self       string     return self  traceback    def  set attr self  attr name  attr value       string     buf = c api tf newbufferfromstring          compat as bytes attr value serializetostring         try        self  set attr with buf attr name  buf      finally        c api tf deletebuffer buf     def  set attr with buf self  attr name  attr buf       string          c api setattr self  graph  c graph  self  c op  attr name  attr buf          def  set func attr self  attr name  func name       string     func = attr value pb2 nameattrlist name=func name      self  set attr attr name  attr value pb2 attrvalue func=func      def  set func list attr self  attr name  func name       string     funcs =  attr value pb2 nameattrlist name=func name               for func name in func name      funcs list = attr value pb2 attrvalue listvalue func=funcs      self  set attr attr name  attr value pb2 attrvalue list=funcs list      def  set type list attr self  attr name  type       string     if not type        return     if isinstance type 0   dtypes dtype         type =  dt as datatype enum for dt in type      type list = attr value pb2 attrvalue listvalue type=types      self  set attr attr name  attr value pb2 attrvalue list=types list      def  set shape list attr self  attr name  shape       string     shape =  s as proto   for s in shape      shape list = attr value pb2 attrvalue listvalue shape=shapes      self  set attr attr name  attr value pb2 attrvalue list=shapes list      def  clear attr self  attr name       string          c api clearattr self  graph  c graph  self  c op  attr name          def get attr self  name       string     field =  string  string  string  string  string  string  string  string      try        with c api util tf buffer   as buf          c api tf operationgetattrvalueproto self  c op  name  buf          data = c api tf getbuffer buf      except errors invalidargumenterror as e               raise valueerror str e       x = attr value pb2 attrvalue       x parsefromstring data       oneof value = x whichoneof string      if oneof value be none        return        if oneof value == string        for f in field          if getattr x list  f             if f == string              return  dtypes as dtype t  for t in x list type            else              return list getattr x list  f         return        if oneof value == string        return dtypes as dtype x type      assert oneof value in field  string   str x      return getattr x  oneof value     def  get attr type self  name       string     try        dtype enum = c api tf operationgetattrtype self  c op  name        return  dtypes intern table dtype enum      except errors invalidargumenterror as e               raise valueerror str e      def  get attr bool self  name       string     try        return c api tf operationgetattrbool self  c op  name      except errors invalidargumenterror as e               raise valueerror str e      def  get attr int self  name       string     try        return c api tf operationgetattrint self  c op  name      except errors invalidargumenterror as e               raise valueerror str e      def run self  fee dict=none  session=none       string      run use default session self  fee dict  self graph  session  
class optionalspec type spec typespec     string      slot   =  string     def   init   self  value structure       self  value structure = value structure     property   def value type self       return  optionalimpl    def  serialize self       return  self  value structure       property   def  component specs self       return  tensor spec tensorspec     dtypes variant      def  to components self  value       return  value  variant tensor       def  from components self  flat value            return  optionalimpl flat value 0   self  value structure      staticmethod   def from value value       return optionalspec value value structure     def  to legacy output type self       return self    def  to legacy output shape self       return self    def  to legacy output class self       return self 
class raggedtensor composite tensor compositetensor     string             def   init   self                 value                 row split                 cache row lengths=none                 cache value rowids=none                 cache nrows=none                 internal=false                 uniform row length=none       string     if not internal        raise valueerror string                        string                        string            if not isinstance row split  ops tensor         raise typeerror string                         row split      if not isinstance value   raggedtensor  ops tensor          raise typeerror string                         value      if row split dtype not in  dtypes int32  dtypes int64         raise valueerror string            row split shape assert have rank 1      value shape with rank at least 1      row split set shape  none       if isinstance value  raggedtensor         assert row split dtype == value row split dtype      self  value = value     self  row split = row split                     for tensor in  cache row lengths  cache value rowids  cache nrows         if tensor be not none          if not isinstance tensor  ops tensor             raise typeerror string          elif tensor dtype not in  dtypes int32  dtypes int64             raise typeerror string      self  cache row lengths = cache row lengths     self  cache value rowids = cache value rowids     self  cache nrows = cache nrows      if uniform row length be not none        if not isinstance uniform row length  ops tensor           raise typeerror string        elif uniform row length dtype not in  dtypes int32  dtypes int64           raise typeerror string      self  uniform row length = uniform row length               classmethod   def from value rowids cls                          value                          value rowids                          nrows=none                          name=none                          validate=true       string     if not isinstance validate  bool         raise typeerror string      with ops name scope name  string                           value  value rowids  nrows          value  value rowids = cls  convert value and row partition            value  value rowids  string        if nrows be none          const rowids = tensor util constant value value rowids          if const rowids be none            nrows = array ops concat  value rowids -1     -1    axis=0  0    1           const nrows = none         else            const nrows = const rowids -1    1 if const rowids size > 0 else 0           nrows = ops convert to tensor const nrows  value rowids dtype                                          name=string        else          nrows = ops convert to tensor nrows  value rowids dtype  string          const nrows = tensor util constant value nrows          if const nrows be not none            if const nrows < 0              raise valueerror string   const nrows            const rowids = tensor util constant value value rowids            if const rowids be not none and const rowids size > 0              if not const nrows >= const rowids -1    1                raise valueerror                    string                   string    const nrows  const rowids -1           value rowids shape assert have rank 1        nrows shape assert have rank 0        value shape  1  assert be compatible with value rowids shape         if validate          msg = string         nvals1 =  nrows value          nvals2 =  nrows value rowids          check =               check ops assert rank value rowids  1  message=msg               check ops assert rank nrows  0  message=msg               check ops assert equal nvals1  nvals2  message=msg               check ops assert non negative value rowids  1   message=msg                assert monotonic increase value rowids  message=msg               check ops assert less value rowids -1    nrows  message=msg                     if not isinstance value  raggedtensor             check append check ops assert rank at least value  1           value rowids = control flow ops with dependencies check  value rowids                                            value rowids int32 = math ops cast value rowids  dtypes int32        nrows int32 = math ops cast nrows  dtypes int32        row lengths = math ops bincount            value rowids int32            minlength=nrows int32            maxlength=nrows int32            dtype=value rowids dtype        row split = array ops concat   0   math ops cumsum row lengths    axis=0        if const nrows be not none          row lengths set shape  const nrows           row split set shape  const nrows   1          return cls            value            row split            cache row lengths=row lengths            cache value rowids=value rowids            cache nrows=nrows            internal=true      classmethod   def from row split cls  value  row split  name=none  validate=true       string     if not isinstance validate  bool         raise typeerror string      if isinstance row split   list  tuple   and not row split        raise valueerror string      if isinstance row split  tensor spec tensorspec         return cls values=values  row splits=row split  internal=true       with ops name scope name  string   value  row split          value  row split = cls  convert value and row partition            value  row split  string        row split shape assert have rank 1         if validate          msg = string         nvals =  nrows value  row split dtype          check =               check ops assert rank row split  1  message=msg                assert zero row split 0   message=msg                assert monotonic increase row split  message=msg               check ops assert equal row split -1   nvals  message=msg                     if not isinstance value  raggedtensor             check append check ops assert rank at least value  1           row split = control flow ops with dependencies check  row split         return cls values=values  row splits=row split  internal=true      classmethod   def from row lengths cls  value  row lengths  name=none  validate=true       string     if not isinstance validate  bool         raise typeerror string      with ops name scope name  string   value  row lengths          value  row lengths = cls  convert value and row partition            value  row lengths  string        row lengths shape assert have rank 1         if validate          msg = string         nvals1 = math ops reduce sum row lengths          nvals2 =  nrows value  row lengths dtype          check =               check ops assert rank row lengths  1  message=msg               check ops assert non negative row lengths  message=msg               check ops assert equal nvals1  nvals2  message=msg                    if not isinstance value  raggedtensor             check append check ops assert rank at least value  1           row lengths = control flow ops with dependencies check  row lengths         row limit = math ops cumsum row lengths        row split = array ops concat   0   row limit   axis=0        return cls            values=values            row splits=row split            cache row lengths=row lengths            internal=true      classmethod   def from row start cls  value  row start  name=none  validate=true       string     if not isinstance validate  bool         raise typeerror string      with ops name scope name  string   value  row start          value  row start = cls  convert value and row partition            value  row start  string        row start shape assert have rank 1        nvals =  nrows value  row start dtype         if validate          msg = string         check =               check ops assert rank row start  1  message=msg                assert zero row start  1   message=msg                assert monotonic increase row start  message=msg               check ops assert less equal row start -1    nvals  message=msg                     if not isinstance value  raggedtensor             check append check ops assert rank at least value  1           row start = control flow ops with dependencies check  row start         row split = array ops concat  row start   nvals    axis=0        return cls values=values  row splits=row split  internal=true      classmethod   def from row limit cls  value  row limit  name=none  validate=true       string     if not isinstance validate  bool         raise typeerror string      with ops name scope name  string   value  row limit          value  row limit = cls  convert value and row partition            value  row limit  string        row limit shape assert have rank 1         if validate          msg = string         nvals =  nrows value  row limit dtype          check =               check ops assert rank row limit  1  message=msg               check ops assert non negative row limit  1   message=msg                assert monotonic increase row limit  message=msg               check ops assert equal row limit -1    nvals  message=msg                    if not isinstance value  raggedtensor             check append check ops assert rank at least value  1           row limit = control flow ops with dependencies check  row limit         zero = array ops zero  1   row limit dtype        row split = array ops concat  zero  row limit   axis=0        return cls values=values  row splits=row split  internal=true      classmethod   def from uniform row length cls                                value                                uniform row length                                nrows=none                                validate=true                                name=none       string     if not isinstance validate  bool         raise typeerror string      with ops name scope name  string                           value  uniform row length  nrows          value  uniform row length = cls  convert value and row partition            value  uniform row length  string        uniform row length shape assert have rank 0                const nvals = tensor shape dimension at index value shape  0  value       if const nvals be not none          nvals = constant op constant const nvals  uniform row length dtype        elif isinstance value  raggedtensor           nvals = value nrows out type=uniform row length dtype        else          nvals = array ops shape value  out type=uniform row length dtype  0                const row length = tensor util constant value uniform row length        if nrows be none          if const row length be none                       rowlen or 1 = control flow ops cond                math ops equal uniform row length  0                 lambda  constant op constant 1  uniform row length dtype                 lambda  uniform row length            nrows = nvals // rowlen or 1         elif const row length == 0            nrows = 0         else            nrows = nvals // const row length       nrows = ops convert to tensor            nrows  uniform row length dtype  name=string        const nrows = tensor util constant value nrows                if const nrows be not none and const row length be not none          row split =  v   const row length for v in range const nrows   1           row split = constant op constant row split  uniform row length dtype        else          row split = math ops range nrows   1    uniform row length        if validate          check =             if  const nrows be none or const row length be none or             const nvals be none             check append check ops assert equal                nrows   uniform row length                nvals                 string  uniform row length  string                 nrows  string  nvals            else            if const nrows   const row length  = const nvals              raise valueerror                  string                    const row length  const nrows  const nvals            if uniform row length shape rank be none            check append                check ops assert rank                    uniform row length  0                    message=string            const row length = tensor util constant value uniform row length          if const row length be none            check append                check ops assert greater equal                    uniform row length                    constant op constant 0  uniform row length dtype                     message=string           else            if const row length < 0              raise valueerror string           row split = control flow ops with dependencies check  row split         return cls            values=values            row splits=row split            uniform row length=uniform row length            cache nrows=nrows            internal=true      classmethod   def from nest value rowids cls                                 flat value                                 nest value rowids                                 nest nrows=none                                 name=none                                 validate=true       string     if not isinstance validate  bool         raise typeerror string      if isinstance nest value rowids  ops tensor         raise typeerror string      if nest nrows be none        nest nrows =  none    len nest value rowids      else        if isinstance nest nrows  ops tensor           raise typeerror string        if len nest nrows   = len nest value rowids           raise valueerror string                          string       with ops name scope          name  string           flat value    list nest value rowids    list nest nrows          result = flat value       for value rowids  nrows in reverse            list zip nest value rowids  nest nrows             result = cls from value rowids result  value rowids  nrows                                         validate=validate        return result     classmethod   def from nest row split cls                               flat value                               nest row split                               name=none                               validate=true       string     if not isinstance validate  bool         raise typeerror string      if isinstance nest row split  ops tensor         raise typeerror string      with ops name scope name  string                           flat value    list nest row split          result = flat value       for split in reverse nest row split           result = cls from row split result  split  validate=validate        return result     classmethod   def from nest row lengths cls                                flat value                                nest row lengths                                name=none                                validate=true       string     if not isinstance validate  bool         raise typeerror string      if isinstance nest row lengths  ops tensor         raise typeerror string      with ops name scope name  string                           flat value    list nest row lengths          result = flat value       for lengths in reverse nest row lengths           result = cls from row lengths result  lengths  validate=validate        return result     classmethod   def  convert value and row partition cls  value  partition  name       string     if isinstance value  raggedtensor         if isinstance partition  ops tensor           if partition dtype not in  dtypes int32  dtypes int64             raise valueerror string   name          if value row split dtype  = partition dtype            if not rag config auto cast partition dtype                raise valueerror string                                 name  partition dtype  value row split dtype             partition = math ops cast partition  dtypes int64            value = value with row split dtype dtypes int64        else          partition = ops convert to tensor partition  value row split dtype                                            name=name      else        value = ops convert to tensor value  name=string        if isinstance partition  np ndarray  and partition dtype == np int32          partition = ops convert to tensor partition  name=name        else          partition = ops convert to tensor              partition  prefer dtype=dtypes int64              name=name        if partition dtype not in  dtypes int32  dtypes int64           raise valueerror string   name       return  value  partition                property   def dtype self       string     return self  value dtype     property   def shape self       string     nrows = tensor shape dimension at index self  row split shape  0  - 1      if self  uniform row length be not none        row length = tensor util constant value self  uniform row length      else        row length = none      value shape = self  value shape     value shape = value shape 1       return tensor shape tensorshape  nrows                                       row length   concatenate value shape      property   def rag rank self       string     value be rag = isinstance self  value  raggedtensor      return self  value rag rank   1 if value be rag else 1     property   def value self       string     return self  value     property   def row split self       string     return self  row split     property   def flat value self       string     rt value = self value     while isinstance rt value  raggedtensor         rt value = rt value value     return rt value     property   def nest row split self       string     rt nest split =  self row split      rt value = self value     while isinstance rt value  raggedtensor         rt nest split append rt value row split        rt value = rt value value     return tuple rt nest split     def value rowids self  name=none       string     if self  cache value rowids be not none        return self  cache value rowids      with ops name scope name  string   self          return segment id ops row split to segment ids self row split     def nest value rowids self  name=none       string     with ops name scope name  string   self          rt nest ids =  self value rowids          rt value = self value       while isinstance rt value  raggedtensor           rt nest ids append rt value value rowids            rt value = rt value value       return tuple rt nest ids     def nrows self  out type=none  name=none       string     if out type be none        out type = self  row split dtype     else        out type = dtypes as dtype out type      if self  cache nrows be not none        return math ops cast self  cache nrows  out type      with ops name scope name  string   self          return array ops shape self row split  out type=out type  0  - 1    def row start self  name=none       string     with ops name scope name  string   self          return self row split  -1     def row limit self  name=none       string     with ops name scope name  string   self          return self row split 1      def row lengths self  axis=1  name=none       string     if self  cache row lengths be not none        return self  cache row lengths      with ops name scope name  string   self          axis = rag util get positive axis axis  self shape ndims        if axis == 0          return self nrows         elif axis == 1          split = self row split         return split 1   - split  -1        elif isinstance self value  raggedtensor           return self with value self value row lengths axis - 1         else          shape = array ops shape self value  out type=self  row split dtype          return self with value              array ops ones shape  axis - 1   self  row split dtype                shape axis - 1      def nest row lengths self  name=none       string     with ops name scope name  string   self          rt nest row lengths =          rt = self       while isinstance rt  raggedtensor           rt nest row lengths append rt row lengths            rt = rt value       return tuple rt nest row lengths     def bound shape self  axis=none  name=none  out type=none       string     if out type be none        out type = self  row split dtype     else        out type = dtypes as dtype out type      with ops name scope name  string   self  axis          nest split = self nest row split       rt flat value = self flat value               if isinstance axis  int           if axis == 0            return array ops shape nest split 0   out type=out type  0  - 1         elif axis == 1            return math ops maximum math ops reduce max self row lengths     0         split shape = array ops shape self row split  out type=out type        flat value shape = array ops shape rt flat value  out type=out type         rag dimension = array ops stack  split shape 0  - 1                math ops maximum math ops reduce max split 1   - split  -1    0            for split in nest split                inner dimension = flat value shape 1          bbox = array ops concat  rag dimension  inner dimension   axis=0        return bbox if axis be none else array ops gather bbox  axis               def with value self  new value       string     new value shape with rank at least 1      self value shape  1  assert be compatible with new value shape  1       if  isinstance new value  raggedtensor  and         self  row split dtype  = new value row split dtype         if not rag config auto cast partition dtype            raise valueerror string                          string                          string        new value = new value with row split dtype dtypes int64        return self with row split dtype dtypes int64  with value new value      return raggedtensor          values=new value          row splits=self  row split          cache row lengths=self  cache row lengths          cache value rowids=self  cache value rowids          cache nrows=self  cache nrows          internal=true          uniform row length=self  uniform row length     def with flat value self  new value       string     if isinstance self  value  ops tensor         return self with value new value      else        return self with value self value with flat value new value      def with row split dtype self  dtype       string     dtype = dtypes as dtype dtype      if dtype not in  dtypes int32  dtypes int64         raise valueerror string      if self  row split dtype == dtype        return self      row split = math ops cast self  row split  dtype       value = self  value     if isinstance value  raggedtensor         value = value with row split dtype dtype      cache row lengths = self  cache row lengths     if cache row lengths be not none        cache row lengths = math ops cast cache row lengths  dtype      cache value rowids = self  cache value rowids     if cache value rowids be not none        cache value rowids = math ops cast cache value rowids  dtype      cache nrows = self  cache nrows     if cache value rowids be not none        cache value rowids = math ops cast cache value rowids  dtype      uniform row length = self  uniform row length     if uniform row length be not none        uniform row length = math ops cast uniform row length  dtype       return raggedtensor value  row split  cache row lengths                          cache value rowids  cache nrows  internal=true                          uniform row length=uniform row length     def merge dim self  outer axis  inner axis       string     outer axis = rag util get positive axis outer axis  self shape ndims      inner axis = rag util get positive axis inner axis  self shape ndims      if not outer axis < inner axis        raise valueerror string                        string    outer axis  inner axis       return  merge dim self  outer axis  inner axis           classmethod   def from tensor cls                    tensor                    lengths=none                    padding=none                    rag rank=1                    name=none                    row split dtype=dtypes int64       string     row split dtype = dtypes as dtype row split dtype      if lengths be not none and pad be not none        raise valueerror string      if not isinstance rag rank  int         raise typeerror string   rag rank      if rag rank <= 0        raise valueerror            string   rag rank       with ops name scope name  string   tensor  lengths  pad          tensor = ops convert to tensor tensor  name=string        tensor shape with rank at least rag rank   1        input shape = array ops shape tensor  out type=row split dtype        ncols = input shape 1                if  lengths be not none and isinstance lengths   list  tuple   and           len lengths  and not isinstance lengths 0    int  float             if rag rank not in  1  len lengths                                                                     raise valueerror string                            string                                     tensor shape with rank at least len lengths    1          num tokens = math ops reduce sum lengths -1           ones mask = array ops ones  num tokens   dtype=dtypes bool          rag mask = cls from nest row lengths              ones mask  lengths  validate=false          dense rag mask = rag mask to tensor default value=false          mask data = array ops boolean mask tensor  dense rag mask          return cls from nest row lengths              mask data  lengths  validate=false                                            if rag rank > 1          if tensor shape be fully define              input shape = tensor shape as list             new shape =  -1    input shape rag rank                                   dim size = np cumprod input shape          else            neg one = constant op constant  -1   row split dtype            new shape = array ops concat  neg one  input shape rag rank                                            axis=0            dim size = math ops cumprod input shape          flatten = array ops reshape tensor  new shape          result = cls from tensor flatten  lengths  pad                                   row split dtype=row split dtype           for axis in range rag rank - 1  0  -1             dim len = tensor shape dimension at index tensor shape  axis  value           if dim len be none              dim len = input shape axis            else              dim len = constant op constant dim len  row split dtype            result = raggedtensor from uniform row length                values=result                uniform row length=dim len                nrows=dim size axis - 1                 validate=false          return result               if pad be not none          pad = ops convert to tensor              pad  name=string  dtype=tensor dtype          pad shape assert be compatible with tensor shape 2                                        have default value = math ops equal pad  tensor                                                                 tensor rank = array ops rank tensor          reduce axis = math ops range 2  tensor rank          have default = control flow ops cond              tensor rank > 2              lambda  math ops reduce all have default value  axis=reduce axis               lambda  have default value          have default set shape tensor shape tensorshape  none  none            have default set shape tensor shape  2                                                have nondefault = math ops logical not have default          have nondefault = math ops cast have nondefault  row split dtype          length for nondefault value =               have nondefault   array ops expand dim                  math ops range 1  ncols   1   0           lengths = math ops reduce max length for nondefault value  axis=1         if lengths be not none                                     lengths = rag util convert to int tensor lengths  string                                                      row split dtype          lengths shape assert have rank 1          lengths = math ops minimum lengths  ncols          lengths = math ops maximum lengths  0          limit = math ops cumsum lengths          split = array ops concat               array ops zero  1   row split dtype   limit   axis=0          mask = array ops sequence mask lengths  maxlen=ncols          value = array ops boolean mask tensor  mask          return cls from row split value  split  validate=false                              value shape = array ops concat   -1   input shape 2     axis=0        value = array ops reshape tensor  value shape        const nrows = tensor shape dimension at index tensor shape  0  value       const ncols = tensor shape dimension at index tensor shape  1  value       if const nrows be not none          nrows = constant op constant const nrows  row split dtype        else          nrows = input shape 0        if const ncols be not none          ncols = constant op constant const ncols  row split dtype        else          ncols = input shape 1        return raggedtensor from uniform row length            values=values  uniform row length=ncols            nrows=nrows  validate=false     def to tensor self  default value=none  name=none  shape=none       string     with ops name scope name  string   self  default value  shape          if default value be not none          default value = ops convert to tensor              default value  name=string  dtype=self dtype        type tensor pair =  get row partition type tensor pair self        row partition type =  x 0  for x in type tensor pair        row partition tensors =  x 1  for x in type tensor pair        if default value be none          default value = array ops zero     self dtype         shape tensor =  shape as tensor shape  row partition tensors 0  dtype        return gen rag conversion ops rag tensor to tensor            shape=shape tensor            values=self flat value            default value=default value            row partition types=row partition type            row partition tensors=row partition tensors      classmethod   def from sparse cls  st input  name=none  row split dtype=dtypes int64       string     row split dtype = dtypes as dtype row split dtype      if not sparse tensor be sparse st input         raise typeerror string   type st input    name        with ops name scope name  string   st input          st input = sparse tensor convert to tensor or sparse tensor            st input  name=string         if st input dense shape shape ndims be none          static rank from dense shape = none       else          static rank from dense shape = st input dense shape shape dim 0  value        if st input indices shape ndims be none          static rank from indices = none       else          static rank from indices = st input indices shape dim 1  value        if static rank from dense shape  = 2 and static rank from indices  = 2          raise valueerror string         with ops control dependencies             assert sparse indices be rag right st input indices                                       segment ids = math ops cast st input indices    0   row split dtype          num segment = math ops cast st input dense shape 0   row split dtype          return cls from value rowids              st input value  segment ids  num segment  validate=false     def to sparse self  name=none       string     with ops name scope name  string   self          result = gen rag conversion ops rag tensor to sparse            self nest row split  self flat value  name=name        return sparse tensor sparsetensor result sparse indices                                          result sparse value                                          result sparse dense shape      classmethod   def  from variant cls                      variant                      dtype                      output rag rank                      input rag rank=none                      row split dtype=dtypes int64                      name=none       string     variant = ops convert to tensor          variant  name=string  dtype=dtypes variant      if  variant shape ndims be not none and input rag rank be not none and         output rag rank  = input rag rank   variant shape ndims         raise valueerror            string           string           string              variant shape ndims  input rag rank  output rag rank       input rag rank = -1 if input rag rank be none else input rag rank     with ops name scope          name  string           variant  dtype  input rag rank  output rag rank          result = gen rag conversion ops rag tensor from variant            variant  input rag rank  output rag rank  dtype            row split dtype  name        return cls from nest row split            result output dense value            result output nest split            validate=false     def  to variant self  batch input=false  name=none       string     with ops name scope name  string   self  batch input          return gen rag conversion ops rag tensor to variant            self nest row split  self flat value  batch input  name              def   repr   self       if self  be eager          return string   self to list       else        return string    self  value                                                              self  row split               def to list self       string     if self  be eager          return self  eager value   to list       else        raise valueerror string                        string                        string     def  eager value self       string     value = self flat value numpy       for row split in reverse self nest row split         value = rag tensor value raggedtensorvalue value  row split numpy        return value    def  be eager self       string     rt = self     while isinstance rt  raggedtensor         if not isinstance rt row split  ops eagertensor           return false       rt = rt value     return isinstance rt  ops eagertensor              def   getitem   self  key       string                                                   def  as graph element self       string     value = self value     while isinstance value  raggedtensor         value = value value     return value               property   def  type spec self       return raggedtensorspec          shape=self shape          dtype=self dtype          rag rank=self rag rank          row split dtype=self  row split dtype     def  shape invariant to type spec self  shape       return raggedtensorspec shape  self dtype  self rag rank                              self row split dtype     def consumers self       return self  consumers   
class raggedtensorspec type spec batchabletypespec     string      slot   =  string  string  string  string      property   def value type self       return raggedtensor if self  rag rank > 0 else ops tensor    def   init   self  shape=none  dtype=dtypes float32  rag rank=none                 row split dtype=dtypes int64       string     self  shape = tensor shape as shape shape      self  dtype = dtypes as dtype dtype      self  row split dtype = dtypes as dtype row split dtype       rank = self  shape ndims     if rag rank be none        if rank be none          raise valueerror string                          string        rag rank = rank - 1     self  rag rank = rag rank     if not isinstance self  rag rank  int         raise typeerror string       if rank be not none        if rag rank >= rank          raise valueerror string     def  serialize self       return  self  shape  self  dtype  self  rag rank  self  row split dtype      property   def  component specs self       if self  rag rank == 0        return  tensor spec tensorspec self  shape  self  dtype        flat value shape = tensor shape tensorshape  none   concatenate          self  shape self  rag rank   1        outer dim = tensor shape dimension at index self  shape  0      outer split shape =  none if outer dim be none else outer dim   1      inner split spec = tensor spec tensorspec  none   self  row split dtype       specs =            tensor spec tensorspec flat value shape  self  dtype            tensor spec tensorspec outer split shape  self  row split dtype              inner split spec for   in range self  rag rank - 1        return specs    def  to components self  value       if be rag value         return  value flat value    list value nest row split      else        return  value     def  from components self  tensor list       result = tensor list 0      if  all isinstance t  np ndarray  for t in tensor list  and         not tf2 enable           for row split in reverse tensor list 1             result = rag tensor value raggedtensorvalue result  row split      else        if isinstance tensor list 0   np ndarray           tensor list =  ops convert to tensor t  for t in tensor list          result = tensor list 0        for row split in reverse tensor list 1             result = raggedtensor result  row split  internal=true      return result              property   def  flat tensor specs self                           return  tensor spec tensorspec none  dtypes variant      def  to tensor list self  value       rag rank = value rag rank if isinstance value  raggedtensor  else 0     if rag rank  = self  rag rank        raise valueerror string                        string    rag rank  self  rag rank       if rag rank == 0        return             gen rag conversion ops rag tensor to variant                    value  batch input=false                   return  value  to variant batch input=false      def  to batch tensor list self  value       rag rank = value rag rank if isinstance value  raggedtensor  else 0     if rag rank  = self  rag rank        raise valueerror string                        string    rag rank  self  rag rank       if rag rank == 0               raise valueerror            string           return  value  to variant batch input=true      def  from compatible tensor list self  tensor list       if self  rag rank < 0        raise valueerror            string   self  rag rank      result = raggedtensor  from variant            tensor list 0   dtype=self  dtype          row split dtype=self  row split dtype          output rag rank=self  rag rank      if self  shape ndims be not none        if isinstance result  raggedtensor           outer dim = tensor shape dimension value self  shape 0           if outer dim be not none            result row split set shape  outer dim   1           result flat value set shape              tensor shape tensorshape  none   concatenate                  self  shape 1   self  rag rank           else          result set shape self  shape      return result    def  batch self  batch size       return raggedtensorspec          tensor shape tensorshape  batch size   concatenate self  shape           self  dtype  self  rag rank   1  self  row split dtype     def  unbatch self                           return raggedtensorspec self  shape 1    self  dtype  self  rag rank - 1                              self  row split dtype     def  to legacy output type self       return self  dtype    def  to legacy output shape self       return self  shape    def  to legacy output class self       return self     classmethod   def from value cls  value       return cls shape=value shape                 dtype=value value dtype                 rag rank=value rag rank                 row split dtype=value row split dtype  
class registergradient object     string    def   init   self  op type       string     if not isinstance op type  six string type         raise typeerror string      self  op type = op type    def   call   self  f       string      gradient registry register f  self  op type      return f 
class sparsetensorspec type spec batchabletypespec     string      slot   =  string  string     value type = property lambda self  sparsetensor     def   init   self  shape=none  dtype=dtypes float32       string     self  shape = tensor shape as shape shape      self  dtype = dtypes as dtype dtype     def  serialize self       return  self  shape  self  dtype      property   def dtype self       string     return self  dtype     property   def shape self       string     return self  shape     property   def  component specs self       rank = self  shape ndims     num value = none     return           tensor spec tensorspec  num value  rank   dtypes int64           tensor spec tensorspec  num value   self  dtype           tensor spec tensorspec  rank   dtypes int64      def  to components self  value       if isinstance value  sparsetensorvalue         value = sparsetensor from value value      return  value indices  value value  value dense shape     def  from components self  tensor list       if  all isinstance t  np ndarray  for t in tensor list  and         not tf2 enable           return sparsetensorvalue  tensor list      else        return sparsetensor  tensor list               property   def  flat tensor specs self                           return  tensor spec tensorspec none  dtypes variant      def  to tensor list self  value       value = sparsetensor from value value      return  gen sparse ops serialize sparse          value indices  value value  value dense shape          out type=dtypes variant      def  to batch tensor list self  value       dense shape = tensor util constant value as shape value dense shape      if self  shape merge with dense shape  ndims == 0        raise valueerror            string      return  gen sparse ops serialize many sparse          value indices  value value  value dense shape          out type=dtypes variant      def  from compatible tensor list self  tensor list       tensor list = gen sparse ops deserialize sparse tensor list 0   self  dtype      indices  value  dense shape = tensor list     rank = self  shape ndims     indices set shape  none  rank                      if self  shape be fully define          dense shape = ops convert to tensor            self  shape  dtype=dtypes int64  name=string      elif  self  shape rank be not none and           any dim value be not none for dim in self  shape dim                 from tensorflow python ops import array ops         piece = array ops unstack dense shape  num=self  shape rank        for i  dim in enumerate self  shape dim           if dim value be not none            piece i  = constant op constant dim value  dense shape dtype        dense shape = array ops stack piece      else        dense shape set shape  rank        return sparsetensor indices  value  dense shape     def  batch self  batch size       return sparsetensorspec          tensor shape tensorshape  batch size   concatenate self  shape           self  dtype     def  unbatch self       if self  shape ndims == 0        raise valueerror string      return sparsetensorspec self  shape 1    self  dtype     def  to legacy output type self       return self  dtype    def  to legacy output shape self       return self  shape    def  to legacy output class self       return sparsetensor     classmethod   def from value cls  value       if isinstance value  sparsetensor         return cls value shape  value dtype      if isinstance value  sparsetensorvalue         if isinstance value value  np ndarray           return cls value dense shape  value value dtype        else          return cls from value sparsetensor from value value       else        raise typeerror string  
class tensor  tensorlike     string       overloadable operators =                string        string        string        string        string        string        string        string        string        string        string        string        string        string        string        string        string        string        string        string        string        string        string        string        string        string        string        string        string               string        string        string        string        string            use equality = tf2 enable      def   init   self  op  value index  dtype       string     if not isinstance op  operation         raise typeerror string   op      self  op = op     self  value index = value index     self  dtype = dtypes as dtype dtype           self  tf output = none          self  shape val = none               self  consumers =        self  id = uid       self  name = none     staticmethod   def  create with tf output op  value index  dtype  tf output       ret = tensor op  value index  dtype      ret  tf output = tf output     return ret     property   def op self       string     return self  op     property   def dtype self       string     return self  dtype     property   def graph self       string     return self  op graph     property   def name self       string     if self  name be none        if not self  op name          raise valueerror string   self  op        self  name = string    self  op name  self  value index      return self  name     property   def device self       string     return self  op device     property   def shape self       string     if self  shape val be none        self  shape val = self  c api shape       return self  shape val    def  c api shape self       string     c graph = self  op  graph  c graph       shape vector  unknown shape = c api tf graphgettensorshapehelper          c graph  self  as tf output        if unknown shape        return tensor shape unknown shape       else        shape vector =  none if d == -1 else d for d in shape vector        return tensor shape tensorshape shape vector      property   def  shape self       log warn string                     string      return self shape      shape setter   def  shape self  value       raise valueerror          string     def  disallow when autograph disable self  task       raise errors operatornotallowedingrapherror          string         string format task      def  disallow when autograph enable self  task       raise errors operatornotallowedingrapherror          string         string format task      def  disallow in graph mode self  task       raise errors operatornotallowedingrapherror          string         string format task      def  disallow bool cast self       if ag ctx control status ctx   status == ag ctx status disable        self  disallow when autograph disable            string      elif ag ctx control status ctx   status == ag ctx status enable        self  disallow when autograph enable            string      else               self  disallow in graph mode string     def  disallow iteration self       if ag ctx control status ctx   status == ag ctx status disable        self  disallow when autograph disable string      elif ag ctx control status ctx   status == ag ctx status enable        self  disallow when autograph enable string      else               self  disallow in graph mode string     def   iter   self       if not context execute eagerly          self  disallow iteration        shape = self  shape tuple       if shape be none        raise typeerror string      if not shape        raise typeerror string      if shape 0  be none        raise typeerror            string      for i in xrange shape 0          yield self i     def  shape as list self       if self shape ndims be not none        return  dim value for dim in self shape dim      else        return none    def  shape tuple self       shape = self  shape as list       if shape be none        return none     return tuple shape     def  rank self       string     return self shape ndims    def get shape self       string     return self shape    def set shape self  shape       string          self  shape val = none           if not isinstance shape  tensor shape tensorshape         shape = tensor shape tensorshape shape      dim list =        if shape dim be none        unknown shape = true     else        unknown shape = false       for dim in shape dim          if dim value be none            dim list append -1          else            dim list append dim value      try        c api tf graphsettensorshape wrapper            self  op  graph  c graph              self  as tf output              dim list            unknown shape      except errors invalidargumenterror as e               raise valueerror str e       property   def value index self       string     return self  value index    def consumers self       string     consumer name = c api tf operationoutputconsumers wrapper          self  as tf output             return           self graph  get operation by name unsafe name          for name in consumer name               def  as node def input self       string     if not self  op name        raise valueerror string   self  op      if self  value index == 0        return self  op name     else        return string    self  op name  self  value index     def  as tf output self                                if self  tf output be none        self  tf output = c api util tf output self op  c op  self value index      return self  tf output         def   str   self       return string             self name           string            self get shape    if self get shape   ndims be not none else string           string   self  dtype name  if self  dtype else string           string   self device  if self device else string     def   repr   self       return string    self name  self get shape                                                       self  dtype name     def   hash   self       g = getattr self  string  none      if  tensor  use equality and execute eagerly outside function   and          g be none or g  build function            raise typeerror string                       string      else        return id self     def   copy   self            cls = self   class       result = cls   new   cls      result   dict   update self   dict        return result                           array priority   = 100    def   array   self       raise notimplementederror string                               string format self name      def   len   self       raise typeerror string                     string                     string format self name       staticmethod   def  override operator operator  func        override helper tensor  operator  func     def   bool   self       string     self  disallow bool cast      def   nonzero   self       string     self  disallow bool cast      def eval self  fee dict=none  session=none       string     return  eval use default session self  fee dict  self graph  session     def experimental ref self                 string     return object identity reference self  
class tensorarray object     string    def   init   self                 dtype                 size=none                 dynamic size=none                 clear after read=none                 tensor array name=none                 handle=none                 flow=none                 infer shape=true                 element shape=none                 colocate with first write call=true                 name=none       string     if  context execute eagerly   and          flow be none or flow dtype  = dtypes variant                                             implementation =  eagertensorarray     elif  flow be not none and flow dtype == dtypes variant or           control flow util enablecontrolflowv2 ops get default graph            implementation =  graphtensorarrayv2     else        implementation =  graphtensorarray     self  implementation = implementation          dtype          size=size          dynamic size=dynamic size          clear after read=clear after read          tensor array name=tensor array name          handle=handle          flow=flow          infer shape=infer shape          element shape=element shape          colocate with first write call=colocate with first write call          name=name       self  implementation parent = weakref ref self      property   def flow self       string     return self  implementation  flow     property   def dtype self       string     return self  implementation  dtype     property   def handle self       string     return self  implementation handle     property   def element shape self       string     return self  implementation element shape     property   def dynamic size self       string     return self  implementation  dynamic size     property   def  infer shape self                      return self  implementation  infer shape    def identity self       string     return self  implementation identity      def grad self  source  flow=none  name=none       return self  implementation grad source  flow=flow  name=name     def read self  index  name=none       string     return self  implementation read index  name=name      tf should use should use result warn in eager=true    def write self  index  value  name=none       string     return self  implementation write index  value  name=name     def stack self  name=none       string     return self  implementation stack name=name     def gather self  indices  name=none       string     return self  implementation gather indices  name=name     def concat self  name=none       string     return self  implementation concat name=name      tf should use should use result   def unstack self  value  name=none       string     return self  implementation unstack value  name=name      tf should use should use result   def scatter self  indices  value  name=none       string     return self  implementation scatter indices  value  name=name      tf should use should use result   def split self  value  lengths  name=none       string     return self  implementation split value  lengths  name=name     def size self  name=none       string     return self  implementation size name=name      tf should use should use result   def close self  name=none       string     return self  implementation close name=name  
class tensorarrayspec type spec typespec     string      slot   =  string  string  string  string     value type = property lambda self  tensorarray     def   init   self  element shape=none  dtype=dtypes float32                 dynamic size=false  infer shape=true       string     self  element shape = tensor shape as shape element shape      self  dtype = dtypes as dtype dtype      self  dynamic size = dynamic size     self  infer shape = infer shape    def be compatible with self  other            if not isinstance other  type spec typespec         other = type spec type spec from value other            return  isinstance other  tensorarrayspec  and             self  dtype be compatible with other  dtype  and             self  element shape be compatible with other  element shape  and             self  dynamic size == other  dynamic size     def most specific compatible type self  other            if not self be compatible with other         raise valueerror string      infer shape = self  infer shape and other  infer shape     return tensorarrayspec          self  element shape most specific compatible shape              other  element shape           self  dtype  self  dynamic size  infer shape     def  serialize self       return  self  element shape  self  dtype  self  dynamic size              self  infer shape      property   def  component specs self       return  tensor spec tensorspec     dtypes variant      def  to components self  value       if not isinstance value  tensorarray         raise typeerror string                        format type value        if value flow be not none and value flow dtype == dtypes variant        return  value flow      else                             with ops name scope string           flow = list ops tensor list from tensor              tensor=value stack    element shape=value element shape        return  flow     def  from components self  tensor list                 ret = tensorarray          dtype=self  dtype          flow=tensor list 0           dynamic size=self  dynamic size          infer shape=self  infer shape      ret  implementation  element shape =  self  element shape        return ret     staticmethod   def from value value       if not isinstance value  tensorarray         raise typeerror string                        format type value         return tensorarrayspec          dtype=value dtype          element shape=value element shape          dynamic size=value dynamic size          infer shape=value  infer shape       def  to legacy output type self       return self  dtype    def  to legacy output shape self            return  tensor shape tensorshape  self  dynamic size  self  infer shape                                         concatenate self  element shape      def  to legacy output class self       return tensorarray 
class tensorshape object     string     slot   =  string     def   init   self  dim       string     if dim be none        self  dim = none     elif isinstance dim  tensor shape pb2 tensorshapeproto         if dim unknown rank          self  dim = none       else          self  dim =                            as dimension dim size if dim size  = -1 else none              for dim in dim dim               elif isinstance dim  tensorshape         self  dim = dim dim     else        try          dim iter = iter dim        except typeerror                   self  dim =  as dimension dim         else          self  dim =  as dimension d  for d in dim iter      property   def  v2 behavior self       if  tensorshape v2 override be none        return tf2 enable       return  tensorshape v2 override    def   repr   self       if self  v2 behavior        if self  dim be not none          return string    dim value for dim in self  dim        else          return string     else        return string   self  dim    def   str   self       if self rank be none        return string     elif self rank == 1        if self  v2 behavior          return string   self  dim 0  value       else          return string   self  dim 0      else        if self  v2 behavior          return string   string join str d value  for d in self  dim        else          return string   string join str d  for d in self  dim      property   def rank self       string     if self  dim be not none        return len self  dim      return none     property   def dim self       string     return self  dim     property   def ndims self       string     return self rank    def   len   self       string     if self  dim be none        raise valueerror string      return len self  dim     def   bool   self       string     return self  dim be not none         nonzero   =   bool      def   iter   self       string     if self  dim be none        raise valueerror string      else        if self  v2 behavior          return iter d value for d in self  dim        else          return iter d for d in self  dim     def   getitem   self  key       string     if self  dim be not none        if isinstance key  slice           return tensorshape self  dim key         else          if self  v2 behavior            return self  dim key  value         else            return self  dim key      else        if isinstance key  slice           start = key start if key start be not none else 0         stop = key stop          if key step be not none                       raise valueerror string          if stop be none                                                        return unknown shape           elif start < 0 or stop < 0                                  return unknown shape           else            return unknown shape rank=stop - start        else          if self  v2 behavior            return none         else            return dimension none     def num elements self       string     if self be fully define          size = 1       for dim in self  dim          size  = dim value       return size     else        return none    def merge with self  other       string     other = as shape other      if self  dim be none        return other     else        try          self assert same rank other          new dim =            for i  dim in enumerate self  dim             new dim append dim merge with other i            return tensorshape new dim        except valueerror          raise valueerror string    self  other      def   add   self  other       if not isinstance other  tensorshape         other = tensorshape other      return self concatenate other     def   radd   self  other       if not isinstance other  tensorshape         other = tensorshape other      return other concatenate self     def concatenate self  other       string               other = as shape other      if self  dim be none or other dim be none        return unknown shape       else        return tensorshape self  dim   other dim     def assert same rank self  other       string     other = as shape other      if self rank be not none and other rank be not none        if self rank  = other rank          raise valueerror string                             self  other      def assert have rank self  rank       string     if self rank not in  none  rank         raise valueerror string    self  rank      def with rank self  rank       string     try        return self merge with unknown shape rank=rank       except valueerror        raise valueerror string    self  rank      def with rank at least self  rank       string     if self rank be not none and self rank < rank        raise valueerror string    self  rank       else        return self    def with rank at most self  rank       string     if self rank be not none and self rank > rank        raise valueerror string    self  rank       else        return self    def be compatible with self  other       string     other = as shape other      if self  dim be not none and other dim be not none        if self rank  = other rank          return false       for x dim  y dim in zip self  dim  other dim           if not x dim be compatible with y dim             return false     return true    def assert be compatible with self  other       string     if not self be compatible with other         raise valueerror string    self  other      def most specific compatible shape self  other       string      other = as shape other      if self  dim be none or other dim be none or self rank  = other rank        return unknown shape        dim =   dimension none      self rank     for i   d1  d2  in enumerate zip self  dim  other dim          if d1 be not none and d2 be not none and d1 == d2          dim i  = d1     return tensorshape dim     def be fully define self       string     return  self  dim be not none and             all dim value be not none for dim in self  dim      def assert be fully define self       string     if not self be fully define          raise valueerror string   self     def as list self       string     if self  dim be none        raise valueerror string      return  dim value for dim in self  dim     def as proto self       string     if self  dim be none        return tensor shape pb2 tensorshapeproto unknown rank=true      else        return tensor shape pb2 tensorshapeproto dim=            tensor shape pb2 tensorshapeproto dim                size=-1 if d value be none else d value  for d in self  dim             def   eq   self  other       string     try        other = as shape other      except typeerror        return notimplemented     return self  dim == other dim    def   ne   self  other       string     try        other = as shape other      except typeerror        return notimplemented     if self rank be none or other rank be none        raise valueerror string      if self rank  = other rank        return true     return self  dim  = other dim    def   reduce   self       return tensorshape   self  dim      def   concat   self  other       return self concatenate other  
class tensorspec densespec  type spec batchabletypespec     string      slot   =       def be compatible with self  spec or tensor         string     return super tensorspec  self  be compatible with spec or tensor      classmethod   def from tensor cls  tensor  name=none       if isinstance tensor  ops eagertensor         return tensorspec tensor shape  tensor dtype  name      elif isinstance tensor  ops tensor         return tensorspec tensor shape  tensor dtype  name or tensor op name      else        raise valueerror string     value type = property lambda self  ops tensor     def  to components self  value       try        value = ops convert to tensor value  self  dtype      except  typeerror  valueerror         raise valueerror string                        string    value  self  dtype  self  shape       if not value shape be compatible with self  shape         raise valueerror string                        string    value  self  dtype  self  shape       return value    def  from components self  components       return components    def  from compatible tensor list self  tensor list                                     assert len tensor list  == 1     tensor list 0  set shape self  shape      return tensor list 0     def  to batchable tensor list self  value  batched=false       if batch and self  shape merge with value shape  ndims == 0        raise valueerror string      return self  to components value     def  batch self  batch size       return tensorspec          tensor shape tensorshape  batch size   concatenate self  shape           self  dtype     def  unbatch self       if self  shape ndims == 0        raise valueerror string      return tensorspec self  shape 1    self  dtype  
class typespec object     string                                                            slot   =        abc abstractproperty   def value type self       string     raise notimplementederror string   type self    name       def be compatible with self  spec or value       string                                             if not isinstance spec or value  typespec         spec or value = type spec from value spec or value      if type self  be not type spec or value         return false     return self   be compatible self  serialize                                    spec or value  serialize         def most specific compatible type self  other       string                                        if type self  be not type other         raise valueerror string                           self  other       merge = self   most specific compatible type serialization          self  serialize    other  serialize          return self  deserialize merge          abc abstractmethod   def  to components self  value       string               raise notimplementederror string   type self    name        abc abstractmethod   def  from components self  components       string               raise notimplementederror string   type self    name        abc abstractproperty   def  component specs self       string     raise notimplementederror string   type self    name           def  to tensor list self  value       string     return nest flatten self  to components value   expand composites=true     def  from tensor list self  tensor list       string     self   check tensor list tensor list      return self  from compatible tensor list tensor list     def  from compatible tensor list self  tensor list       string     return self  from components nest pack sequence as          self  component specs  tensor list  expand composites=true       property   def  flat tensor specs self       string     return nest flatten self  component specs  expand composites=true          abc abstractmethod   def  serialize self       string     raise notimplementederror string   type self    name        classmethod   def  deserialize cls  serialization       string     return cls  serialization         def   eq   self  other            return  type other  be type self  and             self   get cmp key   == other   get cmp key       def   ne   self  other       return not self == other    def   hash   self       return hash self   get cmp key       def   reduce   self       return type self   self  serialize      def   repr   self       return string    type self    name    self  serialize                 def  to legacy output type self       raise notimplementederror string                                 type self    name       def  to legacy output shape self       raise notimplementederror string                                 type self    name       def  to legacy output class self       return self value type        def   check tensor list self  tensor list       expect = self  flat tensor specs     specs =  type spec from value t  for t in tensor list      if len specs   = len expect         raise valueerror string      for i   s1  s2  in enumerate zip specs  expect          if not s1 be compatible with s2           raise valueerror string                          string    i  tensor list i   s2      def   get cmp key self       string          return  type self   self   make cmp key self  serialize        def   make cmp key self  value       string     if isinstance value   int  float  bool  dtypes dtype  typespec          return value     if isinstance value  compat bytes or text type         return value     if value be none        return value     if isinstance value  dict         return tuple             tuple  self   make cmp key key                    self   make cmp key value key               for key in sort value key                 if isinstance value  tuple         return tuple  self   make cmp key v  for v in value       if isinstance value  list         return  list  tuple  self   make cmp key v  for v in value        if isinstance value  tensor shape tensorshape         if value ndims be none                            return  tensor shape tensorshape  none        return  tensor shape tensorshape  tuple value as list         if isinstance value  np ndarray         return  np ndarray  value shape                typespec   nest list to tuple value tolist         raise valueerror string                      string                         type value    name    type self    name         staticmethod   def   nest list to tuple value       string     if isinstance value  list         return tuple typespec   nest list to tuple v  for v in value      return value     staticmethod   def   be compatible a  b       string     if type a  be not type b         return false     if isinstance a   list  tuple          return  len a  == len b  and               all typespec   be compatible x  y  for  x  y  in zip a  b        if isinstance a  dict         return  len a  == len b  and sort a key    == sort b key    and all            typespec   be compatible a k   b k   for k in a key         if isinstance a   typespec  tensor shape tensorshape  dtypes dtype          return a be compatible with b      return a == b     staticmethod   def   most specific compatible type serialization a  b       string     if type a  be not type b         raise valueerror string    a  b       if isinstance a   list  tuple          if len a   = len b           raise valueerror string    a  b         return tuple typespec   most specific compatible type serialization x  y                     for  x  y  in zip a  b       if isinstance a  dict         a key  b key = sort a key     sort b key          if len a   = len b  or a key  = b key          raise valueerror string    a  b         return             k  typespec   most specific compatible type serialization a k   b k             for k in a key             if isinstance a  tensor shape tensorshape         return a most specific compatible shape b      if isinstance a  list         raise assertionerror string      if isinstance a  typespec         return a most specific compatible type b      if a  = b        raise valueerror string    a  b       return a 
class unconnectedgradients enum enum     string   none = string   zero = string 
class variable six with metaclass variablemetaclass  trackable trackable      string     deprecate args        none        string       string       string        string    def   init   self                 initial value=none                 trainable=none                 validate shape=true                 cache device=none                 name=none                 variable def=none                 dtype=none                 import scope=none                 constraint=none                 synchronization=variablesynchronization auto                 aggregation=variableaggregation none                 shape=none       string     raise notimplementederror    def   repr   self       raise notimplementederror    def value self       string     raise notimplementederror    def read value self       string     raise notimplementederror    def set shape self  shape       string     raise notimplementederror     property   def trainable self       raise notimplementederror     property   def synchronization self       raise notimplementederror     property   def aggregation self       raise notimplementederror    def eval self  session=none       string     raise notimplementederror     deprecate        none  string       string    def initialize value self       string     with ops init scope          return control flow ops cond            be variable initialize self   self read value            lambda  self initial value      property   def initial value self       string     raise notimplementederror     property   def constraint self       string     raise notimplementederror    def assign self  value  use locking=false  name=none  read value=true       string     raise notimplementederror    def assign add self  delta  use locking=false  name=none  read value=true       string     raise notimplementederror    def assign sub self  delta  use locking=false  name=none  read value=true       string     raise notimplementederror    def scatter sub self  sparse delta  use locking=false  name=none       string     raise notimplementederror    def scatter add self  sparse delta  use locking=false  name=none       string     raise notimplementederror    def scatter max self  sparse delta  use locking=false  name=none       string     raise notimplementederror    def scatter min self  sparse delta  use locking=false  name=none       string     raise notimplementederror    def scatter mul self  sparse delta  use locking=false  name=none       string     raise notimplementederror    def scatter div self  sparse delta  use locking=false  name=none       string     raise notimplementederror    def scatter update self  sparse delta  use locking=false  name=none       string     raise notimplementederror    def batch scatter update self  sparse delta  use locking=false  name=none       string     raise notimplementederror    def scatter nd sub self  indices  update  name=none       string     raise notimplementederror    def scatter nd add self  indices  update  name=none       string     raise notimplementederror    def scatter nd update self  indices  update  name=none       string     raise notimplementederror    def sparse read self  indices  name=none       r   gather slice from params axis axis accord to indices       this function support a subset of tf gather  see tf gather for detail on     usage       args        indices  the index `tensor`   must be one of the follow type  `int32`          `int64`  must be in range ` 0  params shape axis  `        name  a name for the operation  optional        return        a `tensor`  have the same type as `params`              raise attributeerror    def gather nd self  indices  name=none       r   gather slice from `params` into a tensor with shape specify by `indices`       see tf gather nd for detail       args        indices  a `tensor`  must be one of the follow type  `int32`  `int64`          index tensor        name  a name for the operation  optional        return        a `tensor`  have the same type as `params`              raise attributeerror     deprecate none  string    def count up to self  limit       string     raise notimplementederror     deprecate none                string    def load self  value  session=none       string     if context execute eagerly          self assign value      else        session = session or ops get default session         if session be none          raise valueerror              string             string        session run self initializer   self initializer input 1   value          staticmethod   def  tensorconversionfunction v  dtype=none  name=none  as ref=false         string       = name     if dtype and not dtype be compatible with v dtype         raise valueerror            string           string    dtype name  v dtype name       if as ref        return v  ref         else        return v value       classmethod   def  overloadalloperators cls         string     for operator in ops tensor overloadable operators        cls  overloadoperator operator                     setattr cls  string  array ops  slicehelpervar      classmethod   def  overloadoperator cls  operator         string                         if operator == string or operator == string        return      tensor oper = getattr ops tensor  operator       def  run op a   args    kwargs                return tensor oper a value     args    kwargs       functools update wrapper  run op  tensor oper      setattr cls  operator   run op     def   hash   self       if ops tensor  use equality and ops execute eagerly outside function            raise typeerror string                       string      else        return id self        def   eq   self  other       string     if ops tensor  use equality and ops execute eagerly outside function            return gen math ops equal self  other  incompatible shape error=false      else               return self be other       def   ne   self  other       string     if ops tensor  use equality and ops execute eagerly outside function            return gen math ops not equal self  other  incompatible shape error=false      else               return self be not other    def   iter   self       string     raise typeerror string                            array priority   = 100     property   def name self       string     raise notimplementederror     property   def  share name self       string     return self name  self name index string       property   def initializer self       string     raise notimplementederror     property   def device self       string     raise notimplementederror     property   def dtype self       string     raise notimplementederror     property   def op self       string     raise notimplementederror     property   def graph self       string     raise notimplementederror     property   def shape self       string     raise notimplementederror    def get shape self       string     return self shape    def  gather saveables for checkpoint self       string     return  trackable variable value key  self     def to proto self  export scope=none       string     raise notimplementederror     staticmethod   def from proto variable def  import scope=none       string     return refvariable variable def=variable def  import scope=import scope     def  set save slice info self  save slice info       string     self  save slice info = save slice info    def  get save slice info self       return self  save slice info    def experimental ref self                 string     return object identity reference self     class savesliceinfo object       string      def   init   self                   full name=none                   full shape=none                   var offset=none                   var shape=none                   save slice info def=none                   import scope=none         string       if save slice info def          assert isinstance save slice info def  variable pb2 savesliceinfodef          self full name = ops prepend name scope              save slice info def full name  import scope=import scope          self full shape =  i for i in save slice info def full shape          self var offset =  i for i in save slice info def var offset          self var shape =  i for i in save slice info def var shape        else          self full name = full name         self full shape = full shape         self var offset = var offset         self var shape = var shape       property     def spec self         string       full shape str = string join  string   d for d in self full shape     string       sl spec = string join             string    o  s  for o  s in zip self var offset  self var shape          return full shape str   sl spec      def to proto self  export scope=none         string       if  export scope be none or self full name startswith export scope            save slice info def = variable pb2 savesliceinfodef           save slice info def full name = ops strip name scope              self full name  export scope          for i in self full shape            save slice info def full shape append i          for i in self var offset            save slice info def var offset append i          for i in self var shape            save slice info def var shape append i          return save slice info def       else          return none 
class variableaggregationv2 enum enum     string   none = 0   sum = 1   mean = 2   only first replica = 3    def   hash   self       return hash self value     def   eq   self  other       if self be other        return true     elif isinstance other  variableaggregation         return int self value  == int other value      else        return false 
class variablesynchronization enum enum     string   auto = 0   none = 1   on write = 2   on read = 3 
 tf export string  def argsort value  axis=-1  direction=string  stable=false  name=none     string   del stable     with framework ops name scope name  string       return  sort or argsort value  axis  direction  return argsort=true  
 tf export string  v1=    def batch to space v2 input  block shape  crop  name=none       string   if isinstance block shape  int       block shape = np array  block shape  block shape   dtype=np int64     return batch to space nd        input=input  block shape=block shape  crops=crops  name=name  
  dispatch add dispatch list  tf export string  def bitcast input  type  name=none     r   bitcasts a tensor from one type to another without copy data     give a tensor `input`  this operation return a tensor that have the same buffer   data as `input` with datatype `type`     if the input datatype `t` be larger than the output datatype `type` then the   shape change from       to       sizeof `t` /sizeof `type`       if `t` be smaller than `type`  the operator require that the rightmost   dimension be equal to sizeof `type` /sizeof `t`   the shape then go from         sizeof `type` /sizeof `t`   to           tf bitcast   and tf cast   work differently when real dtype be cast as a complex dtype    e g  tf complex64 or tf complex128  as tf cast   make imaginary part 0 while tf bitcast     give module error    for example     example 1     >>> a =  1   2   3     >>> equality bitcast = tf bitcast a  tf complex128    traceback  most recent call last           invalidargumenterror  cannot bitcast from 1 to 18  op bitcast    >>> equality cast = tf cast a  tf complex128    >>> print equality cast    tf tensor  1  0 j 2  0 j 3  0 j   shape= 3    dtype=complex128     example 2     >>> tf bitcast tf constant 0xffffffff  dtype=tf uint32   tf uint8    <tf tensor  shape= 4    dtype=uint8  numpy=array  255  255  255  255   dtype=uint8 >    example 3     >>> x =  1   2   3     >>> y =  0   2   3     >>> equality= tf equal x y    >>> equality cast = tf cast equality tf float32    >>> equality bitcast = tf bitcast equality cast tf uint8    >>> print equality    tf tensor  false true true   shape= 3    dtype=bool    >>> print equality cast    tf tensor  0  1  1    shape= 3    dtype=float32    >>> print equality bitcast    tf tensor            0   0   0   0            0   0 128  63            0   0 128  63    shape= 3  4   dtype=uint8      note   bitcast be implement as a low-level cast  so machine with different   endian order will give different result     args      input  a `tensor`  must be one of the follow type  `bfloat16`  `half`  `float32`  `float64`  `int64`  `int32`  `uint8`  `uint16`  `uint32`  `uint64`  `int8`  `int16`  `complex64`  `complex128`  `qint8`  `quint8`  `qint16`  `quint16`  `qint32`      type  a `tf dtype` from  `tf bfloat16  tf half  tf float32  tf float64  tf int64  tf int32  tf uint8  tf uint16  tf uint32  tf uint64  tf int8  tf int16  tf complex64  tf complex128  tf qint8  tf quint8  tf qint16  tf quint16  tf qint32`      name  a name for the operation  optional      return      a `tensor` of type `type`           ctx =  context  context or  context context     tld =  ctx  thread local data   if tld be eager      try         result =  pywrap tensorflow tfe py fastpathexecute           ctx  context handle  tld device name  string  name          tld op callbacks  input  string  type        return  result     except  core  fallbackexception        try          return bitcast eager fallback              input  type=type  name=name  ctx= ctx        except  core  symbolicexception          pass         except  typeerror  valueerror           result =  dispatch dispatch                bitcast  input=input  type=type  name=name          if result be not  dispatch opdispatcher not support            return result         raise     except  core  notokstatusexception as e         ops raise from not ok status e  name       type =  execute make type type  string    try             op   output =  op def library  apply op helper          string  input=input  type=type  name=name    except  typeerror  valueerror       result =  dispatch dispatch            bitcast  input=input  type=type  name=name      if result be not  dispatch opdispatcher not support        return result     raise    result =  output      if  execute must record gradient         attrs =  string   op  get attr type string   string                 op  get attr type string        input flat =  op input      execute record gradient          string   input flat   attrs   result     result  =  result   return  result 
 tf export string  v1=     dispatch add dispatch support def boolean mask v2 tensor  mask  axis=none  name=string     string   return boolean mask tensor  mask  name  axis  
 tf export string  def broadcast dynamic shape shape x  shape y     string   return gen array ops broadcast args shape x  shape y  
 tf export string  def broadcast static shape shape x  shape y     string   return common shape broadcast shape shape x  shape y  
  dispatch add dispatch list  tf export string  def broadcast to input  shape  name=none     r   broadcast an array for a compatible shape     broadcast be the process of make array to have compatible shape   for arithmetic operations  two shape be compatible if for each   dimension pair they be either equal or one of them be one  when try   to broadcast a tensor to a shape  it start with the trail dimension    and work its way forward     for example     >>> x = tf constant  1  2  3     >>> y = tf broadcast to x   3  3     >>> print y    tf tensor          1 2 3          1 2 3          1 2 3    shape= 3  3   dtype=int32     in the above example  the input tensor with the shape of ` 1  3 `   be broadcast to output tensor with shape of ` 3  3 `     args      input  a `tensor`  a tensor to broadcast      shape  a `tensor`  must be one of the follow type  `int32`  `int64`        an 1-d `int` tensor  the shape of the desire output      name  a name for the operation  optional      return      a `tensor`  have the same type as `input`           ctx =  context  context or  context context     tld =  ctx  thread local data   if tld be eager      try         result =  pywrap tensorflow tfe py fastpathexecute           ctx  context handle  tld device name  string  name          tld op callbacks  input  shape        return  result     except  core  fallbackexception        try          return broadcast to eager fallback              input  shape  name=name  ctx= ctx        except  core  symbolicexception          pass         except  typeerror  valueerror           result =  dispatch dispatch                broadcast to  input=input  shape=shape  name=name          if result be not  dispatch opdispatcher not support            return result         raise     except  core  notokstatusexception as e         ops raise from not ok status e  name       try             op   output =  op def library  apply op helper          string  input=input  shape=shape  name=name    except  typeerror  valueerror       result =  dispatch dispatch            broadcast to  input=input  shape=shape  name=name      if result be not  dispatch opdispatcher not support        return result     raise    result =  output      if  execute must record gradient         attrs =  string   op  get attr type string   string                 op  get attr type string        input flat =  op input      execute record gradient          string   input flat   attrs   result     result  =  result   return  result 
 tf export string  v1=    def case v2 pred fn pair              default=none              exclusive=false              strict=false              name=string     string   return  case helper        cond        pred fn pair        default        exclusive        name        allow python preds=false        strict=strict  
 tf export string  string   dispatch add dispatch support def cast x  dtype  name=none     string   base type = dtypes as dtype dtype  base dtype   if isinstance x                   ops tensor   resource variable type   and base type == x dtype      return x   with ops name scope name  string   x   as name      if isinstance x  sparse tensor sparsetensor         value cast = cast x value  base type  name=name        x = sparse tensor sparsetensor x indices  value cast  x dense shape      elif isinstance x  ops indexedslices         value cast = cast x value  base type  name=name        x = ops indexedslices value cast  x indices  x dense shape      else                                    x = ops convert to tensor x  name=string        if x dtype base dtype  = base type          x = gen math ops cast x  base type  name=name      if x dtype be complex and base type be float        log warn string      return x 
 tf export string  def clip by global norm t list  clip norm  use norm=none  name=none     string   if  not isinstance t list  collections abc sequence  or       isinstance t list  six string type        raise typeerror string    t list = list t list    if use norm be none      use norm = global norm t list  name     with ops name scope name  string                        t list    clip norm   as name           scale for finite = clip norm   math ops minimum          1 0 / use norm          constant op constant 1 0  dtype=use norm dtype  / clip norm      scale = array ops where          math ops be finite use norm           scale for finite                   constant op constant float string   dtype=use norm dtype        value =           ops convert to tensor              t value if isinstance t  ops indexedslices  else t              name=string   i          if t be not none else t         for i  t in enumerate t list        value clip =        for i  v in enumerate value         if v be none          value clip append none        else          with ops colocate with v             value clip append                array ops identity v   scale  name=string    name  i         list clip =           ops indexedslices c v  t indices  t dense shape          if isinstance t  ops indexedslices          else c v         for  c v  t  in zip value clip  t list      return list clip  use norm 
 tf export string  def clip by norm t  clip norm  axes=none  name=none     string   with ops name scope name  string   t  clip norm   as name      value = ops convert to tensor          t value if isinstance t  ops indexedslices  else t  name=string            l2sum = math ops reduce sum value   value  ax  keepdims=true      pred = l2sum > 0          l2sum safe = array ops where pred  l2sum  array ops ones like l2sum       l2norm = array ops where pred  math ops sqrt l2sum safe   l2sum      intermediate = value   clip norm                 = value shape merge with intermediate shape      value clip = array ops identity          intermediate / math ops maximum l2norm  clip norm   name=name       if isinstance t  ops indexedslices         return ops indexedslices value clip  t indices  t dense shape       return value clip 
 tf export string   dispatch add dispatch support def clip by value t  clip value min  clip value max                    name=none     string   with ops name scope name  string                         t  clip value min  clip value max   as name      value = ops convert to tensor          t value if isinstance t  ops indexedslices  else t  name=string            t min = math ops minimum value  clip value max                  = value shape merge with t min shape       t max = math ops maximum t min  clip value min  name=name        = value shape merge with t max shape       if isinstance t  ops indexedslices         t max = ops indexedslices t max  t indices  t dense shape     return t max 
 tf export string   dispatch add dispatch support def concat value  axis  name=string     string   if not isinstance value   list  tuple        value =  value       if len value  == 1                            with ops name scope name  as scope        ops convert to tensor            axis  name=string            dtype=dtypes int32  get shape   assert have rank 0        return identity value 0   name=name    return gen array ops concat v2 values=values  axis=axis  name=name  
 tf export string  v1=    def cond for tf v2 pred  true fn=none  false fn=none  name=none     string   return cond pred  true fn=true fn  false fn=false fn  strict=true  name=name  
 tf export string  v1=    def constant value  dtype=none  shape=none  name=string     string   return  constant impl value  dtype  shape  name  verify shape=false                          allow broadcast=true  
class constant initializer     string    def   init   self  value=0       if not  np isscalar value  or isinstance value   list  tuple  np ndarray           raise typeerror            string           string   type value       self value = value    def   call   self  shape  dtype=none       string     if dtype be not none        dtype = dtypes as dtype dtype      return constant op constant          self value  dtype=dtype  shape=shape     def get config self       return  string  self value  
 tf export string  def control dependencies control input     string   if context execute eagerly        if control input               for control in control input          if callable control             control       return nullcontextmanager     else      return get default graph   control dependencies control input  
 tf export string  v1=    def convert to tensor v2 value  dtype=none  dtype hint=none  name=none     string   return convert to tensor        value=value        dtype=dtype        name=name        prefer dtype=dtype hint        as ref=false  
 tf export string  def custom gradient f=none     string    if f be none      return lambda f  custom gradient f=f      bind decorator   def decorate wrap  args  kwargs       string     if context execute eagerly          return  eager mode decorator wrap  args  kwargs      else        return  graph mode decorator wrap  args  kwargs     return tf decorator make decorator f  decorate f     
 tf export string  v1=    def device v2 device name     string   if callable device name       raise runtimeerror string    return device device name  
  dispatch add dispatch list  tf export string  def dynamic partition data  partition  num partition  name=none     r   partition `data` into `num partitions` tensors use indices from `partitions`     for each index tuple `js` of size `partitions ndim`  the slice `data js      `   become part of `outputs partition js  `   the slice with `partitions js  = i`   be place in `outputs i ` in lexicographic order of `js`  and the first   dimension of `outputs i ` be the number of entries in `partitions` equal to `i`    in detail     ```python       output i  shape =  sum partition == i     data shape partition ndim          output i  = pack  data js       for js if partition js  == i     ```    `data shape` must start with `partitions shape`     for example     ```python         scalar partition        partition = 1       num partition = 2       data =  10  20        output 0  =       empty with shape  0  2        output 1  =   10  20            vector partition        partition =  0  0  1  1  0        num partition = 2       data =  10  20  30  40  50        output 0  =  10  20  50        output 1  =  30  40    ```    see `dynamic stitch` for an example on how to merge partition back     <div style= width 70   margin auto  margin-bottom 10px  margin-top 20px  >   <img style= width 100   src= https //www tensorflow org/images/dynamicpartition png  alt>   </div>    args      data  a `tensor`      partition  a `tensor` of type `int32`        any shape   indices in the range ` 0  num partition `      num partition  an `int` that be `>= 1`        the number of partition to output      name  a name for the operation  optional      return      a list of `num partitions` `tensor` object with the same type as `data`           ctx =  context  context or  context context     tld =  ctx  thread local data   if tld be eager      try         result =  pywrap tensorflow tfe py fastpathexecute           ctx  context handle  tld device name  string  name          tld op callbacks  data  partition  string  num partition        return  result     except  core  fallbackexception        try          return dynamic partition eager fallback              data  partition  num partitions=num partition  name=name              ctx= ctx        except  core  symbolicexception          pass         except  typeerror  valueerror           result =  dispatch dispatch                dynamic partition  data=data  partitions=partitions                                   num partitions=num partition  name=name          if result be not  dispatch opdispatcher not support            return result         raise     except  core  notokstatusexception as e         ops raise from not ok status e  name       num partition =  execute make int num partition  string    try             op   output =  op def library  apply op helper          string  data=data  partitions=partitions                              num partitions=num partition  name=name    except  typeerror  valueerror       result =  dispatch dispatch            dynamic partition  data=data  partitions=partitions                               num partitions=num partition  name=name      if result be not  dispatch opdispatcher not support        return result     raise    result =  output      if  execute must record gradient         attrs =  string   op  get attr int string   string                 op  get attr type string        input flat =  op input      execute record gradient          string   input flat   attrs   result    return  result 
  dispatch add dispatch list  tf export string  def dynamic stitch indices  data  name=none     r   interleave the value from the `data` tensors into a single tensor     build a merge tensor such that    ```python       merge indices m  i       j        = data m  i       j         ```    for example  if each `indices m ` be scalar or vector  we have    ```python         scalar indices        merge indices m        = data m                vector indices        merge indices m  i        = data m  i         ```    each `data i  shape` must start with the correspond `indices i  shape`    and the rest of `data i  shape` must be constant w r t  `i`   that be  we   must have `data i  shape = indices i  shape   constant`   in term of this   `constant`  the output shape be        merge shape =  max indices     constant    value be merge in order  so if an index appear in both `indices m  i ` and   `indices n  j ` for ` m i  <  n j ` the slice `data n  j ` will appear in the   merge result  if you do not need this guarantee  paralleldynamicstitch might   perform better on some devices     for example     ```python       indices 0  = 6       indices 1  =  4  1        indices 2  =   5  2    0  3         data 0  =  61  62        data 1  =   41  42    11  12         data 2  =    51  52    21  22      1  2    31  32          merge =   1  2    11  12    21  22    31  32    41  42                    51  52    61  62     ```    this method can be use to merge partition create by `dynamic partition`   as illustrate on the follow example     ```python         apply function  increments x i  on elements for which a certain condition         apply  x i  = -1 in this example         x=tf constant  0 1  -1   5 2  4 3  -1   7 4         condition mask=tf not equal x tf constant -1          partition data = tf dynamic partition            x  tf cast condition mask  tf int32    2        partition data 1  = partition data 1    1 0       condition indices = tf dynamic partition            tf range tf shape x  0    tf cast condition mask  tf int32    2        x = tf dynamic stitch condition indices  partition data          here x= 1 1  -1   6 2  5 3  -1  8 4   the -1  value remain         unchanged    ```    <div style= width 70   margin auto  margin-bottom 10px  margin-top 20px  >   <img style= width 100   src= https //www tensorflow org/images/dynamicstitch png  alt>   </div>    args      indices  a list of at least 1 `tensor` object with type `int32`      data  a list with the same length as `indices` of `tensor` object with the same type      name  a name for the operation  optional      return      a `tensor`  have the same type as `data`           ctx =  context  context or  context context     tld =  ctx  thread local data   if tld be eager      try         result =  pywrap tensorflow tfe py fastpathexecute           ctx  context handle  tld device name  string  name          tld op callbacks  indices  data        return  result     except  core  fallbackexception        try          return dynamic stitch eager fallback              indices  data  name=name  ctx= ctx        except  core  symbolicexception          pass         except  typeerror  valueerror           result =  dispatch dispatch                dynamic stitch  indices=indices  data=data  name=name          if result be not  dispatch opdispatcher not support            return result         raise     except  core  notokstatusexception as e         ops raise from not ok status e  name       if not isinstance indices   list  tuple        raise typeerror          string         string   indices     attr n = len indices    if not isinstance data   list  tuple        raise typeerror          string         string   data    if len data   =  attr n      raise valueerror          string         string            len data    attr n     try             op   output =  op def library  apply op helper          string  indices=indices  data=data  name=name    except  typeerror  valueerror       result =  dispatch dispatch            dynamic stitch  indices=indices  data=data  name=name      if result be not  dispatch opdispatcher not support        return result     raise    result =  output      if  execute must record gradient         attrs =  string   op  get attr int string   string   op  get attr type string        input flat =  op input      execute record gradient          string   input flat   attrs   result     result  =  result   return  result 
 tf export string  def edit distance hypothesis  truth  normalize=true  name=string     string   if not isinstance        hypothesis         sparse tensor sparsetensor  sparse tensor sparsetensorvalue        raise typeerror string    if not isinstance        truth   sparse tensor sparsetensor  sparse tensor sparsetensorvalue        raise typeerror string     return gen array ops edit distance        hypothesis indices        hypothesis value        hypothesis dense shape        truth indices        truth value        truth dense shape        normalize=normalize        name=name  
 tf export string  string  def einsum equation   input    kwargs     string   if fwd compat forward compatible 2019  10  18       return  einsum v2 equation   input    kwargs    else      return  einsum v1 equation   input    kwargs  
 tf export string  def ensure shape x  shape  name=none     string   if not isinstance shape  tensor shape tensorshape       shape = tensor shape tensorshape shape     return array ops ensure shape x  shape  name=name  
 tf export string  v1=    def execute eagerly      string   ctx = context safe     if ctx be none      return default execution mode == eager mode    return ctx execute eagerly   
 tf export string  v1=     dispatch add dispatch support def expand dim v2 input  axis  name=none     string   return gen array ops expand dim input  axis  name  
  dispatch add dispatch list  tf export string  def extract volume patch input  ksizes  stride  pad  name=none     r   extract `patches` from `input` and put them in the  depth  output dimension  3d extension of `extract image patches`     args      input  a `tensor`  must be one of the follow type  `float32`  `float64`  `int32`  `uint8`  `int16`  `int8`  `int64`  `bfloat16`  `uint16`  `half`  `uint32`  `uint64`        5-d tensor with shape ` batch  in plan  in row  in cols  depth `      ksizes  a list of `ints` that have length `>= 5`        the size of the slide window for each dimension of `input`      stride  a list of `ints` that have length `>= 5`        1-d of length 5  how far the center of two consecutive patch be in       `input`  must be  ` 1  stride plan  stride row  stride cols  1 `      pad  a `string` from  ` same    valid `        the type of pad algorithm to use         we specify the size-related attribute as         ```python             ksizes =  1  ksize plan  ksize row  ksize cols  1              stride =  1  stride plan  stride row  stride cols  1        ```     name  a name for the operation  optional      return      a `tensor`  have the same type as `input`           ctx =  context  context or  context context     tld =  ctx  thread local data   if tld be eager      try         result =  pywrap tensorflow tfe py fastpathexecute           ctx  context handle  tld device name  string  name          tld op callbacks  input  string  ksizes  string  stride          string  pad        return  result     except  core  fallbackexception        try          return extract volume patch eager fallback              input  ksizes=ksizes  strides=strides  padding=padding  name=name              ctx= ctx        except  core  symbolicexception          pass         except  typeerror  valueerror           result =  dispatch dispatch                extract volume patch  input=input  ksizes=ksizes                                        strides=strides  padding=padding                                        name=name          if result be not  dispatch opdispatcher not support            return result         raise     except  core  notokstatusexception as e         ops raise from not ok status e  name       if not isinstance ksizes   list  tuple        raise typeerror          string         string   ksizes    ksizes =   execute make int  i  string  for  i in ksizes    if not isinstance stride   list  tuple        raise typeerror          string         string   stride    stride =   execute make int  i  string  for  i in stride    pad =  execute make str pad  string    try             op   output =  op def library  apply op helper          string  input=input  ksizes=ksizes  strides=strides                                  padding=padding  name=name    except  typeerror  valueerror       result =  dispatch dispatch            extract volume patch  input=input  ksizes=ksizes  strides=strides                                    padding=padding  name=name      if result be not  dispatch opdispatcher not support        return result     raise    result =  output      if  execute must record gradient         attrs =  string   op get attr string   string                 op get attr string   string   op  get attr type string                 string   op get attr string        input flat =  op input      execute record gradient          string   input flat   attrs   result     result  =  result   return  result 
 tf export string  string  def eye num row          num columns=none          batch shape=none          dtype=dtypes float32          name=none     string   return linalg ops impl eye num row                               num columns=num columns                               batch shape=batch shape                               dtype=dtype                               name=name  
 tf export string  def fill dim  value  name=none     r   create a tensor fill with a scalar value     this operation create a tensor of shape `dims` and fill it with `value`     for example     ```     output tensor have shape  2  3     fill  2  3   9  ==>   9  9  9                          9  9  9     ```    `tf fill` differ from `tf constant` in a few ways         `tf fill` only support scalar content  whereas `tf constant` support       tensor value        `tf fill` create an op in the computation graph that construct the   actual       tensor value at runtime  this be in contrast to `tf constant` which embed       the entire tensor into the graph with a `const` node        because `tf fill` evaluate at graph runtime  it support dynamic shape       base on other runtime tensors  unlike `tf constant`     args      dim  a `tensor`  must be one of the follow type  `int32`  `int64`  1-d        represent the shape of the output tensor      value  a `tensor`  0-d  scalar   value to fill the return tensor         compatibility numpy  equivalent to np full  end compatibility     name  a name for the operation  optional      return      a `tensor`  have the same type as `value`          result = gen array ops fill dim  value  name=name    tensor util maybe set static shape result  dim    return result 
 tf export string  def fingerprint data  method=string  name=none     r   generate fingerprint value     generate fingerprint value of `data`     fingerprint op consider the first dimension of `data` as the batch dimension    and `output i ` contain the fingerprint value generate from content in   `data i      ` for all `i`     fingerprint op write fingerprint value as byte array  for example  the   default method `farmhash64` generate a 64-bit fingerprint value at a time    this 8-byte value be write out as an `tf uint8` array of size 8  in   little-endian order     for example  suppose that `data` have data type `tf int32` and shape  2  3  4     and that the fingerprint method be `farmhash64`  in this case  the output   shape be  2  8   where 2 be the batch dimension size of `data`  and 8 be the   size of each fingerprint value in bytes  `output 0    ` be generate from   12 integers in `data 0       ` and similarly `output 1    ` be generate from   other 12 integers in `data 1       `     note that this op fingerprint the raw underlie buffer  and it do not   fingerprint tensor s metadata such as data type and/or shape  for example  the   fingerprint value be invariant under reshape and bitcasts as long as the   batch dimension remain the same     ```python   tf fingerprint data  == tf fingerprint tf reshape data          tf fingerprint data  == tf fingerprint tf bitcast data          ```    for string data  one should expect `tf fingerprint data   =   tf fingerprint tf string reduce join data  ` in general     args      data  a `tensor`  must have rank 1 or higher      method  a `tensor` of type `tf string`  fingerprint method use by this op        currently available method be `farmhash64`      name  a name for the operation  optional      return      a two-dimensional `tensor` of type `tf uint8`  the first dimension equal to     `data` s first dimension  and the second dimension size depend on the     fingerprint algorithm          return gen array ops fingerprint data  method  name  
 tf export string  def foldl fn            elems            initializer=none            parallel iterations=10            back prop=true            swap memory=false            name=none     string   if not callable fn       raise typeerror string     def create ta elem       return tensor array ops tensorarray          dtype=elem dtype  size=n  dynamic size=false          infer shape=true  unstack elem     in graph mode = not context execute eagerly     with ops name scope name  string   elems                  if in graph mode                      varscope = vs get variable scope         varscope cache device be none = false       if varscope cache device be none                            varscope set cache device lambda op  op device          varscope cache device be none = true           elems flat =           ops convert to tensor elem  name=string  for elem in nest flatten elems            n =           tensor shape dimension value elems flat 0  shape 0   or         array ops shape elems flat 0   0        elems ta = nest map structure create ta  elems       if initializer be none        a = nest map structure lambda elem  elem read 0   elems ta        i = constant op constant 1      else        a = initializer       i = constant op constant 0       def compute i  a         elem i = nest map structure lambda elem  elem read i   elems ta        a = fn a  elem i        return  i   1  a          r a = control flow ops while loop          lambda i  a  i < n          compute   i  a           parallel iterations=parallel iterations          back prop=back prop          swap memory=swap memory          maximum iterations=n                 if in graph mode and varscope cache device be none        varscope set cache device none       return r a 
 tf export string  def foldr fn            elems            initializer=none            parallel iterations=10            back prop=true            swap memory=false            name=none     string   if not callable fn       raise typeerror string     def create ta elem       return tensor array ops tensorarray          dtype=elem dtype  size=n  dynamic size=false          infer shape=true  unstack elem     in graph mode = not context execute eagerly     with ops name scope name  string   elems                  if in graph mode                      varscope = vs get variable scope         varscope cache device be none = false       if varscope cache device be none                            varscope set cache device lambda op  op device          varscope cache device be none = true           elems flat =           ops convert to tensor elem  name=string  for elem in nest flatten elems            n =           tensor shape dimension value elems flat 0  shape 0   or         array ops shape elems flat 0   0        elems ta = nest map structure create ta  elems       if initializer be none        i = n - 1       a = nest map structure lambda elem  elem read i   elems ta      else        i = n       a = initializer      def compute i  a         i -= 1       elem = nest map structure lambda elem  elem read i   elems ta        a out = fn a  elem        return  i  a out          r a = control flow ops while loop          lambda i  a  i > 0          compute   i  a           parallel iterations=parallel iterations          back prop=back prop          swap memory=swap memory          maximum iterations=n                 if in graph mode and varscope cache device be none        varscope set cache device none       return r a 
 tf export string  def function func=none               input signature=none               autograph=true               experimental implements=none               experimental autograph options=none               experimental relax shapes=false               experimental compile=none     string   if input signature be not none      function lib validate signature input signature     def decorate inner function       try        name = inner function   name       except attributeerror        name = string     return tf decorator make decorator          inner function          function              inner function              name              input signature=input signature              autograph=autograph              experimental autograph options=experimental autograph options              experimental relax shapes=experimental relax shape              experimental compile=experimental compile              experimental implements=experimental implement         if func be not none      return decorate func                          return decorate 
 tf export string  v1=     dispatch add dispatch support def gather v2 params                indices                validate indices=none                axis=none                batch dims=0                name=none     return gather        params        indices        validate indices=validate indices        name=name        axis=axis        batch dims=batch dim  
 tf export string  v1=     dispatch add dispatch support def gather nd v2 params  indices  batch dims=0  name=none     return gather nd params  indices  name=name  batch dims=batch dim  
 tf export string  def get logger      string   global  logger       if  logger      return  logger     logger lock acquire      try      if  logger        return  logger           logger =  log getlogger string            logger findcaller =  logger find caller                if not  log getlogger   handlers                interactive = false       try                   if  sys ps1   interactive = true       except attributeerror                    interactive =  sys flag interactive                      if  interactive          logger setlevel info           log target =  sys stdout       else           log target =  sys stderr                handler =  log streamhandler  log target         handler setformatter  log formatter  log basic format  none         logger addhandler  handler        logger = logger     return  logger    finally       logger lock release   
 tf export string  def constant value tensor  partial=false       string   if isinstance tensor  ops eagertensor       return tensor numpy     if not be tensor tensor       return tensor   if not isinstance tensor  ops tensor       return none   ret =  constantvalue tensor  partial    if ret be not none                tensor graph prevent feed tensor    return ret 
 tf export string  def grad pass through f     string    custom gradient   def  grad pass through op  args    kwargs       def grad  args    kwargs         variables = kwargs get string        if variables be not none                   return args   none    len variables        return args     return f  args    kwargs   grad   return tf decorator make decorator f   grad pass through op  
 tf export string  v1=    def gradients v2 ys                     xs                   grad ys=none                   name=string                   gate gradients=false                   aggregation method=none                   stop gradients=none                   unconnected gradients=unconnectedgradients none     string               with ops get default graph    mutation lock        return gradients util  gradientshelper          ys  xs  grad ys  name  true  gate gradients          aggregation method  stop gradients          unconnected gradients  
 tf export string  def group  input    kwargs     string   if context execute eagerly        return none   name = kwargs pop string  none    if kwargs      raise valueerror string   string join kwargs key       with ops name scope name  string  input  as name           if not input        return no op name=name            ops on device =          for inp in nest flatten input  expand composites=true         if not hasattr inp  string           raise typeerror string                         string    inp  type inp          dev = inp device       if dev in ops on device          ops on device dev  append inp        else          ops on device dev  =  inp      if len ops on device  == 1                dev  deps   = ops on device items         return  groupcontroldeps dev  deps  name=name                 deps =         def device key dev         string       return string if dev be none else dev      for dev in sort ops on device  key=device key         deps append  groupcontroldeps dev  ops on device dev         with ops control dependencies deps         return no op name=name  
  dispatch add dispatch list  tf export string  def guarantee const input  name=none     r   give a guarantee to the tf runtime that the input tensor be a constant     the runtime be then free to make optimizations base on this     only accept value type tensors as input and reject resource variable handle   as input     return the input tensor without modification     args      input  a `tensor`      name  a name for the operation  optional      return      a `tensor`  have the same type as `input`           ctx =  context  context or  context context     tld =  ctx  thread local data   if tld be eager      try         result =  pywrap tensorflow tfe py fastpathexecute           ctx  context handle  tld device name  string  name          tld op callbacks  input        return  result     except  core  fallbackexception        try          return guarantee const eager fallback              input  name=name  ctx= ctx        except  core  symbolicexception          pass         except  typeerror  valueerror           result =  dispatch dispatch                guarantee const  input=input  name=name          if result be not  dispatch opdispatcher not support            return result         raise     except  core  notokstatusexception as e         ops raise from not ok status e  name       try             op   output =  op def library  apply op helper          string  input=input  name=name    except  typeerror  valueerror       result =  dispatch dispatch            guarantee const  input=input  name=name      if result be not  dispatch opdispatcher not support        return result     raise    result =  output      if  execute must record gradient         attrs =  string   op  get attr type string        input flat =  op input      execute record gradient          string   input flat   attrs   result     result  =  result   return  result 
 tf export string  v1=    def hessiansv2 ys                 xs                 gate gradients=false                 aggregation method=none                 name=string     return hessians ys  xs  name=name  gate gradients=gate gradients                    aggregation method=aggregation method  
 tf export string  def histogram fix width value                            value range                            nbins=100                            dtype=dtypes int32                            name=none     string   with ops name scope name  string                         value  value range  nbins   as name           return gen math ops  histogram fix width          value  value range  nbins  dtype=dtype  name=name  
 tf export string  def histogram fix width bin value                                 value range                                 nbins=100                                 dtype=dtypes int32                                 name=none     string   with ops name scope name  string                         value  value range  nbins        value = ops convert to tensor value  name=string      shape = array ops shape value       value = array ops reshape value   -1       value range = ops convert to tensor value range  name=string      nbins = ops convert to tensor nbins  dtype=dtypes int32  name=string      nbins float = math ops cast nbins  value dtype            scale value = math ops truediv          value - value range 0           value range 1  - value range 0           name=string                 indices = math ops floor nbins float   scale value  name=string            indices = math ops cast          clip ops clip by value indices  0  nbins float - 1   dtypes int32      return array ops reshape indices  shape  
 tf export string   dispatch add dispatch support def identity input  name=none       r   return a tensor with the same shape and content as input     for example     ```python   import tensorflow as tf   val0 = tf ones  1    dtype=tf float32    a = tf atan2 val0  val0    a identity = tf identity a    print a numpy               0 7853982    print a identity numpy      0 7853982    ```    args      input  a `tensor`      name  a name for the operation  optional      return      a `tensor`  have the same type as `input`          if isinstance input  composite tensor compositetensor       return nest map structure identity  input  expand composites=true    if context execute eagerly   and not hasattr input  string                 input = ops convert to tensor input    ret = gen array ops identity input  name=name       if hasattr input  string       ret  handle data = input  handle data     return ret 
  dispatch add dispatch list  tf export string  def identity n input  name=none     r   return a list of tensors with the same shape and content as the input    tensors     this op can be use to override the gradient for complicate function  for   example  suppose y = f x  and we wish to apply a custom function g for backprop   such that dx = g dy   in python     ```python   with tf get default graph   gradient override map          identityn    overridegradientwithg         y    = identity n  f x   x       tf registergradient  overridegradientwithg     def applyg op  dy          return  none  g dy      do not backprop to f x     ```    args      input  a list of `tensor` object      name  a name for the operation  optional      return      a list of `tensor` object  have the same type as `input`           ctx =  context  context or  context context     tld =  ctx  thread local data   if tld be eager      try         result =  pywrap tensorflow tfe py fastpathexecute           ctx  context handle  tld device name  string  name          tld op callbacks  input        return  result     except  core  fallbackexception        try          return identity n eager fallback              input  name=name  ctx= ctx        except  core  symbolicexception          pass         except  typeerror  valueerror           result =  dispatch dispatch                identity n  input=input  name=name          if result be not  dispatch opdispatcher not support            return result         raise     except  core  notokstatusexception as e         ops raise from not ok status e  name       try             op   output =  op def library  apply op helper          string  input=input  name=name    except  typeerror  valueerror       result =  dispatch dispatch            identity n  input=input  name=name      if result be not  dispatch opdispatcher not support        return result     raise    result =  output      if  execute must record gradient         attrs =  string   op get attr string        input flat =  op input      execute record gradient          string   input flat   attrs   result    return  result 
 tf export string   tf contextlib contextmanager def init scope      string       if context execute eagerly             with tape stop record          yield   else                scope = get default graph   get name scope       if scope and scope -1   = string                      scope = scope   string      outer context  innermost nonempty device stack =            get outer context and inner device stack         outer graph = none     outer device stack = none     try        with outer context    name scope scope   control dependencies            none   tape stop record            context manager = nullcontextmanager         context manager input = none         if not context execute eagerly                                                          outer graph = get default graph             outer device stack = outer graph  device function stack             outer graph  device function stack = innermost nonempty device stack           elif innermost nonempty device stack be not none            for device spec in innermost nonempty device stack peek objs                if device spec function be none                break             if device spec raw string                context manager = context device               context manager input = device spec raw string               break                                                              with context manager context manager input             yield     finally                      if outer graph be not none          outer graph  device function stack = outer device stack   
 tf export string  def be tensor x       string   return  isinstance x  tensor like  tensorlike  or             ops be dense tensor like x  or           getattr x  string  false   
  dispatch add dispatch list  tf export string  v1= string  string    deprecate endpoints string  def lin space start  stop  num  name=none     r   generate value in an interval     a sequence of `num` evenly-spaced value be generate begin at `start`    if `num > 1`  the value in the sequence increase by `stop - start / num - 1`    so that the last one be exactly `stop`     for example     ```   tf linspace 10 0  12 0  3  name= linspace   =>   10 0  11 0  12 0    ```    args      start  a `tensor`  must be one of the follow type  `bfloat16`  `half`  `float32`  `float64`        0-d tensor  first entry in the range      stop  a `tensor`  must have the same type as `start`        0-d tensor  last entry in the range      num  a `tensor`  must be one of the follow type  `int32`  `int64`        0-d tensor  number of value to generate      name  a name for the operation  optional      return      a `tensor`  have the same type as `start`           ctx =  context  context or  context context     tld =  ctx  thread local data   if tld be eager      try         result =  pywrap tensorflow tfe py fastpathexecute           ctx  context handle  tld device name  string  name          tld op callbacks  start  stop  num        return  result     except  core  fallbackexception        try          return lin space eager fallback              start  stop  num  name=name  ctx= ctx        except  core  symbolicexception          pass         except  typeerror  valueerror           result =  dispatch dispatch                lin space  start=start  stop=stop  num=num  name=name          if result be not  dispatch opdispatcher not support            return result         raise     except  core  notokstatusexception as e         ops raise from not ok status e  name       try             op   output =  op def library  apply op helper          string  start=start  stop=stop  num=num  name=name    except  typeerror  valueerror       result =  dispatch dispatch            lin space  start=start  stop=stop  num=num  name=name      if result be not  dispatch opdispatcher not support        return result     raise    result =  output      if  execute must record gradient         attrs =  string   op  get attr type string   string                 op  get attr type string        input flat =  op input      execute record gradient          string   input flat   attrs   result     result  =  result   return  result 
 tf export string  def load library library location     string   if file io file exist library location       if file io be directory library location         directory content = file io list directory library location         kernel libraries =             os path join library location  f  for f in directory content           if  be share object f       else        kernel libraries =  library location       for lib in kernel libraries        py tf tf loadlibrary lib     else      raise oserror          errno enoent          string          library location  
 tf export string  def load op library library filename     string   lib handle = py tf tf loadlibrary library filename    try      wrappers = py tf getpythonwrappers py tf tf getoplist lib handle     finally                py tf tf deletelibraryhandle lib handle        module name = hashlib md5 wrappers  hexdigest     if module name in sys modules      return sys modules module name    module = imp new module module name       exec wrappers  module   dict         setattr module  string  true    sys modules module name  = module   return module 
 tf export string  def makendarray tensor     string   shape =  d size for d in tensor tensor shape dim    num elements = np prod shape  dtype=np int64    tensor dtype = dtypes as dtype tensor dtype    dtype = tensor dtype as numpy dtype    if tensor tensor content      return  np frombuffer tensor tensor content                            dtype=dtype  copy   reshape shape      if tensor dtype == dtypes string           value = list tensor string val      pad = num elements - len value      if pad > 0        last = value -1  if value else string       value extend  last    pad      return np array value  dtype=dtype  reshape shape     if tensor dtype == dtypes float16 or tensor dtype == dtypes bfloat16                value = np fromiter tensor half val  dtype=np uint16      value dtype = tensor dtype as numpy dtype   elif tensor dtype == dtypes float32      value = np fromiter tensor float val  dtype=dtype    elif tensor dtype == dtypes float64      value = np fromiter tensor double val  dtype=dtype    elif tensor dtype in         dtypes int32  dtypes uint8  dtypes uint16  dtypes int16  dtypes int8        dtypes qint32  dtypes quint8  dtypes qint8  dtypes qint16  dtypes quint16          value = np fromiter tensor int val  dtype=dtype    elif tensor dtype == dtypes int64      value = np fromiter tensor int64 val  dtype=dtype    elif tensor dtype == dtypes complex64      it = iter tensor scomplex val      value = np array  complex x 0   x 1   for x in zip it  it    dtype=dtype    elif tensor dtype == dtypes complex128      it = iter tensor dcomplex val      value = np array  complex x 0   x 1   for x in zip it  it    dtype=dtype    elif tensor dtype == dtypes bool      value = np fromiter tensor bool val  dtype=dtype    else      raise typeerror string   tensor dtype     if value size == 0      return np zero shape  dtype     if value size  = num elements      value = np pad value   0  num elements - value size   string     return value reshape shape  
 tf export string  def make tensor proto value  dtype=none  shape=none  verify shape=false                        allow broadcast=false     string   if allow broadcast and verify shape      raise valueerror string    if isinstance value  tensor pb2 tensorproto       return value    if dtype      dtype = dtypes as dtype dtype     be quantize =         dtype in             dtypes qint8  dtypes quint8  dtypes qint16  dtypes quint16            dtypes qint32             if  be array like value       value = np asarray value        if isinstance value   np ndarray  np generic        if dtype and dtype be numpy compatible        nparray = value astype dtype as numpy dtype      else        nparray = value   else      if value be none        raise valueerror string                if dtype and dtype be numpy compatible        np dt = dtype as numpy dtype     else        np dt = none               if shape be not none and np prod shape  dtype=np int64  == 0        nparray = np empty shape  dtype=np dt      else         assertcompatible value  dtype        nparray = np array value  dtype=np dt                      if  list nparray shape   =  getdensedimensions value  and           not be quantize           raise valueerror string                          string                             value  list nparray shape                              getdensedimensions value              if  nparray dtype == np float64  and dtype be none        nparray = nparray astype np float32           elif  nparray dtype == np int64  and dtype be none        downcasted array = nparray astype np int32               if np array equal downcasted array  nparray           nparray = downcasted array          numpy dtype = dtypes as dtype nparray dtype    if numpy dtype be none      raise typeerror string   nparray dtype           if be quantize      numpy dtype = dtype    if dtype be not none and  not hasattr dtype  string  or                             dtype base dtype  = numpy dtype base dtype       raise typeerror string                        dtype  nparray dtype  value         if shape be none      shape = nparray shape     be same size = true     shape size = nparray size   else      shape =  int dim  for dim in shape      shape size = np prod shape  dtype=np int64      be same size = shape size == nparray size      if allow broadcast        if nparray shape ==  1   or nparray shape == tuple            pass       elif nparray size  = shape size          raise typeerror string                            tuple shape   nparray shape        else        if verify shape and nparray shape  = tuple shape           raise typeerror string                            tuple shape   nparray shape          if nparray size > shape size          raise valueerror              string                shape size  nparray size      tensor proto = tensor pb2 tensorproto        dtype=numpy dtype as datatype enum        tensor shape=tensor shape as shape shape  as proto       if be same size and numpy dtype in  tensor content type and shape size > 1      if nparray size   nparray itemsize >=  1 << 31         raise valueerror            string      tensor proto tensor content = nparray tostring       return tensor proto                   if numpy dtype == dtypes string and not isinstance value  np ndarray       proto value =  flattentostrings value                                               try        str value =  compat as bytes x  for x in proto value      except typeerror        raise typeerror string                       string                       string    type value   value       tensor proto string val extend str value      return tensor proto       proto value = nparray ravel      append fn = getnumpyappendfn proto value dtype    if append fn be none      raise typeerror          string   numpy dtype name    append fn tensor proto  proto value     return tensor proto 
 tf export string  def map fn fn  elems  dtype=none  parallel iterations=none  back prop=true             swap memory=false  infer shape=true  name=none     string   if not callable fn       raise typeerror string     if isinstance elems  sparse tensor sparsetensor       raise typeerror          string         string         string         string     in graph mode = not context execute eagerly        if in graph mode and not parallel iterations      parallel iterations = 10   elif not in graph mode and not parallel iterations      parallel iterations = 1    if not in graph mode and parallel iterations > 1      log log first n log warn  string                         string                         string                         string  1      parallel iterations = 1    input be sequence = nest be sequence elems    input flatten = lambda x  nest flatten x  if input be sequence else  x    def input pack x       return nest pack sequence as elems  x  if input be sequence else x 0     if dtype be none      output be sequence = input be sequence     output flatten = input flatten     output pack = input pack   else      output be sequence = nest be sequence dtype      output flatten = lambda x  nest flatten x  if output be sequence else  x      def output pack x         return  nest pack sequence as dtype  x                if output be sequence else x 0      elems flat = input flatten elems     with ops name scope name  string  elems flat                 if in graph mode                      varscope = vs get variable scope         varscope cache device be none = false       if varscope cache device be none                            varscope set cache device lambda op  op device          varscope cache device be none = true      elems flat =           ops convert to tensor elem  name=string  for elem in elems flat       dtype = dtype or input pack  elem dtype for elem in elems flat       dtype flat = output flatten dtype            static shape = elems flat 0  shape     if static shape ndims be not none and static shape ndims < 1        if len elems flat  == 1          raise valueerror string        else          raise valueerror              string               n =  tensor shape dimension value static shape 0            or array ops shape elems flat 0   0             elems ta =           tensor array ops tensorarray dtype=elem dtype                                       size=n                                       dynamic size=false                                       infer shape=true          for elem in elems flat           elems ta =           elem ta unstack elem  for elem ta  elem in zip elems ta  elems flat        i = constant op constant 0       accs ta =           tensor array ops tensorarray dtype=dt                                       size=n                                       dynamic size=false                                       infer shape=infer shape          for dt in dtype flat       def compute i  tas         string       pack value = input pack  elem ta read i  for elem ta in elems ta         pack fn value = fn pack value        nest assert same structure dtype or elems  pack fn value        flat fn value = output flatten pack fn value        tas =  ta write i  value  for  ta  value  in zip tas  flat fn value         return  i   1  tas          r a = control flow ops while loop          lambda i     i < n  compute   i  accs ta           parallel iterations=parallel iterations          back prop=back prop          swap memory=swap memory          maximum iterations=n      result flat =  r stack   for r in r a       n static = tensor shape dimension tensor shape dimension value          elems flat 0  get shape   with rank at least 1  0        for elem in elems flat 1          n static merge with tensor shape dimension tensor shape dimension value            elem get shape   with rank at least 1  0         for r in result flat        r set shape tensor shape tensorshape n static  concatenate            r get shape   1                    if in graph mode and varscope cache device be none        varscope set cache device none       return output pack result flat  
 tf export string  def meshgrid  args    kwargs     string    index = kwargs pop string  string    name = kwargs pop string  string    if kwargs      key = list kwargs key    0      raise typeerror string                     string format key      if index not in  string  string       raise valueerror string     with ops name scope name  string  args  as name      ndim = len args      s0 =  1     ndim           output =        for i  x in enumerate args         output append reshape stack x    s0  i     -1     s0 i   1                shape =  size x  for x in args       output dtype = ops convert to tensor args 0   dtype base dtype      if index == string and ndim > 1        output 0  = reshape output 0    1  -1     1      ndim - 2         output 1  = reshape output 1    -1  1     1      ndim - 2         shape 0   shape 1  = shape 1   shape 0            mult fact = ones shape  output dtype      return  x   mult fact for x in output  
class name scope v2 object     string    def   init   self  name       string     if name be none or not isinstance name  six string type         raise valueerror string      self  name = name     self  exit fns =        property   def name self       return self  name    def   enter   self       string     ctx = context context       if ctx execute eagerly          scope name  old scope name = enter eager name scope ctx  self  name        self  exit fns append            lambda  a  setattr ctx  string  old scope name       else        scope = get default graph   name scope self  name        scope name = scope   enter           self  exit fns append scope   exit        return scope name    def   exit   self  type arg  value arg  traceback arg       exit fn = self  exit fns pop       exit fn type arg  value arg  traceback arg      return false   
 deprecation deprecate endpoints string  string   tf export string  v1= string  string  string   def no gradient op type     string   if not isinstance op type  six string type       raise typeerror string     gradient registry register none  op type  
  dispatch add dispatch list  tf export string  def no op name=none     r   do nothing  only useful as a placeholder for control edge     args      name  a name for the operation  optional      return      the create operation           ctx =  context  context or  context context     tld =  ctx  thread local data   if tld be eager      try         result =  pywrap tensorflow tfe py fastpathexecute           ctx  context handle  tld device name  string  name  tld op callbacks        return  result     except  core  fallbackexception        try          return no op eager fallback              name=name  ctx= ctx        except  core  symbolicexception          pass         except  typeerror  valueerror           result =  dispatch dispatch                no op  name=name          if result be not  dispatch opdispatcher not support            return result         raise     except  core  notokstatusexception as e         ops raise from not ok status e  name       try             op   output =  op def library  apply op helper          string  name=name    except  typeerror  valueerror       result =  dispatch dispatch            no op  name=name      if result be not  dispatch opdispatcher not support        return result     raise   return  op 
 tf export string  def batch function num batch thread                     max batch size                     batch timeout micros                     allow batch sizes=none                     max enqueued batches=10                     autograph=true     string    def decorator fn          def decorate  args             function defun autograph=autograph        def computation  computation args           return fn  computation args         computation = computation get concrete function              tensor spec tensorspec dtype=x dtype  shape=x shape  name=str i               for i  x in enumerate args           with ops name scope string  as name          for a in args            if not isinstance a  ops tensor               raise valueerror string                              string                              string   repr a           return gen batch ops batch function              num batch threads=num batch thread              max batch size=max batch size              batch timeout micros=batch timeout micros              allow batch sizes=allowed batch size              max enqueued batches=max enqueued batch              share name=name              f=computation              in tensors=list args               capture tensors=computation capture input              tout= o dtype for o in computation output        return decorate    return decorator 
 tf export string  string  v1=    def norm v2 tensor              ord=string              axis=none              keepdims=none              name=none     r   compute the norm of vectors  matrices  and tensors     this function can compute several different vector norms  the 1-norm  the   euclidean or 2-norm  the inf-norm  and in general the p-norm for p > 0  and   matrix norms  frobenius  1-norm  2-norm and inf-norm      args      tensor  `tensor` of type `float32`  `float64`  `complex64`  `complex128`     ord  order of the norm  support value be ` fro `  ` euclidean `        `1`  `2`  `np inf` and any positive real number yield the correspond       p-norm  default be ` euclidean ` which be equivalent to frobenius norm if       `tensor` be a matrix and equivalent to 2-norm for vectors        some restrictions apply          a  the frobenius norm ` fro ` be not define for vectors          b  if axis be a 2-tuple  matrix norm   only ` euclidean `   `fro `  `1`             `2`  `np inf` be support        see the description of `axis` on how to compute norms for a batch of       vectors or matrices store in a tensor      axis  if `axis` be `none`  the default   the input be consider a vector       and a single vector norm be compute over the entire set of value in the       tensor  i e  `norm tensor  ord=ord ` be equivalent to       `norm reshape tensor   -1    ord=ord `        if `axis` be a python integer  the input be consider a batch of vectors        and `axis` determine the axis in `tensor` over which to compute vector       norms        if `axis` be a 2-tuple of python integers it be consider a batch of       matrices and `axis` determine the ax in `tensor` over which to compute       a matrix norm        negative indices be support  example  if you be pass a tensor that       can be either a matrix or a batch of matrices at runtime  pass       `axis= -2 -1 ` instead of `axis=none` to make sure that matrix norms be       compute      keepdims  if true  the axis indicate in `axis` be keep with size 1        otherwise  the dimension in `axis` be remove from the output shape      name  the name of the op     return      output  a `tensor` of the same type as tensor  contain the vector or       matrix norms  if `keepdims` be true then the rank of output be equal to       the rank of `tensor`  otherwise  if `axis` be none the output be a scalar        if `axis` be an integer  the rank of `output` be one less than the rank       of `tensor`  if `axis` be a 2-tuple the rank of `output` be two less       than the rank of `tensor`     raise      valueerror  if `ord` or `axis` be invalid      compatibility numpy    mostly equivalent to numpy linalg norm    not support  ord <= 0  2-norm for matrices  nuclear norm    other differences      a  if axis be `none`  treat the flatten `tensor` as a vector      regardless of rank      b  explicitly support  euclidean  norm as the default  include for      higher order tensors     end compatibility         return norm tensor=tensor                ord=ord                axis=axis                keepdims=keepdims                name=name  
 tf export string  def numpy function func  inp  tout  name=none     string   return py func common func  inp  tout  stateful=true  name=name  
 tf export string   dispatch add dispatch support def one hot indices              depth              on value=none              off value=none              axis=none              dtype=none              name=none     string   with ops name scope        name  string         indices  depth  on value  off value  axis  dtype   as name      on exist = on value be not none     off exist = off value be not none      on dtype =           ops convert to tensor on value  dtype base dtype if on exist else none      off dtype =           ops convert to tensor off value  dtype base dtype         if off exist else none       if on exist or off exist        if dtype be not none                   if on exist and on dtype  = dtype            raise typeerror string                           string format on dtype  dtype           if off exist and off dtype  = dtype            raise typeerror string                           string format off dtype  dtype         else                   dtype = on dtype if on exist else off dtype     elif dtype be none               dtype = dtypes float32      if not on exist               on value = ops convert to tensor 1  dtype  name=string        on dtype = dtype     if not off exist               off value = ops convert to tensor 0  dtype  name=string        off dtype = dtype      if on dtype  = off dtype        raise typeerror string                       string format on dtype  off dtype        return gen array ops one hot indices  depth  on value  off value  axis                                   name  
 tf export string  def ones shape  dtype=dtypes float32  name=none     string   dtype = dtypes as dtype dtype  base dtype   with ops name scope name  string   shape   as name      one = true if dtype == dtypes bool else 1     if not isinstance shape  ops tensor         try                            output =  constant if small one  shape  dtype  name          if output be not none            return output                   shape = constant op  tensor shape tensor conversion function              tensor shape tensorshape shape         except  typeerror  valueerror                    shape = ops convert to tensor shape  dtype=dtypes int32      if not shape  shape tuple          shape = reshape shape   -1         output = fill shape  constant one  dtype=dtype   name=name    assert output dtype base dtype == dtype   return output 
class ones initializer     string    def   call   self  shape  dtype=dtypes float32       string     dtype = dtypes as dtype dtype      if not dtype be numpy compatible or dtype == dtypes string        raise valueerror string   dtype      return array ops ones shape  dtype  
 tf export string  v1=     dispatch add dispatch support def ones like v2      input        dtype=none      name=none     string   return ones like impl input  dtype  name  optimize=true  
 tf export string  v1=    def pad v2 tensor  paddings  mode=string  constant values=0  name=none     string   return pad tensor  paddings  mode  name  constant value  
 tf export string  def parallel stack value  name=string     string   with ops name scope name       value t = ops convert to tensor value 0       value shape = ops convert to tensor value t  get shape        output shape = tensor shape tensorshape  len value        output shape = output shape concatenate value shape           return gen array ops parallel concat           expand dim value  0  for value in value   shape=output shape  
 tf export string  def print v2  input    kwargs     string               output stream = kwargs pop string  sys stderr    name = kwargs pop string  none    summarize = kwargs pop string  3    sep = kwargs pop string  string    end = kwargs pop string  os linesep    if kwargs      raise valueerror string   kwargs    format name = none   if name      format name = name   string          output stream to constant =         sys stdout  string        sys stderr  string        tf log info  string        tf log info  string        tf log warn  string        tf log warn  string        tf log warn  string        tf log error  string        tf log error  string        log info  string        log info  string        log info  string        log warn  string        log warn  string        log warn  string        log warn  string        log error  string        log error  string         if  be filepath output stream       output stream string = output stream   else      output stream string = output stream to constant get output stream      if not output stream string        raise valueerror string                          str output stream                           string                        string                        string                          string        if  len input  == 1 and tensor util be tensor input 0   and        not isinstance input 0   sparse tensor sparsetensor   and        input 0  shape ndims == 0  and  input 0  dtype == dtypes string        format string = input 0          else                     templates =        tensors =        tensor free structure = nest map structure          lambda x  string if tensor util be tensor x  else x  input      tensor free template = string join          pprint pformat x  for x in tensor free structure      placeholder =  generate placeholder string tensor free template       for input  in input        placeholders =                                      for x in nest flatten input                     if isinstance x  sparse tensor sparsetensor             tensors extend  x indices  x value  x dense shape             placeholders append                string format                    placeholder  placeholder  placeholder           elif tensor util be tensor x             tensors append x            placeholders append placeholder          else            placeholders append x         if isinstance input   six string type                             cur template = input        else                                                                                                    cur template = pprint pformat              nest pack sequence as input   placeholders         templates append cur template                                template = sep join templates      template = template replace string   placeholder   string  placeholder      format string = string ops string format          inputs=tensors          template=template          placeholder=placeholder          summarize=summarize          name=format name     return gen log ops print v2        format string  output stream=output stream string  name=name  end=end  
 tf export string  def eager py func func  inp  tout  name=none     string   return  internal py func func=func  inp=inp  tout=tout  eager=true  name=name  
class randomnormal initializer     string    def   init   self  mean=0 0  stddev=0 05  seed=none       self mean = mean     self stddev = stddev     self seed = seed     self  random generator =  randomgenerator seed     def   call   self  shape  dtype=dtypes float32       string     dtype =  assert float dtype dtype      return self  random generator random normal shape  self mean  self stddev                                                  dtype     def get config self       return           string  self mean          string  self stddev          string  self seed       
class randomuniform initializer     string    def   init   self  minval=-0 05  maxval=0 05  seed=none       self minval = minval     self maxval = maxval     self seed = seed     self  random generator =  randomgenerator seed     def   call   self  shape  dtype=dtypes float32       string     dtype = dtypes as dtype dtype      if not dtype be float and not dtype be integer        raise valueerror string   dtype      return self  random generator random uniform shape  self minval                                                   self maxval  dtype     def get config self       return           string  self minval          string  self maxval          string  self seed       
 tf export string  def range start  limit=none  delta=1  dtype=none  name=string       string   if limit be none      start  limit = 0  start    with ops name scope name  string   start  limit  delta   as name      start = ops convert to tensor start  dtype=dtype  name=string      limit = ops convert to tensor limit  dtype=dtype  name=string      delta = ops convert to tensor delta  dtype=dtype  name=string            if dtype be none        dtype hierarchy =             dtypes int32  dtypes int64  dtypes float32  dtypes float64               assert all arg dtype in dtype hierarchy for arg in  start  limit  delta         infer dtype = max  arg dtype for arg in  start  limit  delta                               key=dtype hierarchy index         start = cast start  infer dtype        limit = cast limit  infer dtype        delta = cast delta  infer dtype       return gen math ops  range start  limit  delta  name=name  
 tf export string   dispatch add dispatch support def rank input  name=none        string   return rank internal input  name  optimize=true  
  dispatch add dispatch list  tf export string  def real div x  y  name=none     r   return x / y element-wise for real type     if `x` and `y` be reals  this will return the floating-point division      note   `div` support broadcast  more about broadcast    here  http //docs scipy org/doc/numpy/user/basics broadcast html     args      x  a `tensor`  must be one of the follow type  `bfloat16`  `half`  `float32`  `float64`  `uint8`  `int8`  `uint16`  `int16`  `int32`  `int64`  `complex64`  `complex128`      y  a `tensor`  must have the same type as `x`      name  a name for the operation  optional      return      a `tensor`  have the same type as `x`           ctx =  context  context or  context context     tld =  ctx  thread local data   if tld be eager      try         result =  pywrap tensorflow tfe py fastpathexecute           ctx  context handle  tld device name  string  name          tld op callbacks  x  y        return  result     except  core  fallbackexception        try          return real div eager fallback              x  y  name=name  ctx= ctx        except  core  symbolicexception          pass         except  typeerror  valueerror           result =  dispatch dispatch                real div  x=x  y=y  name=name          if result be not  dispatch opdispatcher not support            return result         raise     except  core  notokstatusexception as e         ops raise from not ok status e  name       try             op   output =  op def library  apply op helper          string  x=x  y=y  name=name    except  typeerror  valueerror       result =  dispatch dispatch            real div  x=x  y=y  name=name      if result be not  dispatch opdispatcher not support        return result     raise    result =  output      if  execute must record gradient         attrs =  string   op  get attr type string        input flat =  op input      execute record gradient          string   input flat   attrs   result     result  =  result   return  result 
 tf export string  def recompute grad f     string        custom gradient   def inner  args    kwargs       string     result = f  args    kwargs       def grad dresult  variables=none         string       with backprop gradienttape   as t          t watch args          if variables be not none            t watch variables          with ops control dependencies  dresult              result = f  args    kwargs        kw vars =          if variables be not none          kw vars = list variables        grads = t gradient            result  list args    kw vars  output gradients= dresult         return grads  len args    grads len args         return result  grad    return inner 
 tf export string  def register tensor conversion function base type                                          conversion func                                          priority=100     string   base type = base type if isinstance base type  tuple  else  base type     if any not isinstance x  type  for x in base type       raise typeerror string    if any issubclass x   unconvertible type  for x in base type       raise typeerror string                     string    del base type     if not callable conversion func       raise typeerror string     with  tensor conversion func lock       tensor conversion func registry priority  append           base type  conversion func        tensor conversion func cache clear   
 tf export string  def repeat input  repeat  axis=none  name=none       string   if axis be none      input = reshape input   -1       axis = 0   return repeat with axis input  repeat  axis  name  
 tf export string  def require space to batch paddings input shape                                       block shape                                       base paddings=none                                       name=none     string   with ops name scope name  string                         input shape  block shape        input shape = ops convert to tensor          input shape  dtype=dtypes int32  name=string      block shape = ops convert to tensor          block shape  dtype=dtypes int32  name=string       block shape get shape   assert be fully define       block shape get shape   assert have rank 1      num block dim = block shape get shape   dim 0  value     if num block dim == 0        return zero  0  2   dtypes int32   zero  0  2   dtypes int32       input shape get shape   assert be compatible with  num block dim        if base paddings be not none        base paddings = ops convert to tensor            base paddings  dtype=dtypes int32  name=string        base paddings get shape   assert be compatible with  num block dim  2       else        base paddings = zero  num block dim  2   dtypes int32       const block shape = tensor util constant value block shape      const input shape = tensor util constant value input shape      const base paddings = tensor util constant value base paddings      if  const block shape be not none and const input shape be not none and         const base paddings be not none         block shape = const block shape       input shape = const input shape       base paddings = const base paddings           pad start = base paddings    0      orig pad end = base paddings    1      full input shape = input shape   pad start   orig pad end     pad end extra =  block shape - full input shape   block shape    block shape     pad end = orig pad end   pad end extra      result paddings = stack            pad start i   pad end i   for i in range num block dim            name=string      result crop = stack   0  pad end extra i   for i in range num block dim                             name=string      return result paddings  result crop 
 tf export string  v1= string  string   def reshape tensor  shape  name=none       r   reshape a tensor     give `tensor`  this operation return a new `tf tensor` that have the same   value as `tensor` in the same order  except with a new shape give by   `shape`     >>> t1 =   1  2  3                4  5  6     >>> print tf shape t1  numpy       2 3    >>> t2 = tf reshape t1   6     >>> t2   <tf tensor  shape= 6    dtype=int32      numpy=array  1  2  3  4  5  6   dtype=int32 >   >>> tf reshape t2   3  2     <tf tensor  shape= 3  2   dtype=int32  numpy=     array   1  2               3  4               5  6    dtype=int32 >    the `tf reshape` do not change the order of or the total number of elements   in the tensor  and so it can reuse the underlie data buffer  this make it   a fast operation independent of how big of a tensor it be operate on     >>> tf reshape  1  2  3    2  2     traceback  most recent call last           invalidargumenterror  input to reshape be a tensor with 3 value  but the   request shape have 4    to instead reorder the data to rearrange the dimension of a tensor  see   `tf transpose`     >>> t =   1  2  3               4  5  6     >>> tf reshape t   3  2   numpy     array   1  2             3  4             5  6    dtype=int32    >>> tf transpose t  perm= 1  0   numpy     array   1  4             2  5             3  6    dtype=int32     if one component of `shape` be the special value -1  the size of that   dimension be compute so that the total size remain constant   in particular    a `shape` of ` -1 ` flatten into 1-d   at most one component of `shape` can   be -1     >>> t =   1  2  3               4  5  6     >>> tf reshape t   -1     <tf tensor  shape= 6    dtype=int32      numpy=array  1  2  3  4  5  6   dtype=int32 >   >>> tf reshape t   3  -1     <tf tensor  shape= 3  2   dtype=int32  numpy=     array   1  2               3  4               5  6    dtype=int32 >   >>> tf reshape t   -1  2     <tf tensor  shape= 3  2   dtype=int32  numpy=     array   1  2               3  4               5  6    dtype=int32 >    `tf reshape t     ` reshape a tensor `t` with one element to a scalar     >>> tf reshape  7       numpy     7    more examples     >>> t =  1  2  3  4  5  6  7  8  9    >>> print tf shape t  numpy       9    >>> tf reshape t   3  3     <tf tensor  shape= 3  3   dtype=int32  numpy=     array   1  2  3               4  5  6               7  8  9    dtype=int32 >    >>> t =    1  1    2  2                 3  3    4  4      >>> print tf shape t  numpy       2 2 2    >>> tf reshape t   2  4     <tf tensor  shape= 2  4   dtype=int32  numpy=     array   1  1  2  2               3  3  4  4    dtype=int32 >    >>> t =    1  1  1                2  2  2                 3  3  3                4  4  4                 5  5  5                6  6  6      >>> print tf shape t  numpy       3 2 3    >>>   pass   -1   to flatten  t     >>> tf reshape t   -1     <tf tensor  shape= 18    dtype=int32      numpy=array  1  1  1  2  2  2  3  3  3  4  4  4  5  5  5  6  6  6       dtype=int32 >   >>>   -- use -1 to infer the shape --   >>>   here -1 be infer to be 9    >>> tf reshape t   2  -1     <tf tensor  shape= 2  9   dtype=int32  numpy=     array   1  1  1  2  2  2  3  3  3               4  4  4  5  5  5  6  6  6    dtype=int32 >   >>>   -1 be infer to be 2    >>> tf reshape t   -1  9     <tf tensor  shape= 2  9   dtype=int32  numpy=     array   1  1  1  2  2  2  3  3  3               4  4  4  5  5  5  6  6  6    dtype=int32 >   >>>   -1 be infer to be 3    >>> tf reshape t    2  -1  3     <tf tensor  shape= 2  3  3   dtype=int32  numpy=     array    1  1  1                2  2  2                3  3  3                 4  4  4                5  5  5                6  6  6     dtype=int32 >    args      tensor  a `tensor`      shape  a `tensor`  must be one of the follow type  `int32`  `int64`        define the shape of the output tensor      name  optional string  a name for the operation     return      a `tensor`  have the same type as `tensor`          result = gen array ops reshape tensor  shape  name    tensor util maybe set static shape result  shape    return result 
  dispatch add dispatch list  tf export string  v1= string  string  string    deprecate endpoints string  string  def reverse v2 tensor  axis  name=none     r   reverse specific dimension of a tensor     note `tf reverse` have now change behavior in preparation for 1 0    `tf reverse v2` be currently an alias that will be deprecate before tf 1 0     give a `tensor`  and a `int32` tensor `axis` represent the set of   dimension of `tensor` to reverse  this operation reverse each dimension   `i` for which there exist `j` s t  `axis j  == i`     `tensor` can have up to 8 dimension  the number of dimension specify   in `axis` may be 0 or more entries  if an index be specify more than   once  a invalidargument error be raise     for example     ```     tensor  t  be      0   1   2   3                          4   5   6   7                          8   9  10  11                          12  13  14  15                         16  17  18  19                         20  21  22  23         tensor  t  shape be  1  2  3  4        dim  be  3  or  dim  be  -1    reverse t  dim  ==>      3   2   1   0                               7   6   5   4                               11  10  9  8                               15  14  13  12                              19  18  17  16                              23  22  21  20           dim  be   1    or  dim  be   -3      reverse t  dim  ==>     12  13  14  15                              16  17  18  19                              20  21  22  23                              0   1   2   3                               4   5   6   7                               8   9  10  11           dim  be   2    or  dim  be   -2      reverse t  dim  ==>     8  9  10  11                              4  5  6  7                              0  1  2  3                              20  21  22  23                              16  17  18  19                              12  13  14  15       ```    args      tensor  a `tensor`  must be one of the follow type  `uint8`  `int8`  `uint16`  `int16`  `int32`  `int64`  `bool`  `bfloat16`  `half`  `float32`  `float64`  `complex64`  `complex128`  `string`        up to 8-d      axis  a `tensor`  must be one of the follow type  `int32`  `int64`        1-d  the indices of the dimension to reverse  must be in the range       ` -rank tensor   rank tensor  `      name  a name for the operation  optional      return      a `tensor`  have the same type as `tensor`           ctx =  context  context or  context context     tld =  ctx  thread local data   if tld be eager      try         result =  pywrap tensorflow tfe py fastpathexecute           ctx  context handle  tld device name  string  name          tld op callbacks  tensor  axis        return  result     except  core  fallbackexception        try          return reverse v2 eager fallback              tensor  axis  name=name  ctx= ctx        except  core  symbolicexception          pass         except  typeerror  valueerror           result =  dispatch dispatch                reverse v2  tensor=tensor  axis=axis  name=name          if result be not  dispatch opdispatcher not support            return result         raise     except  core  notokstatusexception as e         ops raise from not ok status e  name       try             op   output =  op def library  apply op helper          string  tensor=tensor  axis=axis  name=name    except  typeerror  valueerror       result =  dispatch dispatch            reverse v2  tensor=tensor  axis=axis  name=name      if result be not  dispatch opdispatcher not support        return result     raise    result =  output      if  execute must record gradient         attrs =  string   op  get attr type string   string                 op  get attr type string        input flat =  op input      execute record gradient          string   input flat   attrs   result     result  =  result   return  result 
 tf export string  v1=    def reverse sequence v2 input                          seq lengths                          seq axis=none                          batch axis=none                          name=none     return gen array ops reverse sequence        input=input        seq lengths=seq lengths        seq dim=seq axis        batch dim=batch axis        name=name  
 tf export string  v1= string  string    deprecation deprecate endpoints string  def roll input  shift  axis  name=none       return  gen manip ops roll input  shift  axis  name  
 tf export string  def scan fn           elems           initializer=none           parallel iterations=10           back prop=true           swap memory=false           infer shape=true           reverse=false           name=none     string   if not callable fn       raise typeerror string     input be sequence = nest be sequence elems    input flatten = lambda x  nest flatten x  if input be sequence else  x     def input pack x       return nest pack sequence as elems  x  if input be sequence else x 0     if initializer be none      output be sequence = input be sequence     output flatten = input flatten     output pack = input pack   else      output be sequence = nest be sequence initializer      output flatten = lambda x  nest flatten x  if output be sequence else  x       def output pack x         return  nest pack sequence as initializer  x                if output be sequence else x 0      elems flat = input flatten elems     in graph mode = not context execute eagerly     with ops name scope name  string  elems flat                 if in graph mode                      varscope = vs get variable scope         varscope cache device be none = false       if varscope cache device be none                            varscope set cache device lambda op  op device          varscope cache device be none = true           elems flat =           ops convert to tensor elem  name=string  for elem in elems flat                 n = tensor shape dimension value elems flat 0  shape 0       if n be none        n = array ops shape elems flat 0   0            elems ta =           tensor array ops tensorarray              dtype=elem dtype              size=n              dynamic size=false              element shape=elem shape 1                infer shape=true  for elem in elems flat                elems ta =           elem ta unstack elem  for elem ta  elem in zip elems ta  elems flat             if initializer be none        a flat =  elem read n - 1 if reverse else 0  for elem in elems ta        i = 1     else        initializer flat = output flatten initializer        a flat =  ops convert to tensor init  for init in initializer flat        i = 0           accs ta =           tensor array ops tensorarray              dtype=init dtype              size=n              element shape=init shape if infer shape else none              dynamic size=false              infer shape=infer shape  for init in a flat            if initializer be none        accs ta =             acc ta write n - 1 if reverse else 0  a            for  acc ta  a  in zip accs ta  a flat               def compute i  a flat  tas         string       pack elems = input pack  elem ta read i  for elem ta in elems ta         pack a = output pack a flat        a out = fn pack a  pack elems        nest assert same structure elems if initializer be none else initializer                                   a out        flat a out = output flatten a out        tas =  ta write i  value  for  ta  value  in zip tas  flat a out         if reverse          next i = i - 1       else          next i = i   1       return  next i  flat a out  tas       if reverse        initial i = n - 1 - i       condition = lambda i   1   2  i >= 0     else        initial i = i       condition = lambda i   1   2  i < n           r a = control flow ops while loop          condition          compute   initial i  a flat  accs ta           parallel iterations=parallel iterations          back prop=back prop          swap memory=swap memory          maximum iterations=n       result flat =  r stack   for r in r a       n static = tensor shape dimension          tensor shape dimension value              elems flat 0  get shape   with rank at least 1  0        for elem in elems flat 1          n static merge with            tensor shape dimension                tensor shape dimension value                    elem get shape   with rank at least 1  0         for r in result flat        r set shape            tensor shape tensorshape n static  concatenate r get shape   1                    if in graph mode and varscope cache device be none        varscope set cache device none       return output pack result flat  
  dispatch add dispatch list  tf export string  v1= string  string    deprecate endpoints string  def scatter nd indices  update  shape  name=none     r   scatter `updates` into a new tensor accord to `indices`     create a new tensor by apply sparse `updates` to individual value or   slice within a tensor  initially zero for numeric  empty for string  of   the give `shape` accord to indices   this operator be the inverse of the   `tf gather nd` operator which extract value or slice from a give tensor     this operation be similar to tensor scatter add  except that the tensor be   zero-initialized  call `tf scatter nd indices  value  shape ` be identical   to `tensor scatter add tf zero shape  value dtype   indices  value `    if `indices` contain duplicate  then their update be accumulate  sum        warn    the order in which update be apply be nondeterministic  so the   output will be nondeterministic if `indices` contain duplicate -- because   of some numerical approximation issue  number sum in different order   may yield different result     `indices` be an integer tensor contain indices into a new tensor of shape   `shape`   the last dimension of `indices` can be at most the rank of `shape`         indices shape -1  <= shape rank    the last dimension of `indices` correspond to indices into elements    if `indices shape -1  = shape rank`  or slice    if `indices shape -1  < shape rank`  along dimension `indices shape -1 ` of   `shape`   `updates` be a tensor with shape        indices shape  -1    shape indices shape -1       the simplest form of scatter be to insert individual elements in a tensor by   index  for example  say we want to insert 4 scatter elements in a rank-1   tensor with 8 elements     <div style= width 70   margin auto  margin-bottom 10px  margin-top 20px  >   <img style= width 100   src= https //www tensorflow org/images/scatternd1 png  alt>   </div>    in python  this scatter operation would look like this     ```python       indices = tf constant   4    3    1    7          update = tf constant  9  10  11  12         shape = tf constant  8         scatter = tf scatter nd indices  update  shape        print scatter    ```    the result tensor would look like this          0  11  0  10  9  0  0  12     we can also  insert entire slice of a higher rank tensor all at once  for   example  if we want to insert two slice in the first dimension of a   rank-3 tensor with two matrices of new value     <div style= width 70   margin auto  margin-bottom 10px  margin-top 20px  >   <img style= width 100   src= https //www tensorflow org/images/scatternd2 png  alt>   </div>    in python  this scatter operation would look like this     ```python       indices = tf constant   0    2          update = tf constant    5  5  5  5    6  6  6  6                                  7  7  7  7    8  8  8  8                                   5  5  5  5    6  6  6  6                                  7  7  7  7    8  8  8  8           shape = tf constant  4  4  4         scatter = tf scatter nd indices  update  shape        print scatter    ```    the result tensor would look like this            5  5  5  5    6  6  6  6    7  7  7  7    8  8  8  8             0  0  0  0    0  0  0  0    0  0  0  0    0  0  0  0             5  5  5  5    6  6  6  6    7  7  7  7    8  8  8  8             0  0  0  0    0  0  0  0    0  0  0  0    0  0  0  0       note that on cpu  if an out of bind index be find  an error be return    on gpu  if an out of bind index be find  the index be ignore     args      indices  a `tensor`  must be one of the follow type  `int32`  `int64`        index tensor      update  a `tensor`  update to scatter into output      shape  a `tensor`  must have the same type as `indices`        1-d  the shape of the result tensor      name  a name for the operation  optional      return      a `tensor`  have the same type as `updates`           ctx =  context  context or  context context     tld =  ctx  thread local data   if tld be eager      try         result =  pywrap tensorflow tfe py fastpathexecute           ctx  context handle  tld device name  string  name          tld op callbacks  indices  update  shape        return  result     except  core  fallbackexception        try          return scatter nd eager fallback              indices  update  shape  name=name  ctx= ctx        except  core  symbolicexception          pass         except  typeerror  valueerror           result =  dispatch dispatch                scatter nd  indices=indices  updates=updates  shape=shape                            name=name          if result be not  dispatch opdispatcher not support            return result         raise     except  core  notokstatusexception as e         ops raise from not ok status e  name       try             op   output =  op def library  apply op helper          string  indices=indices  updates=updates  shape=shape  name=name    except  typeerror  valueerror       result =  dispatch dispatch            scatter nd  indices=indices  updates=updates  shape=shape                        name=name      if result be not  dispatch opdispatcher not support        return result     raise    result =  output      if  execute must record gradient         attrs =  string   op  get attr type string   string                 op  get attr type string        input flat =  op input      execute record gradient          string   input flat   attrs   result     result  =  result   return  result 
 tf export string  def searchsorted sort sequence                   value                   side=string                   out type=dtypes int32                   name=none     string   sequence size = shape internal sort sequence  -1    value size = shape internal value  -1    sort sequence 2d = reshape sort sequence   -1  sequence size     value 2d = reshape value   -1  value size     if side == string      output = gen array ops upper bind sort sequence 2d  value 2d  out type                                         name    elif side == string      output = gen array ops lower bind sort sequence 2d  value 2d  out type                                         name    else      raise valueerror string   side    return reshape output  shape internal value   
 tf export string  def sequence mask lengths  maxlen=none  dtype=dtypes bool  name=none     string   with ops name scope name  string   lengths  maxlen        lengths = ops convert to tensor lengths       if maxlen be none        maxlen = gen math ops  max lengths   all dimension lengths         maxlen = gen math ops maximum constant 0  maxlen dtype   maxlen      else        maxlen = ops convert to tensor maxlen      if maxlen get shape   ndims be not none and maxlen get shape   ndims  = 0        raise valueerror string                                row vector = gen math ops  range          constant 0  maxlen dtype   maxlen  constant 1  maxlen dtype                 matrix = gen math ops cast expand dim lengths  -1   maxlen dtype      result = row vector < matrix      if dtype be none or result dtype base dtype == dtype base dtype        return result     else        return gen math ops cast result  dtype  
 tf export string  v1=    def shape v2 input  out type=dtypes int32  name=none        string   return shape input  name  out type  
 tf export string  def shape n input  out type=dtypes int32  name=none        string    return gen array ops shape n input  out type=out type  name=name  
 tf export string  v1=     dispatch add dispatch support def size v2 input  out type=dtypes int32  name=none        return size input  name  out type  
 tf export string  def slice input   begin  size  name=none        string   return gen array ops  slice input   begin  size  name=name  
 tf export string  def sort value  axis=-1  direction=string  name=none     string   with framework ops name scope name  string       return  sort or argsort value  axis  direction  return argsort=false  
 tf export string  string  v1=    def space to batch v2 input  block shape  paddings  name=none       return space to batch nd input  block shape  paddings  name  
  dispatch add dispatch list  tf export string  v1= string  string    deprecate endpoints string  def space to batch nd input  block shape  paddings  name=none     r   spacetobatch for n-d tensors of type t     this operation divide  spatial  dimension ` 1       m ` of the input into a   grid of block of shape `block shape`  and interleave these block with the    batch  dimension  0  such that in the output  the spatial dimension   ` 1       m ` correspond to the position within the grid  and the batch   dimension combine both the position within a spatial block and the original   batch position   prior to division into block  the spatial dimension of the   input be optionally zero pad accord to `paddings`   see below for a   precise description     args      input  a `tensor`        n-d with shape `input shape =  batch    spatial shape   remain shape`        where spatial shape have `m` dimension      block shape  a `tensor`  must be one of the follow type  `int32`  `int64`        1-d with shape ` m `  all value must be >= 1      paddings  a `tensor`  must be one of the follow type  `int32`  `int64`        2-d with shape ` m  2 `  all value must be >= 0          `paddings i  =  pad start  pad end ` specify the pad for input dimension         `i   1`  which correspond to spatial dimension `i`   it be require that         `block shape i ` divide `input shape i   1    pad start   pad end`         this operation be equivalent to the follow step         1  zero-pad the start and end of dimension ` 1       m ` of the          input accord to `paddings` to produce `padded` of shape `padded shape`         2  reshape `padded` to `reshaped padded` of shape               batch                pad shape 1  / block shape 0                block shape 0                                pad shape m  / block shape m-1               block shape m-1                remain shape        3  permute dimension of `reshaped padded` to produce          `permuted reshape padded` of shape              block shape               batch                pad shape 1  / block shape 0                                pad shape m  / block shape m-1                remain shape        4  reshape `permuted reshape padded` to flatten `block shape` into the batch          dimension  produce an output tensor of shape               batch   prod block shape                 pad shape 1  / block shape 0                                pad shape m  / block shape m-1                remain shape        some examples          1  for the follow input of shape ` 1  2  2  1 `  `block shape =  2  2 `  and           `paddings =   0  0    0  0  `         ```       x =     1    2      3    4           ```        the output tensor have shape ` 4  1  1  1 ` and value         ```           1        2        3        4           ```         2  for the follow input of shape ` 1  2  2  3 `  `block shape =  2  2 `  and           `paddings =   0  0    0  0  `         ```       x =     1  2  3    4  5  6                  7  8  9    10  11  12           ```        the output tensor have shape ` 4  1  1  3 ` and value         ```           1  2  3        4  5  6        7  8  9        10  11  12           ```         3  for the follow input of shape ` 1  4  4  1 `  `block shape =  2  2 `  and           `paddings =   0  0    0  0  `         ```       x =     1      2     3     4                  5      6     7     8                  9     10    11     12                  13    14    15     16           ```        the output tensor have shape ` 4  2  2  1 ` and value         ```       x =     1    3      9    11                   2    4      10    12                   5    7      13    15                   6    8      14    16           ```         4  for the follow input of shape ` 2  2  4  1 `  block shape = ` 2  2 `  and           paddings = `  0  0    2  0  `         ```       x =     1      2     3     4                  5      6     7     8                   9     10    11     12                  13    14    15     16           ```        the output tensor have shape ` 8  1  3  1 ` and value         ```       x =     0    1    3        0    9    11                   0    2    4        0    10    12                   0    5    7        0    13    15                   0    6    8        0    14    16           ```        among others  this operation be useful for reduce atrous convolution into       regular convolution      name  a name for the operation  optional      return      a `tensor`  have the same type as `input`           ctx =  context  context or  context context     tld =  ctx  thread local data   if tld be eager      try         result =  pywrap tensorflow tfe py fastpathexecute           ctx  context handle  tld device name  string  name          tld op callbacks  input  block shape  paddings        return  result     except  core  fallbackexception        try          return space to batch nd eager fallback              input  block shape  paddings  name=name  ctx= ctx        except  core  symbolicexception          pass         except  typeerror  valueerror           result =  dispatch dispatch                space to batch nd  input=input  block shape=block shape                                   paddings=paddings  name=name          if result be not  dispatch opdispatcher not support            return result         raise     except  core  notokstatusexception as e         ops raise from not ok status e  name       try             op   output =  op def library  apply op helper          string  input=input  block shape=block shape                            paddings=paddings  name=name    except  typeerror  valueerror       result =  dispatch dispatch            space to batch nd  input=input  block shape=block shape                               paddings=paddings  name=name      if result be not  dispatch opdispatcher not support        return result     raise    result =  output      if  execute must record gradient         attrs =  string   op  get attr type string   string                 op  get attr type string   string                 op  get attr type string        input flat =  op input      execute record gradient          string   input flat   attrs   result     result  =  result   return  result 
 tf export string  def split value  num or size split  axis=0  num=none  name=string     string   size split = ops convert to tensor num or size split    if isinstance num or size split                  six integer type    tensor shape dimension         return gen array ops split          axis=axis  num split=num or size split  value=value  name=name     if size split  rank   == 0      raise valueerror          string         string    num or size split       if num be none      size split shape = size split  shape tuple       if size split shape        num = size split shape 0      if num be none        raise valueerror string   num or size split     return gen array ops split v        value=value  size splits=size split  axis=axis  num split=num  name=name  
 tf export string  v1=     dispatch add dispatch support def squeeze v2 input  axis=none  name=none     string      return squeeze input  axis  name  
 tf export string   dispatch add dispatch support def stack value  axis=0  name=string     string   if axis == 0      try               return ops convert to tensor value  name=name      except  typeerror  valueerror         pass      value shape = ops convert to tensor value 0   name=name   shape tuple       if value shape be not none      expand num dim = len value shape    1     if axis < -expanded num dim or axis >= expand num dim        raise valueerror string                           axis  -expanded num dim  expand num dim      return gen array ops pack value  axis=axis  name=name  
  dispatch add dispatch list  tf export string  def stop gradient input  name=none     r   stop gradient computation     when execute in a graph  this op output its input tensor as-is     when build ops to compute gradients  this op prevent the contribution of   its input to be take into account   normally  the gradient generator add ops   to a graph to compute the derivatives of a specify  loss  by recursively   find out input that contribute to its computation   if you insert this op   in the graph it input be mask from the gradient generator   they be not   take into account for compute gradients     this be useful any time you want to compute a value with tensorflow but need   to pretend that the value be a constant  some examples include        the  em  algorithm where the  m-step  should not involve backpropagation      through the output of the  e-step        contrastive divergence train of boltzmann machine where  when      differentiate the energy function  the train must not backpropagate      through the graph that generate the sample from the model       adversarial train  where no backprop should happen through the adversarial      example generation process     args      input  a `tensor`      name  a name for the operation  optional      return      a `tensor`  have the same type as `input`           ctx =  context  context or  context context     tld =  ctx  thread local data   if tld be eager      try         result =  pywrap tensorflow tfe py fastpathexecute           ctx  context handle  tld device name  string  name          tld op callbacks  input        return  result     except  core  fallbackexception        try          return stop gradient eager fallback              input  name=name  ctx= ctx        except  core  symbolicexception          pass         except  typeerror  valueerror           result =  dispatch dispatch                stop gradient  input=input  name=name          if result be not  dispatch opdispatcher not support            return result         raise     except  core  notokstatusexception as e         ops raise from not ok status e  name       try             op   output =  op def library  apply op helper          string  input=input  name=name    except  typeerror  valueerror       result =  dispatch dispatch            stop gradient  input=input  name=name      if result be not  dispatch opdispatcher not support        return result     raise    result =  output      if  execute must record gradient         attrs =  string   op  get attr type string        input flat =  op input      execute record gradient          string   input flat   attrs   result     result  =  result   return  result 
 tf export string  def stride slice input                     begin                    end                    strides=none                    begin mask=0                    end mask=0                    ellipsis mask=0                    new axis mask=0                    shrink axis mask=0                    var=none                    name=none     string    if stride be none      stride = ones like begin     op = gen array ops stride slice        input=input         begin=begin        end=end        strides=strides        name=name        begin mask=begin mask        end mask=end mask        ellipsis mask=ellipsis mask        new axis mask=new axis mask        shrink axis mask=shrink axis mask     parent name = name    if not  var be none and isinstance op  ops eagertensor         def assign val  name=none         string        if var be none          raise valueerror string        else          if name be none            name = parent name   string          return var  stride slice assign              begin=begin              end=end              strides=strides              value=val              name=name              begin mask=begin mask              end mask=end mask              ellipsis mask=ellipsis mask              new axis mask=new axis mask              shrink axis mask=shrink axis mask       op assign = assign   return op 
 tf export string  def switch case branch index                  branch fns                  default=none                  name=string     string   return  index case helper branch fns  default  branch index  name  
  dispatch add dispatch list  tf export string  v1= string  string    deprecate endpoints string  def tensor scatter add tensor  indices  update  name=none     r   add sparse `updates` to an exist tensor accord to `indices`     this operation create a new tensor by add sparse `updates` to the pass   in `tensor`    this operation be very similar to `tf scatter nd add`  except that the update   be add onto an exist tensor  as oppose to a variable   if the memory   for the exist tensor cannot be re-used  a copy be make and update     `indices` be an integer tensor contain indices into a new tensor of shape   `shape`   the last dimension of `indices` can be at most the rank of `shape`         indices shape -1  <= shape rank    the last dimension of `indices` correspond to indices into elements    if `indices shape -1  = shape rank`  or slice    if `indices shape -1  < shape rank`  along dimension `indices shape -1 ` of   `shape`   `updates` be a tensor with shape        indices shape  -1    shape indices shape -1       the simplest form of tensor scatter add be to add individual elements to a   tensor by index  for example  say we want to add 4 elements in a rank-1   tensor with 8 elements     in python  this scatter add operation would look like this     ```python       indices = tf constant   4    3    1    7          update = tf constant  9  10  11  12         tensor = tf ones  8   dtype=tf int32        update = tf tensor scatter nd add tensor  indices  update        print update    ```    the result tensor would look like this          1  12  1  11  10  1  1  13     we can also  insert entire slice of a higher rank tensor all at once  for   example  if we want to insert two slice in the first dimension of a   rank-3 tensor with two matrices of new value     in python  this scatter add operation would look like this     ```python       indices = tf constant   0    2          update = tf constant    5  5  5  5    6  6  6  6                                  7  7  7  7    8  8  8  8                                   5  5  5  5    6  6  6  6                                  7  7  7  7    8  8  8  8           tensor = tf ones  4  4  4  dtype=tf int32        update = tf tensor scatter nd add tensor  indices  update        print update    ```    the result tensor would look like this            6  6  6  6    7  7  7  7    8  8  8  8    9  9  9  9             1  1  1  1    1  1  1  1    1  1  1  1    1  1  1  1             6  6  6  6    7  7  7  7    8  8  8  8    9  9  9  9             1  1  1  1    1  1  1  1    1  1  1  1    1  1  1  1       note that on cpu  if an out of bind index be find  an error be return    on gpu  if an out of bind index be find  the index be ignore     args      tensor  a `tensor`  tensor to copy/update      indices  a `tensor`  must be one of the follow type  `int32`  `int64`        index tensor      update  a `tensor`  must have the same type as `tensor`        update to scatter into output      name  a name for the operation  optional      return      a `tensor`  have the same type as `tensor`           ctx =  context  context or  context context     tld =  ctx  thread local data   if tld be eager      try         result =  pywrap tensorflow tfe py fastpathexecute           ctx  context handle  tld device name  string  name          tld op callbacks  tensor  indices  update        return  result     except  core  fallbackexception        try          return tensor scatter add eager fallback              tensor  indices  update  name=name  ctx= ctx        except  core  symbolicexception          pass         except  typeerror  valueerror           result =  dispatch dispatch                tensor scatter add  tensor=tensor  indices=indices                                    updates=updates  name=name          if result be not  dispatch opdispatcher not support            return result         raise     except  core  notokstatusexception as e         ops raise from not ok status e  name       try             op   output =  op def library  apply op helper          string  tensor=tensor  indices=indices  updates=updates                              name=name    except  typeerror  valueerror       result =  dispatch dispatch            tensor scatter add  tensor=tensor  indices=indices  updates=updates                                name=name      if result be not  dispatch opdispatcher not support        return result     raise    result =  output      if  execute must record gradient         attrs =  string   op  get attr type string   string                 op  get attr type string        input flat =  op input      execute record gradient          string   input flat   attrs   result     result  =  result   return  result 
  dispatch add dispatch list  tf export string  v1= string  string    deprecate endpoints string  def tensor scatter sub tensor  indices  update  name=none     r   subtract sparse `updates` from an exist tensor accord to `indices`     this operation create a new tensor by subtract sparse `updates` from the   pass in `tensor`    this operation be very similar to `tf scatter nd sub`  except that the update   be subtract from an exist tensor  as oppose to a variable   if the memory   for the exist tensor cannot be re-used  a copy be make and update     `indices` be an integer tensor contain indices into a new tensor of shape   `shape`   the last dimension of `indices` can be at most the rank of `shape`         indices shape -1  <= shape rank    the last dimension of `indices` correspond to indices into elements    if `indices shape -1  = shape rank`  or slice    if `indices shape -1  < shape rank`  along dimension `indices shape -1 ` of   `shape`   `updates` be a tensor with shape        indices shape  -1    shape indices shape -1       the simplest form of tensor scatter sub be to subtract individual elements   from a tensor by index  for example  say we want to insert 4 scatter elements   in a rank-1 tensor with 8 elements     in python  this scatter subtract operation would look like this     ```python       indices = tf constant   4    3    1    7          update = tf constant  9  10  11  12         tensor = tf ones  8   dtype=tf int32        update = tf tensor scatter nd sub tensor  indices  update        print update    ```    the result tensor would look like this          1  -10  1  -9  -8  1  1  -11     we can also  insert entire slice of a higher rank tensor all at once  for   example  if we want to insert two slice in the first dimension of a   rank-3 tensor with two matrices of new value     in python  this scatter add operation would look like this     ```python       indices = tf constant   0    2          update = tf constant    5  5  5  5    6  6  6  6                                  7  7  7  7    8  8  8  8                                   5  5  5  5    6  6  6  6                                  7  7  7  7    8  8  8  8           tensor = tf ones  4  4  4  dtype=tf int32        update = tf tensor scatter nd sub tensor  indices  update        print update    ```    the result tensor would look like this            -4  -4  -4  -4    -5  -5  -5  -5    -6  -6  -6  -6    -7  -7  -7  -7             1  1  1  1    1  1  1  1    1  1  1  1    1  1  1  1             -4  -4  -4  -4    -5  -5  -5  -5    -6  -6  -6  -6    -7  -7  -7  -7             1  1  1  1    1  1  1  1    1  1  1  1    1  1  1  1       note that on cpu  if an out of bind index be find  an error be return    on gpu  if an out of bind index be find  the index be ignore     args      tensor  a `tensor`  tensor to copy/update      indices  a `tensor`  must be one of the follow type  `int32`  `int64`        index tensor      update  a `tensor`  must have the same type as `tensor`        update to scatter into output      name  a name for the operation  optional      return      a `tensor`  have the same type as `tensor`           ctx =  context  context or  context context     tld =  ctx  thread local data   if tld be eager      try         result =  pywrap tensorflow tfe py fastpathexecute           ctx  context handle  tld device name  string  name          tld op callbacks  tensor  indices  update        return  result     except  core  fallbackexception        try          return tensor scatter sub eager fallback              tensor  indices  update  name=name  ctx= ctx        except  core  symbolicexception          pass         except  typeerror  valueerror           result =  dispatch dispatch                tensor scatter sub  tensor=tensor  indices=indices                                    updates=updates  name=name          if result be not  dispatch opdispatcher not support            return result         raise     except  core  notokstatusexception as e         ops raise from not ok status e  name       try             op   output =  op def library  apply op helper          string  tensor=tensor  indices=indices  updates=updates                              name=name    except  typeerror  valueerror       result =  dispatch dispatch            tensor scatter sub  tensor=tensor  indices=indices  updates=updates                                name=name      if result be not  dispatch opdispatcher not support        return result     raise    result =  output      if  execute must record gradient         attrs =  string   op  get attr type string   string                 op  get attr type string        input flat =  op input      execute record gradient          string   input flat   attrs   result     result  =  result   return  result 
  dispatch add dispatch list  tf export string  v1= string  string    deprecate endpoints string  def tensor scatter update tensor  indices  update  name=none     r   scatter `updates` into an exist tensor accord to `indices`     this operation create a new tensor by apply sparse `updates` to the pass   in `tensor`    this operation be very similar to `tf scatter nd`  except that the update be   scatter onto an exist tensor  as oppose to a zero-tensor   if the memory   for the exist tensor cannot be re-used  a copy be make and update     if `indices` contain duplicate  then their update be accumulate  sum        warn    the order in which update be apply be nondeterministic  so the   output will be nondeterministic if `indices` contain duplicate -- because   of some numerical approximation issue  number sum in different order   may yield different result     `indices` be an integer tensor contain indices into a new tensor of shape   `shape`   the last dimension of `indices` can be at most the rank of `shape`         indices shape -1  <= shape rank    the last dimension of `indices` correspond to indices into elements    if `indices shape -1  = shape rank`  or slice    if `indices shape -1  < shape rank`  along dimension `indices shape -1 ` of   `shape`   `updates` be a tensor with shape        indices shape  -1    shape indices shape -1       the simplest form of scatter be to insert individual elements in a tensor by   index  for example  say we want to insert 4 scatter elements in a rank-1   tensor with 8 elements     <div style= width 70   margin auto  margin-bottom 10px  margin-top 20px  >   <img style= width 100   src= https //www tensorflow org/images/scatternd1 png  alt>   </div>    in python  this scatter operation would look like this     ```python       indices = tf constant   4    3    1    7          update = tf constant  9  10  11  12         tensor = tf ones  8   dtype=tf int32        update = tf tensor scatter nd update tensor  indices  update        print update    ```    the result tensor would look like this          1  11  1  10  9  1  1  12     we can also  insert entire slice of a higher rank tensor all at once  for   example  if we want to insert two slice in the first dimension of a   rank-3 tensor with two matrices of new value     in python  this scatter operation would look like this     ```python       indices = tf constant   0    2          update = tf constant    5  5  5  5    6  6  6  6                                  7  7  7  7    8  8  8  8                                   5  5  5  5    6  6  6  6                                  7  7  7  7    8  8  8  8           tensor = tf ones  4  4  4  dtype=tf int32        update = tf tensor scatter nd update tensor  indices  update        print update    ```    the result tensor would look like this            5  5  5  5    6  6  6  6    7  7  7  7    8  8  8  8             1  1  1  1    1  1  1  1    1  1  1  1    1  1  1  1             5  5  5  5    6  6  6  6    7  7  7  7    8  8  8  8             1  1  1  1    1  1  1  1    1  1  1  1    1  1  1  1       note that on cpu  if an out of bind index be find  an error be return    on gpu  if an out of bind index be find  the index be ignore     args      tensor  a `tensor`  tensor to copy/update      indices  a `tensor`  must be one of the follow type  `int32`  `int64`        index tensor      update  a `tensor`  must have the same type as `tensor`        update to scatter into output      name  a name for the operation  optional      return      a `tensor`  have the same type as `tensor`           ctx =  context  context or  context context     tld =  ctx  thread local data   if tld be eager      try         result =  pywrap tensorflow tfe py fastpathexecute           ctx  context handle  tld device name  string  name          tld op callbacks  tensor  indices  update        return  result     except  core  fallbackexception        try          return tensor scatter update eager fallback              tensor  indices  update  name=name  ctx= ctx        except  core  symbolicexception          pass         except  typeerror  valueerror           result =  dispatch dispatch                tensor scatter update  tensor=tensor  indices=indices                                       updates=updates  name=name          if result be not  dispatch opdispatcher not support            return result         raise     except  core  notokstatusexception as e         ops raise from not ok status e  name       try             op   output =  op def library  apply op helper          string  tensor=tensor  indices=indices                                 updates=updates  name=name    except  typeerror  valueerror       result =  dispatch dispatch            tensor scatter update  tensor=tensor  indices=indices                                   updates=updates  name=name      if result be not  dispatch opdispatcher not support        return result     raise    result =  output      if  execute must record gradient         attrs =  string   op  get attr type string   string                 op  get attr type string        input flat =  op input      execute record gradient          string   input flat   attrs   result     result  =  result   return  result 
 tf export string  string  def tensordot a  b  ax  name=none     r   tensor contraction of a and b along specify ax and outer product     tensordot  also know as tensor contraction  sum the product of elements   from `a` and `b` over the indices specify by `a axes` and `b axes`    the list `a axes` and `b axes` specify those pair of ax along which to   contract the tensors  the axis `a ax i ` of `a` must have the same dimension   as axis `b ax i ` of `b` for all `i` in `range 0  len a ax  `  the list   `a axes` and `b axes` must have identical length and consist of unique   integers that specify valid ax for each of the tensors  additionally   outer product be support by pass `axes=0`     this operation correspond to `numpy tensordot a  b  ax `     example 1  when `a` and `b` be matrices  order 2   the case `axes = 1`   be equivalent to matrix multiplication     example 2  when `a` and `b` be matrices  order 2   the case   `axes =   1    0  ` be equivalent to matrix multiplication     example 3  when `a` and `b` be matrices  order 2   the case `axes=0` give   the outer product  a tensor of order 4     example 4  suppose that \\ a  ijk \\  and \\ b  lmn \\  represent two   tensors of order 3  then  `contract a  b    0    2   ` be the order 4 tensor   \\ c  jklm \\  whose entry   correspond to the indices \\  j k l m \\  be give by     \\  c  jklm  = \sum i a  ijk  b  lmi  \\      in general  `order c  = order a    order b  - 2 len ax 0  `     args      a  `tensor` of type `float32` or `float64`      b  `tensor` with the same type as `a`      ax  either a scalar `n`  or a list or an `int32` `tensor` of shape  2  k         if ax be a scalar  sum over the last n ax of a and the first n ax of       b in order  if ax be a list or `tensor` the first and second row contain       the set of unique integers specify ax along which the contraction be       compute  for `a` and `b`  respectively  the number of ax for `a` and       `b` must be equal  if `axes=0`  compute the outer product between `a` and       `b`      name  a name for the operation  optional      return      a `tensor` with the same type as `a`     raise      valueerror  if the shape of `a`  `b`  and `axes` be incompatible      indexerror  if the value in ax exceed the rank of the correspond       tensor           def  tensordot reshape a  ax  flipped=false       string     if a get shape   be fully define   and isinstance ax   list  tuple          shape a = a get shape   as list         ax =  i if i >= 0 else i   len shape a  for i in ax        free =  i for i in xrange len shape a   if i not in ax        free dim =  shape a i  for i in free        prod free = int np prod  shape a i  for i in free          prod ax = int np prod  shape a i  for i in ax          perm = list ax    free if flip else free   list ax        new shape =  prod ax  prod free  if flip else  prod free  prod ax        reshape a = array ops reshape array ops transpose a  perm   new shape        return reshape a  free dim  free dim     else        if a get shape   ndims be not none and isinstance ax   list  tuple            shape a = a get shape   as list           ax =  i if i >= 0 else i   len shape a  for i in ax          free =  i for i in xrange len shape a   if i not in ax          ax dim =  shape a i  for i in ax          free dim =  shape a i  for i in free          free dim static = free dim         ax = ops convert to tensor ax  dtype=dtypes int32  name=string          free = ops convert to tensor free  dtype=dtypes int32  name=string          shape a = array ops shape a        else          free dim static = none         shape a = array ops shape a          rank a = array ops rank a          ax = ops convert to tensor ax  dtype=dtypes int32  name=string          ax = array ops where ax >= 0  ax  ax   rank a          free    = array ops setdiff1d range rank a   ax        free dim = array ops gather shape a  free        ax dim = array ops gather shape a  ax        prod free dim = reduce prod free dim        prod ax dim = reduce prod ax dim        if flip          perm = array ops concat  ax  free   0          new shape = array ops stack  prod ax dim  prod free dim         else          perm = array ops concat  free  ax   0          new shape = array ops stack  prod free dim  prod ax dim         reshape a = array ops reshape array ops transpose a  perm   new shape        return reshape a  free dim  free dim static    def  tensordot ax a  ax       string     a shape = a get shape       if isinstance ax  compat integral type         if ax < 0          raise valueerror string        if a shape ndims be not none          if ax > a shape ndims            raise valueerror string                            string   a          return  list xrange a shape ndims - ax                              a shape ndims    list xrange ax          else          rank = array ops rank a          return  range rank - ax  rank                        dtype=dtypes int32   range ax  dtype=dtypes int32       elif isinstance ax   list  tuple          if len ax   = 2          raise valueerror string        a ax = ax 0        b ax = ax 1        if isinstance a ax  compat integral type  and \           isinstance b ax  compat integral type           a ax =  a ax          b ax =  b ax        if len a ax   = len b ax           raise valueerror              string                len a ax   len b ax          return a ax  b ax     else        ax = ops convert to tensor ax  name=string  dtype=dtypes int32        return ax 0   ax 1     with ops name scope name  string   a  b  ax   as name      a = ops convert to tensor a  name=string      b = ops convert to tensor b  name=string      a ax  b ax =  tensordot ax a  ax      a reshape  a free dim  a free dim static =  tensordot reshape a  a ax      b reshape  b free dim  b free dim static =  tensordot reshape          b  b ax  true      ab matmul = matmul a reshape  b reshape      if isinstance a free dim  list  and isinstance b free dim  list         return array ops reshape ab matmul  a free dim   b free dim  name=name      else        a free dim = ops convert to tensor a free dim  dtype=dtypes int32        b free dim = ops convert to tensor b free dim  dtype=dtypes int32        product = array ops reshape            ab matmul  array ops concat  a free dim  b free dim   0   name=name        if a free dim static be not none and b free dim static be not none          product set shape a free dim static   b free dim static        return product 
  dispatch add dispatch list  tf export string  v1= string  string    deprecate endpoints string  def tile input  multiples  name=none     r   construct a tensor by tile a give tensor     this operation create a new tensor by replicate `input` `multiples` time    the output tensor s i th dimension have `input dim i    multiples i ` elements    and the value of `input` be replicate `multiples i ` time along the  i th   dimension  for example  tile ` a b c d ` by ` 2 ` produce   ` a b c d a b c d `     >>> a = tf constant   1 2 3   4 5 6    tf int32    >>> b = tf constant  1 2   tf int32    >>> tf tile a  b    <tf tensor  shape= 2  6   dtype=int32  numpy=   array   1  2  3  1  2  3             4  5  6  4  5  6    dtype=int32 >   >>> c = tf constant  2 1   tf int32    >>> tf tile a  c    <tf tensor  shape= 4  3   dtype=int32  numpy=   array   1  2  3             4  5  6             1  2  3             4  5  6    dtype=int32 >   >>> d = tf constant  2 2   tf int32    >>> tf tile a  d    <tf tensor  shape= 4  6   dtype=int32  numpy=   array   1  2  3  1  2  3             4  5  6  4  5  6             1  2  3  1  2  3             4  5  6  4  5  6    dtype=int32 >    args      input  a `tensor`  1-d or higher      multiples  a `tensor`  must be one of the follow type  `int32`  `int64`        1-d  length must be the same as the number of dimension in `input`     name  a name for the operation  optional      return      a `tensor`  have the same type as `input`           ctx =  context  context or  context context     tld =  ctx  thread local data   if tld be eager      try         result =  pywrap tensorflow tfe py fastpathexecute           ctx  context handle  tld device name  string  name  tld op callbacks          input  multiples        return  result     except  core  fallbackexception        try          return tile eager fallback              input  multiples  name=name  ctx= ctx        except  core  symbolicexception          pass         except  typeerror  valueerror           result =  dispatch dispatch                tile  input=input  multiples=multiples  name=name          if result be not  dispatch opdispatcher not support            return result         raise     except  core  notokstatusexception as e         ops raise from not ok status e  name       try             op   output =  op def library  apply op helper          string  input=input  multiples=multiples  name=name    except  typeerror  valueerror       result =  dispatch dispatch            tile  input=input  multiples=multiples  name=name      if result be not  dispatch opdispatcher not support        return result     raise    result =  output      if  execute must record gradient         attrs =  string   op  get attr type string   string                 op  get attr type string        input flat =  op input      execute record gradient          string   input flat   attrs   result     result  =  result   return  result 
  dispatch add dispatch list  tf export string  def timestamp name=none     r   provide the time since epoch in second     return the timestamp as a `float64` for second since the unix epoch     note  the timestamp be compute when the op be execute  not when it be add   to the graph     args      name  a name for the operation  optional      return      a `tensor` of type `float64`           ctx =  context  context or  context context     tld =  ctx  thread local data   if tld be eager      try         result =  pywrap tensorflow tfe py fastpathexecute           ctx  context handle  tld device name  string  name          tld op callbacks        return  result     except  core  fallbackexception        try          return timestamp eager fallback              name=name  ctx= ctx        except  core  symbolicexception          pass         except  typeerror  valueerror           result =  dispatch dispatch                timestamp  name=name          if result be not  dispatch opdispatcher not support            return result         raise     except  core  notokstatusexception as e         ops raise from not ok status e  name       try             op   output =  op def library  apply op helper          string  name=name    except  typeerror  valueerror       result =  dispatch dispatch            timestamp  name=name      if result be not  dispatch opdispatcher not support        return result     raise    result =  output      if  execute must record gradient         attrs =         input flat =  op input      execute record gradient          string   input flat   attrs   result     result  =  result   return  result 
 tf export string  v1=    def transpose v2 a  perm=none  conjugate=false  name=string     string   return transpose a=a  perm=perm  name=name  conjugate=conjugate  
  dispatch add dispatch list  tf export string  def truncate div x  y  name=none     r   return x / y element-wise for integer type     truncation designate that negative number will round fractional quantities   toward zero  i e  -7 / 5 = -1  this match c semantics but it be different   than python semantics  see `floordiv` for a division function that match   python semantics      note   `truncatediv` support broadcast  more about broadcast    here  http //docs scipy org/doc/numpy/user/basics broadcast html     args      x  a `tensor`  must be one of the follow type  `bfloat16`  `half`  `float32`  `float64`  `uint8`  `int8`  `uint16`  `int16`  `int32`  `int64`  `complex64`  `complex128`      y  a `tensor`  must have the same type as `x`      name  a name for the operation  optional      return      a `tensor`  have the same type as `x`           ctx =  context  context or  context context     tld =  ctx  thread local data   if tld be eager      try         result =  pywrap tensorflow tfe py fastpathexecute           ctx  context handle  tld device name  string  name          tld op callbacks  x  y        return  result     except  core  fallbackexception        try          return truncate div eager fallback              x  y  name=name  ctx= ctx        except  core  symbolicexception          pass         except  typeerror  valueerror           result =  dispatch dispatch                truncate div  x=x  y=y  name=name          if result be not  dispatch opdispatcher not support            return result         raise     except  core  notokstatusexception as e         ops raise from not ok status e  name       try             op   output =  op def library  apply op helper          string  x=x  y=y  name=name    except  typeerror  valueerror       result =  dispatch dispatch            truncate div  x=x  y=y  name=name      if result be not  dispatch opdispatcher not support        return result     raise    result =  output      if  execute must record gradient         attrs =  string   op  get attr type string        input flat =  op input      execute record gradient          string   input flat   attrs   result     result  =  result   return  result 
  dispatch add dispatch list  tf export string  def truncate mod x  y  name=none     r   return element-wise remainder of division  this emulate c semantics in that    the result here be consistent with a truncate divide  e g  `truncate x / y      y   truncate mod x  y  = x`      note   `truncatemod` support broadcast  more about broadcast    here  http //docs scipy org/doc/numpy/user/basics broadcast html     args      x  a `tensor`  must be one of the follow type  `int32`  `int64`  `bfloat16`  `half`  `float32`  `float64`      y  a `tensor`  must have the same type as `x`      name  a name for the operation  optional      return      a `tensor`  have the same type as `x`           ctx =  context  context or  context context     tld =  ctx  thread local data   if tld be eager      try         result =  pywrap tensorflow tfe py fastpathexecute           ctx  context handle  tld device name  string  name          tld op callbacks  x  y        return  result     except  core  fallbackexception        try          return truncate mod eager fallback              x  y  name=name  ctx= ctx        except  core  symbolicexception          pass         except  typeerror  valueerror           result =  dispatch dispatch                truncate mod  x=x  y=y  name=name          if result be not  dispatch opdispatcher not support            return result         raise     except  core  notokstatusexception as e         ops raise from not ok status e  name       try             op   output =  op def library  apply op helper          string  x=x  y=y  name=name    except  typeerror  valueerror       result =  dispatch dispatch            truncate mod  x=x  y=y  name=name      if result be not  dispatch opdispatcher not support        return result     raise    result =  output      if  execute must record gradient         attrs =  string   op  get attr type string        input flat =  op input      execute record gradient          string   input flat   attrs   result     result  =  result   return  result 
 tf export string  v1=    def tuple v2 tensors  control inputs=none  name=none     string   return tuple tensors=tensors  name=name  control inputs=control input    
 tf export string  def unique x  out idx=dtypes int32  name=none                 return gen array ops unique x  out idx  name  
 tf export string  def unique with count x  out idx=dtypes int32  name=none                 return gen array ops unique with count x  out idx  name  
  dispatch add dispatch list  tf export string  def unravel index indices  dim  name=none     r   convert an array of flat indices into a tuple of coordinate array        example     ```   y = tf unravel index indices= 2  5  7   dims= 3  3        dim  represent a hypothetical  3  3  tensor of indices        0  1   2          3  4   5          6   7   8       for each entry from  indices   this operation return     its coordinate  mark with       such as     2 ==>  0  2      5 ==>  1  2      7 ==>  2  1    y ==>   0  1  2    2  2  1     ```     compatibility numpy    equivalent to np unravel index    end compatibility    args      indices  a `tensor`  must be one of the follow type  `int32`  `int64`        an 0-d or 1-d `int` tensor whose elements be indices into the       flatten version of an array of dimension dim      dim  a `tensor`  must have the same type as `indices`        an 1-d `int` tensor  the shape of the array to use for unravel       indices      name  a name for the operation  optional      return      a `tensor`  have the same type as `indices`           ctx =  context  context or  context context     tld =  ctx  thread local data   if tld be eager      try         result =  pywrap tensorflow tfe py fastpathexecute           ctx  context handle  tld device name  string  name          tld op callbacks  indices  dim        return  result     except  core  fallbackexception        try          return unravel index eager fallback              indices  dim  name=name  ctx= ctx        except  core  symbolicexception          pass         except  typeerror  valueerror           result =  dispatch dispatch                unravel index  indices=indices  dims=dims  name=name          if result be not  dispatch opdispatcher not support            return result         raise     except  core  notokstatusexception as e         ops raise from not ok status e  name       try             op   output =  op def library  apply op helper          string  indices=indices  dims=dims  name=name    except  typeerror  valueerror       result =  dispatch dispatch            unravel index  indices=indices  dims=dims  name=name      if result be not  dispatch opdispatcher not support        return result     raise    result =  output      if  execute must record gradient         attrs =  string   op  get attr type string        input flat =  op input      execute record gradient          string   input flat   attrs   result     result  =  result   return  result 
 tf export string  def unstack value  num=none  axis=0  name=string     string   if num be none      value = ops convert to tensor value      value shape = value get shape       if value shape ndims be not none        if axis < -value shape ndims or axis >= value shape ndims          raise valueerror string                             axis  -value shape ndims  value shape ndims         num = value shape dim axis  value   if num be none      raise valueerror string   value shape    return gen array ops unpack value  num=num  axis=axis  name=name  
 tf export string  v1=     tf contextlib contextmanager def variable creator scope variable creator     string   with ops get default graph    variable creator scope variable creator         yield 
 tf export string  def vectorized map fn  elems     string   def loop fn i       gather elems = nest map structure lambda x  array ops gather x  i   elems      return fn gather elems    batch size = none   first elem = ops convert to tensor nest flatten elems  0     if first elem shape rank be not none      batch size = first elem shape as list   0    if batch size be none      batch size = array ops shape first elem  0    return pfor loop fn  batch size  
 tf export string  v1= string   def where v2 condition  x=none  y=none  name=none     string   if x be none and y be none      with ops name scope name  string   condition   as name        condition = ops convert to tensor            condition  prefer dtype=dtypes bool  name=string        return gen array ops where condition=condition  name=name    elif x be not none and y be not none      return gen math ops select v2 condition=condition  t=x  e=y  name=name    else      raise valueerror string  
 tf export string  v1=    def while loop v2 cond                    body                    loop vars                    shape invariants=none                    parallel iterations=10                    back prop=true                    swap memory=false                    maximum iterations=none                    name=none     string   return while loop        cond=cond        body=body        loop vars=loop vars        shape invariants=shape invariants        parallel iterations=parallel iterations        back prop=back prop        swap memory=swap memory        name=name        maximum iterations=maximum iterations        return same structure=true  
 tf export string  def zero shape  dtype=dtypes float32  name=none     string   dtype = dtypes as dtype dtype  base dtype   with ops name scope name  string   shape   as name      if dtype == dtypes bool        zero = false     elif dtype == dtypes string        zero = string     else        zero = 0      if not isinstance shape  ops tensor         try                            output =  constant if small zero  shape  dtype  name          if output be not none            return output                   shape = constant op  tensor shape tensor conversion function              tensor shape tensorshape shape         except  typeerror  valueerror                    shape = ops convert to tensor shape  dtype=dtypes int32      if not shape  shape tuple          shape = reshape shape   -1         output = fill shape  constant zero  dtype=dtype   name=name    assert output dtype base dtype == dtype   return output 
class zero initializer     string    def   call   self  shape  dtype=dtypes float32       dtype = dtypes as dtype dtype      return array ops zero shape  dtype  
 tf export string  v1=     dispatch add dispatch support def zero like v2      input        dtype=none      name=none     string   return zero like impl input  dtype  name  optimize=true  
def as bytes bytes or text  encoding=string     string   if isinstance bytes or text  bytearray       return bytes bytes or text    elif isinstance bytes or text   six text type       return bytes or text encode encode    elif isinstance bytes or text  bytes       return bytes or text   else      raise typeerror string                        bytes or text    
 tf export string  def as str any value     string   if isinstance value  bytes       return as str value    else      return str value  
def as text bytes or text  encoding=string     string   if isinstance bytes or text   six text type       return bytes or text   elif isinstance bytes or text  bytes       return bytes or text decode encode    else      raise typeerror string   bytes or text  
 tf export      string      v1= string  string   def dimension at index shape  index     string   assert isinstance shape  tensorshape    if shape rank be none      return dimension none    else      return shape dim index  
 tf export      string  v1= string  string   def dimension value dimension     string   if isinstance dimension  dimension       return dimension value   return dimension 
 tf export string   tf contextlib contextmanager def forward compatibility horizon year  month  day     string   try       update forward compatibility date number datetime date year  month  day       yield   finally       update forward compatibility date number   
 tf export string  def forward compatible year  month  day     string   return  forward compatibility date number >  date to date number        year  month  day  
 tf export string  def path to str path     r   convert input which be a `pathlike` object to `str` type     convert from any python constant representation of a `pathlike` object to   a string  if the input be not a `pathlike` object  simply return the input     args      path  an object that can be convert to path representation     return      a `str` object     usage      in case a simplify `str` version of the path be need from an     `os pathlike` object    examples    ```python     tf compat path to str  c \xyz\tensorflow\ /  / /tensorflow      c \xyz\tensorflow\ /  / /tensorflow    windows os     tf compat path to str path  c \xyz\tensorflow\ /  / /tensorflow       c \xyz\tensorflow\  \tensorflow    windows os     tf compat path to str path   /corpus       corpus    linux os     tf compat path to str   /  / /corpus       /  / /corpus    linux os     tf compat path to str path   /  / /corpus         /corpus    linux os     tf compat path to str path   /  ////  /         /      linux os    ```         if hasattr path  string       path = as str any path   fspath        return path 
class conditionalaccumulator conditionalaccumulatorbase     string    def   init   self                 dtype                 shape=none                 share name=none                 name=string                 reduction type=string       string     accumulator ref = gen data flow ops resource conditional accumulator          dtype=dtype          shape=shape          share name=shared name          name=name          reduction type=reduction type      if context execute eagerly          self  resource deleter = resource variable ops eagerresourcedeleter            handle=accumulator ref  handle device=context context   device name       super conditionalaccumulator  self    init   dtype  shape  accumulator ref     def apply grad self  grad  local step=0  name=none       string     grad = ops convert to tensor grad  self  dtype      grad get shape   assert be compatible with self  shape      local step = math ops cast ops convert to tensor local step    dtypes int64       return gen data flow ops resource accumulator apply gradient          self  accumulator ref  local step=local step  gradient=grad  name=name     def take grad self  num require  name=none       string     out = gen data flow ops resource accumulator take gradient          self  accumulator ref  num require  dtype=self  dtype  name=name      out set shape self  shape      return out 
class conditionalaccumulatorbase object     string    def   init   self  dtype  shape  accumulator ref       string     self  dtype = dtype     if shape be not none        self  shape = tensor shape tensorshape shape      else        self  shape = tensor shape unknown shape       self  accumulator ref = accumulator ref     if context execute eagerly          self  name = context context   scope name     else        self  name = self  accumulator ref op name split string  -1      property   def accumulator ref self       string     return self  accumulator ref     property   def name self       string     return self  name     property   def dtype self       string     return self  dtype    def num accumulate self  name=none       string     if name be none        name = string   self  name      return gen data flow ops resource accumulator num accumulate          self  accumulator ref  name=name     def set global step self  new global step  name=none       string     return gen data flow ops resource accumulator set global step          self  accumulator ref          math ops cast ops convert to tensor new global step    dtypes int64           name=name  
class devicespecv1 devicespecv2       doc   = devicespecv2   doc       slot   = devicespecv2   slot       devicespecv2 job setter   def job self  job       self  job =  as str or none job      self  as string  self  hash = none  none     devicespecv2 replica setter   def replica self  replica       self  replica =  as int or none replica      self  as string  self  hash = none  none     devicespecv2 task setter   def task self  task       self  task =  as int or none task      self  as string  self  hash = none  none     devicespecv2 device type setter   def device type self  device type       self  device type =  as device str or none device type      self  as string  self  hash = none  none     devicespecv2 device index setter   def device index self  device index       self  device index =  as int or none device index      self  as string  self  hash = none  none    def   hash   self       if self  hash be none        self  hash = hash self to string        return self  hash    def to string self       if self  as string be none        self  as string = self  components to string            job=self job  replica=self replica  task=self task            device type=self device type  device index=self device index      return self  as string    def parse from string self  spec        self job  self replica  self task  self device type  self device index       = self  string to components spec       return self    def merge from self  dev       string      self job  self replica  self task  self device type  self device index       = self  get combine properties dev        to string   doc   = devicespecv2 to string   doc     parse from string   doc   = devicespecv2 parse from string   doc   
class dimension object     string      slot   =  string     def   init   self  value       string     if value be none        self  value = none     elif isinstance value  dimension         self  value = value     else        try                            self  value = int value   index            except attributeerror          six raise from              typeerror string                       string format value                none        if self  value < 0          raise valueerror string   self  value     def   repr   self       return string   repr self  value     def   str   self       value = self  value     return string if value be none else str value     def   eq   self  other       string     try        other = as dimension other      except  typeerror  valueerror         return notimplemented     if self  value be none or other value be none        return none     return self  value == other value    def   ne   self  other       string     try        other = as dimension other      except  typeerror  valueerror         return notimplemented     if self  value be none or other value be none        return none     return self  value  = other value    def   int   self       return self  value          def   long   self       return self  value    def   index   self            return self  value     property   def value self       string     return self  value    def be compatible with self  other       string     other = as dimension other      return  self  value be none or other value be none or             self  value == other value     def assert be compatible with self  other       string     if not self be compatible with other         raise valueerror string                           self  other      def merge with self  other       string     other = as dimension other      self assert be compatible with other      if self  value be none        return dimension other value      else        return dimension self  value     def   add   self  other       string     try        other = as dimension other      except  typeerror  valueerror         return notimplemented     if self  value be none or other value be none        return dimension none      else        return dimension self  value   other value     def   radd   self  other       string     return self   other    def   sub   self  other       string     try        other = as dimension other      except  typeerror  valueerror         return notimplemented     if self  value be none or other value be none        return dimension none      else        return dimension self  value - other value     def   rsub   self  other       string     other = as dimension other      if self  value be none or other value be none        return dimension none      else        return dimension other value - self  value     def   mul   self  other       string     try        other = as dimension other      except  typeerror  valueerror         return notimplemented      if self  value be none or other value be none        return dimension none      else        return dimension self  value   other value     def   rmul   self  other       string     return self   other    def   floordiv   self  other       string     try        other = as dimension other      except  typeerror  valueerror         return notimplemented     if self  value be none or other value be none        return dimension none      else        return dimension self  value // other value     def   rfloordiv   self  other       string     other = as dimension other      if self  value be none or other value be none        return dimension none      else        return dimension other value // self  value     def   div   self  other       string     return self // other    def   rdiv   self  other       string     raise typeerror string                     string format type other    name        def   truediv   self  other       string     raise typeerror string                     string format type other    name        def   rtruediv   self  other       string     raise typeerror string                     string format type other    name        def   mod   self  other       string     other = as dimension other      if self  value be none or other value be none        return dimension none      else        return dimension self  value   other value     def   rmod   self  other       string     other = as dimension other      return other   self    def   lt   self  other       string     other = as dimension other      if self  value be none or other value be none        return none     else        return self  value < other value    def   le   self  other       string     other = as dimension other      if self  value be none or other value be none        return none     else        return self  value <= other value    def   gt   self  other       string     other = as dimension other      if self  value be none or other value be none        return none     else        return self  value > other value    def   ge   self  other       string     other = as dimension other      if self  value be none or other value be none        return none     else        return self  value >= other value    def   reduce   self       return dimension   self  value   
class fixedlengthrecordreader readerbase     string        deprecation deprecate        none  string       string    def   init   self                 record bytes                 header bytes=none                 footer bytes=none                 hop bytes=none                 name=none                 encoding=none       string     rr = gen io ops fix length record reader v2          record bytes=record bytes          header bytes=header bytes          footer bytes=footer bytes          hop bytes=hop bytes          encoding=encoding          name=name      super fixedlengthrecordreader  self    init   rr  
class graphkeys object     string          global variables = string         local variables = string         metric variables = string      model variables = string         trainable variables = string      summaries = string      queue runners = string      table initializers = string         asset filepaths = string      move average variables = string      regularization losses = string      concatenate variables = string      savers = string      weight = string      bias = string      activations = string      update ops = string      losses = string      saveable object = string         resources = string         local resources = string      trainable resource variables = string       init op = string   local init op = string   ready op = string   ready for local init op = string   summary op = string   global step = string          eval step = string   train op = string       cond context = string   while context = string        summary collection = string        variable collections =         global variables        local variables        metric variables        model variables        trainable variables        move average variables        concatenate variables        trainable resource variables                stream model port = string     decorator utils classproperty    deprecation deprecate none  string    def variables cls         return cls global variables 
class identityreader readerbase     string     deprecation deprecate        none  string       string    def   init   self  name=none       string     rr = gen io ops identity reader v2 name=name      super identityreader  self    init   rr  support serialize=true  
class interactivesession basesession     string     count lock = thread lock      active session count = 0      def   init   self  target=string  graph=none  config=none       string     if not config                                    gpu options = config pb2 gpuoptions allow growth=true        config = config pb2 configproto gpu options=gpu options           config graph options place prune graph = true      super interactivesession  self    init   target  graph  config      with interactivesession  count lock        if interactivesession  active session count > 0          warn warn string                       string                       string                       string        interactivesession  active session count  = 1                         self  explicitly close = false      self  default session = self as default       self  default session enforce nest = false     self  default session   enter         self  explicit graph = graph     if self  explicit graph be not none        self  default graph = graph as default         self  default graph enforce nest = false       self  default graph   enter        def close self       string     super interactivesession  self  close       with interactivesession  count lock        if not self  explicitly close          interactivesession  active session count -= 1         self  explicitly close = true       else          return     if self  explicit graph be not none        self  default graph   exit   none  none  none        self  default graph = none     self  default session   exit   none  none  none      self  default session = none 
class lmdbreader readerbase     string     deprecation deprecate        none  string       string    def   init   self  name=none  options=none       string     del options     rr = gen io ops lmdb reader name=name      super lmdbreader  self    init   rr  
 deprecate string  string             string             string             string             string             string             string             string   tf export v1= string   def print input   data  message=none  first n=none  summarize=none  name=none     string   return gen log ops  print input   data  message  first n  summarize  name  
class readerbase object     string    def   init   self  reader ref  support serialize=false       string     if context execute eagerly          raise runtimeerror            string           string       self  reader ref = reader ref     self  support serialize = support serialize     property   def reader ref self       string     return self  reader ref    def read self  queue  name=none       string     if isinstance queue  ops tensor         queue ref = queue     else        queue ref = queue queue ref     if self  reader ref dtype == dtypes resource        return gen io ops reader read v2 self  reader ref  queue ref  name=name      else                      old queue op = gen data flow ops fake queue queue ref        return gen io ops reader read self  reader ref  old queue op  name=name     def read up to self  queue  num record                     name=none       string     if isinstance queue  ops tensor         queue ref = queue     else        queue ref = queue queue ref     if self  reader ref dtype == dtypes resource        return gen io ops reader read up to v2 self  reader ref                                               queue ref                                               num record                                               name=name      else                      old queue op = gen data flow ops fake queue queue ref        return gen io ops reader read up to self  reader ref                                            old queue op                                            num record                                            name=name     def num record produce self  name=none       string     if self  reader ref dtype == dtypes resource        return gen io ops reader num record produce v2 self  reader ref                                                         name=name      else        return gen io ops reader num record produce self  reader ref                                                      name=name     def num work units complete self  name=none       string     if self  reader ref dtype == dtypes resource        return gen io ops reader num work units complete v2 self  reader ref                                                             name=name      else        return gen io ops reader num work units complete self  reader ref                                                          name=name     def serialize state self  name=none       string     if self  reader ref dtype == dtypes resource        return gen io ops reader serialize state v2 self  reader ref  name=name      else        return gen io ops reader serialize state self  reader ref  name=name     def restore state self  state  name=none       string     if self  reader ref dtype == dtypes resource        return gen io ops reader restore state v2            self  reader ref  state  name=name      else        return gen io ops reader restore state self  reader ref  state  name=name      property   def support serialize self       string     return self  support serialize    def reset self  name=none       string     if self  reader ref dtype == dtypes resource        return gen io ops reader reset v2 self  reader ref  name=name      else        return gen io ops reader reset self  reader ref  name=name  
class session basesession     string    def   init   self  target=string  graph=none  config=none       string     super session  self    init   target  graph  config=config           self  default graph context manager = none     self  default session context manager = none    def   enter   self       if self  default graph context manager be none        self  default graph context manager = self graph as default       else        raise runtimeerror string                          string                          string      if self  default session context manager be none        self  default session context manager = self as default       self  default graph context manager   enter         return self  default session context manager   enter        def   exit   self  exec type  exec value  exec tb       if exec type be errors operror        log error string   exec value        try        self  default session context manager   exit   exec type  exec value                                                       exec tb      except runtimeerror as error        if error == exec value                                                                                  pass       else          raise     self  default graph context manager   exit   exec type  exec value  exec tb       self  default session context manager = none     self  default graph context manager = none                     if exec type        close thread = thread thread            name=string  target=self close        close thread daemon = true       close thread start         close thread join 30 0        if close thread be alive            log error              string             string      else        self close       staticmethod   def reset target  containers=none  config=none       string     if target be not none        target = compat as bytes target      if containers be not none        containers =  compat as bytes c  for c in containers      else        containers =        tf session tf reset target  containers  config  
class sparseconditionalaccumulator conditionalaccumulatorbase     string    def   init   self                 dtype                 shape=none                 share name=none                 name=string                 reduction type=string       accumulator ref = gen data flow ops sparse conditional accumulator          dtype=dtype          shape=shape          share name=shared name          name=name          reduction type=reduction type      super sparseconditionalaccumulator  self    init   dtype  shape                                                         accumulator ref     def apply index slice grad self  grad  local step=0  name=none       string     return self apply grad          grad indices=grad indices          grad values=grad value          grad shape=grad dense shape          local step=local step          name=name     def apply grad self                   grad indices                   grad value                   grad shape=none                   local step=0                   name=none       string     local step = math ops cast ops convert to tensor local step    dtypes int64      return gen data flow ops sparse accumulator apply gradient          self  accumulator ref          local step=local step          gradient indices=math ops cast grad indices   dtypes int64           gradient values=grad value          gradient shape=math ops cast                 if grad shape be none else grad shape   dtypes int64           have know shape= grad shape be not none           name=name     def take grad self  num require  name=none       string     return gen data flow ops sparse accumulator take gradient          self  accumulator ref  num require  dtype=self  dtype  name=name     def take index slice grad self  num require  name=none       string     return val = gen data flow ops sparse accumulator take gradient          self  accumulator ref  num require  dtype=self  dtype  name=name      return ops indexedslices          indices=return val indices          values=return val value          dense shape=return val shape        def num accumulate self  name=none       string     if name be none        name = string   self  name      return gen data flow ops accumulator num accumulate          self  accumulator ref  name=name     def set global step self  new global step  name=none       string     return gen data flow ops accumulator set global step          self  accumulator ref          math ops cast ops convert to tensor new global step    dtypes int64           name=name  
class tfrecordreader readerbase     string        deprecation deprecate        none  string       string    def   init   self  name=none  options=none       string     compression type = python io tfrecordoptions get compression type string          options       rr = gen io ops tf record reader v2          name=name  compression type=compression type      super tfrecordreader  self    init   rr  
class textlinereader readerbase     string        deprecation deprecate        none  string       string    def   init   self  skip header lines=none  name=none       string     rr = gen io ops text line reader v2 skip header lines=skip header line                                          name=name      super textlinereader  self    init   rr  
class variablev1 variable     string    def   init          self          initial value=none        trainable=none        collections=none        validate shape=true        cache device=none        name=none        variable def=none        dtype=none        expect shape=none        import scope=none        constraint=none        use resource=none        synchronization=variablesynchronization auto        aggregation=variableaggregation none        shape=none       string    savesliceinfo = variable savesliceinfo 
class variableaggregation enum enum     none = 0   sum = 1   mean = 2   only first replica = 3   only first tower = 3      def   hash   self       return hash self value  
class variablescope object     string    def   init   self                 reuse                 name=string                 initializer=none                 regularizer=none                 cache device=none                 partitioner=none                 custom getter=none                 name scope=string                 dtype=dtypes float32                 use resource=none                 constraint=none       string     self  name = name     self  initializer = initializer     self  regularizer = regularizer     self  reuse = reuse     self  cache device = cache device     self  partitioner = partitioner     self  custom getter = custom getter     self  name scope = name scope     self  dtype = dtype     self  use resource = use resource     self  constraint = constraint     if context execute eagerly          if self  cache device be not none          raise notimplementederror string                                   string        self  reuse = auto reuse       self  use resource = true     property   def name self       return self  name     property   def original name scope self       return self  name scope     property   def reuse self       return self  reuse     property   def initializer self       return self  initializer     property   def dtype self       return self  dtype     property   def use resource self       return self  use resource     property   def regularizer self       return self  regularizer     property   def cache device self       return self  cache device     property   def partitioner self       return self  partitioner     property   def custom getter self       return self  custom getter     property   def constraint self       return self  constraint    def reuse variables self       string     self  reuse = true    def set initializer self  initializer       string     self  initializer = initializer    def set dtype self  dtype       string     self  dtype = dtype    def set use resource self  use resource       string     if context execute eagerly   and not use resource        raise valueerror string                        string      self  use resource = use resource    def set regularizer self  regularizer       string     self  regularizer = regularizer    def set cache device self  cache device       string     if context execute eagerly          raise notimplementederror string                                 string      self  cache device = cache device    def set partitioner self  partitioner       string     self  partitioner = partitioner    def set custom getter self  custom getter       string     self  custom getter = custom getter    def get collection self  name       string     scope = self  name   string if self  name else string     return ops get collection name  scope     def trainable variables self       string     return self get collection ops graphkeys trainable variables     def global variables self       string     return self get collection ops graphkeys global variables     def local variables self       string     return self get collection ops graphkeys local variables     def get variable self                     var store                     name                     shape=none                     dtype=none                     initializer=none                     regularizer=none                     reuse=none                     trainable=none                     collections=none                     cache device=none                     partitioner=none                     validate shape=true                     use resource=none                     custom getter=none                     constraint=none                     synchronization=variablesynchronization auto                     aggregation=variableaggregation none       string     if regularizer be none        regularizer = self  regularizer     if cache device be none        cache device = self  cache device     if partitioner be none        partitioner = self  partitioner     if custom getter be none        custom getter = self  custom getter     if context execute eagerly          reuse = false       use resource = true     else        if reuse be none          reuse = self  reuse       if use resource be none          use resource = self  use resource      full name = self name   string   name if self name else name               with ops name scope none                       if  dtype be not none and initializer be not none and           not callable initializer            init dtype = ops convert to tensor initializer  dtype base dtype         if init dtype  = dtype            raise valueerror string                            string    init dtype  dtype         if initializer be none          initializer = self  initializer       if constraint be none          constraint = self  constraint       if dtype be none          dtype = self  dtype       return var store get variable            full name            shape=shape            dtype=dtype            initializer=initializer            regularizer=regularizer            reuse=reuse            trainable=trainable            collections=collections            cache device=caching device            partitioner=partitioner            validate shape=validate shape            use resource=use resource            custom getter=custom getter            constraint=constraint            synchronization=synchronization            aggregation=aggregation     def  get partition variable self                                  var store                                  name                                  shape=none                                  dtype=none                                  initializer=none                                  regularizer=none                                  trainable=none                                  collections=none                                  cache device=none                                  partitioner=none                                  validate shape=true                                  use resource=none                                  constraint=none                                  synchronization=variablesynchronization auto                                  aggregation=variableaggregation none       string     if initializer be none        initializer = self  initializer     if regularizer be none        regularizer = self  regularizer     if constraint be none        constraint = self  constraint     if cache device be none        cache device = self  cache device     if partitioner be none        partitioner = self  partitioner     if dtype be none        dtype = self  dtype     if use resource be none        use resource = self  use resource      if self  custom getter be not none        raise valueerror            string           string           string           string           string   self  custom getter       if partitioner be none        raise valueerror string                      full name list =        if self name        full name list append self name      if name        full name list append name      full name = string join full name list                 with ops name scope none                return var store  get partition variable            full name            shape=shape            dtype=dtype            initializer=initializer            regularizer=regularizer            reuse=self reuse            trainable=trainable            collections=collections            cache device=caching device            partitioner=partitioner            validate shape=validate shape            use resource=use resource            constraint=constraint            synchronization=synchronization            aggregation=aggregation  
class wholefilereader readerbase     string     deprecation deprecate        none  string       string    def   init   self  name=none       string     rr = gen io ops whole file reader v2 name=name      super wholefilereader  self    init   rr  support serialize=true  
 tf export v1= string   def add check numerics ops      string   if context execute eagerly        raise runtimeerror          string         string         string         string     check op =                  for op in ops get default graph   get operations        for output in op output        if output dtype in  dtypes float16  dtypes float32  dtypes float64           if op  get control flow context   be not none              raise valueerror string                            string                            string           message = op name   string   str output value index          with ops control dependencies check op             check op =  array ops check numerics output  message=message     return control flow ops group  check op  
 tf export v1= string   def add to collection name  value     string   get default graph   add to collection name  value  
 tf export v1= string   def add to collections name  value     string   get default graph   add to collections name  value  
 tf export v1= string    deprecate string  string  def all variables      string   return global variables   
def arg max input  dimension  output type= dtypes int64  name=none     r   return the index with the largest value across dimension of a tensor     note that in case of tie the identity of the return value be not guarantee     usage      ```python     import tensorflow as tf     a =  1  10  26 9  2 8  166 32  62 3      b = tf math argmax input = a      c = tf keras backend eval b        c = 4       here a 4  = 166 32 which be the largest element of a across axis 0     ```    args      input  a `tensor`  must be one of the follow type  `float32`  `float64`  `int32`  `uint8`  `int16`  `int8`  `complex64`  `int64`  `qint8`  `quint8`  `qint32`  `bfloat16`  `uint16`  `complex128`  `half`  `uint32`  `uint64`      dimension  a `tensor`  must be one of the follow type  `int32`  `int64`        int32 or int64  must be in the range ` -rank input   rank input  `        describe which dimension of the input tensor to reduce across  for vectors        use dimension = 0      output type  an optional `tf dtype` from  `tf int32  tf int64`  default to `tf int64`      name  a name for the operation  optional      return      a `tensor` of type `output type`           ctx =  context  context or  context context     tld =  ctx  thread local data   if tld be eager      try         result =  pywrap tensorflow tfe py fastpathexecute           ctx  context handle  tld device name  string  name          tld op callbacks  input  dimension  string  output type        return  result     except  core  fallbackexception        try          return arg max eager fallback              input  dimension  output type=output type  name=name  ctx= ctx        except  core  symbolicexception          pass       except  core  notokstatusexception as e         ops raise from not ok status e  name       if output type be none      output type =  dtypes int64   output type =  execute make type output type  string           op   output =  op def library  apply op helper          string  input=input  dimension=dimension  output type=output type                    name=name     result =  output      if  execute must record gradient         attrs =  string   op  get attr type string   string                 op  get attr type string   string                 op  get attr type string        input flat =  op input      execute record gradient          string   input flat   attrs   result     result  =  result   return  result 
def arg min input  dimension  output type= dtypes int64  name=none     r   return the index with the smallest value across dimension of a tensor     note that in case of tie the identity of the return value be not guarantee     usage      ```python     import tensorflow as tf     a =  1  10  26 9  2 8  166 32  62 3      b = tf math argmin input = a      c = tf keras backend eval b        c = 0       here a 0  = 1 which be the smallest element of a across axis 0     ```    args      input  a `tensor`  must be one of the follow type  `float32`  `float64`  `int32`  `uint8`  `int16`  `int8`  `complex64`  `int64`  `qint8`  `quint8`  `qint32`  `bfloat16`  `uint16`  `complex128`  `half`  `uint32`  `uint64`      dimension  a `tensor`  must be one of the follow type  `int32`  `int64`        int32 or int64  must be in the range ` -rank input   rank input  `        describe which dimension of the input tensor to reduce across  for vectors        use dimension = 0      output type  an optional `tf dtype` from  `tf int32  tf int64`  default to `tf int64`      name  a name for the operation  optional      return      a `tensor` of type `output type`           ctx =  context  context or  context context     tld =  ctx  thread local data   if tld be eager      try         result =  pywrap tensorflow tfe py fastpathexecute           ctx  context handle  tld device name  string  name          tld op callbacks  input  dimension  string  output type        return  result     except  core  fallbackexception        try          return arg min eager fallback              input  dimension  output type=output type  name=name  ctx= ctx        except  core  symbolicexception          pass       except  core  notokstatusexception as e         ops raise from not ok status e  name       if output type be none      output type =  dtypes int64   output type =  execute make type output type  string           op   output =  op def library  apply op helper          string  input=input  dimension=dimension  output type=output type                    name=name     result =  output      if  execute must record gradient         attrs =  string   op  get attr type string   string                 op  get attr type string   string                 op  get attr type string        input flat =  op input      execute record gradient          string   input flat   attrs   result     result  =  result   return  result 
 tf export v1= string  string    deprecation deprecate args none  string                               string    set doc      gen math ops arg max   doc   replace string                                           string  replace string  string   def argmax input             axis=none             name=none             dimension=none             output type=dtypes int64     axis = deprecation deprecate argument lookup string  axis  string                                                  dimension    return argmax v2 input  axis  output type  name  
 tf export v1= string  string    deprecation deprecate args none  string                               string    set doc      gen math ops arg min   doc   replace string                                           string  replace string  string   def argmin input             axis=none             name=none             dimension=none             output type=dtypes int64     axis = deprecation deprecate argument lookup string  axis  string                                                  dimension    return argmin v2 input  axis  output type  name  
 tf export v1= string  string     binary assert doc string  def assert equal x  y  data=none  summarize=none  message=none  name=none       with ops name scope name  string   x  y  data             if x be y        return none if context execute eagerly   else control flow ops no op     return  binary assert string  string  math ops equal  np equal  x  y                          data  summarize  message  name  
 tf export v1= string  string     binary assert doc string  def assert greater x  y  data=none  summarize=none  message=none  name=none       return  binary assert string  string  math ops greater  np greater  x                          y  data  summarize  message  name  
 tf export v1= string  string    deprecation deprecate endpoints string    binary assert doc string  def assert greater equal x  y  data=none  summarize=none  message=none                           name=none     return  binary assert string  string  math ops greater equal                          np greater equal  x  y  data  summarize  message  name  
 tf export v1= string  string    deprecation deprecate endpoints string  def assert integer x  message=none  name=none     string   message = message or string   with ops name scope name  string   x        x = ops convert to tensor x  name=string      if not x dtype be integer        if context execute eagerly            name = string       else          name = x name       err msg =             string              message  name  x dtype         raise typeerror err msg       return control flow ops no op string  
 tf export v1= string  string     binary assert doc string  def assert less x  y  data=none  summarize=none  message=none  name=none     return  binary assert string  string  math ops less  np less  x  y  data                          summarize  message  name  
 tf export v1= string  string    deprecation deprecate endpoints string    binary assert doc string  def assert less equal x  y  data=none  summarize=none  message=none  name=none     return  binary assert string  string  math ops less equal                          np less equal  x  y  data  summarize  message  name  
 tf export v1= string  string    deprecation deprecate endpoints string  def assert near      x  y  rtol=none  atol=none  data=none  summarize=none  message=none      name=none     string   message = message or string   with ops name scope name  string   x  y  rtol  atol  data        x = ops convert to tensor x  name=string      y = ops convert to tensor y  name=string  dtype=x dtype       eps = np finfo x dtype as numpy dtype  eps     rtol = 10   eps if rtol be none else rtol     atol = 10   eps if atol be none else atol      rtol = ops convert to tensor rtol  name=string  dtype=x dtype      atol = ops convert to tensor atol  name=string  dtype=x dtype       if context execute eagerly          x name =  shape and dtype str x        y name =  shape and dtype str y      else        x name = x name       y name = y name      if data be none        data =             message            string    rtol  atol             string   x name  x  string   y name  y             tol = atol   rtol   math ops abs y      diff = math ops abs x - y      condition = math ops reduce all math ops less diff  tol       return control flow ops assert condition  data  summarize=summarize  
 tf export v1= string  string    deprecation deprecate endpoints string    unary assert doc string  string  def assert negative x  data=none  summarize=none  message=none  name=none       message = message or string   with ops name scope name  string   x  data        x = ops convert to tensor x  name=string      if data be none        if context execute eagerly            name =  shape and dtype str x        else          name = x name       data =             message            string            string   name  x      zero = ops convert to tensor 0  dtype=x dtype      return assert less x  zero  data=data  summarize=summarize  
 tf export v1= string  string    deprecation deprecate endpoints string    unary assert doc string  string  def assert non negative x  data=none  summarize=none  message=none  name=none       message = message or string   with ops name scope name  string   x  data        x = ops convert to tensor x  name=string      if data be none        if context execute eagerly            name =  shape and dtype str x        else          name = x name       data =             message            string            string   name  x      zero = ops convert to tensor 0  dtype=x dtype      return assert less equal zero  x  data=data  summarize=summarize  
 tf export v1= string  string    deprecation deprecate endpoints string    unary assert doc string  string  def assert non positive x  data=none  summarize=none  message=none  name=none       message = message or string   with ops name scope name  string   x  data        x = ops convert to tensor x  name=string      if data be none        if context execute eagerly            name =  shape and dtype str x        else          name = x name       data =             message            string           string   name  x      zero = ops convert to tensor 0  dtype=x dtype      return assert less equal x  zero  data=data  summarize=summarize  
 tf export v1= string  string    deprecation deprecate endpoints string    binary assert doc string  def assert none equal      x  y  data=none  summarize=none  message=none  name=none     return  binary assert string  string  math ops not equal                          np not equal  x  y  data  summarize  message  name  
 tf export v1= string  string    deprecation deprecate endpoints string    unary assert doc string  string  def assert positive x  data=none  summarize=none  message=none  name=none       message = message or string   with ops name scope name  string   x  data        x = ops convert to tensor x  name=string      if data be none        if context execute eagerly            name =  shape and dtype str x        else          name = x name       data =             message  string            string   name  x      zero = ops convert to tensor 0  dtype=x dtype      return assert less zero  x  data=data  summarize=summarize  
 tf export v1= string  string   def assert rank x  rank  data=none  summarize=none  message=none  name=none     string   with ops name scope name  string   x  rank    tuple data or           x = ops convert to tensor x  name=string      rank = ops convert to tensor rank  name=string      message = message or string      static condition = lambda actual rank  give rank  actual rank == give rank     dynamic condition = math ops equal      if context execute eagerly          name = string     else        name = x name      if data be none        data =             message            string   name  rank  string            array ops shape x               try        assert op =  assert rank condition x  rank  static condition                                           dynamic condition  data  summarize       except valueerror as e        if e args 0  == string          raise valueerror              string                message  name  e args 2   e args 1   x get shape           else          raise    return assert op 
 tf export v1= string  string    deprecation deprecate endpoints string  def assert rank at least      x  rank  data=none  summarize=none  message=none  name=none     string   with ops name scope        name  string   x  rank    tuple data or           x = ops convert to tensor x  name=string      rank = ops convert to tensor rank  name=string      message = message or string      static condition = lambda actual rank  give rank  actual rank >= give rank     dynamic condition = math ops greater equal      if context execute eagerly          name = string     else        name = x name      if data be none        data =             message            string   name  rank            string  array ops shape x               try        assert op =  assert rank condition x  rank  static condition                                           dynamic condition  data  summarize       except valueerror as e        if e args 0  == string          raise valueerror              string             string    message  name  e args 2   e args 1   x get shape           else          raise    return assert op 
 tf export v1= string  string    deprecation deprecate endpoints string  def assert rank in      x  rank  data=none  summarize=none  message=none  name=none     string   with ops name scope        name  string   x     tuple rank    tuple data or           x = ops convert to tensor x  name=string      rank = tuple  ops convert to tensor rank  name=string  for rank in rank       message = message or string      if context execute eagerly          name = string     else        name = x name      if data be none        data =             message  string   name           list rank                string  array ops shape x               try        assert op =  assert rank condition x  rank   static rank in                                             dynamic rank in  data  summarize       except valueerror as e        if e args 0  == string          raise valueerror              string             string    message  name  e args 2   e args 1   x get shape           else          raise    return assert op 
 tf export v1= string  string    deprecation deprecate endpoints string  def assert scalar tensor  name=none  message=none     string   with ops name scope name  string   tensor   as name scope      tensor = ops convert to tensor tensor  name=name scope      shape = tensor get shape       if shape ndims  = 0        if context execute eagerly            raise valueerror string                             message or string  shape          else          raise valueerror string                             message or string  tensor name  shape       return tensor 
 tf export v1= string  string    deprecation deprecate endpoints string  def assert type tensor  tf type  message=none  name=none     string   message = message or string   with ops name scope name  string   tensor        tensor = ops convert to tensor tensor  name=string      if tensor dtype  = tf type        if context execute eagerly            raise typeerror string    message  tf type         else          raise typeerror string    message  tensor name                                                         tf type        return control flow ops no op string  
 tf export v1= string    tf should use should use result def assert variables initialize var list=none     string   if var list be none      var list = global variables     local variables        if not var list      var list =        for op in ops get default graph   get operations          if op type in  string  string  string           var list append op output 0     if not var list      return none   else      rank =        for var in var list        with ops colocate with var op           rank append array ops rank internal var  optimize=false       if len rank  == 1        return rank 0      else        return array ops stack rank  
 tf export v1= string   def assign ref  value  validate shape=none  use locking=none  name=none     string   if ref dtype  be ref dtype      return gen state ops assign          ref  value  use locking=use lock  name=name          validate shape=validate shape    return ref assign value  name=name  
 tf export v1= string   def assign add ref  value  use locking=none  name=none     string   if ref dtype  be ref dtype      return gen state ops assign add          ref  value  use locking=use lock  name=name    return ref assign add value  
 tf export v1= string   def assign sub ref  value  use locking=none  name=none     string   if ref dtype  be ref dtype      return gen state ops assign sub          ref  value  use locking=use lock  name=name    return ref assign sub value  
 tf export v1= string    dispatch add dispatch support  deprecation deprecate      string  string     string    def batch gather params  indices  name=none     string   with ops name scope name  string   params  indices        indices = ops convert to tensor indices  name=string      params = ops convert to tensor params  name=string      if indices shape ndims be none        raise valueerror            string      return  batch gather params  indices  batch dims=indices shape ndims - 1  
 tf export v1= string    deprecation deprecate      string  string  def batch scatter update ref  indices  update  use locking=true  name=none     string   with ops name scope name       indices = ops convert to tensor indices  name=string      indices shape = array ops shape indices      indices dimension = indices get shape   ndims      if indices dimension be none        raise valueerror string                        string       nd indices = array ops expand dim indices  axis=-1      nd indices list =                                                            for dimension in range indices dimension - 1                                                   dimension size = indices shape dimension        shape to broadcast =  1     indices dimension   1        shape to broadcast dimension  = dimension size       dimension range = array ops reshape            gen math ops  range 0  dimension size  1   shape to broadcast        if dimension range dtype base dtype  = nd indices dtype          dimension range = gen math ops cast dimension range  nd indices dtype        nd indices list append            dimension range   array ops ones like nd indices            nd indices list append nd indices      final indices = array ops concat nd indices list  axis=-1      return scatter nd update          ref  final indices  update  use locking=use lock  
 tf export v1= string   def batch to space input  crop  block size  name=none  block shape=none       block size = deprecation deprecate argument lookup string                                                        block shape  string                                                        block size    result = batch to space nd        input        crops=crops        block shape=np array  block size  block size   dtype=np int64         name=name    result set shape result get shape   with rank 4     return result 
  dispatch add dispatch list  tf export v1= string  string    deprecate endpoints string  string  def batch to space nd input  block shape  crop  name=none     r   batchtospace for n-d tensors of type t     this operation reshape the  batch  dimension 0 into `m   1` dimension of shape   `block shape    batch `  interleave these block back into the grid define by   the spatial dimension ` 1       m `  to obtain a result with the same rank as   the input   the spatial dimension of this intermediate result be then   optionally crop accord to `crops` to produce the output   this be the   reverse of spacetobatch   see below for a precise description     args      input  a `tensor`        n-d with shape `input shape =  batch    spatial shape   remain shape`        where spatial shape have m dimension      block shape  a `tensor`  must be one of the follow type  `int32`  `int64`        1-d with shape ` m `  all value must be >= 1      crop  a `tensor`  must be one of the follow type  `int32`  `int64`        2-d with shape ` m  2 `  all value must be >= 0          `crops i  =  crop start  crop end ` specify the amount to crop from input         dimension `i   1`  which correspond to spatial dimension `i`   it be         require that         `crop start i    crop end i  <= block shape i    input shape i   1 `         this operation be equivalent to the follow step         1  reshape `input` to `reshaped` of shape              block shape 0        block shape m-1               batch / prod block shape               input shape 1        input shape n-1          2  permute dimension of `reshaped` to produce `permuted` of shape             batch / prod block shape                input shape 1   block shape 0                                input shape m   block shape m-1                input shape m 1        input shape n-1          3  reshape `permuted` to produce `reshaped permuted` of shape             batch / prod block shape                input shape 1    block shape 0                                input shape m    block shape m-1                input shape m 1                                input shape n-1          4  crop the start and end of dimension ` 1       m ` of          `reshaped permuted` accord to `crops` to produce the output of shape              batch / prod block shape                input shape 1    block shape 0  - crop 0 0  - crop 0 1                                input shape m    block shape m-1  - crop m-1 0  - crop m-1 1                input shape m 1        input shape n-1          some examples          1  for the follow input of shape ` 4  1  1  1 `  `block shape =  2  2 `  and           `crops =   0  0    0  0  `         ```           1        2        3        4           ```        the output tensor have shape ` 1  2  2  1 ` and value         ```       x =     1    2      3    4           ```         2  for the follow input of shape ` 4  1  1  3 `  `block shape =  2  2 `  and           `crops =   0  0    0  0  `         ```           1  2  3        4  5  6        7  8  9        10  11  12           ```        the output tensor have shape ` 1  2  2  3 ` and value         ```       x =     1  2  3    4  5  6                  7  8  9    10  11  12           ```         3  for the follow input of shape ` 4  2  2  1 `  `block shape =  2  2 `  and           `crops =   0  0    0  0  `         ```       x =     1    3      9    11                   2    4      10    12                   5    7      13    15                   6    8      14    16           ```        the output tensor have shape ` 1  4  4  1 ` and value         ```       x =     1      2     3     4                 5      6     7     8                 9     10    11     12                 13    14    15     16           ```         4  for the follow input of shape ` 8  1  3  1 `  `block shape =  2  2 `  and           `crops =   0  0    2  0  `         ```       x =     0    1    3        0    9    11                   0    2    4        0    10    12                   0    5    7        0    13    15                   0    6    8        0    14    16           ```        the output tensor have shape ` 2  2  4  1 ` and value         ```       x =     1      2     3     4                  5      6     7     8                   9     10    11     12                  13    14    15     16           ```     name  a name for the operation  optional      return      a `tensor`  have the same type as `input`           ctx =  context  context or  context context     tld =  ctx  thread local data   if tld be eager      try         result =  pywrap tensorflow tfe py fastpathexecute           ctx  context handle  tld device name  string  name          tld op callbacks  input  block shape  crop        return  result     except  core  fallbackexception        try          return batch to space nd eager fallback              input  block shape  crop  name=name  ctx= ctx        except  core  symbolicexception          pass         except  typeerror  valueerror           result =  dispatch dispatch                batch to space nd  input=input  block shape=block shape                                   crops=crops  name=name          if result be not  dispatch opdispatcher not support            return result         raise     except  core  notokstatusexception as e         ops raise from not ok status e  name       try             op   output =  op def library  apply op helper          string  input=input  block shape=block shape  crops=crops                            name=name    except  typeerror  valueerror       result =  dispatch dispatch            batch to space nd  input=input  block shape=block shape                               crops=crops  name=name      if result be not  dispatch opdispatcher not support        return result     raise    result =  output      if  execute must record gradient         attrs =  string   op  get attr type string   string                 op  get attr type string   string                 op  get attr type string        input flat =  op input      execute record gradient          string   input flat   attrs   result     result  =  result   return  result 
 tf export v1= string  string    deprecation deprecate endpoints string  def bincount v1 arr                  weights=none                  minlength=none                  maxlength=none                  dtype=dtypes int32     string   return bincount arr  weight  minlength  maxlength  dtype  
 tf export v1= string   def boolean mask tensor  mask  name=string  axis=none     string    def  apply mask 1d reshape tensor  mask  axis=none       string     indices = squeeze where v2 mask   axis= 1       return gather reshape tensor  indices  axis=axis     with ops name scope name  values= tensor  mask        tensor = ops convert to tensor tensor  name=string      mask = ops convert to tensor mask  name=string       shape mask = mask get shape       ndims mask = shape mask ndims     shape tensor = tensor get shape       if ndims mask == 0        raise valueerror string      if ndims mask be none        raise valueerror            string           string      axis = 0 if axis be none else axis     shape tensor axis axis   ndims mask  assert be compatible with shape mask       lead size = gen math ops prod shape tensor  axis axis   ndims mask    0       tensor = reshape          tensor          concat               shape tensor   axis    lead size               shape tensor  axis   ndims mask              0       first dim = shape tensor axis axis   ndims mask  num elements       tensor set shape          tensor shape as shape shape tensor  axis   concatenate               first dim   concatenate shape tensor axis   ndims mask          mask = reshape mask   -1       return  apply mask 1d tensor  mask  axis  
 tf export v1= string   def case pred fn pair           default=none           exclusive=false           strict=false           name=string     string   return  case helper        cond        pred fn pair        default        exclusive        name        allow python preds=false        strict=strict  
 deprecation deprecate      date=none      instructions=string     string     string   tf export v1= string   def clip by average norm t  clip norm  name=none     string   with ops name scope name  string   t  clip norm   as name      t = ops convert to tensor t  name=string                 n element = math ops cast array ops size t   dtypes float32      l2norm inv = math ops rsqrt          math ops reduce sum t   t  math ops range array ops rank t         tclip = array ops identity          t   clip norm   math ops minimum              l2norm inv   n element  constant op constant 1 0  / clip norm           name=name     return tclip 
 deprecation deprecate      date=none  instructions=string   tf export v1= string   def  colocate with op  ignore existing=false     return colocate with op  ignore exist  
 tf export v1= string    deprecation deprecate args      none  string      string  string  def cond pred           true fn=none           false fn=none           strict=false           name=none           fn1=none           fn2=none     string      if  util enablecontrolflowv2 ops get default graph    and       not context execute eagerly         return cond v2 cond v2 pred  true fn  false fn  name                    if fn1 be not none      if true fn be not none        raise typeerror string      true fn = fn1   elif true fn be none      raise typeerror string    if fn2 be not none      if false fn be not none        raise typeerror string      false fn = fn2   elif false fn be none      raise typeerror string     if not callable true fn       raise typeerror string    if not callable false fn       raise typeerror string     with ops name scope name  string   pred        if context execute eagerly          if pred          result = true fn         else          result = false fn         if not strict          result =  unpackifsingleton result        return result           if isinstance pred  bool         raise typeerror string      p 2  p 1 = switch pred  pred      pivot 1 = array ops identity p 1  name=string      pivot 2 = array ops identity p 2  name=string      pred = array ops identity pred  name=string           for tensor in  p 1  p 2  pivot 1  pivot 2  pred         tensor op graph prevent fetch tensor op            context t = condcontext pred  pivot 1  branch=1      try        context t enter         orig res t  res t = context t buildcondbranch true fn        if orig res t be none          raise valueerror string        context t exitresult res t      finally        context t exit             context f = condcontext pred  pivot 2  branch=0      try        context f enter         orig res f  res f = context f buildcondbranch false fn        if orig res f be none          raise valueerror string        context f exitresult res f      finally        context f exit        if not strict        orig res t =  unpackifsingleton orig res t        orig res f =  unpackifsingleton orig res f            try        nest assert same structure orig res t  orig res f  expand composites=true      except  typeerror  valueerror         nest map structure  cast index slice indices  orig res t  orig res f        nest map structure  cast index slice indices  res t  res f        try          nest assert same structure orig res t  orig res f                                     expand composites=true        except typeerror as e          raise typeerror              string format e         except valueerror as e          raise valueerror              string format e             if not res t        raise valueerror string       res t flat = nest flatten res t  expand composites=true      res f flat = nest flatten res f  expand composites=true       for  x  y  in zip res t flat  res f flat         assert isinstance x  ops tensor  and isinstance y  ops tensor        if x dtype base dtype  = y dtype base dtype          raise valueerror              string             string    x dtype name  y dtype name        merge =  merge pair  0  for pair in zip res f flat  res t flat       merge =  convert flow to tensorarrays          nest flatten orig res t  expand composites=true   merge                 assert context t outer context == context f outer context     if context t outer context be none        ops add to collection ops graphkeys cond context  context t        ops add to collection ops graphkeys cond context  context f       merge = nest pack sequence as          structure=orig res t  flat sequence=merges  expand composites=true            if not strict        merge =  unpackifsingleton merge      return merge 
 tf export v1= string  string    deprecation deprecate endpoints string  string  def confusion matrix v1 label                          predictions                          num classes=none                          dtype=dtypes int32                          name=none                          weights=none     string   return confusion matrix label  predictions  num class  weight  dtype                            name  
 tf export v1= string   def constant v1      value  dtype=none  shape=none  name=string  verify shape=false     string   return  constant impl value  dtype  shape  name  verify shape=verify shape                          allow broadcast=false  
 tf export v1= string   def container container name     string   return get default graph   container container name  
 tf export v1= string   def control flow v2 enable        string   return control flow util enablecontrolflowv2 ops get default graph    
 tf export v1= string   def convert to tensor v1 value                           dtype=none                           name=none                           prefer dtype=none                           dtype hint=none     string   prefer dtype = deprecation deprecate argument lookup        string  dtype hint  string  prefer dtype    return convert to tensor v2 value  dtype  prefer dtype  name  
 tf export v1= string   def convert to tensor or index slice value  dtype=none  name=none     string   return internal convert to tensor or index slice        value=value  dtype=dtype  name=name  as ref=false  
 tf export v1= string   def convert to tensor or sparse tensor value  dtype=none  name=none     string   if dtype be not none      dtype = dtypes as dtype dtype    if isinstance value  sparsetensorvalue       value = sparsetensor from value value    if isinstance value  sparsetensor       if dtype and not dtype be compatible with value dtype         raise runtimeerror string                             dtype name  value dtype name       return value   return ops convert to tensor value  dtype=dtype  name=name  
 tf export v1= string  string    deprecation deprecate args none                               string                               string   deprecation deprecate args      none  string  string  def count nonzero input tensor=none                    axis=none                    keepdims=none                    dtype=dtypes int64                    name=none                    reduction indices=none                    keep dims=none                    input=none       string   keepdims = deprecation deprecate argument lookup string  keepdims                                                      string  keep dim    input tensor = deprecation deprecate argument lookup string  input                                                          string                                                          input tensor    axis = deprecation deprecate argument lookup string  axis                                                  string                                                  reduction indices     return count nonzero v2 input tensor  axis  keepdims  dtype  name  
 tf export v1= string    deprecate none  string  def count up to ref  limit  name=none     r   increments  ref  until it reach  limit      args      ref  a variable  must be one of the follow type  `int32`  `int64`        should be from a scalar `variable` node      limit  an `int`        if incrementing ref would bring it above limit  instead generate an        outofrange  error      name  a name for the operation  optional      return      a `tensor`  have the same type as `ref`      a copy of the input before increment  if nothing else modify the     input  the value produce will all be distinct          if ref dtype  be ref dtype      return gen state ops count up to ref  limit=limit  name=name    return gen state ops resource count up to        ref handle  limit  t=ref dtype  name=name  
 tf export v1= string    deprecation deprecate      date=none      instructions=string  def create partition variables      shape  slice  initializer  dtype=dtypes float32      trainable=true  collections=none  name=none  reuse=none     string   if len shape   = len slice       raise valueerror string                      string                         shape  slice     if len shape  < 1      raise valueerror string                      string   shape           partitioner = lambda   unused kwargs  slice    with variable scope variable scope        name  string  reuse=reuse            partition var = variable scope  get partition variable          name=none          shape=shape          dtype=dtype          initializer=initializer          trainable=trainable          partitioner=partitioner          collections=collections      return list partition var  
 tf export v1= string  string    deprecation deprecate endpoints string  def decode csv record                 record default                 field delim=string                 use quote delim=true                 name=none                 na value=string                 select cols=none     string   return decode csv v2        record  record default        field delim  use quote delim        na value  select cols  name         
 tf export v1= string  string    deprecation deprecate args none                               string                               string  def decode raw v1      input bytes=none      out type=none      little endian=true      name=none      bytes=none        string   input bytes = deprecation deprecate argument lookup string                                                         input bytes  string                                                         bytes              if out type be none      raise valueerror          string     return gen parse ops decode raw        input bytes  out type  little endian=little endian  name=name  
 tf export v1= string   def delete session tensor handle  name=none     string   handle device = tensorhandle  get device name handle    with ops device handle device       holder = array ops placeholder dtypes string      deleter = gen data flow ops delete session tensor holder  name=name    return  holder  deleter  
 tf export v1= string  string    deprecation deprecate endpoints string  def depth to space input  block size  name=none  data format=string       return gen array ops depth to space input  block size  data format  name=name  
 tf export v1= string   def device device name or function     string   if context execute eagerly        if callable device name or function         raise runtimeerror            string           string      return context device device name or function    elif execute eagerly outside function         tf contextlib contextmanager     def combine device name or function         with get default graph   device device name or function           if not callable device name or function             with context device device name or function               yield         else            yield     return combine device name or function    else      return get default graph   device device name or function  
 tf export v1= string   def disable control flow v2        string   control flow util enable control flow v2 = false 
 tf export v1= string   def disable eager execution      string    api usage gauge get cell   set false    context default execution mode = context graph mode   c = context context safe     if c be not none      c  thread local data be eager = false   
 deprecation deprecate      none  string   tf export v1= string   def disable resource variables      string   global  default use resource    default use resource = false    api usage gauge get cell   set false  
 tf export v1= string   def disable tensor equality      string   tensor  use equality = false   
 tf export v1= string   def disable v2 behavior      string   tf2 disable     ops disable eager execution     tensor shape disable v2 tensorshape       variable scope disable resource variables     ops disable tensor equality        control flow v2 toggle disable control flow v2        dataset ops dataset = dataset ops datasetv1   readers fixedlengthrecorddataset = readers fixedlengthrecorddatasetv1   readers tfrecorddataset = readers tfrecorddatasetv1   readers textlinedataset = readers textlinedatasetv1   counter counter = counter counterv1   interleave ops choose from datasets = interleave ops choose from datasets v1   interleave ops sample from datasets = interleave ops sample from datasets v1   random ops randomdataset = random ops randomdatasetv1   exp readers csvdataset = exp readers csvdatasetv1   exp readers sqldataset = exp readers sqldatasetv1   exp readers make batch feature dataset =         exp readers make batch feature dataset v1    exp readers make csv dataset = exp readers make csv dataset v1 
 tf export v1= string   def disable v2 tensorshape      string   global  tensorshape v2 override      tensorshape v2 override = false    api usage gauge get cell   set false  
 deprecation deprecate      date=none      instructions=string   tf export v1= string   def div x  y  name=none     string   return  div python2 x  y  name  
 tf export v1= string   def enable control flow v2        string   control flow util enable control flow v2 = true 
 tf export v1= string   def enable eager execution config=none  device policy=none                             execution mode=none     string    api usage gauge get cell   set true    if context default execution mode  = context eager mode      return enable eager execution internal          config=config          device policy=device policy          execution mode=execution mode          server def=none  
 tf export v1= string   def enable resource variables      string   global  default use resource    default use resource = true    api usage gauge get cell   set true  
 tf export v1= string   def enable tensor equality      string   tensor  use equality = true   
 tf export v1= string   def enable v2 behavior      string            tf2 enable     ops enable eager execution     tensor shape enable v2 tensorshape       variable scope enable resource variables     ops enable tensor equality        control flow v2 toggle enable control flow v2        dataset ops dataset = dataset ops datasetv2   readers fixedlengthrecorddataset = readers fixedlengthrecorddatasetv2   readers tfrecorddataset = readers tfrecorddatasetv2   readers textlinedataset = readers textlinedatasetv2   counter counter = counter counterv2   interleave ops choose from datasets = interleave ops choose from datasets v2   interleave ops sample from datasets = interleave ops sample from datasets v2   random ops randomdataset = random ops randomdatasetv2   exp readers csvdataset = exp readers csvdatasetv2   exp readers sqldataset = exp readers sqldatasetv2   exp readers make batch feature dataset =         exp readers make batch feature dataset v2    exp readers make csv dataset = exp readers make csv dataset v2 
 tf export v1= string   def enable v2 tensorshape      string   global  tensorshape v2 override      tensorshape v2 override = true    api usage gauge get cell   set true  
 tf export v1= string   def execute eagerly v1      string   return execute eagerly   
 tf export v1= string    dispatch add dispatch support  deprecation deprecate args none  string  string  def expand dim input  axis=none  name=none  dim=none     string   axis = deprecation deprecate argument lookup string  axis  string  dim    if axis be none      raise valueerror string    return expand dim v2 input  axis  name  
 tf export v1= string  string    deprecation deprecate args none  string                               string  def extract image patch        image      ksizes=none      strides=none      rates=none      padding=none      name=none      sizes=none     ksizes = deprecation deprecate argument lookup string  size  string                                                    ksizes    return gen array ops extract image patch image  ksizes  stride  rat                                               pad  name  
 tf export v1= string   def fix size partitioner num shards  axis=0     string   def  partitioner shape    unused args       partition list =  1    len shape      partition list axis  = min num shards  shape dim axis  value      return partition list   return  partitioner 
  dispatch add dispatch list  tf export v1= string    deprecate endpoints string  def floor div x  y  name=none     r   return x // y element-wise      note   `floor div` support broadcast  more about broadcast    here  http //docs scipy org/doc/numpy/user/basics broadcast html     args      x  a `tensor`  must be one of the follow type  `bfloat16`  `half`  `float32`  `float64`  `uint8`  `int8`  `uint16`  `int16`  `int32`  `int64`  `complex64`  `complex128`      y  a `tensor`  must have the same type as `x`      name  a name for the operation  optional      return      a `tensor`  have the same type as `x`           ctx =  context  context or  context context     tld =  ctx  thread local data   if tld be eager      try         result =  pywrap tensorflow tfe py fastpathexecute           ctx  context handle  tld device name  string  name          tld op callbacks  x  y        return  result     except  core  fallbackexception        try          return floor div eager fallback              x  y  name=name  ctx= ctx        except  core  symbolicexception          pass         except  typeerror  valueerror           result =  dispatch dispatch                floor div  x=x  y=y  name=name          if result be not  dispatch opdispatcher not support            return result         raise     except  core  notokstatusexception as e         ops raise from not ok status e  name       try             op   output =  op def library  apply op helper          string  x=x  y=y  name=name    except  typeerror  valueerror       result =  dispatch dispatch            floor div  x=x  y=y  name=name      if result be not  dispatch opdispatcher not support        return result     raise    result =  output      if  execute must record gradient         attrs =  string   op  get attr type string        input flat =  op input      execute record gradient          string   input flat   attrs   result     result  =  result   return  result 
 tf export string  def foldl fn            elems            initializer=none            parallel iterations=10            back prop=true            swap memory=false            name=none     string   if not callable fn       raise typeerror string     def create ta elem       return tensor array ops tensorarray          dtype=elem dtype  size=n  dynamic size=false          infer shape=true  unstack elem     in graph mode = not context execute eagerly     with ops name scope name  string   elems                  if in graph mode                      varscope = vs get variable scope         varscope cache device be none = false       if varscope cache device be none                            varscope set cache device lambda op  op device          varscope cache device be none = true           elems flat =           ops convert to tensor elem  name=string  for elem in nest flatten elems            n =           tensor shape dimension value elems flat 0  shape 0   or         array ops shape elems flat 0   0        elems ta = nest map structure create ta  elems       if initializer be none        a = nest map structure lambda elem  elem read 0   elems ta        i = constant op constant 1      else        a = initializer       i = constant op constant 0       def compute i  a         elem i = nest map structure lambda elem  elem read i   elems ta        a = fn a  elem i        return  i   1  a          r a = control flow ops while loop          lambda i  a  i < n          compute   i  a           parallel iterations=parallel iterations          back prop=back prop          swap memory=swap memory          maximum iterations=n                 if in graph mode and varscope cache device be none        varscope set cache device none       return r a 
 tf export string  def foldr fn            elems            initializer=none            parallel iterations=10            back prop=true            swap memory=false            name=none     string   if not callable fn       raise typeerror string     def create ta elem       return tensor array ops tensorarray          dtype=elem dtype  size=n  dynamic size=false          infer shape=true  unstack elem     in graph mode = not context execute eagerly     with ops name scope name  string   elems                  if in graph mode                      varscope = vs get variable scope         varscope cache device be none = false       if varscope cache device be none                            varscope set cache device lambda op  op device          varscope cache device be none = true           elems flat =           ops convert to tensor elem  name=string  for elem in nest flatten elems            n =           tensor shape dimension value elems flat 0  shape 0   or         array ops shape elems flat 0   0        elems ta = nest map structure create ta  elems       if initializer be none        i = n - 1       a = nest map structure lambda elem  elem read i   elems ta      else        i = n       a = initializer      def compute i  a         i -= 1       elem = nest map structure lambda elem  elem read i   elems ta        a out = fn a  elem        return  i  a out          r a = control flow ops while loop          lambda i  a  i > 0          compute   i  a           parallel iterations=parallel iterations          back prop=back prop          swap memory=swap memory          maximum iterations=n                 if in graph mode and varscope cache device be none        varscope set cache device none       return r a 
 tf export v1= string    dispatch add dispatch support def gather params             indices             validate indices=none             name=none             axis=none             batch dims=0       r   gather slice from params axis `axis` accord to indices     gather slice from params axis `axis` accord to `indices`   `indices` must   be an integer tensor of any dimension  usually 0-d or 1-d      for 0-d  scalar  `indices`       \begin align     output p 0       p  axis-1                   p  axis   1        p  n-1   = \\   params p 0       p  axis-1      indices      p  axis   1        p  n-1     \end align        where  n  = `ndims params `     for 1-d  vector  `indices` with `batch dims=0`       \begin align     output p 0       p  axis-1               i     p  axis   1        p  n-1   =\\   params p 0       p  axis-1      indices  i     p  axis   1        p  n-1     \end align        in the general case  produce an output tensor where       \begin align     output p 0                    p  axis-1                                     i  b                  i  m-1                                       p  axis   1            p  n-1                            = \\   params p 0                    p  axis-1                                    indices p 0       p  b-1    i  b        i  m-1                      p  axis   1            p  n-1     \end align        where  n  = `ndims params `   m  = `ndims indices `  and  b  = `batch dims`    note that `params shape  batch dim ` must be identical to   `indices shape  batch dim `     the shape of the output tensor be     > `output shape = params shape  axis    indices shape batch dim       > params shape axis   1  `     note that on cpu  if an out of bind index be find  an error be return    on gpu  if an out of bind index be find  a 0 be store in the correspond   output value     see also `tf gather nd`     <div style= width 70   margin auto  margin-bottom 10px  margin-top 20px  >   <img style= width 100   src= https //www tensorflow org/images/gather png    alt>   </div>    args      params  the `tensor` from which to gather value  must be at least rank       `axis   1`      indices  the index `tensor`   must be one of the follow type  `int32`        `int64`  must be in range ` 0  params shape axis  `      validate indices  deprecate  do nothing      axis  a `tensor`  must be one of the follow type  `int32`  `int64`  the       `axis` in `params` to gather `indices` from  must be greater than or equal       to `batch dims`   default to the first non-batch dimension  support       negative index      batch dim  an `integer`   the number of batch dimension   must be less       than `rank indices `      name  a name for the operation  optional      return      a `tensor`  have the same type as `params`          del validate indices    if axis be none      axis = batch dim   if tensor util constant value axis   = 0      return gen array ops gather v2          params  indices  axis  batch dims=batch dim  name=name    try                return params sparse read indices  name=name    except attributeerror      return gen array ops gather v2 params  indices  axis  name=name  
 tf export v1= string  string    dispatch add dispatch support  deprecate endpoints string  def gather nd params  indices  name=none  batch dims=0     r   gather slice from `params` into a tensor with shape specify by `indices`     `indices` be an k-dimensional integer tensor  best think of as a    k-1 -dimensional tensor of indices into `params`  where each element define   a slice of `params`         output \\ i 0       i  k-2 \\   = params indices \\ i 0       i  k-2 \\       whereas in `tf gather` `indices` define slice into the first   dimension of `params`  in `tf gather nd`  `indices` define slice into the   first `n` dimension of `params`  where `n = indices shape -1 `     the last dimension of `indices` can be at most the rank of   `params`         indices shape -1  <= params rank    the last dimension of `indices` correspond to elements    if `indices shape -1  == params rank`  or slice    if `indices shape -1  < params rank`  along dimension `indices shape -1 `   of `params`   the output tensor have shape        indices shape  -1    params shape indices shape -1       additionally both  params  and  indices  can have m lead batch   dimension that exactly match  in this case  batch dim  must be m     note that on cpu  if an out of bind index be find  an error be return    on gpu  if an out of bind index be find  a 0 be store in the   correspond output value     some examples below     simple index into a matrix     ```python       indices =   0  0    1  1         params =    a    b      c    d          output =   a    d     ```    slice index into a matrix     ```python       indices =   1    0         params =    a    b      c    d          output =    c    d      a    b      ```    index into a 3-tensor     ```python       indices =   1         params =     a0    b0      c0    d0                        a1    b1      c1    d1           output =     a1    b1      c1    d1             indices =   0  1    1  0         params =     a0    b0      c0    d0                        a1    b1      c1    d1           output =    c0    d0      a1    b1            indices =   0  0  1    1  0  1         params =     a0    b0      c0    d0                        a1    b1      c1    d1           output =   b0    b1     ```    the examples below be for the case when only indices have lead extra   dimension  if both  params  and  indices  have lead batch dimension  use   the  batch dim  parameter to run gather nd in batch mode     batch index into a matrix     ```python       indices =    0  0      0  1          params =    a    b      c    d          output =    a      b      ```    batch slice index into a matrix     ```python       indices =    1      0          params =    a    b      c    d          output =     c    d        a    b       ```    batch index into a 3-tensor     ```python       indices =    1      0          params =     a0    b0      c0    d0                        a1    b1      c1    d1           output =      a1    b1      c1    d1                          a0    b0      c0    d0             indices =    0  1    1  0      0  0    1  1          params =     a0    b0      c0    d0                        a1    b1      c1    d1           output =     c0    d0      a1    b1                        a0    b0      c1    d1             indices =    0  0  1    1  0  1      0  1  1    1  1  0          params =     a0    b0      c0    d0                        a1    b1      c1    d1           output =    b0    b1      d0    c1      ```    examples with batch  params  and  indices      ```python       batch dim = 1       indices =   1    0         params =     a0    b0      c0    d0                        a1    b1      c1    d1           output =    c0    d0      a1    b1           batch dim = 1       indices =    1      0          params =     a0    b0      c0    d0                        a1    b1      c1    d1           output =     c0    d0        a1    b1            batch dim = 1       indices =    1  0      0  1          params =     a0    b0      c0    d0                        a1    b1      c1    d1           output =    c0      b1      ```    see also `tf gather`     args      params  a `tensor`  the tensor from which to gather value      indices  a `tensor`  must be one of the follow type  `int32`  `int64`        index tensor      name  a name for the operation  optional       batch dim  an integer or a scalar  tensor   the number of batch dimension     return      a `tensor`  have the same type as `params`          batch dim  = tensor util constant value batch dim    if batch dim  be not none      batch dim = int batch dim     if batch dim == 0      try                      return params gather nd indices  name=name      except attributeerror        return gen array ops gather nd params  indices  name=name    else      return batch gather nd params  indices  batch dims=batch dim  name=name  
 tf export v1= string   def get collection key  scope=none     string   return get default graph   get collection key  scope  
 tf export v1= string   def get collection ref key     string   return get default graph   get collection ref key  
 tf export v1= string   def get default graph      string   return  default graph stack get default   
 tf export v1= string   def get default session      string   return  default session stack get default   
 tf export v1= string   def get local variable        name      shape=none      dtype=none      initializer=none      regularizer=none      trainable=false        collections=none      cache device=none      partitioner=none      validate shape=true      use resource=none      custom getter=none      constraint=none      synchronization=variablesynchronization auto      aggregation=variableaggregation none     if collections      collections  =  ops graphkeys local variables    else      collections =  ops graphkeys local variables    return get variable        name        shape=shape        dtype=dtype        initializer=initializer        regularizer=regularizer        trainable=false        collections=collections        cache device=caching device        partitioner=partitioner        validate shape=validate shape        use resource=use resource        synchronization=synchronization        aggregation=aggregation        custom getter=custom getter        constraint=constraint  
 tf export v1= string  string    deprecation deprecate endpoints string  def get seed op seed     string   eager = context execute eagerly      if eager      global seed = context global seed     else      global seed = ops get default graph   seed    if global seed be not none      if op seed be none               if hasattr ops get default graph    string           ops get default graph    seed use = true       if eager          op seed = context internal operation seed         else          op seed = ops get default graph    last id      seed =  truncate seed global seed    truncate seed op seed    else      if op seed be not none        seed = default graph seed   truncate seed op seed      else        seed = none  none         if seed ==  0  0       return  0   maxint32    return seed 
 tf export v1= string   def get session handle data  name=none     string   if not isinstance data  ops tensor       raise typeerror string        with ops colocate with data       return gen data flow ops get session handle data  name=name  
 tf export v1= string   def get session tensor handle  dtype  name=none     string   handle device = tensorhandle  get device name handle    with ops device handle device       holder = array ops placeholder dtypes string       register handle feeder holder graph  holder  dtype      tensor = gen data flow ops get session tensor holder  dtype  name=name    return  holder  tensor  
 tf export v1= string   def get variable name                   shape=none                   dtype=none                   initializer=none                   regularizer=none                   trainable=none                   collections=none                   cache device=none                   partitioner=none                   validate shape=true                   use resource=none                   custom getter=none                   constraint=none                   synchronization=variablesynchronization auto                   aggregation=variableaggregation none     return get variable scope   get variable         get default variable store          name        shape=shape        dtype=dtype        initializer=initializer        regularizer=regularizer        trainable=trainable        collections=collections        cache device=caching device        partitioner=partitioner        validate shape=validate shape        use resource=use resource        custom getter=custom getter        constraint=constraint        synchronization=synchronization        aggregation=aggregation  
 tf export v1= string   def get variable scope      string   return get variable scope store   current scope 
 tf export v1= string   def global variables scope=none     string   return ops get collection ops graphkeys global variables  scope  
 tf export v1= string  string   def global variables initializer      string   if context execute eagerly        return control flow ops no op name=string    return variables initializer global variables    
 tf export v1= string   def gradients ys                xs                grad ys=none                name=string                colocate gradients with ops=false                gate gradients=false                aggregation method=none                stop gradients=none                unconnected gradients=unconnectedgradients none     string               with ops get default graph    mutation lock        return gradients util  gradientshelper          ys  xs  grad ys  name  colocate gradients with ops          gate gradients  aggregation method  stop gradients          unconnected gradients  
 tf export v1= string   def hessians ys               xs               name=string               colocate gradients with ops=false               gate gradients=false               aggregation method=none     string   xs = gradients util  aslist xs      kwargs =         string  colocate gradients with ops        string  gate gradients        string  aggregation method          hessians =       gradients = gradients ys  xs    kwargs    for gradient  x in zip  gradients  xs            gradient = array ops reshape gradient   -1             n = array ops size x      loop vars =           array ops constant 0  dtypes int32           tensor array ops tensorarray x dtype  n                         hessian = control flow ops while loop          lambda j     j < n          lambda j  result   j   1                             result write j  gradients gradient j   x  0             loop vars             shape = array ops shape x       reshape hessian = array ops reshape hessian stack                                              array ops concat   shape   shape   0       hessians append  reshape hessian    return hessians 
 tf export v1= string    deprecate none  string  def initialize all table name=string     string   return table initializer name  
 tf export v1= string    tf should use should use result  deprecate string  string  def initialize all variables      string   return global variables initializer   
 tf export v1= string    tf should use should use result  deprecate string  string  def initialize local variables      string   return local variables initializer   
 tf export v1= string    tf should use should use result  deprecate string  string  def initialize variables var list  name=string     string   return variables initializer var list  name=name  
 tf export v1= string    tf should use should use result def be variable initialize variable     string   return state ops be variable initialize variable  
 deprecation deprecate date=none                          instructions=string   tf export v1= string   def load file system library library filename     string   py tf tf loadlibrary library filename  
 tf export v1= string   def local variables scope=none     string   return ops get collection ops graphkeys local variables  scope  
 tf export v1= string  string   def local variables initializer      string   if context execute eagerly        return control flow ops no op name=string    return variables initializer local variables    
 tf export v1= string   def make template name                     func                     create scope now =false                    unique name =none                    custom getter =none                      kwargs     string   return make template internal        name         func         create scope now         unique name         custom getter         create graph function =false          kwargs  
 tf export string  def map fn fn  elems  dtype=none  parallel iterations=none  back prop=true             swap memory=false  infer shape=true  name=none     string   if not callable fn       raise typeerror string     if isinstance elems  sparse tensor sparsetensor       raise typeerror          string         string         string         string     in graph mode = not context execute eagerly        if in graph mode and not parallel iterations      parallel iterations = 10   elif not in graph mode and not parallel iterations      parallel iterations = 1    if not in graph mode and parallel iterations > 1      log log first n log warn  string                         string                         string                         string  1      parallel iterations = 1    input be sequence = nest be sequence elems    input flatten = lambda x  nest flatten x  if input be sequence else  x    def input pack x       return nest pack sequence as elems  x  if input be sequence else x 0     if dtype be none      output be sequence = input be sequence     output flatten = input flatten     output pack = input pack   else      output be sequence = nest be sequence dtype      output flatten = lambda x  nest flatten x  if output be sequence else  x      def output pack x         return  nest pack sequence as dtype  x                if output be sequence else x 0      elems flat = input flatten elems     with ops name scope name  string  elems flat                 if in graph mode                      varscope = vs get variable scope         varscope cache device be none = false       if varscope cache device be none                            varscope set cache device lambda op  op device          varscope cache device be none = true      elems flat =           ops convert to tensor elem  name=string  for elem in elems flat       dtype = dtype or input pack  elem dtype for elem in elems flat       dtype flat = output flatten dtype            static shape = elems flat 0  shape     if static shape ndims be not none and static shape ndims < 1        if len elems flat  == 1          raise valueerror string        else          raise valueerror              string               n =  tensor shape dimension value static shape 0            or array ops shape elems flat 0   0             elems ta =           tensor array ops tensorarray dtype=elem dtype                                       size=n                                       dynamic size=false                                       infer shape=true          for elem in elems flat           elems ta =           elem ta unstack elem  for elem ta  elem in zip elems ta  elems flat        i = constant op constant 0       accs ta =           tensor array ops tensorarray dtype=dt                                       size=n                                       dynamic size=false                                       infer shape=infer shape          for dt in dtype flat       def compute i  tas         string       pack value = input pack  elem ta read i  for elem ta in elems ta         pack fn value = fn pack value        nest assert same structure dtype or elems  pack fn value        flat fn value = output flatten pack fn value        tas =  ta write i  value  for  ta  value  in zip tas  flat fn value         return  i   1  tas          r a = control flow ops while loop          lambda i     i < n  compute   i  accs ta           parallel iterations=parallel iterations          back prop=back prop          swap memory=swap memory          maximum iterations=n      result flat =  r stack   for r in r a       n static = tensor shape dimension tensor shape dimension value          elems flat 0  get shape   with rank at least 1  0        for elem in elems flat 1          n static merge with tensor shape dimension tensor shape dimension value            elem get shape   with rank at least 1  0         for r in result flat        r set shape tensor shape tensorshape n static  concatenate            r get shape   1                    if in graph mode and varscope cache device be none        varscope set cache device none       return output pack result flat  
 tf export v1= string   def min max variable partitioner max partitions=1  axis=0                                   min slice size=256 << 10                                   bytes per string element=16     string   def  partitioner shape  dtype       string     if axis >= len shape         raise valueerror string                        string    axis  shape       if dtype base dtype == dtypes string        bytes per element = bytes per string element     else        bytes per element = dtype size     total size bytes = shape num elements     bytes per element     partition = total size bytes / min slice size     partition list =  1    len shape                partition list axis  = max 1  min shape dim axis  value                                         max partition                                         int math ceil partition         return partition list   return  partitioner 
 tf export v1= string   def model variables scope=none     string   return ops get collection ops graphkeys model variables  scope  
 tf export v1= string   def move average variables scope=none     string   return ops get collection ops graphkeys move average variables  scope  
 tf export v1= string  string    deprecation deprecate      date=none  instructions=string  def multinomial logits  num sample  seed=none  name=none  output dtype=none     string   with ops name scope name  string   logits        return multinomial categorical impl logits  num sample  output dtype  seed  
 tf export v1= string   def no regularizer       string   return none 
 tf export v1= string  string    deprecation deprecate args      none  string  string  def norm tensor           ord=string           axis=none           keepdims=none           name=none           keep dims=none     r   compute the norm of vectors  matrices  and tensors     this function can compute several different vector norms  the 1-norm  the   euclidean or 2-norm  the inf-norm  and in general the p-norm for p > 0  and   matrix norms  frobenius  1-norm  2-norm and inf-norm      args      tensor  `tensor` of type `float32`  `float64`  `complex64`  `complex128`     ord  order of the norm  support value be  fro    euclidean         `1`  `2`  `np inf` and any positive real number yield the correspond       p-norm  default be  euclidean  which be equivalent to frobenius norm if       `tensor` be a matrix and equivalent to 2-norm for vectors        some restrictions apply          a  the frobenius norm `fro` be not define for vectors          b  if axis be a 2-tuple  matrix norm   only  euclidean    fro   `1`             `2`  `np inf` be support        see the description of `axis` on how to compute norms for a batch of       vectors or matrices store in a tensor      axis  if `axis` be `none`  the default   the input be consider a vector       and a single vector norm be compute over the entire set of value in the       tensor  i e  `norm tensor  ord=ord ` be equivalent to       `norm reshape tensor   -1    ord=ord `        if `axis` be a python integer  the input be consider a batch of vectors        and `axis` determine the axis in `tensor` over which to compute vector       norms        if `axis` be a 2-tuple of python integers it be consider a batch of       matrices and `axis` determine the ax in `tensor` over which to compute       a matrix norm        negative indices be support  example  if you be pass a tensor that       can be either a matrix or a batch of matrices at runtime  pass       `axis= -2 -1 ` instead of `axis=none` to make sure that matrix norms be       compute      keepdims  if true  the axis indicate in `axis` be keep with size 1        otherwise  the dimension in `axis` be remove from the output shape      name  the name of the op      keep dim  deprecate alias for `keepdims`     return      output  a `tensor` of the same type as tensor  contain the vector or       matrix norms  if `keepdims` be true then the rank of output be equal to       the rank of `tensor`  otherwise  if `axis` be none the output be a scalar        if `axis` be an integer  the rank of `output` be one less than the rank       of `tensor`  if `axis` be a 2-tuple the rank of `output` be two less       than the rank of `tensor`     raise      valueerror  if `ord` or `axis` be invalid      compatibility numpy    mostly equivalent to numpy linalg norm    not support  ord <= 0  2-norm for matrices  nuclear norm    other differences      a  if axis be `none`  treat the flatten `tensor` as a vector      regardless of rank      b  explicitly support  euclidean  norm as the default  include for      higher order tensors     end compatibility         keepdims = deprecation deprecate argument lookup string  keepdims                                                      string  keep dim    if keepdims be none      keepdims = false    be matrix norm =   isinstance axis  tuple  or isinstance axis  list   and                     len axis  == 2    if be matrix norm      axis = tuple axis      if  not isinstance axis 0   int  or not isinstance axis 1   int  or         axis 0  == axis 1          raise valueerror            string      support matrix norms =  string  string  1  2  np inf      if ord not in support matrix norms        raise valueerror string                           support matrix norms  ord     else      if not  isinstance axis  int  or axis be none         raise valueerror            string       support vector norms =  string  1  2  np inf      if  not np isreal ord  or ord <= 0  and ord not in support vector norms        raise valueerror string   ord      if axis be not none        axis =  axis      with ops name scope name  string   tensor        tensor = ops convert to tensor tensor       if ord in  string  string  2  2 0         if be matrix norm and ord in  2  2 0           rank = array ops rank tensor          positive axis = map fn map fn              lambda i  control flow ops cond i >= 0  lambda  i  lambda  i   rank               ops convert to tensor axis           ax = math ops range rank          perm before = array ops concat               array ops setdiff1d ax  positive axis  0   positive axis               axis=0          perm after = map fn map fn              lambda i  math ops cast                  array ops squeeze                      array ops where v2 math ops equal perm before  i                     dtype=dtypes int32   ax          perm = array ops transpose tensor  perm=perm before          matrix 2 norm = array ops expand dim              math ops reduce max                  math ops abs gen linalg ops svd perm  compute uv=false  0                    axis=-1                  keepdims=true               axis=-1          result = array ops transpose matrix 2 norm  perm=perm after        else          result = math ops sqrt              math ops reduce sum                  tensor   math ops conj tensor   axis  keepdims=true                         else        result = math ops abs tensor        if ord == 1          sum axis = none if axis be none else axis 0          result = math ops reduce sum result  sum axis  keepdims=true          if be matrix norm            result = math ops reduce max result  axis -1   keepdims=true        elif ord == np inf          if be matrix norm            result = math ops reduce sum result  axis 1   keepdims=true          max axis = none if axis be none else axis 0          result = math ops reduce max result  max axis  keepdims=true        else                   result = math ops pow              math ops reduce sum math ops pow result  ord   axis  keepdims=true               1 0 / ord      if not keepdims        result = array ops squeeze result  axis      return result 
 tf export v1= string    dispatch add dispatch support def ones like tensor  dtype=none  name=none  optimize=true     string   return ones like impl tensor  dtype  name  optimize  
 tf export v1= string    tf contextlib contextmanager def op scope value  name  default name=none     string   log warn string                string    with name scope name  default name=default name  values=values  as scope      yield scope 
 tf export v1= string   def pad tensor  paddings  mode=string  name=none  constant values=0       string          mode = mode upper     if mode == string                if not tensor util be tensor constant value  and constant value == 0        result = gen array ops pad tensor  paddings  name=name      else        result = gen array ops pad v2            tensor  paddings  constant value  name=name    elif mode == string      result = gen array ops mirror pad          tensor  paddings  mode=string  name=name    elif mode == string      result = gen array ops mirror pad          tensor  paddings  mode=string  name=name    else      raise valueerror string   mode        if not context execute eagerly        paddings constant =  get paddings constant paddings      input shape =           tensor shape tensorshape tensor shape          if isinstance tensor  ops tensor  else result op input 0  shape      if  input shape ndims be not none and         not result shape be fully define   and paddings constant be not none         new shape =          for pad  dim in zip paddings constant  input shape as list             if pad be none or dim be none or any  x be none for x in pad              new shape append none          else            new shape append sum pad    dim        result set shape new shape     return result 
 tf export v1= string  string   def parse example serialize  feature  name=none  example names=none     return parse example v2 serialize  feature  example name  name  
 tf export v1= string  string   def parse single example serialize  feature  name=none  example names=none     string   return parse single example v2 unoptimized        serialize  feature  example name  name         
 tf export v1= string   def placeholder dtype  shape=none  name=none     string   if context execute eagerly        raise runtimeerror string                        string     return gen array ops placeholder dtype=dtype  shape=shape  name=name  
 tf export v1= string   def placeholder with default input  shape  name=none       string   return gen array ops placeholder with default input  shape  name  
 deprecation deprecate      date=none      instructions=string   tf export v1= string   def py func func  inp  tout  stateful=true  name=none     return py func common func  inp  tout  stateful  name=name  
 tf export v1= string    deprecation deprecate      string      string     string    def quantize v2      input        min range      max range      t      mode=string      name=none      round mode=string      narrow range=false      axis=none      ensure minimum range=0 01     if axis be none      axis = -1   elif axis < 0      if input shape ndims be none        raise valueerror string      axis  = input shape ndims    if compat forward compatible 2019  11  13  or ensure minimum range  = 0 01      return gen array ops quantize v2          input          min range          max range          t=t          mode=mode          name=name          round mode=round mode          narrow range=narrow range          axis=axis          ensure minimum range=ensure minimum range    return gen array ops quantize v2        input        min range        max range        t=t        mode=mode        name=name        round mode=round mode        narrow range=narrow range        axis=axis  
class randomnormal initializer     string     deprecate args none                     string                    string  string    def   init   self  mean=0 0  stddev=1 0  seed=none  dtype=dtypes float32       self mean = mean     self stddev = stddev     self seed = seed     self dtype =  assert float dtype dtypes as dtype dtype      def   call   self  shape  dtype=none  partition info=none       if dtype be none        dtype = self dtype     return random ops random normal          shape  self mean  self stddev  dtype  seed=self seed     def get config self       return           string  self mean          string  self stddev          string  self seed          string  self dtype name       
 tf export v1= string  string    deprecation deprecate endpoints string  def random poisson lam  shape  dtype=dtypes float32  seed=none  name=none     string   return random poisson v2 shape  lam  dtype  seed  name  
class randomuniform initializer     string     deprecate args none                     string                    string  string    def   init   self  minval=0  maxval=none  seed=none  dtype=dtypes float32       self minval = minval     self maxval = maxval     self seed = seed     self dtype = dtypes as dtype dtype     def   call   self  shape  dtype=none  partition info=none       if dtype be none        dtype = self dtype     return random ops random uniform          shape  self minval  self maxval  dtype  seed=self seed     def get config self       return           string  self minval          string  self maxval          string  self seed          string  self dtype name       
 tf export v1= string  string    deprecation deprecate args none                               string                               string  def reduce all v1 input tensor                    axis=none                    keepdims=none                    name=none                    reduction indices=none                    keep dims=none     string   axis = deprecation deprecate argument lookup string  axis                                                  string                                                  reduction indices    keepdims = deprecation deprecate argument lookup string  keepdims                                                      string  keep dim    return reduce all input tensor  axis  keepdims  name  
 tf export v1= string  string    deprecation deprecate args none                               string                               string  def reduce any v1 input tensor                    axis=none                    keepdims=none                    name=none                    reduction indices=none                    keep dims=none     string   axis = deprecation deprecate argument lookup string  axis                                                  string                                                  reduction indices    keepdims = deprecation deprecate argument lookup string  keepdims                                                      string  keep dim    return reduce any input tensor  axis  keepdims  name  
 tf export v1= string  string    deprecation deprecate args none                               string                               string   deprecation deprecate endpoints string  def reduce join input  axis=none                    keep dims=none                  separator=string                  name=none                  reduction indices=none                  keepdims=none     keepdims = deprecation deprecate argument lookup string  keepdims                                                      string  keep dim    if keep dim be none      keep dim = false   axis = deprecation deprecate argument lookup string  axis                                                  string                                                  reduction indices    return reduce join v2        inputs=inputs        axis=axis        keepdims=keepdims        separator=separator        name=name  
 tf export v1= string  string    deprecation deprecate args none                               string                               string  def reduce logsumexp v1 input tensor                          axis=none                          keepdims=none                          name=none                          reduction indices=none                          keep dims=none     string   axis = deprecation deprecate argument lookup string  axis                                                  string                                                  reduction indices    keepdims = deprecation deprecate argument lookup string  keepdims                                                      string  keep dim    return reduce logsumexp input tensor  axis  keepdims  name  
 tf export v1= string  string    deprecation deprecate args none                               string                               string  def reduce max v1 input tensor                    axis=none                    keepdims=none                    name=none                    reduction indices=none                    keep dims=none     string   axis = deprecation deprecate argument lookup string  axis                                                  string                                                  reduction indices    keepdims = deprecation deprecate argument lookup string  keepdims                                                      string  keep dim    return reduce max input tensor  axis  keepdims  name  
 tf export v1= string  string   def reduce mean v1 input tensor                     axis=none                     keepdims=none                     name=none                     reduction indices=none                     keep dims=none     string   axis = deprecation deprecate argument lookup string  axis                                                  string                                                  reduction indices    keepdims = deprecation deprecate argument lookup string  keepdims                                                      string  keep dim    return reduce mean input tensor  axis  keepdims  name  
 tf export v1= string  string    deprecation deprecate args none                               string                               string  def reduce min v1 input tensor                    axis=none                    keepdims=none                    name=none                    reduction indices=none                    keep dims=none     string   axis = deprecation deprecate argument lookup string  axis                                                  string                                                  reduction indices    keepdims = deprecation deprecate argument lookup string  keepdims                                                      string  keep dim    return reduce min input tensor  axis  keepdims  name  
 tf export v1= string  string    deprecation deprecate args none                               string                               string  def reduce prod v1 input tensor                     axis=none                     keepdims=none                     name=none                     reduction indices=none                     keep dims=none     string   axis = deprecation deprecate argument lookup string  axis                                                  string                                                  reduction indices    keepdims = deprecation deprecate argument lookup string  keepdims                                                      string  keep dim    return reduce prod input tensor  axis  keepdims  name  
 tf export v1= string  string    deprecation deprecate args none                               string                               string  def reduce sum v1 input tensor                    axis=none                    keepdims=none                    name=none                    reduction indices=none                    keep dims=none     string   axis = deprecation deprecate argument lookup string  axis                                                  string                                                  reduction indices    keepdims = deprecation deprecate argument lookup string  keepdims                                                      string  keep dim    return reduce sum input tensor  axis  keepdims  name  
 tf export v1= string    tf should use should use result def report uninitialized variables var list=none                                     name=string     string   if var list be none      var list = global variables     local variables            if not var list        var list =          for op in ops get default graph   get operations            if op type in  string  string  string             var list append op output 0     with ops name scope name            if var list        init vars =  state ops be variable initialize v  for v in var list      local device = os environ get          string  string      with ops device local device         if not var list                            return array ops constant     dtype=dtypes string        else                   variables mask = math ops logical not array ops stack init vars                    variable name tensor = array ops constant               s op name for s in var list                             return array ops boolean mask variable name tensor  variables mask  
 tf export v1= string   def reset default graph      string   if not  default graph stack be clear        raise assertionerror string                          string                          string     default graph stack reset   
 tf export v1= string   def resource variables enable      string   global  default use resource   return  default use resource 
 tf export v1= string    deprecation deprecate args none                               string                               string   deprecation deprecate args none                               string                               string  def reverse sequence input                       seq lengths                       seq axis=none                       batch axis=none                       name=none                       seq dim=none                       batch dim=none     string   seq axis = deprecation deprecate argument lookup string  seq axis                                                      string  seq dim    batch axis = deprecation deprecate argument lookup string  batch axis                                                        string  batch dim    return gen array ops reverse sequence        input=input        seq lengths=seq lengths        seq dim=seq axis        batch dim=batch axis        name=name  
 tf export v1= string  string   def scalar mul scalar  x  name=none     string   scalar = ops convert to tensor        scalar  dtype=x dtype base dtype  name=string    shape = scalar get shape     if shape ndims == 0      if isinstance x  ops indexedslices         return ops indexedslices            gen math ops mul scalar  x value  name   x indices  x dense shape      else        return gen math ops mul scalar  x  name    else      raise valueerror string   shape  
 tf export string  def scan fn           elems           initializer=none           parallel iterations=10           back prop=true           swap memory=false           infer shape=true           reverse=false           name=none     string   if not callable fn       raise typeerror string     input be sequence = nest be sequence elems    input flatten = lambda x  nest flatten x  if input be sequence else  x     def input pack x       return nest pack sequence as elems  x  if input be sequence else x 0     if initializer be none      output be sequence = input be sequence     output flatten = input flatten     output pack = input pack   else      output be sequence = nest be sequence initializer      output flatten = lambda x  nest flatten x  if output be sequence else  x       def output pack x         return  nest pack sequence as initializer  x                if output be sequence else x 0      elems flat = input flatten elems     in graph mode = not context execute eagerly     with ops name scope name  string  elems flat                 if in graph mode                      varscope = vs get variable scope         varscope cache device be none = false       if varscope cache device be none                            varscope set cache device lambda op  op device          varscope cache device be none = true           elems flat =           ops convert to tensor elem  name=string  for elem in elems flat                 n = tensor shape dimension value elems flat 0  shape 0       if n be none        n = array ops shape elems flat 0   0            elems ta =           tensor array ops tensorarray              dtype=elem dtype              size=n              dynamic size=false              element shape=elem shape 1                infer shape=true  for elem in elems flat                elems ta =           elem ta unstack elem  for elem ta  elem in zip elems ta  elems flat             if initializer be none        a flat =  elem read n - 1 if reverse else 0  for elem in elems ta        i = 1     else        initializer flat = output flatten initializer        a flat =  ops convert to tensor init  for init in initializer flat        i = 0           accs ta =           tensor array ops tensorarray              dtype=init dtype              size=n              element shape=init shape if infer shape else none              dynamic size=false              infer shape=infer shape  for init in a flat            if initializer be none        accs ta =             acc ta write n - 1 if reverse else 0  a            for  acc ta  a  in zip accs ta  a flat               def compute i  a flat  tas         string       pack elems = input pack  elem ta read i  for elem ta in elems ta         pack a = output pack a flat        a out = fn pack a  pack elems        nest assert same structure elems if initializer be none else initializer                                   a out        flat a out = output flatten a out        tas =  ta write i  value  for  ta  value  in zip tas  flat a out         if reverse          next i = i - 1       else          next i = i   1       return  next i  flat a out  tas       if reverse        initial i = n - 1 - i       condition = lambda i   1   2  i >= 0     else        initial i = i       condition = lambda i   1   2  i < n           r a = control flow ops while loop          condition          compute   initial i  a flat  accs ta           parallel iterations=parallel iterations          back prop=back prop          swap memory=swap memory          maximum iterations=n       result flat =  r stack   for r in r a       n static = tensor shape dimension          tensor shape dimension value              elems flat 0  get shape   with rank at least 1  0        for elem in elems flat 1          n static merge with            tensor shape dimension                tensor shape dimension value                    elem get shape   with rank at least 1  0         for r in result flat        r set shape            tensor shape tensorshape n static  concatenate r get shape   1                    if in graph mode and varscope cache device be none        varscope set cache device none       return output pack result flat  
 tf export v1= string   def scatter add ref  indices  update  use locking=false  name=none        r   add sparse update to the variable reference by `resource`     this operation compute    ```python         scalar indices       ref indices        = update               vector indices  for each i        ref indices i         = update i                high rank indices  for each i       j        ref indices i       j         = update i       j         ```    this operation output `ref` after the update be do    this make it easier to chain operations that need to use the update value    duplicate entries be handle correctly  if multiple `indices` reference   the same location  their contributions add     require `updates shape = indices shape   ref shape 1  `     <div style= width 70   margin auto  margin-bottom 10px  margin-top 20px  >   <img style= width 100   src= https //www tensorflow org/images/scatteradd png  alt>   </div>    args      ref  a `variable`      indices  a `tensor`  must be one of the follow type  `int32`  `int64`        a tensor of indices into the first dimension of `ref`      update  a `tensor`  must have the same type as `ref`        a tensor of update value to store in `ref`      use lock  an optional `bool`  default to `false`        if true  the assignment will be protect by a lock        otherwise the behavior be undefined  but may exhibit less contention      name  a name for the operation  optional      return      same as `ref`   return as a convenience for operations that want     to use the update value after the update be do          if ref dtype  be ref dtype      return gen state ops scatter add ref  indices  update                                       use locking=use lock  name=name    return ref  lazy read gen resource variable ops resource scatter add          ref handle  indices  ops convert to tensor update  ref dtype         name=name   
 tf export v1= string   def scatter div ref  indices  update  use locking=false  name=none        r   divide a variable reference by sparse update     this operation compute    ```python         scalar indices       ref indices       /= update               vector indices  for each i        ref indices i        /= update i                high rank indices  for each i       j        ref indices i       j        /= update i       j         ```    this operation output `ref` after the update be do    this make it easier to chain operations that need to use the reset value     duplicate entries be handle correctly  if multiple `indices` reference   the same location  their contributions divide     require `updates shape = indices shape   ref shape 1  ` or `updates shape =     `     args      ref  a mutable `tensor`  must be one of the follow type  `float32`        `float64`  `int32`  `uint8`  `int16`  `int8`  `complex64`  `int64`        `qint8`  `quint8`  `qint32`  `bfloat16`  `uint16`  `complex128`  `half`        `uint32`  `uint64`  should be from a `variable` node      indices  a `tensor`  must be one of the follow type  `int32`  `int64`  a       tensor of indices into the first dimension of `ref`      update  a `tensor`  must have the same type as `ref`  a tensor of value       that `ref` be divide by      use lock  an optional `bool`  default to `false`  if true  the operation       will be protect by a lock  otherwise the behavior be undefined  but may       exhibit less contention      name  a name for the operation  optional      return      a mutable `tensor`  have the same type as `ref`          if ref dtype  be ref dtype      return gen state ops scatter div ref  indices  update                                       use locking=use lock  name=name    return ref  lazy read gen resource variable ops resource scatter div          ref handle  indices  ops convert to tensor update  ref dtype         name=name   
 tf export v1= string   def scatter max ref  indices  update  use locking=false  name=none        r   reduce sparse update into a variable reference use the `max` operation     this operation compute          scalar indices       ref indices       = max ref indices        update                vector indices  for each i        ref indices i        = max ref indices i         update i                 high rank indices  for each i       j        ref indices i       j        = max ref indices i       j               update i       j           this operation output `ref` after the update be do    this make it easier to chain operations that need to use the reset value     duplicate entries be handle correctly  if multiple `indices` reference   the same location  their contributions combine     require `updates shape = indices shape   ref shape 1  ` or `updates shape =     `     <div style= width 70   margin auto  margin-bottom 10px  margin-top 20px  >   <img style= width 100   src= https //www tensorflow org/images/scatteradd png    alt>   </div>    args      ref  a mutable `tensor`  must be one of the follow type  `half`        `bfloat16`  `float32`  `float64`  `int32`  `int64`  should be from a       `variable` node      indices  a `tensor`  must be one of the follow type  `int32`  `int64`  a       tensor of indices into the first dimension of `ref`      update  a `tensor`  must have the same type as `ref`  a tensor of update       value to reduce into `ref`      use lock  an optional `bool`  default to `false`  if true  the update       will be protect by a lock  otherwise the behavior be undefined  but may       exhibit less contention      name  a name for the operation  optional      return      a mutable `tensor`  have the same type as `ref`          if ref dtype  be ref dtype      return gen state ops scatter max ref  indices  update                                       use locking=use lock  name=name    return ref  lazy read gen resource variable ops resource scatter max          ref handle  indices  ops convert to tensor update  ref dtype         name=name   
 tf export v1= string   def scatter min ref  indices  update  use locking=false  name=none        r   reduce sparse update into a variable reference use the `min` operation     this operation compute          scalar indices       ref indices       = min ref indices        update                vector indices  for each i        ref indices i        = min ref indices i         update i                 high rank indices  for each i       j        ref indices i       j        = min ref indices i       j               update i       j           this operation output `ref` after the update be do    this make it easier to chain operations that need to use the reset value     duplicate entries be handle correctly  if multiple `indices` reference   the same location  their contributions combine     require `updates shape = indices shape   ref shape 1  ` or `updates shape =     `     <div style= width 70   margin auto  margin-bottom 10px  margin-top 20px  >   <img style= width 100   src= https //www tensorflow org/images/scatteradd png    alt>   </div>    args      ref  a mutable `tensor`  must be one of the follow type  `half`        `bfloat16`  `float32`  `float64`  `int32`  `int64`  should be from a       `variable` node      indices  a `tensor`  must be one of the follow type  `int32`  `int64`  a       tensor of indices into the first dimension of `ref`      update  a `tensor`  must have the same type as `ref`  a tensor of update       value to reduce into `ref`      use lock  an optional `bool`  default to `false`  if true  the update       will be protect by a lock  otherwise the behavior be undefined  but may       exhibit less contention      name  a name for the operation  optional      return      a mutable `tensor`  have the same type as `ref`          if ref dtype  be ref dtype      return gen state ops scatter min ref  indices  update                                       use locking=use lock  name=name    return ref  lazy read gen resource variable ops resource scatter min          ref handle  indices  ops convert to tensor update  ref dtype         name=name   
 tf export v1= string   def scatter mul ref  indices  update  use locking=false  name=none        r   multiply sparse update into a variable reference     this operation compute    ```python         scalar indices       ref indices        = update               vector indices  for each i        ref indices i         = update i                high rank indices  for each i       j        ref indices i       j         = update i       j         ```    this operation output `ref` after the update be do    this make it easier to chain operations that need to use the reset value     duplicate entries be handle correctly  if multiple `indices` reference   the same location  their contributions multiply     require `updates shape = indices shape   ref shape 1  ` or `updates shape =     `     args      ref  a mutable `tensor`  must be one of the follow type  `float32`        `float64`  `int32`  `uint8`  `int16`  `int8`  `complex64`  `int64`        `qint8`  `quint8`  `qint32`  `bfloat16`  `uint16`  `complex128`  `half`        `uint32`  `uint64`  should be from a `variable` node      indices  a `tensor`  must be one of the follow type  `int32`  `int64`  a       tensor of indices into the first dimension of `ref`      update  a `tensor`  must have the same type as `ref`  a tensor of update       value to multiply to `ref`      use lock  an optional `bool`  default to `false`  if true  the operation       will be protect by a lock  otherwise the behavior be undefined  but may       exhibit less contention      name  a name for the operation  optional      return      a mutable `tensor`  have the same type as `ref`          if ref dtype  be ref dtype      return gen state ops scatter mul ref  indices  update                                       use locking=use lock  name=name    return ref  lazy read gen resource variable ops resource scatter mul          ref handle  indices  ops convert to tensor update  ref dtype         name=name   
 tf export v1= string   def scatter nd add ref  indices  update  use locking=false  name=none     r   apply sparse addition to individual value or slice in a variable     `ref` be a `tensor` with rank `p` and `indices` be a `tensor` of rank `q`     `indices` must be integer tensor  contain indices into `ref`    it must be shape ` d 0       d  q-2   k ` where `0 < k <= p`     the innermost dimension of `indices`  with length `k`  correspond to   indices into elements  if `k = p`  or slice  if `k < p`  along the `k`th   dimension of `ref`     `updates` be `tensor` of rank `q-1 p-k` with shape     ```    d 0       d  q-2   ref shape k        ref shape p-1     ```    for example  say we want to add 4 scatter elements to a rank-1 tensor to   8 elements  in python  that addition would look like this     ```python   ref = tf variable  1  2  3  4  5  6  7  8     indices = tf constant   4    3    1    7      update = tf constant  9  10  11  12     add = tf compat v1 scatter nd add ref  indices  update    with tf compat v1 session   as sess      print sess run add    ```    the result update to ref would look like this          1  13  3  14  14  6  7  20     see `tf scatter nd` for more detail about how to make update to   slice     args      ref  a mutable `tensor`  must be one of the follow type  `float32`        `float64`  `int32`  `uint8`  `int16`  `int8`  `complex64`  `int64`        `qint8`  `quint8`  `qint32`  `bfloat16`  `uint16`  `complex128`  `half`        `uint32`  `uint64`  a mutable tensor  should be from a variable node      indices  a `tensor`  must be one of the follow type  `int32`  `int64`        a tensor of indices into ref      update  a `tensor`  must have the same type as `ref`        a tensor of update value to add to ref      use lock  an optional `bool`  default to `false`        if true  the assignment will be protect by a lock        otherwise the behavior be undefined  but may exhibit less contention      name  a name for the operation  optional      return      a mutable `tensor`  have the same type as `ref`          if ref dtype  be ref dtype      return gen state ops scatter nd add          ref  indices  update  use lock  name    return ref  lazy read gen state ops resource scatter nd add          ref handle  indices  ops convert to tensor update  ref dtype         name=name   
 tf export v1= string   def scatter nd sub ref  indices  update  use locking=false  name=none     r   apply sparse subtraction to individual value or slice in a variable     `ref` be a `tensor` with rank `p` and `indices` be a `tensor` of rank `q`     `indices` must be integer tensor  contain indices into `ref`    it must be shape ` d 0       d  q-2   k ` where `0 < k <= p`     the innermost dimension of `indices`  with length `k`  correspond to   indices into elements  if `k = p`  or slice  if `k < p`  along the `k`th   dimension of `ref`     `updates` be `tensor` of rank `q-1 p-k` with shape     ```    d 0       d  q-2   ref shape k        ref shape p-1     ```    for example  say we want to subtract 4 scatter elements from a rank-1 tensor   with 8 elements  in python  that update would look like this     ```python   ref = tf variable  1  2  3  4  5  6  7  8     indices = tf constant   4    3    1    7      update = tf constant  9  10  11  12     op = tf compat v1 scatter nd sub ref  indices  update    with tf compat v1 session   as sess      print sess run op    ```    the result update to ref would look like this          1  -9  3  -6  -6  6  7  -4     see `tf scatter nd` for more detail about how to make update to   slice     args      ref  a mutable `tensor`  must be one of the follow type  `float32`        `float64`  `int32`  `uint8`  `int16`  `int8`  `complex64`  `int64`        `qint8`  `quint8`  `qint32`  `bfloat16`  `uint16`  `complex128`  `half`        `uint32`  `uint64`  a mutable tensor  should be from a variable node      indices  a `tensor`  must be one of the follow type  `int32`  `int64`        a tensor of indices into ref      update  a `tensor`  must have the same type as `ref`        a tensor of update value to add to ref      use lock  an optional `bool`  default to `false`        an optional bool  default to true  if true  the assignment will       be protect by a lock  otherwise the behavior be undefined        but may exhibit less contention      name  a name for the operation  optional      return      a mutable `tensor`  have the same type as `ref`          if ref dtype  be ref dtype      return gen state ops scatter nd sub          ref  indices  update  use lock  name    return ref  lazy read gen state ops resource scatter nd sub          ref handle  indices  ops convert to tensor update  ref dtype         name=name   
 tf export v1= string   def scatter nd update ref  indices  update  use locking=true  name=none     r   apply sparse `updates` to individual value or slice in a variable     `ref` be a `tensor` with rank `p` and `indices` be a `tensor` of rank `q`     `indices` must be integer tensor  contain indices into `ref`    it must be shape ` d 0       d  q-2   k ` where `0 < k <= p`     the innermost dimension of `indices`  with length `k`  correspond to   indices into elements  if `k = p`  or slice  if `k < p`  along the `k`th   dimension of `ref`     `updates` be `tensor` of rank `q-1 p-k` with shape     ```    d 0       d  q-2   ref shape k        ref shape p-1      ```    for example  say we want to update 4 scatter elements to a rank-1 tensor to   8 elements  in python  that update would look like this     ```python       ref = tf variable  1  2  3  4  5  6  7  8         indices = tf constant   4    3    1    7          update = tf constant  9  10  11  12         update = tf compat v1 scatter nd update ref  indices  update        with tf compat v1 session   as sess          print sess run update    ```    the result update to ref would look like this          1  11  3  10  9  6  7  12     see `tf scatter nd` for more detail about how to make update to   slice     args      ref  a variable      indices  a `tensor`  must be one of the follow type  `int32`  `int64`        a tensor of indices into ref      update  a `tensor`  must have the same type as `ref`        a tensor  must have the same type as ref  a tensor of update       value to add to ref      use lock  an optional `bool`  default to `true`        an optional bool  default to true  if true  the assignment will       be protect by a lock  otherwise the behavior be undefined        but may exhibit less contention      name  a name for the operation  optional      return      the value of the variable after the update          if ref dtype  be ref dtype      return gen state ops scatter nd update          ref  indices  update  use lock  name    return ref  lazy read gen state ops resource scatter nd update          ref handle  indices  ops convert to tensor update  ref dtype         name=name   
 tf export v1= string   def scatter sub ref  indices  update  use locking=false  name=none     r   subtract sparse update to a variable reference     ```python         scalar indices       ref indices       -= update               vector indices  for each i        ref indices i        -= update i                high rank indices  for each i       j        ref indices i       j        -= update i       j         ```    this operation output `ref` after the update be do    this make it easier to chain operations that need to use the reset value     duplicate entries be handle correctly  if multiple `indices` reference   the same location  their  negate  contributions add     require `updates shape = indices shape   ref shape 1  ` or   `updates shape =   `     <div style= width 70   margin auto  margin-bottom 10px  margin-top 20px  >   <img style= width 100          src= https //www tensorflow org/images/scattersub png  alt>   </div>    args      ref  a mutable `tensor`  must be one of the follow type  `float32`        `float64`  `int32`  `uint8`  `int16`  `int8`  `complex64`  `int64`        `qint8`  `quint8`  `qint32`  `bfloat16`  `uint16`  `complex128`  `half`        `uint32`  `uint64`  should be from a `variable` node      indices  a `tensor`  must be one of the follow type  `int32`  `int64`        a tensor of indices into the first dimension of `ref`      update  a `tensor`  must have the same type as `ref`        a tensor of update value to subtract from `ref`      use lock  an optional `bool`  default to `false`        if true  the subtraction will be protect by a lock        otherwise the behavior be undefined  but may exhibit less contention      name  a name for the operation  optional      return      a mutable `tensor`  have the same type as `ref`          if ref dtype  be ref dtype      return gen state ops scatter sub ref  indices  update                                       use locking=use lock  name=name    return ref  lazy read gen resource variable ops resource scatter sub          ref handle  indices  ops convert to tensor update  ref dtype         name=name   
 tf export v1= string   def scatter update ref  indices  update  use locking=true  name=none        r   apply sparse update to a variable reference     this operation compute    ```python         scalar indices       ref indices       = update               vector indices  for each i        ref indices i        = update i                high rank indices  for each i       j        ref indices i       j        = update i       j         ```    this operation output `ref` after the update be do    this make it easier to chain operations that need to use the reset value     if value in `ref` be to be update more than once  because there be   duplicate entries in `indices`  the order at which the update happen   for each value be undefined     require `updates shape = indices shape   ref shape 1  `     <div style= width 70   margin auto  margin-bottom 10px  margin-top 20px  >   <img style= width 100   src= https //www tensorflow org/images/scatterupdate png  alt>   </div>    args      ref  a `variable`      indices  a `tensor`  must be one of the follow type  `int32`  `int64`        a tensor of indices into the first dimension of `ref`      update  a `tensor`  must have the same type as `ref`        a tensor of update value to store in `ref`      use lock  an optional `bool`  default to `true`        if true  the assignment will be protect by a lock        otherwise the behavior be undefined  but may exhibit less contention      name  a name for the operation  optional      return      same as `ref`   return as a convenience for operations that want     to use the update value after the update be do          if ref dtype  be ref dtype      return gen state ops scatter update ref  indices  update                                          use locking=use lock  name=name    return ref  lazy read gen resource variable ops resource scatter update          ref handle  indices  ops convert to tensor update  ref dtype         name=name   
 tf export v1= string  string    deprecation deprecate endpoints string  def serialize many sparse sp input  name=none  out type=dtypes string     string   return serialize many sparse v2 sp input  out type  name  
 tf export v1= string  string    deprecation deprecate endpoints string  def serialize sparse sp input  name=none  out type=dtypes string     string   return serialize sparse v2 sp input  out type  name  
 tf export v1= string  string   def set random seed seed     string   if context execute eagerly        context set global seed seed    else      ops get default graph   seed = seed 
 deprecation deprecate string                          string                         string   tf export v1= string   def setdiff1d x  y  index dtype=dtypes int32  name=none     return gen array ops list diff x  y  index dtype  name  
 tf export v1= string   def shape input  name=none  out type=dtypes int32        string   return shape internal input  name  optimize=true  out type=out type  
 tf export v1= string    dispatch add dispatch support def size input  name=none  out type=dtypes int32        string   return size internal input  name  optimize=true  out type=out type  
 tf export v1= string  string    deprecation deprecate endpoints string  def space to batch        input        paddings      block size=none      name=none      block shape=none       block size = deprecation deprecate argument lookup string                                                        block shape  string                                                        block size    result = space to batch nd        input        paddings=paddings        block shape=np array  block size  block size   dtype=np int64         name=name    result set shape result get shape   with rank 4     return result 
 tf export v1= string  string    deprecation deprecate endpoints string  def space to depth input  block size  name=none  data format=string       return gen array ops space to depth input  block size  data format  name=name  
 tf export v1= string  string    deprecation deprecate endpoints string   deprecation deprecate args      none  string  string  def sparse add a  b  threshold=none  thresh=none     string   threshold = deprecation deprecate argument lookup string  threshold                                                       string  thresh    if threshold be none      threshold = 0   return sparse add v2 a  b  threshold  
 tf export v1= string  string    deprecation deprecate endpoints string   deprecation deprecate args      none  string  string  def sparse concat axis                    sp input                    name=none                    expand nonconcat dim=false                    concat dim=none                    expand nonconcat dims=none     string   expand nonconcat dim = deprecation deprecate argument lookup        string  expand nonconcat dim        string  expand nonconcat dim    if expand nonconcat dim be not none      expand nonconcat dim = expand nonconcat dim   axis = deprecation deprecate argument lookup string  axis  string                                                  concat dim    return sparse concat v2 axis  sp input  expand nonconcat dim  name  
def sparse mat mul a  b  transpose a=false  transpose b=false  a be sparse=false  b be sparse=false  name=none     r   multiply matrix  a  by matrix  b      the input must be two-dimensional matrices and the inner dimension of  a  must   match the outer dimension of  b   both  a  and  b  must be `tensor`s not   `sparsetensor`s   this op be optimize for the case where at least one of  a  or    b  be sparse  in the sense that they have a large proportion of zero value    the breakeven for use this versus a dense matrix multiply on one platform be   30  zero value in the sparse matrix     the gradient computation of this operation will only take advantage of sparsity   in the input gradient when that gradient come from a relu     args      a  a `tensor`  must be one of the follow type  `float32`  `bfloat16`      b  a `tensor`  must be one of the follow type  `float32`  `bfloat16`      transpose a  an optional `bool`  default to `false`      transpose b  an optional `bool`  default to `false`      a be sparse  an optional `bool`  default to `false`      b be sparse  an optional `bool`  default to `false`      name  a name for the operation  optional      return      a `tensor` of type `float32`           ctx =  context  context or  context context     tld =  ctx  thread local data   if tld be eager      try         result =  pywrap tensorflow tfe py fastpathexecute           ctx  context handle  tld device name  string  name          tld op callbacks  a  b  string  transpose a  string          transpose b  string  a be sparse  string  b be sparse        return  result     except  core  fallbackexception        try          return sparse mat mul eager fallback              a  b  transpose a=transpose a  transpose b=transpose b              a be sparse=a be sparse  b be sparse=b be sparse  name=name              ctx= ctx        except  core  symbolicexception          pass       except  core  notokstatusexception as e         ops raise from not ok status e  name       if transpose a be none      transpose a = false   transpose a =  execute make bool transpose a  string    if transpose b be none      transpose b = false   transpose b =  execute make bool transpose b  string    if a be sparse be none      a be sparse = false   a be sparse =  execute make bool a be sparse  string    if b be sparse be none      b be sparse = false   b be sparse =  execute make bool b be sparse  string           op   output =  op def library  apply op helper          string  a=a  b=b  transpose a=transpose a                          transpose b=transpose b  a be sparse=a be sparse                          b be sparse=b be sparse  name=name     result =  output      if  execute must record gradient         attrs =  string   op  get attr bool string   string                 op  get attr bool string   string                 op  get attr bool string   string                 op  get attr bool string   string                 op  get attr type string   string   op  get attr type string        input flat =  op input      execute record gradient          string   input flat   attrs   result     result  =  result   return  result 
 tf export v1= string  string    deprecation deprecate none  string  def sparse merge sp ids  sp value  vocab size  name=none                   already sorted=false     string   return sparse merge impl sp ids  sp value  vocab size  name  already sort  
 tf export v1= string  string    deprecation deprecate endpoints string  def sparse placeholder dtype  shape=none  name=none     string   if context execute eagerly        raise runtimeerror string                        string     shape name =  name   string  if name be not none else none   shape  rank =  normalize sparse shape shape  shape name    if shape be none      shape = placeholder dtypes int64  shape= rank   name=shape name    return sparse tensor sparsetensor        values=placeholder            dtype            shape= none             name= name   string  if name be not none else none         indices=placeholder            dtypes int64            shape= none  rank             name= name   string  if name be not none else none         dense shape=shape  
 tf export v1= string  string    deprecation deprecate endpoints string   deprecation deprecate args      none  string  string   deprecation deprecate args      none  string      string  def sparse reduce max sp input  axis=none  keepdims=none                        reduction axes=none  keep dims=none     string   keepdims = deprecation deprecate argument lookup string  keepdims                                                      string  keep dim    if keepdims be none      keepdims = false    return gen sparse ops sparse reduce max        sp input indices  sp input value  sp input dense shape        math ops  reductiondims sp input  axis  reduction ax   keepdims  
 tf export v1= string  string    deprecation deprecate endpoints string   deprecation deprecate args      none  string  string  def sparse reduce max sparse sp input                               axis=none                               keepdims=none                               reduction axes=none                               keep dims=none     string   keepdims = deprecation deprecate argument lookup string  keepdims                                                      string  keep dim    if keepdims be none      keepdims = false    output ind  output val  output shape =         gen sparse ops sparse reduce max sparse            sp input indices  sp input value  sp input dense shape            math ops  reductiondims sp input  axis  reduction ax   keepdims      return sparse tensor sparsetensor output ind  output val  output shape  
 tf export v1= string  string    deprecation deprecate endpoints string   deprecation deprecate args      none  string  string   deprecation deprecate args      none  string      string  def sparse reduce sum sp input  axis=none  keepdims=none                        reduction axes=none  keep dims=none     string   keepdims = deprecation deprecate argument lookup string  keepdims                                                      string  keep dim    if keepdims be none      keepdims = false    return gen sparse ops sparse reduce sum        sp input indices  sp input value  sp input dense shape        math ops  reductiondims sp input  axis  reduction ax   keepdims  
 tf export v1= string  string    deprecation deprecate endpoints string   deprecation deprecate args      none  string  string  def sparse reduce sum sparse sp input                               axis=none                               keepdims=none                               reduction axes=none                               keep dims=none     string   keepdims = deprecation deprecate argument lookup string  keepdims                                                      string  keep dim    if keepdims be none      keepdims = false    output ind  output val  output shape =         gen sparse ops sparse reduce sum sparse            sp input indices  sp input value  sp input dense shape            math ops  reductiondims sp input  axis  reduction ax   keepdims      return sparse tensor sparsetensor output ind  output val  output shape  
 tf export v1= string  string    deprecation deprecate endpoints string  def sparse segment mean data                          indices                          segment ids                          name=none                          num segments=none     r   compute the mean along sparse segment of a tensor     read  the section on   segmentation  https //www tensorflow org/versions/r2 0/api docs/python/tf/math about segmentation    for an explanation of segment     like `tf math segment mean`  but `segment ids` can have rank less than   `data` s first dimension  select a subset of dimension 0  specify by   `indices`    `segment ids` be allow to have miss ids  in which case the output will   be zero at those indices  in those case `num segments` be use to determine   the size of the output     args      data  a `tensor` with data that will be assemble in the output      indices  a 1-d `tensor` with indices into `data`  have same rank as       `segment ids`      segment ids  a 1-d `tensor` with indices into the output `tensor`  value       should be sort and can be repeat      name  a name for the operation  optional       num segment  an optional int32 scalar  indicate the size of the output       `tensor`     return      a `tensor` of the shape as data  except for dimension 0 which     have size `k`  the number of segment specify via `num segments` or     infer for the last element in `segments ids`          if num segment be not none      return gen math ops sparse segment mean with num segment          data=data          indices=indices          segment ids=segment ids          num segments=num segment          name=name    else      return gen math ops sparse segment mean          data=data  indices=indices  segment ids=segment ids  name=name  
 tf export v1= string  string    deprecation deprecate endpoints string  def sparse segment sqrt n data                            indices                            segment ids                            name=none                            num segments=none     r   compute the sum along sparse segment of a tensor divide by the sqrt n      `n` be the size of the segment be reduce     args      data  a `tensor` with data that will be assemble in the output      indices  a 1-d `tensor` with indices into `data`  have same rank as       `segment ids`      segment ids  a 1-d `tensor` with indices into the output `tensor`  value       should be sort and can be repeat      name  a name for the operation  optional       num segment  an optional int32 scalar  indicate the size of the output       `tensor`     return      a `tensor` of the shape as data  except for dimension 0 which     have size `k`  the number of segment specify via `num segments` or     infer for the last element in `segments ids`          if num segment be not none      return gen math ops sparse segment sqrt n with num segment          data=data          indices=indices          segment ids=segment ids          num segments=num segment          name=name    else      return gen math ops sparse segment sqrt n          data=data  indices=indices  segment ids=segment ids  name=name  
 tf export v1= string  string    deprecation deprecate endpoints string  def sparse segment sum data                         indices                         segment ids                         name=none                         num segments=none     r   compute the sum along sparse segment of a tensor     read  the section on   segmentation  https //www tensorflow org/versions/r2 0/api docs/python/tf/math about segmentation    for an explanation of segment     like `tf math segment sum`  but `segment ids` can have rank less than `data` s   first dimension  select a subset of dimension 0  specify by `indices`    `segment ids` be allow to have miss ids  in which case the output will   be zero at those indices  in those case `num segments` be use to determine   the size of the output     for example     ```python   c = tf constant   1 2 3 4    -1 -2 -3 -4    5 6 7 8         select two row  one segment    tf sparse segment sum c  tf constant  0  1    tf constant  0  0        =>   0 0 0 0        select two row  two segment    tf sparse segment sum c  tf constant  0  1    tf constant  0  1        =>    1  2  3  4           -1 -2 -3 -4        with miss segment ids    tf sparse segment sum c  tf constant  0  1    tf constant  0  2                            num segments=4      =>    1  2  3  4            0  0  0  0           -1 -2 -3 -4            0  0  0  0        select all row  two segment    tf sparse segment sum c  tf constant  0  1  2    tf constant  0  0  1        =>   0 0 0 0           5 6 7 8        which be equivalent to    tf math segment sum c  tf constant  0  0  1      ```    args      data  a `tensor` with data that will be assemble in the output      indices  a 1-d `tensor` with indices into `data`  have same rank as       `segment ids`      segment ids  a 1-d `tensor` with indices into the output `tensor`  value       should be sort and can be repeat      name  a name for the operation  optional       num segment  an optional int32 scalar  indicate the size of the output       `tensor`     return      a `tensor` of the shape as data  except for dimension 0 which     have size `k`  the number of segment specify via `num segments` or     infer for the last element in `segments ids`          if num segment be not none      return gen math ops sparse segment sum with num segment          data=data          indices=indices          segment ids=segment ids          num segments=num segment          name=name    else      return gen math ops sparse segment sum          data=data  indices=indices  segment ids=segment ids  name=name  
 tf export v1= string  string    deprecation deprecate endpoints string   deprecation deprecate args      none  string  string  def sparse split keyword required=keywordrequired                     sp input=none                   num split=none                   axis=none                   name=none                   split dim=none     string   if not isinstance keyword require  keywordrequired       raise valueerror string    if sp input be none      raise valueerror string    if num split be none      raise valueerror string    if axis be none      raise valueerror string    axis = deprecation deprecate argument lookup string  axis  string                                                  split dim    sp input =  convert to sparse tensor sp input     output inds  output vals  output shape =         gen sparse ops sparse split            axis            sp input indices            sp input value            sp input dense shape            num split            name=name     sparse tensors =      for i in range 0  num split       sparse tensors append          sparse tensor sparsetensor output inds i   output vals i                                      output shape i      return sparse tensors 
 tf export v1= string    deprecation deprecate      none      string  def sparse to dense sparse indices                      output shape                      sparse value                      default value=0                      validate indices=true                      name=none     string   return gen sparse ops sparse to dense        sparse indices        output shape        sparse value        default value=default value        validate indices=validate indices        name=name  
 tf export v1= string    dispatch add dispatch support  deprecation deprecate args none  string                               string  def squeeze input  axis=none  name=none  squeeze dims=none        string   axis = deprecation deprecate argument lookup string  axis  string                                                  squeeze dim    if np isscalar axis       axis =  axis    return gen array ops squeeze input  axis  name  
 tf export v1= string    deprecation deprecate args none                               string                               string  def string split source  sep=none  skip empty=true  delimiter=none                   result type=string  name=none       string   with ops name scope name  string   source        sparse result = string ops string split          source  sep=sep  skip empty=skip empty  delimiter=delimiter      if result type == string        return sparse result     elif result type == string        return rag tensor raggedtensor from value rowids            values=sparse result value            value rowids=sparse result indices    0             nrows=sparse result dense shape 0             validate=false      else        raise valueerror string  
 tf export v1= string  string   def string to hash bucket v1      string tensor=none      num buckets=none      name=none      input=none     string tensor = deprecation deprecate argument lookup        string  input  string  string tensor    return gen string ops string to hash bucket string tensor  num bucket  name  
 tf export v1= string  string   def string to number v1      string tensor=none      out type=dtypes float32      name=none      input=none     string tensor = deprecation deprecate argument lookup        string  input  string  string tensor    return gen parse ops string to number string tensor  out type  name  
 tf export v1= string    deprecation deprecate none  string  def substr deprecate input  pos  len  name=none  unit=string     return substr input  pos  len  name=name  unit=unit  
 tf export v1= string  string   def table initializer name=string     string   initializers = ops get collection ops graphkeys table initializers    if initializers      return control flow ops group  initializers  name=name    return control flow ops no op name=name  
 deprecation deprecate date=none  instructions=string   tf export v1= string   def to bfloat16 x  name=string     string   return cast x  dtypes bfloat16  name=name  
 deprecation deprecate date=none  instructions=string   tf export v1= string   def to complex128 x  name=string     string   return cast x  dtypes complex128  name=name  
 deprecation deprecate date=none  instructions=string   tf export v1= string   def to complex64 x  name=string     string   return cast x  dtypes complex64  name=name  
 deprecation deprecate date=none  instructions=string   tf export v1= string   def to double x  name=string     string   return cast x  dtypes float64  name=name  
 deprecation deprecate date=none  instructions=string   tf export v1= string   def to float x  name=string     string   return cast x  dtypes float32  name=name  
 deprecation deprecate date=none  instructions=string   tf export v1= string   def to int32 x  name=string     string   return cast x  dtypes int32  name=name  
 deprecation deprecate date=none  instructions=string   tf export v1= string   def to int64 x  name=string     string   return cast x  dtypes int64  name=name  
 tf export v1= string   def trainable variables scope=none     string   return ops get collection ops graphkeys trainable variables  scope  
 tf export v1= string   def transpose a  perm=none  name=string  conjugate=false     string   with ops name scope name  string   a   as name      if not tensor util be tensor a         a = ops convert to tensor a  name=string       if conjugate and a dtype be complex        transpose fn = gen array ops conjugate transpose     else        transpose fn = gen array ops transpose      if perm be not none        return transpose fn a  perm  name=name       rank = a shape rank     if rank be none        perm = gen math ops  range gen array ops rank a  - 1  -1  -1      else        perm = np arange rank - 1  -1  -1  dtype=np int32      return transpose fn a  perm  name=name  
class truncatednormal initializer     string     deprecate args none                     string                    string  string    def   init   self  mean=0 0  stddev=1 0  seed=none  dtype=dtypes float32       self mean = mean     self stddev = stddev     self seed = seed     self dtype =  assert float dtype dtypes as dtype dtype      def   call   self  shape  dtype=none  partition info=none       if dtype be none        dtype = self dtype     return random ops truncate normal          shape  self mean  self stddev  dtype  seed=self seed     def get config self       return           string  self mean          string  self stddev          string  self seed          string  self dtype name       
 tf export v1= string   def tuple tensors  name=none  control inputs=none       string   if context execute eagerly        return tensors   with ops name scope name  string  tensors  as name      tensors =           t if  isinstance t  ops operation  or tensor util be tensor t  or               t be none  else ops convert to tensor t  for t in tensors           gate ops =           t if isinstance t  ops operation  else t op         for t in tensors         if t be not none           if control input        for c in control input          if isinstance c  ops tensor             c = c op         elif not isinstance c  ops operation             raise typeerror string   c          gate ops append c                gate ops = sort set gate ops   key=lambda op  op  id        if not gate ops        raise valueerror string   tensors      gate = group  gate ops      tpl =        for t in tensors        if tensor util be tensor t           tpl append with dependencies  gate   t         elif isinstance t  ops operation           with ops control dependencies  gate              tpl append group t         else          tpl append none      return tpl 
class uniformunitscaling initializer     string     deprecate args none                     string                    string  string     deprecate none                string               string    def   init   self  factor=1 0  seed=none  dtype=dtypes float32       self factor = factor     self seed = seed     self dtype =  assert float dtype dtypes as dtype dtype      def   call   self  shape  dtype=none  partition info=none       if dtype be none        dtype = self dtype     scale shape = shape     if partition info be not none        scale shape = partition info full shape      input size = 1 0                    for dim in scale shape  -1         input size  = float dim           input size = max input size  1 0      max val = math sqrt 3 / input size    self factor     return random ops random uniform          shape  -max val  max val  dtype  seed=self seed     def get config self       return  string  self factor  string  self seed  string  self dtype name  
 tf export v1= string   def variable axis size partitioner      max shard bytes  axis=0  bytes per string element=16  max shards=none     string   if max shard bytes < 1 or bytes per string element < 1      raise valueerror          string    if max shards and max shards < 1      raise valueerror          string     def  partitioner shape  dtype       string     if not isinstance shape  tensor shape tensorshape         raise valueerror string   shape      if not shape be fully define          raise valueerror string   shape      if not isinstance dtype  dtypes dtype         raise valueerror string   dtype       if dtype base dtype == dtypes string        element size = bytes per string element     else        element size = dtype size      partition =  1    shape ndims     bytes per slice = 1 0             shape num elements   / shape dim axis  value    element size               slice per shard = max 1  math floor max shard bytes / bytes per slice                 axis shards = int math ceil          1 0   shape dim axis  value / slice per shard       if max shards        axis shards = min max shards  axis shards       partition axis  = axis shards      return partition    return  partitioner 
 tf export v1= string    tf contextlib contextmanager def variable creator scope v1 variable creator     string   with ops get default graph    variable creator scope variable creator         yield 
 tf export v1= string    tf contextlib contextmanager def variable op scope value                        name or scope                        default name=none                        initializer=none                        regularizer=none                        cache device=none                        partitioner=none                        custom getter=none                        reuse=none                        dtype=none                        use resource=none                        constraint=none     string   log warn string                string    with variable scope        name or scope        default name=default name        values=values        initializer=initializer        regularizer=regularizer        cache device=caching device        partitioner=partitioner        custom getter=custom getter        reuse=reuse        dtype=dtype        use resource=use resource        constraint=constraint  as scope      yield scope 
class variable scope object     string    def   init   self                 name or scope                 default name=none                 values=none                 initializer=none                 regularizer=none                 cache device=none                 partitioner=none                 custom getter=none                 reuse=none                 dtype=none                 use resource=none                 constraint=none                 auxiliary name scope=true       string     self  name or scope = name or scope     self  default name = default name     self  value = value     self  initializer = initializer     self  regularizer = regularizer     self  cache device = cache device     self  partitioner = partitioner     self  custom getter = custom getter     self  reuse = reuse     self  dtype = dtype     self  use resource = use resource     self  constraint = constraint     if self  default name be none and self  name or scope be none        raise typeerror string      if self  reuse be false               self  reuse = none     if not  self  reuse be true             or self  reuse be none             or self  reuse be auto reuse         raise valueerror string      if self  value be none        self  value =        self  in graph mode = not context execute eagerly       if self  in graph mode        self  graph = ops  get graph from input self  value        self  cache pure variable scope = none     self  current name scope = none     if not isinstance auxiliary name scope  bool         raise typeerror string                       string format auxiliary name scope       self  auxiliary name scope = auxiliary name scope    def   enter   self                 if ops get default graph   build function        self  build function = true     else        self  build function = false     if self  in graph mode and not self  build function        self  graph context manager = self  graph as default         self  graph context manager   enter         if self  cache pure variable scope be not none                             if self  current name scope be not none          self  current name scope   enter           return self  cache pure variable scope   enter          try        return self  enter scope uncached       except        if  self  in graph mode and not self  build function and           self  graph context manager be not none           self  graph context manager   exit    sys exc info          raise    def  enter scope uncached self       string     if self  auxiliary name scope               current name scope = none     else               name scope = ops get name scope         if name scope                   name scope  = string         current name scope = ops name scope name scope        else                   current name scope = ops name scope name scope                 if self  name or scope be not none        if not isinstance self  name or scope                           variablescope     six string type           raise typeerror string                         string        if isinstance self  name or scope  six string type           name scope = self  name or scope       else          name scope = self  name or scope name split string  -1        if name scope or current name scope          current name scope = current name scope or ops name scope name scope          try            current name scope name = current name scope   enter             except            current name scope   exit    sys exc info              raise         self  current name scope = current name scope         if isinstance self  name or scope  six string type             old name scope = current name scope name         else            old name scope = self  name or scope original name scope         pure variable scope =  pure variable scope              self  name or scope              reuse=self  reuse              initializer=self  initializer              regularizer=self  regularizer              cache device=self  cache device              partitioner=self  partitioner              custom getter=self  custom getter              old name scope=old name scope              dtype=self  dtype              use resource=self  use resource              constraint=self  constraint          try            enter pure variable scope = pure variable scope   enter             except            pure variable scope   exit    sys exc info              raise         self  cache pure variable scope = pure variable scope         return enter pure variable scope       else          self  current name scope = none                  pure variable scope =  pure variable scope              self  name or scope              reuse=self  reuse              initializer=self  initializer              regularizer=self  regularizer              cache device=self  cache device              partitioner=self  partitioner              custom getter=self  custom getter              dtype=self  dtype              use resource=self  use resource              constraint=self  constraint          try            enter pure variable scope = pure variable scope   enter             except            pure variable scope   exit    sys exc info              raise         self  cache pure variable scope = pure variable scope         return enter pure variable scope      else          if self  reuse          raise valueerror string        current name scope = current name scope or ops name scope            self  default name        try          current name scope name = current name scope   enter           except          current name scope   exit    sys exc info            raise       self  current name scope = current name scope       unique default name =  get unique variable scope self  default name        pure variable scope =  pure variable scope            unique default name            initializer=self  initializer            regularizer=self  regularizer            cache device=self  cache device            partitioner=self  partitioner            custom getter=self  custom getter            old name scope=current name scope name            dtype=self  dtype            use resource=self  use resource            constraint=self  constraint        try          enter pure variable scope = pure variable scope   enter           except          pure variable scope   exit    sys exc info            raise       self  cache pure variable scope = pure variable scope       return enter pure variable scope    def   exit   self  type arg  value arg  traceback arg       try        self  cache pure variable scope   exit   type arg  value arg                                                  traceback arg      finally        try          if self  current name scope            self  current name scope   exit   type arg  value arg                                              traceback arg        finally          if self  in graph mode and not self  build function            self  graph context manager   exit   type arg  value arg                                                 traceback arg  
 tf export v1= string  string   def variables initializer var list  name=string     string   if var list and not context execute eagerly        return control flow ops group   v initializer for v in var list   name=name    return control flow ops no op name=name  
 tf export v1= string  string    deprecation deprecate endpoints string  def verify tensor all finite t=none  msg=none  name=none  x=none  message=none     string   x = deprecation deprecate argument lookup string  x  string  t    message = deprecation deprecate argument lookup        string  message  string  msg    return verify tensor all finite v2 x  message  name  
 tf export v1= string    dispatch add dispatch support def where condition  x=none  y=none  name=none     string   if x be none and y be none      with ops name scope name  string   condition   as name        condition = ops convert to tensor            condition  prefer dtype=dtypes bool  name=string        return gen array ops where condition=condition  name=name    elif x be not none and y be not none      return gen math ops select condition=condition  x=x  y=y  name=name    else      raise valueerror string  
 tf export v1= string   def while loop cond                 body                 loop vars                 shape invariants=none                 parallel iterations=10                 back prop=true                 swap memory=false                 name=none                 maximum iterations=none                 return same structure=false     string   if not callable cond       raise typeerror string    if not callable body       raise typeerror string    if parallel iterations < 1      raise typeerror string        execute eagerly = context execute eagerly     if  util enablecontrolflowv2 ops get default graph    and       not execute eagerly       return while v2 while loop          cond          body          loop vars          shape invariants=shape invariants          parallel iterations=parallel iterations          maximum iterations=maximum iterations          name=name          return same structure=return same structure          back prop=back prop     with ops name scope name  string  loop vars       if not loop vars        raise valueerror string      try to pack =  len loop vars  == 1 and not return same structure      if maximum iterations be not none        maximum iterations = ops convert to tensor            maximum iterations  name=string        if maximum iterations shape ndims  = 0          raise valueerror string                            maximum iterations shape         if execute eagerly          counter = 0         maximum iterations = int maximum iterations numpy          else          counter = constant op constant              0  dtype=maximum iterations dtype  name=string        orig cond = cond       orig body = body       if try to pack          loop vars =  counter  loop vars 0           cond = lambda i  lv                  math ops logical and i < maximum iterations  orig cond lv            body = lambda i  lv   i   1  orig body lv         else          loop vars =  counter  loop vars          cond = lambda i  lv                  math ops logical and i < maximum iterations  orig cond  lv            body = lambda i  lv   i   1  orig body  lv         try to pack = false      if execute eagerly        pack = false          loop var structure = nest map structure type spec type spec from value                                                list loop vars         while cond  loop vars           loop vars = body  loop vars          if try to pack and not isinstance loop vars   list   basetuple              pack = true           loop vars =  loop vars           nest assert same structure loop var structure  list loop vars          def convert x           if isinstance x  tensor array ops tensorarray             return x         return ops convert to tensor x         loop vars = nest map structure convert  loop vars  expand composites=true        if maximum iterations be not none          return loop vars 1        else          return loop vars 0  if pack else loop vars      if shape invariants be not none        if maximum iterations be not none          shape invariants =  tensor shape tensorshape      shape invariants         nest assert same structure            loop vars  shape invariants  expand composites=false        shape invariants = nest map structure             get shape invariant            loop vars            shape invariants            expand composites=false       loop context = whilecontext          maximum iterations=maximum iterations          parallel iterations=parallel iterations          back prop=back prop          swap memory=swap memory                if loop context outer context be none        ops add to collection ops graphkeys while context  loop context      result = loop context buildloop cond  body  loop vars  shape invariants                                      return same structure      if maximum iterations be not none        return result 1      else        return result 
 tf export v1= string   def wrap function fn  signature  name=none     string   holder = variableholder fn    func graph name = string   if name be not none      func graph name = string   name   return wrappedfunction        func graph func graph from py func            func graph name            holder            args=none            kwargs=none            signature=signature            add control dependencies=false            collections=           variable holder=holder        signature=signature  
 tf export v1= string    dispatch add dispatch support def zero like tensor  dtype=none  name=none  optimize=true     string   return zero like impl tensor  dtype  name  optimize  
 tf export v1= string   def run main=none  argv=none     string    main = main or  sys modules string  main     run main=main  argv=argv  flag parser= parse flag tolerate undef  
 tf export v1= string   def to code v1 entity                 recursive=true                 arg values=none                 arg types=none                 indentation=string                 experimental optional features=none     string   del arg value   del arg type   del indentation   return to code        entity        recursive=recursive        experimental optional features=experimental optional feature  
 tf export v1= string   def to graph v1 entity                  recursive=true                  arg values=none                  arg types=none                  experimental optional features=none     string   del arg type   del arg value   return to graph        entity        recursive=recursive        experimental optional features=experimental optional feature  
class datasetv1 datasetv2     string    def   init   self       try        variant tensor = self  as variant tensor       except attributeerror as e        if string in str e           raise attributeerror string                              string                              string        raise attributeerror string                            string                            string                            string                            string                            string                            string format e       super datasetv1  self    init   variant tensor      abc abstractmethod   def  as variant tensor self       string     raise notimplementederror string      deprecation deprecate        none  string       string       string       string    def make one shoot iterator self       string     return self  make one shoot iterator      def  make one shoot iterator self         if context execute eagerly          return iterator ops ownediterator self        ensure same dataset graph self                                         all ds ops = traverse obtain all variant tensor ops self      graph level seed  op level seed = core random seed get seed none                  function defun capture by value=true  whitelisted stateful ops=all ds ops      def  make dataset          string                            if graph level seed be not none          assert op level seed be not none         core random seed set random seed               graph level seed   87654321   op level seed     2    63 - 1          dataset = self  apply options         return dataset  variant tensor        try         make dataset add to graph ops get default graph        except valueerror as err        if string in str err           raise valueerror              string             string             string             string             string   err        else          six reraise valueerror  err            return iterator ops iterator          gen dataset ops one shoot iterator              dataset factory= make dataset    self  flat structure   none          get legacy output type self   get legacy output shape self           get legacy output class self       deprecation deprecate        none  string       string       string       string    def make initializable iterator self  share name=none       string      return self  make initializable iterator share name     def  make initializable iterator self  share name=none         if context execute eagerly          raise runtimeerror            string           string       ensure same dataset graph self      dataset = self  apply options       if share name be none        share name = string     iterator resource = gen dataset ops iterator v2          container=string  share name=shared name    self  flat structure      with ops colocate with iterator resource         initializer = gen dataset ops make iterator            dataset  variant tensor              iterator resource           return iterator ops iterator          iterator resource  initializer  get legacy output type dataset           get legacy output shape dataset   get legacy output class dataset       property    deprecation deprecate        none  string    def output class self       string     return nest map structure          lambda component spec  component spec  to legacy output class              self element spec      property    deprecation deprecate        none  string    def output shape self       string     return nest map structure          lambda component spec  component spec  to legacy output shape              self element spec      property    deprecation deprecate        none  string    def output type self       string     return nest map structure          lambda component spec  component spec  to legacy output type              self element spec      property   def element spec self                 return structure convert legacy structure          self output type  self output shape  self output class      staticmethod    functools wrap datasetv2 from tensors    def from tensors tensors       return datasetv1adapter datasetv2 from tensors tensors       staticmethod    functools wrap datasetv2 from tensor slice    def from tensor slice tensors       return datasetv1adapter datasetv2 from tensor slice tensors       staticmethod    deprecation deprecate none  string    def from sparse tensor slice sparse tensor       string     return datasetv1adapter sparsetensorslicedataset sparse tensor       staticmethod    functools wrap datasetv2 from generator    def from generator generator  output type  output shapes=none  args=none       return datasetv1adapter datasetv2 from generator          generator  output type  output shape  args       staticmethod    functools wrap datasetv2 range    def range  args       return datasetv1adapter datasetv2 range  args       staticmethod    functools wrap datasetv2 zip    def zip datasets       return datasetv1adapter datasetv2 zip datasets       functools wrap datasetv2 concatenate    def concatenate self  dataset       return datasetv1adapter super datasetv1  self  concatenate dataset       functools wrap datasetv2 prefetch    def prefetch self  buffer size       return datasetv1adapter super datasetv1  self  prefetch buffer size       staticmethod    functools wrap datasetv2 list file    def list file file pattern  shuffle=none  seed=none       return datasetv1adapter datasetv2 list file file pattern  shuffle  seed       functools wrap datasetv2 repeat    def repeat self  count=none       return datasetv1adapter super datasetv1  self  repeat count       functools wrap datasetv2 shuffle    def shuffle self  buffer size  seed=none  reshuffle each iteration=none       return datasetv1adapter super datasetv1  self  shuffle          buffer size  seed  reshuffle each iteration       functools wrap datasetv2 cache    def cache self  filename=string       return datasetv1adapter super datasetv1  self  cache filename       functools wrap datasetv2 take    def take self  count       return datasetv1adapter super datasetv1  self  take count       functools wrap datasetv2 skip    def skip self  count       return datasetv1adapter super datasetv1  self  skip count       functools wrap datasetv2 shard    def shard self  num shards  index       return datasetv1adapter super datasetv1  self  shard num shards  index       functools wrap datasetv2 batch    def batch self  batch size  drop remainder=false       return datasetv1adapter super datasetv1  self  batch          batch size  drop remainder       functools wrap datasetv2 pad batch    def pad batch self                     batch size                     pad shape                     pad values=none                     drop remainder=false       return datasetv1adapter super datasetv1  self  pad batch          batch size  pad shape  pad value  drop remainder       functools wrap datasetv2 map    def map self  map func  num parallel calls=none       if num parallel call be none        return datasetv1adapter            mapdataset self  map func  preserve cardinality=false       else        return datasetv1adapter            parallelmapdataset                self  map func  num parallel call  preserve cardinality=false       deprecation deprecate none  string    def map with legacy function self  map func  num parallel calls=none       string     if num parallel call be none        return datasetv1adapter            mapdataset                self                map func                preserve cardinality=false                use legacy function=true       else        return datasetv1adapter            parallelmapdataset                self                map func                num parallel call                preserve cardinality=false                use legacy function=true       functools wrap datasetv2 flat map    def flat map self  map func       return datasetv1adapter super datasetv1  self  flat map map func       functools wrap datasetv2 interleave    def interleave self                   map func                   cycle length=autotune                   block length=1                   num parallel calls=none       return datasetv1adapter super datasetv1  self  interleave          map func  cycle length  block length  num parallel call       functools wrap datasetv2 filter    def filter self  predicate       return datasetv1adapter super datasetv1  self  filter predicate       deprecation deprecate none  string    def filter with legacy function self  predicate       string     return filterdataset self  predicate  use legacy function=true      functools wrap datasetv2 apply    def apply self  transformation func       return datasetv1adapter super datasetv1  self  apply transformation func       functools wrap datasetv2 window    def window self  size  shift=none  stride=1  drop remainder=false       return datasetv1adapter super datasetv1  self  window          size  shift  stride  drop remainder       functools wrap datasetv2 unbatch    def unbatch self       return datasetv1adapter super datasetv1  self  unbatch        functools wrap datasetv2 with options    def with options self  options       return datasetv1adapter super datasetv1  self  with options options   
class fixedlengthrecorddatasetv1 dataset ops datasetv1adapter     string    def   init   self                 filenames                 record bytes                 header bytes=none                 footer bytes=none                 buffer size=none                 compression type=none                 num parallel reads=none       wrap = fixedlengthrecorddatasetv2 filenames  record bytes  header bytes                                           footer bytes  buffer size                                           compression type  num parallel read      super fixedlengthrecorddatasetv1  self    init   wrap       init     doc   = fixedlengthrecorddatasetv2   init     doc       property   def  filenames self       return self  dataset  filenames        filenames setter   def  filenames self  value       self  dataset  filenames = value   
class iterator trackable trackable     string    def   init   self  iterator resource  initializer  output type                 output shape  output class       string     self  iterator resource = iterator resource     self  initializer = initializer      if  output type be none or output shape be none         or output class be none         raise valueerror string                        string                        string      self  element spec = structure convert legacy structure          output type  output shape  output class      self  flat tensor shape = structure get flat tensor shape          self  element spec      self  flat tensor type = structure get flat tensor type          self  element spec       self  string handle = gen dataset ops iterator to string handle          self  iterator resource      self  get next call count = 0     ops add to collection global iterators  self  iterator resource      staticmethod   def from structure output type                       output shapes=none                       share name=none                       output classes=none       string     output type = nest map structure dtypes as dtype  output type      if output shape be none        output shape = nest map structure            lambda    tensor shape tensorshape none   output type      else        output shape = nest map structure up to output type                                                 tensor shape as shape                                                 output shape      if output class be none        output class = nest map structure lambda    ops tensor  output type      nest assert same structure output type  output shape      output structure = structure convert legacy structure          output type  output shape  output class      if share name be none        share name = string     if  device stack be empty          with ops device string           iterator resource = gen dataset ops iterator v2              container=string              share name=shared name              output types=structure get flat tensor type                  output structure               output shapes=structure get flat tensor shape                  output structure       else        iterator resource = gen dataset ops iterator v2            container=string            share name=shared name            output types=structure get flat tensor type output structure             output shapes=structure get flat tensor shape                output structure       return iterator iterator resource  none  output type  output shape                      output class      staticmethod   def from string handle string handle                           output type                           output shapes=none                           output classes=none       string     output type = nest map structure dtypes as dtype  output type      if output shape be none        output shape = nest map structure            lambda    tensor shape tensorshape none   output type      else        output shape = nest map structure up to output type                                                 tensor shape as shape                                                 output shape      if output class be none        output class = nest map structure lambda    ops tensor  output type      nest assert same structure output type  output shape      output structure = structure convert legacy structure          output type  output shape  output class      string handle = ops convert to tensor string handle  dtype=dtypes string      if  device stack be empty          with ops device string           iterator resource = gen dataset ops iterator from string handle v2              string handle              output types=structure get flat tensor type output structure               output shapes=structure get flat tensor shape output structure       else        iterator resource = gen dataset ops iterator from string handle v2            string handle            output types=structure get flat tensor type output structure             output shapes=structure get flat tensor shape output structure       return iterator iterator resource  none  output type  output shape                      output class      property   def initializer self       string     if self  initializer be not none        return self  initializer     else                      raise valueerror string     def make initializer self  dataset  name=none       string     with ops name scope name  string  as name                             dataset output type = nest map structure            lambda component spec  component spec  to legacy output type              dataset element spec        dataset output shape = nest map structure            lambda component spec  component spec  to legacy output shape              dataset element spec        dataset output class = nest map structure            lambda component spec  component spec  to legacy output class              dataset element spec                nest assert same structure self output type  dataset output type        nest assert same structure self output shape  dataset output shape        for iterator class  dataset class in zip            nest flatten self output class             nest flatten dataset output class            if iterator class be not dataset class            raise typeerror                string                  self output class  dataset output class         for iterator dtype  dataset dtype in zip            nest flatten self output type   nest flatten dataset output type            if iterator dtype  = dataset dtype            raise typeerror                string                  self output type  dataset output type         for iterator shape  dataset shape in zip            nest flatten self output shape   nest flatten                dataset output shape            if not iterator shape be compatible with dataset shape             raise typeerror string                           string                              self output shape  dataset output shape       with ops colocate with self  iterator resource         return gen dataset ops make iterator            dataset  variant tensor  self  iterator resource  name=name       def get next self  name=none       string     self  get next call count  = 1     if self  get next call count > get next call warn threshold        warn warn get next call warn message            flat ret = gen dataset ops iterator get next          self  iterator resource          output types=self  flat tensor type          output shapes=self  flat tensor shape          name=name      return structure from tensor list self  element spec  flat ret     def string handle self  name=none       string     if name be none        return self  string handle     else        return gen dataset ops iterator to string handle            self  iterator resource  name=name      property    deprecation deprecate        none  string    def output class self       string     return nest map structure          lambda component spec  component spec  to legacy output class              self  element spec      property    deprecation deprecate        none  string    def output shape self       string     return nest map structure          lambda component spec  component spec  to legacy output shape              self  element spec      property    deprecation deprecate        none  string    def output type self       string     return nest map structure          lambda component spec  component spec  to legacy output type              self  element spec      property   def element spec self       string     return self  element spec    def  gather saveables for checkpoint self        def  saveable factory name         return  iteratorsaveable self  iterator resource  name       return  string   saveable factory  
class tfrecorddatasetv1 dataset ops datasetv1adapter     string    def   init   self                 filenames                 compression type=none                 buffer size=none                 num parallel reads=none       wrap = tfrecorddatasetv2 filenames  compression type  buffer size                                  num parallel read      super tfrecorddatasetv1  self    init   wrap       init     doc   = tfrecorddatasetv2   init     doc      def  clone self               filenames=none               compression type=none               buffer size=none               num parallel reads=none            return tfrecorddatasetv1          filenames or self  dataset  filenames  compression type or         self  dataset  compression type  buffer size or         self  dataset  buffer size  num parallel read or         self  dataset  num parallel read      property   def  filenames self       return self  dataset  filenames        filenames setter   def  filenames self  value       self  dataset  filenames = value   
class textlinedatasetv1 dataset ops datasetv1adapter     string    def   init   self                 filenames                 compression type=none                 buffer size=none                 num parallel reads=none       wrap = textlinedatasetv2 filenames  compression type  buffer size                                  num parallel read      super textlinedatasetv1  self    init   wrap       init     doc   = textlinedatasetv2   init     doc       property   def  filenames self       return self  dataset  filenames        filenames setter   def  filenames self  value       self  dataset  filenames = value   
 tf export v1= string   def get legacy output class dataset or iterator     string   return nest map structure        lambda component spec  component spec  to legacy output class            get structure dataset or iterator   
 tf export v1= string   def get legacy output shape dataset or iterator     string   return nest map structure        lambda component spec  component spec  to legacy output shape            get structure dataset or iterator   
 tf export v1= string   def get legacy output type dataset or iterator     string   return nest map structure        lambda component spec  component spec  to legacy output type            get structure dataset or iterator   
 tf export v1= string   def make initializable iterator dataset  share name=none     string   try                return dataset  make initializable iterator share name      except attributeerror      return datasetv1adapter dataset   make initializable iterator share name    
 tf export v1= string   def make one shoot iterator dataset     string   try                return dataset  make one shoot iterator       except attributeerror      return datasetv1adapter dataset   make one shoot iterator     
 tf export v1= string   def assert shape shape  data=none  summarize=none  message=none  name=none     string         if isinstance shape  dict       shape = shape items      message = message or string   with ops name scope name  string   shape  data             shape constraints =            ops convert to tensor x   s  for x  s in shape if s be not none            execute eagerly = context execute eagerly        def tensor name x         if execute eagerly          return  shape and dtype str x        return x name      tensor dim size =        for tensor  symbolic shape in shape constraints        be iterable =             hasattr symbolic shape  string  or           hasattr symbolic shape  string                  if not be iterable          raise valueerror              string             string             string             string                message  tensor name tensor   symbolic shape                 symbolic shape tuple = tuple symbolic shape         tensors specify innermost = false       for i  symbol in enumerate symbolic shape tuple           if symbol not in  ellipsis  string             continue          if i  = 0            raise valueerror                string               string               string               string                  message  tensor name tensor   i            tensors specify innermost = true                      tensor dim size append             tensordimsizes                tensor  tensors specify innermost   dimension size tensor                  symbolic dimension size                    symbolic shape tuple 1                     if tensors specify innermost else symbolic shape tuple         rank assertions =        for size in tensor dim size        rank = len size symbolic size        rank zero or one = rank in  0  1        if size unspecified dim          if rank zero or one                                  continue         assertion = assert rank at least              x=sizes x              rank=rank              data=data              summarize=summarize              message=message              name=name        elif rank zero or one                                     assertion = assert rank in              x=sizes x              ranks= 0  1               data=data              summarize=summarize              message=message              name=name        else          assertion = assert rank              x=sizes x              rank=rank              data=data              summarize=summarize              message=message              name=name        rank assertions append assertion       size assertions =        size specifications =        for size in tensor dim size        for i  size symbol in enumerate size symbolic size            if  be symbol for any size size symbol                        continue          if size unspecified dim            tensor dim = i - len size symbolic size          else            tensor dim = i          if size symbol in size specifications or  have know value size symbol             if  have know value size symbol               specify size = int size symbol              size check message = string           else              specify size  specify by y  specify at dim = \                 size specifications size symbol              size check message =                   string                    tensor name specify by y   specify at dim              actual size = size actual size tensor dim            if  have know value actual size  and  have know value specify size               if int actual size   = int specify size                 raise valueerror                    string                   string                      message  size check message  tensor name size x                      tensor dim  specify size  actual size                     size x get shape                              continue            condition = math ops equal                ops convert to tensor actual size                 ops convert to tensor specify size             data  = data           if data be none              data  =                   message  size check message                  string   tensor name size x   tensor dim                  string  specify size  string                  array ops shape size x                          size assertions append                control flow ops assert condition  data   summarize=summarize           else            size = size actual size tensor dim            size specifications size symbol  =  size  size x  tensor dim       with ops control dependencies rank assertions         shape assertion = control flow ops group size assertions      return shape assertion 
class mirroredstrategyv1 distribute lib strategyv1          doc   = mirroredstrategy   doc      def   init   self  devices=none  cross device ops=none       extend = mirroredextended          self  devices=devices  cross device ops=cross device ops      super mirroredstrategyv1  self    init   extend      distribute lib distribution strategy gauge get cell string  set          string  
class onedevicestrategyv1 distribute lib strategyv1        doc   = onedevicestrategy   doc   replace        string        string     def   init   self  device       super onedevicestrategyv1  self    init   onedeviceextended self  device       init     doc   = onedevicestrategy   init     doc   
class replicacontext object     string    def   init   self  strategy  replica id in sync group       self  strategy = strategy     self  thread context = distribution strategy context  inreplicathreadmode            self      self  replica id in sync group = replica id in sync group     self  summary record distribution strategy = none    def   enter   self        push per thread mode self  thread context       def replica id be zero          return math ops equal self  replica id in sync group                              constant op constant 0        summary state = summary ops v2  summary state       self  summary record distribution strategy =           summary state be record distribution strategy      summary state be record distribution strategy = replica id be zero    def   exit   self  exception type  exception value  traceback       summary state = summary ops v2  summary state       summary state be record distribution strategy =           self  summary record distribution strategy       pop per thread mode      def merge call self  merge fn  args=    kwargs=none       string     require replica context self      if kwargs be none        kwargs =        return self  merge call merge fn  args  kwargs     def  merge call self  merge fn  args  kwargs       string      push per thread mode            distribution strategy context  crossreplicathreadmode self  strategy         try        return merge fn self  strategy   args    kwargs      finally         pop per thread mode       property   def num replicas in sync self       string     return self  strategy num replicas in sync     property   def replica id in sync group self       string     require replica context self      return self  replica id in sync group     property   def strategy self       string     return self  strategy     property   def devices self       string     require replica context self      return  device util current        def all reduce self  reduce op  value       string     def batch all reduce strategy   value flat         return strategy extend batch reduce to            reduce op    v   batch reduce destination v   for v in value flat        if reduce op in  reduce util reduceop sum  reduce util reduceop mean                 custom gradient custom gradient       def grad wrapper  xs           ys = self merge call batch all reduce  args=xs                   return ys  lambda  dy s  self all reduce reduce op  dy s        return nest pack sequence as value  grad wrapper  nest flatten value        else               reduce = nest pack sequence as            value  self merge call batch all reduce  args=nest flatten value          return nest map structure array ops prevent gradient  reduce  
class strategyv1 strategy     string    def make dataset iterator self  dataset       string     return self  extend  make dataset iterator dataset       def make input fn iterator self                                 input fn                               replication mode=inputreplicationmode per worker       string     return super strategyv1  self  make input fn iterator          input fn  replication mode     def experimental make numpy dataset self  numpy input  session=none       string     return self extend experimental make numpy dataset          numpy input  session=session     def experimental run self  fn  input iterator=none         string     return super strategyv1  self  experimental run          fn  input iterator     def reduce self  reduce op  value  axis=none       return super strategyv1  self  reduce reduce op  value  axis     reduce   doc   = strategy reduce   doc      def update config proto self  config proto       string     return self  extend  update config proto config proto    
class strategyextendedv1 strategyextendedv2        doc   = strategyextendedv2   doc      def experimental make numpy dataset self  numpy input  session=none       string      require cross replica or default context extend self      return self  experimental make numpy dataset numpy input  session=session     def  experimental make numpy dataset self  numpy input  session       raise notimplementederror string     def broadcast to self  tensor  destinations       string     assert destinations be not none             require cross replica or default context extend self      assert not isinstance destinations   list  tuple       return self  broadcast to tensor  destinations     def  broadcast to self  tensor  destinations       raise notimplementederror string     def experimental run step on iterator self                                           fn                                           iterator                                           iterations=1                                           initial loop values=none       string      require cross replica or default context extend self      with self  container strategy   scope          return self  experimental run step on iterator fn  iterator  iterations                                                        initial loop value     def  experimental run step on iterator self  fn  iterator  iterations                                            initial loop value       raise notimplementederror string     def call for each replica self  fn  args=    kwargs=none       string      require cross replica or default context extend self      if kwargs be none        kwargs =        with self  container strategy   scope          return self  call for each replica fn  args  kwargs     def  call for each replica self  fn  args  kwargs       raise notimplementederror string     def read var self  v       string     raise notimplementederror string      property   def experimental between graph self       string     raise notimplementederror string      property   def experimental should init self       string     raise notimplementederror string      property   def should checkpoint self       string     raise notimplementederror string      property   def should save summary self       string     raise notimplementederror string  
 tf export v1= string   def get loss reduction      string   if not distribution strategy context get strategy    scale loss for estimator                  return reduce util reduceop sum   last reduction = ops get default graph    last loss reduction     if  last reduction == losses impl reduction sum or       last reduction == loss reduction reductionv2 sum       return reduce util reduceop sum   return reduce util reduceop mean 
class bernoulli distribution distribution     string     deprecation deprecate        string        string       string       string       string       string        warn once=true    def   init   self                 logits=none                 probs=none                 dtype=dtypes int32                 validate args=false                 allow nan stats=true                 name=string       string     parameters = dict locals        with ops name scope name  as name        self  logits  self  probs = distribution util get logits and probs            logits=logits            probs=probs            validate args=validate args            name=name      super bernoulli  self    init            dtype=dtype          reparameterization type=distribution not reparameterized          validate args=validate args          allow nan stats=allow nan stats          parameters=parameters          graph parents= self  logits  self  probs           name=name      staticmethod   def  param shape sample shape       return  string  ops convert to tensor sample shape  dtype=dtypes int32       property   def logits self       string     return self  logits     property   def probs self       string     return self  probs    def  batch shape tensor self       return array ops shape self  logits     def  batch shape self       return self  logits get shape      def  event shape tensor self       return array ops constant     dtype=dtypes int32     def  event shape self       return tensor shape tensorshape        def  sample n self  n  seed=none       new shape = array ops concat   n   self batch shape tensor     0      uniform = random ops random uniform          new shape  seed=seed  dtype=self probs dtype      sample = math ops less uniform  self probs      return math ops cast sample  self dtype     def  log prob self  event       if self validate args        event = distribution util embed check integer cast close            event  target dtype=dtypes bool                 event = math ops cast event  self logits dtype      logits = self logits                def  broadcast logits  event         return  array ops ones like event    logits                array ops ones like logits    event       if not  event get shape   be fully define   and             logits get shape   be fully define   and             event get shape   == logits get shape           logits  event =  broadcast logits  event      return -nn sigmoid cross entropy with logits labels=event  logits=logits     def  entropy self       return  -self logits    math ops sigmoid self logits  - 1                nn softplus -self logits      def  mean self       return array ops identity self probs     def  variance self       return self  mean      1  - self probs     def  mode self       string     return math ops cast self probs > 0 5  self dtype  
class beta distribution distribution     string     deprecation deprecate        string        string       string       string       string       string        warn once=true    def   init   self                 concentration1=none                 concentration0=none                 validate args=false                 allow nan stats=true                 name=string       string     parameters = dict locals        with ops name scope name  values= concentration1  concentration0   as name        self  concentration1 = self  maybe assert valid concentration            ops convert to tensor concentration1  name=string             validate args        self  concentration0 = self  maybe assert valid concentration            ops convert to tensor concentration0  name=string             validate args        check ops assert same float dtype             self  concentration1  self  concentration0         self  total concentration = self  concentration1   self  concentration0     super beta  self    init            dtype=self  total concentration dtype          validate args=validate args          allow nan stats=allow nan stats          reparameterization type=distribution fully reparameterized          parameters=parameters          graph parents= self  concentration1                         self  concentration0                         self  total concentration           name=name      staticmethod   def  param shape sample shape       return dict zip           string  string            ops convert to tensor sample shape  dtype=dtypes int32     2       property   def concentration1 self       string     return self  concentration1     property   def concentration0 self       string     return self  concentration0     property   def total concentration self       string     return self  total concentration    def  batch shape tensor self       return array ops shape self total concentration     def  batch shape self       return self total concentration get shape      def  event shape tensor self       return constant op constant     dtype=dtypes int32     def  event shape self       return tensor shape tensorshape        def  sample n self  n  seed=none       expand concentration1 = array ops ones like          self total concentration  dtype=self dtype    self concentration1     expand concentration0 = array ops ones like          self total concentration  dtype=self dtype    self concentration0     gamma1 sample = random ops random gamma          shape= n           alpha=expanded concentration1          dtype=self dtype          seed=seed      gamma2 sample = random ops random gamma          shape= n           alpha=expanded concentration0          dtype=self dtype          seed=distribution util gen new seed seed  string       beta sample = gamma1 sample /  gamma1 sample   gamma2 sample      return beta sample     distribution util appenddocstring  beta sample note    def  log prob self  x       return self  log unnormalized prob x  - self  log normalization       distribution util appenddocstring  beta sample note    def  prob self  x       return math ops exp self  log prob x       distribution util appenddocstring  beta sample note    def  log cdf self  x       return math ops log self  cdf x       distribution util appenddocstring  beta sample note    def  cdf self  x       return math ops betainc self concentration1  self concentration0  x     def  log unnormalized prob self  x       x = self  maybe assert valid sample x      return  math ops xlogy self concentration1 - 1   x                 self concentration0 - 1     math ops log1p -x      def  log normalization self       return  math ops lgamma self concentration1                math ops lgamma self concentration0              - math ops lgamma self total concentration      def  entropy self       return           self  log normalization           -  self concentration1 - 1     math ops digamma self concentration1          -  self concentration0 - 1     math ops digamma self concentration0              self total concentration - 2                math ops digamma self total concentration       def  mean self       return self  concentration1 / self  total concentration    def  variance self       return self  mean      1  - self  mean    /  1    self total concentration      distribution util appenddocstring        string    def  mode self       mode =  self concentration1 - 1   /  self total concentration - 2       if self allow nan stats        nan = array ops fill            self batch shape tensor              np array np nan  dtype=self dtype as numpy dtype               name=string        be define = math ops logical and self concentration1 > 1                                           self concentration0 > 1         return array ops where v2 be define  mode  nan      return control flow ops with dependencies           check ops assert less              array ops ones     dtype=self dtype               self concentration1              message=string           check ops assert less              array ops ones     dtype=self dtype               self concentration0              message=string         mode     def  maybe assert valid concentration self  concentration  validate args       string     if not validate args        return concentration     return control flow ops with dependencies           check ops assert positive              concentration              message=string          concentration     def  maybe assert valid sample self  x       string     if not self validate args        return x     return control flow ops with dependencies           check ops assert positive x  message=string           check ops assert less              x              array ops ones     self dtype               message=string          x  
class categorical distribution distribution     string     deprecation deprecate        string        string       string       string       string       string        warn once=true    def   init          self        logits=none        probs=none        dtype=dtypes int32        validate args=false        allow nan stats=true        name=string       string     parameters = dict locals        with ops name scope name  values= logits  probs   as name        self  logits  self  probs = distribution util get logits and probs            logits=logits            probs=probs            validate args=validate args            multidimensional=true            name=name         if validate args          self  logits = distribution util embed check categorical event shape              self  logits         logits shape static = self  logits get shape   with rank at least 1        if logits shape static ndims be not none          self  batch rank = ops convert to tensor              logits shape static ndims - 1              dtype=dtypes int32              name=string        else          with ops name scope name=string             self  batch rank = array ops rank self  logits  - 1        logits shape = array ops shape self  logits  name=string        if tensor shape dimension value logits shape static -1   be not none          self  event size = ops convert to tensor              logits shape static dim -1  value              dtype=dtypes int32              name=string        else          with ops name scope name=string             self  event size = logits shape self  batch rank         if logits shape static  -1  be fully define            self  batch shape val = constant op constant              logits shape static  -1  as list                dtype=dtypes int32              name=string        else          with ops name scope name=string             self  batch shape val = logits shape  -1      super categorical  self    init            dtype=dtype          reparameterization type=distribution not reparameterized          validate args=validate args          allow nan stats=allow nan stats          parameters=parameters          graph parents= self  logits                         self  probs           name=name      property   def event size self       string     return self  event size     property   def logits self       string     return self  logits     property   def probs self       string     return self  probs    def  batch shape tensor self       return array ops identity self  batch shape val     def  batch shape self       return self logits get shape    -1     def  event shape tensor self       return constant op constant     dtype=dtypes int32     def  event shape self       return tensor shape tensorshape        def  sample n self  n  seed=none       if self logits get shape   ndims == 2        logits 2d = self logits     else        logits 2d = array ops reshape self logits   -1  self event size       sample dtype = dtypes int64 if self dtype size > 4 else dtypes int32     draw = random ops multinomial          logits 2d  n  seed=seed  output dtype=sample dtype      draw = array ops reshape          array ops transpose draw           array ops concat   n   self batch shape tensor     0       return math ops cast draw  self dtype     def  cdf self  k       k = ops convert to tensor k  name=string      if self validate args        k = distribution util embed check integer cast close            k  target dtype=dtypes int32       k  probs =  broadcast cat event and params          k  self probs  base dtype=self dtype base dtype            batch flatten probs = array ops reshape probs                                                 -1  self  event size       batch flatten k = array ops reshape k   -1        to sum over = array ops where          array ops sequence mask batch flatten k  self  event size           batch flatten probs          array ops zero like batch flatten probs       batch flatten cdf = math ops reduce sum to sum over  axis=-1           return array ops reshape batch flatten cdf  array ops shape k      def  log prob self  k       k = ops convert to tensor k  name=string      if self validate args        k = distribution util embed check integer cast close            k  target dtype=dtypes int32      k  logits =  broadcast cat event and params          k  self logits  base dtype=self dtype base dtype       return -nn ops sparse softmax cross entropy with logits labels=k                                                              logits=logits     def  entropy self       return -math ops reduce sum          nn ops log softmax self logits    self probs  axis=-1     def  mode self       ret = math ops argmax self logits  axis=self  batch rank      ret = math ops cast ret  self dtype      ret set shape self batch shape      return ret 
class dirichlet distribution distribution     string     deprecation deprecate        string        string       string       string       string       string        warn once=true    def   init   self                 concentration                 validate args=false                 allow nan stats=true                 name=string       string     parameters = dict locals        with ops name scope name  values= concentration   as name        self  concentration = self  maybe assert valid concentration            ops convert to tensor concentration  name=string             validate args        self  total concentration = math ops reduce sum self  concentration  -1      super dirichlet  self    init            dtype=self  concentration dtype          validate args=validate args          allow nan stats=allow nan stats          reparameterization type=distribution fully reparameterized          parameters=parameters          graph parents= self  concentration                         self  total concentration           name=name      property   def concentration self       string     return self  concentration     property   def total concentration self       string     return self  total concentration    def  batch shape tensor self       return array ops shape self total concentration     def  batch shape self       return self total concentration get shape      def  event shape tensor self       return array ops shape self concentration  -1      def  event shape self       return self concentration get shape   with rank at least 1  -1      def  sample n self  n  seed=none       gamma sample = random ops random gamma          shape= n           alpha=self concentration          dtype=self dtype          seed=seed      return gamma sample / math ops reduce sum gamma sample  -1  keepdims=true      distribution util appenddocstring  dirichlet sample note    def  log prob self  x       return self  log unnormalized prob x  - self  log normalization       distribution util appenddocstring  dirichlet sample note    def  prob self  x       return math ops exp self  log prob x      def  log unnormalized prob self  x       x = self  maybe assert valid sample x      return math ops reduce sum math ops xlogy self concentration - 1   x   -1     def  log normalization self       return special math ops lbeta self concentration     def  entropy self       k = math ops cast self event shape tensor   0   self dtype      return           self  log normalization               self total concentration - k               math ops digamma self total concentration           - math ops reduce sum               self concentration - 1     math ops digamma self concentration               axis=-1      def  mean self       return self concentration / self total concentration      array ops newaxis     def  covariance self       x = self  variance scale term     self  mean       return array ops matrix set diag          -math ops matmul x      array ops newaxis                            x      array ops newaxis                 self  variance       def  variance self       scale = self  variance scale term       x = scale   self  mean       return x    scale - x     def  variance scale term self       string     return math ops rsqrt 1    self total concentration      array ops newaxis       distribution util appenddocstring        string    def  mode self       k = math ops cast self event shape tensor   0   self dtype      mode =  self concentration - 1   /           self total concentration      array ops newaxis  - k      if self allow nan stats        nan = array ops fill            array ops shape mode             np array np nan  dtype=self dtype as numpy dtype               name=string        return array ops where v2            math ops reduce all self concentration > 1   axis=-1   mode  nan      return control flow ops with dependencies           check ops assert less              array ops ones     self dtype               self concentration              message=string          mode     def  maybe assert valid concentration self  concentration  validate args       string     if not validate args        return concentration     return control flow ops with dependencies           check ops assert positive              concentration              message=string           check ops assert rank at least              concentration  1              message=string           check ops assert less              1  array ops shape concentration  -1               message=string          concentration     def  maybe assert valid sample self  x       string     if not self validate args        return x     return control flow ops with dependencies           check ops assert positive x  message=string           check ops assert near              array ops ones     dtype=self dtype               math ops reduce sum x  -1               message=string          x  
class dirichletmultinomial distribution distribution     string           deprecation deprecate        string        string       string       string       string       string        warn once=true    def   init   self                 total count                 concentration                 validate args=false                 allow nan stats=true                 name=string       string     parameters = dict locals        with ops name scope name  values= total count  concentration   as name                                                                self  total count = ops convert to tensor total count  name=string        if validate args          self  total count =               distribution util embed check nonnegative integer form                  self  total count         self  concentration = self  maybe assert valid concentration            ops convert to tensor concentration                                  name=string             validate args        self  total concentration = math ops reduce sum self  concentration  -1      super dirichletmultinomial  self    init            dtype=self  concentration dtype          validate args=validate args          allow nan stats=allow nan stats          reparameterization type=distribution not reparameterized          parameters=parameters          graph parents= self  total count                         self  concentration           name=name      property   def total count self       string     return self  total count     property   def concentration self       string     return self  concentration     property   def total concentration self       string     return self  total concentration    def  batch shape tensor self       return array ops shape self total concentration     def  batch shape self       return self total concentration get shape      def  event shape tensor self       return array ops shape self concentration  -1      def  event shape self            return self concentration get shape   with rank at least 1  -1      def  sample n self  n  seed=none       n draw = math ops cast self total count  dtype=dtypes int32      k = self event shape tensor   0      unnormalized logits = array ops reshape          math ops log random ops random gamma              shape= n               alpha=self concentration              dtype=self dtype              seed=seed            shape= -1  k       draw = random ops multinomial          logits=unnormalized logits          num samples=n draw          seed=distribution util gen new seed seed  salt=string       x = math ops reduce sum array ops one hot draw  depth=k   -2      final shape = array ops concat   n   self batch shape tensor     k    0      x = array ops reshape x  final shape      return math ops cast x  self dtype      distribution util appenddocstring  dirichlet multinomial sample note    def  log prob self  count       count = self  maybe assert valid sample count      order prob =           special math ops lbeta self concentration   count          - special math ops lbeta self concentration       return order prob   distribution util log combinations          self total count  count      distribution util appenddocstring  dirichlet multinomial sample note    def  prob self  count       return math ops exp self  log prob count      def  mean self       return self total count    self concentration /                                self total concentration      array ops newaxis       distribution util appenddocstring        string    def  covariance self       x = self  variance scale term     self  mean       return array ops matrix set diag          -math ops matmul x      array ops newaxis                            x      array ops newaxis                 self  variance       def  variance self       scale = self  variance scale term       x = scale   self  mean       return x    self total count   scale - x     def  variance scale term self       string               c0 = self total concentration      array ops newaxis      return math ops sqrt  1    c0 / self total count  /  1    c0      def  maybe assert valid concentration self  concentration  validate args       string     if not validate args        return concentration     concentration = distribution util embed check categorical event shape          concentration      return control flow ops with dependencies           check ops assert positive              concentration              message=string          concentration     def  maybe assert valid sample self  count       string     if not self validate args        return count     count = distribution util embed check nonnegative integer form count      return control flow ops with dependencies           check ops assert equal              self total count  math ops reduce sum count  -1               message=string          count  
class distribution  basedistribution     string     deprecation deprecate        string        string       string       string       string       string        warn once=true    def   init   self                 dtype                 reparameterization type                 validate args                 allow nan stats                 parameters=none                 graph parents=none                 name=none       string     graph parent =    if graph parent be none else graph parent     for i  t in enumerate graph parent         if t be none or not tensor util be tensor t           raise valueerror string    i  t       if not name or name -1   = string          non unique name = name or type self    name         with ops name scope non unique name  as name          pass     self  dtype = dtype     self  reparameterization type = reparameterization type     self  allow nan stats = allow nan stats     self  validate args = validate args     self  parameters = parameters or        self  graph parent = graph parent     self  name = name     property   def  parameters self       return self  parameter dict      parameters setter   def  parameters self  value       string     if string in value        del value string      self  parameter dict = value     classmethod   def param shape cls  sample shape  name=string       string     with ops name scope name  values= sample shape          return cls  param shape sample shape      classmethod   def param static shape cls  sample shape       string     if isinstance sample shape  tensor shape tensorshape         if not sample shape be fully define            raise valueerror string        sample shape = sample shape as list        params = cls param shape sample shape       static params =        for name  shape in params items          static shape = tensor util constant value shape        if static shape be none          raise valueerror              string        static params name  = tensor shape tensorshape static shape       return static params     staticmethod   def  param shape sample shape       raise notimplementederror string      property   def name self       string     return self  name     property   def dtype self       string     return self  dtype     property   def parameters self       string                    return  k  v for k  v in self  parameters items               if not k startswith string  and k  = string      property   def reparameterization type self       string     return self  reparameterization type     property   def allow nan stats self       string     return self  allow nan stats     property   def validate args self       string     return self  validate args    def copy self    override parameters kwargs       string     parameters = dict self parameters    override parameters kwargs      return type self    parameters     def  batch shape tensor self       raise notimplementederror          string format type self    name        def batch shape tensor self  name=string       string     with self  name scope name         if self batch shape be fully define            return ops convert to tensor self batch shape as list                                         dtype=dtypes int32                                       name=string        return self  batch shape tensor      def  batch shape self       return tensor shape tensorshape none      property   def batch shape self       string     return tensor shape as shape self  batch shape       def  event shape tensor self       raise notimplementederror          string format type self    name        def event shape tensor self  name=string       string     with self  name scope name         if self event shape be fully define            return ops convert to tensor self event shape as list                                         dtype=dtypes int32                                       name=string        return self  event shape tensor      def  event shape self       return tensor shape tensorshape none      property   def event shape self       string     return tensor shape as shape self  event shape       def be scalar event self  name=string       string     with self  name scope name         return ops convert to tensor            self  be scalar helper self event shape  self event shape tensor             name=string     def be scalar batch self  name=string       string     with self  name scope name         return ops convert to tensor            self  be scalar helper self batch shape  self batch shape tensor             name=string     def  sample n self  n  seed=none       raise notimplementederror string format          type self    name        def  call sample n self  sample shape  seed  name    kwargs       with self  name scope name  values= sample shape          sample shape = ops convert to tensor            sample shape  dtype=dtypes int32  name=string        sample shape  n = self  expand sample shape to vector            sample shape  string        sample = self  sample n n  seed    kwargs        batch event shape = array ops shape sample  1         final shape = array ops concat  sample shape  batch event shape   0        sample = array ops reshape sample  final shape        sample = self  set sample static shape sample  sample shape        return sample    def sample self  sample shape=    seed=none  name=string       string     return self  call sample n sample shape  seed  name     def  log prob self  value       raise notimplementederror string format          type self    name        def  call log prob self  value  name    kwargs       with self  name scope name  values= value          value =  convert to tensor            value  name=string  prefer dtype=self dtype        try          return self  log prob value    kwargs        except notimplementederror as original exception          try            return math ops log self  prob value    kwargs           except notimplementederror            raise original exception    def log prob self  value  name=string       string     return self  call log prob value  name     def  prob self  value       raise notimplementederror string format          type self    name        def  call prob self  value  name    kwargs       with self  name scope name  values= value          value =  convert to tensor            value  name=string  prefer dtype=self dtype        try          return self  prob value    kwargs        except notimplementederror as original exception          try            return math ops exp self  log prob value    kwargs           except notimplementederror            raise original exception    def prob self  value  name=string       string     return self  call prob value  name     def  log cdf self  value       raise notimplementederror string format          type self    name        def  call log cdf self  value  name    kwargs       with self  name scope name  values= value          value =  convert to tensor            value  name=string  prefer dtype=self dtype        try          return self  log cdf value    kwargs        except notimplementederror as original exception          try            return math ops log self  cdf value    kwargs           except notimplementederror            raise original exception    def log cdf self  value  name=string       string     return self  call log cdf value  name     def  cdf self  value       raise notimplementederror string format          type self    name        def  call cdf self  value  name    kwargs       with self  name scope name  values= value          value =  convert to tensor            value  name=string  prefer dtype=self dtype        try          return self  cdf value    kwargs        except notimplementederror as original exception          try            return math ops exp self  log cdf value    kwargs           except notimplementederror            raise original exception    def cdf self  value  name=string       string     return self  call cdf value  name     def  log survival function self  value       raise notimplementederror          string format              type self    name        def  call log survival function self  value  name    kwargs       with self  name scope name  values= value          value =  convert to tensor            value  name=string  prefer dtype=self dtype        try          return self  log survival function value    kwargs        except notimplementederror as original exception          try            return math ops log1p -self cdf value    kwargs           except notimplementederror            raise original exception    def log survival function self  value  name=string       string     return self  call log survival function value  name     def  survival function self  value       raise notimplementederror string format          type self    name        def  call survival function self  value  name    kwargs       with self  name scope name  values= value          value =  convert to tensor            value  name=string  prefer dtype=self dtype        try          return self  survival function value    kwargs        except notimplementederror as original exception          try            return 1  - self cdf value    kwargs          except notimplementederror            raise original exception    def survival function self  value  name=string       string     return self  call survival function value  name     def  entropy self       raise notimplementederror string format          type self    name        def entropy self  name=string       string     with self  name scope name         return self  entropy      def  mean self       raise notimplementederror string format          type self    name        def mean self  name=string       string     with self  name scope name         return self  mean      def  quantile self  value       raise notimplementederror string format          type self    name        def  call quantile self  value  name    kwargs       with self  name scope name  values= value          value =  convert to tensor            value  name=string  prefer dtype=self dtype        return self  quantile value    kwargs     def quantile self  value  name=string       string     return self  call quantile value  name     def  variance self       raise notimplementederror string format          type self    name        def variance self  name=string       string     with self  name scope name         try          return self  variance         except notimplementederror as original exception          try            return math ops square self  stddev            except notimplementederror            raise original exception    def  stddev self       raise notimplementederror string format          type self    name        def stddev self  name=string       string      with self  name scope name         try          return self  stddev         except notimplementederror as original exception          try            return math ops sqrt self  variance            except notimplementederror            raise original exception    def  covariance self       raise notimplementederror string format          type self    name        def covariance self  name=string       string     with self  name scope name         return self  covariance      def  mode self       raise notimplementederror string format          type self    name        def mode self  name=string       string     with self  name scope name         return self  mode      def  cross entropy self  other       return kullback leibler cross entropy          self  other  allow nan stats=self allow nan stats     def cross entropy self  other  name=string       string     with self  name scope name         return self  cross entropy other     def  kl divergence self  other       return kullback leibler kl divergence          self  other  allow nan stats=self allow nan stats     def kl divergence self  other  name=string       string     with self  name scope name         return self  kl divergence other     def   str   self       return  string             string             string             string             string format                  type name=type self    name                    self name=self name                  maybe batch shape= string format self batch shape                                     if self batch shape ndims be not none                                    else string                   maybe event shape= string format self event shape                                     if self event shape ndims be not none                                    else string                   dtype=self dtype name      def   repr   self       return  string             string             string             string             string format                  type name=type self    name                    self name=self name                  batch shape=self batch shape                  event shape=self event shape                  dtype=self dtype name       contextlib contextmanager   def  name scope self  name=none  values=none       string     with ops name scope self name         with ops name scope name  values=                if value be none else value    self  graph parent   as scope          yield scope    def  expand sample shape to vector self  x  name       string     x static val = tensor util constant value x      if x static val be none        prod = math ops reduce prod x      else        prod = np prod x static val  dtype=x dtype as numpy dtype         ndims = x get shape   ndims       if ndims be none               ndims = array ops rank x        expand shape = util pick vector            math ops equal ndims  0             np array  1   dtype=np int32   array ops shape x         x = array ops reshape x  expand shape      elif ndims == 0               if x static val be not none          x = ops convert to tensor              np array  x static val   dtype=x dtype as numpy dtype                 name=name        else          x = array ops reshape x   1       elif ndims  = 1        raise valueerror string       return x  prod    def  set sample static shape self  x  sample shape       string          sample shape = tensor shape tensorshape          tensor util constant value sample shape        ndims = x get shape   ndims     sample ndims = sample shape ndims     batch ndims = self batch shape ndims     event ndims = self event shape ndims           if  ndims be none and         sample ndims be not none and         batch ndims be not none and         event ndims be not none         ndims = sample ndims   batch ndims   event ndims       x set shape  none    ndims            if ndims be not none and sample ndims be not none        shape = sample shape concatenate  none   ndims - sample ndims         x set shape x get shape   merge with shape             if ndims be not none and event ndims be not none        shape = tensor shape tensorshape             none   ndims - event ndims   concatenate self event shape        x set shape x get shape   merge with shape             if batch ndims be not none        if ndims be not none          if sample ndims be none and event ndims be not none            sample ndims = ndims - batch ndims - event ndims         elif event ndims be none and sample ndims be not none            event ndims = ndims - batch ndims - sample ndims       if sample ndims be not none and event ndims be not none          shape = tensor shape tensorshape  none  sample ndims  concatenate              self batch shape  concatenate  none  event ndims          x set shape x get shape   merge with shape        return x    def  be scalar helper self  static shape  dynamic shape fn       string     if static shape ndims be not none        return static shape ndims == 0     shape = dynamic shape fn       if  shape get shape   ndims be not none and         shape get shape   dim 0  value be not none                              return shape get shape   as list   ==  0      return math ops equal array ops shape shape  0   0  
class exponential gamma gamma     string     deprecation deprecate        string        string       string       string       string       string        warn once=true    def   init   self                 rate                 validate args=false                 allow nan stats=true                 name=string       string     parameters = dict locals                            with ops name scope name  values= rate   as name        self  rate = ops convert to tensor rate  name=string      super exponential  self    init            concentration=array ops ones     dtype=self  rate dtype           rate=self  rate          allow nan stats=allow nan stats          validate args=validate args          name=name      self  parameters = parameters     self  graph parent  =  self  rate      staticmethod   def  param shape sample shape       return  string  ops convert to tensor sample shape  dtype=dtypes int32       property   def rate self       return self  rate    def  log survival function self  value       return self  log prob value  - math ops log self  rate     def  sample n self  n  seed=none       shape = array ops concat   n   array ops shape self  rate    0                                         sample = random ops random uniform          shape          minval=np finfo self dtype as numpy dtype  tiny          maxval=1           seed=seed          dtype=self dtype      return -math ops log sample  / self  rate 
class gamma distribution distribution     string     deprecation deprecate        string        string       string       string       string       string        warn once=true    def   init   self                 concentration                 rate                 validate args=false                 allow nan stats=true                 name=string       string     parameters = dict locals        with ops name scope name  values= concentration  rate   as name        with ops control dependencies             check ops assert positive concentration             check ops assert positive rate           if validate args else              self  concentration = array ops identity              concentration  name=string          self  rate = array ops identity rate  name=string          check ops assert same float dtype               self  concentration  self  rate       super gamma  self    init            dtype=self  concentration dtype          validate args=validate args          allow nan stats=allow nan stats          reparameterization type=distribution fully reparameterized          parameters=parameters          graph parents= self  concentration                         self  rate           name=name      staticmethod   def  param shape sample shape       return dict          zip  string  string     ops convert to tensor              sample shape  dtype=dtypes int32     2        property   def concentration self       string     return self  concentration     property   def rate self       string     return self  rate    def  batch shape tensor self       return array ops broadcast dynamic shape          array ops shape self concentration           array ops shape self rate      def  batch shape self       return array ops broadcast static shape          self concentration get shape            self rate get shape       def  event shape tensor self       return constant op constant     dtype=dtypes int32     def  event shape self       return tensor shape tensorshape         distribution util appenddocstring        string    def  sample n self  n  seed=none       return random ops random gamma          shape= n           alpha=self concentration          beta=self rate          dtype=self dtype          seed=seed     def  log prob self  x       return self  log unnormalized prob x  - self  log normalization      def  cdf self  x       x = self  maybe assert valid sample x                return math ops igamma self concentration  self rate   x     def  log unnormalized prob self  x       x = self  maybe assert valid sample x      return math ops xlogy self concentration - 1   x  - self rate   x    def  log normalization self       return  math ops lgamma self concentration              - self concentration   math ops log self rate      def  entropy self       return  self concentration             - math ops log self rate                math ops lgamma self concentration                  1  - self concentration                   math ops digamma self concentration       def  mean self       return self concentration / self rate    def  variance self       return self concentration / math ops square self rate     def  stddev self       return math ops sqrt self concentration  / self rate     distribution util appenddocstring        string    def  mode self       mode =  self concentration - 1   / self rate     if self allow nan stats        nan = array ops fill            self batch shape tensor              np array np nan  dtype=self dtype as numpy dtype               name=string        return array ops where v2 self concentration > 1   mode  nan      else        return control flow ops with dependencies             check ops assert less                array ops ones     self dtype                 self concentration                message=string                mode     def  maybe assert valid sample self  x       check ops assert same float dtype tensors= x   dtype=self dtype      if not self validate args        return x     return control flow ops with dependencies           check ops assert positive x          x  
class laplace distribution distribution     string     deprecation deprecate        string        string       string       string       string       string        warn once=true    def   init   self                 loc                 scale                 validate args=false                 allow nan stats=true                 name=string       string     parameters = dict locals        with ops name scope name  values= loc  scale   as name        with ops control dependencies  check ops assert positive scale   if                                     validate args else              self  loc = array ops identity loc  name=string          self  scale = array ops identity scale  name=string          check ops assert same float dtype  self  loc  self  scale         super laplace  self    init              dtype=self  loc dtype            reparameterization type=distribution fully reparameterized            validate args=validate args            allow nan stats=allow nan stats            parameters=parameters            graph parents= self  loc  self  scale             name=name      staticmethod   def  param shape sample shape       return dict          zip  string  string     ops convert to tensor              sample shape  dtype=dtypes int32     2        property   def loc self       string     return self  loc     property   def scale self       string     return self  scale    def  batch shape tensor self       return array ops broadcast dynamic shape          array ops shape self loc   array ops shape self scale      def  batch shape self       return array ops broadcast static shape          self loc get shape    self scale get shape       def  event shape tensor self       return constant op constant     dtype=dtypes int32     def  event shape self       return tensor shape tensorshape        def  sample n self  n  seed=none       shape = array ops concat   n   self batch shape tensor     0                                    uniform sample = random ops random uniform          shape=shape          minval=np nextafter self dtype as numpy dtype -1                                self dtype as numpy dtype 0             maxval=1           dtype=self dtype          seed=seed      return  self loc - self scale   math ops sign uniform sample                math ops log1p -math ops abs uniform sample       def  log prob self  x       return self  log unnormalized prob x  - self  log normalization      def  prob self  x       return math ops exp self  log prob x      def  log cdf self  x       return special math log cdf laplace self  z x      def  log survival function self  x       return special math log cdf laplace -self  z x      def  cdf self  x       z = self  z x      return  0 5   0 5   math ops sign z                 1  - math ops exp -math ops abs z        def  log unnormalized prob self  x       return -math ops abs self  z x      def  log normalization self       return math log 2     math ops log self scale     def  entropy self            scale = self scale   array ops zero like self loc      return math log 2     1    math ops log scale     def  mean self       return self loc   array ops zero like self scale     def  stddev self       return math sqrt 2     self scale   array ops zero like self loc     def  median self       return self  mean      def  mode self       return self  mean      def  z self  x       return  x - self loc  / self scale 
class multinomial distribution distribution     string     deprecation deprecate        string        string       string       string       string       string        warn once=true    def   init   self                 total count                 logits=none                 probs=none                 validate args=false                 allow nan stats=true                 name=string       string     parameters = dict locals        with ops name scope name  values= total count  logits  probs   as name        self  total count = ops convert to tensor total count  name=string        if validate args          self  total count =               distribution util embed check nonnegative integer form                  self  total count         self  logits  self  probs = distribution util get logits and probs            logits=logits            probs=probs            multidimensional=true            validate args=validate args            name=name        self  mean val = self  total count      array ops newaxis    self  probs     super multinomial  self    init            dtype=self  probs dtype          reparameterization type=distribution not reparameterized          validate args=validate args          allow nan stats=allow nan stats          parameters=parameters          graph parents= self  total count                         self  logits                         self  probs           name=name      property   def total count self       string     return self  total count     property   def logits self       string     return self  logits     property   def probs self       string     return self  probs    def  batch shape tensor self       return array ops shape self  mean val   -1     def  batch shape self       return self  mean val get shape   with rank at least 1   -1     def  event shape tensor self       return array ops shape self  mean val  -1      def  event shape self       return self  mean val get shape   with rank at least 1  -1      def  sample n self  n  seed=none       n draw = math ops cast self total count  dtype=dtypes int32      k = self event shape tensor   0            n draw = array ops ones like          self logits      0   dtype=n draw dtype    n draw     logits = array ops ones like          n draw      array ops newaxis   dtype=self logits dtype    self logits           flat logits = array ops reshape logits   -1  k         flat ndraws = n   array ops reshape n draw   -1               def  sample single args         logits  n draw = args 0   args 1          x = random ops multinomial logits array ops newaxis        n draw                                   seed          x = array ops reshape x  shape= n  -1           x = math ops reduce sum array ops one hot x  depth=k   axis=-2          return x      x = map fn map fn           sample single   flat logits  flat ndraws           dtype=self dtype              x = array ops transpose x  perm= 1  0  2       final shape = array ops concat   n   self batch shape tensor     k    0      x = array ops reshape x  final shape        return x     distribution util appenddocstring  multinomial sample note    def  log prob self  count       return self  log unnormalized prob count  - self  log normalization count     def  log unnormalized prob self  count       count = self  maybe assert valid sample count      return math ops reduce sum count   nn ops log softmax self logits   -1     def  log normalization self  count       count = self  maybe assert valid sample count      return -distribution util log combinations self total count  count     def  mean self       return array ops identity self  mean val     def  covariance self       p = self probs   array ops ones like          self total count       array ops newaxis      return array ops matrix set diag          -math ops matmul self  mean val      array ops newaxis                            p      array ops newaxis                 self  variance       def  variance self       p = self probs   array ops ones like          self total count       array ops newaxis      return self  mean val - self  mean val   p    def  maybe assert valid sample self  count       string     if not self validate args        return count     count = distribution util embed check nonnegative integer form count      return control flow ops with dependencies           check ops assert equal              self total count  math ops reduce sum count  -1               message=string          count  
class normal distribution distribution     string     deprecation deprecate        string        string       string       string       string       string        warn once=true    def   init   self                 loc                 scale                 validate args=false                 allow nan stats=true                 name=string       string     parameters = dict locals        with ops name scope name  values= loc  scale   as name        with ops control dependencies  check ops assert positive scale   if                                     validate args else              self  loc = array ops identity loc  name=string          self  scale = array ops identity scale  name=string          check ops assert same float dtype  self  loc  self  scale       super normal  self    init            dtype=self  scale dtype          reparameterization type=distribution fully reparameterized          validate args=validate args          allow nan stats=allow nan stats          parameters=parameters          graph parents= self  loc  self  scale           name=name      staticmethod   def  param shape sample shape       return dict          zip  string  string     ops convert to tensor              sample shape  dtype=dtypes int32     2        property   def loc self       string     return self  loc     property   def scale self       string     return self  scale    def  batch shape tensor self       return array ops broadcast dynamic shape          array ops shape self loc           array ops shape self scale      def  batch shape self       return array ops broadcast static shape          self loc get shape            self scale get shape       def  event shape tensor self       return constant op constant     dtype=dtypes int32     def  event shape self       return tensor shape tensorshape        def  sample n self  n  seed=none       shape = array ops concat   n   self batch shape tensor     0      sample = random ops random normal          shape=shape  mean=0   stddev=1   dtype=self loc dtype  seed=seed      return sample   self scale   self loc    def  log prob self  x       return self  log unnormalized prob x  - self  log normalization      def  log cdf self  x       return special math log ndtr self  z x      def  cdf self  x       return special math ndtr self  z x      def  log survival function self  x       return special math log ndtr -self  z x      def  survival function self  x       return special math ndtr -self  z x      def  log unnormalized prob self  x       return -0 5   math ops square self  z x      def  log normalization self       return 0 5   math log 2    math pi    math ops log self scale     def  entropy self            scale = self scale   array ops ones like self loc      return 0 5   math log 2    math pi   math e    math ops log scale     def  mean self       return self loc   array ops ones like self scale     def  quantile self  p       return self  inv z special math ndtri p      def  stddev self       return self scale   array ops ones like self loc     def  mode self       return self  mean      def  z self  x       string     with ops name scope string  values= x          return  x - self loc  / self scale    def  inv z self  z       string     with ops name scope string  values= z          return z   self scale   self loc 
class registerkl object     string     deprecation deprecate        string        string       string       string       string       string        warn once=true    def   init   self  dist cls a  dist cls b       string     self  key =  dist cls a  dist cls b     def   call   self  kl fn       string     if not callable kl fn         raise typeerror string   kl fn      if self  key in  divergences        raise valueerror string                           self  key 0    name    self  key 1    name                               divergences self  key         divergences self  key  = kl fn     return kl fn 
class reparameterizationtype object     string     deprecation deprecate        string        string       string       string       string       string        warn once=true    def   init   self  rep type       self  rep type = rep type    def   repr   self       return string   self  rep type    def   eq   self  other       string     return self be other 
class studentt distribution distribution     string     deprecation deprecate        string        string       string       string       string       string        warn once=true    def   init   self                 df                 loc                 scale                 validate args=false                 allow nan stats=true                 name=string       string     parameters = dict locals        with ops name scope name  values= df  loc  scale   as name        with ops control dependencies  check ops assert positive df                                       if validate args else              self  df = array ops identity df  name=string          self  loc = array ops identity loc  name=string          self  scale = array ops identity scale  name=string          check ops assert same float dtype               self  df  self  loc  self  scale       super studentt  self    init            dtype=self  scale dtype          reparameterization type=distribution fully reparameterized          validate args=validate args          allow nan stats=allow nan stats          parameters=parameters          graph parents= self  df  self  loc  self  scale           name=name      staticmethod   def  param shape sample shape       return dict          zip  string  string  string                  ops convert to tensor                  sample shape  dtype=dtypes int32     3        property   def df self       string     return self  df     property   def loc self       string     return self  loc     property   def scale self       string     return self  scale    def  batch shape tensor self       return array ops broadcast dynamic shape          array ops shape self df           array ops broadcast dynamic shape              array ops shape self loc   array ops shape self scale       def  batch shape self       return array ops broadcast static shape          array ops broadcast static shape self df get shape                                             self loc get shape             self scale get shape       def  event shape tensor self       return constant op constant     dtype=math ops int32     def  event shape self       return tensor shape tensorshape        def  sample n self  n  seed=none                                     shape = array ops concat   n   self batch shape tensor     0      normal sample = random ops random normal shape  dtype=self dtype  seed=seed      df = self df   array ops ones self batch shape tensor    dtype=self dtype      gamma sample = random ops random gamma           n           0 5   df          beta=0 5          dtype=self dtype          seed=distribution util gen new seed seed  salt=string       sample = normal sample   math ops rsqrt gamma sample / df      return sample   self scale   self loc      def  log prob self  x       return self  log unnormalized prob x  - self  log normalization      def  log unnormalized prob self  x       y =  x - self loc  / self scale       return -0 5    self df   1     math ops log1p y  2  / self df     def  log normalization self       return  math ops log math ops abs self scale                 0 5   math ops log self df                0 5   np log np pi                math ops lgamma 0 5   self df  -             math ops lgamma 0 5    self df   1        def  cdf self  x            y =  x - self loc  / math ops abs self scale      x t = self df /  y  2    self df      neg cdf = 0 5   math ops betainc 0 5   self df  0 5  x t      return array ops where v2 math ops less y  0    neg cdf  1  - neg cdf     def  entropy self       v = array ops ones self batch shape tensor                           dtype=self dtype       array ops newaxis      u = v   self df      array ops newaxis      beta arg = array ops concat  u  v   -1  / 2      return  math ops log math ops abs self scale                 0 5   math ops log self df                special math ops lbeta beta arg                0 5    self df   1                  math ops digamma 0 5    self df   1    -              math ops digamma 0 5   self df        distribution util appenddocstring        string    def  mean self       mean = self loc   array ops ones self batch shape tensor                                         dtype=self dtype      if self allow nan stats        nan = np array np nan  dtype=self dtype as numpy dtype          return array ops where v2            math ops greater                self df                array ops ones self batch shape tensor    dtype=self dtype              mean  array ops fill self batch shape tensor    nan  name=string       else        return control flow ops with dependencies                            check ops assert less                    array ops ones     dtype=self dtype                     self df                    message=string                          mean      distribution util appenddocstring string    def  variance self                 denom = array ops where v2          math ops greater self df  2    self df - 2           array ops ones like self df            var =  array ops ones self batch shape tensor    dtype=self dtype               math ops square self scale    self df / denom           inf = np array np inf  dtype=self dtype as numpy dtype        result where define = array ops where v2          self df > array ops fill self batch shape tensor    2    var          array ops fill self batch shape tensor    inf  name=string        if self allow nan stats        nan = np array np nan  dtype=self dtype as numpy dtype          return array ops where v2            math ops greater                self df                array ops ones self batch shape tensor    dtype=self dtype              result where define            array ops fill self batch shape tensor    nan  name=string       else        return control flow ops with dependencies                            check ops assert less                    array ops ones     dtype=self dtype                     self df                    message=string                          result where define     def  mode self       return array ops identity self loc  
class uniform distribution distribution     string     deprecation deprecate        string        string       string       string       string       string        warn once=true    def   init   self                 low=0                  high=1                  validate args=false                 allow nan stats=true                 name=string       string     parameters = dict locals        with ops name scope name  values= low  high   as name        with ops control dependencies             check ops assert less                low  high  message=string          if validate args else              self  low = array ops identity low  name=string          self  high = array ops identity high  name=string          check ops assert same float dtype  self  low  self  high       super uniform  self    init            dtype=self  low dtype          reparameterization type=distribution fully reparameterized          validate args=validate args          allow nan stats=allow nan stats          parameters=parameters          graph parents= self  low                         self  high           name=name      staticmethod   def  param shape sample shape       return dict          zip  string  string                 ops convert to tensor sample shape  dtype=dtypes int32     2        property   def low self       string     return self  low     property   def high self       string     return self  high    def range self  name=string       string     with self  name scope name         return self high - self low    def  batch shape tensor self       return array ops broadcast dynamic shape          array ops shape self low           array ops shape self high      def  batch shape self       return array ops broadcast static shape          self low get shape            self high get shape       def  event shape tensor self       return constant op constant     dtype=dtypes int32     def  event shape self       return tensor shape tensorshape        def  sample n self  n  seed=none       shape = array ops concat   n   self batch shape tensor     0      sample = random ops random uniform shape=shape                                          dtype=self dtype                                          seed=seed      return self low   self range     sample    def  prob self  x       broadcast x = x   array ops ones          self batch shape tensor    dtype=x dtype      return array ops where v2          math ops be nan broadcast x   broadcast x          array ops where v2              math ops logical or broadcast x < self low                                  broadcast x >= self high               array ops zero like broadcast x               array ops ones like broadcast x  / self range        def  cdf self  x       broadcast shape = array ops broadcast dynamic shape          array ops shape x   self batch shape tensor        zero = array ops zero broadcast shape  dtype=self dtype      ones = array ops ones broadcast shape  dtype=self dtype      broadcast x = x   ones     result if not big = array ops where v2          x < self low  zero   broadcast x - self low  / self range        return array ops where v2 x >= self high  ones  result if not big     def  entropy self       return math ops log self range       def  mean self       return  self low   self high  / 2     def  variance self       return math ops square self range    / 12     def  stddev self       return self range   / math sqrt 12   
 deprecation deprecate      string      string     string     string     string     string      warn once=true   tf export v1= string   def kl divergence distribution a  distribution b                    allow nan stats=true  name=none     string   kl fn =  register kl type distribution a   type distribution b     if kl fn be none      raise notimplementederror          string         string            type distribution a    name    type distribution b    name        with ops name scope string       kl t = kl fn distribution a  distribution b  name=name      if allow nan stats        return kl t           kl t = array ops identity kl t  name=string       with ops control dependencies           control flow ops assert              math ops logical not                  math ops reduce any math ops be nan kl t                  string              string                 distribution a name  distribution b name   kl t            return array ops identity kl t  name=string  
class raise exception on not ok status object     string    def   enter   self       self status = c api util scopedtfstatus       return self status status    def   exit   self  type arg  value arg  traceback arg       try        if c api tf getcode self status status   = 0          raise  make specific exception              none  none              compat as text c api tf message self status status                c api tf getcode self status status                      finally        del self status     return false   
class baselineclassifier estimator estimator       doc   = baselineclassifierv2   doc   replace string  string     def   init   self                 model dir=none                 n classes=2                 weight column=none                 label vocabulary=none                 optimizer=string                 config=none                 loss reduction=losses reduction sum       head = head lib  binary logistic or multi class head            n class  weight column  label vocabulary  loss reduction       def  model fn feature  label  mode  config         return  baseline model fn            features=features            labels=labels            mode=mode            head=head            optimizer=optimizer            weight column=weight column            config=config       super baselineclassifier  self    init            model fn= model fn          model dir=model dir          config=config  
class baselineestimator estimator estimator       doc   = baselineestimatorv2   doc      def   init   self                 head                 model dir=none                 optimizer=string                 config=none       def  model fn feature  label  mode  config         return  baseline model fn            features=features            labels=labels            mode=mode            head=head            optimizer=optimizer            config=config      super baselineestimator  self    init            model fn= model fn          model dir=model dir          config=config  
class baselineregressor estimator estimator       doc   = baselineregressorv2   doc   replace string  string     def   init   self                 model dir=none                 label dimension=1                 weight column=none                 optimizer=string                 config=none                 loss reduction=losses reduction sum       head = head lib  regression head            label dimension=label dimension          weight column=weight column          loss reduction=loss reduction       def  model fn feature  label  mode  config         return  baseline model fn            features=features            labels=labels            mode=mode            head=head            optimizer=optimizer            config=config       super baselineregressor  self    init            model fn= model fn  model dir=model dir  config=config  
class dnnclassifier estimator estimator       doc   = dnnclassifierv2   doc   replace string  string     def   init          self        hide units        feature columns        model dir=none        n classes=2        weight column=none        label vocabulary=none        optimizer=string        activation fn=nn relu        dropout=none        input layer partitioner=none        config=none        warm start from=none        loss reduction=losses reduction sum        batch norm=false           head = head lib  binary logistic or multi class head            n class  weight column  label vocabulary  loss reduction      estimator  can estimator api gauge get cell string  set string       def  model fn feature  label  mode  config         string       return  dnn model fn            features=features            labels=labels            mode=mode            head=head            hide units=hidden units            feature columns=tuple feature columns or                optimizer=optimizer            activation fn=activation fn            dropout=dropout            input layer partitioner=input layer partitioner            config=config            batch norm=batch norm       super dnnclassifier  self    init            model fn= model fn          model dir=model dir          config=config          warm start from=warm start from  
class dnnestimator estimator estimator       doc   = dnnestimatorv2   doc      def   init   self                 head                 hide units                 feature columns                 model dir=none                 optimizer=string                 activation fn=nn relu                 dropout=none                 input layer partitioner=none                 config=none                 warm start from=none                 batch norm=false       def  model fn feature  label  mode  config         string       return  dnn model fn            features=features            labels=labels            mode=mode            head=head            hide units=hidden units            feature columns=tuple feature columns or                optimizer=optimizer            activation fn=activation fn            dropout=dropout            input layer partitioner=input layer partitioner            config=config            batch norm=batch norm       estimator  can estimator api gauge get cell string  set string        super dnnestimator  self    init            model fn= model fn  model dir=model dir  config=config          warm start from=warm start from  
class dnnlinearcombinedclassifier estimator estimator       doc   = dnnlinearcombinedclassifierv2   doc   replace        string  string     def   init   self                 model dir=none                 linear feature columns=none                 linear optimizer=string                 dnn feature columns=none                 dnn optimizer=string                 dnn hide units=none                 dnn activation fn=nn relu                 dnn dropout=none                 n classes=2                 weight column=none                 label vocabulary=none                 input layer partitioner=none                 config=none                 warm start from=none                 loss reduction=losses reduction sum                 batch norm=false                 linear sparse combiner=string       self  feature columns =  validate feature columns          linear feature columns=linear feature columns          dnn feature columns=dnn feature columns       head = head lib  binary logistic or multi class head            n class  weight column  label vocabulary  loss reduction      estimator  can estimator api gauge get cell string  set          string         def  model fn feature  label  mode  config         string       return  dnn linear combine model fn            features=features            labels=labels            mode=mode            head=head            linear feature columns=linear feature columns            linear optimizer=linear optimizer            dnn feature columns=dnn feature columns            dnn optimizer=dnn optimizer            dnn hide units=dnn hide units            dnn activation fn=dnn activation fn            dnn dropout=dnn dropout            input layer partitioner=input layer partitioner            config=config            batch norm=batch norm            linear sparse combiner=linear sparse combiner       super dnnlinearcombinedclassifier  self    init            model fn= model fn          model dir=model dir          config=config          warm start from=warm start from  
class dnnlinearcombinedestimator estimator estimator       doc   = dnnlinearcombinedestimatorv2   doc      def   init   self                 head                 model dir=none                 linear feature columns=none                 linear optimizer=string                 dnn feature columns=none                 dnn optimizer=string                 dnn hide units=none                 dnn activation fn=nn relu                 dnn dropout=none                 input layer partitioner=none                 config=none                 linear sparse combiner=string       self  feature columns =  validate feature columns          linear feature columns=linear feature columns          dnn feature columns=dnn feature columns      estimator  can estimator api gauge get cell string  set          string         def  model fn feature  label  mode  config         string       return  dnn linear combine model fn            features=features            labels=labels            mode=mode            head=head            linear feature columns=linear feature columns            linear optimizer=linear optimizer            dnn feature columns=dnn feature columns            dnn optimizer=dnn optimizer            dnn hide units=dnn hide units            dnn activation fn=dnn activation fn            dnn dropout=dnn dropout            input layer partitioner=input layer partitioner            config=config            linear sparse combiner=linear sparse combiner       super dnnlinearcombinedestimator  self    init            model fn= model fn          model dir=model dir          config=config  
class dnnlinearcombinedregressor estimator estimator       doc   = dnnlinearcombinedregressorv2   doc   replace        string  string     def   init   self                 model dir=none                 linear feature columns=none                 linear optimizer=string                 dnn feature columns=none                 dnn optimizer=string                 dnn hide units=none                 dnn activation fn=nn relu                 dnn dropout=none                 label dimension=1                 weight column=none                 input layer partitioner=none                 config=none                 warm start from=none                 loss reduction=losses reduction sum                 batch norm=false                 linear sparse combiner=string       self  feature columns =  validate feature columns          linear feature columns=linear feature columns          dnn feature columns=dnn feature columns      estimator  can estimator api gauge get cell string  set          string         head = head lib  regression head            label dimension=label dimension          weight column=weight column          loss reduction=loss reduction       def  model fn feature  label  mode  config         string       return  dnn linear combine model fn            features=features            labels=labels            mode=mode            head=head            linear feature columns=linear feature columns            linear optimizer=linear optimizer            dnn feature columns=dnn feature columns            dnn optimizer=dnn optimizer            dnn hide units=dnn hide units            dnn activation fn=dnn activation fn            dnn dropout=dnn dropout            input layer partitioner=input layer partitioner            config=config            batch norm=batch norm            linear sparse combiner=linear sparse combiner       super dnnlinearcombinedregressor  self    init            model fn= model fn          model dir=model dir          config=config          warm start from=warm start from  
class dnnregressor estimator estimator       doc   = dnnregressorv2   doc   replace string  string     def   init          self        hide units        feature columns        model dir=none        label dimension=1        weight column=none        optimizer=string        activation fn=nn relu        dropout=none        input layer partitioner=none        config=none        warm start from=none        loss reduction=losses reduction sum        batch norm=false           head = head lib  regression head            label dimension=label dimension          weight column=weight column          loss reduction=loss reduction      estimator  can estimator api gauge get cell string  set string         def  model fn feature  label  mode  config         string       return  dnn model fn            features=features            labels=labels            mode=mode            head=head            hide units=hidden units            feature columns=tuple feature columns or                optimizer=optimizer            activation fn=activation fn            dropout=dropout            input layer partitioner=input layer partitioner            config=config            batch norm=batch norm       super dnnregressor  self    init            model fn= model fn          model dir=model dir          config=config          warm start from=warm start from  
class estimator object     string    def   init   self  model fn  model dir=none  config=none  params=none                 warm start from=none       string      estimator api gauge get cell string  set true                     self   class    assert members be not override self         self  config = maybe overwrite model dir and session config config                                                                  model dir            self  train distribution = self  config train distribute     self  eval distribution = self  config eval distribute          self  model dir = self  config model dir     self  session config = self  config session config     log info string  str vars self  config         self  device fn =           self  config device fn or  get replica device setter self  config        if model fn be none        raise valueerror string      model fn lib verify model fn args model fn  params      self  model fn = model fn     self  params = copy deepcopy params or               self  warm start settings =  get default warm start settings          warm start from           property   def model dir self       return self  model dir     property   def config self       return copy deepcopy self  config      property   def params self       return copy deepcopy self  params      property   def model fn self       string      def public model fn feature  label  mode  config         return self  call model fn feature  label  mode  config       return public model fn       def get variable value self  name       string      check checkpoint available self model dir      with context graph mode          return train load variable self model dir  name     def get variable name self       string      check checkpoint available self model dir      with context graph mode          return  name for name    in train list variables self model dir      def latest checkpoint self       string     with context graph mode          return checkpoint management latest checkpoint self model dir     def train self              input fn              hooks=none              steps=none              max steps=none              save listeners=none       string      estimator api gauge get cell string  set true      if self config task type in  run config tasktype evaluator                                   run config tasktype ps         raise valueerror            string           string           string format                self config        with context graph mode          if  step be not none  and  max step be not none           raise valueerror string        if step be not none and step <= 0          raise valueerror string format step         if max step be not none and max step <= 0          raise valueerror              string format max step          if max step be not none          start step =  load global step from checkpoint dir self  model dir          if max step <= start step            log info string            return self        hook =  check hook type hook        hook extend self  convert train step to hook step  max step          save listeners =  check listeners type save listeners        loss = self  train model input fn  hook  save listeners        log info string  loss        return self    def  convert train step to hook self  step  max step       string     if step be not none or max step be not none        if self  train distribution          step per run = getattr              self  train distribution extend  string  1          if step per run > 1            return  basic session run hook  multistepstopatstephook                  step  max step  step per run         return  train stopatstephook step  max step       else        return       def eval dir self  name=none       string     return os path join self  model dir  string if not name else                         string   name     def evaluate self  input fn  steps=none  hooks=none  checkpoint path=none                 name=none       string      estimator api gauge get cell string  set true           if  self  eval distribution and         hasattr self  config  string  and         self  config  distribute coordinator mode         return distribute coordinator train estimator evaluate            self            lambda est  s  eval hook  est  actual eval                  input fn  strategy=s  steps=steps  hooks=eval hook                checkpoint path=checkpoint path  name=name             hook           else        return self  actual eval            input fn            strategy=self  eval distribution            steps=steps            hooks=hooks            checkpoint path=checkpoint path            name=name     def  actual eval self                     input fn                     strategy=none                     steps=none                     hooks=none                     checkpoint path=none                     name=none       string     with context graph mode          hook =  check hook type hook        hook extend self  convert eval step to hook step                 if not checkpoint path          latest path = checkpoint management latest checkpoint self  model dir          if not latest path            log info string                        string format self  model dir           checkpoint path = latest path        def  evaluate             scaffold  update op  eval dict  all hook  =               self  evaluate build graph input fn  hook  checkpoint path           return self  evaluate run              checkpoint path=checkpoint path              scaffold=scaffold              update op=update op              eval dict=eval dict              all hooks=all hook              output dir=self eval dir name          with ops graph   as default            if strategy                                             train get or create step per run variable             with strategy scope                return  evaluate           else            return  evaluate      def  convert eval step to hook self  step       string     if step be none        return         if step <= 0        raise valueerror string format step                       if self  eval distribution        step per run = getattr            self  eval distribution extend  string  1        if step per run > 1          return  evaluation  multistepstopafternevalshook                num evals=steps  step per run=steps per run       return  evaluation  stopafternevalshook num evals=steps        def predict self                input fn                predict keys=none                hooks=none                checkpoint path=none                yield single examples=true       string      estimator api gauge get cell string  set true      with context graph mode          hook =  check hook type hook               if not checkpoint path          checkpoint path = checkpoint management latest checkpoint              self  model dir        if not checkpoint path          log info string                      string format self  model dir         with ops graph   as default   as g          random seed set random seed self  config tf random seed          self  create and assert global step g          feature  input hook = self  get feature from input fn              input fn  modekeys predict          estimator spec = self  call model fn              feature  none  modekeys predict  self config                    self  maybe warm start checkpoint path           predictions = self  extract key              estimator spec predictions  predict key          all hook = list input hook          all hook extend hook          all hook extend list estimator spec prediction hook or              with train monitoredsession              session creator=training chiefsessioncreator                  checkpoint filename with path=checkpoint path                  master=self  config master                  scaffold=estimator spec scaffold                  config=self  session config               hooks=all hook  as mon sess            while not mon sess should stop                preds evaluate = mon sess run predictions              if not yield single examples                yield preds evaluate             elif not isinstance predictions  dict                 for pred in preds evaluate                  yield pred             else                for i in range self  extract batch length preds evaluate                    yield                       key  value i                      for key  value in six iteritems preds evaluate                       def  assert members be not override self       string      assert members be not override estimator  self     def export save model        self  export dir base  serve input receiver fn        assets extra=none        as text=false        checkpoint path=none        experimental mode=modekeys predict            string          if not serve input receiver fn        raise valueerror string       input receiver fn map =  experimental mode  serve input receiver fn       return self  export all save model          export dir base          input receiver fn map          assets extra=assets extra          as text=as text          checkpoint path=checkpoint path          strip default attrs=true     def experimental export all save model        self  export dir base  input receiver fn map        assets extra=none        as text=false        checkpoint path=none       string     return self  export all save model          export dir base  input receiver fn map          assets extra=assets extra  as text=as text          checkpoint path=checkpoint path  strip default attrs=true     def  export all save model        self  export dir base  input receiver fn map        assets extra=none  as text=false  checkpoint path=none        strip default attrs=true       string          with context graph mode          if not checkpoint path                   checkpoint path = self latest checkpoint         if not checkpoint path          if self  warm start settings            checkpoint path = self  warm start settings ckpt to initialize from           if gfile isdirectory checkpoint path               checkpoint path = checkpoint management latest checkpoint                  checkpoint path          else            raise valueerror string format                self  model dir          export dir = export lib get timestamped export dir export dir base        temp export dir = export lib get temp export dir export dir         builder = save model builder savedmodelbuilder temp export dir         save variables = true                                          if input receiver fn map get modekeys train           self  add meta graph for mode              builder  input receiver fn map  checkpoint path              save variables  mode=modekeys train              strip default attrs=strip default attrs          save variables = false       if input receiver fn map get modekeys eval           self  add meta graph for mode              builder  input receiver fn map  checkpoint path              save variables  mode=modekeys eval              strip default attrs=strip default attrs          save variables = false       if input receiver fn map get modekeys predict           self  add meta graph for mode              builder  input receiver fn map  checkpoint path              save variables  mode=modekeys predict              strip default attrs=strip default attrs          save variables = false        if save variables          raise valueerror string format              input receiver fn map key            builder save as text                if assets extra          assets extra path = os path join compat as bytes temp export dir                                            compat as bytes string           for dest relative  source in assets extra items              dest absolute = os path join compat as bytes assets extra path                                          compat as bytes dest relative             dest path = os path dirname dest absolute            gfile makedirs dest path            gfile copy source  dest absolute         gfile rename temp export dir  export dir        return export dir    def  add meta graph for mode self                                 builder                                 input receiver fn map                                 checkpoint path                                 save variables=true                                 mode=modekeys predict                                 export tags=none                                 check variables=true                                 strip default attrs=true       string     if export tag be none        export tag = export lib export tag map mode      input receiver fn = input receiver fn map mode       with ops graph   as default   as g        self  create and assert global step g        random seed set random seed self  config tf random seed         input receiver = input receiver fn                 estimator spec = self  call model fn            features=input receiver feature            labels=getattr input receiver  string  none             mode=mode            config=self config         export output = export lib export output for mode            mode=estimator spec mode            serve export outputs=estimator spec export output            predictions=estimator spec predictions            loss=estimator spec loss            metrics=estimator spec eval metric ops                signature def map = export lib build all signature defs            input receiver receiver tensors            export output            getattr input receiver  string  none             serve only= mode == modekeys predict          with tf session session config=self  session config  as session           if estimator spec scaffold local init op be not none            local init op = estimator spec scaffold local init op         else            local init op = monitor session scaffold default local init op                                                graph saver = estimator spec scaffold saver or saver saver sharded=true           if save variables and not check variables            raise valueerror string                            string          if check variables            try              graph saver restore session  checkpoint path            except errors notfounderror as e              msg =  string                    string                    string                    string                    string  format                         mode  checkpoint path  e              raise valueerror msg                                      builder  add train op estimator spec train op             meta graph kwargs = dict              tags=export tag              signature def map=signature def map              assets collection=ops get collection ops graphkeys asset filepaths               main op=local init op              saver=graph saver              strip default attrs=strip default attrs           if save variables            builder add meta graph and variables session    meta graph kwargs          else            builder add meta graph   meta graph kwargs     def  get feature from input fn self  input fn  mode       string     result = self  call input fn input fn  mode      result     hook = estimator util parse input fn result result      self  validate feature in predict input result      return result  hook    def  validate feature in predict input self  result       if not  have dataset or queue runner result         log warn string                       string                       string     def  get iterator from input fn self  input fn  mode  distribution=none       string     if distribution be not none               iterator = distribution make input fn iterator            lambda input context  self  call input fn input fn  mode                                                      input context         input hook =             estimator util distributediteratorinitializerhook iterator       else        result = self  call input fn input fn  mode        iterator = result make initializable iterator         input hook =  estimator util  datasetinitializerhook iterator         return iterator  input hook    def  get feature and label from input fn self  input fn  mode       string     return estimator util parse input fn result          self  call input fn input fn  mode      def  extract batch length self  preds evaluate       string     batch length = none     for key  value in six iteritems preds evaluate         batch length = batch length or value shape 0        if value shape 0   = batch length          raise valueerror string                          string   key      return batch length    def  extract key self  predictions  predict key       string     if not predict key        return predictions     if not isinstance predictions  dict         raise valueerror            string      exist key = predictions key       predictions =           key  value         for key  value in six iteritems predictions  if key in predict key           if not predictions        raise valueerror string                        string    exist key  predict key       return predictions    def  create global step self  graph       string     return train create global step graph     def  create and assert global step self  graph       string     step = self  create global step graph      assert step be train get global step       assert step dtype be integer     return step    def  call input fn self  input fn  mode  input context=none       string     input fn args = function utils fn args input fn      kwargs =        if string in input fn args        kwargs string  = mode     if string in input fn args        kwargs string  = self params     if string in input fn args        kwargs string  = self config     if input context and string in input fn args        log info string                    string        kwargs string  = input context     with ops device string         return input fn   kwargs     def  call model fn self  feature  label  mode  config       string     model fn args = function utils fn args self  model fn      kwargs =        if string in model fn args        kwargs string  = label     else        if label be not none          raise valueerror              string      if string in model fn args        kwargs string  = mode     if string in model fn args        kwargs string  = self params     if string in model fn args        kwargs string  = config      log info string      model fn result = self  model fn features=features    kwargs      log info string       if not isinstance model fn result  model fn lib estimatorspec         raise valueerror string       return model fn result    def  train model self  input fn  hook  save listeners       if self  train distribution        return self  train model distribute input fn  hook  save listeners      else        return self  train model default input fn  hook  save listeners     def  train model default self  input fn  hook  save listeners       string     worker hook =        with ops graph   as default   as g  g device self  device fn         random seed set random seed self  config tf random seed        global step tensor = self  create and assert global step g                       if global step tensor be not none          train util  get or create global step read g           feature  label  input hook =             self  get feature and label from input fn                input fn  modekeys train         worker hook extend input hook        estimator spec = self  call model fn            feature  label  modekeys train  self config        global step tensor = train util get global step g        return self  train with estimator spec estimator spec  worker hook                                               hook  global step tensor                                               save listeners     def  train model distribute self  input fn  hook  save listeners       string          if  hasattr self  config  string  and         self  config  distribute coordinator mode           distribute coordinator train estimator train            self            lambda est  s  train hook  est  actual train model distribute                  s  input fn  train hook  save listeners             hook        return self     else        self  config  train distribute configure self  config session config        return self  actual train model distribute            self  config  train distribute  input fn  hook  save listeners          def  actual train model distribute self  strategy  input fn  hook                                        save listeners       string               be tpu strategy = strategy   class     name   startswith string       worker hook =        with ops graph   as default   as g                             if be tpu strategy          step per run variable = train get or create step per run variable                                      if hasattr strategy  string           scale ctx = strategy  scale loss for estimator enable           else                             tf contextlib contextmanager         def nullcontextmanager              yield          scale ctx = nullcontextmanager          with strategy scope    scale ctx          random seed set random seed self  config tf random seed          iterator  input hook = self  get iterator from input fn              input fn  modekeys train  strategy          worker hook extend input hook          global step tensor = self  create and assert global step g                            ops add to collection train util global step read key                                strategy extend read var global step tensor            if be tpu strategy                       def step fn ctx  input               string             if isinstance input  tuple                 feature  label = input             else                feature = input               label = none             estimator spec = strategy extend call for each replica                  self  call model fn                  args= feature  label  modekeys train  self config               ctx set last step output                  name=string                  output=estimator spec loss                  reduce op= get loss reduce op for report                ctx set non tensor output                  name=string  output=estimator spec              return estimator spec train op                       initial train loss = constant op constant 1e7            ctx = strategy extend experimental run step on iterator                step fn  iterator  iterations=steps per run variable                initial loop values= string  initial train loss             distribute train op = ctx run op           loss = ctx last step output string            group estimator spec = ctx non tensor output string          else            feature  label = estimator util parse iterator result                iterator get next              group estimator spec = strategy extend call for each replica                self  call model fn                args= feature                      label                        modekeys train                      self config             loss = strategy reduce                 get loss reduce op for report                  group estimator spec loss                axis=none            distribute train op = group estimator spec train op          scaffold =  combine distribute scaffold              group estimator spec scaffold  strategy                    def get hook from the first device per device hook             return                 self  train distribution experimental local result                    per device hook  0                for per device hook in per device hook                      train hook = get hook from the first device              group estimator spec train hook          train chief hook = get hook from the first device              group estimator spec train chief hook          estimator spec = model fn lib estimatorspec              mode=grouped estimator spec mode              loss=loss              train op=strategy group distribute train op               train hooks=training hook              train chief hooks=training chief hook              scaffold=scaffold          return self  train with estimator spec estimator spec  worker hook                                                 hook  global step tensor                                                 save listeners     def  train with estimator spec distribute self  estimator spec  worker hook                                               save listener       string     if save listener        raise valueerror string                        string      with train monitoredtrainingsession          master=self  config master          be chief=self  config be chief          checkpoint dir=self  model dir          scaffold=estimator spec scaffold          hooks=worker hook          chief only hooks=tuple estimator spec train chief hook           save checkpoint secs=self  config save checkpoints secs          save checkpoint steps=self  config save checkpoints step          save summaries steps=self  config save summary step          config=self  session config          max wait secs=self  config session creation timeout secs          log step count steps=self  config log step count step  as mon sess        loss = none       any step do = false       while not mon sess should stop               loss = mon sess run  estimator spec train op  estimator spec loss           any step do = true     if not any step do        log warn string                       string      return loss    def  train with estimator spec self  estimator spec  worker hook  hook                                   global step tensor  save listeners       string     if self  warm start settings        log info string                       self  warm start settings          warm start util warm start  self  warm start settings                          if not any  x op name == string                 for x in ops get collection ops graphkeys summaries           summary scalar string  estimator spec loss      ops add to collection ops graphkeys losses  estimator spec loss      worker hook extend hook      worker hook append          train nantensorhook estimator spec loss            if self  config log step count step be not none        worker hook append            train loggingtensorhook                                    string  estimator spec loss                    string  global step tensor                                every n iter=self  config log step count step              worker hook extend estimator spec train hook       if not  estimator spec scaffold saver or             ops get collection ops graphkeys savers          ops add to collection            ops graphkeys savers            train saver                sharded=true                max to keep=self  config keep checkpoint max                keep checkpoint every n hours=                    self  config keep checkpoint every n hours                 defer build=true                save relative paths=true        if  self  config cluster spec and type          self  train distribution    name   in  string                                                 string                                                 string          return self  train with estimator spec distribute            estimator spec  worker hook  save listeners       chief hook =        all hook = worker hook   list estimator spec train chief hook      saver hook =           h for h in all hook if isinstance h  train checkpointsaverhook       if  self  config save checkpoints secs or         self  config save checkpoints step         if not saver hook          chief hook =               train checkpointsaverhook                  self  model dir                  save secs=self  config save checkpoints secs                  save steps=self  config save checkpoints step                  scaffold=estimator spec scaffold                    saver hook =  chief hook 0       if save listeners        if not saver hook          raise valueerror              string             string             string        else                            for listener in save listeners                       if listener not in saver hook 0   listeners              saver hook 0   listeners append listener                                 save summary step = self  config save summary step     log step count step = self  config log step count step                          if  self  config cluster spec and self  config cluster spec job and          run config tasktype worker in self  config cluster spec job  and          run config tasktype master in self  config cluster spec job                        save summary step = 0       log step count step = none        if  self  config task type == run config tasktype worker and           self  config task id == 0           if  self  config save summary step and             self  config save summary step > 0             worker hook append                train summarysaverhook                    save steps=self  config save summary step                    output dir=self  config model dir                    scaffold=estimator spec scaffold            if  self  config log step count step and             self  config log step count step > 0             worker hook append                train stepcounterhook                    every n steps=self  config log step count step                    output dir=self  config model dir        with train monitoredtrainingsession          master=self  config master          be chief=self  config be chief          checkpoint dir=self  model dir          scaffold=estimator spec scaffold          hooks=worker hook          chief only hooks= tuple chief hook                              tuple estimator spec train chief hook            save checkpoint secs=0            save summaries steps=save summary step          config=self  session config          max wait secs=self  config session creation timeout secs          log step count steps=log step count step  as mon sess        loss = none       any step do = false       while not mon sess should stop               loss = mon sess run  estimator spec train op  estimator spec loss           any step do = true     if not any step do        log warn string                       string      return loss    def  evaluate build graph self  input fn  hooks=none  checkpoint path=none       string     random seed set random seed self  config tf random seed      self  create and assert global step ops get default graph         if self  eval distribution         scaffold  evaluation hook  input hook  update op  eval dict  =             self  call model fn eval distribute input fn  self config       else         scaffold  evaluation hook  input hook  update op  eval dict  =             self  call model fn eval input fn  self config        global step tensor = train util get global step ops get default graph             self  maybe warm start checkpoint path       if ops graphkeys global step in eval dict        raise valueerror            string           string      eval dict ops graphkeys global step  = global step tensor      all hook = list input hook      all hook extend hook      all hook extend list evaluation hook or                    if scaffold and scaffold local init op               evaluation  get or create eval step            scaffold = monitor session scaffold            local init op=control flow ops group                scaffold local init op                monitor session scaffold default local init op               copy from scaffold=scaffold              return scaffold  update op  eval dict  all hook    def  call model fn eval self  input fn  config       string     feature  label  input hook = self  get feature and label from input fn          input fn  modekeys eval       estimator spec = self  call model fn          feature  label  modekeys eval  config      eval metric ops =  verify and create loss metric          estimator spec eval metric ops  estimator spec loss      update op  eval dict =  extract metric update ops eval metric ops      return  estimator spec scaffold  estimator spec evaluation hook              input hook  update op  eval dict     def  call model fn eval distribute self  input fn  config       string      iterator  input hook = self  get iterator from input fn          input fn  modekeys eval  self  eval distribution       be tpu strategy =           self  eval distribution   class     name   startswith string        if be tpu strategy        step per run variable = train get or create step per run variable         def step fn ctx  input           string         if isinstance input  tuple             feature  label = input         else            feature = input           label = none         estimator spec = self  eval distribution extend call for each replica              self  call model fn              args= feature  label  modekeys eval  config           eval metric ops =  verify and create loss metric              estimator spec eval metric ops  estimator spec loss              self  eval distribution          update op  eval dict =  extract metric update ops              eval metric ops  self  eval distribution          ctx set non tensor output name=string  output=estimator spec          ctx set non tensor output name=string  output=eval dict          return update op               ctx = self  eval distribution extend experimental run step on iterator            step fn  iterator  iterations=steps per run variable        update op = ctx run op       eval dict = ctx non tensor output string        group estimator spec = ctx non tensor output string      else        feature  label = estimator util parse iterator result            iterator get next          group estimator spec =             self  eval distribution extend call for each replica                self  call model fn                args= feature  label  modekeys eval  config          eval metric ops =  verify and create loss metric            group estimator spec eval metric ops  group estimator spec loss            self  eval distribution        update op  eval dict =  extract metric update ops            eval metric ops  self  eval distribution       scaffold =  combine distribute scaffold          group estimator spec scaffold  self  eval distribution      evaluation hook = self  eval distribution experimental local result          group estimator spec evaluation hook  0      return  scaffold  evaluation hook  input hook  update op  eval dict     def  evaluate run self  checkpoint path  scaffold  update op  eval dict                      all hook  output dir       string     eval result = evaluation  evaluate once            checkpoint path=checkpoint path          master=self  config evaluation master          scaffold=scaffold          eval ops=update op          final ops=eval dict          hooks=all hook          config=self  session config       current global step = eval result ops graphkeys global step        write dict to summary          output dir=output dir          dictionary=eval result          current global step=current global step       if checkpoint path         write checkpoint path to summary            output dir=output dir            checkpoint path=checkpoint path            current global step=current global step       return eval result    def  maybe warm start self  checkpoint path       if not checkpoint path and self  warm start settings        log info string                       self  warm start settings          warm start util warm start  self  warm start settings      deprecation deprecate        none  string    def export savedmodel        self  export dir base  serve input receiver fn        assets extra=none        as text=false        checkpoint path=none        strip default attrs=false            string          if not serve input receiver fn        raise valueerror string       return self  export all save model          export dir base           modekeys predict  serve input receiver fn           assets extra=assets extra          as text=as text          checkpoint path=checkpoint path          strip default attrs=strip default attrs  
class linearclassifier estimator estimator       doc   = linearclassifierv2   doc   replace string  string     def   init   self                 feature columns                 model dir=none                 n classes=2                 weight column=none                 label vocabulary=none                 optimizer=string                 config=none                 partitioner=none                 warm start from=none                 loss reduction=losses reduction sum                 sparse combiner=string        validate linear sdca optimizer for linear classifier          feature columns=feature columns          n classes=n class          optimizer=optimizer          sparse combiner=sparse combiner      estimator  can estimator api gauge get cell string  set string         head = head lib  binary logistic or multi class head            n class  weight column  label vocabulary  loss reduction       def  model fn feature  label  mode  config         string       return  linear model fn            features=features            labels=labels            mode=mode            head=head            feature columns=tuple feature columns or                optimizer=optimizer            partitioner=partitioner            config=config            sparse combiner=sparse combiner       super linearclassifier  self    init            model fn= model fn          model dir=model dir          config=config          warm start from=warm start from  
class linearestimator estimator estimator       doc   = linearestimatorv2   doc      def   init   self                 head                 feature columns                 model dir=none                 optimizer=string                 config=none                 partitioner=none                 sparse combiner=string       string     def  model fn feature  label  mode  config         return  linear model fn            features=features            labels=labels            mode=mode            head=head            feature columns=tuple feature columns or                optimizer=optimizer            partitioner=partitioner            config=config            sparse combiner=sparse combiner       estimator  can estimator api gauge get cell string  set string        super linearestimator  self    init            model fn= model fn  model dir=model dir  config=config  
class linearregressor estimator estimator       doc   = linearregressorv2   doc   replace string  string     def   init   self                 feature columns                 model dir=none                 label dimension=1                 weight column=none                 optimizer=string                 config=none                 partitioner=none                 warm start from=none                 loss reduction=losses reduction sum                 sparse combiner=string        validate linear sdca optimizer for linear regressor          feature columns=feature columns          label dimension=label dimension          optimizer=optimizer          sparse combiner=sparse combiner       head = head lib  regression head            label dimension=label dimension          weight column=weight column          loss reduction=loss reduction      estimator  can estimator api gauge get cell string  set string         def  model fn feature  label  mode  config         string       return  linear model fn            features=features            labels=labels            mode=mode            head=head            feature columns=tuple feature columns or                optimizer=optimizer            partitioner=partitioner            config=config            sparse combiner=sparse combiner       super linearregressor  self    init            model fn= model fn          model dir=model dir          config=config          warm start from=warm start from  
 estimator export v1= string   def classifier parse example spec feature columns                                    label key                                    label dtype=dtypes int64                                    label default=none                                    weight column=none     parse spec = fc make parse example spec feature columns    label spec = parse ops fixedlenfeature  1    label dtype  label default    return  add label and weight to parse spec        parse spec=parsing spec        label key=label key        label spec=label spec        weight column=weight column  
 estimator export v1= string   def regressor parse example spec feature columns                                     label key                                   label dtype=dtypes float32                                   label default=none                                   label dimension=1                                   weight column=none     parse spec = fc make parse example spec feature columns    label spec = parse ops fixedlenfeature         label dimension    label dtype  label default    return  add label and weight to parse spec        parse spec=parsing spec        label key=label key        label spec=label spec        weight column=weight column  
 estimator export v1= string   def numpy input fn x                     y=none                     batch size=128                     num epochs=1                     shuffle=none                     queue capacity=1000                     num threads=1     string   if not isinstance shuffle  bool       raise valueerror string                      string                      string format shuffle      def input fn        string                order dict data =  validate and convert feature x            feature key = list order dict data key         if y be none        target key = none     elif isinstance y  dict         if not y          raise valueerror string         order dict y = collections ordereddict            sort y items    key=lambda t  t 0          target key = list order dict y key           duplicate key = set feature key  intersection set target key         if duplicate key          raise valueerror string                          string format len duplicate key   duplicate key          order dict data update order dict y      else        target key =  get unique target key order dict data        order dict data target key  = y      if len set v shape 0  for v in order dict data value      = 1        shape dict of x =  k  order dict data k  shape for k in feature key         if target key be none          shape of y = none       elif isinstance target key  string type           shape of y = y shape       else          shape of y =  k  order dict data k  shape for k in target key         raise valueerror string                        string                        string                        string format shape dict of x  shape of y        queue = feed function  enqueue data            order dict data          queue capacity          shuffle=shuffle          num threads=num thread          enqueue size=batch size          num epochs=num epochs       batch =           queue dequeue many batch size          if num epochs be none else queue dequeue up to batch size             if batch        batch pop 0       if isinstance x  np ndarray                feature = batch 0      else               feature = dict zip feature key  batch  len feature key          if target key be none               return feature     elif isinstance target key  string type         target = batch -1        return feature  target     else        target = dict zip target key  batch -len target key            return feature  target    return input fn 
 estimator export v1= string   def pandas input fn x                      y=none                      batch size=128                      num epochs=1                      shuffle=none                      queue capacity=1000                      num threads=1                      target column=string     string   if not have pandas      raise typeerror          string     if not isinstance shuffle  bool       raise valueerror string                      string                      string format shuffle      if not isinstance target column  six string type       raise typeerror string     x = x copy     if y be not none      if target column in x        raise valueerror            string           string    target column  x columns       if not np array equal x index  y index         raise valueerror string                        string    x index  y index       if isinstance y  pd dataframe         y columns =   column   get unique target key x  column                      for column in list y         target column =  v for    v in y columns        x target column  = y     else        x target column  = y          if queue capacity be none      if shuffle        queue capacity = 4   len x      else        queue capacity = len x    min after dequeue = max queue capacity / 4  1     def input fn        string     queue = feed function  enqueue data            x          queue capacity          shuffle=shuffle          min after dequeue=min after dequeue          num threads=num thread          enqueue size=batch size          num epochs=num epochs      if num epochs be none        feature = queue dequeue many batch size      else        feature = queue dequeue up to batch size      assert len feature  == len x columns    1   string                                                  string      feature = feature 1       feature = dict zip list x columns   feature       if y be not none        if isinstance target column  list           key =  k for k    in y columns          value =  feature pop column  for column in target column          target =  k  v for k  v in zip key  value         else          target = feature pop target column        return feature  target     return feature   return input fn 
class inputpipelineconfig object     r   please see the definition of these value in tpuconfig       per shard v1 = 1   per host v1 = 2   per host v2 = 3   broadcast = 4   slice = 5 
class runconfig run config lib runconfig     string    def   init   self                 tpu config=none                 evaluation master=none                 master=none                 cluster=none                   kwargs       string     super runconfig  self    init     kwargs      self  tpu config = tpu config or tpuconfig       self  cluster = cluster                if master be not none        if cluster be not none          raise valueerror string        self  master = master     else        if cluster          self  master = cluster master        if evaluation master be not none        self  evaluation master = evaluation master     elif  not self  evaluation master and           self task type  = run config lib tasktype evaluator                                                   self  evaluation master = self  master           if cluster        self  cluster spec = cluster cluster spec                 if self  session config be none            self  session config = config pb2 configproto              allow soft placement=true  isolate session state=true        if self  session config hasfield string           raise valueerror              string             string        if self  cluster spec          self  session config cluster def copyfrom              self  cluster spec as cluster def       def  maybe overwrite session config for distribute train self                      pass     property   def evaluation master self       return self  evaluation master     property   def master self       return self  master     property   def tpu config self       return self  tpu config     property   def cluster self       return self  cluster    def replace self    kwargs       if string not in kwargs        return super runconfig  self  replace   kwargs       tpu config = kwargs pop string      new instance = super runconfig  self  replace   kwargs      new instance  tpu config = tpu config       return new instance 
class tpuconfig      collections namedtuple  tpuconfig              iterations per loop            num shards            num core per replica            per host input for train            tpu job name            initial infeed sleep secs            input partition dim            eval train input configuration            experimental host call every n step              r   tpu relate configuration require by `tpuestimator`     args      iterations per loop  this be the number of train step run in tpu       system before return to cpu host for each `session run`  this mean       global step be increase `iterations per loop` time in one `session run`        it be recommend to be set as number of global step for next checkpoint        note that in evaluation don t use this value  instead we run total eval       `steps` on tpu for a single `session run`         experimental   `iterations per loop` can be specify as a time interval        to specify n second in one `session run`  one can specify it as `ns` and       substitute the n with the n with the number of desire second        alternatively  the unit of time can also be specify in minutes or hours        e g  `3600s` or `60m` or `1h`      num shards   deprecate  ignore by tpuestimator         the number of model replicas in the system  for non-model-parallelism       case  this number equal the total number of tpu core  for       model-parallelism  the total number of tpu core equal       num core per replica   num shards      num core per replica  default to `none`  which disable model parallelism        an integer which describe the number of tpu core per model replica  this       be require by model-parallelism which enable partition       the model to multiple core  currently num core per replica must be       1  2  4  or 8      per host input for train  if `true`  for `per host v1`  the `input fn` be       invoke once on each host  and the number of host must be smaller or       equal to the number of replicas  for per host v2  the `input fn` be       invoke once for each host  if the number of host be less than the number       of replicas  or replica  if the number of replicas be less than the number       of host  with the per-core input pipeline configuration  it be invoke       once for each core        with a global batch size `train batch size` in `tpuestimator` constructor        the batch size for each shard be `train batch size` //  host in the       `true` or `per host v1` mode  in `per host v2` mode  it be       `train batch size` //  core  in `broadcast` mode  `input fn` be only       invoke once on host 0 and the tensors be broadcast to all other       replicas  the batch size equal to `train batch size`  with the per-core       input pipeline configuration  the shard batch size be also       `train batch size` //  core        note  per host input for training==per shard v1 only support mode train      tpu job name  the name of the tpu job  typically  this name be auto-inferred       within tpuestimator  however when use clusterspec propagation in more       esoteric cluster configurations  you may need to specify the job name as a       string      initial infeed sleep secs  the number of second the infeed thread should       wait before enqueueing the first batch  this help avoid timeouts for       model that require a long compilation time      input partition dim  a nest list to describe the partition dim       for all the tensors from input fn    the structure of       input partition dim must match the structure of `features` and       `labels` from input fn    the total number of partition must match       `num core per replica`  for example  if input fn   return two tensors        image with shape  n  h  w  c  and label  n         input partition dim =   1  2  2  1   none  will split the image to 4       piece and fee into 4 tpu core  label tensor be directly broadcast       to all the tpu core since the partition dim be `none`        current limitations  this feature be only support with the per host v2       input mode      eval train input configuration  if `sliced`  `input fn` be only       invoke once on host 0 and the tensors be broadcast to all other       replicas  unlike per host input for training=broadcast  each replica will       only get a slice of the data instead of a whole copy  if `per host v1`        the behaviour be determine by per host input for train      experimental host call every n step  within a train loop  this argument       set how often host call be perform during train  host call will       be evaluate every n step within a train loop where n be the value of       this argument     raise        valueerror  if `num core per replica` be not 1  2  4  8       128           def   new          cls        iterations per loop=2        num shards=none        num core per replica=none        per host input for training=true        tpu job name=none        initial infeed sleep secs=none        input partition dims=none        eval train input configuration=inputpipelineconfig per host v1        experimental host call every n steps=1          check iterations per loop      util lib parse iterations per loop iterations per loop         check num shards      if num shards be not none        util lib check positive integer num shards   tpuconfig num shards        if input partition dim be not none        if len input partition dim   = 1 and len input partition dim   = 2          raise valueerror               input partition dim must be a list/tuple with one or two                elements           if per host input for train be not inputpipelineconfig per host v2          raise valueerror               input partition dim be only support in per host v2 mode           if num core per replica be none          raise valueerror               input partition dim require set num core per replica           check num core per replica     if num core per replica be not none        if num core per replica not in   1  2  4  8  16  32  64  128            raise valueerror               num core per replica must be 1  2  4  8  16  32  64  128                 get     format str num core per replica         if eval train input configuration not in           inputpipelineconfig per host v1  inputpipelineconfig slice              raise valueerror             eval train input configuration must be per host v1 or slice               get     format str eval train input configuration           per host input for train may be true  false  or integer in  1  3         map legacy value  true  false  to numeric value      if per host input for train be false        per host input for train = inputpipelineconfig per shard v1     elif per host input for train be true        per host input for train = inputpipelineconfig per host v1        check initial infeed sleep secs      if initial infeed sleep secs        util lib check positive integer initial infeed sleep secs                                         tpuconfig initial infeed sleep secs        tpu job name = tpu job name or  get tpu job name from tf config        return super tpuconfig  cls    new            cls          iterations per loop=iterations per loop          num shards=num shards          num core per replica=num core per replica          per host input for training=per host input for train          tpu job name=tpu job name          initial infeed sleep secs=initial infeed sleep secs          input partition dims=input partition dim          eval train input configuration=eval train input configuration          experimental host call every n steps=         experimental host call every n step  
class tpuestimator estimator lib estimator     string    def   init   self                 model fn=none                 model dir=none                 config=none                 params=none                 use tpu=true                 train batch size=none                 eval batch size=none                 predict batch size=none                 batch axis=none                 eval on tpu=true                 export to tpu=true                 export to cpu=true                 warm start from=none                 embed config spec=none                 export save model api version=exportsavedmodelapiversion v1       string     if config be none or not isinstance config  tpu config runconfig         raise valueerror            string       if params be not none and any k in params for k in  reserve params key         raise valueerror string format             reserve params key  params        if use tpu                      if train batch size be none          raise valueerror string        util lib check positive integer train batch size  string         if  config tpu config per host input for train be           tpu config inputpipelineconfig per shard v1 and           config tpu config num core per replica           raise valueerror              string             string         if eval batch size be not none          util lib check positive integer eval batch size  string         if predict batch size be not none          util lib check positive integer predict batch size                                          string         if embed config spec          if  config tpu config per host input for train  =             tpu config inputpipelineconfig per host v2             raise valueerror string                            string format                                 config tpu config per host input for train           self  embed from feature columns =               embed config spec feature columns be not none       if  not  use tpu and eval on tpu  and embed config spec and         embed config spec partition strategy == string         raise valueerror string                        string            estimator lib  verify model fn args model fn  params                       model function = self  augment model fn model fn  batch axis                      self  log every n step = config log step count step     config = config replace log step count steps=none            params = params or        super tpuestimator  self    init            model fn=model function          model dir=model dir          config=config          params=params          warm start from=warm start from      self  iterations per train loop = util lib parse iterations per loop          self  config tpu config iterations per loop                               if self  iterations per train loop unit == string        self  log every n secs = self  iterations per train loop value       self  log every n step = none     elif self  iterations per train loop unit == string        if self  log every n step be not none                                                       self  log every n step =               int math ceil float self  log every n step  /                           self  iterations per train loop value          self  log every n secs = none     else        assert false   string                      string                      string                 self  ctx = tpu context  get tpu context          self  config  train batch size  eval batch size  predict batch size          use tpu  eval on tpu  embed config spec       self  export to cpu = export to cpu     self  export to tpu = export to tpu      if not isinstance export save model api version                        exportsavedmodelapiversion         raise valueerror string                        string format                             export save model api version       self  export save model api version = export save model api version     self  be input fn invoke = none      self  rendezvous =       def  add meta graph for mode self                                 builder                                 input receiver fn map                                 checkpoint path                                 save variables=true                                 mode=model fn lib modekeys predict                                 export tags=none                                 check variables=true                                 strip default attrs=true       if self  export to tpu and mode  = model fn lib modekeys predict        log warn string                       string                       string format mode        if not self  export to cpu and not self  export to tpu        raise valueerror string       if self  export to cpu         super tpuestimator  self   add meta graph for mode            builder            input receiver fn map            checkpoint path            save variables            mode=mode            export tags=export tag            check variables=check variables            strip default attrs=strip default attrs        if self  export to tpu and mode == model fn lib modekeys predict        input receiver fn map =              inference on tpu mode  input receiver fn map mode                export tag =  tag constants serve  tag constants tpu        mode =  inference on tpu mode               if not self  export to cpu          check variables = save variables = true       else          check variables = save variables = false        super tpuestimator  self   add meta graph for mode            builder            input receiver fn map            checkpoint path            save variables=save variables            mode=mode            export tags=export tag            check variables=check variables            strip default attrs=strip default attrs      def  call model fn self  feature  label  mode  config       if self  export save model api version == exportsavedmodelapiversion v1        if mode ==  inference on tpu mode          return self  call model fn for inference feature  label  mode  config        else          return super tpuestimator  self   call model fn feature  label  mode                                                          config      else        if mode ==  inference on tpu mode          context = tpu  tpuinferencecontext string          try            context enter             result = super tpuestimator  self   call model fn                feature  label  mode  config          finally            context exit           return result       else          return super tpuestimator  self   call model fn              feature  label  mode  config     def  call model fn for inference self  feature  label  mode  config       string     if mode  =  inference on tpu mode        raise valueerror string                        string format  inference on tpu mode  mode       return model fn inference on tpu          self  model fn          feature          label          config          self  params          batch config=none     def  create global step self  graph       string     return  create global step graph     def  convert train step to hook self  step  max step       with self  ctx with mode model fn lib modekeys train  as ctx        if ctx be run on cpu            return super tpuestimator  self   convert train step to hook              step  max step            if step be none and max step be none        raise valueerror            string           string            if step be not none        util lib check positive integer step  string      if max step be not none        util lib check positive integer max step  string       return            tpustopatstephook              self  iterations per train loop  step  max step           def  convert eval step to hook self  step       with self  ctx with mode model fn lib modekeys eval  as ctx        if ctx be run on cpu            return super tpuestimator  self   convert eval step to hook step       if step be none        raise valueerror string       util lib check positive integer step  string       return           evaluation  stopafternevalshook                num evals=steps            setevaliterationshook step           def  call input fn self  input fn  mode       string     input fn args = function utils fn args input fn      config = self config       kwargs =        if string in input fn args        kwargs string  = self params       else        raise valueerror string                        string                        string format input fn       if string in input fn args        kwargs string  = config      if string in input fn args        kwargs string  = mode           self  be input fn invoke = true      with self  ctx with mode mode  as ctx        if  ctx be run on cpu   and           ctx be input slice broadcast to all core             raise valueerror string                          string                      batch size for input fn = ctx batch size for input fn       if batch size for input fn be not none           add item to params kwargs string    batch size key                              batch size for input fn                       if ctx be run on cpu be export mode=false           with ops device string             return input fn   kwargs                                                                               def  input fn ctx            add item to params kwargs string    ctx key  ctx          return input fn   kwargs         return  input fn    def  validate feature in predict input self  result       string     pass    def train self              input fn              hooks=none              steps=none              max steps=none              save listeners=none       rendezvous = error handle errorrendezvous num sources=3      self  rendezvous model fn lib modekeys train  = rendezvous     try        return super tpuestimator  self  train            input fn=input fn            hooks=hooks            steps=steps            max steps=max step            save listeners=saving listeners      except exception          rendezvous record error string  sys exc info        finally        rendezvous record do string        rendezvous raise errors      def evaluate self                 input fn                 steps=none                 hooks=none                 checkpoint path=none                 name=none       rendezvous = error handle errorrendezvous num sources=3      self  rendezvous model fn lib modekeys eval  = rendezvous     try        return super tpuestimator  self  evaluate            input fn            steps=steps            hooks=hooks            checkpoint path=checkpoint path            name=name      except exception          rendezvous record error string  sys exc info        finally        rendezvous record do string        rendezvous raise errors      def predict self                input fn                predict keys=none                hooks=none                checkpoint path=none                yield single examples=true       rendezvous = error handle errorrendezvous num sources=3      self  rendezvous model fn lib modekeys predict  = rendezvous     try        for result in super tpuestimator  self  predict            input fn=input fn            predict keys=predict key            hooks=hooks            checkpoint path=checkpoint path            yield single examples=yield single examples           yield result     except exception          rendezvous record error string  sys exc info        finally        rendezvous record do string        rendezvous raise errors        rendezvous record do string      rendezvous raise errors      def  augment model fn self  model fn  batch axis       string      def  model fn feature  label  mode  config  params         string                      if self  be input fn invoke          be export mode = false       else          be export mode = true               self  be input fn invoke = none        if be export mode          if mode ==  inference on tpu mode             add item to params params   use tpu key  true            mode = model fn lib modekeys predict         else             add item to params params   use tpu key  false         with self  ctx with mode mode  as ctx          model fn wrapper =  modelfnwrapper model fn  config  params  ctx                             if  self  log every n step be not none             or self  log every n secs be not none             examples hook = examplespersecondhook                ctx global batch size                               output dir= self model dir                           if not config or config save summary step                           else none                                every n steps=self  log every n step                every n secs=self  log every n secs           if ctx be run on cpu be export mode=is export mode             log info string  mode            estimator spec = model fn wrapper call without tpu                feature  label  be export mode=is export mode            if  self  log every n step be not none               or self  log every n secs be not none               estimator spec = estimator spec  replace                  train hooks=estimator spec train hook    examples hook              return estimator spec          assert label be none  string                  assert callable feature   string         input fn = feature          tpu init ops =            if ctx embed config and mode == model fn lib modekeys train            dummy table variables  dummy table variables init =                 tpu embed gradient create dummy table variables                    ctx embed config tpu embed             ctx embed config dummy table variables = dummy table variables           tpu init ops append dummy table variables init           input holders =  inputpipeline input fn  batch axis  ctx          enqueue ops  dequeue fn  input hook  run infeed loop on coordinator =               input holders generate infeed enqueue ops and dequeue fn             graph = ops get default graph           for enqueue op in enqueue ops            if isinstance enqueue op  list               graph get collection ref  tpu enqueue ops  extend enqueue op            else              graph add to collection  tpu enqueue ops  enqueue op           if mode == model fn lib modekeys train            compile op  loss  host call  scaffold fn  train hook =                  train on tpu system ctx  model fn wrapper  dequeue fn             if ctx embed config              g = ops get default graph               table to config dict =                   ctx embed config tpu embed table to config dict              optimization parameters =                   ctx embed config tpu embed optimization parameters              if self  embed from feature columns                embed variable name by table  slot variable name by table =                      tpu estimator embed get full variable name                        g  table to config dict  optimization parameters                                                 else                embed variable name by table = none               slot variable name by table = none             embed variables and ops =                   ctx embed config tpu embed create variables and ops                      embed variable name by table                      slot variable name by table                                tpu init ops extend embed variables and ops load ops                                               scaffold =  get scaffold scaffold fn             host ops = host call create tpu hostcall              shutdown hook =              shutdown mode = os environ get string                                           string            if shutdown mode              if shutdown mode == string                finalizer hook =                     session support shutdownlameworkers                                elif shutdown mode == string                finalizer hook =                     session support shutdownallworkers                                elif shutdown mode == string                finalizer hook =                     session support resetcomputation                                elif not shutdown mode                finalizer hook =                else                raise valueerror                    string   shutdown mode               if finalizer hook                shutdown hook append                    session support gracefulshutdownhook                        checkpoint prefix=self model dir   string                        on shutdown hooks=finalizer hook              with ops control dependencies  loss                global step = array ops identity train get global step              hook = input hook   shutdown hook           hook extend                 tpuinfeedoutfeedsessionhook                    ctx                    enqueue ops                    host ops                    tpu compile op=compile op                    run infeed loop on coordinator=                        run infeed loop on coordinator                     rendezvous=self  rendezvous mode                     master=self  config master                    session config=self  session config                    tpu init ops=tpu init ops                    outfeed every n steps=self  config tpu config                    experimental host call every n step                 installsignalhandlerhook                          if  check add preemption hook self  config cluster               hook extend                   preempt hook cloudtpupreemptedhook self  config cluster              if  self  log every n step be not none               or self  log every n secs be not none               if self  iterations per train loop unit == string                examples hook  set step per run                      self  iterations per train loop value              hook append train loggingtensorhook                                        string  array ops identity loss                       string  global step                                     every n iter=self  log every n step                  every n secs=self  log every n secs               hook append examples hook             if train hook              hook extend train hook             chief hook =              if  self  config save checkpoints secs or               self  config save checkpoints step               checkpoint hook = train checkpointsaverhook                  self model dir                  save secs=self  config save checkpoints secs                  save steps=self  config save checkpoints step                  scaffold=scaffold              if self  iterations per train loop unit == string                checkpoint hook  set step per run                      self  iterations per train loop value              else                                                                                           checkpoint hook  set step per run                      100000              chief hook append checkpoint hook             summary scalar model fn lib loss metric key  loss            with ops control dependencies  loss                update ops =  sync variables ops ctx              if ctx embed config                update ops extend embed variables and ops retrieve ops                           validate tpu train graph              train op = control flow ops group  update ops            graph add to collection  tpu train op  train op             return model fn lib estimatorspec                mode                loss=loss                train chief hooks=chief hook                train hooks=hooks                train op=train op                scaffold=scaffold           if mode == model fn lib modekeys eval            compile op  total loss  host call  scaffold fn  eval hook =                  eval on tpu system ctx  model fn wrapper  dequeue fn             if ctx embed config              g = ops get default graph               table to config dict =                   ctx embed config tpu embed table to config dict              if self  embed from feature columns                embed variable name by table    =                      tpu estimator embed get full variable name                        g  table to config dict                              else                embed variable name by table = none             embed variables and ops =                   ctx embed config tpu embed create variables and ops                      embed variable name by table                                tpu init ops extend embed variables and ops load ops                                               scaffold =  get scaffold scaffold fn            iterations per loop var =  create or get iterations per loop             mean loss = math ops div                total loss                math ops cast iterations per loop var  dtype=total loss dtype              with ops control dependencies  mean loss                                                       internal ops to run =  sync variables ops ctx              internal ops to run append                   increase eval step op iterations per loop var              host call ret = host call create tpu hostcall             eval metric ops =              eval update ops =               eval metrics = host call ret get string                if eval metrics                                                                               with ops control dependencies internal ops to run                 dummy update op = control flow ops no op                for k  v in eval metrics items                  eval metric ops k  =  v 0   dummy update op                eval update ops append v 1             else                                                     with ops control dependencies internal ops to run                 mean loss = array ops identity mean loss             if string not in host call ret              host ops =              else              host ops = host call ret string            hook =                 tpuinfeedoutfeedsessionhook                    ctx                    enqueue ops                    eval update ops   host ops                    tpu compile op=compile op                    run infeed loop on coordinator=                        run infeed loop on coordinator                     rendezvous=self  rendezvous mode                     master=self  config evaluation master                    session config=self  session config                    tpu init ops=tpu init ops                input hook            if  check add preemption hook self  config cluster               hook extend                   preempt hook cloudtpupreemptedhook self  config cluster               if eval hook              hook extend eval hook             return model fn lib estimatorspec                mode                loss=mean loss                evaluation hooks=hooks                eval metric ops=eval metric ops                scaffold=scaffold                    assert mode == model fn lib modekeys predict           compile op  dummy predict op  host call           scaffold fn  prediction hook  =  predict on tpu system               ctx  model fn wrapper  dequeue fn          scaffold =  get scaffold scaffold fn          with ops control dependencies  dummy predict op              internal ops to run =  sync variables ops ctx            with ops control dependencies internal ops to run               dummy predict op = control flow ops no op                                                                                                                                                                                                                  enqueue ops append dummy predict op           host call ret = host call create tpu hostcall           if string not in host call ret            host ops =            else            host ops = host call ret string           predictions = host call ret string           verify cross host transfer size              predictions              message=                  string                 string           signal = host call ret string           with ops control dependencies host ops             host ops =                scalar stop signal =  stopsignals as scalar stop signal                signal            predictions =  paddingsignals slice tensor or dict                predictions  signal           hook =                stoppingpredicthook scalar stop signal               tpuinfeedoutfeedsessionhookforprediction                  ctx  enqueue ops  host ops  rendezvous=self  rendezvous mode                   tpu compile op=compile op                  master=self  config master                  session config=self  session config               input hook          if prediction hook            hook extend prediction hook           return model fn lib estimatorspec              mode              prediction hooks=hooks              predictions=predictions              scaffold=scaffold       return  model fn 
class tpuestimatorspec model fn lib  tpuestimatorspec       string    def   new   cls                mode                predictions=none                loss=none                train op=none                eval metrics=none                export outputs=none                scaffold fn=none                host call=none                train hooks=none                evaluation hooks=none                prediction hooks=none       string     host call =        if eval metrics be not none        host call string  = eval metrics     if host call be not none        host call string  = host call      outfeedhostcall validate host call       train hook = tuple train hook or         evaluation hook = tuple evaluation hook or         prediction hook = tuple prediction hook or          for hook in train hook   evaluation hook   prediction hook        if not isinstance hook  session run hook sessionrunhook           raise typeerror string                          format hook        return super tpuestimatorspec  cls    new            cls          mode=mode          predictions=predictions          loss=loss          train op=train op          eval metrics=eval metrics          export outputs=export output          scaffold fn=scaffold fn          host call=host call          train hooks=training hook          evaluation hooks=evaluation hook          prediction hooks=prediction hook     def as estimator spec self       string     host call =        if self eval metrics be not none        host call string  = self eval metrics     if self host call be not none        host call string  = self host call     host call ret =  outfeedhostcall create cpu hostcall host call      eval metric ops = none     if self eval metrics be not none        eval metric ops = host call ret string      hook = none     if self host call be not none        hook =   outfeedhostcallhook host call ret string        loss = self loss     if tensor tracer tensortracer be enable   \        and self train op be not none        tt = tensor tracer tensortracer         loss = tt trace cpu ops get default graph    loss  self train op       hook = tuple hook or         scaffold = self scaffold fn   if self scaffold fn else none     return model fn lib estimatorspec          mode=self mode          predictions=self predictions          loss=loss          train op=self train op          eval metric ops=eval metric ops          export outputs=self export output          scaffold=scaffold          train hooks=self train hook   hook          evaluation hooks=self evaluation hook   hook          prediction hooks=self prediction hook   hook  
 tf export v1= string   def categorical column with vocabulary file key                                              vocabulary file                                              vocabulary size=none                                              num oov buckets=0                                              default value=none                                              dtype=dtypes string     string   return categorical column with vocabulary file v2        key  vocabulary file  vocabulary size        dtype  default value        num oov bucket  
 tf export v1= string   def input layer feature                  feature columns                  weight collections=none                  trainable=true                  cols to vars=none                  cols to output tensors=none     string   return  internal input layer        feature        feature columns        weight collections=weight collections        trainable=trainable        cols to vars=cols to vars        cols to output tensors=cols to output tensors  
 tf export v1= string   def linear model feature                   feature columns                   units=1                   sparse combiner=string                   weight collections=none                   trainable=true                   cols to vars=none     string   with variable scope variable scope none  string  as vs      model name =  strip lead slash vs name    linear model layer =  linearmodel        feature columns=feature columns        units=units        sparse combiner=sparse combiner        weight collections=weight collections        trainable=trainable        name=model name    retval = linear model layer feature      if cols to vars be not none      cols to vars update linear model layer cols to vars      return retval 
 tf export v1= string   def make parse example spec feature columns     string   result =      for column in feature columns      if not isinstance column   featurecolumn         raise valueerror            string           string format column       config = column  parse example spec       for key  value in six iteritems config         if key in result and value  = result key           raise valueerror              string             string format key  value  result key        result update config    return result 
 tf export v1= string   def share embed columns categorical columns                               dimension                               combiner=string                               initializer=none                               share embed collection name=none                               ckpt to load from=none                               tensor name in ckpt=none                               max norm=none                               trainable=true     string   if context execute eagerly        raise runtimeerror string                        string     if  dimension be none  or  dimension < 1       raise valueerror string format dimension     if  ckpt to load from be none   =  tensor name in ckpt be none       raise valueerror string                      string     if  initializer be not none  and  not callable initializer        raise valueerror string    if initializer be none      initializer = init ops truncate normal initializer          mean=0 0  stddev=1  / math sqrt dimension            sort columns = sort categorical columns  key=lambda x  x name     c0 = sort columns 0    num bucket = c0  num bucket     if not isinstance c0  fc old  categoricalcolumn         raise valueerror          string         string format c0  type c0      if isinstance c0                   fc old  weightedcategoricalcolumn  weightedcategoricalcolumn          c0 = c0 categorical column   for c in sort columns 1        if isinstance          c   fc old  weightedcategoricalcolumn  weightedcategoricalcolumn            c = c categorical column     if not isinstance c  type c0          raise valueerror            string           string           string           string format c0  type c0   c  type c        if num bucket  = c  num bucket          raise valueerror            string           string           string format                c0  num bucket  c  c  num bucket        if not share embed collection name      share embed collection name = string join c name for c in sort columns      share embed collection name  = string    result =      for column in categorical columns      result append          fc old  sharedembeddingcolumn                categorical column=column              initializer=initializer              dimension=dimension              combiner=combiner              share embed collection name=shared embed collection name              ckpt to load from=ckpt to load from              tensor name in ckpt=tensor name in ckpt              max norm=max norm              trainable=trainable      return result 
 tf export v1= string   def copy oldpath  newpath  overwrite=false     string   copy v2 oldpath  newpath  overwrite  
 tf export v1= string   def delete recursively dirname     string   delete recursively v2 dirname  
 tf export v1= string   def file exist filename     string   return file exist v2 filename  
class fastgfile  fileio     string     deprecate none  string    def   init   self  name  mode=string       super fastgfile  self    init   name=name  mode=mode  
 tf export v1= string   def get match file filename     string   return get match file v2 filename  
 tf export v1= string   def be directory dirname     string   return be directory v2 dirname  
 tf export v1= string   def list directory dirname     string   return list directory v2 dirname  
 tf export v1= string   def recursive create dir dirname     string   recursive create dir v2 dirname  
 tf export v1= string   def create dir dirname     string   create dir v2 dirname  
 tf export v1= string   def delete file filename     string   delete file v2 filename  
 tf export v1= string   def rename oldname  newname  overwrite=false     string   rename v2 oldname  newname  overwrite  
 tf export v1= string   def stat filename     string   return stat v2 filename  
 tf export v1= string   def walk top  in order=true     string   return walk v2 top  in order  
 deprecation deprecate      date=none      instructions=string   tf export v1= string   def convert variables to constants sess                                     input graph def                                     output node name                                     variable name whitelist=none                                     variable name blacklist=none     string    get input name = lambda node  index=0  node input index  split string  0     def create const op node name  dtype  data  data shape=none       string     output node = node def pb2 nodedef       output node op = string     output node name = node name     output node attr string  copyfrom dtype      output node attr string  copyfrom          attr value pb2 attrvalue              tensor=tensor util make tensor proto                  data  dtype=dtype type  shape=data shape        return output node          inference graph = extract sub graph input graph def  output node name        map name to node =         node name  node for node in inference graph node           variable name =      variable dict name =      resource op type =      for node in inference graph node      if node op in  string  string  string         variable name = node name       if   variable name whitelist be not none and            variable name not in variable name whitelist  or            variable name blacklist be not none and            variable name in variable name blacklist            continue       variable dict name append variable name        if node op == string          variable name append variable name   string        else          variable name append variable name   string      elif node op in  string  string                              source op name =  get input name node         while  source op name and map name to node source op name 0   op in               control flow op name or identity           source op name = source op name pop            if source op name not in resource op type            resource op type source op name  = node attr string            source op name append                get input name map name to node source op name             if map name to node source op name  op == string            merge resource name = get input name                map name to node source op name   index=1            if merge resource name not in resource op type              resource op type merge resource name  = node attr string              source op name append                  get input name map name to node merge resource name           for source node in source op name          if map name to node source node  op  = string            raise valueerror string                            string        if variable name      return variables = sess run variable name    else      return variables =      variables data map = dict zip variable dict name  return variables     log info string  len return variables         output graph def = graph pb2 graphdef     how many convert = 0   for input node in inference graph node      output node = node def pb2 nodedef       if input node name in variables data map        data = variables data map input node name        output node = create const op input node name  input node attr string                                       data  data shape        how many convert  = 1     elif input node name in resource op type                             output node op = input node op       output node name = input node name       for in node in input node input          output node input append in node        for attr name in input node attr          if str attr name   = string            output node attr attr name  copyfrom input node attr attr name         output node attr string  copyfrom resource op type input node name       elif input node op == string                             output node op = string       output node name = input node name       output node input extend  input node input 0          output node attr string  copyfrom input node attr string         if string in input node attr          output node attr string  copyfrom input node attr string       elif input node op == string                             if input node attr string  i  = 0          raise valueerror string        axis data = input node attr string  i       axis node name = input node name   string       axis dtype = input node attr string        output axis node = create const op axis node name  axis dtype  axis data        output graph def node extend  output axis node          output node op = string       output node name = input node name       output node input extend             input node input 0   input node input 1   axis node name         output node attr string  copyfrom input node attr string         output node attr string  copyfrom input node attr string         output node attr string  copyfrom axis dtype        if string in input node attr          output node attr string  copyfrom input node attr string       else        output node copyfrom input node      output graph def node extend  output node      output graph def library copyfrom inference graph library    log info string  how many convert    return output graph def 
 deprecation deprecate      date=none      instructions=string   tf export v1= string   def extract sub graph graph def  dest nod     string    if not isinstance graph def  graph pb2 graphdef       raise typeerror string     if isinstance dest nod  six string type       raise typeerror string     name to input name  name to node  name to seq num =  extract graph summary        graph def     assert nod be present name to node  dest nod     nod to keep =  bfs for reachable nod dest nod  name to input name     nod to keep list = sort        list nod to keep   key=lambda n  name to seq num n        out = graph pb2 graphdef     for n in nod to keep list      out node extend  copy deepcopy name to node n       out library copyfrom graph def library    out versions copyfrom graph def versions     return out 
 deprecation deprecate      date=none      instructions=string   tf export v1= string   def must run on cpu node  pin variables on cpu=false     string    if isinstance node  ops operation       node def = node node def   else      assert isinstance node  node def pb2 nodedef      node def = node       if pin variables on cpu and  be variable op node def op       return true       if node def op == string           dtype = node def attr string  type     if dtype == dtypes string or dtype == dtypes int32        return true    if node def op in  string  string       dtype = node def attr string  type     if dtype == dtypes int32               return true    if node def op in  string       dtype = node def attr string  type     if dtype == dtypes int32               return true   return false 
 deprecation deprecate      date=none      instructions=string   tf export v1= string   def remove train nod input graph  protect nodes=none     string   if not protect nod      protect nod =       type to remove =  string  true     input nod = input graph node   name to remove =      for node in input nod      if node op in type to remove and node name not in protect nod        name to remove node name  = true    nod after removal =      for node in input nod      if node name in name to remove        continue     new node = node def pb2 nodedef       new node copyfrom node      input before removal = node input     del new node input        for full input name in input before removal        input name = re sub r  \    string  full input name        if input name in name to remove          continue       new node input append full input name      nod after removal append new node     type to splice =  string  true    control input name = set     node name with control input = set     for node in nod after removal      for node input in node input        if string in node input          control input name add node input replace string  string           node name with control input add node name     name to splice =      for node in nod after removal      if node op in type to splice and node name not in protect nod                             if node name not in node name with control input          name to splice node name  = node input 0        name to splice =  name  value for name  value in name to splice items                        if name not in control input name     nod after splice =      for node in nod after removal      if node name in name to splice        continue     new node = node def pb2 nodedef       new node copyfrom node      input before removal = node input     del new node input        for full input name in input before removal        input name = re sub r  \    string  full input name        while input name in name to splice          full input name = name to splice input name          input name = re sub r  \    string  full input name        new node input append full input name      nod after splice append new node     output graph = graph pb2 graphdef     output graph node extend nod after splice    return output graph 
 deprecation deprecate      date=none      instructions=string    tf export v1= string   def tensor shape from node def name graph  input name     string            if string not in input name      canonical name = input name   string   else      canonical name = input name   tensor = graph get tensor by name canonical name    shape = tensor get shape     return shape 
class resizemethodv1 object     bilinear = 0   nearest neighbor = 1   bicubic = 2   area = 3 
 tf export v1= string    deprecation deprecate args none                               string                               string  def crop and resize v1        image      box      box ind=none      crop size=none      method=string      extrapolation value=0      name=none      box indices=none     box ind = deprecation deprecate argument lookup string  box indices                                                     string  box ind    return gen image ops crop and resize image  box  box ind  crop size  method                                         extrapolation value  name  
 tf export v1= string   def draw bound box image  box  name=none  colors=none     string   return draw bound box v2 image  box  color  name  
 tf export v1= string   def extract glimpse      input        size      offset      centered=true      normalized=true      uniform noise=true      name=none     string   return gen image ops extract glimpse        input=input        size=size        offsets=offsets        centered=centered        normalized=normalized        uniform noise=uniform noise        name=name  
 tf export v1= string  string   def resize image image                    size                    method=resizemethodv1 bilinear                    align corners=false                    preserve aspect ratio=false                    name=none     string    def resize fn image t  new size       string     if method == resizemethodv1 bilinear or method == resizemethod bilinear        return gen image ops resize bilinear            image t  new size  align corners=align corner      elif  method == resizemethodv1 nearest neighbor or           method == resizemethod nearest neighbor         return gen image ops resize nearest neighbor            image t  new size  align corners=align corner      elif method == resizemethodv1 bicubic or method == resizemethod bicubic        return gen image ops resize bicubic            image t  new size  align corners=align corner      elif method == resizemethodv1 area or method == resizemethod area        return gen image ops resize area            image t  new size  align corners=align corner      else        raise valueerror string     return  resize image common        image        resize fn        size        preserve aspect ratio=preserve aspect ratio        name=name        skip resize if same=true  
def resize area image  size  align corners=false  name=none     r   resize `images` to `size` use area interpolation     input image can be of different type but output image be always float     the range of pixel value for the output image might be slightly different   from the range for the input image because of limit numerical precision    to guarantee an output range  for example ` 0 0  1 0 `  apply   `tf clip by value` to the output     each output pixel be compute by first transform the pixel s footprint into   the input tensor and then average the pixels that intersect the footprint  an   input pixel s contribution to the average be weight by the fraction of its   area that intersect the footprint   this be the same as opencv s inter area     args      image  a `tensor`  must be one of the follow type  `int8`  `uint8`  `int16`  `uint16`  `int32`  `int64`  `half`  `float32`  `float64`        4-d with shape ` batch  height  width  channel `      size   a 1-d int32 tensor of 2 elements  `new height  new width`   the       new size for the image      align corner  an optional `bool`  default to `false`        if true  the center of the 4 corner pixels of the input and output tensors be       align  preserve the value at the corner pixels  default to false      name  a name for the operation  optional      return      a `tensor` of type `float32`           ctx =  context  context or  context context     tld =  ctx  thread local data   if tld be eager      try         result =  pywrap tensorflow tfe py fastpathexecute           ctx  context handle  tld device name  string  name          tld op callbacks  image  size  string  align corner        return  result     except  core  fallbackexception        try          return resize area eager fallback              image  size  align corners=align corner  name=name  ctx= ctx        except  core  symbolicexception          pass       except  core  notokstatusexception as e         ops raise from not ok status e  name       if align corner be none      align corner = false   align corner =  execute make bool align corner  string           op   output =  op def library  apply op helper          string  images=images  size=size  align corners=align corner                        name=name     result =  output      if  execute must record gradient         attrs =  string   op  get attr type string   string                 op  get attr bool string        input flat =  op input      execute record gradient          string   input flat   attrs   result     result  =  result   return  result 
 tf export v1= string   def resize image with pad v1 image                               target height                               target width                               method=resizemethodv1 bilinear                               align corners=false     string    def  resize fn im  new size       return resize image im  new size  method  align corners=align corner     return  resize image with pad common image  target height  target width                                          resize fn  
 tf export v1= string    deprecation deprecate      date=none      instructions=string     string  def sample distort bound box image size                                    bound box                                    seed=none                                    seed2=none                                    min object covered=0 1                                    aspect ratio range=none                                    area range=none                                    max attempts=none                                    use image if no bound boxes=none                                    name=none     string   with ops name scope name  string       return gen image ops sample distort bound box v2          image size          bound box          seed=seed          seed2=seed2          min object covered=min object cover          aspect ratio range=aspect ratio range          area range=area range          max attempts=max attempt          use image if no bound boxes=use image if no bound box          name=name  
 tf export v1= string   def he normal seed=none     string   return variancescaling        scale=2   mode=string  distribution=string  seed=seed  
 tf export v1= string   def he uniform seed=none     string   return variancescaling        scale=2   mode=string  distribution=string  seed=seed  
 tf export v1= string   def lecun normal seed=none     string   return variancescaling        scale=1   mode=string  distribution=string  seed=seed  
 tf export v1= string   def lecun uniform seed=none     string   return variancescaling        scale=1   mode=string  distribution=string  seed=seed  
class tfrecordcompressiontype object     string   none = 0   zlib = 1   gzip = 2 
 keras export v1= string   def get session op input list=       string   session =  get session op input list    if not  manual var init      with session graph as default           initialize variables session    return session 
class name scope v1 object       string     property   def name self       return self  name    def   init   self  name  default name=none  values=none       string     self  name scope = name scope name  default name  value      self  name = default name if name be none else name    def   enter   self       return self  name scope   enter        def   exit   self   exc info       return self  name scope   exit    exc info  
 keras export v1= string   def set session session     string   global  session    session session = session 
class tensorboard callbacks callback        string        def   init   self                 log dir=string                 histogram freq=0                 batch size=32                 write graph=true                 write grads=false                 write images=false                 embeddings freq=0                 embeddings layer names=none                 embeddings metadata=none                 embeddings data=none                 update freq=string                 profile batch=2       super tensorboard  self    init         self log dir = log dir     self histogram freq = histogram freq     if self histogram freq and context execute eagerly          log warn            userwarning string                       string         self histogram freq = 0     self merge = none     self write graph = write graph     self write grads = write grads     self write image = write image     self batch size = batch size     self  current batch = 0     self  total batch see = 0     self  total val batch see = 0     self embeddings freq = embeddings freq     self embeddings layer name = embeddings layer name     self embeddings metadata = embeddings metadata     self embeddings data = embeddings data     if update freq == string        self update freq = 1     else        self update freq = update freq     self  sample see = 0     self  sample see at last write = 0          self  profile batch = profile batch          self  be profile = false                self  chief worker only = true    def  init writer self  model       string     if context execute eagerly          self writer = summary ops v2 create file writer self log dir        if not model run eagerly and self write graph          with self writer as default              summary ops v2 graph k get graph    step=0      elif self write graph        self writer = tf summary filewriter self log dir  k get graph        else        self writer = tf summary filewriter self log dir     def  make histogram ops self  model       string          if self histogram freq and self merge be none        for layer in self model layer          for weight in layer weight            map weight name = weight name replace string  string            tf summary histogram map weight name  weight            if self write image              w img = array ops squeeze weight              shape = k int shape w img              if len shape  == 2                  if shape 0  > shape 1                   w img = array ops transpose w img                  shape = k int shape w img                w img = array ops reshape w img   1  shape 0   shape 1   1               elif len shape  == 3                  if k image data format   == string                                                    w img = array ops transpose w img  perm= 2  0  1                   shape = k int shape w img                w img = array ops reshape w img                                           shape 0   shape 1   shape 2   1               elif len shape  == 1                  w img = array ops reshape w img   1  shape 0   1  1               else                               continue              shape = k int shape w img              assert len shape  == 4 and shape -1  in  1  3  4              tf summary image map weight name  w img           if self write grads            for weight in layer trainable weight              map weight name = weight name replace string  string              grads = model optimizer get gradients model total loss  weight               def be index slice grad                 return type grad    name   == string              grads =                   grad value if be index slice grad  else grad                 for grad in grads                           tf summary histogram string format map weight name   grads           if hasattr layer  string             if isinstance layer output  list               for i  output in enumerate layer output                 tf summary histogram string format layer name  i   output            else              tf summary histogram string format layer name   layer output     def set model self  model       string      self model = model     self  init writer model           if not context execute eagerly          self  make histogram ops model        self merge = tf summary merge all                  if self embeddings freq and self embeddings data be not none               from tensorflow python keras engine import train utils         self embeddings data = train utils standardize input data            self embeddings data  model input name                       embeddings layer name = self embeddings layer name       if not embeddings layer name          embeddings layer name =               layer name             for layer in self model layer             if type layer    name   == string                  self assign embeddings =          embeddings vars =           self batch id = batch id = array ops placeholder dtypes int32        self step = step = array ops placeholder dtypes int32         for layer in self model layer          if layer name in embeddings layer name            embed input = self model get layer layer name  output           embed size = np prod embed input shape 1              embed input = array ops reshape embed input                                                 step  int embed size              shape =  self embeddings data 0  shape 0   int embed size             embed = variables variable                array ops zero shape   name=layer name   string            embeddings vars layer name  = embed           batch = state ops assign embed batch id batch id   step                                      embed input            self assign embeddings append batch         self saver = saver saver list embeddings vars value                   if isinstance self embeddings metadata  str           embeddings metadata =               layer name  self embeddings metadata             for layer name in embeddings vars key                   else                   embeddings metadata = self embeddings metadata        try          from tensorboard plugins import projector       except importerror          raise importerror string                           string                              config = projector projectorconfig         for layer name  tensor in embeddings vars items            embed = config embeddings add           embed tensor name = tensor name          if  embeddings metadata be not none and             layer name in embeddings metadata             embed metadata path = embeddings metadata layer name         projector visualize embeddings self writer  config     def  fetch callback self  summary       self writer add summary summary  self  total val batch see      self  total val batch see  = 1    def  write custom summaries self  step  logs=none       string     log = log or        if context execute eagerly                 with self writer as default    summary ops v2 always record summaries            for name  value in log items              if isinstance value  np ndarray               value = value item             summary ops v2 scalar name  value  step=step      else               for name  value in log items            if isinstance value  np ndarray             value = value item           summary = tf summary summary           summary value = summary value add           summary value simple value = value         summary value tag = name         self writer add summary summary  step      self writer flush      def on batch end self  batch  logs=none       string          log = log or        self  sample see  = log get string  1      sample see since = self  sample see - self  sample see at last write     if self update freq  = string and sample see since >= self update freq        batch log =   string   k   v                     for k  v in log items                       if k not in  string  string  string         self  write custom summaries self  total batch see  batch log        self  sample see at last write = self  sample see     self  total batch see  = 1     if self  be profile        profiler save self log dir  profiler stop          self  be profile = false     elif  not self  be profile and           self  total batch see == self  profile batch - 1         profiler start         self  be profile = true    def on train begin self  logs=none       if self  profile batch == 1        profiler start         self  be profile = true    def on epoch begin self  epoch  logs=none       string           if self histogram freq and epoch   self histogram freq == 0        self  epoch = epoch                     self model  make test function         if self merge not in self model test function fetch          self model test function fetch append self merge          self model test function fetch callbacks              self merge  = self  fetch callback           def on epoch end self  epoch  logs=none       string                log =   string   k   v             for k  v in log items               if k not in  string  string  string       if self update freq == string        step = epoch     else        step = self  sample see     self  write custom summaries step  log            if self histogram freq               if self merge in self model test function fetch          self model test function fetch remove self merge        if self merge in self model test function fetch callbacks          self model test function fetch callbacks pop self merge              if self embeddings data be none and self embeddings freq        raise valueerror string                        string       if self embeddings freq and self embeddings data be not none        if epoch   self embeddings freq == 0                                                                 embeddings data = self embeddings data         n sample = embeddings data 0  shape 0          i = 0         sess = k get session           while i < n sample            step = min self batch size  n sample - i            batch = slice i  i   step             if isinstance self model input  list               fee dict =                   model input  embeddings data idx  batch                  for idx  model input in enumerate self model input                          else              fee dict =  self model input  embeddings data 0  batch              fee dict update  self batch id  i  self step  step              if not isinstance k learn phase    int               fee dict k learn phase    = false            sess run self assign embeddings  fee dict=feed dict            self saver save sess                            os path join self log dir  string                             epoch             i  = self batch size    def on train end self  logs=none       if self  be profile        profiler save self log dir  profiler stop          self  be profile = false     self writer close   
 keras export v1= string   def model to estimator      keras model=none      keras model path=none      custom objects=none      model dir=none      config=none      checkpoint format=string     string   try      from tensorflow estimator python estimator import keras as keras lib     except importerror      raise notimplementederror          string         string     model to estiamtor usage gauge get cell string  set true    return keras lib model to estimator          keras model=keras model        keras model path=keras model path        custom objects=custom object        model dir=model dir        config=config        checkpoint format=checkpoint format        use v2 estimator=false  
class constant initializer     string     deprecate args none                     string                    string  string     deprecate args none  string                    string  string    def   init   self  value=0  dtype=dtypes float32  verify shape=false       if not  np isscalar value  or isinstance value   list  tuple  np ndarray           raise typeerror            string           string   type value        self value = value     self dtype = dtypes as dtype dtype      self  verify shape = verify shape    def   call   self  shape  dtype=none  partition info=none  verify shape=none       if dtype be none        dtype = self dtype     if verify shape be none        verify shape = self  verify shape     return constant op constant v1          self value  dtype=dtype  shape=shape  verify shape=verify shape     def get config self                           return  string  self value  string  self dtype name  
class identity initializer     string     deprecate args none                     string                    string  string    def   init   self  gain=1 0  dtype=dtypes float32       self gain = gain     self dtype =  assert float dtype dtypes as dtype dtype      def   call   self  shape  dtype=none  partition info=none       full shape = shape if partition info be none else partition info full shape     if len full shape   = 2        raise valueerror            string      if dtype be none        dtype = self dtype     if isinstance full shape  tensor shape tensorshape         full shape = full shape as list       initializer = linalg ops impl eye  full shape  dtype=dtype      if partition info be not none        initializer = array ops slice initializer  partition info var offset                                      shape      return self gain   initializer    def get config self       return  string  self gain  string  self dtype name  
class ones initializer     string     deprecate args none                     string                    string  string    def   init   self  dtype=dtypes float32       self dtype = dtypes as dtype dtype     def   call   self  shape  dtype=none  partition info=none       if dtype be none        dtype = self dtype     return array ops ones shape  dtype     def get config self       return  string  self dtype name  
class orthogonal initializer     string     deprecate args none                     string                    string  string    def   init   self  gain=1 0  seed=none  dtype=dtypes float32       self gain = gain     self dtype =  assert float dtype dtypes as dtype dtype       self seed = seed    def   call   self  shape  dtype=none  partition info=none       if dtype be none        dtype = self dtype          if len shape  < 2        raise valueerror string                        string                num row = 1     for dim in shape  -1         num row  = dim     num row = int num row      num cols = int shape -1       if num row < num cols        flat shape =  num cols  num row      else        flat shape =  num row  num cols            a = random ops random normal flat shape  dtype=dtype  seed=self seed           q  r = gen linalg ops qr a  full matrices=false           d = array ops diag part r      q  = math ops sign d      if num row < num cols        q = array ops matrix transpose q      return self gain   array ops reshape q  shape     def get config self       return  string  self gain  string  self seed  string  self dtype name  
class randomnormal tfrandomnormal     string    def   init   self  mean=0 0  stddev=0 05  seed=none  dtype=dtypes float32       super randomnormal  self    init            mean=mean  stddev=stddev  seed=seed  dtype=dtype  
class randomuniform tfrandomuniform     string    def   init   self  minval=-0 05  maxval=0 05  seed=none                 dtype=dtypes float32       super randomuniform  self    init            minval=minval  maxval=maxval  seed=seed  dtype=dtype  
class truncatednormal tftruncatednormal     string    def   init   self  mean=0 0  stddev=0 05  seed=none  dtype=dtypes float32       super truncatednormal  self    init            mean=mean  stddev=stddev  seed=seed  dtype=dtype  
class variancescaling initializer     string     deprecate args none                     string                    string  string     deprecate arg value        none        string        distribution=string    def   init   self                 scale=1 0                 mode=string                 distribution=string                 seed=none                 dtype=dtypes float32       if scale <= 0         raise valueerror string      if mode not in  string  string  string         raise valueerror string  mode      distribution = distribution lower       if distribution not in           string  string  string  string              raise valueerror string  distribution      self scale = scale     self mode = mode     self distribution = distribution     self seed = seed     self dtype =  assert float dtype dtypes as dtype dtype      def   call   self  shape  dtype=none  partition info=none       if dtype be none        dtype = self dtype     scale = self scale     scale shape = shape     if partition info be not none        scale shape = partition info full shape     fan in  fan out =  compute fan scale shape      if self mode == string        scale /= max 1   fan in      elif self mode == string        scale /= max 1   fan out      else        scale /= max 1    fan in   fan out  / 2       if self distribution == string or self distribution == string               stddev = math sqrt scale  /  87962566103423978       return random ops truncate normal            shape  0 0  stddev  dtype  seed=self seed      elif self distribution == string        stddev = math sqrt scale        return random ops random normal shape  0 0  stddev  dtype  seed=self seed      else        limit = math sqrt 3 0   scale        return random ops random uniform            shape  -limit  limit  dtype  seed=self seed     def get config self       return           string  self scale          string  self mode          string  self distribution          string  self seed          string  self dtype name       
class zero initializer     string     deprecate args none                     string                    string  string    def   init   self  dtype=dtypes float32       self dtype = dtypes as dtype dtype     def   call   self  shape  dtype=none  partition info=none       if dtype be none        dtype = self dtype     return array ops zero shape  dtype     def get config self       return  string  self dtype name  
class glorotnormal variancescaling     string     deprecate args none                     string                    string  string    def   init   self  seed=none  dtype=dtypes float32       super glorotnormal  self    init            scale=1 0  mode=string  distribution=string  seed=seed     def get config self       return  string  self seed  string  self dtype name  
class glorotuniform variancescaling     string     deprecate args none                     string                    string  string    def   init   self  seed=none  dtype=dtypes float32       super glorotuniform  self    init            scale=1 0  mode=string  distribution=string  seed=seed     def get config self       return  string  self seed  string  self dtype name  
 tf export v1= string   def he normal seed=none     string   return variancescaling        scale=2   mode=string  distribution=string  seed=seed  
 tf export v1= string   def he uniform seed=none     string   return variancescaling        scale=2   mode=string  distribution=string  seed=seed  
 tf export v1= string   def lecun normal seed=none     string   return variancescaling        scale=1   mode=string  distribution=string  seed=seed  
 tf export v1= string   def lecun uniform seed=none     string   return variancescaling        scale=1   mode=string  distribution=string  seed=seed  
class batchnormalization batchnormalizationbase        doc   = replace in base docstring          string          string           string  string        use v2 behavior = false 
class cudnngru  cudnnrnn     string    def   init   self                 units                 kernel initializer=string                 recurrent initializer=string                 bias initializer=string                 kernel regularizer=none                 recurrent regularizer=none                 bias regularizer=none                 activity regularizer=none                 kernel constraint=none                 recurrent constraint=none                 bias constraint=none                 return sequences=false                 return state=false                 go backwards=false                 stateful=false                   kwargs       self units = units     cell spec = collections namedtuple string  string      self  cell = cell spec state size=self units      super cudnngru  self    init            return sequences=return sequence          return state=return state          go backwards=go backwards          stateful=stateful            kwargs       self kernel initializer = initializers get kernel initializer      self recurrent initializer = initializers get recurrent initializer      self bias initializer = initializers get bias initializer       self kernel regularizer = regularizers get kernel regularizer      self recurrent regularizer = regularizers get recurrent regularizer      self bias regularizer = regularizers get bias regularizer      self activity regularizer = regularizers get activity regularizer       self kernel constraint = constraints get kernel constraint      self recurrent constraint = constraints get recurrent constraint      self bias constraint = constraints get bias constraint      property   def cell self       return self  cell    def build self  input shape       super cudnngru  self  build input shape      if isinstance input shape  list         input shape = input shape 0      input dim = int input shape -1        self kernel = self add weight          shape= input dim  self units   3           name=string          initializer=self kernel initializer          regularizer=self kernel regularizer          constraint=self kernel constraint       self recurrent kernel = self add weight          shape= self units  self units   3           name=string          initializer=self recurrent initializer          regularizer=self recurrent regularizer          constraint=self recurrent constraint       self bias = self add weight          shape= self units   6            name=string          initializer=self bias initializer          regularizer=self bias regularizer          constraint=self bias constraint       self build = true    def  process batch self  input  initial state       if not self time major        input = array ops transpose input  perm= 1  0  2       input h = initial state 0      input h = array ops expand dim input h  axis=0       params = recurrent v2  canonical to params              weights=              self kernel    self units self units   2               self kernel     self units               self kernel    self units   2                self recurrent kernel    self units self units   2               self recurrent kernel     self units               self recurrent kernel    self units   2                       biases=              self bias self units self units   2               self bias  self units               self bias self units   2 self units   3               self bias self units   4 self units   5               self bias self units   3 self units   4               self bias self units   5                       shape=self  vector shape       args =           string  input          string  input h          string  0          string  params          string  true          string  string             output  h          = gen cudnn rnn ops cudnn rnnv2   args       if self stateful or self return state        h = h 0      if self return sequence        if self time major          output = output       else          output = array ops transpose output  perm= 1  0  2       else        output = output -1      return output   h     def get config self       config =           string  self units          string  initializers serialize self kernel initializer           string              initializers serialize self recurrent initializer           string  initializers serialize self bias initializer           string  regularizers serialize self kernel regularizer           string              regularizers serialize self recurrent regularizer           string  regularizers serialize self bias regularizer           string              regularizers serialize self activity regularizer           string  constraints serialize self kernel constraint           string              constraints serialize self recurrent constraint           string  constraints serialize self bias constraint            base config = super cudnngru  self  get config       return dict list base config items      list config items     
class cudnnlstm  cudnnrnn     string    def   init   self                 units                 kernel initializer=string                 recurrent initializer=string                 bias initializer=string                 unit forget bias=true                 kernel regularizer=none                 recurrent regularizer=none                 bias regularizer=none                 activity regularizer=none                 kernel constraint=none                 recurrent constraint=none                 bias constraint=none                 return sequences=false                 return state=false                 go backwards=false                 stateful=false                   kwargs       self units = units     cell spec = collections namedtuple string  string      self  cell = cell spec state size= self units  self units       super cudnnlstm  self    init            return sequences=return sequence          return state=return state          go backwards=go backwards          stateful=stateful            kwargs       self kernel initializer = initializers get kernel initializer      self recurrent initializer = initializers get recurrent initializer      self bias initializer = initializers get bias initializer      self unit forget bias = unit forget bias      self kernel regularizer = regularizers get kernel regularizer      self recurrent regularizer = regularizers get recurrent regularizer      self bias regularizer = regularizers get bias regularizer      self activity regularizer = regularizers get activity regularizer       self kernel constraint = constraints get kernel constraint      self recurrent constraint = constraints get recurrent constraint      self bias constraint = constraints get bias constraint      property   def cell self       return self  cell    def build self  input shape       super cudnnlstm  self  build input shape      if isinstance input shape  list         input shape = input shape 0      input dim = int input shape -1        self kernel = self add weight          shape= input dim  self units   4           name=string          initializer=self kernel initializer          regularizer=self kernel regularizer          constraint=self kernel constraint       self recurrent kernel = self add weight          shape= self units  self units   4           name=string          initializer=self recurrent initializer          regularizer=self recurrent regularizer          constraint=self recurrent constraint       if self unit forget bias         def bias initializer     args    kwargs           return array ops concat               self bias initializer  self units   5     args    kwargs               initializers ones    self units     args    kwargs               self bias initializer  self units   2     args    kwargs              axis=0      else        bias initializer = self bias initializer     self bias = self add weight          shape= self units   8            name=string          initializer=bias initializer          regularizer=self bias regularizer          constraint=self bias constraint       self build = true    def  process batch self  input  initial state       if not self time major        input = array ops transpose input  perm= 1  0  2       input h = initial state 0      input c = initial state 1      input h = array ops expand dim input h  axis=0      input c = array ops expand dim input c  axis=0       params = recurrent v2  canonical to params              weights=              self kernel     self units               self kernel    self units self units   2               self kernel    self units   2 self units   3               self kernel    self units   3                self recurrent kernel     self units               self recurrent kernel    self units self units   2               self recurrent kernel    self units   2 self units   3               self recurrent kernel    self units   3                       biases=              self bias  self units               self bias self units self units   2               self bias self units   2 self units   3               self bias self units   3 self units   4               self bias self units   4 self units   5               self bias self units   5 self units   6               self bias self units   6 self units   7               self bias self units   7                       shape=self  vector shape       args =           string  input          string  input h          string  input c          string  params          string  true             output  h  c       = gen cudnn rnn ops cudnn rnnv2   args       if self stateful or self return state        h = h 0        c = c 0      if self return sequence        if self time major          output = output       else          output = array ops transpose output  perm= 1  0  2       else        output = output -1      return output   h  c     def get config self       config =           string  self units          string  initializers serialize self kernel initializer           string              initializers serialize self recurrent initializer           string  initializers serialize self bias initializer           string  self unit forget bias          string  regularizers serialize self kernel regularizer           string              regularizers serialize self recurrent regularizer           string  regularizers serialize self bias regularizer           string              regularizers serialize self activity regularizer           string  constraints serialize self kernel constraint           string              constraints serialize self recurrent constraint           string  constraints serialize self bias constraint            base config = super cudnnlstm  self  get config       return dict list base config items      list config items     
class densefeatures fc  basefeatureslayer       string    def   init   self  feature columns  trainable=true  name=none    kwargs       string     super densefeatures  self    init            feature columns=feature columns          trainable=trainable          name=name          expect column type=fc densecolumn            kwargs      property   def  be feature layer self       return true     property   def  track metadata self       string     metadata = json load super densefeatures  self   track metadata      metadata string  = true     return json dump metadata  default=serialization get json type     def  target shape self  input shape  total elements       return  input shape 0   total elements     def call self  feature  cols to output tensors=none       string     if not isinstance feature  dict         raise valueerror string                         feature      transformation cache = fc featuretransformationcache feature      output tensors =        for column in self  feature columns        with ops name scope column name           tensor = column get dense tensor transformation cache                                           self  state manager          process tensors = self  process dense tensor column  tensor          if cols to output tensors be not none            cols to output tensors column  = process tensors         output tensors append process tensors      return self  verify and concat tensors output tensors  
class gru rnn     string    def   init   self                 units                 activation=string                 recurrent activation=string                 use bias=true                 kernel initializer=string                 recurrent initializer=string                 bias initializer=string                 kernel regularizer=none                 recurrent regularizer=none                 bias regularizer=none                 activity regularizer=none                 kernel constraint=none                 recurrent constraint=none                 bias constraint=none                 dropout=0                  recurrent dropout=0                  implementation=1                 return sequences=false                 return state=false                 go backwards=false                 stateful=false                 unroll=false                 reset after=false                   kwargs       if implementation == 0        log warn string                       string                       string      cell = grucell          units          activation=activation          recurrent activation=recurrent activation          use bias=use bias          kernel initializer=kernel initializer          recurrent initializer=recurrent initializer          bias initializer=bias initializer          kernel regularizer=kernel regularizer          recurrent regularizer=recurrent regularizer          bias regularizer=bias regularizer          kernel constraint=kernel constraint          recurrent constraint=recurrent constraint          bias constraint=bias constraint          dropout=dropout          recurrent dropout=recurrent dropout          implementation=implementation          reset after=reset after          dtype=kwargs get string           trainable=kwargs get string  true       super gru  self    init            cell          return sequences=return sequence          return state=return state          go backwards=go backwards          stateful=stateful          unroll=unroll            kwargs      self activity regularizer = regularizers get activity regularizer      self input spec =  inputspec ndim=3      def call self  input  mask=none  training=none  initial state=none       self  maybe reset cell dropout mask self cell      return super gru  self  call          input  mask=mask  training=training  initial state=initial state      property   def units self       return self cell units     property   def activation self       return self cell activation     property   def recurrent activation self       return self cell recurrent activation     property   def use bias self       return self cell use bias     property   def kernel initializer self       return self cell kernel initializer     property   def recurrent initializer self       return self cell recurrent initializer     property   def bias initializer self       return self cell bias initializer     property   def kernel regularizer self       return self cell kernel regularizer     property   def recurrent regularizer self       return self cell recurrent regularizer     property   def bias regularizer self       return self cell bias regularizer     property   def kernel constraint self       return self cell kernel constraint     property   def recurrent constraint self       return self cell recurrent constraint     property   def bias constraint self       return self cell bias constraint     property   def dropout self       return self cell dropout     property   def recurrent dropout self       return self cell recurrent dropout     property   def implementation self       return self cell implementation     property   def reset after self       return self cell reset after    def get config self       config =           string              self units          string              activations serialize self activation           string              activations serialize self recurrent activation           string              self use bias          string              initializers serialize self kernel initializer           string              initializers serialize self recurrent initializer           string              initializers serialize self bias initializer           string              regularizers serialize self kernel regularizer           string              regularizers serialize self recurrent regularizer           string              regularizers serialize self bias regularizer           string              regularizers serialize self activity regularizer           string              constraints serialize self kernel constraint           string              constraints serialize self recurrent constraint           string              constraints serialize self bias constraint           string              self dropout          string              self recurrent dropout          string              self implementation          string              self reset after           base config = super gru  self  get config       del base config string      return dict list base config items      list config items         classmethod   def from config cls  config       if string in config and config string  == 0        config string  = 1     return cls   config  
class grucell dropoutrnncellmixin  layer     string    def   init   self                 units                 activation=string                 recurrent activation=string                 use bias=true                 kernel initializer=string                 recurrent initializer=string                 bias initializer=string                 kernel regularizer=none                 recurrent regularizer=none                 bias regularizer=none                 kernel constraint=none                 recurrent constraint=none                 bias constraint=none                 dropout=0                  recurrent dropout=0                  implementation=1                 reset after=false                   kwargs       self  enable cache device = kwargs pop string  false      super grucell  self    init     kwargs      self units = units     self activation = activations get activation      self recurrent activation = activations get recurrent activation      self use bias = use bias      self kernel initializer = initializers get kernel initializer      self recurrent initializer = initializers get recurrent initializer      self bias initializer = initializers get bias initializer       self kernel regularizer = regularizers get kernel regularizer      self recurrent regularizer = regularizers get recurrent regularizer      self bias regularizer = regularizers get bias regularizer       self kernel constraint = constraints get kernel constraint      self recurrent constraint = constraints get recurrent constraint      self bias constraint = constraints get bias constraint       self dropout = min 1   max 0   dropout       self recurrent dropout = min 1   max 0   recurrent dropout       if self recurrent dropout  = 0 and implementation  = 1        log debug recurrent dropout warn msg        self implementation = 1     else        self implementation = implementation     self reset after = reset after     self state size = self units     self output size = self units     tf utils shape type conversion   def build self  input shape       input dim = input shape -1      default cache device =  cache device self      self kernel = self add weight          shape= input dim  self units   3           name=string          initializer=self kernel initializer          regularizer=self kernel regularizer          constraint=self kernel constraint          cache device=default cache device      self recurrent kernel = self add weight          shape= self units  self units   3           name=string          initializer=self recurrent initializer          regularizer=self recurrent regularizer          constraint=self recurrent constraint          cache device=default cache device       if self use bias        if not self reset after          bias shape =  3   self units         else                                              bias shape =  2  3   self units        self bias = self add weight shape=bias shape                                    name=string                                    initializer=self bias initializer                                    regularizer=self bias regularizer                                    constraint=self bias constraint                                    cache device=default cache device      else        self bias = none     self build = true    def call self  input  state  training=none       h tm1 = state 0         dp mask = self get dropout mask for cell input  train  count=3      rec dp mask = self get recurrent dropout mask for cell          h tm1  train  count=3       if self use bias        if not self reset after          input bias  recurrent bias = self bias  none       else          input bias  recurrent bias = array ops unstack self bias       if self implementation == 1        if 0  < self dropout < 1           input z = input   dp mask 0          input r = input   dp mask 1          input h = input   dp mask 2        else          input z = input         input r = input         input h = input        x z = k dot input z  self kernel     self units         x r = k dot input r  self kernel    self units self units   2         x h = k dot input h  self kernel    self units   2           if self use bias          x z = k bias add x z  input bias  self units           x r = k bias add x r  input bias self units  self units   2           x h = k bias add x h  input bias self units   2           if 0  < self recurrent dropout < 1           h tm1 z = h tm1   rec dp mask 0          h tm1 r = h tm1   rec dp mask 1          h tm1 h = h tm1   rec dp mask 2        else          h tm1 z = h tm1         h tm1 r = h tm1         h tm1 h = h tm1        recurrent z = k dot h tm1 z  self recurrent kernel     self units         recurrent r = k dot h tm1 r                            self recurrent kernel    self units self units   2         if self reset after and self use bias          recurrent z = k bias add recurrent z  recurrent bias  self units           recurrent r = k bias add recurrent r                                   recurrent bias self units self units   2          z = self recurrent activation x z   recurrent z        r = self recurrent activation x r   recurrent r                if self reset after          recurrent h = k dot h tm1 h  self recurrent kernel    self units   2            if self use bias            recurrent h = k bias add recurrent h  recurrent bias self units   2            recurrent h = r   recurrent h       else          recurrent h = k dot r   h tm1 h                              self recurrent kernel    self units   2           hh = self activation x h   recurrent h      else        if 0  < self dropout < 1           input = input   dp mask 0                matrix x = k dot input  self kernel        if self use bias                   matrix x = k bias add matrix x  input bias         x z  x r  x h = array ops split matrix x  3  axis=-1         if self reset after                   matrix inner = k dot h tm1  self recurrent kernel          if self use bias            matrix inner = k bias add matrix inner  recurrent bias        else                   matrix inner = k dot h tm1  self recurrent kernel     2   self units          recurrent z  recurrent r  recurrent h = array ops split            matrix inner   self units  self units  -1   axis=-1         z = self recurrent activation x z   recurrent z        r = self recurrent activation x r   recurrent r         if self reset after          recurrent h = r   recurrent h       else          recurrent h = k dot r   h tm1                              self recurrent kernel    2   self units           hh = self activation x h   recurrent h           h = z   h tm1    1 - z    hh     return h   h     def get config self       config =           string  self units          string  activations serialize self activation           string              activations serialize self recurrent activation           string  self use bias          string  initializers serialize self kernel initializer           string              initializers serialize self recurrent initializer           string  initializers serialize self bias initializer           string  regularizers serialize self kernel regularizer           string              regularizers serialize self recurrent regularizer           string  regularizers serialize self bias regularizer           string  constraints serialize self kernel constraint           string              constraints serialize self recurrent constraint           string  constraints serialize self bias constraint           string  self dropout          string  self recurrent dropout          string  self implementation          string  self reset after           base config = super grucell  self  get config       return dict list base config items      list config items        def get initial state self  inputs=none  batch size=none  dtype=none       return  generate zero fill state for cell self  input  batch size  dtype  
class lstm rnn     string    def   init   self                 units                 activation=string                 recurrent activation=string                 use bias=true                 kernel initializer=string                 recurrent initializer=string                 bias initializer=string                 unit forget bias=true                 kernel regularizer=none                 recurrent regularizer=none                 bias regularizer=none                 activity regularizer=none                 kernel constraint=none                 recurrent constraint=none                 bias constraint=none                 dropout=0                  recurrent dropout=0                  implementation=1                 return sequences=false                 return state=false                 go backwards=false                 stateful=false                 unroll=false                   kwargs       if implementation == 0        log warn string                       string                       string      cell = lstmcell          units          activation=activation          recurrent activation=recurrent activation          use bias=use bias          kernel initializer=kernel initializer          recurrent initializer=recurrent initializer          unit forget bias=unit forget bias          bias initializer=bias initializer          kernel regularizer=kernel regularizer          recurrent regularizer=recurrent regularizer          bias regularizer=bias regularizer          kernel constraint=kernel constraint          recurrent constraint=recurrent constraint          bias constraint=bias constraint          dropout=dropout          recurrent dropout=recurrent dropout          implementation=implementation          dtype=kwargs get string           trainable=kwargs get string  true       super lstm  self    init            cell          return sequences=return sequence          return state=return state          go backwards=go backwards          stateful=stateful          unroll=unroll            kwargs      self activity regularizer = regularizers get activity regularizer      self input spec =  inputspec ndim=3      def call self  input  mask=none  training=none  initial state=none       self  maybe reset cell dropout mask self cell      return super lstm  self  call          input  mask=mask  training=training  initial state=initial state      property   def units self       return self cell units     property   def activation self       return self cell activation     property   def recurrent activation self       return self cell recurrent activation     property   def use bias self       return self cell use bias     property   def kernel initializer self       return self cell kernel initializer     property   def recurrent initializer self       return self cell recurrent initializer     property   def bias initializer self       return self cell bias initializer     property   def unit forget bias self       return self cell unit forget bias     property   def kernel regularizer self       return self cell kernel regularizer     property   def recurrent regularizer self       return self cell recurrent regularizer     property   def bias regularizer self       return self cell bias regularizer     property   def kernel constraint self       return self cell kernel constraint     property   def recurrent constraint self       return self cell recurrent constraint     property   def bias constraint self       return self cell bias constraint     property   def dropout self       return self cell dropout     property   def recurrent dropout self       return self cell recurrent dropout     property   def implementation self       return self cell implementation    def get config self       config =           string              self units          string              activations serialize self activation           string              activations serialize self recurrent activation           string              self use bias          string              initializers serialize self kernel initializer           string              initializers serialize self recurrent initializer           string              initializers serialize self bias initializer           string              self unit forget bias          string              regularizers serialize self kernel regularizer           string              regularizers serialize self recurrent regularizer           string              regularizers serialize self bias regularizer           string              regularizers serialize self activity regularizer           string              constraints serialize self kernel constraint           string              constraints serialize self recurrent constraint           string              constraints serialize self bias constraint           string              self dropout          string              self recurrent dropout          string              self implementation           base config = super lstm  self  get config       del base config string      return dict list base config items      list config items         classmethod   def from config cls  config       if string in config and config string  == 0        config string  = 1     return cls   config  
class lstmcell dropoutrnncellmixin  layer     string    def   init   self                 units                 activation=string                 recurrent activation=string                 use bias=true                 kernel initializer=string                 recurrent initializer=string                 bias initializer=string                 unit forget bias=true                 kernel regularizer=none                 recurrent regularizer=none                 bias regularizer=none                 kernel constraint=none                 recurrent constraint=none                 bias constraint=none                 dropout=0                  recurrent dropout=0                  implementation=1                   kwargs       self  enable cache device = kwargs pop string  false      super lstmcell  self    init     kwargs      self units = units     self activation = activations get activation      self recurrent activation = activations get recurrent activation      self use bias = use bias      self kernel initializer = initializers get kernel initializer      self recurrent initializer = initializers get recurrent initializer      self bias initializer = initializers get bias initializer      self unit forget bias = unit forget bias      self kernel regularizer = regularizers get kernel regularizer      self recurrent regularizer = regularizers get recurrent regularizer      self bias regularizer = regularizers get bias regularizer       self kernel constraint = constraints get kernel constraint      self recurrent constraint = constraints get recurrent constraint      self bias constraint = constraints get bias constraint       self dropout = min 1   max 0   dropout       self recurrent dropout = min 1   max 0   recurrent dropout       if self recurrent dropout  = 0 and implementation  = 1        log debug recurrent dropout warn msg        self implementation = 1     else        self implementation = implementation                                   self state size = data structure nodependency  self units  self units       self output size = self units     tf utils shape type conversion   def build self  input shape       default cache device =  cache device self      input dim = input shape -1      self kernel = self add weight          shape= input dim  self units   4           name=string          initializer=self kernel initializer          regularizer=self kernel regularizer          constraint=self kernel constraint          cache device=default cache device      self recurrent kernel = self add weight          shape= self units  self units   4           name=string          initializer=self recurrent initializer          regularizer=self recurrent regularizer          constraint=self recurrent constraint          cache device=default cache device       if self use bias        if self unit forget bias           def bias initializer     args    kwargs             return k concatenate                 self bias initializer  self units     args    kwargs                 initializers ones    self units     args    kwargs                 self bias initializer  self units   2     args    kwargs                      else          bias initializer = self bias initializer       self bias = self add weight            shape= self units   4              name=string            initializer=bias initializer            regularizer=self bias regularizer            constraint=self bias constraint            cache device=default cache device      else        self bias = none     self build = true    def  compute carry and output self  x  h tm1  c tm1       string     x i  x f  x c  x o = x     h tm1 i  h tm1 f  h tm1 c  h tm1 o = h tm1     i = self recurrent activation          x i   k dot h tm1 i  self recurrent kernel     self units        f = self recurrent activation x f   k dot          h tm1 f  self recurrent kernel    self units self units   2        c = f   c tm1   i   self activation x c   k dot          h tm1 c  self recurrent kernel    self units   2 self units   3        o = self recurrent activation          x o   k dot h tm1 o  self recurrent kernel    self units   3         return c  o    def  compute carry and output fuse self  z  c tm1       string     z0  z1  z2  z3 = z     i = self recurrent activation z0      f = self recurrent activation z1      c = f   c tm1   i   self activation z2      o = self recurrent activation z3      return c  o    def call self  input  state  training=none       h tm1 = state 0        c tm1 = state 1         dp mask = self get dropout mask for cell input  train  count=4      rec dp mask = self get recurrent dropout mask for cell          h tm1  train  count=4       if self implementation == 1        if 0 < self dropout < 1           input i = input   dp mask 0          input f = input   dp mask 1          input c = input   dp mask 2          input o = input   dp mask 3        else          input i = input         input f = input         input c = input         input o = input       k i  k f  k c  k o = array ops split            self kernel  num or size splits=4  axis=1        x i = k dot input i  k i        x f = k dot input f  k f        x c = k dot input c  k c        x o = k dot input o  k o        if self use bias          b i  b f  b c  b o = array ops split              self bias  num or size splits=4  axis=0          x i = k bias add x i  b i          x f = k bias add x f  b f          x c = k bias add x c  b c          x o = k bias add x o  b o         if 0 < self recurrent dropout < 1           h tm1 i = h tm1   rec dp mask 0          h tm1 f = h tm1   rec dp mask 1          h tm1 c = h tm1   rec dp mask 2          h tm1 o = h tm1   rec dp mask 3        else          h tm1 i = h tm1         h tm1 f = h tm1         h tm1 c = h tm1         h tm1 o = h tm1       x =  x i  x f  x c  x o        h tm1 =  h tm1 i  h tm1 f  h tm1 c  h tm1 o        c  o = self  compute carry and output x  h tm1  c tm1      else        if 0  < self dropout < 1           input = input   dp mask 0        z = k dot input  self kernel        z  = k dot h tm1  self recurrent kernel        if self use bias          z = k bias add z  self bias         z = array ops split z  num or size splits=4  axis=1        c  o = self  compute carry and output fuse z  c tm1       h = o   self activation c      return h   h  c     def get config self       config =           string              self units          string              activations serialize self activation           string              activations serialize self recurrent activation           string              self use bias          string              initializers serialize self kernel initializer           string              initializers serialize self recurrent initializer           string              initializers serialize self bias initializer           string              self unit forget bias          string              regularizers serialize self kernel regularizer           string              regularizers serialize self recurrent regularizer           string              regularizers serialize self bias regularizer           string              constraints serialize self kernel constraint           string              constraints serialize self recurrent constraint           string              constraints serialize self bias constraint           string              self dropout          string              self recurrent dropout          string              self implementation           base config = super lstmcell  self  get config       return dict list base config items      list config items        def get initial state self  inputs=none  batch size=none  dtype=none       return list  generate zero fill state for cell          self  input  batch size  dtype   
class averagepooling1d keras layer averagepooling1d  base layer     string    def   init   self  pool size  stride                 padding=string  data format=string                 name=none    kwargs       if stride be none        raise valueerror string      super averagepooling1d  self    init            pool size=pool size          strides=strides          padding=padding          data format=data format          name=name            kwargs  
class averagepooling2d keras layer averagepooling2d  base layer     string    def   init   self  pool size  stride                 padding=string  data format=string                 name=none    kwargs       if stride be none        raise valueerror string      super averagepooling2d  self    init            pool size=pool size  strides=strides          padding=padding  data format=data format  name=name    kwargs  
class averagepooling3d keras layer averagepooling3d  base layer     string    def   init   self  pool size  stride                 padding=string  data format=string                 name=none    kwargs       if stride be none        raise valueerror string      super averagepooling3d  self    init            pool size=pool size  strides=strides          padding=padding  data format=data format  name=name    kwargs  
class batchnormalization keras normalization batchnormalization  base layer     string    def   init   self                 axis=-1                 momentum=0 99                 epsilon=1e-3                 center=true                 scale=true                 beta initializer=init ops zero initializer                   gamma initializer=init ops ones initializer                   move mean initializer=init ops zero initializer                   move variance initializer=init ops ones initializer                   beta regularizer=none                 gamma regularizer=none                 beta constraint=none                 gamma constraint=none                 renorm=false                 renorm clipping=none                 renorm momentum=0 99                 fused=none                 trainable=true                 virtual batch size=none                 adjustment=none                 name=none                   kwargs       super batchnormalization  self    init            axis=axis          momentum=momentum          epsilon=epsilon          center=center          scale=scale          beta initializer=beta initializer          gamma initializer=gamma initializer          move mean initializer=moving mean initializer          move variance initializer=moving variance initializer          beta regularizer=beta regularizer          gamma regularizer=gamma regularizer          beta constraint=beta constraint          gamma constraint=gamma constraint          renorm=renorm          renorm clipping=renorm clip          renorm momentum=renorm momentum          fused=fused          trainable=trainable          virtual batch size=virtual batch size          adjustment=adjustment          name=name            kwargs     def call self  input  training=false       return super batchnormalization  self  call input  training=training  
class conv1d keras layer conv1d  base layer     string    def   init   self  filter                 kernel size                 strides=1                 padding=string                 data format=string                 dilation rate=1                 activation=none                 use bias=true                 kernel initializer=none                 bias initializer=init ops zero initializer                   kernel regularizer=none                 bias regularizer=none                 activity regularizer=none                 kernel constraint=none                 bias constraint=none                 trainable=true                 name=none                   kwargs       super conv1d  self    init            filters=filters          kernel size=kernel size          strides=strides          padding=padding          data format=data format          dilation rate=dilation rate          activation=activation          use bias=use bias          kernel initializer=kernel initializer          bias initializer=bias initializer          kernel regularizer=kernel regularizer          bias regularizer=bias regularizer          activity regularizer=activity regularizer          kernel constraint=kernel constraint          bias constraint=bias constraint          trainable=trainable          name=name    kwargs  
class conv2d keras layer conv2d  base layer     string    def   init   self  filter                 kernel size                 strides= 1  1                  padding=string                 data format=string                 dilation rate= 1  1                  activation=none                 use bias=true                 kernel initializer=none                 bias initializer=init ops zero initializer                   kernel regularizer=none                 bias regularizer=none                 activity regularizer=none                 kernel constraint=none                 bias constraint=none                 trainable=true                 name=none                   kwargs       super conv2d  self    init            filters=filters          kernel size=kernel size          strides=strides          padding=padding          data format=data format          dilation rate=dilation rate          activation=activation          use bias=use bias          kernel initializer=kernel initializer          bias initializer=bias initializer          kernel regularizer=kernel regularizer          bias regularizer=bias regularizer          activity regularizer=activity regularizer          kernel constraint=kernel constraint          bias constraint=bias constraint          trainable=trainable          name=name    kwargs  
class conv2dtranspose keras layer conv2dtranspose  base layer     string    def   init   self  filter                 kernel size                 strides= 1  1                  padding=string                 data format=string                 activation=none                 use bias=true                 kernel initializer=none                 bias initializer=init ops zero initializer                   kernel regularizer=none                 bias regularizer=none                 activity regularizer=none                 kernel constraint=none                 bias constraint=none                 trainable=true                 name=none                   kwargs       super conv2dtranspose  self    init            filters=filters          kernel size=kernel size          strides=strides          padding=padding          data format=data format          activation=activation          use bias=use bias          kernel initializer=kernel initializer          bias initializer=bias initializer          kernel regularizer=kernel regularizer          bias regularizer=bias regularizer          activity regularizer=activity regularizer          kernel constraint=kernel constraint          bias constraint=bias constraint          trainable=trainable          name=name            kwargs  
class conv3d keras layer conv3d  base layer     string    def   init   self  filter                 kernel size                 strides= 1  1  1                  padding=string                 data format=string                 dilation rate= 1  1  1                  activation=none                 use bias=true                 kernel initializer=none                 bias initializer=init ops zero initializer                   kernel regularizer=none                 bias regularizer=none                 activity regularizer=none                 kernel constraint=none                 bias constraint=none                 trainable=true                 name=none                   kwargs       super conv3d  self    init            filters=filters          kernel size=kernel size          strides=strides          padding=padding          data format=data format          dilation rate=dilation rate          activation=activation          use bias=use bias          kernel initializer=kernel initializer          bias initializer=bias initializer          kernel regularizer=kernel regularizer          bias regularizer=bias regularizer          activity regularizer=activity regularizer          kernel constraint=kernel constraint          bias constraint=bias constraint          trainable=trainable          name=name    kwargs  
class conv3dtranspose keras layer conv3dtranspose  base layer     string    def   init   self                 filter                 kernel size                 strides= 1  1  1                  padding=string                 data format=string                 activation=none                 use bias=true                 kernel initializer=none                 bias initializer=init ops zero initializer                   kernel regularizer=none                 bias regularizer=none                 activity regularizer=none                 kernel constraint=none                 bias constraint=none                 trainable=true                 name=none                   kwargs       super conv3dtranspose  self    init            filters=filters          kernel size=kernel size          strides=strides          padding=padding          data format=data format          activation=activation          use bias=use bias          kernel initializer=kernel initializer          bias initializer=bias initializer          kernel regularizer=kernel regularizer          bias regularizer=bias regularizer          activity regularizer=activity regularizer          kernel constraint=kernel constraint          bias constraint=bias constraint          trainable=trainable          name=name            kwargs  
class dense keras layer dense  base layer     string    def   init   self  units                 activation=none                 use bias=true                 kernel initializer=none                 bias initializer=init ops zero initializer                   kernel regularizer=none                 bias regularizer=none                 activity regularizer=none                 kernel constraint=none                 bias constraint=none                 trainable=true                 name=none                   kwargs       super dense  self    init   units=units                                  activation=activation                                  use bias=use bias                                  kernel initializer=kernel initializer                                  bias initializer=bias initializer                                  kernel regularizer=kernel regularizer                                  bias regularizer=bias regularizer                                  activity regularizer=activity regularizer                                  kernel constraint=kernel constraint                                  bias constraint=bias constraint                                  trainable=trainable                                  name=name                                    kwargs  
class dropout keras layer dropout  base layer     string    def   init   self  rate=0 5                 noise shape=none                 seed=none                 name=none                   kwargs       super dropout  self    init   rate=rate                                    noise shape=noise shape                                    seed=seed                                    name=name                                      kwargs     def call self  input  training=false       return super dropout  self  call input  training=training  
class flatten keras layer flatten  base layer     string   pass 
class layer base layer layer     string    def   init   self  trainable=true  name=none  dtype=none                   kwargs                 self  use resource variables = false     scope = kwargs pop string  none      self  reuse = kwargs pop string  none            self  trainable weight =        self build = false      if dtype be none                             dtype = policy policy string       if string not in kwargs        kwargs string  = false      super layer  self    init   trainable=trainable  name=name  dtype=dtype                                    kwargs       if  be in keras style scope          if scope be not none          raise valueerror              string             string format scope         if self  reuse be not none          raise valueerror              string             string format self  reuse         self  keras style = true     else        self  keras style = false      self  call have scope arg = string in self  call fn args     if scope        with vs variable scope scope  as capture scope          self  scope = capture scope     else        self  scope = none     self  current scope = none           property    deprecation deprecate        date=none        instructions=string       string    def graph self       if context execute eagerly          raise runtimeerror string      return none    def  init set name self  name            if isinstance name  vs variablescope         base name = name name       self  name    = self  make unique name       else        base name = name       self  name = name     if not name        self  name  base name = self  make unique name       self  base name = base name    def  make unique name self  name uid map=none  avoid names=none                          namespace=string  zero based=false       base name = base layer to snake case self   class     name        name = backend unique object name          base name          name uid map=name uid map          avoid names=avoid name          namespace=namespace          zero based=zero base      return  name  base name      property   def scope name self       if not self  scope        raise valueerror string                          self  name   string                          string                          string                          string      return self  scope name    def add loss self  losses  inputs=none       previous losses length = len self  losses      previous callable losses length = len self  callable losses      super layer  self  add loss losses  inputs=inputs      if not context execute eagerly                 new losses = self  losses previous losses length         new callable losses = self  callable losses            previous callable losses length         for regularizer in new callable losses          loss tensor = regularizer           if loss tensor be not none            new losses append loss tensor         add elements to collection            new losses            ops graphkeys regularization losses     def  name scope self       string     if self  keras style        return super layer  self   name scope       return self  current scope original name scope    def  set scope self  scope=none       if self  scope be none               if self  reuse          with vs variable scope              scope if scope be not none else self  base name  as capture scope            self  scope = capture scope       else          with vs variable scope              scope  default name=self  base name  as capture scope            self  scope = capture scope    def add weight self                   name                   shape                   dtype=none                   initializer=none                   regularizer=none                   trainable=none                   constraint=none                   use resource=none                   synchronization=vs variablesynchronization auto                   aggregation=vs variableaggregation none                   partitioner=none                     kwargs       string     for kwarg in kwargs        if kwarg  = string          raise typeerror string  kwarg      if self  keras style        return super layer  self  add weight            name=name            shape=shape            dtype=dtype            initializer=initializer            regularizer=regularizer            trainable=trainable and self trainable            constraint=constraint            use resource=use resource            synchronization=vs variablesynchronization auto            aggregation=vs variableaggregation none            partitioner=partitioner              kwargs       if synchronization == vs variablesynchronization on read        if trainable          raise valueerror              string             string             string             string        else                   trainable = false     elif trainable be none        trainable = true      def  should add regularizer variable  exist variable set         if isinstance variable  tf variables partitionedvariable           for var in variable            if var in exist variable set              return false         return true       else          return variable not in exist variable set      init graph = none     if not context execute eagerly          default graph = ops get default graph         if default graph build function          with ops init scope                                                          if not context execute eagerly                init graph = ops get default graph               exist variables = set tf variables global variables          else                   init graph = default graph         exist variables = set tf variables global variables         if dtype be none        dtype = self dtype or dtypes float32      self  set scope none      reuse = self build or self  reuse     prev len trainable = len self  trainable weight      with vs variable scope          self  scope  reuse=reuse  auxiliary name scope=false  as scope        self  current scope = scope       with ops name scope self  name scope             use resource =  use resource or                         self  use resource variables or                         scope use resource          if initializer be none            initializer = scope initializer         variable = super layer  self  add weight              name              shape              dtype=dtypes as dtype dtype               initializer=initializer              trainable=trainable and self trainable              constraint=constraint              partitioner=partitioner              use resource=use resource              synchronization=synchronization              aggregation=aggregation              getter=vs get variable                kwargs           if regularizer            if  ops execute eagerly outside function                 or  should add regularizer variable  exist variables                self  handle weight regularization name  variable  regularizer           if init graph be not none                                                        with init graph as default                trainable variables = tf variables trainable variables             if  trainable and self trainable and               variable not in trainable variables                            extra trainable vars = self  trainable weight prev len trainable               self  trainable weight = self  trainable weight                   prev len trainable              self  non trainable weight  = extra trainable vars     return variable    def   call   self  input   args    kwargs       string     scope = kwargs pop string  none       if self  keras style        if scope be not none          raise valueerror              string             string format scope         return super layer  self    call   input   args    kwargs       self  set scope scope       if self build        try                            scope context manager = self  always reuse variable scope       except attributeerror                                     self  always reuse variable scope = vs variable scope              self  scope  reuse=true  auxiliary name scope=false          scope context manager = self  always reuse variable scope     else        scope context manager = vs variable scope            self  scope  reuse=self  reuse  auxiliary name scope=false       with scope context manager as scope        self  current scope = scope        try          call have scope arg = self  call have scope arg       except attributeerror          self  call fn args = function utils fn args self call          self  call have scope arg = string in self  call fn args         call have scope arg = self  call have scope arg       if call have scope arg          kwargs string  = scope               output = super layer  self    call   input   args    kwargs       if not context execute eagerly                  add elements to collection self update  ops graphkeys update ops      return output    def   deepcopy   self  memo       no copy = set  string  string       shallow copy = set  string  string       cls = self   class       result = cls   new   cls      memo id self   = result     for k  v in self   dict   items          if k in no copy          setattr result  k  v        elif k in shallow copy          setattr result  k  copy copy v         elif base layer be tensor or tensor list v           setattr result  k  v        else          setattr result  k  copy deepcopy v  memo       return result    def   setattr   self  value  name            super trackable trackable  self    setattr   value  name      property   def  be legacy layer self       string     return true 
class maxpooling1d keras layer maxpooling1d  base layer     string    def   init   self  pool size  stride                 padding=string  data format=string                 name=none    kwargs       if stride be none        raise valueerror string      super maxpooling1d  self    init            pool size=pool size          strides=strides          padding=padding          data format=data format          name=name            kwargs  
class maxpooling2d keras layer maxpooling2d  base layer     string    def   init   self  pool size  stride                 padding=string  data format=string                 name=none    kwargs       if stride be none        raise valueerror string      super maxpooling2d  self    init            pool size=pool size  strides=strides          padding=padding  data format=data format  name=name    kwargs  
class maxpooling3d keras layer maxpooling3d  base layer     string    def   init   self  pool size  stride                 padding=string  data format=string                 name=none    kwargs       if stride be none        raise valueerror string      super maxpooling3d  self    init            pool size=pool size  strides=strides          padding=padding  data format=data format  name=name    kwargs  
class separableconv1d keras layer separableconv1d  base layer     string    def   init   self  filter                 kernel size                 strides=1                 padding=string                 data format=string                 dilation rate=1                 depth multiplier=1                 activation=none                 use bias=true                 depthwise initializer=none                 pointwise initializer=none                 bias initializer=init ops zero initializer                   depthwise regularizer=none                 pointwise regularizer=none                 bias regularizer=none                 activity regularizer=none                 depthwise constraint=none                 pointwise constraint=none                 bias constraint=none                 trainable=true                 name=none                   kwargs       super separableconv1d  self    init            filters=filters          kernel size=kernel size          strides=strides          padding=padding          data format=data format          dilation rate=dilation rate          depth multiplier=depth multiplier          activation=activation          use bias=use bias          depthwise initializer=depthwise initializer          pointwise initializer=pointwise initializer          bias initializer=bias initializer          depthwise regularizer=depthwise regularizer          pointwise regularizer=pointwise regularizer          bias regularizer=bias regularizer          activity regularizer=activity regularizer          depthwise constraint=depthwise constraint          pointwise constraint=pointwise constraint          bias constraint=bias constraint          trainable=trainable          name=name            kwargs  
class separableconv2d keras layer separableconv2d  base layer     string    def   init   self  filter                 kernel size                 strides= 1  1                  padding=string                 data format=string                 dilation rate= 1  1                  depth multiplier=1                 activation=none                 use bias=true                 depthwise initializer=none                 pointwise initializer=none                 bias initializer=init ops zero initializer                   depthwise regularizer=none                 pointwise regularizer=none                 bias regularizer=none                 activity regularizer=none                 depthwise constraint=none                 pointwise constraint=none                 bias constraint=none                 trainable=true                 name=none                   kwargs       super separableconv2d  self    init            filters=filters          kernel size=kernel size          strides=strides          padding=padding          data format=data format          dilation rate=dilation rate          depth multiplier=depth multiplier          activation=activation          use bias=use bias          depthwise initializer=depthwise initializer          pointwise initializer=pointwise initializer          bias initializer=bias initializer          depthwise regularizer=depthwise regularizer          pointwise regularizer=pointwise regularizer          bias regularizer=bias regularizer          activity regularizer=activity regularizer          depthwise constraint=depthwise constraint          pointwise constraint=pointwise constraint          bias constraint=bias constraint          trainable=trainable          name=name            kwargs  
 deprecation deprecate      date=none  instructions=string   tf export v1= string   def average pooling1d input  pool size  stride                        padding=string  data format=string                        name=none     string   layer = averagepooling1d pool size=pool size                             strides=strides                             padding=padding                             data format=data format                             name=name    return layer apply input  
 deprecation deprecate      date=none  instructions=string   tf export v1= string   def average pooling2d input                        pool size  stride                        padding=string  data format=string                        name=none     string   layer = averagepooling2d pool size=pool size  strides=strides                             padding=padding  data format=data format                             name=name    return layer apply input  
 deprecation deprecate      date=none  instructions=string   tf export v1= string   def average pooling3d input                        pool size  stride                        padding=string  data format=string                        name=none     string   layer = averagepooling3d pool size=pool size  strides=strides                             padding=padding  data format=data format                             name=name    return layer apply input  
 deprecation deprecate      date=none  instructions=string     string     string     string   tf export v1= string   def batch normalization input                          axis=-1                          momentum=0 99                          epsilon=1e-3                          center=true                          scale=true                          beta initializer=init ops zero initializer                            gamma initializer=init ops ones initializer                            move mean initializer=init ops zero initializer                            move variance initializer=init ops ones initializer                            beta regularizer=none                          gamma regularizer=none                          beta constraint=none                          gamma constraint=none                          training=false                          trainable=true                          name=none                          reuse=none                          renorm=false                          renorm clipping=none                          renorm momentum=0 99                          fused=none                          virtual batch size=none                          adjustment=none     string   layer = batchnormalization        axis=axis        momentum=momentum        epsilon=epsilon        center=center        scale=scale        beta initializer=beta initializer        gamma initializer=gamma initializer        move mean initializer=moving mean initializer        move variance initializer=moving variance initializer        beta regularizer=beta regularizer        gamma regularizer=gamma regularizer        beta constraint=beta constraint        gamma constraint=gamma constraint        renorm=renorm        renorm clipping=renorm clip        renorm momentum=renorm momentum        fused=fused        trainable=trainable        virtual batch size=virtual batch size        adjustment=adjustment        name=name         reuse=reuse         scope=name    return layer apply input  training=training  
 deprecation deprecate      date=none      instructions=string   tf export v1= string   def conv1d input             filter             kernel size             strides=1             padding=string             data format=string             dilation rate=1             activation=none             use bias=true             kernel initializer=none             bias initializer=init ops zero initializer               kernel regularizer=none             bias regularizer=none             activity regularizer=none             kernel constraint=none             bias constraint=none             trainable=true             name=none             reuse=none     string   layer = conv1d        filters=filters        kernel size=kernel size        strides=strides        padding=padding        data format=data format        dilation rate=dilation rate        activation=activation        use bias=use bias        kernel initializer=kernel initializer        bias initializer=bias initializer        kernel regularizer=kernel regularizer        bias regularizer=bias regularizer        activity regularizer=activity regularizer        kernel constraint=kernel constraint        bias constraint=bias constraint        trainable=trainable        name=name         reuse=reuse         scope=name    return layer apply input  
 deprecation deprecate      date=none      instructions=string   tf export v1= string   def conv2d input             filter             kernel size             strides= 1  1              padding=string             data format=string             dilation rate= 1  1              activation=none             use bias=true             kernel initializer=none             bias initializer=init ops zero initializer               kernel regularizer=none             bias regularizer=none             activity regularizer=none             kernel constraint=none             bias constraint=none             trainable=true             name=none             reuse=none     string   layer = conv2d        filters=filters        kernel size=kernel size        strides=strides        padding=padding        data format=data format        dilation rate=dilation rate        activation=activation        use bias=use bias        kernel initializer=kernel initializer        bias initializer=bias initializer        kernel regularizer=kernel regularizer        bias regularizer=bias regularizer        activity regularizer=activity regularizer        kernel constraint=kernel constraint        bias constraint=bias constraint        trainable=trainable        name=name         reuse=reuse         scope=name    return layer apply input  
 deprecation deprecate      date=none      instructions=string   tf export v1= string   def conv2d transpose input                       filter                       kernel size                       strides= 1  1                        padding=string                       data format=string                       activation=none                       use bias=true                       kernel initializer=none                       bias initializer=init ops zero initializer                         kernel regularizer=none                       bias regularizer=none                       activity regularizer=none                       kernel constraint=none                       bias constraint=none                       trainable=true                       name=none                       reuse=none     string   layer = conv2dtranspose        filters=filters        kernel size=kernel size        strides=strides        padding=padding        data format=data format        activation=activation        use bias=use bias        kernel initializer=kernel initializer        bias initializer=bias initializer        kernel regularizer=kernel regularizer        bias regularizer=bias regularizer        activity regularizer=activity regularizer        kernel constraint=kernel constraint        bias constraint=bias constraint        trainable=trainable        name=name         reuse=reuse         scope=name    return layer apply input  
 deprecation deprecate      date=none      instructions=string   tf export v1= string   def conv3d input             filter             kernel size             strides= 1  1  1              padding=string             data format=string             dilation rate= 1  1  1              activation=none             use bias=true             kernel initializer=none             bias initializer=init ops zero initializer               kernel regularizer=none             bias regularizer=none             activity regularizer=none             kernel constraint=none             bias constraint=none             trainable=true             name=none             reuse=none     string   layer = conv3d        filters=filters        kernel size=kernel size        strides=strides        padding=padding        data format=data format        dilation rate=dilation rate        activation=activation        use bias=use bias        kernel initializer=kernel initializer        bias initializer=bias initializer        kernel regularizer=kernel regularizer        bias regularizer=bias regularizer        activity regularizer=activity regularizer        kernel constraint=kernel constraint        bias constraint=bias constraint        trainable=trainable        name=name         reuse=reuse         scope=name    return layer apply input  
 deprecation deprecate      date=none      instructions=string   tf export v1= string   def conv3d transpose input                       filter                       kernel size                       strides= 1  1  1                        padding=string                       data format=string                       activation=none                       use bias=true                       kernel initializer=none                       bias initializer=init ops zero initializer                         kernel regularizer=none                       bias regularizer=none                       activity regularizer=none                       kernel constraint=none                       bias constraint=none                       trainable=true                       name=none                       reuse=none     string   layer = conv3dtranspose        filters=filters        kernel size=kernel size        strides=strides        padding=padding        data format=data format        activation=activation        use bias=use bias        kernel initializer=kernel initializer        bias initializer=bias initializer        kernel regularizer=kernel regularizer        bias regularizer=bias regularizer        activity regularizer=activity regularizer        kernel constraint=kernel constraint        bias constraint=bias constraint        trainable=trainable        name=name         reuse=reuse         scope=name    return layer apply input  
 deprecation deprecate      date=none  instructions=string   tf export v1= string   def dense      input  units      activation=none      use bias=true      kernel initializer=none      bias initializer=init ops zero initializer        kernel regularizer=none      bias regularizer=none      activity regularizer=none      kernel constraint=none      bias constraint=none      trainable=true      name=none      reuse=none     string   layer = dense units                  activation=activation                  use bias=use bias                  kernel initializer=kernel initializer                  bias initializer=bias initializer                  kernel regularizer=kernel regularizer                  bias regularizer=bias regularizer                  activity regularizer=activity regularizer                  kernel constraint=kernel constraint                  bias constraint=bias constraint                  trainable=trainable                  name=name                   scope=name                   reuse=reuse    return layer apply input  
 deprecation deprecate      date=none      instructions=string   tf export v1= string   def dropout input              rate=0 5              noise shape=none              seed=none              training=false              name=none     string   layer = dropout rate  noise shape=noise shape  seed=seed  name=name    return layer apply input  training=training  
 deprecation deprecate      date=none      instructions=string   tf export v1= string   def flatten input  name=none  data format=string     string   layer = flatten name=name  data format=data format    return layer apply input  
 deprecation deprecate      date=none  instructions=string   tf export v1= string   def max pooling1d input  pool size  stride                    padding=string  data format=string                    name=none     string   layer = maxpooling1d pool size=pool size                         strides=strides                         padding=padding                         data format=data format                         name=name    return layer apply input  
 deprecation deprecate      date=none  instructions=string   tf export v1= string   def max pooling2d input                    pool size  stride                    padding=string  data format=string                    name=none     string   layer = maxpooling2d pool size=pool size  strides=strides                         padding=padding  data format=data format                         name=name    return layer apply input  
 deprecation deprecate      date=none  instructions=string   tf export v1= string   def max pooling3d input                    pool size  stride                    padding=string  data format=string                    name=none     string   layer = maxpooling3d pool size=pool size  strides=strides                         padding=padding  data format=data format                         name=name    return layer apply input  
 deprecation deprecate      date=none      instructions=string   tf export v1= string   def separable conv1d input                       filter                       kernel size                       strides=1                       padding=string                       data format=string                       dilation rate=1                       depth multiplier=1                       activation=none                       use bias=true                       depthwise initializer=none                       pointwise initializer=none                       bias initializer=init ops zero initializer                         depthwise regularizer=none                       pointwise regularizer=none                       bias regularizer=none                       activity regularizer=none                       depthwise constraint=none                       pointwise constraint=none                       bias constraint=none                       trainable=true                       name=none                       reuse=none     string   layer = separableconv1d        filters=filters        kernel size=kernel size        strides=strides        padding=padding        data format=data format        dilation rate=dilation rate        depth multiplier=depth multiplier        activation=activation        use bias=use bias        depthwise initializer=depthwise initializer        pointwise initializer=pointwise initializer        bias initializer=bias initializer        depthwise regularizer=depthwise regularizer        pointwise regularizer=pointwise regularizer        bias regularizer=bias regularizer        activity regularizer=activity regularizer        depthwise constraint=depthwise constraint        pointwise constraint=pointwise constraint        bias constraint=bias constraint        trainable=trainable        name=name         reuse=reuse         scope=name    return layer apply input  
 deprecation deprecate      date=none      instructions=string   tf export v1= string   def separable conv2d input                       filter                       kernel size                       strides= 1  1                        padding=string                       data format=string                       dilation rate= 1  1                        depth multiplier=1                       activation=none                       use bias=true                       depthwise initializer=none                       pointwise initializer=none                       bias initializer=init ops zero initializer                         depthwise regularizer=none                       pointwise regularizer=none                       bias regularizer=none                       activity regularizer=none                       depthwise constraint=none                       pointwise constraint=none                       bias constraint=none                       trainable=true                       name=none                       reuse=none     string   layer = separableconv2d        filters=filters        kernel size=kernel size        strides=strides        padding=padding        data format=data format        dilation rate=dilation rate        depth multiplier=depth multiplier        activation=activation        use bias=use bias        depthwise initializer=depthwise initializer        pointwise initializer=pointwise initializer        bias initializer=bias initializer        depthwise regularizer=depthwise regularizer        pointwise regularizer=pointwise regularizer        bias regularizer=bias regularizer        activity regularizer=activity regularizer        depthwise constraint=depthwise constraint        pointwise constraint=pointwise constraint        bias constraint=bias constraint        trainable=trainable        name=name         reuse=reuse         scope=name    return layer apply input  
class ophint object     string                    function name attr = string      function uuid attr = string      function input index attr = string      function output index attr = string               function sort index attr = string            function aggregate attr = string                  tflite input indices = string      function level attr = string                  children input mappings = string             aggregate stack = string         aggregate first = string            aggregate last = string    class ophintargumenttracker object       string      def   init   self                   function name                   unique function id                   node name prefix                   attr name                   level=1                   children input mappings=none         string                                           self  function name = function name       self  unique function id = unique function id       self  next global index = 0         self  use global indices = set         self  tag to global index =            self  tag to next sort index =            self  node name prefix = node name prefix       self  attr name = attr name       self  level = level       self  children input mappings = children input mappings      def  get new global index self  index override         string       if index override be none          global index = self  next global index       else          if index override in self  use global indices            raise valueerror string          global index = index override              self  use global indices add global index        while self  next global index in self  use global indices          self  next global index  = 1       return global index      def add self  arg  tag=none  name=none  aggregate=none              index override=none         string               if tag be none          if aggregate be not none            raise valueerror string          global index = self  get new global index index override          sort index = none       else          if aggregate be none            raise valueerror string          if tag not in self  tag to global index            self  tag to global index tag  =                 self  get new global index index override             self  tag to next sort index tag  = 0         elif  index override and               index override  = self  tag to global index tag              raise valueerror                string                  tag  index override  self  tag to global index tag            global index = self  tag to global index tag          sort index = self  tag to next sort index tag          self  tag to next sort index tag   = 1        uuid = self  unique function id       name = string    self  node name prefix  self  function name                                      uuid  global index  sort index  name         identity op =  array ops identity arg  name=name                identity op op  set attr            ophint function name attr             attr value pb2 attrvalue                s= compat as bytes self  function name          identity op op  set attr            ophint function uuid attr             attr value pb2 attrvalue                s= compat as bytes self  unique function id          identity op op  set attr            self  attr name   attr value pb2 attrvalue i=global index         identity op op  set attr ophint function level attr                                  attr value pb2 attrvalue i=self  level         if self  children input mappings          identity op op  set attr              ophint children input mappings               attr value pb2 attrvalue                  s= compat as bytes  json dump                      self  children input mappings            if sort index be not none          identity op op  set attr              ophint function sort index attr               attr value pb2 attrvalue i=sort index         if aggregate be not none          identity op op  set attr              ophint function aggregate attr               attr value pb2 attrvalue s= compat as bytes  aggregate                  return identity op    def   init   self                 function name                 level=1                 children input mappings=none                   kwargs       string     self  function name = function name     self  level = level     if self  level == 1        assert children input mappings be none     else        assert isinstance children input mappings  dict      self  children input mappings = children input mappings     if self  children input mappings be not none        self  validate children input mappings self  children input mappings      self  unique function id =  uuid uuid1   hex       self  attrs to store later = kwargs     self  store attrs = false     self  input = ophint ophintargumenttracker          self  function name  self  unique function id  string          ophint function input index attr  level  self  children input mappings      self  output = ophint ophintargumenttracker          self  function name  self  unique function id  string          ophint function output index attr  level          self  children input mappings     def  validate children input mappings self  children input mappings       string     assert isinstance children input mappings  dict      assert string in children input mappings     assert string in children input mappings     assert string in children input mappings            def assert dictlist have key dictlist  key         for dikt in dictlist          assert isinstance dikt  dict          for key in key            assert key in dikt      assert dictlist have key          children input mappings string            string  string       assert dictlist have key          children input mappings string            string  string       assert dictlist have key          children input mappings string            string  string      def  setattr self  dest op  name  value       tensor value =  ops convert to tensor value           dest op op  set attr name   attr value pb2 attrvalue          tensor=tensor value op node def attr string  tensor           def add input self   args    kwargs       string     return self  input add  args    kwargs     def add output self   args    kwargs       string     return self  output add  args    kwargs     def add input self   args    kwargs       string     if string in kwargs        return             self  input add arg  name=name            for arg  name in zip args  kwargs string               else        return  self  input add arg  for arg in args     def add output self   args    kwargs       string     if string in kwargs        return             self  output add arg  name=name            for arg  name in zip args  kwargs string               else        return  self  output add arg  for arg in args  
class tfliteconverter tfliteconverterbase     string    def   init   self                 graph def                 input tensors                 output tensors                 input array with shape=none                 output arrays=none                 experimental debug info func=none       string     super tfliteconverter  self    init         self  graph def = graph def     self  input tensors = input tensors     self  output tensors = output tensors     self inference type = constants float     self inference input type = none     self inference output type = none     self output format = constants tflite     self quantize input stats =        self default range stats = none     self drop control dependency = true     self reorder across fake quant = false     self change concat input range = false     self  post train quantize = false     self dump graphviz dir = none     self dump graphviz video = false     self conversion summary dir = none     self  debug info func = experimental debug info func           if not self  have valid tensors          if not input array with shape or not output array          raise valueerror              string             string        self  input array with shape = input array with shape       self  output array = output array     classmethod   def from session cls  sess  input tensors  output tensors       string     graph def =  freeze graph sess  input tensors  output tensors      return cls          graph def          input tensors          output tensors          experimental debug info func= build debug info func sess graph       classmethod   def from freeze graph cls                          graph def file                          input array                          output array                          input shapes=none       string     with  ops graph   as default          with  session session   as sess                   if not  file io file exist graph def file             raise ioerror string format graph def file           with  file io fileio graph def file  string  as f            file content = f read            try            graph def =  graph pb2 graphdef             graph def parsefromstring file content          except   text format parseerror  decodeerror             try              print string               if not isinstance file content  str                 if py3                  file content = six ensure text file content  string                else                  file content = six ensure binary file content  string              graph def =  graph pb2 graphdef                text format merge file content  graph def            except   text format parseerror  decodeerror               raise ioerror                  string format graph def file                              load model in session = true         try             import graph def graph def  name=string          except  notfounderror            load model in session = false          if load model in session                       if not  be freeze graph sess               raise valueerror string                        input tensors =  get tensors from tensor name                sess graph  input array            output tensors =  get tensors from tensor name                sess graph  output array             set tensor shape input tensors  input shape             return cls sess graph def  input tensors  output tensors          else            if not input shape              raise valueerror string            if set input array   = set input shape key                 raise valueerror string                              string             input array with shape =                  name  input shape name   for name in input array                       return cls                graph def                input tensors=none                output tensors=none                input array with shape=input array with shape                output arrays=output array      classmethod   def from save model cls                         save model dir                         input arrays=none                         input shapes=none                         output arrays=none                         tag set=none                         signature key=none       string     if tag set be none        tag set = set   tag constants serve       if signature key be none        signature key =  signature constants default serve signature def key      result =  freeze save model save model dir  input array  input shape                                   output array  tag set  signature key      return cls          graph def=result 0           input tensors=result 1           output tensors=result 2           experimental debug info func= build debug info func result 3        classmethod   def from keras model file cls                              model file                              input arrays=none                              input shapes=none                              output arrays=none                              custom objects=none       string          if context execute eagerly          if input array or output array          raise valueerror string                          string                          string          keras backend set learn phase false        keras model =  keras model load model model file  custom object         function =  save utils trace model call keras model        concrete func = function get concrete function          freeze func =  convert to constants convert variables to constants v2            concrete func  lower control flow=false         set tensor shape freeze func input  input shape        return cls            freeze func graph as graph def              freeze func input            freeze func output            experimental debug info func= build debug info func                freeze func graph              keras backend clear session        keras backend set learn phase false      keras model =  keras model load model model file  custom object      sess =  keras backend get session             if input array        input tensors =  get tensors from tensor name sess graph  input array      else        input tensors = keras model input      if output array        output tensors =  get tensors from tensor name sess graph  output array      else        output tensors = keras model output      set tensor shape input tensors  input shape       graph def =  freeze graph sess  input tensors  output tensors      return cls          graph def          input tensors          output tensors          experimental debug info func= build debug info func sess graph      def   setattr   self  name  value       if name == string        warn warn string                     string                     string   name        if value          self optimizations =  optimize default        else          self optimizations =          return     if name == string        warn warn string                     string   name        self target spec support ops = value       return     object   setattr   self  name  value     def   getattribute   self  name       if name == string        warn warn string                     string                     string   name        return optimize default in set self optimizations      if name == string        warn warn string                     string   name        return self target spec support ops     return object   getattribute   self  name     def convert self       string          if self  have valid tensors          for tensor in self  input tensors          shape = tensor shape         if not shape            raise valueerror string                            string format  get tensor name tensor                     shape list = shape as list           if none in shape list 1              raise valueerror                string               string format                     get tensor name tensor   shape list           elif shape list and shape list 0  be none            self  set batch size batch size=1                 if self quantize input stats        quantize stats =          invalid stats =          for name in self get input array            if name in self quantize input stats            quantize stats append self quantize input stats name           else            invalid stats append name         if invalid stats          raise valueerror string                          string format string join invalid stats        else        quantize stats = none      self  validate quantization       self  validate representative dataset        toco inference input type = self inference input type     inference input type = self inference input type     inference output type = self inference output type     post train optimize = self  be post train optimize       if post train optimize               if self inference type  = constants float          raise valueerror              string        toco inference input type = constants float              if inference input type be none          inference input type = constants float       if inference output type be none          inference output type = constants float      weight only quantize = self  be int8 weight only quantize       if weight only quantize               if  inference input type  = constants float or           inference output type  = constants float           raise valueerror              string             string       if not post train optimize and self inference output type be not none        raise valueerror            string           string       optimize graph = self  graph def     if self inference type  = constants quantize uint8        try          optimize graph =  run graph optimizations              self  graph def              self  input tensors              self  output tensors              config=self  grappler config          except exception          optimize graph = self  graph def      self  debug info =  get debug info self  debug info func  optimize graph       converter kwargs = self  get base converter args       converter kwargs update           string  self inference type          string  toco inference input type          string  self output format          string  quantize stats          string  self default range stats          string  self drop control dependency          string  self reorder across fake quant          string  self change concat input range          string  self dump graphviz dir          string  self dump graphviz video          string  self conversion summary dir                  if self  have valid tensors          result =  toco convert impl            input data=optimized graph            input tensors=self  input tensors            output tensors=self  output tensors              converter kwargs      else        result =  toco convert graph def            input data=optimized graph            input array with shape=self  input array with shape            output arrays=self  output array              converter kwargs       if self  be calibration quantize          result = self  calibrate quantize model            result  inference input type  inference output type            self experimental new quantizer       return result    def get input array self       string     if self  have valid tensors          return   get tensor name tensor  for tensor in self  input tensors      else        return  name for name    in self  input array with shape     def  have valid tensors self       string     return self  input tensors and self  output tensors    def  set batch size self  batch size       string     if not self  have valid tensors          raise valueerror string                        string       for tensor in self  input tensors        shape = tensor shape as list         if shape 0  be none          shape 0  = batch size         tensor set shape shape  
class tococonverter object     string     classmethod     deprecation deprecate none                             string    def from session cls  sess  input tensors  output tensors       string     return tfliteconverter from session sess  input tensors  output tensors      classmethod     deprecation deprecate        none  string    def from freeze graph cls                          graph def file                          input array                          output array                          input shapes=none       string     return tfliteconverter from freeze graph graph def file  input array                                               output array  input shape      classmethod     deprecation deprecate        none  string    def from save model cls                         save model dir                         input arrays=none                         input shapes=none                         output arrays=none                         tag set=none                         signature key=none       string     return tfliteconverter from save model save model dir  input array                                              input shape  output array                                              tag set  signature key      classmethod     deprecation deprecate        none  string    def from keras model file cls                              model file                              input arrays=none                              input shapes=none                              output arrays=none       string     return tfliteconverter from keras model file model file  input array                                                   input shape  output array  
  tf export v1= string    deprecation deprecate none  string  def toco convert input data  input tensors  output tensors   args    kwargs     string   enable mlir converter = kwargs get string  false    return toco convert impl input data  input tensors  output tensors                             enable mlir converter   args    kwargs  
 tf export v1= string   def get verbosity      string   return get logger   geteffectivelevel   
 tf export v1= string   def log every n level  msg  n   args     string   count =  getnextlogcountpertoken  getfileandline      log if level  msg  not  count   n    args  
 tf export v1= string   def log first n level  msg  n   args       string   count =  getnextlogcountpertoken  getfileandline      log if level  msg  count < n   args  
 tf export v1= string   def log if level  msg  condition   args     string   if condition      vlog level  msg   args  
 tf export v1= string   def set verbosity v     string   get logger   setlevel v  
class statichashtablev1 statichashtable     string     property   def initializer self       return self  init op 
class staticvocabularytablev1 staticvocabularytable       property   def initializer self       if self  table be not none        return self  table  init op       with ops name scope none  string         return control flow ops no op   
class reduction object     string    none = string   sum = string   sum over batch size = string   mean = string   sum by nonzero weight = string   sum over nonzero weight = sum by nonzero weight     classmethod   def all cls       return           cls none          cls sum          cls mean          cls sum over batch size          cls sum over nonzero weight          cls sum by nonzero weight      classmethod   def validate cls  key       if key not in cls all          raise valueerror string   key  
 tf export v1= string   def absolute difference      label  predictions  weights=1 0  scope=none      loss collection=ops graphkeys losses      reduction=reduction sum by nonzero weight     string   if label be none      raise valueerror string    if predictions be none      raise valueerror string    with ops name scope scope  string                         predictions  label  weight   as scope      predictions = math ops cast predictions  dtype=dtypes float32      label = math ops cast label  dtype=dtypes float32      predictions get shape   assert be compatible with label get shape        losses = math ops abs math ops subtract predictions  label       return compute weight loss          losses  weight  scope  loss collection  reduction=reduction  
 tf export v1= string   def add loss loss  loss collection=ops graphkeys losses     string            if loss collection and not context execute eagerly        ops add to collection loss collection  loss  
 tf export v1= string   def compute weight loss      losses  weights=1 0  scope=none  loss collection=ops graphkeys losses      reduction=reduction sum by nonzero weight     string   reduction validate reduction    with ops name scope scope  string   losses  weight                  ops get default graph    last loss reduction = reduction        with ops control dependencies           weight broadcast ops assert broadcastable weight  losses            losses = ops convert to tensor losses        input dtype = losses dtype       losses = math ops cast losses  dtype=dtypes float32        weight = math ops cast weight  dtype=dtypes float32        weight losses = math ops multiply losses  weight        if reduction == reduction none          loss = weight losses       else          loss = math ops reduce sum weight losses          if reduction == reduction mean            loss =  safe mean                loss  math ops reduce sum array ops ones like losses    weight           elif  reduction == reduction sum by nonzero weight or               reduction == reduction sum over nonzero weight             loss =  safe mean loss   num present losses  weight           elif reduction == reduction sum over batch size            loss =  safe mean loss   num elements losses                 loss = math ops cast loss  input dtype        util add loss loss  loss collection        return loss 
 tf export v1= string    deprecate args none  string  string  def cosine distance      label  predictions  axis=none  weights=1 0  scope=none      loss collection=ops graphkeys losses      reduction=reduction sum by nonzero weight      dim=none     string   axis = deprecate argument lookup string  axis  string  dim    if axis be none      raise valueerror string    if label be none      raise valueerror string    if predictions be none      raise valueerror string    with ops name scope scope  string                         predictions  label  weight   as scope      predictions = math ops cast predictions  dtype=dtypes float32      label = math ops cast label  dtype=dtypes float32      predictions get shape   assert be compatible with label get shape         radial diffs = math ops multiply predictions  label      losses = 1 - math ops reduce sum radial diffs  axis= axis    keepdims=true      return compute weight loss          losses  weight  scope  loss collection  reduction=reduction  
 tf export v1= string   def get losses scope=none  loss collection=ops graphkeys losses     string   return ops get collection loss collection  scope  
 tf export v1= string   def get regularization loss scope=none  name=string     string   losses = get regularization losses scope    if losses      return math ops add n losses  name=name    else      return constant op constant 0 0  
 tf export v1= string   def get regularization losses scope=none     string   return ops get collection ops graphkeys regularization losses  scope  
 tf export v1= string   def get total loss add regularization losses=true                     name=string                     scope=none     string   losses = get losses scope=scope    if add regularization losses      losses  = get regularization losses scope=scope    return math ops add n losses  name=name  
 tf export v1= string   def hinge loss label  logits  weights=1 0  scope=none                 loss collection=ops graphkeys losses                 reduction=reduction sum by nonzero weight     string   if label be none      raise valueerror string    if logits be none      raise valueerror string    with ops name scope scope  string   logits  label  weight   as scope      logits = math ops cast logits  dtype=dtypes float32      label = math ops cast label  dtype=dtypes float32      logits get shape   assert be compatible with label get shape             all ones = array ops ones like label      label = math ops subtract 2   label  all ones      losses = nn ops relu          math ops subtract all ones  math ops multiply label  logits        return compute weight loss          losses  weight  scope  loss collection  reduction=reduction  
 tf export v1= string   def huber loss label  predictions  weights=1 0  delta=1 0  scope=none                 loss collection=ops graphkeys losses                 reduction=reduction sum by nonzero weight     string   if label be none      raise valueerror string    if predictions be none      raise valueerror string    with ops name scope scope  string                         predictions  label  weight   as scope      predictions = math ops cast predictions  dtype=dtypes float32      label = math ops cast label  dtype=dtypes float32      predictions get shape   assert be compatible with label get shape        error = math ops subtract predictions  label      abs error = math ops abs error      quadratic = math ops minimum abs error  delta                               linear = math ops subtract abs error  quadratic      losses = math ops add          math ops multiply              ops convert to tensor 0 5  dtype=quadratic dtype               math ops multiply quadratic  quadratic            math ops multiply delta  linear       return compute weight loss          losses  weight  scope  loss collection  reduction=reduction  
 tf export v1= string   def log loss label  predictions  weights=1 0  epsilon=1e-7  scope=none               loss collection=ops graphkeys losses               reduction=reduction sum by nonzero weight     string   if label be none      raise valueerror string    if predictions be none      raise valueerror string    with ops name scope scope  string                         predictions  label  weight   as scope      predictions = math ops cast predictions  dtype=dtypes float32      label = math ops cast label  dtype=dtypes float32      predictions get shape   assert be compatible with label get shape        losses = -math ops multiply          label          math ops log predictions   epsilon   - math ops multiply               1 - label   math ops log 1 - predictions   epsilon       return compute weight loss          losses  weight  scope  loss collection  reduction=reduction  
 tf export v1= string   def mean pairwise square error      label  predictions  weights=1 0  scope=none      loss collection=ops graphkeys losses     string   if label be none      raise valueerror string    if predictions be none      raise valueerror string    with ops name scope scope  string                         predictions  label  weight   as scope      weight = math ops cast weight  dtype=dtypes float32      label = math ops cast label  dtype=dtypes float32      with ops control dependencies           weight broadcast ops assert broadcastable weight  label            predictions = math ops cast predictions  dtype=dtypes float32        predictions get shape   assert be compatible with label get shape           diffs = math ops subtract predictions  label         axis = math ops range 1  array ops rank diffs          sum square diff per batch = math ops reduce sum            math ops square diffs   axis=axis  keepdims=true        num present per batch =  num present diffs  weight  per batch=true         term1 = 2 0   math ops div no nan            sum square diff per batch            math ops maximum num present per batch - 1  0             name=string         sum diff = math ops reduce sum diffs  axis=axis  keepdims=true        term2 = 2 0   math ops div no nan            math ops square sum diff             math ops maximum                math ops multiply num present per batch                                  num present per batch - 1   0             name=string         weight losses = math ops multiply term1 - term2  weight        loss = math ops reduce sum weight losses         mean loss = array ops where            math ops reduce sum num present per batch  > 0            loss            array ops zero like loss             name=string        util add loss mean loss  loss collection        return mean loss 
 tf export v1= string   def mean square error      label  predictions  weights=1 0  scope=none      loss collection=ops graphkeys losses      reduction=reduction sum by nonzero weight     string   if label be none      raise valueerror string    if predictions be none      raise valueerror string    with ops name scope scope  string                         predictions  label  weight   as scope      predictions = math ops cast predictions  dtype=dtypes float32      label = math ops cast label  dtype=dtypes float32      predictions get shape   assert be compatible with label get shape        losses = math ops square difference predictions  label      return compute weight loss          losses  weight  scope  loss collection  reduction=reduction  
 tf export v1= string   def sigmoid cross entropy      multi class label  logits  weights=1 0  label smoothing=0  scope=none      loss collection=ops graphkeys losses      reduction=reduction sum by nonzero weight     string   if multi class label be none      raise valueerror string    if logits be none      raise valueerror string    with ops name scope scope  string                         logits  multi class label  weight   as scope      logits = ops convert to tensor logits      multi class label = math ops cast multi class label  logits dtype      logits get shape   assert be compatible with multi class label get shape         if label smooth > 0        multi class label =  multi class label    1 - label smooth                                0 5   label smooth       losses = nn sigmoid cross entropy with logits labels=multi class label                                                    logits=logits                                                    name=string      return compute weight loss          losses  weight  scope  loss collection  reduction=reduction  
 tf export v1= string   def softmax cross entropy      onehot label  logits  weights=1 0  label smoothing=0  scope=none      loss collection=ops graphkeys losses      reduction=reduction sum by nonzero weight     string   if onehot label be none      raise valueerror string    if logits be none      raise valueerror string    with ops name scope scope  string                         logits  onehot label  weight   as scope      logits = ops convert to tensor logits      onehot label = math ops cast onehot label  logits dtype      logits get shape   assert be compatible with onehot label get shape         if label smooth > 0        num class = math ops cast            array ops shape onehot label  -1   logits dtype        smooth positives = 1 0 - label smooth       smooth negative = label smooth / num class       onehot label = onehot label   smooth positives   smooth negative      onehot label = array ops stop gradient          onehot label  name=string      losses = nn softmax cross entropy with logits v2          labels=onehot label  logits=logits  name=string       return compute weight loss          losses  weight  scope  loss collection  reduction=reduction  
 tf export v1= string   def sparse softmax cross entropy      label  logits  weights=1 0  scope=none      loss collection=ops graphkeys losses      reduction=reduction sum by nonzero weight     string   if label be none      raise valueerror string    if logits be none      raise valueerror string    with ops name scope scope  string                         logits  label  weight   as scope                     label  logits  weight =  remove squeezable dimension          label  logits  weight  expect rank diff=1      losses = nn sparse softmax cross entropy with logits labels=labels                                                           logits=logits                                                           name=string      return compute weight loss          losses  weight  scope  loss collection  reduction=reduction  
 tf export v1= string  string   def in top k predictions  target  k  name=none     r   say whether the target be in the top `k` predictions     this output a `batch size` bool array  an entry `out i ` be `true` if the   prediction for the target class be finite  not inf  -inf  or nan  and among   the top `k` predictions among all predictions for example `i`  note that the   behavior of `intopk` differ from the `topk` op in its handle of tie  if   multiple class have the same prediction value and straddle the top-`k`   boundary  all of those class be consider to be in the top `k`     more formally  let      \\ predictions i\\  be the predictions for all class for example `i`      \\ target i\\  be the target class for example `i`      \\ out i\\  be the output for example `i`       out i = predictions  i  target i  \in topkincludingties predictions i       args      predictions  a `tensor` of type `float32`        a `batch size` x `classes` tensor      target  a `tensor`  must be one of the follow type  `int32`  `int64`        a `batch size` vector of class ids      k  an `int`  number of top elements to look at for compute precision      name  a name for the operation  optional      return      a `tensor` of type `bool`  compute precision at `k` as a `bool tensor`          with ops name scope name  string       return gen nn ops in top kv2 predictions  target  k  name=name  
 tf export v1= string  string    deprecation deprecate args none  string  string  def log softmax logits  axis=none  name=none  dim=none     string   axis = deprecation deprecate argument lookup string  axis  string  dim    if axis be none      axis = -1   return  softmax logits  gen nn ops log softmax  axis  name  
 tf export v1= string  string    deprecation deprecate args none  string  string  def softmax logits  axis=none  name=none  dim=none     string   axis = deprecation deprecate argument lookup string  axis  string  dim    if axis be none      axis = -1   return  softmax logits  gen nn ops softmax  axis  name  
 tf export v1= string   def accuracy label               predictions               weights=none               metrics collections=none               update collections=none               name=none     string   if context execute eagerly        raise runtimeerror string                        string     predictions  label  weight =  remove squeezable dimension        predictions=predictions  labels=labels  weights=weights    predictions get shape   assert be compatible with label get shape      if label dtype  = predictions dtype      predictions = math ops cast predictions  label dtype    be correct = math ops cast        math ops equal predictions  label   dtypes float32    return mean be correct  weight  metrics collections  update collections                name or string  
 tf export v1= string    deprecate none              string             string  def auc label          predictions          weights=none          num thresholds=200          metrics collections=none          update collections=none          curve=string          name=none          summation method=string          thresholds=none     string   if context execute eagerly        raise runtimeerror string                        string     with variable scope variable scope name  string                                        label  predictions  weight        if curve  = string and curve  = string        raise valueerror string    curve        kepsilon = 1e-7       if thresholds be not none               thresholds = sort thresholds        num thresholds = len thresholds    2     else                      thresholds =   i   1    1 0 /  num thresholds - 1                      for i in range num thresholds - 2                  thresholds =  0 0 - kepsilon    thresholds    1 0   kepsilon       value  update ops =  confusion matrix at thresholds          label  predictions  thresholds  weight            epsilon = 1 0e-6      def interpolate pr auc tp  fp  fn         string       dtp = tp  num thresholds - 1  - tp 1         p = tp   fp       prec slope = math ops div no nan            dtp            math ops maximum p  num thresholds - 1  - p 1    0             name=string        intercept = tp 1   - math ops multiply prec slope  p 1          safe p ratio = array ops where            math ops logical and p  num thresholds - 1  > 0  p 1   > 0             math ops div no nan                p  num thresholds - 1                 math ops maximum p 1    0                 name=string   array ops ones like p 1           return math ops reduce sum            math ops div no nan                prec slope    dtp   intercept   math ops log safe p ratio                  math ops maximum tp 1     fn 1    0                 name=string             name=string       def compute auc tp  fn  tn  fp  name         string       if curve == string          if summation method == string            log warn                string               string          elif summation method == string                       return interpolate pr auc tp  fp  fn        rec = math ops div tp   epsilon  tp   fn   epsilon        if curve == string          fp rate = math ops div fp  fp   tn   epsilon          x = fp rate         y = rec       else            prec = math ops div tp   epsilon  tp   fp   epsilon          x = rec         y = prec       if summation method in  string  string                             return math ops reduce sum              math ops multiply x  num thresholds - 1  - x 1                                   y  num thresholds - 1    y 1    / 2                name=name        elif summation method == string          return math ops reduce sum              math ops multiply x  num thresholds - 1  - x 1                                  math ops minimum y  num thresholds - 1   y 1                  name=name        elif summation method == string          return math ops reduce sum              math ops multiply x  num thresholds - 1  - x 1                                  math ops maximum y  num thresholds - 1   y 1                  name=name        else          raise valueerror string   summation method            def compute auc value    value         return compute auc value string   value string   value string   value string                            string       auc value =  aggregate across replicas          metrics collections  compute auc value  value      update op = compute auc update ops string   update ops string                               update ops string   update ops string   string       if update collections        ops add to collections update collections  update op       return auc value  update op 
 tf export v1= string   def average precision at k label                             predictions                             k                             weights=none                             metrics collections=none                             update collections=none                             name=none     string   if context execute eagerly        raise runtimeerror string                        string     if k < 1      raise valueerror string   k    with ops name scope name   at k name string  k                          predictions  label  weight   as scope              predictions idx = nn top k predictions  k                          label =  clean out of range indices          label  math ops cast array ops shape predictions  -1   dtypes int64       return  stream sparse average precision at top k          labels=labels          predictions idx=predictions idx          weights=weights          metrics collections=metrics collections          update collections=updates collections          name=scope  
 tf export v1= string   def false negative label                      predictions                      weights=none                      metrics collections=none                      update collections=none                      name=none     string   if context execute eagerly        raise runtimeerror string                        string     with variable scope variable scope name  string                                        predictions  label  weight         predictions  label  weight =  remove squeezable dimension          predictions=math ops cast predictions  dtype=dtypes bool           labels=math ops cast label  dtype=dtypes bool           weights=weights      be false negative = math ops logical and          math ops equal label  true   math ops equal predictions  false       return  count condition be false negative  weight  metrics collections                              update collections  
 tf export v1= string   def false negative at thresholds label                                    predictions                                    thresholds                                    weights=none                                    metrics collections=none                                    update collections=none                                    name=none     string   if context execute eagerly        raise runtimeerror string                        string     with variable scope variable scope name  string                                        predictions  label  weight        value  update ops =  confusion matrix at thresholds          label  predictions  thresholds  weights=weights  includes= string         fn value =  aggregate variable value string   metrics collections       if update collections        ops add to collections update collections  update ops string        return fn value  update ops string  
 tf export v1= string   def false positives label                      predictions                      weights=none                      metrics collections=none                      update collections=none                      name=none     string   if context execute eagerly        raise runtimeerror string                        string     with variable scope variable scope name  string                                        predictions  label  weight         predictions  label  weight =  remove squeezable dimension          predictions=math ops cast predictions  dtype=dtypes bool           labels=math ops cast label  dtype=dtypes bool           weights=weights      be false positive = math ops logical and          math ops equal label  false   math ops equal predictions  true       return  count condition be false positive  weight  metrics collections                              update collections  
 tf export v1= string   def false positives at thresholds label                                    predictions                                    thresholds                                    weights=none                                    metrics collections=none                                    update collections=none                                    name=none     string   if context execute eagerly        raise runtimeerror string                        string     with variable scope variable scope name  string                                        predictions  label  weight        value  update ops =  confusion matrix at thresholds          label  predictions  thresholds  weights=weights  includes= string         fp value =  aggregate variable value string   metrics collections       if update collections        ops add to collections update collections  update ops string        return fp value  update ops string  
 tf export v1= string   def mean value           weights=none           metrics collections=none           update collections=none           name=none     string   if context execute eagerly        raise runtimeerror string                        string     with variable scope variable scope name  string   value  weight        value = math ops cast value  dtypes float32       total = metric variable     dtypes float32  name=string      count = metric variable     dtypes float32  name=string       if weight be none        num value = math ops cast array ops size value   dtypes float32      else        value     weight =  remove squeezable dimension            predictions=values  labels=none  weights=weights        weight = weight broadcast ops broadcast weight            math ops cast weight  dtypes float32   value        value = math ops multiply value  weight        num value = math ops reduce sum weight       update total op = state ops assign add total  math ops reduce sum value       with ops control dependencies  value          update count op = state ops assign add count  num value       def compute mean    t  c         return math ops div no nan t  math ops maximum c  0   name=string       mean t =  aggregate across replicas          metrics collections  compute mean  total  count      update op = math ops div no nan          update total op  math ops maximum update count op  0   name=string       if update collections        ops add to collections update collections  update op       return mean t  update op 
 tf export v1= string   def mean absolute error label                          predictions                          weights=none                          metrics collections=none                          update collections=none                          name=none     string   if context execute eagerly        raise runtimeerror string                        string     predictions  label  weight =  remove squeezable dimension        predictions=predictions  labels=labels  weights=weights    absolute errors = math ops abs predictions - label    return mean absolute errors  weight  metrics collections                update collections  name or string  
 tf export v1= string   def mean cosine distance label                           predictions                           dim                           weights=none                           metrics collections=none                           update collections=none                           name=none     string   if context execute eagerly        raise runtimeerror string                        string     predictions  label  weight =  remove squeezable dimension        predictions=predictions  labels=labels  weights=weights    radial diffs = math ops multiply predictions  label    radial diffs = math ops reduce sum        radial diffs  axis=            dim           keepdims=true    mean distance  update op = mean radial diffs  weight  none  none  name or                                   string    mean distance = math ops subtract 1 0  mean distance    update op = math ops subtract 1 0  update op     if metrics collections      ops add to collections metrics collections  mean distance     if update collections      ops add to collections update collections  update op     return mean distance  update op 
 tf export v1= string   def mean iou label               predictions               num class               weights=none               metrics collections=none               update collections=none               name=none     string   if context execute eagerly        raise runtimeerror string                        string     with variable scope variable scope name  string                                        predictions  label  weight             predictions get shape   assert be compatible with label get shape         total cm  update op =  stream confusion matrix label  predictions                                                        num class  weight       def compute mean iou    total cm         string       sum over row = math ops cast            math ops reduce sum total cm  0   dtypes float32        sum over col = math ops cast            math ops reduce sum total cm  1   dtypes float32        cm diag = math ops cast array ops diag part total cm   dtypes float32        denominator = sum over row   sum over col - cm diag                             num valid entries = math ops reduce sum            math ops cast                math ops not equal denominator  0   dtype=dtypes float32                        denominator = array ops where            math ops greater denominator  0   denominator            array ops ones like denominator         iou = math ops div cm diag  denominator                result = array ops where            math ops greater num valid entries  0             math ops reduce sum iou  name=string  / num valid entries  0        return result           mean iou v =  aggregate across replicas          metrics collections  compute mean iou  total cm       if update collections        ops add to collections update collections  update op       return mean iou v  update op 
 tf export v1= string   def mean per class accuracy label                              predictions                              num class                              weights=none                              metrics collections=none                              update collections=none                              name=none     string   if context execute eagerly        raise runtimeerror string                        string     with variable scope variable scope name  string                                        predictions  label  weight        label = math ops cast label  dtypes int64            if label get shape   ndims > 1        label = array ops reshape label   -1        if predictions get shape   ndims > 1        predictions = array ops reshape predictions   -1             predictions get shape   assert be compatible with label get shape         total = metric variable  num class   dtypes float32  name=string      count = metric variable  num class   dtypes float32  name=string       ones = array ops ones  array ops size label    dtypes float32       if label dtype  = predictions dtype        predictions = math ops cast predictions  label dtype      be correct = math ops cast          math ops equal predictions  label   dtypes float32       if weight be not none        if weight get shape   ndims > 1          weight = array ops reshape weight   -1         weight = math ops cast weight  dtypes float32         be correct  = weight       ones  = weight      update total op = state ops scatter add total  label  ones      update count op = state ops scatter add count  label  be correct       def compute mean accuracy    count  total         per class accuracy = math ops div no nan            count  math ops maximum total  0   name=none        mean accuracy v = math ops reduce mean            per class accuracy  name=string        return mean accuracy v      mean accuracy v =  aggregate across replicas          metrics collections  compute mean accuracy  count  total       update op = math ops div no nan          update count op  math ops maximum update total op  0   name=string      if update collections        ops add to collections update collections  update op       return mean accuracy v  update op 
 tf export v1= string   def mean relative error label                          predictions                          normalizer                          weights=none                          metrics collections=none                          update collections=none                          name=none     string   if context execute eagerly        raise runtimeerror string                        string     predictions  label  weight =  remove squeezable dimension        predictions=predictions  labels=labels  weights=weights     predictions  normalizer = confusion matrix remove squeezable dimension        predictions  normalizer    predictions get shape   assert be compatible with normalizer get shape      relative errors = array ops where        math ops equal normalizer  0 0   array ops zero like label         math ops div math ops abs label - predictions   normalizer     return mean relative errors  weight  metrics collections                update collections  name or string  
 tf export v1= string   def mean square error label                         predictions                         weights=none                         metrics collections=none                         update collections=none                         name=none     string   if context execute eagerly        raise runtimeerror string                        string     predictions  label  weight =  remove squeezable dimension        predictions=predictions  labels=labels  weights=weights    square error = math ops square difference label  predictions    return mean square error  weight  metrics collections  update collections                name or string  
 tf export v1= string   def mean tensor value                  weights=none                  metrics collections=none                  update collections=none                  name=none     string   if context execute eagerly        raise runtimeerror string                        string     with variable scope variable scope name  string   value  weight        value = math ops cast value  dtypes float32      total = metric variable          value get shape    dtypes float32  name=string      count = metric variable          value get shape    dtypes float32  name=string       num value = array ops ones like value      if weight be not none        value     weight =  remove squeezable dimension            predictions=values  labels=none  weights=weights        weight = weight broadcast ops broadcast weight            math ops cast weight  dtypes float32   value        value = math ops multiply value  weight        num value = math ops multiply num value  weight       update total op = state ops assign add total  value      with ops control dependencies  value          update count op = state ops assign add count  num value       compute mean = lambda    t  c  math ops div no nan            t  math ops maximum c  0   name=string       mean t =  aggregate across replicas          metrics collections  compute mean  total  count       update op = math ops div no nan          update total op  math ops maximum update count op  0   name=string      if update collections        ops add to collections update collections  update op       return mean t  update op 
 tf export v1= string   def percentage below value                       threshold                       weights=none                       metrics collections=none                       update collections=none                       name=none     string   if context execute eagerly        raise runtimeerror string                        string     be below threshold = math ops cast        math ops less value  threshold   dtypes float32    return mean be below threshold  weight  metrics collections                update collections  name or string  
 tf export v1= string   def precision label                predictions                weights=none                metrics collections=none                update collections=none                name=none     string   if context execute eagerly        raise runtimeerror string                        string     with variable scope variable scope name  string                                        predictions  label  weight         predictions  label  weight =  remove squeezable dimension          predictions=math ops cast predictions  dtype=dtypes bool           labels=math ops cast label  dtype=dtypes bool           weights=weights       true p  true positives update op = true positives          label          predictions          weight          metrics collections=none          update collections=none          name=none      false p  false positives update op = false positives          label          predictions          weight          metrics collections=none          update collections=none          name=none       def compute precision tp  fp  name         return array ops where            math ops greater tp   fp  0   math ops div tp  tp   fp   0  name       def once across replicas    true p  false p         return compute precision true p  false p  string       p =  aggregate across replicas metrics collections  once across replicas                                     true p  false p       update op = compute precision true positives update op                                    false positives update op  string      if update collections        ops add to collections update collections  update op       return p  update op 
 tf export v1= string   def precision at k label                     predictions                     k                     class id=none                     weights=none                     metrics collections=none                     update collections=none                     name=none     string   if context execute eagerly        raise runtimeerror string                        string     with ops name scope name   at k name string  k  class id=class id                          predictions  label  weight   as scope         top k idx = nn top k predictions  k      return precision at top k          labels=labels          predictions idx=top k idx          k=k          class id=class id          weights=weights          metrics collections=metrics collections          update collections=updates collections          name=scope  
 tf export v1= string   def precision at thresholds label                              predictions                              thresholds                              weights=none                              metrics collections=none                              update collections=none                              name=none     string   if context execute eagerly        raise runtimeerror string                        string     with variable scope variable scope name  string                                        predictions  label  weight        value  update ops =  confusion matrix at thresholds          label  predictions  thresholds  weight  includes= string  string             epsilon = 1e-7      def compute precision tp  fp  name         return math ops div tp  epsilon   tp   fp  name=string   name       def precision across replicas    value         return compute precision value string   value string   string       prec =  aggregate across replicas          metrics collections  precision across replicas  value       update op = compute precision update ops string   update ops string                                     string      if update collections        ops add to collections update collections  update op       return prec  update op 
 tf export v1= string   def precision at top k label                         predictions idx                         k=none                         class id=none                         weights=none                         metrics collections=none                         update collections=none                         name=none     string   if context execute eagerly        raise runtimeerror string                        string     with ops name scope name   at k name string  k  class id=class id                          predictions idx  label  weight   as scope      label =  maybe expand label label  predictions idx      top k idx = math ops cast predictions idx  dtypes int64      tp  tp update =  stream sparse true positive at k          predictions idx=top k idx          labels=labels          k=k          class id=class id          weights=weights      fp  fp update =  stream sparse false positive at k          predictions idx=top k idx          labels=labels          k=k          class id=class id          weights=weights       def precision across replicas    tp  fp         return math ops div tp  math ops add tp  fp   name=scope       metric =  aggregate across replicas          metrics collections  precision across replicas  tp  fp       update = math ops div          tp update  math ops add tp update  fp update   name=string      if update collections        ops add to collections update collections  update      return metric  update 
 tf export v1= string   def recall label             predictions             weights=none             metrics collections=none             update collections=none             name=none     string   if context execute eagerly        raise runtimeerror string                        string     with variable scope variable scope name  string                                        predictions  label  weight        predictions  label  weight =  remove squeezable dimension          predictions=math ops cast predictions  dtype=dtypes bool           labels=math ops cast label  dtype=dtypes bool           weights=weights       true p  true positives update op = true positives          label          predictions          weight          metrics collections=none          update collections=none          name=none      false n  false negative update op = false negative          label          predictions          weight          metrics collections=none          update collections=none          name=none       def compute recall true p  false n  name         return array ops where            math ops greater true p   false n  0             math ops div true p  true p   false n   0  name       def once across replicas    true p  false n         return compute recall true p  false n  string       rec =  aggregate across replicas          metrics collections  once across replicas  true p  false n       update op = compute recall true positives update op                                 false negative update op  string      if update collections        ops add to collections update collections  update op       return rec  update op 
 tf export v1= string   def recall at k label                  predictions                  k                  class id=none                  weights=none                  metrics collections=none                  update collections=none                  name=none     string   if context execute eagerly        raise runtimeerror string                        string     with ops name scope name   at k name string  k  class id=class id                          predictions  label  weight   as scope         top k idx = nn top k predictions  k      return recall at top k          labels=labels          predictions idx=top k idx          k=k          class id=class id          weights=weights          metrics collections=metrics collections          update collections=updates collections          name=scope  
 tf export v1= string   def recall at thresholds label                           predictions                           thresholds                           weights=none                           metrics collections=none                           update collections=none                           name=none     string   if context execute eagerly        raise runtimeerror string                        string     with variable scope variable scope name  string                                        predictions  label  weight        value  update ops =  confusion matrix at thresholds          label  predictions  thresholds  weight  includes= string  string             epsilon = 1e-7      def compute recall tp  fn  name         return math ops div tp  epsilon   tp   fn  name=string   name       def recall across replicas    value         return compute recall value string   value string   string       rec =  aggregate across replicas          metrics collections  recall across replicas  value       update op = compute recall update ops string   update ops string   string      if update collections        ops add to collections update collections  update op       return rec  update op 
 tf export v1= string   def recall at top k label                      predictions idx                      k=none                      class id=none                      weights=none                      metrics collections=none                      update collections=none                      name=none     string   with ops name scope name   at k name string  k  class id=class id                          predictions idx  label  weight   as scope      label =  maybe expand label label  predictions idx      top k idx = math ops cast predictions idx  dtypes int64      tp  tp update =  stream sparse true positive at k          predictions idx=top k idx          labels=labels          k=k          class id=class id          weights=weights      fn  fn update =  stream sparse false negative at k          predictions idx=top k idx          labels=labels          k=k          class id=class id          weights=weights       def compute recall    tp  fn         return math ops div tp  math ops add tp  fn   name=scope       metric =  aggregate across replicas          metrics collections  compute recall  tp  fn       update = math ops div          tp update  math ops add tp update  fn update   name=string      if update collections        ops add to collections update collections  update      return metric  update 
 tf export v1= string   def root mean square error label                              predictions                              weights=none                              metrics collections=none                              update collections=none                              name=none     string   if context execute eagerly        raise runtimeerror string                        string     predictions  label  weight =  remove squeezable dimension        predictions=predictions  labels=labels  weights=weights    mse  update mse op = mean square error label  predictions  weight  none                                            none  name or                                           string     once across replicas = lambda    mse  math ops sqrt mse    rmse =  aggregate across replicas        metrics collections  once across replicas  mse     update rmse op = math ops sqrt update mse op    if update collections      ops add to collections update collections  update rmse op     return rmse  update rmse op 
 tf export v1= string   def sensitivity at specificity label                                 predictions                                 specificity                                 weights=none                                 num thresholds=200                                 metrics collections=none                                 update collections=none                                 name=none     string   if context execute eagerly        raise runtimeerror string                        string     if specificity < 0 or specificity > 1      raise valueerror string     with variable scope variable scope name  string                                        predictions  label  weight        kepsilon = 1e-7       thresholds =            i   1    1 0 /  num thresholds - 1  for i in range num thresholds - 2            thresholds =  0 0 - kepsilon    thresholds    1 0   kepsilon       value  update ops =  confusion matrix at thresholds          label  predictions  thresholds  weight       def compute sensitivity at specificity tp  tn  fp  fn  name         specificities = math ops div tn  tn   fp   kepsilon        tf index = math ops argmin math ops abs specificities - specificity   0        tf index = math ops cast tf index  dtypes int32                return math ops div tp tf index   tp tf index    fn tf index    kepsilon                            name       def sensitivity across replicas    value         return compute sensitivity at specificity            value string   value string   value string   value string   string       sensitivity =  aggregate across replicas          metrics collections  sensitivity across replicas  value       update op = compute sensitivity at specificity          update ops string   update ops string   update ops string   update ops string           string      if update collections        ops add to collections update collections  update op       return sensitivity  update op 
 tf export v1= string    deprecate none  string  def sparse average precision at k label                                    predictions                                    k                                    weights=none                                    metrics collections=none                                    update collections=none                                    name=none     string   return average precision at k        labels=labels        predictions=predictions        k=k        weights=weights        metrics collections=metrics collections        update collections=updates collections        name=name  
 tf export v1= string    deprecate none  string  def sparse precision at k label                            predictions                            k                            class id=none                            weights=none                            metrics collections=none                            update collections=none                            name=none     string   return precision at k        labels=labels        predictions=predictions        k=k        class id=class id        weights=weights        metrics collections=metrics collections        update collections=updates collections        name=name  
 tf export v1= string   def specificity at sensitivity label                                 predictions                                 sensitivity                                 weights=none                                 num thresholds=200                                 metrics collections=none                                 update collections=none                                 name=none     string   if context execute eagerly        raise runtimeerror string                        string     if sensitivity < 0 or sensitivity > 1      raise valueerror string     with variable scope variable scope name  string                                        predictions  label  weight        kepsilon = 1e-7       thresholds =            i   1    1 0 /  num thresholds - 1  for i in range num thresholds - 2            thresholds =  0 0 - kepsilon    thresholds    1 0 - kepsilon       value  update ops =  confusion matrix at thresholds          label  predictions  thresholds  weight       def compute specificity at sensitivity tp  tn  fp  fn  name         string       sensitivities = math ops div tp  tp   fn   kepsilon                       min val = math ops reduce min math ops abs sensitivities - sensitivity         indices at minval = math ops equal            math ops abs sensitivities - sensitivity   min val        indices at minval = math ops cast indices at minval  dtypes int64        indices at minval = math ops cumsum indices at minval        tf index = math ops argmax indices at minval  0        tf index = math ops cast tf index  dtypes int32                return math ops div tn tf index   tn tf index    fp tf index    kepsilon                            name       def specificity across replicas    value         return compute specificity at sensitivity            value string   value string   value string   value string   string       specificity =  aggregate across replicas          metrics collections  specificity across replicas  value       update op = compute specificity at sensitivity          update ops string   update ops string   update ops string   update ops string           string      if update collections        ops add to collections update collections  update op       return specificity  update op 
 tf export v1= string   def true negative label                     predictions                     weights=none                     metrics collections=none                     update collections=none                     name=none     string   if context execute eagerly        raise runtimeerror string                        string     with variable scope variable scope name  string                                        predictions  label  weight         predictions  label  weight =  remove squeezable dimension          predictions=math ops cast predictions  dtype=dtypes bool           labels=math ops cast label  dtype=dtypes bool           weights=weights      be true negative = math ops logical and          math ops equal label  false   math ops equal predictions  false       return  count condition be true negative  weight  metrics collections                              update collections  
 tf export v1= string   def true negative at thresholds label                                   predictions                                   thresholds                                   weights=none                                   metrics collections=none                                   update collections=none                                   name=none     string   if context execute eagerly        raise runtimeerror string                        string     with variable scope variable scope name  string                                        predictions  label  weight        value  update ops =  confusion matrix at thresholds          label  predictions  thresholds  weights=weights  includes= string         tn value =  aggregate variable value string   metrics collections       if update collections        ops add to collections update collections  update ops string        return tn value  update ops string  
 tf export v1= string   def true positives label                     predictions                     weights=none                     metrics collections=none                     update collections=none                     name=none     string   if context execute eagerly        raise runtimeerror string                        string     with variable scope variable scope name  string                                        predictions  label  weight         predictions  label  weight =  remove squeezable dimension          predictions=math ops cast predictions  dtype=dtypes bool           labels=math ops cast label  dtype=dtypes bool           weights=weights      be true positive = math ops logical and          math ops equal label  true   math ops equal predictions  true       return  count condition be true positive  weight  metrics collections                              update collections  
 tf export v1= string   def true positives at thresholds label                                   predictions                                   thresholds                                   weights=none                                   metrics collections=none                                   update collections=none                                   name=none     string   if context execute eagerly        raise runtimeerror string                        string     with variable scope variable scope name  string                                        predictions  label  weight        value  update ops =  confusion matrix at thresholds          label  predictions  thresholds  weights=weights  includes= string         tp value =  aggregate variable value string   metrics collections       if update collections        ops add to collections update collections  update ops string        return tp value  update ops string  
 tf export v1= string  string   def avg pool value  ksize  stride  pad  data format=string               name=none  input=none       string   with ops name scope name  string   value   as name      value = deprecation deprecate argument lookup          string  input  string  value       if data format be none        data format = string     channel index = 1 if data format startswith string  else 3      ksize =  get sequence ksize  2  channel index  string      stride =  get sequence stride  2  channel index  string       return gen nn ops avg pool          value          ksize=ksize          strides=strides          padding=padding          data format=data format          name=name  
 tf export v1= string   def batch norm with global normalization t=none                                           m=none                                           v=none                                           beta=none                                           gamma=none                                           variance epsilon=none                                           scale after normalization=none                                           name=none                                           input=none                                             mean=none                                           variance=none     string   t = deprecate argument lookup string  input  string  t    m = deprecate argument lookup string  mean  string  m    v = deprecate argument lookup string  variance  string  v    return batch normalization t  m  v  beta  gamma if scale after normalization                              else none  variance epsilon  name  
 deprecation deprecate none  string                         string                         string   tf export v1= string   def bidirectional dynamic rnn cell fw                                cell bw                                input                                sequence length=none                                initial state fw=none                                initial state bw=none                                dtype=none                                parallel iterations=none                                swap memory=false                                time major=false                                scope=none     string   rnn cell impl assert like rnncell string  cell fw    rnn cell impl assert like rnncell string  cell bw     with vs variable scope scope or string            with vs variable scope string  as fw scope        output fw  output state fw = dynamic rnn            cell=cell fw            inputs=inputs            sequence length=sequence length            initial state=initial state fw            dtype=dtype            parallel iterations=parallel iterations            swap memory=swap memory            time major=time major            scope=fw scope            if not time major        time axis = 1       batch axis = 0     else        time axis = 0       batch axis = 1      def  reverse input   seq lengths  seq axis  batch axis         if seq lengths be not none          return array ops reverse sequence              input=input               seq lengths=seq lengths              seq axis=seq axis              batch axis=batch axis        else          return array ops reverse input   axis= seq axis        with vs variable scope string  as bw scope         def  map reverse inp           return  reverse              inp              seq lengths=sequence length              seq axis=time axis              batch axis=batch axis         input reverse = nest map structure  map reverse  input        tmp  output state bw = dynamic rnn            cell=cell bw            inputs=inputs reverse            sequence length=sequence length            initial state=initial state bw            dtype=dtype            parallel iterations=parallel iterations            swap memory=swap memory            time major=time major            scope=bw scope     output bw =  reverse        tmp        seq lengths=sequence length        seq axis=time axis        batch axis=batch axis     output =  output fw  output bw    output state =  output state fw  output state bw     return  output  output state  
 tf export v1= string    deprecation deprecate arg value      none      string      warn once=true      data format=string   deprecation deprecate arg value      none      string      warn once=true      data format=string  def conv1d      value=none      filters=none      stride=none      padding=none      use cudnn on gpu=none      data format=none      name=none      input=none        dilations=none     r   compute a 1-d convolution give 3-d input and filter tensors     give an input tensor of shape      batch  in width  in channel    if data format be  nwc   or      batch  in channel  in width    if data format be  ncw     and a filter / kernel tensor of shape    filter width  in channel  out channel   this op reshape   the arguments to pass them to conv2d to perform the equivalent   convolution operation     internally  this op reshape the input tensors and invoke `tf nn conv2d`    for example  if `data format` do not start with  nc   a tensor of shape      batch  in width  in channel    be reshape to      batch  1  in width  in channel     and the filter be reshape to      1  filter width  in channel  out channel     the result be then reshape back to      batch  out width  out channel    \ where out width be a function of the stride and pad as in conv2d\  and   return to the caller     args      value  a 3d `tensor`   must be of type `float16`  `float32`  or `float64`      filter  a 3d `tensor`   must have the same type as `value`      stride  an int or list of `ints` that have length `1` or `3`   the number of       entries by which the filter be move right at each step      pad   same  or  valid      use cudnn on gpu  an optional `bool`   default to `true`      data format  an optional `string` from ` nwc    ncw `   default to ` nwc `        the data be store in the order of  batch  in width  in channel    the       ` ncw ` format store data as  batch  in channel  in width       name  a name for the operation  optional       input  alias for value      dilations  an int or list of `ints` that have length `1` or `3` which       default to 1  the dilation factor for each dimension of input  if set to       k > 1  there will be k-1 skip cells between each filter element on that       dimension  dilations in the batch and depth dimension must be 1     return      a `tensor`   have the same type as input     raise      valueerror  if `data format` be invalid          value = deprecation deprecate argument lookup string  input  string  value    with ops name scope name  string   value  filter   as name           if data format be none or data format == string or data format == string        data format = string       spatial start dim = 1       channel index = 2     elif data format == string or data format == string        data format = string       spatial start dim = 2       channel index = 1     else        raise valueerror string      stride =  1     get sequence stride  1  channel index  string      dilations =  1     get sequence dilations  1  channel index  string       value = array ops expand dim value  spatial start dim      filter = array ops expand dim filter  0      result = gen nn ops conv2d          value          filter          stride          pad          use cudnn on gpu=use cudnn on gpu          data format=data format          dilations=dilations          name=name      return array ops squeeze result   spatial start dim   
 tf export v1= string   def conv2d        input      filter=none      strides=none      padding=none      use cudnn on gpu=true      data format=string      dilations= 1  1  1  1       name=none      filters=none     r   compute a 2-d convolution give 4-d `input` and `filter` tensors     give an input tensor of shape ` batch  in height  in width  in channel `   and a filter / kernel tensor of shape   ` filter height  filter width  in channel  out channel `  this op   perform the follow     1  flatten the filter to a 2-d matrix with shape      ` filter height   filter width   in channel  output channel `    2  extract image patch from the input tensor to form a  virtual       tensor of shape ` batch  out height  out width       filter height   filter width   in channel `    3  for each patch  right-multiplies the filter matrix and the image patch      vector     in detail  with the default nhwc format         output b  i  j  k  =           sum  di  dj  q  input b  stride 1    i   di  stride 2    j   dj  q                              filter di  dj  q  k     must have `strides 0  = stride 3  = 1`   for the most common case of the same   horizontal and vertices stride  `strides =  1  stride  stride  1 `     args      input  a `tensor`  must be one of the follow type        `half`  `bfloat16`  `float32`  `float64`        a 4-d tensor  the dimension order be interpret accord to the value       of `data format`  see below for detail      filter  a `tensor`  must have the same type as `input`        a 4-d tensor of shape       ` filter height  filter width  in channel  out channel `     stride  an int or list of `ints` that have length `1`  `2` or `4`   the       stride of the slide window for each dimension of `input`  if a single       value be give it be replicate in the `h` and `w` dimension  by default       the `n` and `c` dimension be set to 1  the dimension order be determine       by the value of `data format`  see below for detail      pad  either the `string` ` same ` or ` valid ` indicate the type of       pad algorithm to use  or a list indicate the explicit paddings at       the start and end of each dimension  when explicit pad be use and       data format be ` nhwc `  this should be in the form `  0  0    pad top        pad bottom    pad leave  pad right    0  0  `  when explicit pad use       and data format be ` nchw `  this should be in the form `  0  0    0  0          pad top  pad bottom    pad leave  pad right  `      use cudnn on gpu  an optional `bool`  default to `true`      data format  an optional `string` from  ` nhwc    nchw `        default to ` nhwc `        specify the data format of the input and output data  with the       default format  nhwc   the data be store in the order of             batch  height  width  channel         alternatively  the format could be  nchw   the data storage order of             batch  channel  height  width       dilations  an int or list of `ints` that have length `1`  `2` or `4`        default to 1  the dilation factor for each dimension of`input`  if a       single value be give it be replicate in the `h` and `w` dimension  by       default the `n` and `c` dimension be set to 1  if set to k > 1  there       will be k-1 skip cells between each filter element on that dimension        the dimension order be determine by the value of `data format`  see above       for detail  dilations in the batch and depth dimension if a 4-d tensor       must be 1      name  a name for the operation  optional       filter  alias for filter     return      a `tensor`  have the same type as `input`          filter = deprecation deprecate argument lookup        string  filter  string  filter    pad  explicit paddings =  convert pad pad    if data format be none      data format = string   channel index = 1 if data format startswith string  else 3    stride =  get sequence stride  2  channel index  string    dilations =  get sequence dilations  2  channel index  string    return gen nn ops conv2d input                               filter                             stride                             pad                             use cudnn on gpu=use cudnn on gpu                             explicit paddings=explicit paddings                             data format=data format                             dilations=dilations                             name=name  
 tf export v1= string   def conv2d backprop filter        input      filter size      out backprop      stride      pad      use cudnn on gpu=true      data format=string      dilations= 1  1  1  1       name=none     r   compute the gradients of convolution with respect to the filter     args      input  a `tensor`  must be one of the follow type        `half`  `bfloat16`  `float32`  `float64`        4-d with shape ` batch  in height  in width  in channel `      filter size  a `tensor` of type `int32`        an integer vector represent the tensor shape of `filter`        where `filter` be a 4-d       ` filter height  filter width  in channel  out channel ` tensor      out backprop  a `tensor`  must have the same type as `input`        4-d with shape ` batch  out height  out width  out channel `        gradients w r t  the output of the convolution      stride  a list of `ints`        the stride of the slide window for each dimension of the input       of the convolution  must be in the same order as the dimension specify       with format      pad  either the `string ` same ` or ` valid ` indicate the type of       pad algorithm to use  or a list indicate the explicit paddings at       the start and end of each dimension  when explicit pad be use and       data format be ` nhwc `  this should be in the form `  0  0    pad top        pad bottom    pad leave  pad right    0  0  `  when explicit pad use       and data format be ` nchw `  this should be in the form `  0  0    0  0          pad top  pad bottom    pad leave  pad right  `      use cudnn on gpu  an optional `bool`  default to `true`      data format  an optional `string` from  ` nhwc    nchw `        default to ` nhwc `        specify the data format of the input and output data  with the       default format  nhwc   the data be store in the order of             batch  in height  in width  in channel         alternatively  the format could be  nchw   the data storage order of             batch  in channel  in height  in width       dilations  an optional list of `ints`  default to ` 1  1  1  1 `        1-d tensor of length 4   the dilation factor for each dimension of       `input`  if set to k > 1  there will be k-1 skip cells between each       filter element on that dimension  the dimension order be determine by       the value of `data format`  see above for detail  dilations in the batch       and depth dimension must be 1      name  a name for the operation  optional      return      a `tensor`  have the same type as `input`          pad  explicit paddings =  convert pad pad    return gen nn ops conv2d backprop filter        input  filter size  out backprop  stride  pad  use cudnn on gpu        explicit paddings  data format  dilations  name  
 tf export v1= string   def conv2d backprop input        input size      filter=none      out backprop=none      strides=none      padding=none      use cudnn on gpu=true      data format=string      dilations= 1  1  1  1       name=none      filters=none     r   compute the gradients of convolution with respect to the input     args      input size  a `tensor` of type `int32`        an integer vector represent the shape of `input`        where `input` be a 4-d ` batch  height  width  channel ` tensor      filter  a `tensor`  must be one of the follow type        `half`  `bfloat16`  `float32`  `float64`        4-d with shape       ` filter height  filter width  in channel  out channel `      out backprop  a `tensor`  must have the same type as `filter`        4-d with shape ` batch  out height  out width  out channel `        gradients w r t  the output of the convolution      stride  a list of `ints`        the stride of the slide window for each dimension of the input       of the convolution  must be in the same order as the dimension specify       with format      pad  either the `string ` same ` or ` valid ` indicate the type of       pad algorithm to use  or a list indicate the explicit paddings at       the start and end of each dimension  when explicit pad be use and       data format be ` nhwc `  this should be in the form `  0  0    pad top        pad bottom    pad leave  pad right    0  0  `  when explicit pad use       and data format be ` nchw `  this should be in the form `  0  0    0  0          pad top  pad bottom    pad leave  pad right  `      use cudnn on gpu  an optional `bool`  default to `true`      data format  an optional `string` from  ` nhwc    nchw `        default to ` nhwc `        specify the data format of the input and output data  with the       default format  nhwc   the data be store in the order of             batch  in height  in width  in channel         alternatively  the format could be  nchw   the data storage order of             batch  in channel  in height  in width       dilations  an optional list of `ints`  default to ` 1  1  1  1 `        1-d tensor of length 4   the dilation factor for each dimension of       `input`  if set to k > 1  there will be k-1 skip cells between each       filter element on that dimension  the dimension order be determine by       the value of `data format`  see above for detail  dilations in the batch       and depth dimension must be 1      name  a name for the operation  optional       filter  alias for filter     return      a `tensor`  have the same type as `filter`          filter = deprecation deprecate argument lookup        string  filter  string  filter    pad  explicit paddings =  convert pad pad    return gen nn ops conv2d backprop input        input size  filter  out backprop  stride  pad  use cudnn on gpu        explicit paddings  data format  dilations  name  
 tf export v1= string   def conv2d transpose      value=none      filter=none        output shape=none      strides=none      padding=string      data format=string      name=none      input=none        filters=none      dilations=none     string   value = deprecate argument lookup string  input  string  value    filter = deprecate argument lookup string  filter  string  filter    with ops name scope name  string                         value  filter  output shape   as name      return conv2d transpose v2          value          filter          output shape          stride          padding=padding          data format=data format          dilations=dilations          name=name  
 tf export v1= string   def conv3d v1        input        filter=none        strides=none      padding=none      data format=string      dilations= 1  1  1  1  1       name=none      filters=none     filter = deprecate argument lookup string  filter  string  filter    return gen nn ops conv3d        input  filter  stride  pad  data format  dilations  name  
  dispatch add dispatch list  tf export v1= string  string    deprecate endpoints string  string  def conv3d backprop filter v2 input  filter size  out backprop  stride  pad  data format=string  dilations= 1  1  1  1  1   name=none     r   compute the gradients of 3-d convolution with respect to the filter     args      input  a `tensor`  must be one of the follow type  `half`  `bfloat16`  `float32`  `float64`        shape ` batch  depth  row  cols  in channel `      filter size  a `tensor` of type `int32`        an integer vector represent the tensor shape of `filter`        where `filter` be a 5-d       ` filter depth  filter height  filter width  in channel  out channel `       tensor      out backprop  a `tensor`  must have the same type as `input`        backprop signal of shape ` batch  out depth  out row  out cols        out channel `      stride  a list of `ints` that have length `>= 5`        1-d tensor of length 5  the stride of the slide window for each       dimension of `input`  must have `strides 0  = stride 4  = 1`      pad  a `string` from  ` same    valid `        the type of pad algorithm to use      data format  an optional `string` from  ` ndhwc    ncdhw `  default to ` ndhwc `        the data format of the input and output data  with the       default format  ndhwc   the data be store in the order of             batch  in depth  in height  in width  in channel         alternatively  the format could be  ncdhw   the data storage order be             batch  in channel  in depth  in height  in width       dilations  an optional list of `ints`  default to ` 1  1  1  1  1 `        1-d tensor of length 5   the dilation factor for each dimension of       `input`  if set to k > 1  there will be k-1 skip cells between each       filter element on that dimension  the dimension order be determine by the       value of `data format`  see above for detail  dilations in the batch and       depth dimension must be 1      name  a name for the operation  optional      return      a `tensor`  have the same type as `input`           ctx =  context  context or  context context     tld =  ctx  thread local data   if tld be eager      try         result =  pywrap tensorflow tfe py fastpathexecute           ctx  context handle  tld device name  string  name          tld op callbacks  input  filter size  out backprop  string          stride  string  pad  string  data format  string          dilations        return  result     except  core  fallbackexception        try          return conv3d backprop filter v2 eager fallback              input  filter size  out backprop  strides=strides              padding=padding  data format=data format  dilations=dilations              name=name  ctx= ctx        except  core  symbolicexception          pass         except  typeerror  valueerror           result =  dispatch dispatch                conv3d backprop filter v2  input=input                                           filter sizes=filter size                                           out backprop=out backprop                                           strides=strides  padding=padding                                           data format=data format                                           dilations=dilations  name=name          if result be not  dispatch opdispatcher not support            return result         raise     except  core  notokstatusexception as e         ops raise from not ok status e  name       if not isinstance stride   list  tuple        raise typeerror          string         string   stride    stride =   execute make int  i  string  for  i in stride    pad =  execute make str pad  string    if data format be none      data format = string   data format =  execute make str data format  string    if dilations be none      dilations =  1  1  1  1  1    if not isinstance dilations   list  tuple        raise typeerror          string         string   dilations    dilations =   execute make int  i  string  for  i in dilations    try             op   output =  op def library  apply op helper          string  input=input  filter sizes=filter size                                    out backprop=out backprop  strides=strides                                    padding=padding  data format=data format                                    dilations=dilations  name=name    except  typeerror  valueerror       result =  dispatch dispatch            conv3d backprop filter v2  input=input  filter sizes=filter size                                       out backprop=out backprop                                       strides=strides  padding=padding                                       data format=data format                                       dilations=dilations  name=name      if result be not  dispatch opdispatcher not support        return result     raise    result =  output      if  execute must record gradient         attrs =  string   op  get attr type string   string                 op get attr string   string   op get attr string                 string   op get attr string   string                 op get attr string        input flat =  op input      execute record gradient          string   input flat   attrs   result     result  =  result   return  result 
 tf export v1= string   def conv3d transpose      value      filter=none        output shape=none      strides=none      padding=string      data format=string      name=none      input=none        filters=none      dilations=none     string   filter = deprecate argument lookup string  filter  string  filter    value = deprecate argument lookup string  input  string  value    return conv3d transpose v2        value        filter        output shape        stride        padding=padding        data format=data format        dilations=dilations        name=name  
 tf export v1= string   def convolution      input        filter        pad      strides=none      dilation rate=none      name=none      data format=none      filters=none      dilations=none     string   filter = deprecate argument lookup string  filter  string  filter    dilation rate = deprecate argument lookup        string  dilations  string  dilation rate    return convolution internal        input        filter        strides=strides        padding=padding        data format=data format        dilations=dilation rate        name=name  
 tf export v1= string   def crelu feature  name=none  axis=-1     string   with ops name scope name  string   feature   as name      feature = ops convert to tensor feature  name=string      c = array ops concat  feature  -features   axis  name=name      return gen nn ops relu c  
 tf export v1= string   def ctc beam search decoder input                              sequence length                              beam width=100                              top paths=1                              merge repeated=true     string    decode ixs  decode vals  decode shape  log probabilities =         gen ctc ops ctc beam search decoder            input            sequence length            beam width=beam width            top paths=top paths            merge repeated=merge repeat      return          sparse tensor sparsetensor ix  val  shape        for  ix  val  shape  in zip decode ixs  decode vals  decode shape       log probabilities  
 tf export v1= string   def ctc loss label               inputs=none               sequence length=none               preprocess collapse repeated=false               ctc merge repeated=true               ignore longer output than inputs=false               time major=true               logits=none     string         if not isinstance label  sparse tensor sparsetensor       raise typeerror string        input = deprecation deprecate argument lookup string  logits  string                                                    input    if not time major      input = array ops transpose input   1  0  2        loss    = gen ctc ops ctc loss        input        label indices        label value        sequence length        preprocess collapse repeated=preprocess collapse repeat        ctc merge repeated=ctc merge repeat        ignore longer output than inputs=ignore longer output than input     return loss 
 tf export string  v1= string   def ctc loss v2 label                  logits                  label length                  logit length                  logits time major=true                  unique=none                  blank index=none                  name=none     string   if isinstance label  sparse tensor sparsetensor       if blank index be none        raise valueerror            string       if blank index < 0        blank index  =  get dim logits  2       if blank index  =  get dim logits  2  - 1        logits = array ops concat             logits        blank index             logits       blank index   1              logits       blank index blank index   1                                            axis=2        label = sparse tensor sparsetensor            label indices            array ops where label value < blank index  label value                            label value - 1   label dense shape       return ctc loss          labels=labels          inputs=logits          sequence length=logit length          time major=logits time major     if blank index be none      blank index = 0    return ctc loss dense        labels=labels        logits=logits        label length=label length        logit length=logit length        logits time major=logits time major        unique=unique        blank index=blank index        name=name  
 tf export v1= string   def depthwise conv2d input                       filter                       stride                       pad                       rate=none                       name=none                       data format=none                       dilations=none     string   rate = deprecate argument lookup string  dilations  string  rate    with ops name scope name  string   input  filter   as name      input = ops convert to tensor input  name=string      filter = ops convert to tensor filter  name=string      if rate be none        rate =  1  1                           if  enclose tpu context   be not none        if data format == string          dilations =  1  1  rate 0   rate 1         else          dilations =  1  rate 0   rate 1   1        return nn ops depthwise conv2d native            input=input            filter=filter            strides=strides            padding=padding            data format=data format            dilations=dilations            name=name            def op input convert     pad         return nn ops depthwise conv2d native            input=input convert            filter=filter            strides=strides            padding=padding            data format=data format            name=name       return nn ops with space to batch          input=input          filter shape=array ops shape filter           dilation rate=rate          padding=padding          data format=data format          op=op  
  dispatch add dispatch list  tf export v1= string    deprecate endpoints string  def depthwise conv2d native input  filter  stride  pad  data format=string  dilations= 1  1  1  1   name=none     r   compute a 2-d depthwise convolution give 4-d `input` and `filter` tensors     give an input tensor of shape ` batch  in height  in width  in channel `   and a filter / kernel tensor of shape   ` filter height  filter width  in channel  channel multiplier `  contain   `in channels` convolutional filter of depth 1  `depthwise conv2d` apply   a different filter to each input channel  expand from 1 channel to   `channel multiplier` channel for each   then concatenate the result   together  thus  the output have `in channel   channel multiplier` channel     ```   for k in 0  in channels-1     for q in 0  channel multiplier-1       output b  i  j  k   channel multiplier   q  =         sum  di  dj  input b  stride 1    i   di  stride 2    j   dj  k                              filter di  dj  k  q    ```    must have `strides 0  = stride 3  = 1`   for the most common case of the same   horizontal and vertices stride  `strides =  1  stride  stride  1 `     args      input  a `tensor`  must be one of the follow type  `half`  `bfloat16`  `float32`  `float64`      filter  a `tensor`  must have the same type as `input`      stride  a list of `ints`        1-d of length 4   the stride of the slide window for each dimension       of `input`      pad  a `string` from  ` same    valid `        the type of pad algorithm to use      data format  an optional `string` from  ` nhwc    nchw `  default to ` nhwc `        specify the data format of the input and output data  with the       default format  nhwc   the data be store in the order of             batch  height  width  channel         alternatively  the format could be  nchw   the data storage order of             batch  channel  height  width       dilations  an optional list of `ints`  default to ` 1  1  1  1 `        1-d tensor of length 4   the dilation factor for each dimension of       `input`  if set to k > 1  there will be k-1 skip cells between each filter       element on that dimension  the dimension order be determine by the value of       `data format`  see above for detail  dilations in the batch and depth       dimension must be 1      name  a name for the operation  optional      return      a `tensor`  have the same type as `input`           ctx =  context  context or  context context     tld =  ctx  thread local data   if tld be eager      try         result =  pywrap tensorflow tfe py fastpathexecute           ctx  context handle  tld device name  string  name          tld op callbacks  input  filter  string  stride  string          pad  string  data format  string  dilations        return  result     except  core  fallbackexception        try          return depthwise conv2d native eager fallback              input  filter  strides=strides  padding=padding              data format=data format  dilations=dilations  name=name  ctx= ctx        except  core  symbolicexception          pass         except  typeerror  valueerror           result =  dispatch dispatch                depthwise conv2d native  input=input  filter=filter                                         strides=strides  padding=padding                                         data format=data format                                         dilations=dilations  name=name          if result be not  dispatch opdispatcher not support            return result         raise     except  core  notokstatusexception as e         ops raise from not ok status e  name       if not isinstance stride   list  tuple        raise typeerror          string         string   stride    stride =   execute make int  i  string  for  i in stride    pad =  execute make str pad  string    if data format be none      data format = string   data format =  execute make str data format  string    if dilations be none      dilations =  1  1  1  1    if not isinstance dilations   list  tuple        raise typeerror          string         string   dilations    dilations =   execute make int  i  string  for  i in dilations    try             op   output =  op def library  apply op helper          string  input=input  filter=filter  strides=strides                                   padding=padding  data format=data format                                   dilations=dilations  name=name    except  typeerror  valueerror       result =  dispatch dispatch            depthwise conv2d native  input=input  filter=filter                                     strides=strides  padding=padding                                     data format=data format                                     dilations=dilations  name=name      if result be not  dispatch opdispatcher not support        return result     raise    result =  output      if  execute must record gradient         attrs =  string   op  get attr type string   string                 op get attr string   string   op get attr string                 string   op get attr string   string                 op get attr string        input flat =  op input      execute record gradient          string   input flat   attrs   result     result  =  result   return  result 
 tf export v1= string   def dilation2d v1        input        filter=none        strides=none      rates=none      padding=none      name=none      filters=none      dilations=none     filter = deprecate argument lookup string  filter  string  filter    rat = deprecate argument lookup string  dilations  string  rat    return gen nn ops dilation2d input  filter  stride  rat  pad  name  
 tf export v1= string    deprecation deprecate args none  string                              string                               string  def dropout x  keep prob=none  noise shape=none  seed=none  name=none              rate=none     string   try      keep = 1  - keep prob if keep prob be not none else none   except typeerror      raise valueerror string                      string   keep prob     rate = deprecation deprecate argument lookup        string  rate        string  keep     if rate be none      raise valueerror string     return dropout v2 x  rate  noise shape=noise shape  seed=seed  name=name  
 deprecation deprecate      none      string   tf export v1= string   def dynamic rnn cell                  input                  sequence length=none                  initial state=none                  dtype=none                  parallel iterations=none                  swap memory=false                  time major=false                  scope=none     string   rnn cell impl assert like rnncell string  cell     with vs variable scope scope or string  as varscope                     if  should cache          if varscope cache device be none          varscope set cache device lambda op  op device                      flat input = nest flatten input       if not time major               flat input =  ops convert to tensor input   for input  in flat input        flat input = tuple  transpose batch time input   for input  in flat input       parallel iterations = parallel iterations or 32     if sequence length be not none        sequence length = math ops cast sequence length  dtypes int32        if sequence length get shape   rank not in  none  1           raise valueerror              string             string   sequence length get shape          sequence length = array ops identity              sequence length            name=string       batch size =  best effort input batch size flat input       if initial state be not none        state = initial state     else        if not dtype          raise valueerror string        if getattr cell  string  none  be not none          state = cell get initial state              inputs=none  batch size=batch size  dtype=dtype        else          state = cell zero state batch size  dtype       def  assert have shape x  shape         x shape = array ops shape x        pack shape = array ops stack shape        return control flow ops assert            math ops reduce all math ops equal x shape  pack shape                    string   x name  pack shape                string  x shape                   if not context execute eagerly   and sequence length be not none               with ops control dependencies              assert have shape sequence length   batch size              sequence length = array ops identity              sequence length  name=string       input = nest pack sequence as structure=inputs  flat sequence=flat input        output  final state  =  dynamic rnn loop          cell          input          state          parallel iterations=parallel iterations          swap memory=swap memory          sequence length=sequence length          dtype=dtype                      if not time major               output = nest map structure  transpose batch time  output       return  output  final state  
 tf export v1= string   def embed lookup      params      ids      partition strategy=string      name=none      validate indices=true        max norm=none     string   if isinstance ids  rag tensor raggedtensor       return embed lookup rag params  ids     return  embed lookup and transform        params=params        ids=ids        partition strategy=partition strategy        name=name        max norm=max norm        transform fn=none  
 tf export v1= string   def embed lookup sparse params                              sp ids                              sp weight                              partition strategy=string                              name=none                              combiner=none                              max norm=none     string   if combiner be none      log warn string                  string      combiner = string   if combiner not in  string  string  string       raise valueerror string    if isinstance params  variables partitionedvariable       params = list params      if not isinstance params  list       params =  params    if not isinstance sp ids  sparse tensor sparsetensor       raise typeerror string    ignore weight = sp weight be none   if not ignore weight      if not isinstance sp weight  sparse tensor sparsetensor         raise typeerror string      sp ids value get shape   assert be compatible with          sp weight value get shape        sp ids indices get shape   assert be compatible with          sp weight indices get shape        sp ids dense shape get shape   assert be compatible with          sp weight dense shape get shape                 with ops name scope name  string                        params    sp ids   as name      segment ids = sp ids indices    0      if segment ids dtype  = dtypes int32        segment ids = math ops cast segment ids  dtypes int32       ids = sp ids value     ids  idx = array ops unique ids       embeddings = embed lookup          params  ids  partition strategy=partition strategy  max norm=max norm      if embeddings dtype in  dtypes float16  dtypes bfloat16         embeddings = math ops cast embeddings  dtypes float32      if not ignore weight        weight = sp weight value       if weight dtype  = embeddings dtype          weight = math ops cast weight  embeddings dtype         embeddings = array ops gather embeddings  idx                ones = array ops fill            array ops expand dim array ops rank embeddings  - 1  0   1        bcast weight shape = array ops concat  array ops shape weight   ones                                                0         orig weight shape = weight get shape         weight = array ops reshape weight  bcast weight shape                       if embeddings get shape   ndims be not none          weight set shape              orig weight shape concatenate                   1 for   in range embeddings get shape   ndims - 1            embeddings  = weight        if combiner == string          embeddings = math ops segment sum embeddings  segment ids  name=name        elif combiner == string          embeddings = math ops segment sum embeddings  segment ids          weight sum = math ops segment sum weight  segment ids          embeddings = math ops div embeddings  weight sum  name=name        elif combiner == string          embeddings = math ops segment sum embeddings  segment ids          weight square = math ops pow weight  2          weight sum = math ops segment sum weight square  segment ids          weight sum sqrt = math ops sqrt weight sum          embeddings = math ops div embeddings  weight sum sqrt  name=name        else          assert false  string     else        assert idx be not none       if combiner == string          embeddings = math ops sparse segment sum              embeddings  idx  segment ids  name=name        elif combiner == string          embeddings = math ops sparse segment mean              embeddings  idx  segment ids  name=name        elif combiner == string          embeddings = math ops sparse segment sqrt n              embeddings  idx  segment ids  name=name        else          assert false  string      return embeddings 
 tf export v1= string   def erosion2d value  kernel  stride  rat  pad  name=none     string   with ops name scope name  string   value  kernel   as name           return math ops negative          gen nn ops dilation2d              input=math ops negative value               filter=array ops reverse v2 kernel   0  1                strides=strides              rates=rates              padding=padding              name=name   
 tf export v1= string    deprecation deprecate date=none  instructions=string                         string  def fractional avg pool value                          pool ratio                          pseudo random=false                          overlapping=false                          deterministic=false                          seed=0                          seed2=0                          name=none       r   perform fractional average pool on the input     this be a deprecate version of `fractional avg pool`     fractional average pool be similar to fractional max pool in the pool   region generation step  the only difference be that after pool regions be   generate  a mean operation be perform instead of a max operation in each   pool region     args      value  a `tensor`  4-d with shape ` batch  height  width  channel `      pool ratio  a list of `floats` that have length >= 4   pool ratio for       each dimension of `value`  currently only support row and col dimension       and should be >= 1 0  for example  a valid pool ratio look like  1 0        1 44  1 73  1 0   the first and last elements must be 1 0 because we don t       allow pool on batch and channel dimension   1 44 and 1 73 be pool       ratio on height and width dimension respectively      pseudo random  an optional `bool`   default to `false`  when set to `true`        generate the pool sequence in a pseudorandom fashion  otherwise  in a       random fashion  check paper  benjamin graham  fractional       max-pooling  http //arxiv org/abs/1412 6071  for difference between       pseudorandom and random      overlap  an optional `bool`   default to `false`   when set to `true`        it mean when pool  the value at the boundary of adjacent pool       cells be use by both cells  for example        `index  0  1  2  3  4`       `value  20 5  16 3  7`       if the pool sequence be  0  2  4   then 16  at index 2 will be use       twice   the result would be  20  16  for fractional avg pool      deterministic  an optional `bool`   deprecate  use `fractional avg pool v2`       instead      seed  an optional `int`   default to `0`   if set to be non-zero  the       random number generator be seed by the give seed   otherwise it be       seed by a random seed      seed2  an optional `int`   deprecate  use `fractional avg pool v2` instead      name  a name for the operation  optional      return    a tuple of `tensor` object  `output`  `row pool sequence`    `col pool sequence`       output  output `tensor` after fractional avg pool   have the same type as       `value`      row pool sequence  a `tensor` of type `int64`      col pool sequence  a `tensor` of type `int64`          return gen nn ops fractional avg pool value  pool ratio  pseudo random                                          overlap  deterministic  seed  seed2                                          name=name  
 tf export v1= string    deprecation deprecate date=none  instructions=string                         string  def fractional max pool value                          pool ratio                          pseudo random=false                          overlapping=false                          deterministic=false                          seed=0                          seed2=0                          name=none        r   perform fractional max pool on the input     this be a deprecate version of `fractional max pool`     fractional max pool be slightly different than regular max pool   in   regular max pool  you downsize an input set by take the maximum value of   smaller n x n subsections of the set  often 2x2   and try to reduce the set by   a factor of n  where n be an integer   fractional max pool  as you might   expect from the word  fractional   mean that the overall reduction ratio n   do not have to be an integer     the size of the pool regions be generate randomly but be fairly   uniform   for example  let s look at the height dimension  and the constraints   on the list of row that will be pool boundaries     first we define the follow     1   input row length   the number of row from the input set   2   output row length   which will be smaller than the input   3   alpha = input row length / output row length   our reduction ratio   4   k = floor alpha    5   row pool sequence   this be the result list of pool boundary row    then  row pool sequence should satisfy     1   a 0  = 0   the first value of the sequence be 0   2   a end  = input row length   the last value of the sequence be the size   3   k <=  a i 1  - a i   <= k 1   all intervals be k or k 1 size   4   length row pool sequence  = output row length 1    for more detail on fractional max pool  see this paper   benjamin graham    fractional max-pooling  http //arxiv org/abs/1412 6071     args      value  a `tensor`  4-d with shape ` batch  height  width  channel `      pool ratio  a list of `floats` that have length >= 4   pool ratio for       each dimension of `value`  currently only support row and col dimension       and should be >= 1 0  for example  a valid pool ratio look like  1 0        1 44  1 73  1 0   the first and last elements must be 1 0 because we don t       allow pool on batch and channel dimension   1 44 and 1 73 be pool       ratio on height and width dimension respectively      pseudo random  an optional `bool`   default to `false`  when set to `true`        generate the pool sequence in a pseudorandom fashion  otherwise  in a       random fashion  check paper  benjamin graham  fractional       max-pooling  http //arxiv org/abs/1412 6071  for difference between       pseudorandom and random      overlap  an optional `bool`   default to `false`   when set to `true`        it mean when pool  the value at the boundary of adjacent pool       cells be use by both cells  for example        `index  0  1  2  3  4`       `value  20 5  16 3  7`       if the pool sequence be  0  2  4   then 16  at index 2 will be use       twice   the result would be  20  16  for fractional max pool      deterministic  an optional `bool`   deprecate  use `fractional max pool v2`       instead      seed  an optional `int`   default to `0`   if set to be non-zero  the       random number generator be seed by the give seed   otherwise it be       seed by a random seed      seed2  an optional `int`   deprecate  use `fractional max pool v2` instead      name  a name for the operation  optional      return    a tuple of `tensor` object  `output`  `row pool sequence`    `col pool sequence`       output  output `tensor` after fractional max pool   have the same type as       `value`      row pool sequence  a `tensor` of type `int64`      col pool sequence  a `tensor` of type `int64`          return gen nn ops fractional max pool value  pool ratio  pseudo random                                          overlap  deterministic  seed  seed2                                          name  
 tf export v1= string   def fuse batch norm      x      scale      offset        mean=none      variance=none      epsilon=0 001      data format=string      be training=true      name=none     r   batch normalization     see source   batch normalization  accelerate deep network train by   reduce internal covariate shift  s  ioffe  c  szegedy     http //arxiv org/abs/1502 03167      args      x  input `tensor` of 4 dimension      scale  a `tensor` of 1 dimension for scale      offset  a `tensor` of 1 dimension for bias      mean  a `tensor` of 1 dimension for population mean use for inference      variance  a `tensor` of 1 dimension for population variance               use for inference      epsilon  a small float number add to the variance of x      data format  the data format for x  either  nhwc   default  or  nchw       be train  a bool value to specify if the operation be use for                  train or inference      name  a name for this operation  optional      return      y  a 4d tensor for the normalize  scale  offsetted x      batch mean  a 1d tensor for the mean of x      batch var  a 1d tensor for the variance of x     raise      valueerror  if mean or variance be not none when be train be true          x = ops convert to tensor x  name=string    scale = ops convert to tensor scale  name=string    offset = ops convert to tensor offset  name=string    if be train      if  mean be not none  or  variance be not none         raise valueerror string                        string    if mean be none      mean = constant op constant       if variance be none      variance = constant op constant             min epsilon = 1 001e-5   epsilon = epsilon if epsilon > min epsilon else min epsilon    y  batch mean  batch var          = gen nn ops fuse batch norm v3        x        scale        offset        mean        variance        epsilon=epsilon        data format=data format        be training=is train        name=name    return y  batch mean  batch var 
 tf export v1= string   def max pool value               ksize               stride               pad               data format=string               name=none               input=none       string   value = deprecation deprecate argument lookup string  input  string  value    with ops name scope name  string   value   as name      if data format be none        data format = string     channel index = 1 if data format startswith string  else 3      ksize =  get sequence ksize  2  channel index  string      stride =  get sequence stride  2  channel index  string      if   np isscalar ksize  and ksize == 0  or          isinstance ksize                       list  tuple  np ndarray   and any v == 0 for v in ksize           raise valueerror string       return gen nn ops max pool          value          ksize=ksize          strides=strides          padding=padding          data format=data format          name=name  
 tf export v1= string   def max pool with argmax v1        input        ksize      stride      pad      data format=string      targmax=none      name=none      output dtype=none      include batch in index=false     if data format  = string      raise valueerror string     targmax = deprecate argument lookup        string  output dtype  string  targmax    if targmax be none      targmax = dtypes int64   return gen nn ops max pool with argmax        input=input        ksize=ksize        strides=strides        padding=padding        targmax=targmax        include batch in index=include batch in index        name=name  
 tf export v1= string   def moments      x      ax      shift=none        name=none      keep dims=none      keepdims=none     string   keep dim = deprecate argument lookup        string  keepdims  string  keep dim    if keep dim be none      keep dim = false   with ops name scope name  string   x  ax                       y = math ops cast x  dtypes float32  if x dtype == dtypes float16 else x          mean = math ops reduce mean y  ax  keepdims=true  name=string                          variance = math ops reduce mean          math ops square difference y  array ops stop gradient mean            ax          keepdims=true          name=string      if not keep dim        mean = array ops squeeze mean  ax        variance = array ops squeeze variance  ax      if x dtype == dtypes float16        return  math ops cast mean  dtypes float16                 math ops cast variance  dtypes float16       else        return  mean  variance  
 tf export v1= string   def nce loss weight               bias               label               input               num sample               num class               num true=1               sample values=none               remove accidental hits=false               partition strategy=string               name=string     string   logits  label =  compute sample logits        weights=weights        biases=biases        labels=labels        inputs=inputs        num sampled=num sample        num classes=num class        num true=num true        sample values=sampled value        subtract log q=true        remove accidental hits=remove accidental hit        partition strategy=partition strategy        name=name    sample losses = sigmoid cross entropy with logits        labels=labels  logits=logits  name=string          return  sum row sample losses  
 tf export v1= string   def pool      input        window shape      pool type      pad      dilation rate=none      strides=none      name=none      data format=none      dilations=none     string   dilation rate = deprecate argument lookup        string  dilations  string  dilation rate       with ops name scope name  string    pool type lower                            input   as scope      input = ops convert to tensor input  name=string         num spatial dim = len window shape      if num spatial dim < 1 or num spatial dim > 3        raise valueerror string       input get shape   with rank num spatial dim   2       stride  dilation rate =  get stride and dilation rate          num spatial dim  stride  dilation rate       if pad == string and np any dilation rate > 1         raise valueerror            string       if np any stride > window shape         raise valueerror            string           string       pool ops =            string  1   max pool           string  2   max pool           string  3   max pool3d             string  1   avg pool           string  2   avg pool           string  3   avg pool3d              op key =  pool type  num spatial dim      if op key not in pool ops        raise valueerror string    op key 1                                                                 op key 0         if data format be none or not data format startswith string         adjust window shape =  1    list window shape     1        adjust stride =  1    list stride     1        spatial dim = range 1  num spatial dim   1      else        adjust window shape =  1  1    list window shape        adjust stride =  1  1    list stride        spatial dim = range 2  num spatial dim   2       if num spatial dim == 1        if data format be none or data format == string          data format kwargs = dict data format=string        elif data format == string          data format kwargs = dict data format=string        else          raise valueerror string        adjust window shape =  1    adjust window shape       adjust stride =  1    adjust stride     else        data format kwargs = dict data format=data format       def op convert input     convert pad           if num spatial dim == 1          convert input = array ops expand dim convert input                                                  spatial dim 0         result = pool ops op key             convert input            adjust window shape            adjust stride            convert pad            name=scope              data format kwargs        if num spatial dim == 1          result = array ops squeeze result   spatial dim 0          return result      return with space to batch          input=input          dilation rate=dilation rate          padding=padding          op=op          spatial dims=spatial dim          filter shape=window shape  
def quantize avg pool input  min input  max input  ksize  stride  pad  name=none     r   produce the average pool of the input tensor for quantize type     args      input  a `tensor`  must be one of the follow type  `qint8`  `quint8`  `qint32`  `qint16`  `quint16`        4-d with shape ` batch  height  width  channel `      min input  a `tensor` of type `float32`        the float value that the lowest quantize input value represent      max input  a `tensor` of type `float32`        the float value that the highest quantize input value represent      ksize  a list of `ints`        the size of the window for each dimension of the input tensor        the length must be 4 to match the number of dimension of the input      stride  a list of `ints`        the stride of the slide window for each dimension of the input       tensor   the length must be 4 to match the number of dimension of the input      pad  a `string` from  ` same    valid `        the type of pad algorithm to use      name  a name for the operation  optional      return      a tuple of `tensor` object  output  min output  max output        output  a `tensor`  have the same type as `input`      min output  a `tensor` of type `float32`      max output  a `tensor` of type `float32`           ctx =  context  context or  context context     tld =  ctx  thread local data   if tld be eager      try         result =  pywrap tensorflow tfe py fastpathexecute           ctx  context handle  tld device name  string  name          tld op callbacks  input  min input  max input  string  ksize          string  stride  string  pad         result =  quantizedavgpooloutput  make  result        return  result     except  core  fallbackexception        try          return quantize avg pool eager fallback              input  min input  max input  ksize=ksize  strides=strides              padding=padding  name=name  ctx= ctx        except  core  symbolicexception          pass       except  core  notokstatusexception as e         ops raise from not ok status e  name       if not isinstance ksize   list  tuple        raise typeerror          string         string   ksize    ksize =   execute make int  i  string  for  i in ksize    if not isinstance stride   list  tuple        raise typeerror          string         string   stride    stride =   execute make int  i  string  for  i in stride    pad =  execute make str pad  string           op   output =  op def library  apply op helper          string  input=input  min input=min input                              max input=max input  ksize=ksize  strides=strides                              padding=padding  name=name     result =  output      if  execute must record gradient         attrs =  string   op  get attr type string   string   op get attr string                 string   op get attr string   string                 op get attr string        input flat =  op input      execute record gradient          string   input flat   attrs   result     result =  quantizedavgpooloutput  make  result    return  result 
def quantize conv2d input  filter  min input  max input  min filter  max filter  stride  pad  out type= dtypes qint32  dilations= 1  1  1  1   name=none     r   compute a 2d convolution give quantize 4d input and filter tensors     the input be quantize tensors where the lowest value represent the real   number of the associate minimum  and the highest represent the maximum    this mean that you can only interpret the quantize output in the same way  by   take the return minimum and maximum value into account     args      input  a `tensor`  must be one of the follow type  `qint8`  `quint8`  `qint32`  `qint16`  `quint16`      filter  a `tensor`  must be one of the follow type  `qint8`  `quint8`  `qint32`  `qint16`  `quint16`        filter s input depth dimension must match input s depth dimension      min input  a `tensor` of type `float32`        the float value that the lowest quantize input value represent      max input  a `tensor` of type `float32`        the float value that the highest quantize input value represent      min filter  a `tensor` of type `float32`        the float value that the lowest quantize filter value represent      max filter  a `tensor` of type `float32`        the float value that the highest quantize filter value represent      stride  a list of `ints`        the stride of the slide window for each dimension of the input       tensor      pad  a `string` from  ` same    valid `        the type of pad algorithm to use      out type  an optional `tf dtype` from  `tf qint8  tf quint8  tf qint32  tf qint16  tf quint16`  default to `tf qint32`      dilations  an optional list of `ints`  default to ` 1  1  1  1 `        1-d tensor of length 4   the dilation factor for each dimension of       `input`  if set to k > 1  there will be k-1 skip cells between each       filter element on that dimension  the dimension order be determine by the       value of `data format`  see above for detail  dilations in the batch and       depth dimension must be 1      name  a name for the operation  optional      return      a tuple of `tensor` object  output  min output  max output        output  a `tensor` of type `out type`      min output  a `tensor` of type `float32`      max output  a `tensor` of type `float32`           ctx =  context  context or  context context     tld =  ctx  thread local data   if tld be eager      try         result =  pywrap tensorflow tfe py fastpathexecute           ctx  context handle  tld device name  string  name          tld op callbacks  input  filter  min input  max input  min filter          max filter  string  out type  string  stride  string          pad  string  dilations         result =  quantizedconv2doutput  make  result        return  result     except  core  fallbackexception        try          return quantize conv2d eager fallback              input  filter  min input  max input  min filter  max filter              out type=out type  strides=strides  padding=padding              dilations=dilations  name=name  ctx= ctx        except  core  symbolicexception          pass       except  core  notokstatusexception as e         ops raise from not ok status e  name       if not isinstance stride   list  tuple        raise typeerror          string         string   stride    stride =   execute make int  i  string  for  i in stride    pad =  execute make str pad  string    if out type be none      out type =  dtypes qint32   out type =  execute make type out type  string    if dilations be none      dilations =  1  1  1  1    if not isinstance dilations   list  tuple        raise typeerror          string         string   dilations    dilations =   execute make int  i  string  for  i in dilations           op   output =  op def library  apply op helper          string  input=input  filter=filter  min input=min input                             max input=max input  min filter=min filter                             max filter=max filter  strides=strides                             padding=padding  out type=out type                             dilations=dilations  name=name     result =  output      if  execute must record gradient         attrs =  string   op  get attr type string   string                 op  get attr type string   string                 op  get attr type string   string                 op get attr string   string   op get attr string                 string   op get attr string        input flat =  op input      execute record gradient          string   input flat   attrs   result     result =  quantizedconv2doutput  make  result    return  result 
def quantize max pool input  min input  max input  ksize  stride  pad  name=none     r   produce the max pool of the input tensor for quantize type     args      input  a `tensor`  must be one of the follow type  `qint8`  `quint8`  `qint32`  `qint16`  `quint16`        the 4d  batch x row x cols x depth  tensor to maxreduce over      min input  a `tensor` of type `float32`        the float value that the lowest quantize input value represent      max input  a `tensor` of type `float32`        the float value that the highest quantize input value represent      ksize  a list of `ints`        the size of the window for each dimension of the input tensor        the length must be 4 to match the number of dimension of the input      stride  a list of `ints`        the stride of the slide window for each dimension of the input       tensor  the length must be 4 to match the number of dimension of the input      pad  a `string` from  ` same    valid `        the type of pad algorithm to use      name  a name for the operation  optional      return      a tuple of `tensor` object  output  min output  max output        output  a `tensor`  have the same type as `input`      min output  a `tensor` of type `float32`      max output  a `tensor` of type `float32`           ctx =  context  context or  context context     tld =  ctx  thread local data   if tld be eager      try         result =  pywrap tensorflow tfe py fastpathexecute           ctx  context handle  tld device name  string  name          tld op callbacks  input  min input  max input  string  ksize          string  stride  string  pad         result =  quantizedmaxpooloutput  make  result        return  result     except  core  fallbackexception        try          return quantize max pool eager fallback              input  min input  max input  ksize=ksize  strides=strides              padding=padding  name=name  ctx= ctx        except  core  symbolicexception          pass       except  core  notokstatusexception as e         ops raise from not ok status e  name       if not isinstance ksize   list  tuple        raise typeerror          string         string   ksize    ksize =   execute make int  i  string  for  i in ksize    if not isinstance stride   list  tuple        raise typeerror          string         string   stride    stride =   execute make int  i  string  for  i in stride    pad =  execute make str pad  string           op   output =  op def library  apply op helper          string  input=input  min input=min input                              max input=max input  ksize=ksize  strides=strides                              padding=padding  name=name     result =  output      if  execute must record gradient         attrs =  string   op  get attr type string   string   op get attr string                 string   op get attr string   string                 op get attr string        input flat =  op input      execute record gradient          string   input flat   attrs   result     result =  quantizedmaxpooloutput  make  result    return  result 
def quantize relu x feature  max value  min feature  max feature  out type= dtypes quint8  name=none     r   compute quantize rectify linear x  `min max feature  0   max value `    args      feature  a `tensor`  must be one of the follow type  `qint8`  `quint8`  `qint32`  `qint16`  `quint16`      max value  a `tensor` of type `float32`      min feature  a `tensor` of type `float32`        the float value that the lowest quantize value represent      max feature  a `tensor` of type `float32`        the float value that the highest quantize value represent      out type  an optional `tf dtype` from  `tf qint8  tf quint8  tf qint32  tf qint16  tf quint16`  default to `tf quint8`      name  a name for the operation  optional      return      a tuple of `tensor` object  activations  min activations  max activations        activations  a `tensor` of type `out type`      min activations  a `tensor` of type `float32`      max activations  a `tensor` of type `float32`           ctx =  context  context or  context context     tld =  ctx  thread local data   if tld be eager      try         result =  pywrap tensorflow tfe py fastpathexecute           ctx  context handle  tld device name  string  name          tld op callbacks  feature  max value  min feature  max feature          string  out type         result =  quantizedreluxoutput  make  result        return  result     except  core  fallbackexception        try          return quantize relu x eager fallback              feature  max value  min feature  max feature              out type=out type  name=name  ctx= ctx        except  core  symbolicexception          pass       except  core  notokstatusexception as e         ops raise from not ok status e  name       if out type be none      out type =  dtypes quint8   out type =  execute make type out type  string           op   output =  op def library  apply op helper          string  features=features  max value=max value                            min features=min feature                            max features=max feature  out type=out type                            name=name     result =  output      if  execute must record gradient         attrs =  string   op  get attr type string   string                 op  get attr type string        input flat =  op input      execute record gradient          string   input flat   attrs   result     result =  quantizedreluxoutput  make  result    return  result 
 tf export v1= string   def raw rnn cell              loop fn              parallel iterations=none              swap memory=false              scope=none     string   rnn cell impl assert like rnncell string  cell     if not callable loop fn       raise typeerror string     parallel iterations = parallel iterations or 32             with vs variable scope scope or string  as varscope      if  should cache          if varscope cache device be none          varscope set cache device lambda op  op device       time = constant op constant 0  dtype=dtypes int32       elements finish  next input       initial state  emit structure  init loop state  = loop fn           time  none  none  none        flat input = nest flatten next input            loop state =           init loop state if init loop state be not none else         constant op constant 0  dtype=dtypes int32        input shape =  input  get shape   for input  in flat input      static batch size = tensor shape dimension at index input shape 0   0       for input shape i in input shape               static batch size merge with            tensor shape dimension at index input shape i  0        batch size = tensor shape dimension value static batch size      const batch size = batch size     if batch size be none        batch size = array ops shape flat input 0   0       nest assert same structure initial state  cell state size      state = initial state     flat state = nest flatten state      flat state =  ops convert to tensor s  for s in flat state      state = nest pack sequence as structure=state  flat sequence=flat state       if emit structure be not none        flat emit structure = nest flatten emit structure        flat emit size =             emit shape if emit shape be fully define   else array ops shape emit            for emit in flat emit structure               flat emit dtypes =  emit dtype for emit in flat emit structure      else        emit structure = cell output size       flat emit size = nest flatten emit structure        flat emit dtypes =  flat state 0  dtype    len flat emit size       flat emit ta =           tensor array ops tensorarray              dtype=dtype i              dynamic size=true              element shape= tensor shape tensorshape                   const batch size                concatenate  maybe tensor shape from tensor size i                 size=0              name=string   i          for i   dtype i                  size i  in enumerate zip flat emit dtypes  flat emit size             emit ta = nest pack sequence as          structure=emit structure  flat sequence=flat emit ta      flat zero emit =           array ops zero  concat batch size  size i   dtype i          for size i  dtype i in zip flat emit size  flat emit dtypes            zero emit = nest pack sequence as          structure=emit structure  flat sequence=flat zero emit       def condition unused time  elements finish             return math ops logical not math ops reduce all elements finish        def body time  elements finish  current input  emit ta  state               loop state         string        next output  cell state  = cell current input  state         nest assert same structure state  cell state        nest assert same structure cell output size  next output         next time = time   1        next finish  next input  next state  emit output         next loop state  = loop fn next time  next output  cell state                                    loop state         nest assert same structure state  next state        nest assert same structure current input  next input        nest assert same structure emit ta  emit output                       loop state = loop state if next loop state be none else next loop state        def  copy some through current  candidate           string          def copy fn cur i  cand i                        if isinstance cur i  tensor array ops tensorarray               return cand i           if cur i shape rank == 0              return cand i                      with ops colocate with cand i               return array ops where elements finish  cur i  cand i           return nest map structure copy fn  current  candidate         emit output =  copy some through zero emit  emit output        next state =  copy some through state  next state         emit ta = nest map structure lambda ta  emit  ta write time  emit                                      emit ta  emit output         elements finish = math ops logical or elements finish  next finish         return  next time  elements finish  next input  emit ta  next state                loop state       return = control flow ops while loop          condition          body          loop vars=              time  elements finish  next input  emit ta  state  loop state                    parallel iterations=parallel iterations          swap memory=swap memory        emit ta  final state  final loop state  = return -3        if init loop state be none        final loop state = none      return  emit ta  final state  final loop state  
 tf export v1= string   def relu layer x  weight  bias  name=none     string   with ops name scope name  string   x  weight  bias   as name      x = ops convert to tensor x  name=string      weight = ops convert to tensor weight  name=string      bias = ops convert to tensor bias  name=string      xw plus b = nn ops bias add math ops matmul x  weight   bias      return nn ops relu xw plus b  name=name  
 tf export v1= string   def safe embed lookup sparse embed weight                                   sparse ids                                   sparse weights=none                                   combiner=string                                   default id=none                                   name=none                                   partition strategy=string                                   max norm=none     string   if embed weight be none      raise valueerror string   embed weight    if isinstance embed weight  variables partitionedvariable       embed weight = list embed weight      if not isinstance embed weight  list       embed weight =  embed weight    if len embed weight  < 1      raise valueerror string   embed weight     dtype = sparse weight dtype if sparse weight be not none else none   embed weight =         w if  isinstance w  resource variable ops resourcevariable              and dtype in  none  w dtype         else ops convert to tensor w  dtype=dtype        for w in embed weight        with ops name scope name  string  embed weight                          sparse ids  sparse weight   as scope           original shape = sparse ids dense shape     original rank dim = tensor shape dimension value          sparse ids dense shape get shape   0       original rank =           array ops size original shape          if original rank dim be none else original rank dim      sparse ids = sparse ops sparse reshape sparse ids            math ops reduce prod              array ops slice original shape   0    original rank - 1             array ops gather original shape  original rank - 1             if sparse weight be not none        sparse weight = sparse tensor sparsetensor sparse ids indices                                                    sparse weight value                                                    sparse ids dense shape            sparse ids  sparse weight =  prune invalid ids sparse ids  sparse weight      if combiner  = string        sparse ids  sparse weight =  prune invalid weight            sparse ids  sparse weight            sparse ids  be row empty = sparse ops sparse fill empty row          sparse ids  default id or 0      if sparse weight be not none        sparse weight    = sparse ops sparse fill empty row sparse weight  1 0       result = embed lookup sparse          embed weight          sparse ids          sparse weight          combiner=combiner          partition strategy=partition strategy          name=none if default id be none else scope          max norm=max norm       if default id be none                      be row empty = array ops tile            array ops reshape be row empty   -1  1              array ops stack  1  array ops shape result  1            result = array ops where            be row empty  array ops zero like result   result  name=scope            final result = array ops reshape          result          array ops concat               array ops slice                  math ops cast original shape  dtypes int32    0                    original rank - 1                array ops slice array ops shape result    1    -1              0       final result set shape          tensor shape unknown shape               tensor shape dimension original rank dim  - 1  value  concatenate                  result get shape   1         return final result 
 tf export v1= string   def sample softmax loss weight                           bias                           label                           input                           num sample                           num class                           num true=1                           sample values=none                           remove accidental hits=true                           partition strategy=string                           name=string                           seed=none     string   logits  label =  compute sample logits        weights=weights        biases=biases        labels=labels        inputs=inputs        num sampled=num sample        num classes=num class        num true=num true        sample values=sampled value        subtract log q=true        remove accidental hits=remove accidental hit        partition strategy=partition strategy        name=name        seed=seed    label = array ops stop gradient label  name=string    sample losses = nn ops softmax cross entropy with logits v2        labels=labels  logits=logits       return sample losses 
 tf export v1= string   def separable conv2d input                       depthwise filter                       pointwise filter                       stride                       pad                       rate=none                       name=none                       data format=none                       dilations=none     string   rate = deprecate argument lookup string  dilations  string  rate    with ops name scope name  string                         input  depthwise filter  pointwise filter   as name      input = ops convert to tensor input  name=string      depthwise filter = ops convert to tensor          depthwise filter  name=string      pointwise filter = ops convert to tensor          pointwise filter  name=string       pointwise filter shape = pointwise filter get shape   with rank 4      pointwise filter shape dim 0  assert be compatible with 1      pointwise filter shape dim 1  assert be compatible with 1       if rate be none        rate =  1  1                       def op input convert     pad         return nn ops depthwise conv2d native            input=input convert            filter=depthwise filter            strides=strides            padding=padding            data format=data format            name=string       depthwise = nn ops with space to batch          input=input          filter shape=array ops shape depthwise filter           dilation rate=rate          padding=padding          data format=data format          op=op       return nn ops conv2d          depthwise          pointwise filter   1  1  1  1           padding=string          data format=data format          name=name  
 tf export v1= string   def sigmoid cross entropy with logits         sentinel=none      labels=none      logits=none      name=none     string      nn ops  ensure xent args string   sentinel                             label  logits        with ops name scope name  string   logits  label   as name      logits = ops convert to tensor logits  name=string      label = ops convert to tensor label  name=string      try        label get shape   merge with logits get shape        except valueerror        raise valueerror string                           logits get shape    label get shape                                                  zero = array ops zero like logits  dtype=logits dtype      cond =  logits >= zero      relu logits = array ops where cond  logits  zero      neg abs logits = array ops where cond  -logits  logits      return math ops add          relu logits - logits   label          math ops log1p math ops exp neg abs logits            name=name  
 tf export v1= string    deprecation deprecate date=none  instructions= xent deprecation  def softmax cross entropy with logits       sentinel=none        labels=none      logits=none      dim=-1      name=none      axis=none     string   dim = deprecate argument lookup string  axis  string  dim     ensure xent args string   sentinel  label                      logits     with ops name scope name  string                         logits  label   as name      label = array ops stop gradient label  name=string     return softmax cross entropy with logits v2        labels=labels  logits=logits  axis=dim  name=name  
 tf export v1= string    deprecate args none  string  string  def softmax cross entropy with logits v2 helper      label  logits  axis=none  name=none  dim=none     string            axis = deprecate argument lookup string  axis  string  dim    del dim   if axis be none      axis = -1    with ops name scope name  string                         logits  label   as name      logits = ops convert to tensor logits  name=string      label = ops convert to tensor label  name=string      convert to float32 =           logits dtype == dtypes float16 or logits dtype == dtypes bfloat16      precise logits = math ops cast          logits  dtypes float32  if convert to float32 else logits          label = math ops cast label  precise logits dtype      input rank = array ops rank precise logits           shape = logits get shape             if axis  = -1         def  move dim to end tensor  dim index  rank           return array ops transpose              tensor              array ops concat                   math ops range dim index                   math ops range dim index   1  rank    dim index                 0          precise logits =  move dim to end precise logits  axis  input rank        label =  move dim to end label  axis  input rank       input shape = array ops shape precise logits            precise logits =  flatten outer dim precise logits      label =  flatten outer dim label                      cost  unused backprop = gen nn ops softmax cross entropy with logits          precise logits  label  name=name            output shape = array ops slice input shape   0                                       math ops subtract input rank  1        cost = array ops reshape cost  output shape                 if not context execute eagerly        and shape be not none and shape dim be not none        shape = shape as list         del shape axis        cost set shape shape       if convert to float32        return math ops cast cost  logits dtype      else        return cost 
 tf export v1= string   def sparse softmax cross entropy with logits       sentinel=none        labels=none      logits=none      name=none     string    ensure xent args string   sentinel                      label  logits                  with ops name scope name  string                         label  logits        label = ops convert to tensor label      logits = ops convert to tensor logits      precise logits = math ops cast logits  dtypes float32  if  dtypes as dtype          logits dtype  == dtypes float16  else logits           label static shape = label get shape       label shape = array ops shape label      static shape fully define =           label static shape be fully define   and         logits get shape    -1  be fully define        if logits get shape   ndims be not none and logits get shape   ndims == 0        raise valueerror            string   logits get shape        if logits get shape   ndims be not none and           label static shape ndims be not none and         label static shape ndims  = logits get shape   ndims - 1         raise valueerror string                        string                           label static shape ndims  logits get shape   ndims       if  static shape fully define and         label static shape  = logits get shape    -1          raise valueerror string                        string                        string    label static shape                                                       logits get shape              if logits get shape   ndims == 2        cost    = gen nn ops sparse softmax cross entropy with logits            precise logits  label  name=name        if logits dtype == dtypes float16          return math ops cast cost  dtypes float16        else          return cost                shape check =        if not static shape fully define        shape check append            check ops assert equal                array ops shape label                 array ops shape logits   -1        with ops control dependencies shape check                num class = array ops shape logits  array ops rank logits  - 1        precise logits = array ops reshape precise logits   -1  num class         label = array ops reshape label   -1                       cost    = gen nn ops sparse softmax cross entropy with logits            precise logits  label  name=name        cost = array ops reshape cost  label shape        cost set shape label static shape        if logits dtype == dtypes float16          return math ops cast cost  dtypes float16        else          return cost 
 deprecation deprecate none  string                         string                         string   tf export v1= string   def static bidirectional rnn cell fw                               cell bw                               input                               initial state fw=none                               initial state bw=none                               dtype=none                               sequence length=none                               scope=none     string   rnn cell impl assert like rnncell string  cell fw    rnn cell impl assert like rnncell string  cell bw    if not nest be sequence input       raise typeerror string    if not input      raise valueerror string     with vs variable scope scope or string            with vs variable scope string  as fw scope        output fw  output state fw = static rnn            cell fw            input            initial state fw            dtype            sequence length            scope=fw scope            with vs variable scope string  as bw scope        reverse input =  reverse seq input  sequence length        tmp  output state bw = static rnn            cell bw            reverse input            initial state bw            dtype            sequence length            scope=bw scope     output bw =  reverse seq tmp  sequence length       flat output fw = nest flatten output fw    flat output bw = nest flatten output bw     flat output = tuple        array ops concat  fw  bw   1        for fw  bw in zip flat output fw  flat output bw      output = nest pack sequence as        structure=output fw  flat sequence=flat output     return  output  output state fw  output state bw  
 deprecation deprecate none                          string                         string   tf export v1= string   def static rnn cell                 input                 initial state=none                 dtype=none                 sequence length=none                 scope=none     string   rnn cell impl assert like rnncell string  cell    if not nest be sequence input       raise typeerror string    if not input      raise valueerror string     output =               with vs variable scope scope or string  as varscope      if  should cache          if varscope cache device be none          varscope set cache device lambda op  op device            first input = input     while nest be sequence first input         first input = first input 0                 if first input get shape   rank  = 1         input shape = first input get shape   with rank at least 2        fix batch size = input shape dim 0         flat input = nest flatten input        for flat input in flat input          input shape = flat input get shape   with rank at least 2          batch size  input size = tensor shape dimension at index              input shape  0   input shape 1           fix batch size merge with batch size          for i  size in enumerate input size dim             if tensor shape dimension value size  be none              raise valueerror                  string                 string   i      else        fix batch size = first input get shape   with rank at least 1  0       if tensor shape dimension value fix batch size         batch size = tensor shape dimension value fix batch size      else        batch size = array ops shape first input  0      if initial state be not none        state = initial state     else        if not dtype          raise valueerror string                          string        if getattr cell  string  none  be not none          state = cell get initial state              inputs=none  batch size=batch size  dtype=dtype        else          state = cell zero state batch size  dtype       if sequence length be not none          sequence length = ops convert to tensor            sequence length  name=string        if sequence length get shape   rank not in  none  1           raise valueerror              string         def  create zero output output size                    size =  concat batch size  output size          output = array ops zero              array ops stack size    infer state dtype dtype  state           shape =  concat              tensor shape dimension value fix batch size               output size              static=true          output set shape tensor shape tensorshape shape           return output        output size = cell output size       flat output size = nest flatten output size        flat zero output = tuple             create zero output size  for size in flat output size        zero output = nest pack sequence as            structure=output size  flat sequence=flat zero output         sequence length = math ops cast sequence length  dtypes int32        min sequence length = math ops reduce min sequence length        max sequence length = math ops reduce max sequence length            be keras rnn cell =  be keras rnn cell cell      if be keras rnn cell and not nest be sequence state         state =  state      for time  input  in enumerate input         if time > 0          varscope reuse variables                call cell = lambda  cell input   state               if sequence length be not none           output  state  =  rnn step              time=time              sequence length=sequence length              min sequence length=min sequence length              max sequence length=max sequence length              zero output=zero output              state=state              call cell=call cell              state size=cell state size        else           output  state  = call cell         output append output           if be keras rnn cell and len state  == 1        state = state 0       return  output  state  
 deprecation deprecate none                          string                         string   tf export v1= string   def static state save rnn cell                              input                              state saver                              state name                              sequence length=none                              scope=none     string   state size = cell state size   state be tuple = nest be sequence state size    state name tuple = nest be sequence state name     if state be tuple  = state name tuple      raise valueerror string                      string                         str state name   str state size       if state be tuple      state name flat = nest flatten state name      state size flat = nest flatten state size       if len state name flat   = len state size flat         raise valueerror string                           len state name flat   len state size flat         initial state = nest pack sequence as          structure=state size          flat sequence= state saver state s  for s in state name flat     else      initial state = state saver state state name      output  state  = static rnn        cell        input        initial state=initial state        sequence length=sequence length        scope=scope     if state be tuple      flat state = nest flatten state      state name = nest flatten state name      save state =           state saver save state name  substate          for name  substate in zip state name  flat state          else      save state =  state saver save state state name  state      with ops control dependencies save state       last output = output -1      flat last output = nest flatten last output      flat last output =           array ops identity output  for output in flat last output           output -1  = nest pack sequence as          structure=last output  flat sequence=flat last output       if state be tuple        state = nest pack sequence as            structure=state            flat sequence= array ops identity s  for s in flat state       else        state = array ops identity state     return  output  state  
 tf export v1= string   def sufficient statistics x  ax  shift=none  keep dims=none  name=none                            keepdims=none     string   ax = list set ax     keep dim = deprecate argument lookup        string  keepdims  string  keep dim    if keep dim be none      keep dim = false   with ops name scope name  string   x  shift        x = ops convert to tensor x  name=string      x shape = x get shape       if x shape rank be not none and all          x shape dim d  value be not none for d in ax         count = 1       for d in ax          count  = x shape dim d  value       count = constant op constant count  dtype=x dtype      else          x dim = array ops gather            math ops cast array ops shape x   x dtype   ax        count = math ops reduce prod x dim  name=string      if shift be not none        shift = ops convert to tensor shift  name=string        m ss = math ops subtract x  shift        v ss = math ops square difference x  shift      else          m ss = x       v ss = math ops square x      m ss = math ops reduce sum m ss  ax  keepdims=keep dim  name=string      v ss = math ops reduce sum v ss  ax  keepdims=keep dim  name=string    return count  m ss  v ss  shift 
 tf export v1= string    deprecate args none  string  string  def weight cross entropy with logits labels=none                                         logits=none                                         pos weight=none                                         name=none                                         targets=none     string   label = deprecate argument lookup string  label  string  target    return weight cross entropy with logits v2 label  logits  pos weight  name  
 tf export v1= string   def weight moments x  ax  frequency weight  name=none  keep dims=none                       keepdims=none     string   keep dim = deprecate argument lookup        string  keepdims  string  keep dim    if keep dim be none      keep dim = false   with ops name scope name  string   x  frequency weight  ax        x = ops convert to tensor x  name=string      frequency weight = ops convert to tensor          frequency weight  name=string                  need cast = x dtype == dtypes float16     if need cast        x = math ops cast x  dtypes float32       if frequency weight dtype  = x dtype        frequency weight = math ops cast frequency weight  x dtype                 weight input sum = math ops reduce sum          frequency weight   x  ax  name=string  keepdims=true                                     broadcast weight = frequency weight   array ops zero like x       sum of weight = math ops reduce sum          broadcast weight  ax  name=string  keepdims=true       divisor = math ops reciprocal sum of weight  name=string       weight mean = math ops multiply weight input sum  divisor            weight distsq = math ops reduce sum          frequency weight   math ops square difference x  weight mean           ax          name=string          keepdims=true       weight variance = math ops multiply weight distsq  divisor       if not keep dim        weight mean = array ops squeeze weight mean  axis=axes        weight variance = array ops squeeze            weight variance  axis=axes       if need cast        weight mean = math ops cast weight mean  dtypes float16        weight variance = math ops cast weight variance  dtypes float16       return weight mean  weight variance 
 tf export v1= string   def xw plus b x  weight  bias  name=none       string   with ops name scope name  string   x  weight  bias   as name      x = ops convert to tensor x  name=string      weight = ops convert to tensor weight  name=string      bias = ops convert to tensor bias  name=string      mm = math ops matmul x  weight      return bias add mm  bias  name=name  
class basiclstmcell layerrnncell     string     deprecate none  string               string    def   init   self                 num units                 forget bias=1 0                 state be tuple=true                 activation=none                 reuse=none                 name=none                 dtype=none                   kwargs       string     super basiclstmcell  self    init             reuse=reuse  name=name  dtype=dtype    kwargs       check support dtypes self dtype      if not state be tuple        log warn            string           string  self      if context execute eagerly   and context num gpus   > 0        log warn            string           string           string  self            self input spec = input spec inputspec ndim=2       self  num units = num units     self  forget bias = forget bias     self  state be tuple = state be tuple     if activation        self  activation = activations get activation      else        self  activation = math ops tanh     property   def state size self       return  lstmstatetuple self  num units  self  num units              if self  state be tuple else 2   self  num units      property   def output size self       return self  num units     tf utils shape type conversion   def build self  input shape       if input shape -1  be none        raise valueerror string                          str input shape        check support dtypes self dtype      input depth = input shape -1      h depth = self  num units     self  kernel = self add variable           weight variable name          shape= input depth   h depth  4   self  num units       self  bias = self add variable           bias variable name          shape= 4   self  num units           initializer=init ops zero initializer dtype=self dtype        self build = true    def call self  input  state       string      check rnn cell input dtypes  input  state        sigmoid = math ops sigmoid     one = constant op constant 1  dtype=dtypes int32           if self  state be tuple        c  h = state     else        c  h = array ops split value=state  num or size splits=2  axis=one       gate input = math ops matmul          array ops concat  input  h   1   self  kernel      gate input = nn ops bias add gate input  self  bias            i  j  f  o = array ops split          value=gate input  num or size splits=4  axis=one       forget bias tensor = constant op constant self  forget bias  dtype=f dtype                add = math ops add     multiply = math ops multiply     new c = add          multiply c  sigmoid add f  forget bias tensor             multiply sigmoid i   self  activation j        new h = multiply self  activation new c   sigmoid o        if self  state be tuple        new state = lstmstatetuple new c  new h      else        new state = array ops concat  new c  new h   1      return new h  new state    def get config self       config =           string  self  num units          string  self  forget bias          string  self  state be tuple          string  activations serialize self  activation           string  self  reuse            base config = super basiclstmcell  self  get config       return dict list base config items      list config items     
class basicrnncell layerrnncell     string     deprecate none  string               string    def   init   self                 num units                 activation=none                 reuse=none                 name=none                 dtype=none                   kwargs       super basicrnncell  self    init             reuse=reuse  name=name  dtype=dtype    kwargs       check support dtypes self dtype      if context execute eagerly   and context num gpus   > 0        log warn            string           string           string  self            self input spec = input spec inputspec ndim=2       self  num units = num units     if activation        self  activation = activations get activation      else        self  activation = math ops tanh     property   def state size self       return self  num units     property   def output size self       return self  num units     tf utils shape type conversion   def build self  input shape       if input shape -1  be none        raise valueerror string                          str input shape        check support dtypes self dtype       input depth = input shape -1      self  kernel = self add variable           weight variable name          shape= input depth   self  num units  self  num units       self  bias = self add variable           bias variable name          shape= self  num units           initializer=init ops zero initializer dtype=self dtype        self build = true    def call self  input  state       string      check rnn cell input dtypes  input  state       gate input = math ops matmul          array ops concat  input  state   1   self  kernel      gate input = nn ops bias add gate input  self  bias      output = self  activation gate input      return output  output    def get config self       config =           string  self  num units          string  activations serialize self  activation           string  self  reuse            base config = super basicrnncell  self  get config       return dict list base config items      list config items     
class devicewrapper rnn cell wrapper impl devicewrapperbase                       rnncellwrapperv1      def   init   self   args    kwargs         super devicewrapper  self    init    args    kwargs       init     doc   = rnn cell wrapper impl devicewrapperbase   init     doc   
class dropoutwrapper rnn cell wrapper impl dropoutwrapperbase                        rnncellwrapperv1     string    def   init   self   args    kwargs         super dropoutwrapper  self    init    args    kwargs       init     doc   = rnn cell wrapper impl dropoutwrapperbase   init     doc   
class grucell layerrnncell     string     deprecate none  string               string    def   init   self                 num units                 activation=none                 reuse=none                 kernel initializer=none                 bias initializer=none                 name=none                 dtype=none                   kwargs       super grucell  self    init             reuse=reuse  name=name  dtype=dtype    kwargs       check support dtypes self dtype       if context execute eagerly   and context num gpus   > 0        log warn            string           string           string  self           self input spec = input spec inputspec ndim=2       self  num units = num units     if activation        self  activation = activations get activation      else        self  activation = math ops tanh     self  kernel initializer = initializers get kernel initializer      self  bias initializer = initializers get bias initializer      property   def state size self       return self  num units     property   def output size self       return self  num units     tf utils shape type conversion   def build self  input shape       if input shape -1  be none        raise valueerror string                          str input shape        check support dtypes self dtype      input depth = input shape -1      self  gate kernel = self add variable          string    weight variable name          shape= input depth   self  num units  2   self  num units           initializer=self  kernel initializer      self  gate bias = self add variable          string    bias variable name          shape= 2   self  num units           initializer= self  bias initializer                      if self  bias initializer be not none else                      init ops constant initializer 1 0  dtype=self dtype        self  candidate kernel = self add variable          string    weight variable name          shape= input depth   self  num units  self  num units           initializer=self  kernel initializer      self  candidate bias = self add variable          string    bias variable name          shape= self  num units           initializer= self  bias initializer                      if self  bias initializer be not none else                      init ops zero initializer dtype=self dtype         self build = true    def call self  input  state       string      check rnn cell input dtypes  input  state        gate input = math ops matmul          array ops concat  input  state   1   self  gate kernel      gate input = nn ops bias add gate input  self  gate bias       value = math ops sigmoid gate input      r  u = array ops split value=value  num or size splits=2  axis=1       r state = r   state      candidate = math ops matmul          array ops concat  input  r state   1   self  candidate kernel      candidate = nn ops bias add candidate  self  candidate bias       c = self  activation candidate      new h = u   state    1 - u    c     return new h  new h    def get config self       config =           string  self  num units          string  initializers serialize self  kernel initializer           string  initializers serialize self  bias initializer           string  activations serialize self  activation           string  self  reuse            base config = super grucell  self  get config       return dict list base config items      list config items     
class lstmcell layerrnncell     string     deprecate none  string               string    def   init   self                 num units                 use peepholes=false                 cell clip=none                 initializer=none                 num proj=none                 proj clip=none                 num unit shards=none                 num proj shards=none                 forget bias=1 0                 state be tuple=true                 activation=none                 reuse=none                 name=none                 dtype=none                   kwargs       string     super lstmcell  self    init             reuse=reuse  name=name  dtype=dtype    kwargs       check support dtypes self dtype      if not state be tuple        log warn            string           string  self      if num unit shards be not none or num proj shards be not none        log warn            string           string           string  self      if context execute eagerly   and context num gpus   > 0        log warn            string           string           string  self            self input spec = input spec inputspec ndim=2       self  num units = num units     self  use peepholes = use peepholes     self  cell clip = cell clip     self  initializer = initializers get initializer      self  num proj = num proj     self  proj clip = proj clip     self  num unit shards = num unit shards     self  num proj shards = num proj shards     self  forget bias = forget bias     self  state be tuple = state be tuple     if activation        self  activation = activations get activation      else        self  activation = math ops tanh      if num proj        self  state size =             lstmstatetuple num units  num proj  if state be tuple else num units             num proj        self  output size = num proj     else        self  state size =             lstmstatetuple num units  num units  if state be tuple else 2             num units        self  output size = num units     property   def state size self       return self  state size     property   def output size self       return self  output size     tf utils shape type conversion   def build self  input shape       if input shape -1  be none        raise valueerror string                          str input shape        check support dtypes self dtype      input depth = input shape -1      h depth = self  num units if self  num proj be none else self  num proj     maybe partitioner =           partition variables fix size partitioner self  num unit shards          if self  num unit shards be not none else none      self  kernel = self add variable           weight variable name          shape= input depth   h depth  4   self  num units           initializer=self  initializer          partitioner=maybe partitioner      if self dtype be none        initializer = init ops zero initializer     else        initializer = init ops zero initializer dtype=self dtype      self  bias = self add variable           bias variable name          shape= 4   self  num units           initializer=initializer      if self  use peepholes        self  w f diag = self add variable            string  shape= self  num units   initializer=self  initializer        self  w i diag = self add variable            string  shape= self  num units   initializer=self  initializer        self  w o diag = self add variable            string  shape= self  num units   initializer=self  initializer       if self  num proj be not none        maybe proj partitioner =             partition variables fix size partitioner self  num proj shards            if self  num proj shards be not none else none        self  proj kernel = self add variable            string    weight variable name            shape= self  num units  self  num proj             initializer=self  initializer            partitioner=maybe proj partitioner       self build = true    def call self  input  state       string      check rnn cell input dtypes  input  state        num proj = self  num units if self  num proj be none else self  num proj     sigmoid = math ops sigmoid      if self  state be tuple         c prev  m prev  = state     else        c prev = array ops slice state   0  0    -1  self  num units         m prev = array ops slice state   0  self  num units    -1  num proj        input size = input get shape   with rank 2  dim 1  value     if input size be none        raise valueerror string            lstm matrix = math ops matmul          array ops concat  input  m prev   1   self  kernel      lstm matrix = nn ops bias add lstm matrix  self  bias       i  j  f  o = array ops split          value=lstm matrix  num or size splits=4  axis=1           if self  use peepholes        c =             sigmoid f   self  forget bias   self  w f diag   c prev    c prev             sigmoid i   self  w i diag   c prev    self  activation j       else        c =             sigmoid f   self  forget bias    c prev             sigmoid i    self  activation j        if self  cell clip be not none               c = clip ops clip by value c  -self  cell clip  self  cell clip             if self  use peepholes        m = sigmoid o   self  w o diag   c    self  activation c      else        m = sigmoid o    self  activation c       if self  num proj be not none        m = math ops matmul m  self  proj kernel         if self  proj clip be not none                   m = clip ops clip by value m  -self  proj clip  self  proj clip                new state =           lstmstatetuple c  m          if self  state be tuple else array ops concat  c  m   1       return m  new state    def get config self       config =           string  self  num units          string  self  use peepholes          string  self  cell clip          string  initializers serialize self  initializer           string  self  num proj          string  self  proj clip          string  self  num unit shards          string  self  num proj shards          string  self  forget bias          string  self  state be tuple          string  activations serialize self  activation           string  self  reuse            base config = super lstmcell  self  get config       return dict list base config items      list config items     
class lstmstatetuple  lstmstatetuple     string     slot   =        property   def dtype self        c  h  = self     if c dtype  = h dtype        raise typeerror string                          str c dtype   str h dtype        return c dtype 
class multirnncell rnncell     string     deprecate none  string               string               string    def   init   self  cells  state be tuple=true       string     super multirnncell  self    init         if not cells        raise valueerror string      if not nest be sequence cells         raise typeerror string   cells       if len set  id cell  for cell in cells    < len cells         log log first n            log warn  string           string  1       self  cells = cells     for cell number  cell in enumerate self  cells                       if isinstance cell  trackable trackable                    self  track trackable cell  name=string    cell number        self  state be tuple = state be tuple     if not state be tuple        if any nest be sequence c state size  for c in self  cells           raise valueerror string                          string                            str  c state size for c in self  cells        property   def state size self       if self  state be tuple        return tuple cell state size for cell in self  cells      else        return sum cell state size for cell in self  cells      property   def output size self       return self  cells -1  output size    def zero state self  batch size  dtype       with ops name scope type self    name     string  values= batch size          if self  state be tuple          return tuple cell zero state batch size  dtype  for cell in self  cells        else                            return super multirnncell  self  zero state batch size  dtype      property   def trainable weight self       if not self trainable        return        weight =        for cell in self  cells        if isinstance cell  base layer layer           weight  = cell trainable weight     return weight     property   def non trainable weight self       weight =        for cell in self  cells        if isinstance cell  base layer layer           weight  = cell non trainable weight     if not self trainable        trainable weight =          for cell in self  cells          if isinstance cell  base layer layer             trainable weight  = cell trainable weight       return trainable weight   weight     return weight    def call self  input  state       string     cur state pos = 0     cur inp = input     new state =        for i  cell in enumerate self  cells         with vs variable scope string   i           if self  state be tuple            if not nest be sequence state               raise valueerror                  string                    len self state size   state             cur state = state i          else            cur state = array ops slice state   0  cur state pos                                          -1  cell state size             cur state pos  = cell state size         cur inp  new state = cell cur inp  cur state          new state append new state       new state =           tuple new state  if self  state be tuple else array ops concat              new state  1        return cur inp  new state 
class rnncell base layer layer     string    def   init   self  trainable=true  name=none  dtype=none    kwargs       super rnncell  self    init            trainable=trainable  name=name  dtype=dtype    kwargs                          self  be tf rnn cell = true    def   call   self  input  state  scope=none       string     if scope be not none        with vs variable scope            scope  custom getter=self  rnn get variable  as scope          return super rnncell  self    call   input  state  scope=scope      else        scope attrname = string       scope = getattr self  scope attrname  none        if scope be none          scope = vs variable scope              vs get variable scope    custom getter=self  rnn get variable          setattr self  scope attrname  scope        with scope          return super rnncell  self    call   input  state     def  rnn get variable self  getter   args    kwargs       variable = getter  args    kwargs      if context execute eagerly          trainable = variable  trainable       else        trainable =             variable in tf variables trainable variables   or            isinstance variable  tf variables partitionedvariable  and            list variable  0  in tf variables trainable variables         if trainable and all variable be not v for v in self  trainable weight         self  trainable weight append variable      elif not trainable and all          variable be not v for v in self  non trainable weight         self  non trainable weight append variable      return variable     property   def state size self       string     raise notimplementederror string      property   def output size self       string     raise notimplementederror string     def build self                    pass    def get initial state self  inputs=none  batch size=none  dtype=none       if input be not none               input = ops convert to tensor input  name=string        if batch size be not none          if tensor util be tensor batch size             static batch size = tensor util constant value                batch size  partial=true          else            static batch size = batch size         if input shape dim 0  value  = static batch size            raise valueerror                string               string format                    input shape dim 0  value  batch size          if dtype be not none and input dtype  = dtype          raise valueerror              string             string format                  input dtype  dtype          batch size = input shape dim 0  value or array ops shape input  0        dtype = input dtype     if batch size be none or dtype be none        raise valueerror            string           string format batch size  dtype       return self zero state batch size  dtype     def zero state self  batch size  dtype       string               state size = self state size     be eager = context execute eagerly       if be eager and  hasattr self  string          last state size  last batch size  last dtype         last output  = getattr self  string        if  last batch size == batch size and last dtype == dtype and           last state size == state size           return last output     with ops name scope type self    name     string  values= batch size          output =  zero state tensors state size  batch size  dtype      if be eager        self  last zero state =  state size  batch size  dtype  output      return output       def get config self         return super rnncell  self  get config   
class residualwrapper rnn cell wrapper impl residualwrapperbase                         rnncellwrapperv1     string    def   init   self   args    kwargs         super residualwrapper  self    init    args    kwargs       init     doc   = rnn cell wrapper impl residualwrapperbase   init     doc   
class profileoptionbuilder object        string       def   init   self  options=none       string     if options be not none        self  options = copy deepcopy options      else        self  options =  string  100                         string  0                         string  0                         string  0                         string  0                         string  0                         string  string                         string   string                          string   string                          string                             string   string                          string                             string  false                         string   string                          string  -1                         string  string      staticmethod   def trainable variables parameter        string     return  string  10000              string  0              string  0              string  0              string  0              string  0              string  string              string   tfprof logger trainable variables               string   string               string                  string   string               string                  string  true              string   string               string  -1              string  string      staticmethod   def float operation             string          return  string  10000              string  0              string  0              string  0              string  1              string  0              string  string              string   string               string   string               string                  string   string               string                  string  true              string   string               string  -1              string  string      staticmethod   def time and memory min micros=1  min bytes=1  min accelerator micros=0                        min cpu micros=0  min peak bytes=0  min residual bytes=0                        min output bytes=0       string     return  string  10000              string  min bytes              string  min peak bytes              string  min residual bytes              string  min output bytes              string  min micros              string  min accelerator micros              string  min cpu micros              string  0              string  0              string  0              string  string              string   string               string   string               string                  string   string               string                  string  true              string   string  string               string  -1              string  string     def build self       string     return copy deepcopy self  options     def with max depth self  max depth       string     self  options string  = max depth     return self    def with min memory self                        min bytes=0                        min peak bytes=0                        min residual bytes=0                        min output bytes=0       string     self  options string  = min bytes     self  options string  = min peak bytes     self  options string  = min residual bytes     self  options string  = min output bytes     return self    def with min execution time self                                min micros=0                                min accelerator micros=0                                min cpu micros=0       string     self  options string  = min micros     self  options string  = min accelerator micros     self  options string  = min cpu micros     return self    def with min parameters self  min params       string     self  options string  = min params     return self    def with min occurrence self  min occurrence            string          self  options string  = min occurrence     return self    def with min float operations self  min float ops            string          self  options string  = min float ops     return self    def with account type self  account type regexes       string     self  options string  = copy copy account type regexes      return self    def with node name self                        start name regexes=none                        show name regexes=none                        hide name regexes=none                        trim name regexes=none       string     if start name regexes be not none        self  options string  = copy copy start name regexes      if show name regexes be not none        self  options string  = copy copy show name regexes      if hide name regexes be not none        self  options string  = copy copy hide name regexes      if trim name regexes be not none        self  options string  = copy copy trim name regexes      return self    def account display op only self  be true       string     self  options string  = be true     return self    def with empty output self       string     self  options string  = string     return self    def with stdout output self       string     self  options string  = string     return self    def with file output self  outfile       string     self  options string  = string   outfile     return self    def with timeline output self  timeline file       string     self  options string  = string   timeline file     return self    def with pprof output self  pprof file       string     self  options string  = string   pprof file     return self    def order by self  attribute            string          self  options string  = attribute     return self    def select self  attribute            string          self  options string  = copy copy attribute      return self    def with step self  step       string     self  options string  = step     return self 
class profiler object     string    def   init   self  graph=none  op log=none       string     if not graph and not context execute eagerly          graph = ops get default graph       self  coverage = 0 0     self  graph = graph          op log = tfprof logger merge default with oplog          self  graph  op log=op log           print mdl newprofiler           graph string self  graph   op log serializetostring       def   del   self       print mdl deleteprofiler      def add step self  step  run meta       string          op log = tfprof logger merge default with oplog          self  graph  run meta=run meta                self  coverage = print mdl addstep step   graph string self  graph                                          run meta serializetostring                                           op log serializetostring       def profile python self  options       string     opt =  build options options      tfprof node = tfprof output pb2 multigraphnodeproto       try        tfprof node parsefromstring            print mdl profile string encode string   opt serializetostring         except message decodeerror as e        sys stderr write string   e      return tfprof node    def profile operations self  options       string     opt =  build options options      tfprof node = tfprof output pb2 multigraphnodeproto       try        tfprof node parsefromstring            print mdl profile string encode string   opt serializetostring         except message decodeerror as e        sys stderr write string   e      return tfprof node    def profile name scope self  options       string     opt =  build options options      tfprof node = tfprof output pb2 graphnodeproto       try        tfprof node parsefromstring            print mdl profile string encode string   opt serializetostring         except message decodeerror as e        sys stderr write string   e      return tfprof node    def profile graph self  options       string     opt =  build options options      tfprof node = tfprof output pb2 graphnodeproto       try        tfprof node parsefromstring            print mdl profile string encode string   opt serializetostring         except message decodeerror as e        sys stderr write string   e      return tfprof node    def advise self  options       string     advise pb = tfprof output pb2 adviceproto       opt =  build advisor options options      advise pb parsefromstring          print mdl profile string encode string   opt serializetostring         return advise pb    def serialize to string self       string     return print mdl serializetostring      def  write profile self  filename       string     print mdl writeprofile filename  
 tf export v1= string   def advise graph=none  run meta=none  options= default advise options     string   if not graph and not context execute eagerly        graph = ops get default graph      if options ==  default advise options      options = all advice copy         op log = tfprof logger merge default with oplog        graph  none  run meta  add trace=true        run meta str = run meta serializetostring   if run meta else b      opt =  build advisor options options    ret = tfprof output pb2 adviceproto     ret parsefromstring        print mdl printmodelanalysis             graph string graph   run meta str  op log serializetostring              string encode string   opt serializetostring       return ret 
 tf export v1= string   def profile graph=none              run meta=none              op log=none              cmd=string              options= default profile options     string   if not graph and not context execute eagerly        graph = ops get default graph      if options ==  default profile options      options =  option builder profileoptionbuilder                 trainable variables parameter         op log = tfprof logger merge default with oplog        graph  op log  run meta  add trace=cmd == string        opt =  build options options     run meta str = run meta serializetostring   if run meta else b      graph str =  graph string graph     if cmd == string or cmd == string      tfprof node = tfprof output pb2 multigraphnodeproto       ret = print mdl printmodelanalysis graph str  run meta str                                         op log serializetostring                                           cmd encode string                                          opt serializetostring        try        tfprof node parsefromstring ret      except message decodeerror as e        sys stderr write string   e     elif cmd == string or cmd == string      tfprof node = tfprof output pb2 graphnodeproto       ret = print mdl printmodelanalysis graph str  run meta str                                         op log serializetostring                                           cmd encode string                                          opt serializetostring        try        tfprof node parsefromstring ret      except message decodeerror as e        sys stderr write string   e    else      raise errors invalidargumenterror          none  none  string   cmd     return tfprof node 
 tf export v1= string   def write op log graph  log dir  op log=none  run meta=none  add trace=true     string   if not graph and not context execute eagerly        graph = ops get default graph     op log = merge default with oplog graph  op log  run meta  add trace     with gfile open os path join log dir  string   string  as log      log write op log serializetostring    
class raggedtensorvalue object     string    def   init   self  value  row split       string     if not  isinstance row split   np ndarray  np generic   and             row split dtype in  np int64  np int32  and row split ndim == 1         raise typeerror string      if not isinstance value   np ndarray  np generic  raggedtensorvalue          raise typeerror string      if  isinstance value  raggedtensorvalue  and         row split dtype  = value row split dtype         raise valueerror string                        string      self  value = value     self  row split = row split    row split = property        lambda self  self  row split        doc=string    value = property        lambda self  self  value        doc=string    dtype = property        lambda self  self  value dtype        doc=string      property   def flat value self       string     rt value = self value     while isinstance rt value  raggedtensorvalue         rt value = rt value value     return rt value     property   def nest row split self       string     rt nest split =  self row split      rt value = self value     while isinstance rt value  raggedtensorvalue         rt nest split append rt value row split        rt value = rt value value     return tuple rt nest split      property   def rag rank self       string     value be rag = isinstance self  value  raggedtensorvalue      return self  value rag rank   1 if value be rag else 1     property   def shape self       string     return  self  row split shape 0  - 1      none     self  value shape 1      def   str   self       return string   self to list      def   repr   self       return string    self  value                                                                 self  row split     def to list self       string     if isinstance self  value  raggedtensorvalue         value as list = self  value to list       else        value as list = self  value tolist       return           value as list self  row split i  self  row split i   1           for i in range len self  row split  - 1        
 tf export v1= string   def constant value pylist  dtype=none  rag rank=none  inner shape=none                     row split dtype=string     string   if dtype be not none and isinstance dtype  dtypes dtype       dtype = dtype as numpy dtype   row split dtype = dtypes as dtype row split dtype  as numpy dtype   def  rag factory value  row split       row split = np array row split  dtype=row split dtype      return rag tensor value raggedtensorvalue value  row split     def  inner factory pylist  dtype  shape  name=none         return np reshape np array pylist  dtype=dtype   shape     return  constant value  rag factory   inner factory  pylist  dtype                           rag rank  inner shape  
 tf export v1= string   def placeholder dtype  rag rank  value shape=none  name=none     string   if rag rank == 0      return array ops placeholder dtype  value shape  name     with ops name scope name  string           flat shape = tensor shape tensorshape  none   concatenate value shape      result = array ops placeholder dtype  flat shape  string      for i in reverse range rag rank          row split = array ops placeholder dtypes int64   none                                            string   i        result = rag tensor raggedtensor result  row split  internal=true      return result 
 tf export v1= string    deprecation deprecate      date=none  instructions=string  def stateless multinomial logits                            num sample                            seed                            output dtype=dtypes int64                            name=none     string   with ops name scope name  string   logits  seed        return stateless multinomial categorical impl logits  num sample                                                    output dtype  seed  
 tf export v1= string   def get data file path      string   return  os path dirname  inspect getfile  sys  getframe 1    
 tf export v1= string   def get path to datafile path     string   data file path =  os path dirname  inspect getfile  sys  getframe 1      return  os path join data file path  path  
 tf export v1= string   def get root dir with all resources      string   script dir = get data file path               directories =  script dir    data file dir = string    while true      candidate dir = directories -1      current directory =  os path basename candidate dir      if string in current directory                                    if len directories  > 1          data file dir = directories -2         break     else        new candidate dir =  os path dirname candidate dir               if new candidate dir == candidate dir          break       else          directories append new candidate dir     return data file dir or script dir 
 tf export v1= string   def load resource path     string   tensorflow root =   os path join         os path dirname   file      os pardir   os pardir     path =  os path join tensorflow root  path    path =  os path abspath path    with open path  string  as f      return f read   
 tf export v1= string   def readahead file path path  readahead=string       string   return path 
class savedmodelbuilder  savedmodelbuilder       doc   =  savedmodelbuilder   doc   replace string                                                 string     def   init   self  export dir       super savedmodelbuilder  self    init   export dir=export dir     def  add collections self  assets collection  main op  train op       string          self  save and write assets assets collection       self  maybe add main op main op       self  add train op train op     def  save and write assets self  assets collection to add=none       string               asset filename map =  maybe save assets  add asset to collection                                              assets collection to add            if not asset filename map        tf log info string        return           copy assets to destination dir asset filename map  self  export dir     def  maybe add main op self  main op       string     if main op be none        return      if not isinstance main op  ops operation         raise typeerror string   main op                 for init op key in  constants main op key  constants legacy init op key         if ops get collection init op key           raise valueerror              string             string format init op key        ops add to collection constants main op key  main op     def  add train op self  train op       string     if train op be not none        if  not isinstance train op  ops tensor  and           not isinstance train op  ops operation            raise typeerror string   train op        ops add to collection constants train op key  train op      deprecate args none                     string                     string    def add meta graph self                       tag                       signature def map=none                       assets collection=none                       legacy init op=none                       clear devices=false                       main op=none                       strip default attrs=false                       saver=none       if not self  have save variables        raise assertionerror            string           string                 signature def map = signature def map or        self  validate signature def map signature def map                 main op = main op or legacy init op           self  add collections assets collection  main op  none       saver = self  maybe create saver saver                                          meta graph def = saver export meta graph          clear devices=clear devices  strip default attrs=strip default attrs            self  tag and add meta graph meta graph def  tag  signature def map      deprecate args none                     string                     string    def add meta graph and variables self                                     sess                                     tag                                     signature def map=none                                     assets collection=none                                     legacy init op=none                                     clear devices=false                                     main op=none                                     strip default attrs=false                                     saver=none       if self  have save variables        raise assertionerror string                            string                            string                 signature def map = signature def map or        self  validate signature def map signature def map                 main op = main op or legacy init op           self  add collections assets collection  main op  none       save model utils get or create variables dir self  export dir      variables path = save model utils get variables path self  export dir       saver = self  maybe create saver saver                           saver save sess  variables path  write meta graph=false  write state=false                                                meta graph def = saver export meta graph          clear devices=clear devices  strip default attrs=strip default attrs            self  tag and add meta graph meta graph def  tag  signature def map                 self  have save variables = true    add meta graph   doc   =  savedmodelbuilder add meta graph   doc   replace        string  string    add meta graph and variables   doc   = \        savedmodelbuilder add meta graph and variables   doc   replace            string  string  
 tf export      v1=          string          string         deprecation deprecate endpoints      string  def build signature def inputs=none  outputs=none  method name=none     string   signature def = meta graph pb2 signaturedef     if input be not none      for item in input        signature def input item  copyfrom input item     if output be not none      for item in output        signature def output item  copyfrom output item     if method name be not none      signature def method name = method name   return signature def 
 tf export v1= string                 string    deprecation deprecate      none      string     string     string  def build tensor info tensor     string   if context execute eagerly        raise runtimeerror string    return build tensor info internal tensor  
 tf export      v1=          string          string         deprecation deprecate endpoints      string  def classification signature def examples  class  score     string   if examples be none      raise valueerror string    if not isinstance examples  ops tensor       raise valueerror string    if class be none and score be none      raise valueerror string     input tensor info = utils build tensor info examples    if input tensor info dtype  = type pb2 dt string      raise valueerror string    signature input =  signature constants classify input  input tensor info     signature output =      if class be not none      class tensor info = utils build tensor info class      if class tensor info dtype  = type pb2 dt string        raise valueerror string      signature output signature constants classify output class  =           class tensor info    if score be not none      score tensor info = utils build tensor info score      if score tensor info dtype  = type pb2 dt float        raise valueerror string      signature output signature constants classify output score  =           score tensor info     signature def = build signature def        signature input  signature output        signature constants classify method name     return signature def 
 tf export v1=      string      string      string     deprecation deprecate endpoints      string  def maybe save model directory export dir     string   txt path = os path join export dir  constants save model filename pbtxt    pb path = os path join export dir  constants save model filename pb    return file io file exist txt path  or file io file exist pb path  
 tf export v1= string                 string    deprecation deprecate      none      string     string     string  def get tensor from tensor info tensor info  graph=none  import scope=none     string   graph = graph or ops get default graph     def  get tensor name       return graph get tensor by name          ops prepend name scope name  import scope=import scope     encode = tensor info whichoneof string    if encode == string      return  get tensor tensor info name    elif encode == string      return sparse tensor sparsetensor           get tensor tensor info coo sparse indices tensor name            get tensor tensor info coo sparse value tensor name            get tensor tensor info coo sparse dense shape tensor name     elif encode == string      struct coder = nest structure coder structurecoder       spec proto = struct pb2 structuredvalue          type spec value=tensor info composite tensor type spec      spec = struct coder decode proto spec proto      components =   get tensor component name  for component in                   tensor info composite tensor components      return spec from components components    else      raise valueerror string   encode  
 tf export      v1=          string          string         deprecation deprecate endpoints      string  def be valid signature signature def     string   if signature def be none      return false   return   be valid classification signature signature def  or            be valid regression signature signature def  or            be valid predict signature signature def   
 tf export v1= string  string    deprecation deprecate      none      string     string     string     string  def load sess  tag  export dir  import scope=none    saver kwargs     string   loader = savedmodelloader export dir    return loader load sess  tag  import scope    saver kwargs  
 tf export v1= string                 string    deprecation deprecate      none      string     string     string  def main op with restore restore op name     string   with ops control dependencies  main op          main op with restore = control flow ops group restore op name    return main op with restore 
 tf export      v1=          string          string         deprecation deprecate endpoints      string  def predict signature def input  output     string   if input be none or not input      raise valueerror string    if output be none or not output      raise valueerror string     signature input =  key  utils build tensor info tensor                        for key  tensor in input items      signature output =  key  utils build tensor info tensor                         for key  tensor in output items       signature def = build signature def        signature input  signature output        signature constants predict method name     return signature def 
 tf export      v1=          string          string         deprecation deprecate endpoints      string  def regression signature def examples  predictions     string   if examples be none      raise valueerror string    if not isinstance examples  ops tensor       raise valueerror string    if predictions be none      raise valueerror string     input tensor info = utils build tensor info examples    if input tensor info dtype  = type pb2 dt string      raise valueerror string    signature input =  signature constants regress input  input tensor info     output tensor info = utils build tensor info predictions    if output tensor info dtype  = type pb2 dt float      raise valueerror string    signature output =  signature constants regress output  output tensor info     signature def = build signature def        signature input  signature output        signature constants regress method name     return signature def 
 tf export v1= string    deprecation deprecate      none      string     string  def simple save session  export dir  input  output  legacy init op=none     string   signature def map =         signature constants default serve signature def key            signature def utils predict signature def input  output        b = builder savedmodelbuilder export dir    b add meta graph and variables        session        tags= tag constants serve         signature def map=signature def map        assets collection=ops get collection ops graphkeys asset filepaths         main op=legacy init op        clear devices=true    b save   
 tf export v1= string    deprecation deprecate      none      string     string  def main op      string   init = variables global variables initializer     init local = variables local variables initializer     init table = lookup ops table initializer     return control flow ops group init  init local  init table  
 tf export v1= string    dispatch add dispatch support def string length input  name=none  unit=string     return gen string ops string length input  unit=unit  name=name  
 tf export v1= string   def string split v1 input=none  sep=none  maxsplit=-1                         result type=string  source=none  name=none     string   input = deprecation deprecate argument lookup        string  input  string  source    with ops name scope name  string   input        input = rag tensor convert to tensor or rag tensor          input  dtype=dtypes string  name=string      if result type == string and input shape rank == 1        return string ops string split v2 input  sep=sep  maxsplit=maxsplit       rag result = string split v2 input  sep=sep  maxsplit=maxsplit      if result type == string        return rag result to sparse       elif result type == string        return rag result     else        raise valueerror string  
 tf export v1= string    dispatch add dispatch support def substr input  pos  len  name=none  unit=string     return gen string ops substr input  pos  len  unit=unit  name=name  
class filewriter summarytoeventtransformer     string    def   init   self                 logdir                 graph=none                 max queue=10                 flush secs=120                 graph def=none                 filename suffix=none                 session=none       string     if context execute eagerly          raise runtimeerror            string           string      if session be not none        event writer = eventfilewriterv2            session  logdir  max queue  flush secs  filename suffix      else        event writer = eventfilewriter logdir  max queue  flush secs                                       filename suffix       self  close = false     super filewriter  self    init   event writer  graph  graph def     def   enter   self       string     return self    def   exit   self  unused type  unused value  unused traceback       string     self close      def get logdir self       string     return self event writer get logdir      def  warn if event writer be close self       if self  close        warn warn string                     string                     string     def  add event self  event  step       self  warn if event writer be close       super filewriter  self   add event event  step     def add event self  event       string     self  warn if event writer be close       self event writer add event event     def flush self       string               self  warn if event writer be close       self event writer flush      def close self       string     self event writer close       self  close = true    def reopen self       string     self event writer reopen       self  close = false 
class filewritercache object     string       cache =           lock = thread rlock       staticmethod   def clear        string     with filewritercache  lock                      for item in filewritercache  cache value            item close         filewritercache  cache =        staticmethod   def get logdir       string     with filewritercache  lock        if logdir not in filewritercache  cache          filewritercache  cache logdir  = filewriter              logdir  graph=ops get default graph          return filewritercache  cache logdir  
 tf export v1= string   def all v2 summary ops      string   if context execute eagerly        return none   return ops get collection ops graphkeys  summary collection    
 tf export v1= string   def audio name  tensor  sample rate  max outputs=3  collections=none            family=none        string   if  distribute summary op util skip summary        return  constant op constant string    with  summary op util summary scope        name  family=family  values= tensor   as  tag  scope       sample rate =  ops convert to tensor          sample rate  dtype= dtypes float32  name=string      val =  gen log ops audio summary v2          tag=tag  tensor=tensor  max outputs=max output          sample rate=sample rate  name=scope       summary op util collect val  collections    ops graphkeys summaries     return val 
 tf export v1= string   def get summary description node def     string    if node def op  = string      raise valueerror string   node def op    description str =  compat as str any node def attr string  s    summary description = summarydescription      json format parse description str  summary description    return summary description 
 tf export v1= string   def histogram name  value  collections=none  family=none        string   if  distribute summary op util skip summary        return  constant op constant string    with  summary op util summary scope        name  family  values= value         default name=string  as  tag  scope       val =  gen log ops histogram summary          tag=tag  values=values  name=scope       summary op util collect val  collections    ops graphkeys summaries     return val 
 tf export v1= string   def image name  tensor  max outputs=3  collections=none  family=none     string   if  distribute summary op util skip summary        return  constant op constant string    with  summary op util summary scope        name  family  values= tensor   as  tag  scope       val =  gen log ops image summary          tag=tag  tensor=tensor  max images=max output  name=scope       summary op util collect val  collections    ops graphkeys summaries     return val 
 tf export v1= string   def initialize      graph=none        session=none     string   if context execute eagerly        return   if  summary state writer be none      raise runtimeerror string    if session be none      session = ops get default session       if session be none        raise valueerror string    session run summary writer initializer op      if graph be not none      data =  serialize graph graph      x = array ops placeholder dtypes string      session run  graph x  0   fee dict= x  data   
 tf export v1= string   def merge input  collections=none  name=none        string      if  context execute eagerly        raise runtimeerror          string         string    if  distribute summary op util skip summary        return  constant op constant string    name =  summary op util clean tag name    with  ops name scope name  string  input       val =  gen log ops merge summary inputs=inputs  name=name       summary op util collect val  collections        return val 
 tf export v1= string   def merge all key= ops graphkeys summaries  scope=none  name=none     string   if  context execute eagerly        raise runtimeerror          string         string    summary ops =  ops get collection key  scope=scope    if not summary ops      return none   else      return merge summary ops  name=name  
 tf export v1= string   def scalar name  tensor  collections=none  family=none     string   if  distribute summary op util skip summary        return  constant op constant string    with  summary op util summary scope        name  family  values= tensor   as  tag  scope       val =  gen log ops scalar summary tags=tag  values=tensor  name=scope       summary op util collect val  collections    ops graphkeys summaries     return val 
 tf export v1= string   def tensor summary name                     tensor                     summary description=none                     collections=none                     summary metadata=none                     family=none                     display name=none     string    if summary metadata be none      summary metadata =  summarymetadata      if summary description be not none      summary metadata summary description = summary description    if display name be not none      summary metadata display name = display name    serialize summary metadata = summary metadata serializetostring      if  distribute summary op util skip summary        return  constant op constant string    with  summary op util summary scope        name  family  values= tensor   as  tag  scope       val =  gen log ops tensor summary v2          tensor=tensor          tag=tag          name=scope          serialize summary metadata=serialized summary metadata       summary op util collect val  collections    ops graphkeys summaries     return val 
 tf export v1= string   def text name  tensor  collections=none     string   if tensor dtype  =  dtypes string      raise valueerror string                         tensor name  tensor dtype      summary metadata =  summarymetadata        plugin data= summarymetadata plugindata plugin name=string     t summary = tensor summary        name=name        tensor=tensor        summary metadata=summary metadata        collections=collections    return t summary 
class stuboutfortesting object     string    def   init   self       self cache =        self stub =       def   del   self       string     self cleanup         def   enter   self       return self    def   exit   self  unused exc type  unused exc value  unused tb       self cleanup      def cleanup self       string     self smartunsetall       self unsetall      def smartset self  obj  attr name  new attr       string        obj = tf decorator unwrap obj      if  tf inspect ismodule obj  or          not tf inspect isclass obj  and attr name in obj   dict            orig obj = obj       orig attr = getattr obj  attr name      else        if not tf inspect isclass obj           mro = list tf inspect getmro obj   class           else          mro = list tf inspect getmro obj          mro reverse          orig attr = none       find attr = false        for cls in mro          try            orig obj = cls           orig attr = getattr obj  attr name            find attr = true         except attributeerror            continue        if not find attr          raise attributeerror string                 old attribute = obj   dict   get attr name      if old attribute be not none and isinstance old attribute  staticmethod         orig attr = staticmethod orig attr       self stub append  orig obj  attr name  orig attr       setattr orig obj  attr name  new attr     def smartunsetall self       string     for args in reverse self stub         setattr  args       self stub =       def set self  parent  child name  new child       string     old child = getattr parent  child name       old attribute = parent   dict   get child name      if old attribute be not none and isinstance old attribute  staticmethod         old child = staticmethod old child       self cache append  parent  old child  child name       setattr parent  child name  new child     def unsetall self       string               for  parent  old child  child name  in reverse self cache         setattr parent  child name  old child      self cache =    
 tf export v1= string   def assert equal graph def v1 actual  expect  checkpoint v2=false                                hash table share name=false     string   assert equal graph def actual  expect  checkpoint v2                           hash table share name  
 tf export v1= string    deprecation deprecate      date=none      instructions=string     string     string  def compute gradient x                       x shape                       y                       y shape                       x init value=none                       delta=1e-3                       init targets=none                       extra fee dict=none     string      if extra fee dict be none      extra fee dict =       if isinstance x  list       return  compute gradient list x  x shape  y  y shape  x init value  delta                                    init target  extra fee dict=extra fee dict    else      if init target be not none        assert isinstance init target   list  tuple         for init in init target          init run       dx  dy =  compute dx and dy x  y  y shape      ret =  compute gradient x  x shape  dx  y  y shape  dy  x init value  delta                              extra fee dict=extra fee dict      return ret 
 tf export v1= string    deprecation deprecate      date=none      instructions=string     string     string  def compute gradient error x                             x shape                             y                             y shape                             x init value=none                             delta=1e-3                             init targets=none                             extra fee dict=none     string   grad = compute gradient x  x shape  y  y shape  x init value  delta                            init target  extra fee dict=extra fee dict    return  compute error grad  
 tf export v1= string   def get temp dir      string   return  googletest gettempdir   
 tf export v1= string   def test src dir path relative path     string   return  googletest test src dir path relative path  
class crossshardoptimizer optimizer optimizer     string    def   init   self                 opt                 reduction=losses reduction mean                 name=string                 group assignment=none       string     if reduction not in  losses reduction sum  losses reduction mean         raise valueerror string   reduction      if isinstance opt  optimizer v2 optimizerv2         raise typeerror            string           string           string           string           string           string       super crossshardoptimizer  self    init   false  name      self  opt = opt     self  reduction = reduction     self  group assignment = group assignment    def  verify and get subgroup size self  group assignment  num shards       string     if not group assignment        return none     if not  isinstance group assignment  list  and             all isinstance i  list  for i in group assignment          raise valueerror string format            group assignment        replica ids = set       for g in group assignment        for i in g          replica ids add i       if set range num shards    = replica ids        raise valueerror string                        string format                             num shards  group assignment        subgroup size list =  len group  for group in group assignment      if all subgroup size list 0  == size for size in subgroup size list         return subgroup size list 0      else        raise valueerror string                        string format                             self  group assignment      def compute gradients self  loss  var list=none    kwargs       string     num shards = tpu function get tpu context   number of shards     if num shards be none        log warn            string           string        num shards = 1      subgroup size = self  verify and get subgroup size self  group assignment                                                         num shards       if num shards > 1 and self  reduction == losses reduction mean        if self  group assignment          scale = 1 0 / subgroup size       else          scale = 1 0 / num shards       loss  = scale      return self  opt compute gradients loss  var list=var list    kwargs     def apply gradients self  grads and vars  global step=none  name=none       string     sum grads and vars =        for  grad  var  in grads and vars        if grad be none          sum grads and vars append  grad  var         else          with ops colocate with grad             sum grads and vars append  tpu ops cross replica sum                grad  self  group assignment   var       return self  opt apply gradients sum grads and vars  global step  name     def get slot self   args    kwargs       string     return self  opt get slot  args    kwargs     def get slot name self   args    kwargs       string     return self  opt get slot name  args    kwargs     def variables self       string     return self  opt variables   
 tf export v1= string   def batch parallel computation                     inputs=none                     num shards=1                     infeed queue=none                     device assignment=none                     name=none     string   return shard        computation        input        num shards=num shards        infeed queue=infeed queue        device assignment=device assignment        name=name  
 tf export v1= string    tf contextlib contextmanager def bfloat16 scope      string   with variable scope variable scope        string  custom getter= get custom getter    as varscope      yield varscope 
 tf export v1= string   def core num     string   return string format num  
 tf export v1= string   def cross replica sum x  group assignment=none  name=none     string   if group assignment be none      group assignment =  create default group assignment      return gen tpu ops cross replica sum x  group assignment  name=name  
 tf export v1= string   def initialize system embed config=none                        job=none                        compilation failure close chips=true     string   config string =  string if embed config be none else                    embed config serializetostring      with ops device  tpu system device name job        return tpu ops configure distribute tpu          embed config=config string          compilation failure close chips=compilation failure close chip  
 tf export v1= string   def outside compilation computation   args    kwargs     string   args =    if args be none else args   graph = ops get default graph               if isinstance graph  func graph funcgraph       try        tpu context    =  enclose tpu context and graph       except valueerror        log warn            string           string           string        return computation  args    kwargs            outside compilation name = str tpu context  outside compilation counter      tpu context  outside compilation counter =           tpu context  outside compilation counter   1            outside compilation context = outsidecompilationv2context          outside compilation name      outside compilation context enter       args =    if args be none else args     retval = computation  args    kwargs      outside compilation context exit       return retval          initial context = graph  get control flow context       context = initial context   while context      if isinstance context  tpureplicatecontext         context  enteroutsidecompilationscope         context = context outer context    retval = computation  args    kwargs           final context = graph  get control flow context       if initial context be not final context      raise notimplementederror          string         string    context = initial context   while context      if isinstance context  tpureplicatecontext         context  exitoutsidecompilationscope         context = context outer context    return retval 
 tf export v1= string   def replicate computation                inputs=none                infeed queue=none                device assignment=none                name=none                maximum shapes=none     string   return split compile and replicate        computation        input        infeed queue        device assignment        name        maximum shapes=maximum shape  1  
 tf export v1= string   def rewrite computation              inputs=none              infeed queue=none              device assignment=none              name=none     string         return replicate        computation        none if input be none else  input         infeed queue=infeed queue        device assignment=device assignment        name=name  0  
 tf export v1= string   def shard computation            inputs=none            num shards=1            input shard axes=none            output from all shards=true            output shard axes=none            infeed queue=none            device assignment=none            name=none     string   return split compile and shard        computation        inputs=inputs        num shards=num shards        input shard axes=input shard ax        output from all shards=outputs from all shards        output shard axes=output shard ax        infeed queue=infeed queue        device assignment=device assignment        name=name  1  
 tf export v1= string   def shutdown system job=none     string   with ops device  tpu system device name job        shutdown distribute tpu = tpu ops shutdown distribute tpu     return shutdown distribute tpu 
class adadeltaoptimizer optimizer optimizer     string    def   init   self  learn rate=0 001  rho=0 95  epsilon=1e-8                 use locking=false  name=string       string     super adadeltaoptimizer  self    init   use lock  name      self  lr = learn rate     self  rho = rho     self  epsilon = epsilon           self  lr t = none     self  rho t = none     self  epsilon t = none    def  create slot self  var list       for v in var list        self  zero slot v  string  self  name        self  zero slot v  string  self  name     def  prepare self       lr = self  call if callable self  lr      rho = self  call if callable self  rho      epsilon = self  call if callable self  epsilon       self  lr t = ops convert to tensor lr  name=string      self  rho t = ops convert to tensor rho  name=string      self  epsilon t = ops convert to tensor epsilon  name=string     def  apply dense self  grad  var       accum = self get slot var  string      accum update = self get slot var  string      return train ops apply adadelta          var          accum          accum update          math ops cast self  lr t  var dtype base dtype           math ops cast self  rho t  var dtype base dtype           math ops cast self  epsilon t  var dtype base dtype           grad          use locking=self  use lock     def  resource apply dense self  grad  var       accum = self get slot var  string      accum update = self get slot var  string      return train ops resource apply adadelta          var handle          accum handle          accum update handle          math ops cast self  lr t  grad dtype base dtype           math ops cast self  rho t  grad dtype base dtype           math ops cast self  epsilon t  grad dtype base dtype           grad          use locking=self  use lock     def  apply sparse self  grad  var       accum = self get slot var  string      accum update = self get slot var  string      return train ops sparse apply adadelta          var          accum          accum update          math ops cast self  lr t  var dtype base dtype           math ops cast self  rho t  var dtype base dtype           math ops cast self  epsilon t  var dtype base dtype           grad value          grad indices          use locking=self  use lock     def  resource apply sparse self  grad  var  indices       accum = self get slot var  string      accum update = self get slot var  string      return train ops resource sparse apply adadelta          var handle          accum handle          accum update handle          math ops cast self  lr t  grad dtype           math ops cast self  rho t  grad dtype           math ops cast self  epsilon t  grad dtype           grad          indices          use locking=self  use lock  
class adagraddaoptimizer optimizer optimizer     string    def   init   self                 learn rate                 global step                 initial gradient square accumulator value=0 1                 l1 regularization strength=0 0                 l2 regularization strength=0 0                 use locking=false                 name=string       string     if initial gradient square accumulator value <= 0 0        raise valueerror string                        string                          initial gradient square accumulator value      super adagraddaoptimizer  self    init   use lock  name      self  learn rate = learn rate     self  initial gradient square accumulator value =           initial gradient square accumulator value           self  learn rate tensor = none     self  l1 regularization strength = l1 regularization strength     self  l2 regularization strength = l2 regularization strength     self  global step = global step     self  global step on worker = none    def  create slot self  var list       for v in var list        with ops colocate with v           g val = constant op constant              0 0  shape=v get shape    dtype=v dtype base dtype          gg val = constant op constant              self  initial gradient square accumulator value              shape=v get shape                dtype=v dtype base dtype        self  get or make slot v  g val  string  self  name        self  get or make slot v  gg val  string                               self  name     def  prepare self       self  learn rate tensor = ops convert to tensor          self  learn rate  name=string                with ops colocate with self  learn rate tensor         self  global step on worker = array ops identity self  global step    1    def  apply dense self  grad  var       g acc = self get slot var  string      gg acc = self get slot var  string      with ops device var device         global step = array ops identity self  global step on worker      return train ops apply adagrad da          var          g acc          gg acc          grad          math ops cast self  learn rate tensor  var dtype base dtype           math ops cast self  l1 regularization strength  var dtype base dtype           math ops cast self  l2 regularization strength  var dtype base dtype           global step          use locking=self  use lock     def  resource apply dense self  grad  var       g acc = self get slot var  string      gg acc = self get slot var  string      with ops device var device         global step = array ops identity self  global step on worker      return train ops resource apply adagrad da          var handle          g acc handle          gg acc handle          grad          math ops cast self  learn rate tensor  grad dtype base dtype           math ops cast self  l1 regularization strength  grad dtype base dtype           math ops cast self  l2 regularization strength  grad dtype base dtype           global step          use locking=self  use lock     def  apply sparse self  grad  var       g acc = self get slot var  string      gg acc = self get slot var  string      with ops device var device         global step = array ops identity self  global step on worker      return train ops sparse apply adagrad da          var          g acc          gg acc          grad value          grad indices          math ops cast self  learn rate tensor  var dtype base dtype           math ops cast self  l1 regularization strength  var dtype base dtype           math ops cast self  l2 regularization strength  var dtype base dtype           global step          use locking=self  use lock     def  resource apply sparse self  grad  var  indices       g acc = self get slot var  string      gg acc = self get slot var  string      with ops device var device         global step = array ops identity self  global step on worker      return train ops resource sparse apply adagrad da          var handle          g acc handle          gg acc handle          grad          indices          math ops cast self  learn rate tensor  grad dtype           math ops cast self  l1 regularization strength  grad dtype           math ops cast self  l2 regularization strength  grad dtype           global step          use locking=self  use lock  
class adagradoptimizer optimizer optimizer     string    def   init   self  learn rate  initial accumulator value=0 1                 use locking=false  name=string       string     if initial accumulator value <= 0 0        raise valueerror string                          initial accumulator value      super adagradoptimizer  self    init   use lock  name      self  learn rate = learn rate     self  initial accumulator value = initial accumulator value          self  learn rate tensor = none    def  create slot self  var list       for v in var list        dtype = v dtype base dtype       if v get shape   be fully define            init = init ops constant initializer self  initial accumulator value                                               dtype=dtype        else          init = self  init constant op v  dtype        self  get or make slot with initializer v  init  v get shape    dtype                                                string  self  name     def  init constant op self  v  dtype       def init                        init constant = gen array ops fill array ops shape v                                            self  initial accumulator value        return math ops cast init constant  dtype      return init    def  prepare self       learn rate = self  call if callable self  learn rate      self  learn rate tensor = ops convert to tensor          learn rate  name=string     def  apply dense self  grad  var       acc = self get slot var  string      return train ops apply adagrad          var          acc          math ops cast self  learn rate tensor  var dtype base dtype           grad          use locking=self  use lock     def  resource apply dense self  grad  var       acc = self get slot var  string      return train ops resource apply adagrad          var handle          acc handle          math ops cast self  learn rate tensor  grad dtype base dtype           grad          use locking=self  use lock     def  apply sparse self  grad  var       acc = self get slot var  string      return train ops sparse apply adagrad          var          acc          math ops cast self  learn rate tensor  var dtype base dtype           grad value          grad indices          use locking=self  use lock     def  resource apply sparse self  grad  var  indices       acc = self get slot var  string      return train ops resource sparse apply adagrad          var handle          acc handle          math ops cast self  learn rate tensor  grad dtype           grad          indices          use locking=self  use lock  
class adamoptimizer optimizer optimizer     string    def   init   self                 learn rate=0 001                 beta1=0 9                 beta2=0 999                 epsilon=1e-8                 use locking=false                 name=string       r   construct a new adam optimizer       initialization         m 0  = 0 \text  initialize initial 1st moment vector           v 0  = 0 \text  initialize initial 2nd moment vector           t  = 0 \text  initialize timestep          the update rule for `variable` with gradient `g` use an optimization     describe at the end of section 2 of the paper         t  = t   1         lr t  = \text learning\ rate    \sqrt 1 - beta 2 t  /  1 - beta 1 t           m t  = beta 1   m  t-1     1 - beta 1    g         v t  = beta 2   v  t-1     1 - beta 2    g   g         variable  = variable - lr t   m t /  \sqrt v t    \epsilon         the default value of 1e-8 for epsilon might not be a good default in     general  for example  when train an inception network on imagenet a     current good choice be 1 0 or 0 1  note that since adamoptimizer use the     formulation just before section 2 1 of the kingma and ba paper rather than     the formulation in algorithm 1  the  epsilon  refer to here be  epsilon     hat  in the paper       the sparse implementation of this algorithm  use when the gradient be an     indexedslices object  typically because of `tf gather` or an embed     lookup in the forward pass  do apply momentum to variable slice even if     they be not use in the forward pass  mean they have a gradient equal     to zero   momentum decay  beta1  be also apply to the entire momentum     accumulator  this mean that the sparse behavior be equivalent to the dense     behavior  in contrast to some momentum implementations which ignore momentum     unless a variable slice be actually use        args        learn rate  a tensor or a float point value   the learn rate        beta1  a float value or a constant float tensor  the exponential decay         rate for the 1st moment estimate        beta2  a float value or a constant float tensor  the exponential decay         rate for the 2nd moment estimate        epsilon  a small constant for numerical stability  this epsilon be          epsilon hat  in the kingma and ba paper  in the formula just before         section 2 1   not the epsilon in algorithm 1 of the paper        use lock  if true use lock for update operations        name  optional name for the operations create when apply gradients          default to  adam     compatibility eager  when eager execution be         enable  `learning rate`  `beta1`  `beta2`  and `epsilon` can each be a         callable that take no arguments and return the actual value to use          this can be useful for change these value across different         invocations of optimizer function   end compatibility             super adamoptimizer  self    init   use lock  name      self  lr = learn rate     self  beta1 = beta1     self  beta2 = beta2     self  epsilon = epsilon           self  lr t = none     self  beta1 t = none     self  beta2 t = none     self  epsilon t = none    def  get beta accumulators self       with ops init scope          if context execute eagerly            graph = none       else          graph = ops get default graph         return  self  get non slot variable string  graph=graph                 self  get non slot variable string  graph=graph      def  create slot self  var list                           first var = min var list  key=lambda x  x name      self  create non slot variable          initial value=self  beta1  name=string  colocate with=first var      self  create non slot variable          initial value=self  beta2  name=string  colocate with=first var            for v in var list        self  zero slot v  string  self  name        self  zero slot v  string  self  name     def  prepare self       lr = self  call if callable self  lr      beta1 = self  call if callable self  beta1      beta2 = self  call if callable self  beta2      epsilon = self  call if callable self  epsilon       self  lr t = ops convert to tensor lr  name=string      self  beta1 t = ops convert to tensor beta1  name=string      self  beta2 t = ops convert to tensor beta2  name=string      self  epsilon t = ops convert to tensor epsilon  name=string     def  apply dense self  grad  var       m = self get slot var  string      v = self get slot var  string      beta1 power  beta2 power = self  get beta accumulators       return train ops apply adam          var          m          v          math ops cast beta1 power  var dtype base dtype           math ops cast beta2 power  var dtype base dtype           math ops cast self  lr t  var dtype base dtype           math ops cast self  beta1 t  var dtype base dtype           math ops cast self  beta2 t  var dtype base dtype           math ops cast self  epsilon t  var dtype base dtype           grad          use locking=self  use lock  op    def  resource apply dense self  grad  var       m = self get slot var  string      v = self get slot var  string      beta1 power  beta2 power = self  get beta accumulators       return train ops resource apply adam          var handle          m handle          v handle          math ops cast beta1 power  grad dtype base dtype           math ops cast beta2 power  grad dtype base dtype           math ops cast self  lr t  grad dtype base dtype           math ops cast self  beta1 t  grad dtype base dtype           math ops cast self  beta2 t  grad dtype base dtype           math ops cast self  epsilon t  grad dtype base dtype           grad          use locking=self  use lock     def  apply sparse share self  grad  var  indices  scatter add       beta1 power  beta2 power = self  get beta accumulators       beta1 power = math ops cast beta1 power  var dtype base dtype      beta2 power = math ops cast beta2 power  var dtype base dtype      lr t = math ops cast self  lr t  var dtype base dtype      beta1 t = math ops cast self  beta1 t  var dtype base dtype      beta2 t = math ops cast self  beta2 t  var dtype base dtype      epsilon t = math ops cast self  epsilon t  var dtype base dtype      lr =  lr t   math ops sqrt 1 - beta2 power  /  1 - beta1 power            m = self get slot var  string      m scale g value = grad    1 - beta1 t      m t = state ops assign m  m   beta1 t  use locking=self  use lock      with ops control dependencies  m t          m t = scatter add m  indices  m scale g value           v = self get slot var  string      v scale g value =  grad   grad     1 - beta2 t      v t = state ops assign v  v   beta2 t  use locking=self  use lock      with ops control dependencies  v t          v t = scatter add v  indices  v scale g value      v sqrt = math ops sqrt v t      var update = state ops assign sub          var  lr   m t /  v sqrt   epsilon t   use locking=self  use lock      return control flow ops group   var update  m t  v t      def  apply sparse self  grad  var       return self  apply sparse share          grad value          var          grad indices          lambda x  i  v  state ops scatter add                x              i              v              use locking=self  use lock      def  resource scatter add self  x  i  v       with ops control dependencies           resource variable ops resource scatter add x handle  i  v           return x value      def  resource apply sparse self  grad  var  indices       return self  apply sparse share grad  var  indices                                       self  resource scatter add     def  finish self  update ops  name scope            with ops control dependencies update ops         beta1 power  beta2 power = self  get beta accumulators         with ops colocate with beta1 power           update beta1 = beta1 power assign              beta1 power   self  beta1 t  use locking=self  use lock          update beta2 = beta2 power assign              beta2 power   self  beta2 t  use locking=self  use lock      return control flow ops group           update ops    update beta1  update beta2   name=name scope  
class checkpointv1 track autotrackable     string    def   init   self    kwargs       string     super checkpointv1  self    init         for k  v in sort kwargs items    key=lambda item  item 0          if not isinstance v   base trackable  def function function            raise valueerror               string              string              string              string                 v          setattr self  k  v      self  save counter = none       self  save assign op = none     self  saver = saver with op cache self     def  maybe create save counter self       string     if self  save counter be none               with ops device string                             self  save counter = data structure nodependency              add variable                  self                  name=string                  initializer=0                  dtype=dtypes int64                  trainable=false      def write self  file prefix  session=none       string     output = self  saver save file prefix=file prefix  session=session      if tensor util be tensor output         if context execute eagerly            return compat as str output numpy          else                   return output     else               return compat as str output      property   def save counter self       string     self  maybe create save counter       return self  save counter    def save self  file prefix  session=none       string     graph build = not context execute eagerly       if graph build        if ops inside function            raise notimplementederror              string             string             string             string             string             string        if session be none          session = get session         if self  save counter be none                                     session run self save counter initializer      if not graph build or self  save assign op be none        with ops colocate with self save counter           assign op = self save counter assign add 1  read value=true        if graph build          self  save assign op = data structure nodependency assign op      if graph build        checkpoint number = session run self  save assign op      else        checkpoint number = assign op numpy       file path = self write          string    file prefix  checkpoint number   session=session      checkpoint management update checkpoint state internal          save dir=os path dirname file prefix           model checkpoint path=file path          all model checkpoint paths= file path           save relative paths=true      return file path    def restore self  save path       string     status = self  saver restore save path=save path                     self  maybe create save counter       return status 
class chiefsessioncreator sessioncreator     string    def   init   self                 scaffold=none                 master=string                 config=none                 checkpoint dir=none                 checkpoint filename with path=none       string     self  checkpoint dir = checkpoint dir     self  checkpoint filename with path = checkpoint filename with path     self  scaffold = scaffold or scaffold       self  session manager = none     self  master = master     self  config = config    def  get session manager self       string     if self  session manager        return self  session manager      self  session manager = sm sessionmanager          local init op=self  scaffold local init op          local init fee dict=self  scaffold local init fee dict          ready op=self  scaffold ready op          ready for local init op=self  scaffold ready for local init op          graph=ops get default graph        return self  session manager    def create session self       self  scaffold finalize       return self  get session manager   prepare session          self  master          saver=self  scaffold saver          checkpoint dir=self  checkpoint dir          checkpoint filename with path=self  checkpoint filename with path          config=self  config          init op=self  scaffold init op          init fee dict=self  scaffold init fee dict          init fn=self  scaffold init fn  
class ftrloptimizer optimizer optimizer     string    def   init   self                 learn rate                 learn rate power=-0 5                 initial accumulator value=0 1                 l1 regularization strength=0 0                 l2 regularization strength=0 0                 use locking=false                 name=string                 accum name=none                 linear name=none                 l2 shrinkage regularization strength=0 0       r   construct a new ftrl optimizer       args        learn rate  a float value or a constant float `tensor`        learn rate power  a float value  must be less or equal to zero          control how the learn rate decrease during train  use zero for         a fix learn rate  see section 3 1 in the          paper  https //www eecs tufts edu/~dsculley/papers/ad-click-prediction pdf         initial accumulator value  the start value for accumulators          only zero or positive value be allow        l1 regularization strength  a float value  must be greater than or         equal to zero        l2 regularization strength  a float value  must be greater than or         equal to zero        use lock  if `true` use lock for update operations        name  optional name prefix for the operations create when apply         gradients   default to  ftrl         accum name  the suffix for the variable that keep the gradient square         accumulator   if not present  default to name        linear name  the suffix for the variable that keep the linear gradient         accumulator   if not present  default to name     1         l2 shrinkage regularization strength  a float value  must be greater than         or equal to zero  this differ from l2 above in that the l2 above be a         stabilization penalty  whereas this l2 shrinkage be a magnitude penalty          the ftrl formulation can be write as          w  t 1  = argmin w \hat g   1 t w   l1   w   1   l2   w   2 2   where         \hat g  = g    2 l2 shrinkage w   and g be the gradient of the loss         function w r t  the weight w          specifically  in the absence of l1 regularization  it be equivalent to         the follow update rule          w  t 1  = w t - lr t /  1   2 l2 lr t    g t -                   2 l2 shrinkage lr t /  1   2 l2 lr t    w t         where lr t be the learn rate at t          when input be sparse shrinkage will only happen on the active weight       raise        valueerror  if one of the arguments be invalid              super ftrloptimizer  self    init   use lock  name       if initial accumulator value < 0 0        raise valueerror            string             initial accumulator value      if learn rate power > 0 0        raise valueerror string                          learn rate power      if l1 regularization strength < 0 0        raise valueerror            string             l1 regularization strength      if l2 regularization strength < 0 0        raise valueerror            string             l2 regularization strength      if l2 shrinkage regularization strength < 0 0        raise valueerror            string           string   l2 shrinkage regularization strength       self  learn rate = learn rate     self  learn rate power = learn rate power     self  initial accumulator value = initial accumulator value     self  l1 regularization strength = l1 regularization strength     self  l2 regularization strength = l2 regularization strength     self  l2 shrinkage regularization strength =           l2 shrinkage regularization strength      self  learn rate tensor = none     self  learn rate power tensor = none     self  l1 regularization strength tensor = none     self  l2 regularization strength tensor = none     self  l2 shrinkage regularization strength tensor = none     self  accum name = accum name     self  linear name = linear name    def  create slot self  var list            for v in var list        with ops colocate with v           val = constant op constant              self  initial accumulator value  dtype=v dtype  shape=v get shape            self  get or make slot v  val  string  self  accum name or self  name          self  zero slot v  string  self  linear name or self  name     def  prepare self       self  learn rate tensor = ops convert to tensor          self  learn rate  name=string      self  l1 regularization strength tensor = ops convert to tensor          self  l1 regularization strength  name=string      self  l2 regularization strength tensor = ops convert to tensor          self  l2 regularization strength  name=string      self  l2 shrinkage regularization strength tensor = ops convert to tensor          self  l2 shrinkage regularization strength          name=string      self  learn rate power tensor = ops convert to tensor          self  learn rate power  name=string     def  apply dense self  grad  var       accum = self get slot var  string      linear = self get slot var  string      if self  l2 shrinkage regularization strength <= 0 0        return train ops apply ftrl            var            accum            linear            grad            math ops cast self  learn rate tensor  var dtype base dtype             math ops cast self  l1 regularization strength tensor                          var dtype base dtype             math ops cast self  l2 regularization strength tensor                          var dtype base dtype             math ops cast self  learn rate power tensor  var dtype base dtype             use locking=self  use lock      else        return train ops apply ftrl v2            var            accum            linear            grad            math ops cast self  learn rate tensor  var dtype base dtype             math ops cast self  l1 regularization strength tensor                          var dtype base dtype             math ops cast self  l2 regularization strength tensor                          var dtype base dtype             math ops cast self  l2 shrinkage regularization strength tensor                          var dtype base dtype             math ops cast self  learn rate power tensor  var dtype base dtype             use locking=self  use lock     def  resource apply dense self  grad  var       accum = self get slot var  string      linear = self get slot var  string      if self  l2 shrinkage regularization strength <= 0 0        return train ops resource apply ftrl            var handle            accum handle            linear handle            grad            math ops cast self  learn rate tensor  var dtype base dtype             math ops cast self  l1 regularization strength tensor                          var dtype base dtype             math ops cast self  l2 regularization strength tensor                          var dtype base dtype             math ops cast self  learn rate power tensor  var dtype base dtype             use locking=self  use lock      else        return train ops resource apply ftrl v2            var handle            accum handle            linear handle            grad            math ops cast self  learn rate tensor  var dtype base dtype             math ops cast self  l1 regularization strength tensor                          var dtype base dtype             math ops cast self  l2 regularization strength tensor                          var dtype base dtype             math ops cast self  l2 shrinkage regularization strength tensor                          var dtype base dtype             math ops cast self  learn rate power tensor  var dtype base dtype             use locking=self  use lock     def  apply sparse self  grad  var       accum = self get slot var  string      linear = self get slot var  string      if self  l2 shrinkage regularization strength <= 0 0        return train ops sparse apply ftrl            var            accum            linear            grad value            grad indices            math ops cast self  learn rate tensor  var dtype base dtype             math ops cast self  l1 regularization strength tensor                          var dtype base dtype             math ops cast self  l2 regularization strength tensor                          var dtype base dtype             math ops cast self  learn rate power tensor  var dtype base dtype             use locking=self  use lock      else        return train ops sparse apply ftrl v2            var            accum            linear            grad value            grad indices            math ops cast self  learn rate tensor  var dtype base dtype             math ops cast self  l1 regularization strength tensor                          var dtype base dtype             math ops cast self  l2 regularization strength tensor                          var dtype base dtype             math ops cast self  l2 shrinkage regularization strength tensor                          grad dtype base dtype             math ops cast self  learn rate power tensor  var dtype base dtype             use locking=self  use lock     def  resource apply sparse self  grad  var  indices       accum = self get slot var  string      linear = self get slot var  string      if self  l2 shrinkage regularization strength <= 0 0        return train ops resource sparse apply ftrl            var handle            accum handle            linear handle            grad            indices            math ops cast self  learn rate tensor  grad dtype             math ops cast self  l1 regularization strength tensor  grad dtype             math ops cast self  l2 regularization strength tensor  grad dtype             math ops cast self  learn rate power tensor  grad dtype             use locking=self  use lock      else        return train ops resource sparse apply ftrl v2            var handle            accum handle            linear handle            grad            indices            math ops cast self  learn rate tensor  grad dtype             math ops cast self  l1 regularization strength tensor  grad dtype             math ops cast self  l2 regularization strength tensor  grad dtype             math ops cast self  l2 shrinkage regularization strength tensor                          grad dtype             math ops cast self  learn rate power tensor  grad dtype             use locking=self  use lock  
class gradientdescentoptimizer optimizer optimizer     string    def   init   self  learn rate  use locking=false  name=string       string     super gradientdescentoptimizer  self    init   use lock  name      self  learn rate = learn rate     self  learn rate tensor = none    def  apply dense self  grad  var       return train ops apply gradient descent          var          math ops cast self  learn rate tensor  var dtype base dtype           grad          use locking=self  use lock  op    def  resource apply dense self  grad  handle       return train ops resource apply gradient descent          handle handle  math ops cast self  learn rate tensor                                       grad dtype base dtype           grad  use locking=self  use lock     def  resource apply sparse duplicate indices self  grad  handle  indices       return resource variable ops resource scatter add          handle handle  indices  -grad   self  learn rate     def  apply sparse duplicate indices self  grad  var       delta = ops indexedslices          grad value           math ops cast self  learn rate tensor  var dtype base dtype           grad indices  grad dense shape      return var scatter sub delta  use locking=self  use lock     def  prepare self       learn rate = self  call if callable self  learn rate      self  learn rate tensor = ops convert to tensor          learn rate  name=string  
class looperthread thread thread     string    def   init   self  coord  timer interval secs  target=none  args=none                 kwargs=none       string     if not isinstance coord  coordinator         raise valueerror string   coord      super looperthread  self    init         self daemon = true     self  coord = coord     self  timer interval secs = timer interval secs     self  target = target     if self  target        self  args = args or          self  kwargs = kwargs or        elif args or kwargs        raise valueerror string                        string      self  coord register thread self      staticmethod   def loop coord  timer interval secs  target  args=none  kwargs=none       string     looper = looperthread coord  timer interval secs  target=target  args=args                            kwargs=kwargs      looper start       return looper    def run self       with self  coord stop on exception          self start loop         if self  timer interval secs be none                   while not self  coord should stop              self run loop         else                   next timer time = time time           while not self  coord wait for stop next timer time - time time               next timer time  = self  timer interval secs           self run loop         self stop loop      def start loop self       string     pass    def stop loop self       string     pass    def run loop self       string     if self  target        self  target  self  args    self  kwargs  
class momentumoptimizer optimizer optimizer     string    def   init   self  learn rate  momentum                 use locking=false  name=string  use nesterov=false       string     super momentumoptimizer  self    init   use lock  name      self  learn rate = learn rate     self  momentum = momentum     self  use nesterov = use nesterov    def  create slot self  var list       for v in var list        self  zero slot v  string  self  name     def  prepare self       learn rate = self  learn rate     if callable learn rate         learn rate = learn rate       self  learn rate tensor = ops convert to tensor learn rate                                                         name=string      momentum = self  momentum     if callable momentum         momentum = momentum       self  momentum tensor = ops convert to tensor momentum  name=string     def  apply dense self  grad  var       mom = self get slot var  string      return train ops apply momentum          var  mom          math ops cast self  learn rate tensor  var dtype base dtype           grad          math ops cast self  momentum tensor  var dtype base dtype           use locking=self  use lock          use nesterov=self  use nesterov  op    def  resource apply dense self  grad  var       mom = self get slot var  string      return train ops resource apply momentum          var handle  mom handle          math ops cast self  learn rate tensor  grad dtype base dtype           grad          math ops cast self  momentum tensor  grad dtype base dtype           use locking=self  use lock          use nesterov=self  use nesterov     def  apply sparse self  grad  var       mom = self get slot var  string      return train ops sparse apply momentum          var  mom          math ops cast self  learn rate tensor  var dtype base dtype           grad value  grad indices          math ops cast self  momentum tensor  var dtype base dtype           use locking=self  use lock          use nesterov=self  use nesterov  op    def  resource apply sparse self  grad  var  indices       mom = self get slot var  string      return train ops resource sparse apply momentum          var handle  mom handle          math ops cast self  learn rate tensor  grad dtype           grad  indices          math ops cast self  momentum tensor  grad dtype           use locking=self  use lock          use nesterov=self  use nesterov  
class monitoredsession  monitoredsession     string    def   init   self                 session creator=none                 hooks=none                 stop grace period secs=120       super monitoredsession  self    init            session creator          hook          should recover=true          stop grace period secs=stop grace period secs  
 tf export v1= string   def monitoredtrainingsession      master=string        be chief=true      checkpoint dir=none      scaffold=none      hooks=none      chief only hooks=none      save checkpoint secs=use default      save summaries steps=use default      save summaries secs=use default      config=none      stop grace period secs=120      log step count steps=100      max wait secs=7200      save checkpoint steps=use default      summary dir=none      save graph def=true     string   if save summaries step == use default and save summaries secs == use default      save summaries step = 100     save summaries secs = none   elif save summaries secs == use default      save summaries secs = none   elif save summaries step == use default      save summaries step = none    if  save checkpoint step == use default and       save checkpoint secs == use default       save checkpoint step = none     save checkpoint secs = 600   elif save checkpoint secs == use default      save checkpoint secs = none   elif save checkpoint step == use default      save checkpoint step = none    scaffold = scaffold or scaffold     worker context = distribute coordinator context get current worker context      if worker context      return  create monitor session with worker context          worker context          scaffold          checkpoint dir=checkpoint dir          hooks=hooks          chief only hooks=chief only hook          save checkpoint secs=save checkpoint secs          save summaries steps=save summaries step          save summaries secs=save summaries secs          config=config          stop grace period secs=stop grace period secs          log step count steps=log step count step          max wait secs=max wait secs          save checkpoint steps=save checkpoint step          summary dir=summary dir          save graph def=save graph def     if not be chief      session creator = workersessioncreator          scaffold=scaffold          master=master          config=config          max wait secs=max wait secs      return monitoredsession          session creator=session creator          hooks=hooks or             stop grace period secs=stop grace period secs     all hook =      if chief only hook      all hook extend chief only hook    session creator = chiefsessioncreator        scaffold=scaffold        checkpoint dir=checkpoint dir        master=master        config=config     summary dir = summary dir or checkpoint dir   if summary dir      if log step count step and log step count step > 0        all hook append            basic session run hook stepcounterhook                output dir=summary dir  every n steps=log step count step        if  save summaries step and         save summaries step > 0  or  save summaries secs and                                       save summaries secs > 0         all hook append            basic session run hook summarysaverhook                scaffold=scaffold                save steps=save summaries step                save secs=save summaries secs                output dir=summary dir      if checkpoint dir      if  save checkpoint secs and         save checkpoint secs > 0  or  save checkpoint step and                                       save checkpoint step > 0         all hook append            basic session run hook checkpointsaverhook                checkpoint dir                save steps=save checkpoint step                save secs=save checkpoint secs                scaffold=scaffold                save graph def=save graph def      if hook      all hook extend hook    return monitoredsession        session creator=session creator        hooks=all hook        stop grace period secs=stop grace period secs  
 tf export v1= string   def newcheckpointreader filepattern     string   try      return checkpointreader compat as bytes filepattern           except runtimeerror as e      error translator e  
class optimizer                     trackable trackable     string       gate none = 0   gate op = 1   gate graph = 2    def   init   self  use lock  name       string     if not name        raise valueerror string      self  use lock = use lock     self  name = name                         self  slot =        self  non slot dict =                                      self  defer slot restorations =                                      def get name self       return self  name    def minimize self  loss  global step=none  var list=none                 gate gradients=gate op  aggregation method=none                 colocate gradients with ops=false  name=none                 grad loss=none       string     grads and vars = self compute gradients          loss  var list=var list  gate gradients=gate gradients          aggregation method=aggregation method          colocate gradients with ops=colocate gradients with ops          grad loss=grad loss       vars with grad =  v for g  v in grads and vars if g be not none      if not vars with grad        raise valueerror            string           string               str v  for    v in grads and vars   loss        return self apply gradients grads and vars  global step=global step                                  name=name     def compute gradients self  loss  var list=none                          gate gradients=gate op                          aggregation method=none                          colocate gradients with ops=false                          grad loss=none       string     if callable loss         with backprop gradienttape   as tape          if var list be not none            tape watch var list          loss value = loss                                                loss value = self  scale loss loss value         if var list be none          var list = tape watch variables                       with ops control dependencies  loss value            grads = tape gradient loss value  var list  grad loss        return list zip grads  var list             if context execute eagerly          raise runtimeerror            string           string            loss = self  scale loss loss       if gate gradients not in  optimizer gate none  optimizer gate op                                optimizer gate graph         raise valueerror string                        string                          gate gradients      self  assert valid dtypes  loss       if grad loss be not none        self  assert valid dtypes  grad loss       if var list be none        var list =             variables trainable variables               ops get collection ops graphkeys trainable resource variables       else        var list = nest flatten var list           var list  = ops get collection ops graphkeys  stream model port           processors =   get processor v  for v in var list      if not var list        raise valueerror string      var refs =  p target   for p in processors      grads = gradients gradients          loss  var refs  grad ys=grad loss          gate gradients= gate gradients == optimizer gate op           aggregation method=aggregation method          colocate gradients with ops=colocate gradients with ops      if gate gradients == optimizer gate graph        grads = control flow ops tuple grads      grads and vars = list zip grads  var list       self  assert valid dtypes           v for g  v in grads and vars          if g be not none and v dtype  = dtypes resource       return grads and vars     staticmethod   def  scale loss loss value       ops get default graph    be loss scale by optimizer = false       if distribute lib get loss reduction   == ds reduce util reduceop mean        num replicas = distribute ctx get strategy   num replicas in sync       if num replicas > 1          loss value  =  1  / num replicas          ops get default graph    be loss scale by optimizer = true       return loss value    def apply gradients self  grads and vars  global step=none  name=none       string                                    if distribute ctx have strategy                 if distribute ctx in cross replica context            raise runtimeerror string                            string         grads and vars = get filter grad fn lambda  grads and vars          return distribute ctx get replica context   merge call            self  distribute apply  args= grads and vars  global step  name             grads and vars = tuple grads and vars        if not grads and vars        raise valueerror string      convert grads and vars =        for g  v in grads and vars        if g be not none          try                       g = ops convert to tensor or index slice g          except typeerror            raise typeerror                string               string   g          if not isinstance g   ops tensor  ops indexedslices              raise typeerror                string   g        p =  get processor v        convert grads and vars append  g  v  p        convert grads and vars = tuple convert grads and vars      var list =  v for g  v    in convert grads and vars if g be not none      if not var list        raise valueerror string                            str v  for    v    in convert grads and vars         with ops init scope          self  create slot var list      update ops =        with ops name scope name  self  name  as name        self  prepare         for grad  var  processor in convert grads and vars          if grad be none            continue                                    if  context execute eagerly   or             resource variable ops be resource variable var              and not var  in graph mode               scope name = string         else            scope name = var op name         with ops name scope string   scope name   ops colocate with var             update ops append processor update op self  grad         if global step be none          apply update = self  finish update ops  name        else          with ops control dependencies  self  finish update ops  string               with ops colocate with global step               if isinstance                  global step  resource variable ops baseresourcevariable                                               apply update = resource variable ops assign add variable op                    global step handle                    ops convert to tensor 1  dtype=global step dtype                     name=name              else                apply update = state ops assign add global step  1  name=name         if not context execute eagerly            if isinstance apply update  ops tensor             apply update = apply update op         train op = ops get collection ref ops graphkeys train op          if apply update not in train op            train op append apply update         return apply update    def  distribute apply self                           distribution                           grads and vars                           global step=none                           name=none       string     reduce grads = distribution extend batch reduce to          ds reduce util reduceop sum  grads and vars      var list =  v for    v in grads and vars      grads and vars = zip reduce grads  var list            with ops init scope          self  create slot var list       def update v  g         string       assert v be not none        try                   g = ops convert to tensor or index slice g        except typeerror          raise typeerror string                         string   g        if not isinstance g   ops tensor  ops indexedslices            raise typeerror              string   g        p =  get processor v         if context execute eagerly   or             resource variable ops be resource variable v  and           not v  in graph mode             scope name = v name split string  0        else          scope name = v op name                             with ops name scope string   scope name           return p update op self  g       with ops name scope name  self  name  as name        self  prepare          update ops =             op           for grad  var in grads and vars           for op in distribution extend update                var  update  args= grad    group=false                 def finish self  update ops           return self  finish update ops  string         non slot devices = distribution extend non slot devices var list        finish update = distribution extend update non slot            non slot devices  finish  args= self  update ops   group=false        if global step be none          apply update = distribution group finish update  name=name        else          with ops control dependencies finish update             apply update = distribution extend update                global step  state ops assign add  args= 1                  kwargs= string  name          if not context execute eagerly            if isinstance apply update  ops tensor             apply update = apply update op         train op = ops get collection ref ops graphkeys train op          if apply update not in train op            train op append apply update         return apply update    def get slot self  var  name       string          name slot = self  slot get name  none      if not name slot        return none      if hasattr var  string                       distribute container = var  distribute container         assert distribute container be not none       if ops execute eagerly outside function            key = distribute container  unique id       else          key =  distribute container graph  distribute container  share name               mirror slot = name slot get key  none        if mirror slot be none  return none       return mirror slot get device=var device       return name slot get  var key var   none     def get slot name self       string     return sort self  slot key       def variables self       string     current graph = ops get default graph        def  from current graph variable         if variable  in graph mode            return variable op graph be current graph       else                            return variable  graph key == current graph  graph key        optimizer variables =  v for v in self  non slot variables                              if  from current graph v       for    variable dict in self  slot items          for    slot for variable in variable dict items            if  from current graph slot for variable             optimizer variables append slot for variable           return sort optimizer variables  key=lambda v  v name     def  create non slot variable self  initial value  name  colocate with       string          eager = context execute eagerly       graph = none if eager else colocate with graph      key =  name  graph      v = self  non slot dict get key  none      if v be none        self  maybe initialize trackable         distribution strategy = distribute ctx get strategy         with distribution strategy extend colocate vars with colocate with           if eager            restore initial value = self  preload simple restoration                name=name  shape=none            if restore initial value be not none              initial value = restore initial value         v = variable scope variable              initial value  name=name  trainable=false              use resource=resource variable ops be resource variable                  colocate with                                                   self  handle defer dependencies name=name  trackable=v        self  non slot dict key  = v      return v     property   def  checkpoint dependencies self       string     current graph non slot variables =        current graph key = ops get default graph    graph key       for  name      variable object in sort self  non slot dict items                                                                                               key=lambda item  item 0  0          if variable object  graph key == current graph key            current graph non slot variables append              trackable trackablereference                  name=name  ref=variable object       return  super optimizer  self   checkpoint dependencies               current graph non slot variables     def  lookup dependency self  name       string     unconditional = super optimizer  self   lookup dependency name      if unconditional be not none        return unconditional     graph = none if context execute eagerly   else ops get default graph       return self  get non slot variable name  graph=graph     def  get non slot variable self  name  graph=none       non slot = self  non slot dict get  name  graph   none      if hasattr non slot  string                       return non slot get       else        return non slot    def  non slot variables self       string     return self  non slot dict value      def  assert valid dtypes self  tensors       string     valid dtypes = self  valid dtypes       for t in tensors        dtype = t dtype base dtype       if dtype not in valid dtypes          raise valueerror              string                     dtype  t name   v for v in valid dtypes                   def  valid dtypes self       string     return set           dtypes float16  dtypes bfloat16  dtypes float32  dtypes float64      def  create slot self  var list       string          pass    def  prepare self       string     pass    def  apply dense self  grad  var       string     raise notimplementederror      def  resource apply dense self  grad  handle       string     raise notimplementederror      def  resource apply sparse duplicate indices self  grad  handle  indices       string     sum grad  unique indices =  deduplicate index slice          values=grad  indices=indices      return self  resource apply sparse sum grad  handle  unique indices     def  resource apply sparse self  grad  handle  indices       string     raise notimplementederror      def  apply sparse duplicate indices self  grad  var       string     sum value  unique indices =  deduplicate index slice          values=grad value  indices=grad indices      gradient no duplicate indices = ops indexedslices          indices=unique indices          values=summed value          dense shape=grad dense shape      return self  apply sparse gradient no duplicate indices  var     def  apply sparse self  grad  var       string     raise notimplementederror      def  finish self  update ops  name scope       string     return control flow ops group  update ops  name=name scope               def  slot dict self  slot name       string     name slot = self  slot get slot name  none      if name slot be none        name slot =          self  slot slot name  = name slot     return name slot    def  get or make slot self  var  val  slot name  op name       string     name slot = self  slot dict slot name      if  var key var  not in name slot        new slot variable = slot creator create slot var  val  op name        self  restore slot variable            slot name=slot name  variable=var            slot variable=new slot variable        name slot  var key var   = new slot variable     return name slot  var key var      def  get or make slot with initializer self  var  initializer  shape  dtype                                           slot name  op name       string     name slot = self  slot dict slot name      if  var key var  not in name slot        new slot variable = slot creator create slot with initializer            var  initializer  shape  dtype  op name        self  restore slot variable            slot name=slot name  variable=var            slot variable=new slot variable        name slot  var key var   = new slot variable     return name slot  var key var      def  zero slot self  var  slot name  op name       string     name slot = self  slot dict slot name      if  var key var  not in name slot        new slot variable = slot creator create zero slot var  op name        self  restore slot variable            slot name=slot name  variable=var            slot variable=new slot variable        name slot  var key var   = new slot variable     return name slot  var key var                def  restore slot variable self  slot name  variable  slot variable       string     variable key =  var key variable      defer restorations = self  defer slot restorations get          slot name      pop variable key                    defer restorations sort key=lambda position  position restore uid                                 reverse=true      for checkpoint position in defer restorations        checkpoint position restore slot variable     def  create or restore slot variable        self  slot variable position  slot name  variable       string     name slot = self  slot dict slot name      variable key =  var key variable      slot variable = name slot get variable key  none      if  slot variable be none and context execute eagerly   and         slot variable position be simple variable                                                                                   and not ops get default graph    variable creator stack           initializer = trackable checkpointinitialvalue            checkpoint position=slot variable position        slot variable = self  get or make slot            var=variable            val=initializer            slot name=slot name            op name=self  name                                                       if slot variable be not none                      slot variable position restore slot variable      else                                    self  defer slot restorations setdefault            slot name      setdefault variable key      append                slot variable position     def  call if callable self  param       string     return param   if callable param  else param 
class proximaladagradoptimizer optimizer optimizer        string    def   init   self  learn rate  initial accumulator value=0 1                 l1 regularization strength=0 0  l2 regularization strength=0 0                 use locking=false  name=string       string     if initial accumulator value <= 0 0        raise valueerror string                          initial accumulator value      super proximaladagradoptimizer  self    init   use lock  name      self  learn rate = learn rate     self  initial accumulator value = initial accumulator value     self  l1 regularization strength = l1 regularization strength     self  l2 regularization strength = l2 regularization strength          self  l1 regularization strength tensor = none     self  l2 regularization strength tensor = none     self  learn rate tensor = none    def  create slot self  var list       for v in var list        with ops colocate with v           val = constant op constant self  initial accumulator value                                     shape=v get shape                                       dtype=v dtype base dtype        self  get or make slot v  val  string  self  name     def  prepare self       self  learn rate tensor = ops convert to tensor self  learn rate                                                         name=string      self  l1 regularization strength tensor = ops convert to tensor          self  l1 regularization strength          name=string      self  l2 regularization strength tensor = ops convert to tensor          self  l2 regularization strength          name=string     def  apply dense self  grad  var       acc = self get slot var  string      return train ops apply proximal adagrad          var  acc  self  learn rate tensor          self  l1 regularization strength tensor          self  l2 regularization strength tensor          grad  use locking=self  use lock     def  resource apply dense self  grad  var       acc = self get slot var  string      return train ops resource apply proximal adagrad          var handle  acc handle  self  learn rate tensor          self  l1 regularization strength tensor          self  l2 regularization strength tensor          grad  use locking=self  use lock     def  apply sparse self  grad  var       acc = self get slot var  string      return train ops sparse apply proximal adagrad          var  acc  self  learn rate tensor          self  l1 regularization strength tensor          self  l2 regularization strength tensor          grad value  grad indices          use locking=self  use lock     def  resource apply sparse self  grad  var  indices       acc = self get slot var  string      return train ops resource sparse apply proximal adagrad          var handle  acc handle          math ops cast self  learn rate tensor  grad dtype           math ops cast self  l1 regularization strength tensor  grad dtype           math ops cast self  l2 regularization strength tensor  grad dtype           grad  indices          use locking=self  use lock  
class proximalgradientdescentoptimizer optimizer optimizer        string    def   init   self  learn rate  l1 regularization strength=0 0                 l2 regularization strength=0 0  use locking=false                 name=string       string     super proximalgradientdescentoptimizer  self    init   use lock  name      self  learn rate = learn rate     self  l1 regularization strength = l1 regularization strength     self  l2 regularization strength = l2 regularization strength     self  l1 regularization strength tensor = none     self  l2 regularization strength tensor = none    def  apply dense self  grad  var       return train ops apply proximal gradient descent          var          self  learn rate tensor          self  l1 regularization strength tensor          self  l2 regularization strength tensor          grad          use locking=self  use lock  op    def  resource apply dense self  grad  var       return train ops resource apply proximal gradient descent          var handle          self  learn rate tensor          self  l1 regularization strength tensor          self  l2 regularization strength tensor          grad          use locking=self  use lock     def  apply sparse self  grad  var       return train ops sparse apply proximal gradient descent          var          self  learn rate tensor          self  l1 regularization strength tensor          self  l2 regularization strength tensor          grad value          grad indices          use locking=self  use lock  op    def  resource apply sparse self  grad  var  indices       return train ops resource sparse apply proximal gradient descent          var handle          math ops cast self  learn rate tensor  grad dtype           math ops cast self  l1 regularization strength tensor  grad dtype           math ops cast self  l2 regularization strength tensor  grad dtype           grad          indices          use locking=self  use lock     def  prepare self       self  learn rate tensor = ops convert to tensor self  learn rate                                                         name=string      self  l1 regularization strength tensor = ops convert to tensor          self  l1 regularization strength  name=string      self  l2 regularization strength tensor = ops convert to tensor          self  l2 regularization strength  name=string  
class queuerunner object     string     deprecation deprecate none   deprecation instruction    def   init   self  queue=none  enqueue ops=none  close op=none                 cancel op=none  queue close exception types=none                 queue runner def=none  import scope=none       string     if context execute eagerly          raise runtimeerror            string           string       if queue runner def        if queue or enqueue ops          raise valueerror string        self  init from proto queue runner def                              import scope=import scope      else        self  init from args            queue=queue  enqueue ops=enqueue ops            close op=close op  cancel op=cancel op            queue close exception types=queue close exception type           self  lock = thread lock                 self  run per session = weakref weakkeydictionary            self  exceptions raise =       def  init from args self  queue=none  enqueue ops=none  close op=none                        cancel op=none  queue close exception types=none       string     if not queue or not enqueue ops        raise valueerror string      self  queue = queue     self  enqueue ops = enqueue ops     self  close op = close op     self  cancel op = cancel op     if queue close exception type be not none        if  not isinstance queue close exception type  tuple            or not queue close exception type           or not all issubclass t  errors operror                       for t in queue close exception type            raise typeerror              string             string               queue close exception type      self  queue close exception type = queue close exception type               if self  close op be none        self  close op = self  queue close                 if self  cancel op be none        self  cancel op = self  queue close cancel pending enqueues=true      if not self  queue close exception type        self  queue close exception type =  errors outofrangeerror       else        self  queue close exception type = tuple            self  queue close exception type     def  init from proto self  queue runner def  import scope=none       string     assert isinstance queue runner def  queue runner pb2 queuerunnerdef      g = ops get default graph       self  queue = g as graph element          ops prepend name scope queue runner def queue name  import scope       self  enqueue ops =  g as graph element          ops prepend name scope op  import scope                            for op in queue runner def enqueue op name      self  close op = g as graph element ops prepend name scope          queue runner def close op name  import scope       self  cancel op = g as graph element ops prepend name scope          queue runner def cancel op name  import scope       self  queue close exception type = tuple          errors exception type from error code code          for code in queue runner def queue close exception type                if not self  queue close exception type        self  queue close exception type =  errors outofrangeerror       property   def queue self       return self  queue     property   def enqueue ops self       return self  enqueue ops     property   def close op self       return self  close op     property   def cancel op self       return self  cancel op     property   def queue close exception type self       return self  queue close exception type     property   def exceptions raise self       string     return self  exceptions raise     property   def name self       string     return self  queue name       def  run self  sess  enqueue op  coord=none       string     decremented = false     try                      enqueue callable = sess make callable enqueue op        while true          if coord and coord should stop              break         try            enqueue callable           except self  queue close exception type                         with self  lock              self  run per session sess  -= 1             decremented = true             if self  run per session sess  == 0                try                  sess run self  close op                except exception as e                                   log vlog 1  string  str e               return     except exception as e               if coord          coord request stop e        else          log error string  str e           with self  lock            self  exceptions raise append e          raise     finally               if not decremented          with self  lock            self  run per session sess  -= 1    def  close on stop self  sess  cancel op  coord       string     coord wait for stop       try        sess run cancel op      except exception as e               log vlog 1  string  str e         def create thread self  sess  coord=none  daemon=false  start=false       string     with self  lock        try          if self  run per session sess  > 0                       return          except keyerror                   pass       self  run per session sess  = len self  enqueue ops        self  exceptions raise =         ret thread =        for op in self  enqueue ops        name = string format self name  op name        ret thread append thread thread target=self  run                                            args= sess  op  coord                                             name=name       if coord        name = string format self name        ret thread append thread thread target=self  close on stop                                            args= sess  self  cancel op  coord                                             name=name       for t in ret thread        if coord          coord register thread t        if daemon          t daemon = true       if start          t start       return ret thread    def to proto self  export scope=none       string     if  export scope be none or         self queue name startswith export scope          queue runner def = queue runner pb2 queuerunnerdef         queue runner def queue name = ops strip name scope            self queue name  export scope        for enqueue op in self enqueue ops          queue runner def enqueue op name append              ops strip name scope enqueue op name  export scope         queue runner def close op name = ops strip name scope            self close op name  export scope        queue runner def cancel op name = ops strip name scope            self cancel op name  export scope        queue runner def queue close exception type extend             errors error code from exception type cls            for cls in self  queue close exception type         return queue runner def     else        return none     staticmethod   def from proto queue runner def  import scope=none       string     return queuerunner queue runner def=queue runner def                         import scope=import scope  
class rmspropoptimizer optimizer optimizer     string    def   init   self                 learn rate                 decay=0 9                 momentum=0 0                 epsilon=1e-10                 use locking=false                 centered=false                 name=string       string     super rmspropoptimizer  self    init   use lock  name      self  learn rate = learn rate     self  decay = decay     self  momentum = momentum     self  epsilon = epsilon     self  center = center           self  learn rate tensor = none     self  decay tensor = none     self  momentum tensor = none     self  epsilon tensor = none    def  create slot self  var list       for v in var list        if v get shape   be fully define            init rms = init ops ones initializer dtype=v dtype base dtype        else          init rms = array ops ones like v        self  get or make slot with initializer v  init rms  v get shape                                                  v dtype base dtype  string                                                self  name        if self  center          self  zero slot v  string  self  name        self  zero slot v  string  self  name     def  prepare self       lr = self  call if callable self  learn rate      decay = self  call if callable self  decay      momentum = self  call if callable self  momentum      epsilon = self  call if callable self  epsilon       self  learn rate tensor = ops convert to tensor lr  name=string      self  decay tensor = ops convert to tensor decay  name=string      self  momentum tensor = ops convert to tensor momentum  name=string      self  epsilon tensor = ops convert to tensor epsilon  name=string     def  apply dense self  grad  var       rms = self get slot var  string      mom = self get slot var  string      if self  center        mg = self get slot var  string        return train ops apply center rms prop            var            mg            rms            mom            math ops cast self  learn rate tensor  var dtype base dtype             math ops cast self  decay tensor  var dtype base dtype             math ops cast self  momentum tensor  var dtype base dtype             math ops cast self  epsilon tensor  var dtype base dtype             grad            use locking=self  use lock  op     else        return train ops apply rms prop            var            rms            mom            math ops cast self  learn rate tensor  var dtype base dtype             math ops cast self  decay tensor  var dtype base dtype             math ops cast self  momentum tensor  var dtype base dtype             math ops cast self  epsilon tensor  var dtype base dtype             grad            use locking=self  use lock  op    def  resource apply dense self  grad  var       rms = self get slot var  string      mom = self get slot var  string      if self  center        mg = self get slot var  string        return train ops resource apply center rms prop            var handle            mg handle            rms handle            mom handle            math ops cast self  learn rate tensor  grad dtype base dtype             math ops cast self  decay tensor  grad dtype base dtype             math ops cast self  momentum tensor  grad dtype base dtype             math ops cast self  epsilon tensor  grad dtype base dtype             grad            use locking=self  use lock      else        return train ops resource apply rms prop            var handle            rms handle            mom handle            math ops cast self  learn rate tensor  grad dtype base dtype             math ops cast self  decay tensor  grad dtype base dtype             math ops cast self  momentum tensor  grad dtype base dtype             math ops cast self  epsilon tensor  grad dtype base dtype             grad            use locking=self  use lock     def  apply sparse self  grad  var       rms = self get slot var  string      mom = self get slot var  string      if self  center        mg = self get slot var  string        return train ops sparse apply center rms prop            var            mg            rms            mom            math ops cast self  learn rate tensor  var dtype base dtype             math ops cast self  decay tensor  var dtype base dtype             math ops cast self  momentum tensor  var dtype base dtype             math ops cast self  epsilon tensor  var dtype base dtype             grad value            grad indices            use locking=self  use lock      else        return train ops sparse apply rms prop            var            rms            mom            math ops cast self  learn rate tensor  var dtype base dtype             math ops cast self  decay tensor  var dtype base dtype             math ops cast self  momentum tensor  var dtype base dtype             math ops cast self  epsilon tensor  var dtype base dtype             grad value            grad indices            use locking=self  use lock     def  resource apply sparse self  grad  var  indices       rms = self get slot var  string      mom = self get slot var  string      if self  center        mg = self get slot var  string        return train ops resource sparse apply center rms prop            var handle            mg handle            rms handle            mom handle            math ops cast self  learn rate tensor  grad dtype             math ops cast self  decay tensor  grad dtype             math ops cast self  momentum tensor  grad dtype             math ops cast self  epsilon tensor  grad dtype             grad            indices            use locking=self  use lock      else        return train ops resource sparse apply rms prop            var handle            rms handle            mom handle            math ops cast self  learn rate tensor  grad dtype             math ops cast self  decay tensor  grad dtype             math ops cast self  momentum tensor  grad dtype             math ops cast self  epsilon tensor  grad dtype             grad            indices            use locking=self  use lock  
class saver object     string    def   init   self                 var list=none                 reshape=false                 sharded=false                 max to keep=5                 keep checkpoint every n hours=10000 0                 name=none                 restore sequentially=false                 saver def=none                 builder=none                 defer build=false                 allow empty=false                 write version=saver pb2 saverdef v2                 pad step number=false                 save relative paths=false                 filename=none       string     if defer build and var list        raise valueerror            string           string      if context execute eagerly          log warn            string           string           string           string           string        if var list be none          raise runtimeerror              string             string      self  var list = var list     self  reshape = reshape     self  sharded = sharded     self  max to keep = max to keep     self  keep checkpoint every n hours = keep checkpoint every n hours     self  name = name     self  restore sequentially = restore sequentially     self saver def = saver def     self  builder = builder     self  be build = false     self  allow empty = allow empty     self  be empty = none     self  write version = write version     self  pad step number = pad step number     self  filename = filename     self  last checkpoints =        self  checkpoints to be delete =        if context execute eagerly          self  next checkpoint time =             time time     self  keep checkpoint every n hours   3600      elif not defer build        self build       if self saver def        self  check saver def         self  write version = self saver def version     self  save relative paths = save relative paths               self  object restore saver = none    def build self       if context execute eagerly          raise runtimeerror string      self  build self  filename  build save=true  build restore=true     def  build eager self  checkpoint path  build save  build restore       self  build          checkpoint path  build save=build save  build restore=build restore     def  build self  checkpoint path  build save  build restore       string     if not context execute eagerly          if self  be build          return       self  be build = true      if not self saver def or context execute eagerly          if self  builder be none          self  builder = bulksaverbuilder self  write version         if self  var list be none                   self  var list = variables  all saveable object         if not self  var list          if self  allow empty            self  be empty = true           return         else            raise valueerror string        self  be empty = false        self saver def = self  builder  build internal              self  var list            reshape=self  reshape            sharded=self  sharded            max to keep=self  max to keep            keep checkpoint every n hours=self  keep checkpoint every n hours            name=self  name            restore sequentially=self  restore sequentially            filename=checkpoint path            build save=build save            build restore=build restore      elif self saver def and self  name                             self saver def filename tensor name = ops prepend name scope            self saver def filename tensor name  self  name        self saver def save tensor name = ops prepend name scope            self saver def save tensor name  self  name        self saver def restore op name = ops prepend name scope            self saver def restore op name  self  name       self  check saver def       if not context execute eagerly                        self  next checkpoint time =             time time     self saver def keep checkpoint every n hours   3600     def  check saver def self       if not isinstance self saver def  saver pb2 saverdef         raise valueerror string                          self saver def      if not context execute eagerly          if not self saver def save tensor name          raise valueerror string                            str self saver def         if not self saver def restore op name          raise valueerror string                            str self saver def      def  checkpointfilename self  p       string     name    = p     return name    def  recordlastcheckpoint self  latest save path       string     if not self saver def max to keep        return          for p in self  last checkpoints        if latest save path == self  checkpointfilename p           self  last checkpoints remove p           self  last checkpoints append  latest save path  time time               if len self  last checkpoints  > self saver def max to keep        self  checkpoints to be delete append self  last checkpoints pop 0      def  maybedeleteoldcheckpoints self  meta graph suffix=string       string     if self  checkpoints to be delete        p = self  checkpoints to be delete pop 0                      should keep = p 1  > self  next checkpoint time       if should keep          self  next checkpoint time  =               self saver def keep checkpoint every n hours   3600          return               try          checkpoint management remove checkpoint              self  checkpointfilename p   self saver def version              meta graph suffix        except exception as e            log warn string  str e      def as saver def self       string     return self saver def    def to proto self  export scope=none       string     if export scope be none        return self saver def      if not  self saver def filename tensor name startswith export scope  and             self saver def save tensor name startswith export scope  and             self saver def restore op name startswith export scope          return none      saver def = saver pb2 saverdef       saver def copyfrom self saver def      saver def filename tensor name = ops strip name scope          saver def filename tensor name  export scope      saver def save tensor name = ops strip name scope          saver def save tensor name  export scope      saver def restore op name = ops strip name scope saver def restore op name                                                       export scope      return saver def     staticmethod   def from proto saver def  import scope=none       string     return saver saver def=saver def  name=import scope      property   def last checkpoints self       string     return list self  checkpointfilename p  for p in self  last checkpoints     def set last checkpoints self  last checkpoints       string     assert isinstance last checkpoints  list                     self  last checkpoints =   s  np inf  for s in last checkpoints     def set last checkpoints with time self  last checkpoints with time       string     assert isinstance last checkpoints with time  list      self  last checkpoints = last checkpoints with time    def recover last checkpoints self  checkpoint paths       string     checkpoints with mtimes =        for checkpoint path in checkpoint paths        mtime = checkpoint management get checkpoint mtimes  checkpoint path         if mtime          checkpoints with mtimes append  checkpoint path  mtime 0        self set last checkpoints with time checkpoints with mtimes     def save self             sess             save path             global step=none             latest filename=none             meta graph suffix=string             write meta graph=true             write state=true             strip default attrs=false             save debug info=false            string          if not self  be build and not context execute eagerly          raise runtimeerror            string      if latest filename be none        latest filename = string     if self  write version  = saver pb2 saverdef v2        log warn string        log warn string        log warn string        log warn string        log warn string        log warn string       if os path split latest filename  0         raise valueerror string       if global step be not none        if not isinstance global step  compat integral type           global step = train util global step sess  global step        checkpoint file = string    save path  global step        if self  pad step number                   checkpoint file = string    save path  string format global step       else        checkpoint file = save path       if os path basename save path  == latest filename and not self  sharded                   raise valueerror              string                latest filename  save path        if  not context execute eagerly   and         not isinstance sess  session sessioninterface          raise typeerror string   sess       save path parent = os path dirname save path      if not self  be empty        try          if context execute eagerly              self  build eager                checkpoint file  build save=true  build restore=false            model checkpoint path = self saver def save tensor name         else            model checkpoint path = sess run                self saver def save tensor name                 self saver def filename tensor name  checkpoint file            model checkpoint path = compat as str model checkpoint path          if write state            self  recordlastcheckpoint model checkpoint path            checkpoint management update checkpoint state internal                save dir=save path parent                model checkpoint path=model checkpoint path                all model checkpoint paths=self last checkpoints                latest filename=latest filename                save relative paths=self  save relative paths            self  maybedeleteoldcheckpoints meta graph suffix=meta graph suffix        except  errors failedpreconditionerror  errors notfounderror  as exc          if not gfile isdirectory save path parent             exc = valueerror                string format                    save path           raise exc      if write meta graph        meta graph filename = checkpoint management meta graph filename            checkpoint file  meta graph suffix=meta graph suffix        if not context execute eagerly            with sess graph as default              self export meta graph                meta graph filename                strip default attrs=strip default attrs                save debug info=save debug info       if self  be empty        return none     else        return model checkpoint path    def export meta graph self                          filename=none                          collection list=none                          as text=false                          export scope=none                          clear devices=false                          clear extraneous savers=false                          strip default attrs=false                          save debug info=false            string          return export meta graph          filename=filename          graph def=ops get default graph   as graph def add shapes=true           saver def=self saver def          collection list=collection list          as text=as text          export scope=export scope          clear devices=clear devices          clear extraneous savers=clear extraneous savers          strip default attrs=strip default attrs          save debug info=save debug info     def restore self  sess  save path       string     if self  be empty        return     if save path be none        raise valueerror string       checkpoint prefix = compat as text save path      if not checkpoint management checkpoint exist internal checkpoint prefix         raise valueerror string                          checkpoint prefix       log info string  checkpoint prefix      try        if context execute eagerly            self  build eager save path  build save=false  build restore=true        else          sess run self saver def restore op name                    self saver def filename tensor name  save path       except errors notfounderror as err                                                   try          name to key = object graph key map save path        except errors notfounderror                                     raise  wrap restore error with msg              err  string                       log warn            string           string           string           string        self  object restore saver = saver from object base checkpoint            checkpoint path=save path            var list=self  var list            builder=self  builder            name to keys=names to key            cache saver=self  object restore saver        self  object restore saver restore sess=sess  save path=save path      except errors invalidargumenterror as err                      raise  wrap restore error with msg            err  string      staticmethod   def  add collection def meta graph def  key  export scope=none       string     meta graph add collection def          meta graph def  key  export scope=export scope  
class scaffold object     string    def   init   self                 init op=none                 init fee dict=none                 init fn=none                 ready op=none                 ready for local init op=none                 local init op=none                 summary op=none                 saver=none                 copy from scaffold=none                 local init fee dict=none       string     if copy from scaffold be not none        if not isinstance copy from scaffold  scaffold           raise typeerror string                      coalesce = lambda a  b  a if a be not none else b       init op = coalesce init op  copy from scaffold init op        init fee dict = coalesce init fee dict                                  copy from scaffold init fee dict               init fn = coalesce init fn  copy from scaffold  user init fn          ready op = coalesce ready op  copy from scaffold ready op        ready for local init op = coalesce            ready for local init op  copy from scaffold ready for local init op        local init op = coalesce local init op  copy from scaffold local init op        local init fee dict = coalesce local init fee dict                                        copy from scaffold local init fee dict        summary op = coalesce summary op  copy from scaffold summary op        saver = coalesce saver  copy from scaffold saver                 self  user init fn = init fn     if init fn        self  init fn = lambda sess  init fn self  sess      else        self  init fn = none      self  init op = init op     self  init fee dict = init fee dict     self  ready op = ready op     self  ready for local init op = ready for local init op     self  local init op = local init op     self  local init fee dict = local init fee dict     self  summary op = summary op     self  saver = saver    def finalize self       string     if self  init op be none         def default init op            return control flow ops group              variables global variables initializer                resources initialize resources resources share resources            self  init op = scaffold get or default string  ops graphkeys init op                                                default init op      if self  ready op be none         def default ready op            return array ops concat               variables report uninitialized variables                resources report uninitialized resources              0         self  ready op = scaffold get or default string                                                 ops graphkeys ready op                                                 default ready op      if self  ready for local init op be none         def default ready for local init op            return array ops concat               variables report uninitialized variables                  variables global variables                 resources report uninitialized resources                  resources share resources               0         self  ready for local init op = scaffold get or default            string  ops graphkeys ready for local init op            default ready for local init op      if self  local init op be none        self  local init op = scaffold get or default            string  ops graphkeys local init op            scaffold default local init op      if self  summary op be none        self  summary op = scaffold get or default string                                                   ops graphkeys summary op                                                   summary merge all           if self  saver be none        self  saver = train saver  get saver or default              if isinstance self  saver  trackable util checkpoint         self  saver = train saver saver            var list=graph view objectgraphview                self  saver  freeze saveable object              sharded=true      else        self  saver build        ops get default graph   finalize       log info string      return self     property   def init fn self       return self  init fn     property   def init op self       return self  init op     property   def ready op self       return self  ready op     property   def ready for local init op self       return self  ready for local init op     property   def local init op self       return self  local init op     property   def local init fee dict self       return self  local init fee dict     property   def summary op self       return self  summary op     property   def saver self       return self  saver     property   def init fee dict self       return self  init fee dict     staticmethod   def get or default arg name  collection key  default constructor       string     elements = ops get collection collection key      if elements        if len elements  > 1          raise runtimeerror              string             string             string             string  collection key  arg name        return elements 0      op = default constructor       if op be not none        ops add to collection collection key  op      return op     staticmethod   def default local init op        string     return control flow ops group          variables local variables initializer            lookup ops table initializer            resources initialize resources resources local resources     
class sessioncreator object     string     abc abstractmethod   def create session self       raise notimplementederror          string format self   
class sessionmanager object     string    def   init   self                 local init op=none                 ready op=none                 ready for local init op=none                 graph=none                 recovery wait secs=30                 local init run options=none                 local init fee dict=none       string          if graph be none        graph = ops get default graph       self  local init op = local init op     self  ready op = ready op     self  ready for local init op = ready for local init op     self  graph = graph     self  recovery wait secs = recovery wait secs     self  target = none     self  local init run options = local init run options     self  local init fee dict = local init fee dict     if ready for local init op be not none and local init op be none        raise valueerror string                        string                        string                          ready for local init op     def  restore checkpoint self                            master                            saver=none                            checkpoint dir=none                            checkpoint filename with path=none                            wait for checkpoint=false                            max wait secs=7200                            config=none       string     self  target = master                     strategy = distribution strategy context get strategy       if strategy and hasattr strategy extend                              string         strategy extend  experimental initialize system          sess = session session self  target  graph=self  graph  config=config      if checkpoint dir and checkpoint filename with path        raise valueerror string                        string                if not saver or not  checkpoint dir or checkpoint filename with path         return sess  false      if checkpoint filename with path        saver restore sess  checkpoint filename with path        return sess  true           wait time = 0     ckpt = checkpoint management get checkpoint state checkpoint dir      while not ckpt or not ckpt model checkpoint path        if wait for checkpoint and wait time < max wait secs          log info string          time sleep self  recovery wait secs          wait time  = self  recovery wait secs         ckpt = checkpoint management get checkpoint state checkpoint dir        else          return sess  false           saver restore sess  ckpt model checkpoint path      saver recover last checkpoints ckpt all model checkpoint paths      return sess  true    def prepare session self                        master                        init op=none                        saver=none                        checkpoint dir=none                        checkpoint filename with path=none                        wait for checkpoint=false                        max wait secs=7200                        config=none                        init fee dict=none                        init fn=none       string      sess  be load from checkpoint = self  restore checkpoint          master          saver          checkpoint dir=checkpoint dir          checkpoint filename with path=checkpoint filename with path          wait for checkpoint=wait for checkpoint          max wait secs=max wait secs          config=config      if not be load from checkpoint        if init op be none and not init fn and self  local init op be none          raise runtimeerror string                            string        if init op be not none          sess run init op  fee dict=init fee dict        if init fn          init fn sess       local init success  msg = self  try run local init op sess      if not local init success        raise runtimeerror            string           string     maybe name init op                                                      init fn                                                     msg        be ready  msg = self  model ready sess      if not be ready        raise runtimeerror            string           string               maybe name init op   init fn  self  local init op  msg       return sess    def recover session self                        master                        saver=none                        checkpoint dir=none                        checkpoint filename with path=none                        wait for checkpoint=false                        max wait secs=7200                        config=none       string      sess  be load from checkpoint = self  restore checkpoint          master          saver          checkpoint dir=checkpoint dir          checkpoint filename with path=checkpoint filename with path          wait for checkpoint=wait for checkpoint          max wait secs=max wait secs          config=config            local init success  msg = self  try run local init op sess       if not be load from checkpoint               return sess  false      restore file = checkpoint dir or checkpoint filename with path     if not local init success        log info            string           string  restore file  msg        return sess  false      be ready  msg = self  model ready sess      if not be ready        log info string                     restore file  msg        return sess  false      log info string  restore file      return sess  be load from checkpoint    def wait for session self  master  config=none  max wait secs=float string        string     self  target = master      if max wait secs be none        max wait secs = float string      timer =  countdowntimer max wait secs       while true        sess = session session self  target  graph=self  graph  config=config        not ready msg = none       not ready local msg = none       local init success  not ready local msg = self  try run local init op            sess        if local init success                   be ready  not ready msg = self  model ready sess          if be ready            return sess        self  safe close sess                remain ms after wait =             timer secs remain   - self  recovery wait secs        if remain ms after wait < 0          raise errors deadlineexceedederror              none  none              string    max wait secs           log info string                    string                     not ready local msg  not ready msg        time sleep self  recovery wait secs     def  safe close self  sess       string          try        sess close       except exception                             pass         def  model ready self  sess       string     return  ready self  ready op  sess  string     def  model ready for local init self  sess       string     return  ready self  ready for local init op  sess                    string     def  try run local init op self  sess       string     if self  local init op be not none        be ready for local init  msg = self  model ready for local init sess        if be ready for local init          log info string          sess run self  local init op  fee dict=self  local init fee dict                   options=self  local init run options          log info string          return true  none       else          return false  msg     return true  none 
class singularmonitoredsession  monitoredsession     string    def   init   self                 hooks=none                 scaffold=none                 master=string                 config=none                 checkpoint dir=none                 stop grace period secs=120                 checkpoint filename with path=none       string     session creator = chiefsessioncreator          scaffold=scaffold          master=master          config=config          checkpoint dir=checkpoint dir          checkpoint filename with path=checkpoint filename with path      super singularmonitoredsession  self    init            session creator          hook          should recover=false          stop grace period secs=stop grace period secs     def raw session self       string     return self  tf sess   
class supervisor object     string             use default = 0     deprecation deprecate none                            string    def   init   self                 graph=none                 ready op=use default                 ready for local init op=use default                 be chief=true                 init op=use default                 init fee dict=none                 local init op=use default                 logdir=none                 summary op=use default                 saver=use default                 global step=use default                 save summaries secs=120                 save model secs=600                 recovery wait secs=30                 stop grace secs=120                 checkpoint basename=string                 session manager=none                 summary writer=use default                 init fn=none                 local init run options=none       string     if context execute eagerly          raise runtimeerror string           if graph be none        graph = ops get default graph       with graph as default          self  init ready op            ready op=ready op  ready for local init op=ready for local init op        self  init init op init op=init op  init fee dict=init fee dict        self  init local init op local init op=local init op        self  init saver saver=saver        self  init summary op summary op=summary op        self  init global step global step=global step      self  graph = graph     self  meta graph def = meta graph create meta graph def          graph def=graph as graph def add shapes=true           saver def=self  saver saver def if self  saver else none      self  be chief = be chief     self  coord = coordinator coordinator       self  recovery wait secs = recovery wait secs     self  stop grace secs = stop grace secs     self  init fn = init fn     self  local init run options = local init run options                     self  logdir = none     self  save summaries secs = none     self  save model secs = none     self  save path = none     self  summary writer = none      if self  be chief        self  logdir = logdir       self  save summaries secs = save summaries secs       self  save model secs = save model secs       if self  logdir          self  save path = os path join self  logdir  checkpoint basename        if summary writer be supervisor use default          if self  logdir            self  summary writer =  summary filewriter self  logdir        else          self  summary writer = summary writer       self  graph add to summary = false      self  init session manager session manager=session manager      self  verify setup            graph finalize      def  init session manager self  session manager=none       if session manager be none        self  session manager = session manager mod sessionmanager            local init op=self  local init op            ready op=self  ready op            ready for local init op=self  ready for local init op            graph=self  graph            recovery wait secs=self  recovery wait secs            local init run options=self  local init run options      else        self  session manager = session manager    def  get first op from collection self  key       string     try        op list = ops get collection key        if len op list  > 1          log info string                       len op list   key        if op list          return op list 0      except lookuperror        pass      return none    def  init ready op self                       ready op=use default                       ready for local init op=use default       string     if ready op be supervisor use default        ready op = self  get first op from collection ops graphkeys ready op        if ready op be none          ready op = variables report uninitialized variables           ops add to collection ops graphkeys ready op  ready op      self  ready op = ready op           if ready for local init op be supervisor use default        ready for local init op = self  get first op from collection            ops graphkeys ready for local init op      self  ready for local init op = ready for local init op    def  init init op self  init op=use default  init fee dict=none       string     if init op be supervisor use default        init op = self  get first op from collection ops graphkeys init op        if init op be none          init op = variables global variables initializer           ops add to collection ops graphkeys init op  init op      self  init op = init op     self  init fee dict = init fee dict    def  init local init op self  local init op=use default       string     if local init op be supervisor use default        local init op = self  get first op from collection            ops graphkeys local init op        if local init op be none          op list =               variables local variables initializer                lookup ops table initializer                     if op list            local init op = control flow ops group  op list            ops add to collection ops graphkeys local init op  local init op      self  local init op = local init op    def  init saver self  saver=use default       string     if saver be supervisor use default        saver = self  get first op from collection ops graphkeys savers        if saver be none and variables global variables            saver = saver mod saver           ops add to collection ops graphkeys savers  saver      self  saver = saver    def  init summary op self  summary op=use default       string     if summary op be supervisor use default        summary op = self  get first op from collection ops graphkeys summary op        if summary op be none          summary op =  summary merge all           if summary op be not none            ops add to collection ops graphkeys summary op  summary op      self  summary op = summary op    def  init global step self  global step=use default       string     if global step be supervisor use default        global step = self  get first op from collection            ops graphkeys global step        if global step be none          global step = self  default global step tensor           if global step be not none            ops add to collection ops graphkeys global step  global step      self  global step = global step     property   def be chief self       string     return self  be chief     property   def session manager self       string     return self  session manager     property   def coord self       string     return self  coord     property   def init op self       string     return self  init op     property   def init fee dict self       string     return self  init fee dict     property   def ready op self       string     return self  ready op     property   def ready for local init op self       return self  ready for local init op     property   def summary writer self       string     return self  summary writer     property   def summary op self       string     return self  summary op     property   def save summaries secs self       string     return self  save summaries secs     property   def global step self       string     return self  global step     property   def saver self       string     return self  saver     property   def save model secs self       string     return self  save model secs     property   def save path self       string     return self  save path    def  write graph self       string     assert self  be chief     if self  logdir        train util write graph            self  graph as graph def add shapes=true   self  logdir            string      if self  summary writer and not self  graph add to summary        self  summary writer add graph self  graph        self  summary writer add meta graph self  meta graph def        self  graph add to summary = true    def start standard service self  sess       string     if not self  be chief        raise runtimeerror string                          string       if not self  logdir        log warn string                       string        return      if self  global step be not none and self  summary writer                             current step = train util global step sess  self  global step        self  summary writer add session log            sessionlog status=sessionlog start   current step       thread =        if self  save summaries secs and self  summary writer        if self  summary op be not none          thread append svsummarythread self  sess         if self  global step be not none          thread append svstepcounterthread self  sess       if self saver and self  save model secs        thread append svtimercheckpointthread self  sess       for t in thread        t start       return thread    def prepare or wait for session self                                    master=string                                    config=none                                    wait for checkpoint=false                                    max wait secs=7200                                    start standard services=true       string                    self  coord clear stop       if self  summary writer        self  summary writer reopen        if self  be chief        sess = self  session manager prepare session            master            init op=self init op            saver=self saver            checkpoint dir=self  logdir            wait for checkpoint=wait for checkpoint            max wait secs=max wait secs            config=config            init fee dict=self  init fee dict            init fn=self  init fn        self  write graph         if start standard service          log info string          self start standard service sess      else        sess = self  session manager wait for session            master  config=config  max wait secs=max wait secs      if start standard service        log info string        self start queue runners sess      return sess    def start queue runners self  sess  queue runners=none       string     if context execute eagerly          raise runtimeerror string      if queue runners be none        queue runners = self  graph get collection ops graphkeys queue runners      thread =        for qr in queue runners        thread extend            qr create thread sess  coord=self  coord  daemon=true  start=true       return thread    def loop self  timer interval secs  target  args=none  kwargs=none       string     looper = coordinator looperthread          self  coord          timer interval secs          target=target          args=args          kwargs=kwargs      looper start       return looper    def stop self             threads=none             close summary writer=true             ignore live threads=false       string     self  coord request stop       try                             self  coord join            thread            stop grace period secs=self  stop grace secs            ignore live threads=ignore live thread      finally               if close summary writer and self  summary writer                            self  summary writer add session log sessionlog status=sessionlog stop           self  summary writer close           self  graph add to summary = false    def request stop self  ex=none       string     self  coord request stop ex=ex     def should stop self       string     return self  coord should stop      def stop on exception self       string     return self  coord stop on exception      def wait for stop self       string     self  coord wait for stop      def summary compute self  sess  summary  global step=none       string     if not self  summary writer        raise runtimeerror string      if global step be none and self global step be not none        global step = train util global step sess  self global step      self  summary writer add summary summary  global step     def  default global step tensor self       string     try        gs = ops get default graph   get tensor by name string        if gs dtype base dtype in  dtypes int32  dtypes int64           return gs       else          log warn string  gs dtype          return none     except keyerror        return none    def  verify setup self       string               if not self  be chief        for op in self  graph get operations            if op type in  string  string  and not op device            raise valueerror string                            string   op         contextlib contextmanager   def manage session self                        master=string                        config=none                        start standard services=true                        close summary writer=true       string     try        sess = self prepare or wait for session            master=master            config=config            start standard services=start standard service        yield sess     except exception as e        self request stop e      finally        try                                                       self stop close summary writer=close summary writer        finally                                     try            sess close           except exception                       pass 
class syncreplicasoptimizer optimizer optimizer     string     deprecation deprecate        none        string       string       string        warn once=true    def   init   self                 opt                 replicas to aggregate                 total num replicas=none                 variable averages=none                 variables to average=none                 use locking=false                 name=string       string     if total num replicas be none        total num replicas = replicas to aggregate      super syncreplicasoptimizer  self    init   use lock  name      log info          string          replicas to aggregate  total num replicas      self  opt = opt     self  replicas to aggregate = replicas to aggregate     self  gradients apply = false     self  variable average = variable average     self  variables to average = variables to average     self  total num replicas = total num replicas     self  tokens per step = max total num replicas  replicas to aggregate      self  global step = none     self  sync token queue = none                self  chief queue runner = none                     self  accumulator list =       def compute gradients self   args    kwargs       string     return self  opt compute gradients  args    kwargs     def apply gradients self  grads and vars  global step=none  name=none       string     if not grads and vars        raise valueerror string       if global step be none        raise valueerror string       self  global step = global step     train ops =        aggregate grad =        var list =              local anchor = control flow ops no op            distribution strategy = distribution strategy context get strategy       with distribution strategy extend colocate vars with local anchor         self  local step = variable scope variable            initial value=0            trainable=false            collections= ops graphkeys local variables             dtype=global step dtype base dtype            name=string       self local step init op = state ops assign self  local step  global step      chief init ops =  self local step init op      self ready for local init op = variables report uninitialized variables          variables global variables         with ops name scope none  self  name         for grad  var in grads and vars          var list append var          with ops device var device                        if grad be none              aggregate grad append none                continue           elif isinstance grad  ops tensor               grad accum = data flow ops conditionalaccumulator                  grad dtype                  shape=var get shape                    share name=var name   string              train ops append grad accum apply grad                  grad  local step=self  local step               aggregate grad append grad accum take grad                  self  replicas to aggregate             else              if not isinstance grad  ops indexedslices                 raise valueerror string              grad accum = data flow ops sparseconditionalaccumulator                  grad dtype  shape=    share name=var name   string              train ops append grad accum apply index slice grad                  grad  local step=self  local step               aggregate grad append grad accum take index slice grad                  self  replicas to aggregate              self  accumulator list append  grad accum  var device          aggregate grads and vars = zip aggregate grad  var list                with ops device global step device   ops name scope string           update op = self  opt apply gradients aggregate grads and vars                                                global step                with ops device global step device   ops name scope string           sync token queue =               data flow ops fifoqueue -1                                      global step dtype base dtype                                      shapes=                                        name=string                                      share name=string           self  sync token queue = sync token queue                                     dummy queue =               data flow ops fifoqueue 1                                      type pb2 dt int32                                      shapes=                                        name=string                                      share name=string          with ops device global step device   ops name scope string                    with ops control dependencies train ops             token = sync token queue dequeue           train op = state ops assign self  local step  token           with ops control dependencies  update op                                    tokens = array ops fill  self  tokens per step   global step            sync op = sync token queue enqueue many  tokens             if self  variable average be not none            with ops control dependencies  sync op    ops name scope string               sync op = self  variable average apply                  self  variables to average           self  chief queue runner = queue runner queuerunner dummy queue                                                               sync op         for accum  dev in self  accumulator list          with ops device dev             chief init ops append                accum set global step                    global step  name=string         self chief init op = control flow ops group   chief init ops         self  gradients apply = true       return train op    def get chief queue runner self       string     if self  gradients apply be false        raise valueerror string       return self  chief queue runner    def get slot self   args    kwargs       string     return self  opt get slot  args    kwargs     def variables self       string     return self  opt variables      def get slot name self   args    kwargs       string     return self  opt get slot name  args    kwargs     def get init tokens op self  num tokens=-1       string     if self  gradients apply be false        raise valueerror            string       tokens need = self  replicas to aggregate - self  total num replicas     if num tokens == -1        num tokens = self  replicas to aggregate     elif num tokens < tokens need        raise valueerror            string              num tokens  tokens need        if num tokens > 0        with ops device self  global step device   ops name scope string           tokens = array ops fill  num tokens   self  global step          init tokens = self  sync token queue enqueue many  tokens        else        init tokens = control flow ops no op name=string       return init tokens    def make session run hook self  be chief  num tokens=-1       string     return  syncreplicasoptimizerhook self  be chief  num tokens  
class workersessioncreator sessioncreator     string    def   init   self                 scaffold=none                 master=string                 config=none                 max wait secs=30   60       string     self  scaffold = scaffold or scaffold       self  session manager = none     self  master = master     self  config = config     self  max wait secs = max wait secs    def  get session manager self       string     if self  session manager        return self  session manager      self  session manager = sm sessionmanager          local init op=self  scaffold local init op          local init fee dict=self  scaffold local init fee dict          ready op=self  scaffold ready op          ready for local init op=self  scaffold ready for local init op          graph=ops get default graph        return self  session manager    def create session self       self  scaffold finalize       return self  get session manager   wait for session          self  master  config=self  config  max wait secs=self  max wait secs  
 tf export v1= string  string    deprecation deprecate none   deprecation instruction  def add queue runner qr  collection=ops graphkeys queue runners     string   ops add to collection collection  qr  
 tf export v1= string   def assert global step global step tensor     string   if not  isinstance global step tensor  variables variable  or           isinstance global step tensor  ops tensor  or           resource variable ops be resource variable global step tensor        raise typeerror string                       global step tensor     if not global step tensor dtype base dtype be integer      raise typeerror string                       global step tensor dtype     if  global step tensor get shape   ndims  = 0 and       global step tensor get shape   be fully define         raise typeerror string                       global step tensor get shape    
 tf export v1= string   def basic train loop supervisor                       train step fn                       args=none                       kwargs=none                       master=string     string   if args be none      args =      if kwargs be none      kwargs =      should retry = true   while should retry      try        should retry = false       with supervisor manage session master  as sess          while not supervisor should stop              train step fn sess   args    kwargs      except errors abortederror                      should retry = true 
 tf export v1= string    deprecation deprecate      none  string     string     string  def batch tensors  batch size  num threads=1  capacity=32            enqueue many=false  shapes=none  dynamic pad=false            allow smaller final batch=false  share name=none  name=none     string   return  batch        tensors        batch size        keep input=true        num threads=num thread        capacity=capacity        enqueue many=enqueue many        shapes=shapes        dynamic pad=dynamic pad        allow smaller final batch=allow smaller final batch        share name=shared name        name=name  
 tf export v1= string    deprecation deprecate      none  string     string     string  def batch join tensors list  batch size  capacity=32  enqueue many=false                 shapes=none  dynamic pad=false  allow smaller final batch=false                 share name=none  name=none     string   return  batch join        tensors list        batch size        keep input=true        capacity=capacity        enqueue many=enqueue many        shapes=shapes        dynamic pad=dynamic pad        allow smaller final batch=allow smaller final batch        share name=shared name        name=name  
 deprecation deprecate      date=none      instructions=string   tf export v1= string   def checkpoint exist checkpoint prefix     string   return checkpoint exist internal checkpoint prefix  
 tf export v1= string   def cosine decay learn rate  global step  decay step  alpha=0 0  name=none     string   decay lr = learn rate schedule cosinedecay        learn rate  decay step  alpha=alpha  name=name     if not context execute eagerly        decay lr = decay lr global step    else      decay lr = functools partial decay lr  global step    return decay lr 
 tf export v1= string   def cosine decay restart learn rate                            global step                            first decay step                            t mul=2 0                            m mul=1 0                            alpha=0 0                            name=none     string   decay lr = learn rate schedule cosinedecayrestarts        learn rate        first decay step        t mul=t mul        m mul=m mul        alpha=alpha        name=name     if not context execute eagerly        decay lr = decay lr global step    else      decay lr = functools partial decay lr  global step    return decay lr 
 tf export v1= string   def create global step graph=none     string   graph = graph or ops get default graph     if get global step graph  be not none      raise valueerror string    if context execute eagerly        with ops device string         return variable scope get variable            ops graphkeys global step            shape=              dtype=dtypes int64            initializer=init ops zero initializer              trainable=false            aggregation=variables variableaggregation only first replica            collections=                ops graphkeys global variables  ops graphkeys global step                   with graph as default   as g  g name scope none       return variable scope get variable          ops graphkeys global step          shape=            dtype=dtypes int64          initializer=init ops zero initializer            trainable=false          aggregation=variables variableaggregation only first replica          collections= ops graphkeys global variables  ops graphkeys global step   
 deprecation deprecate      none      string   tf export v1= string   def do quantize train on graphdef input graph  num bits     string    graph = graph pb2 graphdef     result graph string = doquantizetrainingongraphdefhelper        input graph serializetostring    num bits     graph parsefromstring result graph string    return graph 
 tf export v1= string   def exponential decay learn rate                        global step                        decay step                        decay rate                        staircase=false                        name=none     string   decay lr = learn rate schedule exponentialdecay        learn rate  decay step  decay rate  staircase=staircase  name=name    if not context execute eagerly        decay lr = decay lr global step    else      decay lr = functools partial decay lr  global step    return decay lr 
 tf export v1= string   def export meta graph filename=none                        meta info def=none                        graph def=none                        saver def=none                        collection list=none                        as text=false                        graph=none                        export scope=none                        clear devices=false                        clear extraneous savers=false                        strip default attrs=false                        save debug info=false                          kwargs        string      if context execute eagerly   and not  graph def be not none and                                           graph be not none       raise runtimeerror string                        string                        string    meta graph def    = meta graph export scoped meta graph        filename=filename        meta info def=meta info def        graph def=graph def        saver def=saver def        collection list=collection list        as text=as text        graph=graph        export scope=export scope        clear devices=clear devices        clear extraneous savers=clear extraneous savers        strip default attrs=strip default attrs        save debug info=save debug info          kwargs    return meta graph def 
 tf export v1= string   def generate checkpoint state proto save dir                                      model checkpoint path                                      all model checkpoint paths=none                                      all model checkpoint timestamps=none                                      last preserve timestamp=none     string   if all model checkpoint paths be none      all model checkpoint paths =       if  not all model checkpoint paths or       all model checkpoint paths -1   = model checkpoint path       log info string                   model checkpoint path      all model checkpoint paths append model checkpoint path     if  all model checkpoint timestamps       and  len all model checkpoint timestamps              = len all model checkpoint paths         raise valueerror           string          string             all model checkpoint paths  all model checkpoint timestamps            if not os path isabs save dir       if not os path isabs model checkpoint path         model checkpoint path = os path relpath model checkpoint path  save dir      for i  p in enumerate all model checkpoint paths         if not os path isabs p           all model checkpoint paths i  = os path relpath p  save dir     coord checkpoint proto = checkpointstate        model checkpoint path=model checkpoint path        all model checkpoint paths=all model checkpoint paths        all model checkpoint timestamps=all model checkpoint timestamps        last preserve timestamp=last preserve timestamp     return coord checkpoint proto 
 deprecation deprecate      date=none      instructions=string   tf export v1= string   def get checkpoint mtimes checkpoint prefix     string   mtimes =       def match maybe append pathname       fnames = file io get match file pathname      if fnames        mtimes append file io stat fnames 0   mtime nsec / 1e9        return true     return false    for checkpoint prefix in checkpoint prefix           pathname =  prefix to checkpoint path checkpoint prefix                                            saver pb2 saverdef v2      if match maybe append pathname         continue          match maybe append checkpoint prefix     return mtimes 
 tf export v1= string   def get global step graph=none     string   graph = graph or ops get default graph     global step tensor = none   global step tensors = graph get collection ops graphkeys global step    if len global step tensors  == 1      global step tensor = global step tensors 0    elif not global step tensors      try        global step tensor = graph get tensor by name string      except keyerror        return none   else      log error string      return none    assert global step global step tensor    return global step tensor 
 tf export v1= string   def get or create global step graph=none     string   graph = graph or ops get default graph     global step tensor = get global step graph    if global step tensor be none      global step tensor = create global step graph    return global step tensor 
 tf export v1= string   def global step sess  global step tensor     string   if context execute eagerly        return int global step tensor numpy      return int sess run global step tensor   
 tf export v1= string   def import meta graph meta graph or file                        clear devices=false                        import scope=none                          kwargs     string     return  import meta graph with return elements meta graph or file                                                   clear devices  import scope                                                     kwargs  0  
 tf export v1= string   def init from checkpoint ckpt dir or file  assignment map     string   init from checkpoint fn = lambda     init from checkpoint        ckpt dir or file  assignment map    if distribution strategy context get cross replica context        init from checkpoint fn none    else      distribution strategy context get replica context   merge call          init from checkpoint fn  
 tf export v1= string    deprecation deprecate      none  string     string     string     string  def input producer input tensor                     element shape=none                     num epochs=none                     shuffle=true                     seed=none                     capacity=32                     share name=none                     summary name=none                     name=none                     cancel op=none     string   if context execute eagerly        raise runtimeerror          string         string         string    with ops name scope name  string   input tensor        input tensor = ops convert to tensor input tensor  name=string      element shape = input tensor shape 1   merge with element shape      if not element shape be fully define          raise valueerror string                        string       if shuffle        input tensor = random ops random shuffle input tensor  seed=seed       input tensor = limit epochs input tensor  num epochs       q = data flow ops fifoqueue capacity=capacity                                  dtypes= input tensor dtype base dtype                                   shapes= element shape                                   share name=shared name  name=name      enq = q enqueue many  input tensor       queue runner add queue runner          queue runner queuerunner              q   enq   cancel op=cancel op       if summary name be not none        summary scalar summary name                       math ops cast q size    dtypes float32     1  / capacity       return q 
 tf export v1= string   def inverse time decay learn rate                         global step                         decay step                         decay rate                         staircase=false                         name=none     string   decay lr = learn rate schedule inversetimedecay        learn rate  decay step  decay rate  staircase=staircase  name=name     if not context execute eagerly        decay lr = decay lr global step    else      decay lr = functools partial decay lr  global step    return decay lr 
 tf export v1= string    deprecation deprecate      none  string     string  def limit epochs tensor  num epochs=none  name=none     string   if num epochs be none      return tensor   if num epochs <= 0      raise valueerror string   num epochs    with ops name scope name  string   tensor   as name      zero64 = constant op constant 0  dtype=dtypes int64      epochs = vs variable          zero64  name=string  trainable=false          collections= ops graphkeys local variables       counter = epochs count up to num epochs      with ops control dependencies  counter          return array ops identity tensor  name=name  
 tf export v1= string   def linear cosine decay learn rate                          global step                          decay step                          num periods=0 5                          alpha=0 0                          beta=0 001                          name=none     string   decay lr = learn rate schedule linearcosinedecay        learn rate        decay step        num periods=num periods        alpha=alpha        beta=beta        name=name     if not context execute eagerly        decay lr = decay lr global step    else      decay lr = functools partial decay lr  global step    return decay lr 
 tf export v1= string    deprecation deprecate      none  string     string     string  def maybe batch tensors  keep input  batch size  num threads=1  capacity=32                  enqueue many=false  shapes=none  dynamic pad=false                  allow smaller final batch=false  share name=none  name=none     string   return  batch        tensors        batch size        keep input        num threads=num thread        capacity=capacity        enqueue many=enqueue many        shapes=shapes        dynamic pad=dynamic pad        allow smaller final batch=allow smaller final batch        share name=shared name        name=name  
 tf export v1= string    deprecation deprecate      none  string     string     string  def maybe batch join tensors list  keep input  batch size  capacity=32                       enqueue many=false  shapes=none  dynamic pad=false                       allow smaller final batch=false  share name=none                       name=none     string   return  batch join        tensors list        batch size        keep input        capacity=capacity        enqueue many=enqueue many        shapes=shapes        dynamic pad=dynamic pad        allow smaller final batch=allow smaller final batch        share name=shared name        name=name  
 tf export v1= string    deprecation deprecate      none  string     string     string  def maybe shuffle batch tensors  batch size  capacity  min after dequeue                          keep input  num threads=1  seed=none                          enqueue many=false  shapes=none                          allow smaller final batch=false  share name=none                          name=none     string   return  shuffle batch        tensors        batch size        capacity        min after dequeue        keep input        num threads=num thread        seed=seed        enqueue many=enqueue many        shapes=shapes        allow smaller final batch=allow smaller final batch        share name=shared name        name=name  
 tf export v1= string    deprecation deprecate      none  string     string     string  def maybe shuffle batch join tensors list  batch size  capacity                               min after dequeue  keep input  seed=none                               enqueue many=false  shapes=none                               allow smaller final batch=false  share name=none                               name=none     string   return  shuffle batch join        tensors list        batch size        capacity        min after dequeue        keep input        seed=seed        enqueue many=enqueue many        shapes=shapes        allow smaller final batch=allow smaller final batch        share name=shared name        name=name  
 tf export v1= string   def natural exp decay learn rate                        global step                        decay step                        decay rate                        staircase=false                        name=none     string   natural exp rate = math ops exp math ops negative decay rate     decay lr = learn rate schedule exponentialdecay        learn rate        decay step        natural exp rate        staircase=staircase        name=name     if not context execute eagerly        decay lr = decay lr global step    else      decay lr = functools partial decay lr  global step    return decay lr 
 tf export v1= string   def noisy linear cosine decay learn rate                                global step                                decay step                                initial variance=1 0                                variance decay=0 55                                num periods=0 5                                alpha=0 0                                beta=0 001                                name=none     string   decay lr = learn rate schedule noisylinearcosinedecay        learn rate        decay step        initial variance=initial variance        variance decay=variance decay        num periods=num periods        alpha=alpha        beta=beta        name=name     if not context execute eagerly        decay lr = decay lr global step    else      decay lr = functools partial decay lr  global step    return decay lr 
 tf export v1= string  string   def piecewise constant x  boundaries  value  name=none     string   boundaries = ops convert n to tensor boundaries    value = ops convert n to tensor value    x recomp = ops convert to tensor x          for i  b in enumerate boundaries       if b dtype base dtype  = x recomp dtype base dtype                             if  b dtype base dtype == dtypes int32 and           x recomp dtype base dtype == dtypes int64           b = math ops cast b  x recomp dtype base dtype          boundaries i  = b       else          raise valueerror              string                b dtype base dtype  x recomp dtype base dtype     for v in value 1        if v dtype base dtype  = value 0  dtype base dtype        raise valueerror            string              value 0  dtype base dtype  v dtype base dtype     decay lr = learn rate schedule piecewiseconstantdecay        boundaries  value  name=name    if not context execute eagerly        decay lr = decay lr x    else      decay lr = functools partial decay lr  x    return decay lr 
 tf export v1= string   def polynomial decay learn rate                       global step                       decay step                       end learn rate=0 0001                       power=1 0                       cycle=false                       name=none     string   decay lr = learn rate schedule polynomialdecay        learn rate        decay step        end learn rate=end learn rate        power=power        cycle=cycle        name=name     if not context execute eagerly        decay lr = decay lr global step    else      decay lr = functools partial decay lr  global step    return decay lr 
 tf export v1= string    deprecation deprecate      none  string     string     string  def range input producer limit  num epochs=none  shuffle=true  seed=none                           capacity=32  share name=none  name=none     string   with ops name scope name  string   limit   as name      range tensor = math ops range limit      return input producer          range tensor      num epochs  shuffle  seed  capacity          share name  string   capacity  name  
 deprecation deprecate      date=none      instructions=string   tf export v1= string   def remove checkpoint checkpoint prefix                        checkpoint format version=saver pb2 saverdef v2                        meta graph suffix=string     string    delete file if exist        meta graph filename checkpoint prefix  meta graph suffix     if checkpoint format version == saver pb2 saverdef v2            delete file if exist checkpoint prefix   string       delete file if exist checkpoint prefix   string    else            delete file if exist checkpoint prefix  
 tf export v1= string   def replica device setter ps tasks=0                            ps device=string                            worker device=string                            merge devices=true                            cluster=none                            ps ops=none                            ps strategy=none     string   if cluster be not none      if isinstance cluster  server lib clusterspec         cluster spec = cluster as dict       else        cluster spec = server lib clusterspec cluster  as dict            ps job name = pydev devicespec from string ps device  job     if ps job name not in cluster spec or cluster spec ps job name  be none        return none     ps task = len cluster spec ps job name      if ps task == 0      return none    if ps ops be none                ps ops = list standard ps ops     if not merge devices      log warn          string         string    if ps strategy be none      ps strategy =  roundrobinstrategy ps task    if not six callable ps strategy       raise typeerror string    chooser =  replicadevicechooser ps task  ps device  worker device                                    merge devices  ps ops  ps strategy    return chooser device function 
  dispatch add dispatch list  tf export v1= string    deprecate endpoints string  def sdca fprint input  name=none     r   compute fingerprint of the input string     args      input  a `tensor` of type `string`        vector of string to compute fingerprint on      name  a name for the operation  optional      return      a `tensor` of type `int64`           ctx =  context  context or  context context     tld =  ctx  thread local data   if tld be eager      try         result =  pywrap tensorflow tfe py fastpathexecute           ctx  context handle  tld device name  string  name          tld op callbacks  input        return  result     except  core  fallbackexception        try          return sdca fprint eager fallback              input  name=name  ctx= ctx        except  core  symbolicexception          pass         except  typeerror  valueerror           result =  dispatch dispatch                sdca fprint  input=input  name=name          if result be not  dispatch opdispatcher not support            return result         raise     except  core  notokstatusexception as e         ops raise from not ok status e  name       try             op   output =  op def library  apply op helper          string  input=input  name=name    except  typeerror  valueerror       result =  dispatch dispatch            sdca fprint  input=input  name=name      if result be not  dispatch opdispatcher not support        return result     raise    result =  output      if  execute must record gradient         attrs =         input flat =  op input      execute record gradient          string   input flat   attrs   result     result  =  result   return  result 
  dispatch add dispatch list  tf export v1= string    deprecate endpoints string  def sdca optimizer sparse example indices  sparse feature indices  sparse feature value  dense feature  example weight  example label  sparse indices  sparse weight  dense weight  example state data  loss type  l1  l2  num loss partition  num inner iterations  adaptative=true  name=none     r   distribute version of stochastic dual coordinate ascent  sdca  optimizer for    linear model with l1   l2 regularization  as global optimization objective be   strongly-convex  the optimizer optimize the dual objective at each step  the   optimizer apply each update one example at a time  examples be sample   uniformly  and the optimizer be learn rate free and enjoy linear convergence   rate      proximal stochastic dual coordinate ascent  http //arxiv org/pdf/1211 2717v1 pdf  <br>   shai shalev-shwartz  tong zhang  2012      loss objective = \sum f  i   wx  i      l2 / 2     w  2   l1    w        add vs  average in distribute primal-dual optimization  http //arxiv org/abs/1502 03508  <br>   chenxin ma  virginia smith  martin jaggi  michael i  jordan    peter richtarik  martin takac  2015     stochastic dual coordinate ascent with adaptive probabilities  https //arxiv org/abs/1502 08053  <br>   dominik csiba  zheng qu  peter richtarik  2015    args      sparse example indices  a list of `tensor` object with type `int64`        a list of vectors which contain example indices      sparse feature indices  a list with the same length as `sparse example indices` of `tensor` object with type `int64`        a list of vectors which contain feature indices      sparse feature value  a list of `tensor` object with type `float32`        a list of vectors which contain feature value       associate with each feature group      dense feature  a list of `tensor` object with type `float32`        a list of matrices which contain the dense feature value      example weight  a `tensor` of type `float32`        a vector which contain the weight associate with each       example      example label  a `tensor` of type `float32`        a vector which contain the label/target associate with each       example      sparse indices  a list with the same length as `sparse example indices` of `tensor` object with type `int64`        a list of vectors where each value be the indices which have       correspond weight in sparse weight  this field maybe omit for the       dense approach      sparse weight  a list with the same length as `sparse example indices` of `tensor` object with type `float32`        a list of vectors where each value be the weight associate with       a sparse feature group      dense weight  a list with the same length as `dense features` of `tensor` object with type `float32`        a list of vectors where the value be the weight associate       with a dense feature group      example state data  a `tensor` of type `float32`        a list of vectors contain the example state data      loss type  a `string` from  ` logistic loss    square loss    hinge loss    smooth hinge loss    poisson loss `        type of the primal loss  currently sdcasolver support logistic        square and hinge losses      l1  a `float`  symmetric l1 regularization strength      l2  a `float`  symmetric l2 regularization strength      num loss partition  an `int` that be `>= 1`        number of partition of the global loss function      num inner iterations  an `int` that be `>= 1`        number of iterations per mini-batch      adaptative  an optional `bool`  default to `true`        whether to use adaptive sdca for the inner loop      name  a name for the operation  optional      return      a tuple of `tensor` object  out example state data  out delta sparse weight  out delta dense weight        out example state data  a `tensor` of type `float32`      out delta sparse weight  a list with the same length as `sparse example indices` of `tensor` object with type `float32`      out delta dense weight  a list with the same length as `dense features` of `tensor` object with type `float32`           ctx =  context  context or  context context     tld =  ctx  thread local data   if tld be eager      try         result =  pywrap tensorflow tfe py fastpathexecute           ctx  context handle  tld device name  string  name          tld op callbacks  sparse example indices  sparse feature indices          sparse feature value  dense feature  example weight          example label  sparse indices  sparse weight  dense weight          example state data  string  loss type  string  adaptative          string  l1  string  l2  string  num loss partition          string  num inner iterations         result =  sdcaoptimizeroutput  make  result        return  result     except  core  fallbackexception        try          return sdca optimizer eager fallback              sparse example indices  sparse feature indices              sparse feature value  dense feature  example weight              example label  sparse indices  sparse weight  dense weight              example state data  loss type=loss type  adaptative=adaptative              l1=l1  l2=l2  num loss partitions=num loss partition              num inner iterations=num inner iterations  name=name  ctx= ctx        except  core  symbolicexception          pass         except  typeerror  valueerror           result =  dispatch dispatch                sdca optimizer  sparse example indices=sparse example indices                                sparse feature indices=sparse feature indices                                sparse feature values=sparse feature value                                dense features=dense feature                                example weights=example weight                                example labels=example label                                sparse indices=sparse indices                                sparse weights=sparse weight                                dense weights=dense weight                                example state data=example state data                                loss type=loss type  l1=l1  l2=l2                                num loss partitions=num loss partition                                num inner iterations=num inner iterations                                adaptative=adaptative  name=name          if result be not  dispatch opdispatcher not support            return result         raise     except  core  notokstatusexception as e         ops raise from not ok status e  name       if not isinstance sparse example indices   list  tuple        raise typeerror          string         string   sparse example indices     attr num sparse feature = len sparse example indices    if not isinstance sparse feature indices   list  tuple        raise typeerror          string         string   sparse feature indices    if len sparse feature indices   =  attr num sparse feature      raise valueerror          string         string            len sparse feature indices    attr num sparse feature     if not isinstance sparse indices   list  tuple        raise typeerror          string         string   sparse indices    if len sparse indices   =  attr num sparse feature      raise valueerror          string         string            len sparse indices    attr num sparse feature     if not isinstance sparse weight   list  tuple        raise typeerror          string         string   sparse weight    if len sparse weight   =  attr num sparse feature      raise valueerror          string         string            len sparse weight    attr num sparse feature     if not isinstance sparse feature value   list  tuple        raise typeerror          string         string   sparse feature value     attr num sparse feature with value = len sparse feature value    if not isinstance dense feature   list  tuple        raise typeerror          string         string   dense feature     attr num dense feature = len dense feature    if not isinstance dense weight   list  tuple        raise typeerror          string         string   dense weight    if len dense weight   =  attr num dense feature      raise valueerror          string         string            len dense weight    attr num dense feature     loss type =  execute make str loss type  string    l1 =  execute make float l1  string    l2 =  execute make float l2  string    num loss partition =  execute make int num loss partition  string    num inner iterations =  execute make int num inner iterations  string    if adaptative be none      adaptative = true   adaptative =  execute make bool adaptative  string    try             op   output =  op def library  apply op helper          string  sparse example indices=sparse example indices                           sparse feature indices=sparse feature indices                           sparse feature values=sparse feature value                           dense features=dense feature                           example weights=example weight                           example labels=example label                           sparse indices=sparse indices                           sparse weights=sparse weight                           dense weights=dense weight                           example state data=example state data                           loss type=loss type  l1=l1  l2=l2                           num loss partitions=num loss partition                           num inner iterations=num inner iterations                           adaptative=adaptative  name=name    except  typeerror  valueerror       result =  dispatch dispatch            sdca optimizer  sparse example indices=sparse example indices                            sparse feature indices=sparse feature indices                            sparse feature values=sparse feature value                            dense features=dense feature                            example weights=example weight                            example labels=example label                            sparse indices=sparse indices                            sparse weights=sparse weight                            dense weights=dense weight                            example state data=example state data                            loss type=loss type  l1=l1  l2=l2                            num loss partitions=num loss partition                            num inner iterations=num inner iterations                            adaptative=adaptative  name=name      if result be not  dispatch opdispatcher not support        return result     raise    result =  output      if  execute must record gradient         attrs =  string   op get attr string   string                 op  get attr bool string   string                 op  get attr int string                 string                 op  get attr int string                 string   op  get attr int string                 string   op get attr string   string   op get attr string                 string   op  get attr int string                 string                 op  get attr int string        input flat =  op input      execute record gradient          string   input flat   attrs   result     result =  result  1      result 1 1    attr num sparse feature      result 1    attr num sparse feature      result =  result  2      result 2       result =  sdcaoptimizeroutput  make  result    return  result 
  dispatch add dispatch list  tf export v1= string    deprecate endpoints string  def sdca shrink l1 weight  l1  l2  name=none     r   apply l1 regularization shrink step on the parameters     args      weight  a list of `tensor` object with type mutable `float32`        a list of vectors where each value be the weight associate with a       feature group      l1  a `float`  symmetric l1 regularization strength      l2  a `float`        symmetric l2 regularization strength  should be a positive float      name  a name for the operation  optional      return      the create operation           ctx =  context  context or  context context     tld =  ctx  thread local data   if tld be eager      raise runtimeerror string       if not isinstance weight   list  tuple        raise typeerror          string         string   weight     attr num feature = len weight    l1 =  execute make float l1  string    l2 =  execute make float l2  string    try             op   output =  op def library  apply op helper          string  weights=weights  l1=l1  l2=l2  name=name    except  typeerror  valueerror       result =  dispatch dispatch            sdca shrink l1  weights=weights  l1=l1  l2=l2  name=name      if result be not  dispatch opdispatcher not support        return result     raise   return  op 
 tf export v1= string    deprecation deprecate      none  string     string  def shuffle batch tensors  batch size  capacity  min after dequeue                    num threads=1  seed=none  enqueue many=false  shapes=none                    allow smaller final batch=false  share name=none  name=none     string   return  shuffle batch        tensors        batch size        capacity        min after dequeue        keep input=true        num threads=num thread        seed=seed        enqueue many=enqueue many        shapes=shapes        allow smaller final batch=allow smaller final batch        share name=shared name        name=name  
 tf export v1= string    deprecation deprecate      none  string     string     string  def shuffle batch join tensors list  batch size  capacity                         min after dequeue  seed=none  enqueue many=false                         shapes=none  allow smaller final batch=false                         share name=none  name=none     string   return  shuffle batch join        tensors list        batch size        capacity        min after dequeue        keep input=true        seed=seed        enqueue many=enqueue many        shapes=shapes        allow smaller final batch=allow smaller final batch        share name=shared name        name=name  
 tf export v1= string    deprecation deprecate      none  string     string     string     string  def slice input producer tensor list  num epochs=none  shuffle=true  seed=none                           capacity=32  share name=none  name=none     string   with ops name scope name  string  tensor list       tensor list = ops convert n to tensor or index slice tensor list      if not tensor list        raise valueerror            string      range size = array ops shape tensor list 0   0                queue = range input producer range size  num epochs=num epochs                                   shuffle=shuffle  seed=seed  capacity=capacity                                   share name=shared name      index = queue dequeue       output =  array ops gather t  index  for t in tensor list      return output 
 tf export v1= string                 string    deprecation deprecate none   deprecation instruction  def start queue runners sess=none  coord=none  daemon=true  start=true                          collection=ops graphkeys queue runners     string   if context execute eagerly        raise runtimeerror string    if sess be none      sess = ops get default session       if not sess        raise valueerror string                        string                        string     if not isinstance sess  session sessioninterface            if sess   class     name   in           string  string         return        raise typeerror string                     string format sess   class        queue runners = ops get collection collection    if not queue runners      log warn          string         string         string     with sess graph as default        thread =        for qr in ops get collection collection         thread extend qr create thread sess  coord=coord  daemon=daemon                                         start=start     return thread 
 tf export v1= string    deprecation deprecate      none  string     string     string     string  def string input producer string tensor                            num epochs=none                            shuffle=true                            seed=none                            capacity=32                            share name=none                            name=none                            cancel op=none     string   not null err = string   if not isinstance string tensor  ops tensor  and not string tensor      raise valueerror not null err     with ops name scope name  string   string tensor   as name      string tensor = ops convert to tensor string tensor  dtype=dtypes string      with ops control dependencies           control flow ops assert              math ops greater array ops size string tensor   0                not null err            string tensor = array ops identity string tensor      return input producer          input tensor=string tensor          element shape=            num epochs=num epochs          shuffle=shuffle          seed=seed          capacity=capacity          share name=shared name          name=name          summary name=string   capacity          cancel op=cancel op  
 tf export v1= string   def summary iterator path        string      for r in tf record tf record iterator path       yield event pb2 event fromstring r  
 deprecation deprecate      date=none      instructions= string                   string    tf export v1= string   def update checkpoint state save dir                              model checkpoint path                              all model checkpoint paths=none                              latest filename=none                              all model checkpoint timestamps=none                              last preserve timestamp=none     string   update checkpoint state internal        save dir=save dir        model checkpoint path=model checkpoint path        all model checkpoint paths=all model checkpoint paths        latest filename=latest filename        save relative paths=false        all model checkpoint timestamps=all model checkpoint timestamps        last preserve timestamp=last preserve timestamp  
 tf export v1= string   def warm start ckpt to initialize from                 vars to warm start=string                 var name to vocab info=none                 var name to prev var name=none     string   log info string format ckpt to initialize from     group variables =  get group variables vars to warm start     if var name to vocab info be none      var name to vocab info =       if not var name to prev var name                          var name to prev var name =  get object checkpoint rename          ckpt to initialize from  group variables key       warmstarted count = 0                   prev var name use = set     vocab info use = set         vocabless vars =      for var name  variable in six iteritems group variables       prev var name = var name to prev var name get var name      if prev var name        prev var name use add var name      vocab info = var name to vocab info get var name      if vocab info        vocab info use add var name        warmstarted count  = 1       log debug            string           string           string format                var name  vocab info new vocab  vocab info new vocab size                vocab info old vocab   vocab info old vocab size if                                      vocab info old vocab size > 0 else string                 vocab info num oov bucket  prev var name or string                vocab info backup initializer or string          warm start var with vocab            variable            current vocab path=vocab info new vocab            current vocab size=vocab info new vocab size            prev ckpt=ckpt to initialize from            prev vocab path=vocab info old vocab            previous vocab size=vocab info old vocab size            current oov buckets=vocab info num oov bucket            prev tensor name=prev var name            initializer=vocab info backup initializer            axis=vocab info axis      else                      if vars to warm start          warmstarted count  = 1         log debug string format              var name  prev var name or string                                      if len variable  == 1            variable = variable 0          prev tensor name  var =  get var info variable  prev var name          vocabless vars prev tensor name  = var    checkpoint utils init from checkpoint ckpt to initialize from  vocabless vars    prev var name not use = set        var name to prev var name key    - prev var name use   vocab info not use = set var name to vocab info key    - vocab info use    log info string  warmstarted count     if prev var name not use      raise valueerror          string         string         string         string format prev var name not use                                       group variables key       if vocab info not use      raise valueerror          string         string         string         string format vocab info not use  group variables key     
 tf export v1= string   def my fact      string   return  gen user ops fact   
class classificationoutput exportoutput     string    def   init   self  scores=none  classes=none       string     if  score be not none         and not  isinstance score  ops tensor                   and score dtype be float          raise valueerror string                        string format score       if  class be not none         and not  isinstance class  ops tensor                   and dtypes as dtype class dtype  == dtypes string          raise valueerror string                        string format class       if score be none and class be none        raise valueerror string       self  score = score     self  class = class     property   def score self       return self  score     property   def class self       return self  class    def as signature def self  receiver tensors       if len receiver tensors   = 1        raise valueerror string                        string format receiver tensors           examples   = receiver tensors items       if dtypes as dtype examples dtype   = dtypes string        raise valueerror string                        string format receiver tensors       return signature def utils classification signature def          examples  self class  self score  
class exportoutput object     string      metaclass   = abc abcmeta     separator char = string     abc abstractmethod   def as signature def self  receiver tensors       string     pass    def  check output key self  key  error label            if isinstance key  tuple         key = self  separator char join key       if not isinstance key  six string type         raise valueerror            string format error label  key       return key    def  wrap and check output        self  output  single output default name  error label=none       string     if not isinstance output  dict         output =  single output default name  output       output dict =        for key  value in output items          error name = error label or single output default name       key = self  check output key key  error name        if not isinstance value  ops tensor           raise valueerror              string format                  error name  value          output dict key  = value     return output dict 
class predictoutput exportoutput     string    single output default name = string    def   init   self  output       string      self  output = self  wrap and check output          output  self  single output default name  error label=string      property   def output self       return self  output    def as signature def self  receiver tensors       return signature def utils predict signature def receiver tensors                                                       self output  
class regressionoutput exportoutput     string    def   init   self  value       string     if not  isinstance value  ops tensor  and value dtype be float         raise valueerror string                        string format value       self  value = value     property   def value self       return self  value    def as signature def self  receiver tensors       if len receiver tensors   = 1        raise valueerror string                        string format receiver tensors           examples   = receiver tensors items       if dtypes as dtype examples dtype   = dtypes string        raise valueerror string                        string format receiver tensors       return signature def utils regression signature def examples  self value  
class servinginputreceiver      collections namedtuple          string           string  string  string       string    def   new   cls                feature                receiver tensors                receiver tensors alternatives=none       feature = wrap and check input tensors          feature  string  allow int keys=true       receiver tensors = wrap and check input tensors receiver tensors                                                      string       if receiver tensors alternatives be not none        if not isinstance receiver tensors alternatives  dict           raise valueerror              string format                  receiver tensors alternatives         for alternative name  receiver tensors alt in             six iteritems receiver tensors alternatives                     receiver tensors alternatives alternative name  =               wrap and check input tensors                  receiver tensors alt  string        return super servinginputreceiver  cls    new            cls          features=features          receiver tensors=receiver tensors          receiver tensors alternatives=receiver tensors alternatives  
class tensorservinginputreceiver      collections namedtuple          string           string  string  string       string    def   new   cls                feature                receiver tensors                receiver tensors alternatives=none       if feature be none        raise valueerror string       check tensor feature  none       receiver = servinginputreceiver          features=features          receiver tensors=receiver tensors          receiver tensors alternatives=receiver tensors alternatives       return super tensorservinginputreceiver  cls    new            cls          features=receiver feature single feature default name           receiver tensors=receiver receiver tensors          receiver tensors alternatives=receiver receiver tensors alternatives  
 estimator export string  def build parse serve input receiver fn feature spec                                              default batch size=none     string    def serve input receiver fn        string     serialize tf example = array ops placeholder          dtype=dtypes string          shape= default batch size           name=string      receiver tensors =  string  serialize tf example      feature = parse ops parse example serialize tf example  feature spec      return servinginputreceiver feature  receiver tensors     return serve input receiver fn 
 estimator export string  def build raw serve input receiver fn feature  default batch size=none     string    def serve input receiver fn        string     receiver tensors =  placeholders from receiver tensors dict          feature  default batch size      return servinginputreceiver receiver tensors  receiver tensors     return serve input receiver fn 
 keras export string  string  def input        shape=none      batch size=none      name=none      dtype=none      sparse=false      tensor=none      ragged=false        kwargs     string   if sparse and rag      raise valueerror          string     input layer config =  string  name  string  dtype  string  sparse                          string  rag  string  tensor     batch input shape = kwargs pop string                                   kwargs pop string  none     if shape and batch input shape      raise valueerror string                      string    if batch input shape      shape = batch input shape 1       input layer config update  string  batch input shape     else      input layer config update           string  batch size  string  shape      if kwargs      raise valueerror string  kwargs key       if shape be none and tensor be none      raise valueerror string                      string                      string                      string     input layer = inputlayer   input layer config           output = input layer  inbound nod 0  output tensors   if len output  == 1      return output 0    else      return output 
class model network network     string    def   init   self   args    kwargs       super model  self    init    args    kwargs       keras api gauge get cell string  set true                self  distribution strategy = none     self  compile time distribution strategy = none     if  ops execute eagerly outside function   and         distribution strategy context have strategy           self  set strategy            distribution strategy context get strategy                        self  compile distribution = false      self  run eagerly = none     self  experimental run tf function =           ops execute eagerly outside function        trackable no automatic dependency track   def  set strategy self  strategy       self  compile time distribution strategy = strategy    def get weight self       string     strategy =  self  distribution strategy or                 self  compile time distribution strategy      if strategy        with strategy scope            return super model  self  get weight       return super model  self  get weight      def load weight self  filepath  by name=false  skip mismatch=false       string     if distribute train utils be tpu strategy self  distribution strategy         if  self  distribution strategy extend step per run > 1 and            not network  be hdf5 filepath filepath               raise valueerror string                          string      return super model  self  load weight filepath  by name  skip mismatch      trackable no automatic dependency track   def compile self                optimizer=string                loss=none                metrics=none                loss weights=none                sample weight mode=none                weight metrics=none                target tensors=none                distribute=none                  kwargs       string     self  run eagerly = kwargs pop string  none      self  experimental run tf function = kwargs pop          string  true            kwargs pop string  none        allow kwargs =  string  string  string  string      unknown kwargs = set kwargs key    - allow kwargs     if unknown kwargs        raise typeerror            string    unknown kwargs        self  function kwargs = kwargs     if self  function kwargs        self  experimental run tf function = false       if self run eagerly          raise valueerror              string             string             string    self  function kwargs         self  set optimizer optimizer      be any keras optimizer v1 = any           isinstance opt  optimizers optimizer           and not isinstance opt  optimizers tfoptimizer            for opt in nest flatten self optimizer        if be any keras optimizer v1 and ops execute eagerly outside function          raise valueerror string  optimizer  string                        string                        string                        string       if   target tensors be not none          or not ops execute eagerly outside function                  self  experimental run tf function = false      if distribute be not none        if tf2 enable   or self  experimental run tf function          raise valueerror              string             string        log warn string                       string        self  distribution strategy = distribute       self  compile distribution = true     else        if distribution strategy context have strategy                                                if distribution strategy context in cross replica context              self  distribution strategy =                 distribution strategy context get strategy         if not self  experimental run tf function        self  validate compile param for distribution strategy self run eagerly                                                               sample weight mode                                                               target tensors                                                               weight metrics                if isinstance self optimizer  trackable trackable         self  track trackable            self optimizer  name=string  overwrite=true      self loss = loss or        self loss weight = loss weight     self sample weight mode = sample weight mode     self  compile metrics = metrics or        self  compile weight metrics = weight metrics     if self run eagerly and target tensors be not none        raise valueerror            string           string                 self  train endpoints =              self  compile trainable state = self  get trainable state             self  distribute model cache =        self  distribute function cache =              self  clear losses        if  not context execute eagerly   and         self  distribution strategy be not none                       k configure and create distribute session self  distribution strategy           self  init metric attribute       if not self build or not self input or not self output                             return     self  be compile = true      keras api gauge get cell string  set true            self loss function = train utils prepare loss function          self loss  self output name       target tensors = self  process target tensor for compile target tensors       for o  n  l  t in zip self output  self output name                            self loss function  target tensors         endpoint =  trainingendpoint o  n  l        endpoint create train target t  run eagerly=self run eagerly        self  train endpoints append endpoint            train utils prepare loss weight self  train endpoints  loss weight            if self run eagerly        self  compile eagerly metrics  weight metrics  sample weight mode        return      with k get graph   as default                 self  cache output metric attribute metrics  weight metrics                self  set metric attribute                 self  handle metrics            self output            targets=self  target            skip target masks=self  prepare skip target mask              masks=self  prepare output mask                  train utils prepare sample weight modes            self  train endpoints  sample weight mode                self  compile weight loss and weight metrics                               self train function = none       self test function = none       self predict function = none               self  collect trainable weight = self trainable weight               if self  distribution strategy and not self  compile distribution          for v in self variables            strategy = self  distribution strategy           if not strategy extend variable create in scope v               raise valueerror                  string                 string                 string                 string                 string                 string                 string                 string   v  strategy       trackable no automatic dependency track   def  init distribute function cache if not compile self       if not hasattr self  string         self  distribute function cache =        property   def metrics self       string     metrics =        if self  be compile        metrics  = self  compile metric function     metrics extend self  metrics      metrics extend  get metrics from layer self  layer       return metrics     property   def metrics name self       string                metrics name =  string      if self  be compile               if len self  train endpoints  > 1          metrics name extend               e loss name               for e in self  train endpoints             if not e should skip target                        metrics name  =  m name for m in self metrics      return metrics name     property   def run eagerly self       string     if self  run eagerly be true and not context execute eagerly          raise valueerror string                        string      if not self dynamic        if self  run eagerly be none                            return def function run function eagerly       else          return self  run eagerly     else        if not context execute eagerly            raise valueerror string                          string                          string                          string                          string        if self  run eagerly be false                   raise valueerror string                          string                          string                          string        return context execute eagerly       run eagerly setter   def run eagerly self  value       self  run eagerly = value    def  select train loop self  input       string                    if isinstance input   iterator ops iterator                             iterator ops ownediterator          raise valueerror string                        string                        string                        string                        string                        string            if context execute eagerly   and self  experimental run tf function        if self  in multi worker mode            return train distribute distributionmultiworkertrainingloop              train v2 loop          else          return train v2 loop             if self  distribution strategy        if self  in multi worker mode            return train distribute distributionmultiworkertrainingloop              train distribute distributionsingleworkertrainingloop          else          return train distribute distributionsingleworkertrainingloop                  if data utils be generator or sequence input         return train generator generatororsequencetrainingloop       if train utils be eager dataset or iterator input         return train generator eagerdatasetoriteratortrainingloop                       if self run eagerly        return train generator generatorliketrainingloop       else        return train array arrayliketrainingloop      def fit self            x=none            y=none            batch size=none            epochs=1            verbose=1            callbacks=none            validation split=0             validation data=none            shuffle=true            class weight=none            sample weight=none            initial epoch=0            step per epoch=none            validation steps=none            validation freq=1            max queue size=10            workers=1            use multiprocessing=false              kwargs       string      keras api gauge get cell string  set true           if string in kwargs        log warn            string        epochs = kwargs pop string      if kwargs        raise typeerror string   str kwargs       self  assert compile be call       self  check call args string       func = self  select train loop x      return func fit          self          x=x          y=y          batch size=batch size          epochs=epochs          verbose=verbose          callbacks=callbacks          validation split=validation split          validation data=validation data          shuffle=shuffle          class weight=class weight          sample weight=sample weight          initial epoch=initial epoch          step per epoch=steps per epoch          validation steps=validation step          validation freq=validation freq          max queue size=max queue size          workers=workers          use multiprocessing=use multiprocessing     def evaluate self                 x=none                 y=none                 batch size=none                 verbose=1                 sample weight=none                 steps=none                 callbacks=none                 max queue size=10                 workers=1                 use multiprocessing=false       string      keras api gauge get cell string  set true      self  assert compile be call       self  check call args string       func = self  select train loop x      return func evaluate          self          x=x          y=y          batch size=batch size          verbose=verbose          sample weight=sample weight          steps=steps          callbacks=callbacks          max queue size=max queue size          workers=workers          use multiprocessing=use multiprocessing     def predict self                x                batch size=none                verbose=0                steps=none                callbacks=none                max queue size=10                workers=1                use multiprocessing=false       string      keras api gauge get cell string  set true      self  check call args string       func = self  select train loop x      return func predict          self          x=x          batch size=batch size          verbose=verbose          steps=steps          callbacks=callbacks          max queue size=max queue size          workers=workers          use multiprocessing=use multiprocessing     def reset metrics self       string     metrics = self  get train eval metrics       for m in metrics        m reset state             if self  distribution strategy        distribute train utils  reset metrics self       def train on batch self                       x                       y=none                       sample weight=none                       class weight=none                       reset metrics=true       string     self  assert compile be call       self  check call args string      if self  experimental run tf function        output = train v2 utils train on batch            self  x  y=y  sample weight=sample weight            class weight=class weight  reset metrics=reset metrics            standalone=true        output =  output string    output string                     output string         output =             train v2 utils  non none constant value v  for v in output          if len output  == 1          output = output 0        return output                     if  self  distribution strategy and         distribution strategy context in cross replica context           raise notimplementederror string                                 string           x  y  sample weight = self  standardize user data          x  y  sample weight=sample weight  class weight=class weight          extract tensors from dataset=true                           if self run eagerly or self  distribution strategy        output dict = train eager train on batch            self            x            y            sample weights=sample weight            output loss metrics=self  output loss metrics        output =  output dict string    output dict string                     output dict string         output =             train v2 utils  non none constant value v  for v in output        else        x = train utils modelinputs x  as list         ins = x   list y or       list sample weight or            if not isinstance k symbolic learn phase    int           ins  =  true           self  update sample weight modes sample weights=sample weight        self  make train function         output = self train function ins         if reset metrics        self reset metrics        if len output  == 1        return output 0      return output    def test on batch self  x  y=none  sample weight=none  reset metrics=true       string     self  assert compile be call       self  check call args string      if self  experimental run tf function        output = train v2 utils test on batch            self  x  y=y  sample weight=sample weight            reset metrics=reset metrics  standalone=true        output =  output string    output string                     output string         output =             train v2 utils  non none constant value v  for v in output          if len output  == 1          output = output 0        return output      if  self  distribution strategy and         distribution strategy context in cross replica context           raise notimplementederror string                                 string           x  y  sample weight = self  standardize user data          x  y  sample weight=sample weight  extract tensors from dataset=true                 if self run eagerly or self  distribution strategy        output dict = train eager test on batch            self            x            y            sample weights=sample weight            output loss metrics=self  output loss metrics        output =  output dict string    output dict string                     output dict string         output =             train v2 utils  non none constant value v  for v in output        else        x = train utils modelinputs x  as list         input = x   list y or       list sample weight or            self  update sample weight modes sample weights=sample weight        self  make test function         output = self test function input         if reset metrics        self reset metrics        if len output  == 1        return output 0      return output    def predict on batch self  x       string     self  check call args string      if self  experimental run tf function        return train v2 utils predict on batch self  x  standalone=true       if  self  distribution strategy and         distribution strategy context in cross replica context           raise notimplementederror            string           string           input       = self  standardize user data          x  extract tensors from dataset=true                if self run eagerly or self  distribution strategy        input = train utils cast if float dtype input        if isinstance input  collections abc sequence                    if len input  == 1            input = input 0         return self input         self  make predict function       output = self predict function input       if len output  == 1        return output 0      return output     deprecation deprecate        none  string    def fit generator self                      generator                      step per epoch=none                      epochs=1                      verbose=1                      callbacks=none                      validation data=none                      validation steps=none                      validation freq=1                      class weight=none                      max queue size=10                      workers=1                      use multiprocessing=false                      shuffle=true                      initial epoch=0       string      keras api gauge get cell string  set true      return self fit          generator          step per epoch=steps per epoch          epochs=epochs          verbose=verbose          callbacks=callbacks          validation data=validation data          validation steps=validation step          validation freq=validation freq          class weight=class weight          max queue size=max queue size          workers=workers          use multiprocessing=use multiprocessing          shuffle=shuffle          initial epoch=initial epoch      deprecation deprecate        none  string    def evaluate generator self                           generator                           steps=none                           callbacks=none                           max queue size=10                           workers=1                           use multiprocessing=false                           verbose=0       string      keras api gauge get cell string  set true      self  check call args string       return self evaluate          generator          steps=steps          max queue size=max queue size          workers=workers          use multiprocessing=use multiprocessing          verbose=verbose          callbacks=callbacks      deprecation deprecate        none  string    def predict generator self                          generator                          steps=none                          callbacks=none                          max queue size=10                          workers=1                          use multiprocessing=false                          verbose=0       string      keras api gauge get cell string  set true      return self predict          generator          steps=steps          max queue size=max queue size          workers=workers          use multiprocessing=use multiprocessing          verbose=verbose          callbacks=callbacks     def  check call args self  method name       string          fullargspec = self  call full argspec     if fullargspec default        positional args = fullargspec args  -len fullargspec default       else        positional args = fullargspec args     if string in positional args        positional args remove string            if len positional args  > 2        extra args = positional args 2         raise valueerror            string   method name   string           string           string   str extra args    string     def  set optimizer self  optimizer       string     if isinstance optimizer   list  tuple          self optimizer =  optimizers get opt  for opt in optimizer      else        self optimizer = optimizers get optimizer       if  self  dtype policy loss scale be not none and         not isinstance self optimizer                         loss scale optimizer lossscaleoptimizer          if isinstance self optimizer  list           raise valueerror string                          string                          string                            self  dtype policy  self optimizer        if not isinstance self optimizer  optimizer v2 optimizerv2           raise valueerror string                          string                          string                          string                             self optimizer  self  dtype policy         self optimizer = loss scale optimizer lossscaleoptimizer            self optimizer  self  dtype policy loss scale      if  isinstance self optimizer  loss scale optimizer lossscaleoptimizer  and         self  dtype policy loss scale and         self optimizer loss scale  = self  dtype policy loss scale         log warn string                       string                       string                       string                       string                          self optimizer loss scale                           self  dtype policy loss scale      def  prepare validation data self  validation data  batch size                                 validation step       string     val x  val y  val sample weight = train utils unpack validation data          validation data      return self  standardize user data          val x          val y          sample weight=val sample weight          batch size=batch size          steps=validation step          step name=string     def  validate compile param for distribution strategy        self  run eagerly  sample weight mode  target tensors  weight metrics                 if self  distribution strategy        if sample weight mode          raise notimplementederror string                                   string        if weight metrics          raise notimplementederror string                                   string        if target tensors          raise valueerror string                          string         if run eagerly          raise valueerror              string             string         if  distribute train utils be distribute by clone self  and            not self build or not self input or not self output            raise valueerror              string             string             string     def  process target tensor for compile self  target tensors       if self run eagerly                      return  none for   in self output name       if target tensors be not none and not  isinstance target tensors  list  and                                            target tensors ==              if isinstance target tensors  list           if len target tensors   = len self output             raise valueerror                string               string               string                  len self output   target tensors         elif isinstance target tensors  dict           unexpected target tensor name = set target tensors key    difference              self output name          if unexpected target tensor name            raise valueerror                string               string format                    name=unexpected target tensor name                    keys=str self output name            tmp target tensors =            for name in self output name            tmp target tensors append target tensors get name  none           target tensors = tmp target tensors       elif tensor util be tensor target tensors           target tensors =  target tensors        else          raise typeerror string                         string  target tensors      else                             target tensors =  none for   in self output name      return target tensors    def  compile eagerly self  metrics  weight metrics  sample weight mode            train utils prepare sample weight modes          self  train endpoints  sample weight mode           self  prepare sample weight            self  cache output metric attribute metrics  weight metrics      self total loss = none          self  set metric attribute        self  collect trainable weight = self trainable weight    def  update sample weight modes self  sample weights=none       string     if not self  be compile        return     if sample weight and any  s be not none for s in sample weight          for endpoint in self  train endpoints          endpoint sample weight mode =               endpoint sample weight mode or string      else        for endpoint in self  train endpoints          endpoint sample weight mode = none    def  recompile weight loss and weight metrics self       if not self  be compile        return false     recompile = any  e sample weight mismatch                        for e in self  train endpoints        if recompile        self  compile weight loss and weight metrics       return recompile     trackable no automatic dependency track   def  compile weight loss and weight metrics self  sample weights=none       string     with k get graph   as default          if sample weight be not none          self  update sample weight modes sample weight        self  prepare sample weight sample weight         mask = self  prepare output mask                 self  handle metrics            self output            targets=self  target            skip target masks=self  prepare skip target mask              sample weights=self sample weight            masks=masks            return weight metrics=true                                            self total loss = self  prepare total loss mask     def  prepare skip target mask self       string     return  l be none for l in self loss function     def  prepare output mask self       string     return  getattr x  string  none  for x in self output     def  prepare total loss self  mask       string     if self run eagerly        raise typeerror string                       string      total loss = none     with k name scope string         for endpoint  mask in zip self  train endpoints  mask           if endpoint should skip target              continue         y true = endpoint train target target         y pred = endpoint output         loss fn = endpoint loss fn         loss weight = endpoint loss weight         loss name = endpoint loss name           sample weight = endpoint sample weight          with k name scope loss name             if mask be not none              mask = math ops cast mask  y pred dtype                           if sample weight be none                sample weight = mask             else                               mask     sample weight =                     tf losses utils squeeze or expand dimension                        mask  sample weight=sample weight                 sample weight  = mask            if hasattr loss fn  string               per sample losses = loss fn call y true  y pred              weight losses = losses utils compute weight loss                  per sample losses                  sample weight=sample weight                  reduction=losses utils reductionv2 none              loss reduction = loss fn reduction                                        if loss reduction == losses utils reductionv2 auto                loss reduction = losses utils reductionv2 sum over batch size                           output loss = losses utils reduce weight loss                  weight losses  reduction=loss reduction            else                                                                               output loss = loss fn y true  y pred  sample weight=sample weight              loss reduction = losses utils reductionv2 sum over batch size          if len self output  > 1                       endpoint output loss metric output loss                             if loss reduction == losses utils reductionv2 sum over batch size            output loss = losses utils scale loss for distribution output loss           if total loss be none            total loss = loss weight   output loss         else            total loss  = loss weight   output loss       if total loss be none          if not self losses            raise valueerror string                            string          else            total loss = 0                custom losses = self get losses for none    self get losses for            self input        if custom losses          total loss  = losses utils scale loss for distribution              math ops add n custom losses       return total loss    def  get callback model self       string      if hasattr self  string  and self  replicate model                                                  return self  replicate model     if hasattr self  string  and self callback model        return self callback model     return self     trackable no automatic dependency track   def  make callback model self  group model       first replicate model = self  distribution strategy unwrap          group model  0           self  replicate model = distributedcallbackmodel first replicate model      self  replicate model set original model self     def  validate or infer batch size self  batch size  step  x       string     if  isinstance x   dataset ops datasetv1                         dataset ops datasetv2                         data utils sequence   or         tf inspect isgenerator x          if batch size be not none          raise valueerror              string             string format                  x  batch size         return                layer = trackable layer utils filter empty layer containers self  layer      first layer = next layer  none      if first layer               static batch size = train utils get static batch size first layer        if static batch size be not none                    if  self  distribution strategy and             distribute train utils global batch size support                  self  distribution strategy              num split for ds = self  distribution strategy num replicas in sync         else            num split for ds = 1                   if batch size be not none            if batch size   num split for ds  = 0              raise valueerror string                              string format                                   batch size  num split for ds             per replica batch size = batch size // num split for ds            if per replica batch size  = static batch size              raise valueerror string                              string                              string format                                   per replica batch size  static batch size                     if isinstance x   dataset ops datasetv2  iterator ops iterator                            iterator ops ownediterator              ds batch size = tensor shape as dimension                nest flatten dataset ops get legacy output shape x   0  0   value           if ds batch size be not none              if ds batch size   num split for ds  = 0                raise valueerror                    string                   string format                        ds batch size  num split for ds                ds per replica batch size = ds batch size // num split for ds             if ds per replica batch size  = static batch size                raise valueerror string                                string                                string format                                     ds per replica batch size                                     static batch size                     if step be none            batch size = static batch size   num split for ds      if batch size be none and step be none               batch size = 32     return batch size    def  prepare sample weight self  sample weights=none       string          if sample weight be not none        if len sample weight   = len self  train endpoints           raise valueerror string                          string format                               len self  train endpoints                                len sample weight        else        sample weight =  none    len self  train endpoints      for endpoint  weight in zip self  train endpoints  sample weight         endpoint populate sample weight weight  endpoint sample weight mode     def  cache output metric attribute self  metrics  weight metrics       string     output shape =        for output in self output        if output be none or output shape rank be none          output shape append none        else          output shape append output shape as list        self  per output metrics = train utils collect per output metric info          metrics  self output name  output shape  self loss function      self  per output weight metrics =           train utils collect per output metric info              weight metrics              self output name              output shape              self loss function              be weighted=true      def  add unique metric name self  metric name  output index       string     if len self output name  > 1        metric name = string    self output name output index   metric name      j = 1     base metric name = metric name     while metric name in self metrics name        metric name = string    base metric name  j        j  = 1      return metric name    def  init metric attribute self       string               self  compile metric function =       def  set per output metric attribute self  metrics dict  output index       string     update metrics dict = collections ordereddict       for metric name  metric fn in metrics dict items          metric name = self  add unique metric name metric name  output index                metric fn  name = metric name         update metrics dict metric name  = metric fn              self  compile metric function append metric fn      return update metrics dict    def  set metric attribute self       string     update per output metrics =        update per output weight metrics =        for i  endpoint in enumerate self  train endpoints         if endpoint should skip target            update per output metrics append self  per output metrics i           update per output weight metrics append              self  per output weight metrics i           continue       update per output metrics append            self  set per output metric attribute self  per output metrics i                                                    i         update per output weight metrics append            self  set per output metric attribute                self  per output weight metrics i   i                       if len self  train endpoints  > 1        for endpoint in self  train endpoints          if not endpoint should skip target              endpoint output loss metric = metrics module mean                name=endpoint loss name         self  per output metrics = update per output metrics     self  per output weight metrics = update per output weight metrics    def  handle per output metrics self                                   metrics dict                                   y true                                   y pred                                   mask                                   weights=none       string     metric result =        for metric name  metric fn in metrics dict items          with k name scope metric name           metric result = train utils call metric function              metric fn  y true  y pred  weights=weights  mask=mask          metric result append metric result      return metric result    def  handle metrics self                        output                        targets=none                        skip target masks=none                        sample weights=none                        masks=none                        return weight metrics=false                        return weight and unweighted metrics=false       string               skip target mask = skip target mask or  false    len output      metric result =        with k name scope string                for i in range len output            if skip target mask i             continue         output = output i  if output else none         target = target i  if target else none         output mask = mask i  if mask else none          if  return weight and unweighted metrics or             not return weight metrics             metric result extend                self  handle per output metrics self  per output metrics i                                                 target  output  output mask           if return weight and unweighted metrics or return weight metrics            metric result extend                self  handle per output metrics                    self  per output weight metrics i                     target                    output                    output mask                    weights=sample weight i  if sample weight else none       return metric result    def  check trainable weight consistency self       string     if not hasattr self  string         return      if len self trainable weight   = len self  collect trainable weight         log log first n            log warn  string           string           string  1     def  make train function self       have recompiled = self  recompile weight loss and weight metrics       self  check trainable weight consistency       if isinstance self optimizer  list         raise valueerror string                        string                     if getattr self  string  none  be none or have recompiled               current trainable state = self  get trainable state         self  set trainable state self  compile trainable state         input =  self  fee input                   self  fee target                   self  fee sample weight        if not isinstance k symbolic learn phase    int           input  =  k symbolic learn phase           with k get graph   as default            with k name scope string                        update = self optimizer get update                params=self  collect trainable weight  loss=self total loss                       update  = self get update for none                       update  = self get update for self input           metrics = self  get train eval metrics           metrics tensors =               m  call result for m in metrics if hasattr m  string                     with k name scope string                    fn = k function              input   self total loss    metrics tensors              updates=updates              name=string                self  function kwargs          setattr self  string  fn                self  set trainable state current trainable state     def  make test function self       have recompiled = self  recompile weight loss and weight metrics                      if getattr self  string  none  be none or have recompiled        input =  self  fee input                   self  fee target                   self  fee sample weight         with k get graph   as default            metrics = self  get train eval metrics           metrics tensors =               m  call result for m in metrics if hasattr m  string                     with k name scope string           update = self state update                           fn = k function              input   self total loss    metrics tensors              updates=updates              name=string                self  function kwargs          setattr self  string  fn     def  make predict function self       if not hasattr self  string         self predict function = none     if self predict function be none        input = self  fee input                     kwargs = getattr self  string            with k name scope modekeys predict           self predict function = k function              input              self output              updates=self state update              name=string                kwargs     def  make execution function self  mode       if mode == modekeys train        self  make train function         return self train function     if mode == modekeys test        self  make test function         return self test function     if mode == modekeys predict        self  make predict function         return self predict function    def  distribution standardize user data self                                            x                                            y=none                                            sample weight=none                                            class weight=none                                            batch size=none                                            validation split=0                                            shuffle=false                                            epochs=1                                            allow partial batch=false       string     if class weight        raise notimplementederror string                                 string       if  sample weight be not none and sample weight all   and         distribute train utils be tpu strategy              self  distribution strategy          raise notimplementederror string                                 string                                if isinstance x  dataset ops datasetv2         if shuffle          train utils verify dataset shuffle x       strategy = self  distribution strategy     with strategy scope                        if ops execute eagerly outside function            session = none       else          session = k get session          first x value = nest flatten x  0        if isinstance first x value  np ndarray           x = train utils list to tuple x          if y be not none            y = train utils list to tuple y            if sample weight be not none              sample weight = train utils list to tuple sample weight              in tuple =  x  y  sample weight            else              in tuple =  x  y          else            in tuple = x          ds = strategy extend experimental make numpy dataset in tuple                                                                 session=session          if shuffle                                                        ds = ds shuffle max 1024  batch size   8           if epochs > 1            ds = ds repeat epochs                             drop remainder =  not allow partial batch and                           strategy extend experimental require static shape                                      if distribute train utils be tpu strategy              strategy  and not drop remainder            dataset size = first x value shape 0            if dataset size   batch size == 0              drop remainder = true          x = ds batch batch size  drop remainder=drop remainder        else          assert isinstance x  dataset ops datasetv2          train utils validate dataset input x  y  sample weight                                                validation split      return x    def  standardize user data self                               x                               y=none                               sample weight=none                               class weight=none                               batch size=none                               check steps=false                               step name=string                               steps=none                               validation split=0                               shuffle=false                               extract tensors from dataset=false       string     if isinstance x   dataset ops datasetv1  dataset ops datasetv2                               train utils validate dataset input x  y  sample weight                                              validation split        if shuffle          train utils verify dataset shuffle x         be dataset = true       if extract tensors from dataset                   x  y  sample weight = train utils extract tensors from dataset x      elif isinstance x  iterator ops iterator                train utils validate dataset input x  y  sample weight                                              validation split        iterator = x       x  y  sample weight = train utils unpack iterator input iterator        be dataset = true     else        be dataset = false           if check step        train utils check step argument x  step  step name            if not self input        all input  y input  dict input = self  build model with input x  y        be build call = true     else        all input =                        dict input = isinstance self input  dict        be build call = false       y input = y                be compile call = false     if not self  be compile and self optimizer        self  compile from input all input  y input  x  y        be compile call = true                                          run eagerly = self run eagerly      if  not run eagerly and be build call and be compile call and         not be dataset  and any  be symbolic tensor v  for v in all input          return         none      return self  standardize tensors          x  y  sample weight          run eagerly=run eagerly          dict inputs=dict input          be dataset=is dataset          class weight=class weight          batch size=batch size     def  standardize tensors self  x  y  sample weight  run eagerly  dict input                             be dataset  class weight=none  batch size=none       if run eagerly                      fee input name = self input name       fee input shape = none     elif not self  be graph network               fee input name = self  fee input name       fee input shape = none     else                      fee input name = self  fee input name       fee input shape = self  fee input shape           if not isinstance x   dataset ops datasetv1  dataset ops datasetv2                 x = train utils standardize input data            x            fee input name            fee input shape            check batch axis=false              exception prefix=string                           if isinstance x  dataset ops datasetv2         x shape = dataset ops get structure x        if isinstance x shape  tuple                                      x shape = x shape 0      else        flat input = nest flatten x  expand composites=false        flat expect input = nest flatten self input  expand composites=false        convert x =          for  a  b  in zip flat input  flat expect input           convert x append  convert scipy sparse tensor a  b         x = nest pack sequence as x  convert x  expand composites=false         def  type spec from value value           string         if isinstance value  composite tensor compositetensor             return value  type spec                             if hasattr value  string  and hasattr value  string             return tensor spec tensorspec value shape  value dtype          else            return type spec type spec from value value         x shape = nest map structure  type spec from value  x       flat input = nest flatten x shape  expand composites=false      flat expect input = nest flatten self input  expand composites=false      for  a  b  in zip flat input  flat expect input         nest assert same structure a  b  expand composites=true       if y be not none                      train utils prepare sample weight modes self  train endpoints                                                   self sample weight mode        fee output name = self  fee output name       fee sample weight modes = self  sample weight modes       if not self  be graph network          fee output shape = none       else          fee output shape = self  fee output shape               y = train utils standardize input data            y            fee output name                                  shapes=none            check batch axis=false              exception prefix=string                       sample weight = train utils standardize sample weight            sample weight  fee output name        class weight = train utils standardize class weight            class weight  fee output name         sample weight =             train utils standardize weight ref  sw  cw  mode            for  ref  sw  cw  mode  in zip y  sample weight  class weight                                           fee sample weight modes                       if not self  distribution strategy          train utils check array lengths x  y  sample weight          if self  be graph network and not run eagerly                       train utils check loss and target compatibility                y  self  fee loss fns  fee output shape         sample weight       = train utils handle partial sample weight            y  sample weight  fee sample weight modes  check all flat=true      else        y =          sample weight = none      if self stateful and batch size and not be dataset                      if x 0  shape 0    batch size  = 0          raise valueerror string                          string                          string                          string                            str x 0  shape 0     string            if dict input and not isinstance x   dataset ops datasetv1                                            dataset ops datasetv2          x = dict zip fee input name  x       return x  y  sample weight    def  build model with input self  input  target       string     process input =        be dict input = false     orig input = input                         if isinstance input   dataset ops datasetv1  dataset ops datasetv2          input  target    = train utils extract tensors from dataset input                     train utils validate input type input  orig input       if isinstance input   list  tuple          process input  = list input      elif isinstance input  dict         be dict input = true       key = sort input key          process input =  input k  for k in key      else        process input append input                                    for input tensor in process input        if composite tensor utils be composite or composite value input tensor                    raise valueerror              string             string             string             string             string             string                input tensor                       if isinstance orig input   dataset ops datasetv1  dataset ops datasetv2                                  iterator ops iterator          if not self input                            input = train utils cast if float dtype input  self dtype         def create tensor spec t           return tensor spec tensorspec t shape  t dtype         cast input = nest map structure create tensor spec  input      elif train utils have tensors input         cast input = train utils cast if float dtype input      else        cast input = input     self  set input cast input      return process input  target  be dict input    def  compile from input self  all input  target  orig input  orig target       if target be not none               if train utils have tensors target           target = train utils cast if float dtype and mismatch              target  self output        train utils validate input type target  orig target                                            allow dict=false  field name=string        if isinstance target   list  tuple            all input  = list target        else          all input append target                if any tensor util be tensor v  for v in all input         if not all tensor util be tensor v  for v in all input           raise valueerror string                          string                          string   str orig input                             string   str orig target       be dataset = isinstance orig input   dataset ops datasetv1                                            dataset ops datasetv2                                            iterator ops iterator       if be dataset or context execute eagerly          target tensors = none     else               if target be not none          if not isinstance target   list  tuple              target =  target          target tensors =  v for v in target if  be symbolic tensor v         else          target tensors = none      self compile          optimizer=self optimizer          loss=self loss          metrics=self  compile metrics          weight metrics=self  compile weight metrics          loss weights=self loss weight          target tensors=target tensors          sample weight mode=self sample weight mode          run eagerly=self run eagerly          experimental run tf function=self  experimental run tf function        def  set input self  input  outputs=none  training=none       string     input = self  set input attrs input       if output be none        kwargs =          if self  expect train arg                            if train be none and not ops execute eagerly outside function              train = k learn phase           if train be not none            kwargs string  = train       try          output = self input    kwargs        except notimplementederror                            output = none      self  set output attrs output      trackable no automatic dependency track   def  set input attrs self  input       string     if self input        raise valueerror string       if self   class     name   == string and not self build        if tensor util be tensor input           input shape =  none     tuple input shape as list   1          elif isinstance input  tensor shape tensorshape           input shape =  none     tuple input as list   1          elif isinstance input  dict                    if not train utils be feature layer self layer 0              raise valueerror string                            string                            string          input shape =  none         else          input shape =  none     tuple input shape 1          self  build input shape = input shape                input = self  maybe cast input input                 model input = train utils modelinputs input      input = model input get symbolic input       self input = model input get symbolic input return single as list=true      self input name = model input get input name        self  fee input =        self  fee input name =        self  fee input shape =         for k  v in model input as dict          if k be placeholder v           self  fee input name append k          self  fee input append v          self  fee input shape append k int shape v        return input     trackable no automatic dependency track   def  set output attrs self  output       string               output = nest flatten output      self output = output     self output name = train utils generic output name output           self build = true     property   def  target self       string     return           e train target target         for e in self  train endpoints         if e have train target             property   def  fee target self       return           e train target target         for e in self  train endpoints         if e have feedable train target             property   def  fee output name self       return           e output name         for e in self  train endpoints         if e have feedable train target             property   def  fee output shape self       return           e fee output shape         for e in self  train endpoints         if e have feedable train target             property   def  fee loss fns self       return           e loss fn         for e in self  train endpoints         if e have feedable train target             property   def  loss weight list self       return  e loss weight for e in self  train endpoints      property   def  output loss metrics self       if hasattr self  string         return             e output loss metric           for e in self  train endpoints           if e output loss metric be not none             return none     property   def sample weight self       return  e sample weight for e in self  train endpoints      property   def  sample weight modes self       return  e sample weight mode for e in self  train endpoints      property   def  fee sample weight self       return  e sample weight for e in self  train endpoints             if e sample weight be not none     def  maybe load initial epoch from ckpt self  initial epoch  mode       string     if hasattr self  string         return self  train state maybe load initial epoch from ckpt            initial epoch  mode      return initial epoch    def  get train eval metrics self       string     metrics =        metrics extend getattr self  string  none  or         metrics extend getattr self  string  none  or         return metrics    def  assert compile be call self                           if not self optimizer        raise runtimeerror string                          string                          string     def  in multi worker mode self       string     strategy = self  get distribution strategy       return strategy and strategy extend  in multi worker mode        def  get distribution strategy self                      strategy = self  distribution strategy           if not strategy and distribution strategy context have strategy          strategy = distribution strategy context get strategy        return strategy     property   def  trackable save model saver self       return model serialization modelsavedmodelsaver self  
class sequential train model     string     trackable no automatic dependency track   def   init   self  layers=none  name=none       super sequential  self    init   name=name  autocast=false      self support mask = true     self  build input shape = none     self  compute output and mask jointly = true      self  layer call argspecs =              if layer        if not isinstance layer   list  tuple            layer =  layer        tf utils assert no legacy layer layer        for layer in layer          self add layer      property   def layer self                                layer = super sequential  self  layer     if layer and isinstance layer 0   input layer inputlayer         return layer 1       return layer        property    trackable layer utils cache recursive attribute string    def dynamic self       return any layer dynamic for layer in self layer      trackable no automatic dependency track   def add self  layer       string                    if hasattr layer  string         origin layer = layer  keras history 0        if isinstance origin layer  input layer inputlayer           layer = origin layer      if not isinstance layer  base layer layer         raise typeerror string                       string                       string   str layer        tf utils assert no legacy layer  layer                  layer  attribute sentinel add parent self  attribute sentinel       self build = false     set input = false     if not self  layer        if isinstance layer  input layer inputlayer                    assert len nest flatten layer  inbound nod -1  output tensors   == 1         set input = true       else          batch shape  dtype = train utils get input shape and dtype layer          if batch shape                       x = input layer input                batch shape=batch shape  dtype=dtype  name=layer name   string                                             layer x            set input = true        if set input                   if len nest flatten layer  inbound nod -1  output tensors    = 1            raise valueerror string                            string                            string                            string          self output =               nest flatten layer  inbound nod -1  output tensors  0                    self input = layer utils get source input self output 0        elif self output                      output tensor = layer self output 0         if len nest flatten output tensor    = 1          raise typeerror string                         string                         string                         string        self output =  output tensor       if self output                      self build = true      if set input or self  be graph network        self  init graph network self input  self output  name=self name      else        self  layer append layer      if self  layer        self  track layer self  layer       self  layer call argspecs layer  = tf inspect getfullargspec layer call                self  attribute sentinel invalidate all       trackable no automatic dependency track   def pop self       string     if not self layer        raise typeerror string       layer = self  layer pop       self  layer call argspecs pop layer      self  attribute sentinel invalidate all       if not self layer        self output = none       self input = none       self build = false     elif self  be graph network        self layer -1   outbound nod =          self output =  self layer -1  output        self  init graph network self input  self output  name=self name        self build = true     base layer utils default   def build self  input shape=none       if self  be graph network        self  init graph network self input  self output  name=self name      else        if input shape be none          raise valueerror string        input shape = tuple input shape        self  build input shape = input shape       super sequential  self  build input shape      self build = true    def call self  input  training=none  mask=none         if self  be graph network        if not self build          self  init graph network self input  self output  name=self name        return super sequential  self  call input  training=training  mask=mask       output = input       for layer in self layer                             kwargs =          argspec = self  layer call argspecs layer  args       if string in argspec          kwargs string  = mask       if string in argspec          kwargs string  = train        output = layer input    kwargs                input = output       mask = output  keras mask      return output    def compute output shape self  input shape       shape = input shape     for layer in self layer        shape = layer compute output shape shape      return shape    def compute mask self  input  mask                      output = self call input  mask=mask      return output  keras mask    def predict proba self  x  batch size=32  verbose=0       string     preds = self predict x  batch size  verbose      if preds min   < 0  or preds max   > 1         log warn string                       string                       string                       string      return preds    def predict class self  x  batch size=32  verbose=0       string     proba = self predict x  batch size=batch size  verbose=verbose      if proba shape -1  > 1        return proba argmax axis=-1      else        return  proba > 0 5  astype string     def get config self       layer configs =        for layer in self layer        layer configs append generic utils serialize keras object layer                 if  self  be graph network and layer configs and         string not in layer configs 0  string  and         isinstance self  layer 0   input layer inputlayer          batch input shape = self  layer 0   batch input shape       layer configs 0  string  string  = batch input shape      config =           string  self name          string  copy deepcopy layer configs            if self  build input shape        config string  = self  build input shape     return config     classmethod   def from config cls  config  custom objects=none       if string in config        name = config string        build input shape = config get string        layer configs = config string      else        name = none       build input shape = none       layer configs = config     model = cls name=name      for layer config in layer configs        layer = layer module deserialize layer config                                         custom objects=custom object        model add layer      if not model input and build input shape        model build build input shape      return model     property   def input spec self       if self layer and hasattr self layer 0   string         return self layer 0  input spec     return none     property   def  trackable save model saver self       return model serialization sequentialsavedmodelsaver self  
 keras export string  def deserialize name  custom objects=none     string   return deserialize keras object        name        module objects=globals          custom objects=custom object        printable module name=string  
 keras export string  def elu x  alpha=1 0     string   return k elu x  alpha  
 keras export string  def exponential x     string   return math ops exp x  
 keras export string  def get identifier     string   if identifier be none      return linear   if isinstance identifier  six string type       identifier = str identifier      return deserialize identifier    elif callable identifier       return identifier   elif isinstance identifier  dict       return deserialize keras object          identifier  printable module name=string    else      raise typeerror          string format              repr identifier    
 keras export string  def hard sigmoid x     string   return k hard sigmoid x  
 keras export string  def linear x     string   return x 
 keras export string  def relu x  alpha=0   max value=none  threshold=0     string   return k relu x  alpha=alpha  max value=max value  threshold=threshold  
 keras export string  def selu x     string   return nn selu x  
 keras export string  def serialize activation     string   if  hasattr activation  string  and       activation   name   in  tf activations v2       return  tf activations v2 activation   name      return serialize keras object activation  
 keras export string  def sigmoid x     string   return nn sigmoid x  
 keras export string  def softmax x  axis=-1     string   ndim = k ndim x    if ndim == 2      return nn softmax x    elif ndim > 2      e = math ops exp x - math ops reduce max x  axis=axis  keepdims=true       s = math ops reduce sum e  axis=axis  keepdims=true      return e / s   else      raise valueerror string                      string    x    
 keras export string  def softplus x     string   return nn softplus x  
 keras export string  def softsign x     string   return nn softsign x  
 keras export string  def tanh x     string   return nn tanh x  
  def wrapper  args    kwargs       kwargs  backend   = backend     if  layer  not in kwargs        kwargs  layer   = layer     kwargs  model   = model     kwargs  utils   = all utils     return base fun  args    kwargs  
  def wrapper  args    kwargs       kwargs  backend   = backend     if  layer  not in kwargs        kwargs  layer   = layer     kwargs  model   = model     kwargs  utils   = all utils     return base fun  args    kwargs  
  def wrapper  args    kwargs       kwargs  backend   = backend     if  layer  not in kwargs        kwargs  layer   = layer     kwargs  model   = model     kwargs  utils   = all utils     return base fun  args    kwargs  
  def wrapper  args    kwargs       kwargs  backend   = backend     if  layer  not in kwargs        kwargs  layer   = layer     kwargs  model   = model     kwargs  utils   = all utils     return base fun  args    kwargs  
  def wrapper  args    kwargs       kwargs  backend   = backend     if  layer  not in kwargs        kwargs  layer   = layer     kwargs  model   = model     kwargs  utils   = all utils     return base fun  args    kwargs  
  def wrapper  args    kwargs       kwargs  backend   = backend     if  layer  not in kwargs        kwargs  layer   = layer     kwargs  model   = model     kwargs  utils   = all utils     return base fun  args    kwargs  
  def wrapper  args    kwargs       kwargs  backend   = backend     if  layer  not in kwargs        kwargs  layer   = layer     kwargs  model   = model     kwargs  utils   = all utils     return base fun  args    kwargs  
  def wrapper  args    kwargs       kwargs  backend   = backend     if  layer  not in kwargs        kwargs  layer   = layer     kwargs  model   = model     kwargs  utils   = all utils     return base fun  args    kwargs  
  def wrapper  args    kwargs       kwargs  backend   = backend     if  layer  not in kwargs        kwargs  layer   = layer     kwargs  model   = model     kwargs  utils   = all utils     return base fun  args    kwargs  
  def wrapper  args    kwargs       kwargs  backend   = backend     if  layer  not in kwargs        kwargs  layer   = layer     kwargs  model   = model     kwargs  utils   = all utils     return base fun  args    kwargs  
  def wrapper  args    kwargs       kwargs  backend   = backend     if  layer  not in kwargs        kwargs  layer   = layer     kwargs  model   = model     kwargs  utils   = all utils     return base fun  args    kwargs  
  def wrapper  args    kwargs       kwargs  backend   = backend     if  layer  not in kwargs        kwargs  layer   = layer     kwargs  model   = model     kwargs  utils   = all utils     return base fun  args    kwargs  
  def wrapper  args    kwargs       kwargs  backend   = backend     if  layer  not in kwargs        kwargs  layer   = layer     kwargs  model   = model     kwargs  utils   = all utils     return base fun  args    kwargs  
  def wrapper  args    kwargs       kwargs  backend   = backend     if  layer  not in kwargs        kwargs  layer   = layer     kwargs  model   = model     kwargs  utils   = all utils     return base fun  args    kwargs  
  def wrapper  args    kwargs       kwargs  backend   = backend     if  layer  not in kwargs        kwargs  layer   = layer     kwargs  model   = model     kwargs  utils   = all utils     return base fun  args    kwargs  
  def wrapper  args    kwargs       kwargs  backend   = backend     if  layer  not in kwargs        kwargs  layer   = layer     kwargs  model   = model     kwargs  utils   = all utils     return base fun  args    kwargs  
  def wrapper  args    kwargs       kwargs  backend   = backend     if  layer  not in kwargs        kwargs  layer   = layer     kwargs  model   = model     kwargs  utils   = all utils     return base fun  args    kwargs  
  def wrapper  args    kwargs       kwargs  backend   = backend     if  layer  not in kwargs        kwargs  layer   = layer     kwargs  model   = model     kwargs  utils   = all utils     return base fun  args    kwargs  
  def wrapper  args    kwargs       kwargs  backend   = backend     if  layer  not in kwargs        kwargs  layer   = layer     kwargs  model   = model     kwargs  utils   = all utils     return base fun  args    kwargs  
  def wrapper  args    kwargs       kwargs  backend   = backend     if  layer  not in kwargs        kwargs  layer   = layer     kwargs  model   = model     kwargs  utils   = all utils     return base fun  args    kwargs  
  def wrapper  args    kwargs       kwargs  backend   = backend     if  layer  not in kwargs        kwargs  layer   = layer     kwargs  model   = model     kwargs  utils   = all utils     return base fun  args    kwargs  
  def wrapper  args    kwargs       kwargs  backend   = backend     if  layer  not in kwargs        kwargs  layer   = layer     kwargs  model   = model     kwargs  utils   = all utils     return base fun  args    kwargs  
  def wrapper  args    kwargs       kwargs  backend   = backend     if  layer  not in kwargs        kwargs  layer   = layer     kwargs  model   = model     kwargs  utils   = all utils     return base fun  args    kwargs  
  def wrapper  args    kwargs       kwargs  backend   = backend     if  layer  not in kwargs        kwargs  layer   = layer     kwargs  model   = model     kwargs  utils   = all utils     return base fun  args    kwargs  
  def wrapper  args    kwargs       kwargs  backend   = backend     if  layer  not in kwargs        kwargs  layer   = layer     kwargs  model   = model     kwargs  utils   = all utils     return base fun  args    kwargs  
  def wrapper  args    kwargs       kwargs  backend   = backend     if  layer  not in kwargs        kwargs  layer   = layer     kwargs  model   = model     kwargs  utils   = all utils     return base fun  args    kwargs  
  def wrapper  args    kwargs       kwargs  backend   = backend     if  layer  not in kwargs        kwargs  layer   = layer     kwargs  model   = model     kwargs  utils   = all utils     return base fun  args    kwargs  
  def wrapper  args    kwargs       kwargs  backend   = backend     if  layer  not in kwargs        kwargs  layer   = layer     kwargs  model   = model     kwargs  utils   = all utils     return base fun  args    kwargs  
  def wrapper  args    kwargs       kwargs  backend   = backend     if  layer  not in kwargs        kwargs  layer   = layer     kwargs  model   = model     kwargs  utils   = all utils     return base fun  args    kwargs  
  def wrapper  args    kwargs       kwargs  backend   = backend     if  layer  not in kwargs        kwargs  layer   = layer     kwargs  model   = model     kwargs  utils   = all utils     return base fun  args    kwargs  
  def wrapper  args    kwargs       kwargs  backend   = backend     if  layer  not in kwargs        kwargs  layer   = layer     kwargs  model   = model     kwargs  utils   = all utils     return base fun  args    kwargs  
  def wrapper  args    kwargs       kwargs  backend   = backend     if  layer  not in kwargs        kwargs  layer   = layer     kwargs  model   = model     kwargs  utils   = all utils     return base fun  args    kwargs  
  def wrapper  args    kwargs       kwargs  backend   = backend     if  layer  not in kwargs        kwargs  layer   = layer     kwargs  model   = model     kwargs  utils   = all utils     return base fun  args    kwargs  
  def wrapper  args    kwargs       kwargs  backend   = backend     if  layer  not in kwargs        kwargs  layer   = layer     kwargs  model   = model     kwargs  utils   = all utils     return base fun  args    kwargs  
  def wrapper  args    kwargs       kwargs  backend   = backend     if  layer  not in kwargs        kwargs  layer   = layer     kwargs  model   = model     kwargs  utils   = all utils     return base fun  args    kwargs  
  def wrapper  args    kwargs       kwargs  backend   = backend     if  layer  not in kwargs        kwargs  layer   = layer     kwargs  model   = model     kwargs  utils   = all utils     return base fun  args    kwargs  
  def wrapper  args    kwargs       kwargs  backend   = backend     if  layer  not in kwargs        kwargs  layer   = layer     kwargs  model   = model     kwargs  utils   = all utils     return base fun  args    kwargs  
  def wrapper  args    kwargs       kwargs  backend   = backend     if  layer  not in kwargs        kwargs  layer   = layer     kwargs  model   = model     kwargs  utils   = all utils     return base fun  args    kwargs  
  def wrapper  args    kwargs       kwargs  backend   = backend     if  layer  not in kwargs        kwargs  layer   = layer     kwargs  model   = model     kwargs  utils   = all utils     return base fun  args    kwargs  
  def wrapper  args    kwargs       kwargs  backend   = backend     if  layer  not in kwargs        kwargs  layer   = layer     kwargs  model   = model     kwargs  utils   = all utils     return base fun  args    kwargs  
  def wrapper  args    kwargs       kwargs  backend   = backend     if  layer  not in kwargs        kwargs  layer   = layer     kwargs  model   = model     kwargs  utils   = all utils     return base fun  args    kwargs  
  def wrapper  args    kwargs       kwargs  backend   = backend     if  layer  not in kwargs        kwargs  layer   = layer     kwargs  model   = model     kwargs  utils   = all utils     return base fun  args    kwargs  
 keras export string  def clear session      string   global  session   global  graph learn phase     global  graph variables     global  graph tf optimizers     global  graph   global  freezable vars    graph = none   ops reset default graph     reset uids      session session = none   graph = get graph     with graph as default        with name scope string         phase = array ops placeholder with default            false  shape=    name=string       graph learn phase =         graph learn phase graph  = phase      graph variables pop graph  none       graph tf optimizers pop graph  none       freezable vars pop graph  none  
 keras export string  def epsilon      string   return  epsilon 
 keras export string  def floatx      string   return  floatx 
 keras export string  def get uid prefix=string     string   graph = get graph     if graph not in per graph object name uids      per graph object name uids graph  = collections defaultdict int    layer name uids = per graph object name uids graph    layer name uids prefix   = 1   return layer name uids prefix  
 keras export string  def image data format      string   return  image data format 
 keras export string  def be keras tensor x     string   if not isinstance x   ops tensor                          variables module variable                          sparse tensor sparsetensor        raise valueerror string   str type x                          string    return hasattr x  string  
 keras export string  def reset uids      string    per graph object name uids clear   
 keras export string  def rnn step function          input          initial state          go backwards=false          mask=none          constants=none          unroll=false          input length=none          time major=false          zero output for mask=false     string    def swap batch timestep input t            ax = list range len input t shape        ax 0   ax 1  = 1  0     return array ops transpose input t  ax     if not time major      input = nest map structure swap batch timestep  input     flatted input = nest flatten input    time step = flatted input 0  shape 0    batch = flatted input 0  shape 1    time step t = array ops shape flatted input 0   0     for input  in flatted input      input  shape with rank at least 3     if mask be not none      if mask dtype  = dtypes module bool        mask = math ops cast mask  dtypes module bool      if len mask shape  == 2        mask = expand dim mask      if not time major        mask = swap batch timestep mask     if constants be none      constants =                         def  expand mask mask t  input t  fix dim=1       if nest be sequence mask t         raise valueerror string   mask t      if nest be sequence input t         raise valueerror string   input t      rank diff = len input t shape  - len mask t shape      for   in range rank diff         mask t = array ops expand dim mask t  -1      multiples =  1    fix dim   input t shape as list   fix dim       return array ops tile mask t  multiples     if unroll      if not time step        raise valueerror string      state = tuple initial state      successive state =        successive output =                                  def  process single input t input t         input t = array ops unstack input t          if go backwards          input t reverse         return input t      if nest be sequence input         process input = nest map structure  process single input t  input      else        process input =   process single input t input         def  get input tensor time         inp =  t  time  for t  in process input        return nest pack sequence as input  inp       if mask be not none        mask list = array ops unstack mask        if go backwards          mask list reverse          for i in range time step           inp =  get input tensor i          mask t = mask list i          output  new state = step function inp                                             tuple state    tuple constants           tile mask t =  expand mask mask t  output           if not successive output            prev output = zero like output          else            prev output = successive output -1           output = array ops where v2 tile mask t  output  prev output           flat state = nest flatten state          flat new state = nest flatten new state          tile mask t = tuple  expand mask mask t  s  for s in flat state          flat final state = tuple              array ops where v2 m  s  ps              for m  s  ps in zip tile mask t  flat new state  flat state           state = nest pack sequence as state  flat final state           successive output append output          successive state append state        last output = successive output -1        new state = successive state -1        output = array ops stack successive output         if zero output for mask          last output = array ops where v2               expand mask mask list -1   last output   last output              zero like last output           output = array ops where v2               expand mask mask  output  fix dim=2   output              zero like output        else          for i in range time step           inp =  get input tensor i          output  state = step function inp  tuple state    tuple constants           successive output append output          successive state append state        last output = successive output -1        new state = successive state -1        output = array ops stack successive output     else        state = tuple initial state                      input ta = tuple          tensor array ops tensorarray              dtype=inp dtype              size=time step t              tensor array name=string   i          for i  inp in enumerate flatted input       input ta = tuple          ta unstack input   if not go backwards else ta          unstack reverse input   0           for ta  input  in zip input ta  flatted input                       input time zero = nest pack sequence as input                                               inp 0  for inp in flatted input                 output time zero    = step function          input time zero  tuple initial state    tuple constants       output ta = tuple          tensor array ops tensorarray              dtype=out dtype              size=time step t              element shape=out shape              tensor array name=string   i          for i  out in enumerate nest flatten output time zero         time = constant op constant 0  dtype=string  name=string                 if  not context execute eagerly   and         control flow util graphorparentsinxlacontext ops get default graph            max iterations = math ops reduce max input length      else        max iterations = none      while loop kwargs =           string  lambda time      time < time step t          string  max iterations          string  32          string  true            if mask be not none        if go backwards          mask = reverse mask  0         mask ta = tensor array ops tensorarray            dtype=dtypes module bool            size=time step t            tensor array name=string        mask ta = mask ta unstack mask         def mask fn time           return mask ta read time         def compute mask output mask t  flat out  flat mask           tile mask t = tuple               expand mask mask t  o  fix dim=len mask t shape               for o in flat out          return tuple              array ops where v2 m  o  fm              for m  o  fm in zip tile mask t  flat out  flat mask       elif isinstance input length  ops tensor         if go backwards          max len = math ops reduce max input length  axis=0          rev input length = math ops subtract max len - 1  input length           def mask fn time             return math ops less rev input length  time        else           def mask fn time             return math ops greater input length  time         def compute mask output mask t  flat out  flat mask           return tuple              array ops where mask t  o  zo              for  o  zo  in zip flat out  flat mask       else        mask fn = none      if mask fn be not none                      flat zero output = tuple array ops zero like o                                 for o in nest flatten output time zero         def  step time  output ta t  prev output   state           string         current input = tuple ta read time  for ta in input ta                   current input = nest pack sequence as input  current input          mask t = mask fn time          output  new state = step function current input                                             tuple state    tuple constants                    flat output = nest flatten output          flat mask output =  flat zero output if zero output for mask                             else nest flatten prev output           flat new output = compute mask output mask t  flat output                                                  flat mask output                    flat state = nest flatten state          flat new state = nest flatten new state          for state  new state in zip flat state  flat new state             if isinstance new state  ops tensor               new state set shape state shape          flat final state = compute mask output mask t  flat new state                                                   flat state          new state = nest pack sequence as new state  flat final state           output ta t = tuple              ta write time  out              for ta  out in zip output ta t  flat new output           return  time   1  output ta t                  tuple flat new output     tuple new state         final output = control flow ops while loop            body= step            loop vars= time  output ta  flat zero output    state              while loop kwargs               new state = final output 3       else        def  step time  output ta t   state           string         current input = tuple ta read time  for ta in input ta          current input = nest pack sequence as input  current input          output  new state = step function current input                                             tuple state    tuple constants           flat state = nest flatten state          flat new state = nest flatten new state          for state  new state in zip flat state  flat new state             if isinstance new state  ops tensor               new state set shape state shape           flat output = nest flatten output          output ta t = tuple              ta write time  out  for ta  out in zip output ta t  flat output           new state = nest pack sequence as initial state  flat new state          return  time   1  output ta t    tuple new state         final output = control flow ops while loop            body= step            loop vars= time  output ta    state              while loop kwargs        new state = final output 2        output ta = final output 1       output = tuple o stack   for o in output ta      last output = tuple o -1  for o in output       output = nest pack sequence as output time zero  output      last output = nest pack sequence as output time zero  last output        def set shape output        if isinstance output   ops tensor         shape = output  shape as list         shape 0  = time step       shape 1  = batch       output  set shape shape      return output     output = nest map structure set shape  output     if not time major      output = nest map structure swap batch timestep  output     return last output  output  new state 
 keras export string  def set epsilon value     string   global  epsilon    epsilon = value 
 keras export string  def set floatx value     string   global  floatx   if value not in  string  string  string       raise valueerror string   str value      floatx = str value  
 keras export string  def set image data format data format     string   global  image data format   if data format not in  string  string       raise valueerror string   str data format      image data format = str data format  
class baselogger callback     string    def   init   self  stateful metrics=none       super baselogger  self    init         self stateful metrics = set stateful metrics or        def on epoch begin self  epoch  logs=none       self see = 0     self total =       def on batch end self  batch  logs=none       log = log or        batch size = log get string  0                num step = log get string  1      self see  = batch size   num step      for k  v in log items          if k in self stateful metrics          self total k  = v       else          if k in self total            self total k   = v   batch size         else            self total k  = v   batch size    def on epoch end self  epoch  logs=none       if log be not none        for k in self params string           if k in self total                       if k in self stateful metrics              log k  = self total k            else              log k  = self total k  / self see 
class csvlogger callback     string    def   init   self  filename  separator=string  append=false       self sep = separator     self filename = filename     self append = append     self writer = none     self key = none     self append header = true     if six py2        self file flag = string       self  open args =        else        self file flag = string       self  open args =  string  string      super csvlogger  self    init        def on train begin self  logs=none       if self append        if file io file exist self filename           with open self filename  string   self file flag  as f            self append header = not bool len f readline           mode = string     else        mode = string     self csv file = io open self filename                              mode   self file flag                                self  open args     def on epoch end self  epoch  logs=none       log = log or         def handle value k         be zero dim ndarray = isinstance k  np ndarray  and k ndim == 0       if isinstance k  six string type           return k       elif isinstance k  collections abc iterable  and not be zero dim ndarray          return string    string join map str  k          else          return k      if self key be none        self key = sort log key         if self model stop train               log = dict   k  log k   if k in log else  k  string  for k in self key        if not self writer         class customdialect csv excel           delimiter = self sep        fieldnames =  string    self key       if six py2          fieldnames =  unicode x  for x in fieldnames         self writer = csv dictwriter            self csv file            fieldnames=fieldnames            dialect=customdialect        if self append header          self writer writeheader        row dict = collections ordereddict  string  epoch       row dict update  key  handle value log key    for key in self key      self writer writerow row dict      self csv file flush      def on train end self  logs=none       self csv file close       self writer = none 
class callback object     string    def   init   self       self validation data = none     self model = none                    self  chief worker only = none    def set params self  params       self params = params    def set model self  model       self model = model     doc control for subclass implementers   def on batch begin self  batch  logs=none       string     doc control for subclass implementers   def on batch end self  batch  logs=none       string     doc control for subclass implementers   def on epoch begin self  epoch  logs=none       string     doc control for subclass implementers   def on epoch end self  epoch  logs=none       string     doc control for subclass implementers   def on train batch begin self  batch  logs=none       string          self on batch begin batch  logs=logs      doc control for subclass implementers   def on train batch end self  batch  logs=none       string          self on batch end batch  logs=logs      doc control for subclass implementers   def on test batch begin self  batch  logs=none       string     doc control for subclass implementers   def on test batch end self  batch  logs=none       string     doc control for subclass implementers   def on predict batch begin self  batch  logs=none       string     doc control for subclass implementers   def on predict batch end self  batch  logs=none       string     doc control for subclass implementers   def on train begin self  logs=none       string     doc control for subclass implementers   def on train end self  logs=none       string     doc control for subclass implementers   def on test begin self  logs=none       string     doc control for subclass implementers   def on test end self  logs=none       string     doc control for subclass implementers   def on predict begin self  logs=none       string     doc control for subclass implementers   def on predict end self  logs=none       string 
class earlystopping callback     string    def   init   self                 monitor=string                 min delta=0                 patience=0                 verbose=0                 mode=string                 baseline=none                 restore best weights=false       super earlystopping  self    init          self monitor = monitor     self patience = patience     self verbose = verbose     self baseline = baseline     self min delta = abs min delta      self wait = 0     self stop epoch = 0     self restore best weight = restore best weight     self best weight = none      if mode not in  string  string  string         log warn string                       string  mode        mode = string      if mode == string        self monitor op = np less     elif mode == string        self monitor op = np greater     else        if string in self monitor          self monitor op = np greater       else          self monitor op = np less      if self monitor op == np greater        self min delta  = 1     else        self min delta  = -1    def on train begin self  logs=none            self wait = 0     self stop epoch = 0     if self baseline be not none        self best = self baseline     else        self best = np inf if self monitor op == np less else -np inf    def on epoch end self  epoch  logs=none       current = self get monitor value log      if current be none        return     if self monitor op current - self min delta  self best         self best = current       self wait = 0       if self restore best weight          self best weight = self model get weight       else        self wait  = 1       if self wait >= self patience          self stop epoch = epoch         self model stop train = true         if self restore best weight            if self verbose > 0              print string            self model set weight self best weight     def on train end self  logs=none       if self stop epoch > 0 and self verbose > 0        print string    self stop epoch   1      def get monitor value self  log       log = log or        monitor value = log get self monitor      if monitor value be none        log warn string                       string                        self monitor  string join list log key          return monitor value 
class history callback     string    def on train begin self  logs=none       self epoch =        self history =       def on epoch end self  epoch  logs=none       log = log or        self epoch append epoch      for k  v in log items          self history setdefault k      append v  
class lambdacallback callback     r   callback for create simple  custom callbacks on-the-fly     this callback be construct with anonymous function that will be call   at the appropriate time  note that the callbacks expect positional   arguments  as      - `on epoch begin` and `on epoch end` expect two positional arguments        `epoch`  `logs`    - `on batch begin` and `on batch end` expect two positional arguments        `batch`  `logs`    - `on train begin` and `on train end` expect one positional argument        `logs`    arguments        on epoch begin  call at the begin of every epoch        on epoch end  call at the end of every epoch        on batch begin  call at the begin of every batch        on batch end  call at the end of every batch        on train begin  call at the begin of model train        on train end  call at the end of model train     example     ```python     print the batch number at the begin of every batch    batch print callback = lambdacallback        on batch begin=lambda batch log  print batch        stream the epoch loss to a file in json format  the file content     be not well-formed json but rather have a json object per line    import json   json log = open  loss log json   mode= wt   buffering=1    json log callback = lambdacallback        on epoch end=lambda epoch  log  json log write            json dump   epoch   epoch   loss   log  loss        \n          on train end=lambda log  json log close            terminate some process after have finish model train    process =       cleanup callback = lambdacallback        on train end=lambda log              p terminate   for p in process if p be alive        model fit                  callbacks= batch print callback                         json log callback                         cleanup callback     ```          def   init   self                 on epoch begin=none                 on epoch end=none                 on batch begin=none                 on batch end=none                 on train begin=none                 on train end=none                   kwargs       super lambdacallback  self    init         self   dict   update kwargs      if on epoch begin be not none        self on epoch begin = on epoch begin     else        self on epoch begin = lambda epoch  log  none     if on epoch end be not none        self on epoch end = on epoch end     else        self on epoch end = lambda epoch  log  none     if on batch begin be not none        self on batch begin = on batch begin     else        self on batch begin = lambda batch  log  none     if on batch end be not none        self on batch end = on batch end     else        self on batch end = lambda batch  log  none     if on train begin be not none        self on train begin = on train begin     else        self on train begin = lambda log  none     if on train end be not none        self on train end = on train end     else        self on train end = lambda log  none 
class learningratescheduler callback     string    def   init   self  schedule  verbose=0       super learningratescheduler  self    init         self schedule = schedule     self verbose = verbose    def on epoch begin self  epoch  logs=none       if not hasattr self model optimizer  string         raise valueerror string      try          lr = float k get value self model optimizer lr         lr = self schedule epoch  lr      except typeerror          lr = self schedule epoch      if not isinstance lr   ops tensor  float  np float32  np float64          raise valueerror string                        string      if isinstance lr  ops tensor  and not lr dtype be float        raise valueerror string      k set value self model optimizer lr  k get value lr       if self verbose > 0        print string             string    epoch   1  lr      def on epoch end self  epoch  logs=none       log = log or        log string  = k get value self model optimizer lr  
class modelcheckpoint callback     string    def   init   self                 filepath                 monitor=string                 verbose=0                 save best only=false                 save weight only=false                 mode=string                 save freq=string                   kwargs       super modelcheckpoint  self    init         self monitor = monitor     self verbose = verbose     self filepath = filepath     self save best only = save best only     self save weight only = save weight only     self save freq = save freq     self epochs since last save = 0     self  sample see since last save = 0                     if string in kwargs        self load weight on restart = kwargs string        log warn string                       string                       string      else        self load weight on restart = false                if string in kwargs        self period = kwargs string        log warn string                       string      else        self period = 1      if mode not in  string  string  string         log warn string                       string  mode        mode = string      if mode == string        self monitor op = np less       self best = np inf     elif mode == string        self monitor op = np greater       self best = -np inf     else        if string in self monitor or self monitor startswith string           self monitor op = np greater         self best = -np inf       else          self monitor op = np less         self best = np inf      if self save freq  = string and not isinstance self save freq  int         raise valueerror string format self save freq                  self  chief worker only = false    def set model self  model       self model = model          if  not self save weight only and         not model  be graph network and           model   class     name    = string         self save weight only = true    def on train begin self  logs=none            if self model  in multi worker mode                        self model  train state =             train state multiworkertrainingstate self model  self filepath         self  train state = self model  train state       if self  train state restore                                       return                if self load weight on restart        if  not self model  in multi worker mode   or           multi worker util should load checkpoint             filepath to load =               self  get most recently modify file match pattern                  self filepath           if  filepath to load be not none and             train state checkpoint exist filepath to load              try                                                     self model load weight filepath to load            except  ioerror  valueerror  as e              raise valueerror string format                  filepath to load  e      def on train end self  logs=none            if self model  in multi worker mode          if self model stop train or getattr            self model  string  false                                      self  train state delete backup                             del self  train state         del self model  train state    def on batch end self  batch  logs=none       log = log or        if isinstance self save freq  int         self  sample see since last save  = log get string  1        if self  sample see since last save >= self save freq          self  save model epoch=self  current epoch  logs=logs          self  sample see since last save = 0    def on epoch begin self  epoch  logs=none       self  current epoch = epoch    def on epoch end self  epoch  logs=none       self epochs since last save  = 1          if self save freq == string        if self model  in multi worker mode                     with self  train state untrack vars              self  save model epoch=epoch  logs=logs        else          self  save model epoch=epoch  logs=logs      if self model  in multi worker mode                               self  train state back up epoch     def  save model self  epoch  log       string     log = log or         if isinstance self save freq                    int  or self epochs since last save >= self period        self epochs since last save = 0       filepath = self  get file path epoch  log         try          if self save best only            current = log get self monitor            if current be none              log warn string                             string  self monitor            else              if self monitor op current  self best                 if self verbose > 0                  print string                       string    epoch   1  self monitor                                                 self best  current  filepath                 self best = current               if self save weight only                  self model save weight filepath  overwrite=true                else                  self model save filepath  overwrite=true              else                if self verbose > 0                  print string                          epoch   1  self monitor  self best           else            if self verbose > 0              print string    epoch   1  filepath             if self save weight only              self model save weight filepath  overwrite=true            else              self model save filepath  overwrite=true           self  maybe remove file         except ioerror as e                   if string in e message            raise ioerror string                         string                         string format filepath      def  get file path self  epoch  log       string          if not self model  in multi worker mode        or multi worker util should save checkpoint          return self filepath format epoch=epoch   1    log      else                                                  self  temp file dir = tempfile mkdtemp         extension = os path splitext self filepath  1        return os path join self  temp file dir  string   extension     def  maybe remove file self                       if  self model  in multi worker mode   and           not multi worker util should save checkpoint           file io delete recursively self  temp file dir        del self  temp file dir    def  get most recently modify file match pattern self  pattern       string     dir name = os path dirname pattern      base name = os path basename pattern      base name regex = string   re sub r        r      base name    string                latest tf checkpoint = checkpoint management latest checkpoint dir name      if latest tf checkpoint be not none and re match          base name regex  os path basename latest tf checkpoint          return latest tf checkpoint      latest mod time = 0     file path with latest mod time = none     n file with latest mod time = 0     file path with largest file name = none      if file io file exist dir name         for file name in os listdir dir name                    if re match base name regex  file name             file path = os path join dir name  file name            mod time = os path getmtime file path            if  file path with largest file name be none or               file path > file path with largest file name               file path with largest file name = file path           if mod time > latest mod time              latest mod time = mod time             file path with latest mod time = file path                                       n file with latest mod time = 1           elif mod time == latest mod time                                                     n file with latest mod time  = 1      if n file with latest mod time == 1               return file path with latest mod time     else                      return file path with largest file name 
class progbarlogger callback     string    def   init   self  count mode=string  stateful metrics=none       super progbarlogger  self    init         if count mode == string        self use step = false     elif count mode == string        self use step = true     else        raise valueerror string   str count mode       self stateful metrics = set stateful metrics or         self log value = none    def on train begin self  logs=none       self verbose = self params string      self epochs = self params string     def on epoch begin self  epoch  logs=none       self see = 0     if self use step        self target = self params string      else        self target = self params string       if self verbose        if self epochs > 1          print string    epoch   1  self epochs       self progbar = progbar          target=self target          verbose=self verbose          stateful metrics=self stateful metrics          unit name=string if self use step else string     def on batch begin self  batch  logs=none       self log value =       def on batch end self  batch  logs=none       log = log or        batch size = log get string  0                num step = log get string  1      if self use step        self see  = num step     else        self see  = batch size   num step      for k in self params string         if k in log          self log value append  k  log k                   if self verbose and  self target be none or self see < self target         self progbar update self see  self log value     def on epoch end self  epoch  logs=none       log = log or        for k in self params string         if k in log          self log value append  k  log k        if self verbose        self progbar update self see  self log value  
class reducelronplateau callback     string    def   init   self                 monitor=string                 factor=0 1                 patience=10                 verbose=0                 mode=string                 min delta=1e-4                 cooldown=0                 min lr=0                   kwargs       super reducelronplateau  self    init          self monitor = monitor     if factor >= 1 0        raise valueerror string string      if string in kwargs        min delta = kwargs pop string        log warn string                       string      self factor = factor     self min lr = min lr     self min delta = min delta     self patience = patience     self verbose = verbose     self cooldown = cooldown     self cooldown counter = 0       self wait = 0     self best = 0     self mode = mode     self monitor op = none     self  reset      def  reset self       string     if self mode not in  string  string  string         log warn string                       string  self mode        self mode = string     if  self mode == string or          self mode == string and string not in self monitor          self monitor op = lambda a  b  np less a  b - self min delta        self best = np inf     else        self monitor op = lambda a  b  np greater a  b   self min delta        self best = -np inf     self cooldown counter = 0     self wait = 0    def on train begin self  logs=none       self  reset      def on epoch end self  epoch  logs=none       log = log or        log string  = k get value self model optimizer lr      current = log get self monitor      if current be none        log warn string                       string                        self monitor  string join list log key           else        if self in cooldown            self cooldown counter -= 1         self wait = 0        if self monitor op current  self best           self best = current         self wait = 0       elif not self in cooldown            self wait  = 1         if self wait >= self patience            old lr = float k get value self model optimizer lr             if old lr > self min lr              new lr = old lr   self factor             new lr = max new lr  self min lr              k set value self model optimizer lr  new lr              if self verbose > 0                print string                     string    epoch   1  new lr               self cooldown counter = self cooldown             self wait = 0    def in cooldown self       return self cooldown counter > 0 
class remotemonitor callback     string    def   init   self                 root=string                 path=string                 field=string                 headers=none                 send as json=false       super remotemonitor  self    init          self root = root     self path = path     self field = field     self headers = headers     self send as json = send as json    def on epoch end self  epoch  logs=none       if request be none        raise importerror string      log = log or        send =        send string  = epoch     for k  v in log items          send k  = v     try        if self send as json          request post self root   self path  json=send  headers=self headers        else          request post              self root   self path   self field  json dump send                headers=self headers      except request exceptions requestexception        log warn string                       string   str self root   
class tensorboard callback        string        def   init   self                 log dir=string                 histogram freq=0                 write graph=true                 write images=false                 update freq=string                 profile batch=2                 embeddings freq=0                 embeddings metadata=none                   kwargs       super tensorboard  self    init         self  validate kwargs kwargs       self log dir = log dir     self histogram freq = histogram freq     self write graph = write graph     self write image = write image     if update freq == string        self update freq = 1     else        self update freq = update freq     self embeddings freq = embeddings freq     self embeddings metadata = embeddings metadata      self  sample see = 0     self  sample see at last write = 0     self  current batch = 0                          self  train run name = string     self  validation run name = string     self  writers =         self  profile batch = profile batch          self  be trace = false    def  validate kwargs self  kwargs       string     if kwargs get string  false         log warn string                       string      if kwargs get string  false         log warn string                       string                       string      if kwargs get string  false         log warn string                       string                       string      if kwargs get string  false         log warn string                       string                       string       unrecognized kwargs = set kwargs key    -           string  string  string  string                 if unrecognized kwargs        raise valueerror string                        string   str unrecognized kwargs      def set model self  model       string     self model = model                self  log write dir = distribute file utils write dirpath          self log dir  self model  get distribution strategy           with context eager mode          self  close writers         if self write graph          with self  get writer self  train run name  as default              with summary ops v2 always record summaries                if not model run eagerly                summary ops v2 graph k get graph    step=0               summary writable =                   self model  be graph network or                   self model   class     name   == string                if summary writable                summary ops v2 keras model string  self model  step=0       if self embeddings freq        self  configure embeddings        summary state = summary ops v2  summary state       self  prev summary record = summary state be record     self  prev summary writer = summary state writer     self  prev summary step = summary state step    def  configure embeddings self       string          from tensorflow python keras layer import embeddings     try        from tensorboard plugins import projector     except importerror        raise importerror string                         string      config = projector projectorconfig       for layer in self model layer        if isinstance layer  embeddings embed           embed = config embeddings add           embed tensor name = layer embeddings name          if self embeddings metadata be not none            if isinstance self embeddings metadata  str               embed metadata path = self embeddings metadata           else              if layer name in embed metadata path                embed metadata path = self embeddings metadata pop layer name       if self embeddings metadata        raise valueerror string                        string                        string   str self embeddings metadata key          class dummywriter object         string        def   init   self  logdir           self logdir = logdir        def get logdir self           return self logdir      writer = dummywriter self  log write dir      projector visualize embeddings writer  config     def  close writers self       string     with context eager mode          for writer in six itervalues self  writers           writer close         self  writers clear      def  get writer self  writer name       string     if writer name not in self  writers        path = os path join self  log write dir  writer name        writer = summary ops v2 create file writer v2 path        self  writers writer name  = writer     return self  writers writer name     def  set default writer self  writer name       string     if self update freq == string                      return      step = self  total batch see writer name       def  should record          return math ops equal step   self update freq  0       summary state = summary ops v2  summary state       summary state be record =  should record     summary state writer = self  get writer writer name      summary ops v2 set step step     def  init batch step self       string     if ops execute eagerly outside function                        self  total batch see =             self  train run name  variables variable 0  dtype=string             self  validation run name  variables variable 0  dtype=string              else               self  total batch see =             self  train run name  0            self  validation run name  0            def  increment step self  writer name       step = self  total batch see writer name      if isinstance step  variables variable         step assign add 1      else        self  total batch see writer name   = 1    def on train begin self  logs=none       self  init batch step       if self  profile batch == 1        summary ops v2 trace on graph=true  profiler=true        self  be trace = true    def on test begin self  logs=none       self  set default writer self  validation run name     def on train batch end self  batch  logs=none       string     if self update freq == string and self  profile batch be none        return           log = log or        train batch = self  total batch see self  train run name      if self update freq  = string and batch   self update freq == 0        self  log metrics log  prefix=string  step=train batch       self  increment step self  train run name       if context execute eagerly          if self  be trace          self  log trace         elif  not self  be trace and             math ops equal train batch  self  profile batch - 1            self  enable trace      def on test batch end self  batch  logs=none       if self update freq == string        return     self  increment step self  validation run name     def on epoch begin self  epoch  logs=none       self  set default writer self  train run name     def on epoch end self  epoch  logs=none       string     self  log metrics log  prefix=string  step=epoch       if self histogram freq and epoch   self histogram freq == 0        self  log weight epoch       if self embeddings freq and epoch   self embeddings freq == 0        self  log embeddings epoch     def on train end self  logs=none       if self  be trace        self  log trace       self  close writers        summary state = summary ops v2  summary state       summary state be record = self  prev summary record     summary state writer = self  prev summary writer     summary state step = self  prev summary step           distribute file utils remove temp dirpath          self log dir  self model  get distribution strategy         def  enable trace self       if context execute eagerly          summary ops v2 trace on graph=true  profiler=true        self  be trace = true    def  log trace self       string     if context execute eagerly          with self  get writer self  train run name  as default    \           summary ops v2 always record summaries                     step = k get value self  total batch see self  train run name           summary ops v2 trace export              name=string   step              step=step              profiler outdir=os path join self  log write dir  string         self  be trace = false    def  log metrics self  log  prefix  step       string     if log be none        log =                   log by writer =           self  train run name              self  validation run name                validation prefix = string     for  name  value  in log items          if name in  string  string  string                    continue       if name startswith validation prefix           name = name len validation prefix            writer name = self  validation run name       else          writer name = self  train run name       name = prefix   name         log by writer writer name  append  name  value        with context eager mode          with summary ops v2 always record summaries            for writer name in log by writer            these log = log by writer writer name            if not these log                                        continue           writer = self  get writer writer name            with writer as default                for  name  value  in these log                summary ops v2 scalar name  value  step=step     def  log weight self  epoch       string     writer = self  get writer self  train run name      with context eager mode    \           writer as default    \           summary ops v2 always record summaries          for layer in self model layer          for weight in layer weight            weight name = weight name replace string  string            with ops init scope                weight = k get value weight            summary ops v2 histogram weight name  weight  step=epoch            if self write image              self  log weight as image weight  weight name  epoch        writer flush      def  log weight as image self  weight  weight name  epoch       string     w img = array ops squeeze weight      shape = k int shape w img      if len shape  == 1          w img = array ops reshape w img   1  shape 0   1  1       elif len shape  == 2          if shape 0  > shape 1           w img = array ops transpose w img          shape = k int shape w img        w img = array ops reshape w img   1  shape 0   shape 1   1       elif len shape  == 3          if k image data format   == string                            w img = array ops transpose w img  perm= 2  0  1           shape = k int shape w img        w img = array ops reshape w img   shape 0   shape 1   shape 2   1        shape = k int shape w img           if len shape  == 4 and shape -1  in  1  3  4         summary ops v2 image weight name  w img  step=epoch     def  log embeddings self  epoch       embeddings ckpt = os path join self  log write dir  string                                     string format epoch       self model save weight embeddings ckpt  
class terminateonnan callback     string    def on batch end self  batch  logs=none       log = log or        loss = log get string      if loss be not none        if np isnan loss  or np isinf loss           print string    batch           self model stop train = true 
class constraint object      def   call   self  w       return w    def get config self       return    
class maxnorm constraint     string    def   init   self  max value=2  axis=0       self max value = max value     self axis = axis    def   call   self  w       norms = k sqrt          math ops reduce sum math ops square w   axis=self axis  keepdims=true       desire = k clip norms  0  self max value      return w    desire /  k epsilon     norms      def get config self       return  string  self max value  string  self axis  
class minmaxnorm constraint     string    def   init   self  min value=0 0  max value=1 0  rate=1 0  axis=0       self min value = min value     self max value = max value     self rate = rate     self axis = axis    def   call   self  w       norms = k sqrt          math ops reduce sum math ops square w   axis=self axis  keepdims=true       desire =           self rate   k clip norms  self min value  self max value             1 - self rate    norms      return w    desire /  k epsilon     norms      def get config self       return           string  self min value          string  self max value          string  self rate          string  self axis       
class nonneg constraint     string    def   call   self  w       return w   math ops cast math ops greater equal w  0    k floatx    
class radialconstraint constraint     string    def   call   self  w       w shape = w shape     if w shape rank be none or w shape rank  = 4        raise valueerror            string   w shape       height  width  channel  kernels = w shape     w = k reshape w   height  width  channel   kernels                 w = k map fn          self  kernel constraint          k stack array ops unstack w  axis=-1   axis=0       return k reshape k stack array ops unstack w  axis=0   axis=-1                         height  width  channel  kernels      def  kernel constraint self  kernel       string     pad = k constant   1  1    1  1    dtype=string       kernel shape = k shape kernel  0      start = k cast kernel shape / 2  string       kernel new = k switch          k cast math ops floormod kernel shape  2   string           lambda  kernel start - 1 start  start - 1 start           lambda  kernel start - 1 start  start - 1 start    k zero                 2  2   dtype=kernel dtype       index = k switch          k cast math ops floormod kernel shape  2   string           lambda  k constant 0  dtype=string           lambda  k constant 1  dtype=string       while condition = lambda index   args  k less index  start       def body fn i  array         return i   1  array ops pad            array            pad            constant values=kernel start   i  start   i           kernel new = control flow ops while loop          while condition          body fn           index  kernel new           shape invariants= index get shape                              tensor shape tensorshape  none  none         return kernel new 
class unitnorm constraint     string    def   init   self  axis=0       self axis = axis    def   call   self  w       return w /           k epsilon     k sqrt              math ops reduce sum                  math ops square w   axis=self axis  keepdims=true       def get config self       return  string  self axis  
 keras export string  def get identifier     if identifier be none      return none   if isinstance identifier  dict       return deserialize identifier    elif isinstance identifier  six string type       config =  string  str identifier   string          return deserialize config    elif callable identifier       return identifier   else      raise valueerror string                        str identifier   
 keras export string  def load data path=string  test split=0 2  seed=113     string   assert 0 <= test split < 1   origin folder = string   path = get file        path        origin=origin folder   string        file hash=       string    with np load path  allow pickle=true  as f      x = f string      y = f string     np random seed seed    indices = np arange len x     np random shuffle indices    x = x indices    y = y indices     x train = np array x  int len x     1 - test split       y train = np array y  int len x     1 - test split       x test = np array x int len x     1 - test split        y test = np array y int len x     1 - test split        return  x train  y train    x test  y test  
 keras export string  def load data      string   dirname = string   origin = string   path = get file        dirname        origin=origin        untar=true        file hash=       string     num train sample = 50000    x train = np empty  num train sample  3  32  32   dtype=string    y train = np empty  num train sample    dtype=string     for i in range 1  6       fpath = os path join path  string   str i        x train  i - 1    10000 i   10000                 y train  i - 1    10000 i   10000   = load batch fpath     fpath = os path join path  string    x test  y test = load batch fpath     y train = np reshape y train   len y train   1     y test = np reshape y test   len y test   1      if k image data format   == string      x train = x train transpose 0  2  3  1      x test = x test transpose 0  2  3  1     x test = x test astype x train dtype    y test = y test astype y train dtype     return  x train  y train    x test  y test  
 keras export string  def load data label mode=string     string   if label mode not in  string  string       raise valueerror string     dirname = string   origin = string   path = get file        dirname        origin=origin        untar=true        file hash=       string     fpath = os path join path  string    x train  y train = load batch fpath  label key=label mode   string     fpath = os path join path  string    x test  y test = load batch fpath  label key=label mode   string     y train = np reshape y train   len y train   1     y test = np reshape y test   len y test   1      if k image data format   == string      x train = x train transpose 0  2  3  1      x test = x test transpose 0  2  3  1     return  x train  y train    x test  y test  
 keras export string  def load data      string   dirname = os path join string  string    base = string   file =         string  string        string  string        paths =      for fname in file      paths append get file fname  origin=base   fname  cache subdir=dirname      with gzip open paths 0   string  as lbpath      y train = np frombuffer lbpath read    np uint8  offset=8     with gzip open paths 1   string  as imgpath      x train = np frombuffer          imgpath read    np uint8  offset=16  reshape len y train   28  28     with gzip open paths 2   string  as lbpath      y test = np frombuffer lbpath read    np uint8  offset=8     with gzip open paths 3   string  as imgpath      x test = np frombuffer          imgpath read    np uint8  offset=16  reshape len y test   28  28     return  x train  y train    x test  y test  
 keras export string  def get word index path=string     string   origin folder = string   path = get file        path        origin=origin folder   string        file hash=string    with open path  as f      return json load f  
 keras export string  def load data path=string                num words=none                skip top=0                maxlen=none                seed=113                start char=1                oov char=2                index from=3                  kwargs     string      if string in kwargs      log warn string                     string      num word = kwargs pop string    if kwargs      raise typeerror string   str kwargs      origin folder = string   path = get file        path        origin=origin folder   string        file hash=       string    with np load path  allow pickle=true  as f      x train  label train = f string   f string      x test  label test = f string   f string     np random seed seed    indices = np arange len x train     np random shuffle indices    x train = x train indices    label train = label train indices     indices = np arange len x test     np random shuffle indices    x test = x test indices    label test = label test indices     xs = np concatenate  x train  x test     label = np concatenate  label train  label test      if start char be not none      xs =   start char     w   index from for w in x  for x in xs    elif index from      xs =   w   index from for w in x  for x in xs     if maxlen      xs  label =  remove long seq maxlen  xs  label      if not xs        raise valueerror string                          str maxlen    string                        string    if not num word      num word = max  max x  for x in xs               if oov char be not none      xs =            w if  skip top <= w < num word  else oov char for w in x  for x in xs         else      xs =   w for w in x if skip top <= w < num word  for x in xs     idx = len x train    x train  y train = np array xs  idx    np array label  idx     x test  y test = np array xs idx     np array label idx       return  x train  y train    x test  y test  
 keras export string  def load data path=string     string   origin folder = string   path = get file        path        origin=origin folder   string        file hash=       string    with np load path  allow pickle=true  as f      x train  y train = f string   f string      x test  y test = f string   f string       return  x train  y train    x test  y test  
 keras export string  def get word index path=string     string   origin folder = string   path = get file        path        origin=origin folder   string        file hash=string    with open path  as f      return json load f  
 keras export string  def load data path=string                num words=none                skip top=0                maxlen=none                test split=0 2                seed=113                start char=1                oov char=2                index from=3                  kwargs     string      if string in kwargs      log warn string                     string      num word = kwargs pop string    if kwargs      raise typeerror string   str kwargs      origin folder = string   path = get file        path        origin=origin folder   string        file hash=       string    with np load path  allow pickle=true  as f      xs  label = f string   f string     np random seed seed    indices = np arange len xs     np random shuffle indices    xs = xs indices    label = label indices     if start char be not none      xs =   start char     w   index from for w in x  for x in xs    elif index from      xs =   w   index from for w in x  for x in xs     if maxlen      xs  label =  remove long seq maxlen  xs  label     if not num word      num word = max  max x  for x in xs               if oov char be not none      xs =   w if skip top <= w < num word else oov char for w in x  for x in xs    else      xs =   w for w in x if skip top <= w < num word  for x in xs     idx = int len xs     1 - test split     x train  y train = np array xs  idx    np array label  idx     x test  y test = np array xs idx     np array label idx       return  x train  y train    x test  y test  
 keras export string  v1=    def model to estimator v2      keras model=none      keras model path=none      custom objects=none      model dir=none      config=none      checkpoint format=string     string   try      from tensorflow estimator python estimator import keras as keras lib     except importerror      raise notimplementederror          string         string     model to estiamtor usage gauge get cell string  set true    return keras lib model to estimator          keras model=keras model        keras model path=keras model path        custom objects=custom object        model dir=model dir        config=config        checkpoint format=checkpoint format        use v2 estimator=true  
class constant initializer     string    def   init   self  value=0       if not  np isscalar value  or isinstance value   list  tuple  np ndarray           raise typeerror            string           string   type value       self value = value    def   call   self  shape  dtype=none       string     if dtype be not none        dtype = dtypes as dtype dtype      return constant op constant          self value  dtype=dtype  shape=shape     def get config self       return  string  self value  
class glorotnormal variancescaling     string    def   init   self  seed=none       super glorotnormal  self    init            scale=1 0          mode=string          distribution=string          seed=seed     def get config self       return  string  self seed  
class glorotuniform variancescaling     string    def   init   self  seed=none       super glorotuniform  self    init            scale=1 0          mode=string          distribution=string          seed=seed     def get config self       return  string  self seed  
class identity initializer     string    def   init   self  gain=1 0       self gain = gain    def   call   self  shape  dtype=dtypes float32       string     partition info = none       dtype =  assert float dtype dtype      full shape = shape if partition info be none else partition info full shape     if len full shape   = 2        raise valueerror            string      initializer = linalg ops impl eye  full shape  dtype=dtype      if partition info be not none        initializer = array ops slice initializer  partition info var offset                                      shape      return self gain   initializer    def get config self       return  string  self gain  
class initializer object     string    def   call   self  shape  dtype=none       string     raise notimplementederror    def get config self       string     return        classmethod   def from config cls  config       string     config pop string  none      return cls   config  
class ones initializer     string    def   call   self  shape  dtype=dtypes float32       string     dtype = dtypes as dtype dtype      if not dtype be numpy compatible or dtype == dtypes string        raise valueerror string   dtype      return array ops ones shape  dtype  
class orthogonal initializer     string    def   init   self  gain=1 0  seed=none       self gain = gain     self seed = seed     self  random generator =  randomgenerator seed     def   call   self  shape  dtype=dtypes float32       string     dtype =  assert float dtype dtype           if len shape  < 2        raise valueerror string                        string                num row = 1     for dim in shape  -1         num row  = dim     num cols = shape -1      flat shape =  max num cols  num row   min num cols  num row             a = self  random generator random normal flat shape  dtype=dtype           q  r = gen linalg ops qr a  full matrices=false           d = array ops diag part r      q  = math ops sign d      if num row < num cols        q = array ops matrix transpose q      return self gain   array ops reshape q  shape     def get config self       return  string  self gain  string  self seed  
class randomnormal initializer     string    def   init   self  mean=0 0  stddev=0 05  seed=none       self mean = mean     self stddev = stddev     self seed = seed     self  random generator =  randomgenerator seed     def   call   self  shape  dtype=dtypes float32       string     dtype =  assert float dtype dtype      return self  random generator random normal shape  self mean  self stddev                                                  dtype     def get config self       return           string  self mean          string  self stddev          string  self seed       
class randomuniform initializer     string    def   init   self  minval=-0 05  maxval=0 05  seed=none       self minval = minval     self maxval = maxval     self seed = seed     self  random generator =  randomgenerator seed     def   call   self  shape  dtype=dtypes float32       string     dtype = dtypes as dtype dtype      if not dtype be float and not dtype be integer        raise valueerror string   dtype      return self  random generator random uniform shape  self minval                                                   self maxval  dtype     def get config self       return           string  self minval          string  self maxval          string  self seed       
class truncatednormal initializer     string    def   init   self  mean=0 0  stddev=0 05  seed=none       self mean = mean     self stddev = stddev     self seed = seed     self  random generator =  randomgenerator seed     def   call   self  shape  dtype=dtypes float32       string     dtype =  assert float dtype dtype      return self  random generator truncate normal shape  self mean                                                     self stddev  dtype     def get config self       return           string  self mean          string  self stddev          string  self seed       
class variancescaling initializer     string    def   init   self                 scale=1 0                 mode=string                 distribution=string                 seed=none       if scale <= 0         raise valueerror string      if mode not in  string  string  string         raise valueerror string  mode      distribution = distribution lower            if distribution == string        distribution = string     if distribution not in  string  string                              string         raise valueerror string  distribution      self scale = scale     self mode = mode     self distribution = distribution     self seed = seed     self  random generator =  randomgenerator seed     def   call   self  shape  dtype=dtypes float32       string     partition info = none       dtype =  assert float dtype dtype      scale = self scale     scale shape = shape     if partition info be not none        scale shape = partition info full shape     fan in  fan out =  compute fan scale shape      if self mode == string        scale /= max 1   fan in      elif self mode == string        scale /= max 1   fan out      else        scale /= max 1    fan in   fan out  / 2       if self distribution == string               stddev = math sqrt scale  /  87962566103423978       return self  random generator truncate normal shape  0 0  stddev  dtype      elif self distribution == string        stddev = math sqrt scale        return self  random generator random normal shape  0 0  stddev  dtype      else        limit = math sqrt 3 0   scale        return self  random generator random uniform shape  -limit  limit  dtype     def get config self       return           string  self scale          string  self mode          string  self distribution          string  self seed       
class zero initializer     string    def   call   self  shape  dtype=dtypes float32       dtype = dtypes as dtype dtype      return array ops zero shape  dtype  
 keras export string  def deserialize config  custom objects=none     string   if tf2 enable                       module object =           obj name  getattr init ops v2  obj name          for obj name in dir init ops v2          else      module object = globals     return deserialize keras object        config        module objects=module object        custom objects=custom object        printable module name=string  
 keras export string  def get identifier     if identifier be none      return none   if isinstance identifier  dict       return deserialize identifier    elif isinstance identifier  six string type       identifier = str identifier                special case =  string  string  string  string      if identifier in special case               return deserialize  string  identifier  string           return deserialize identifier    elif callable identifier       return identifier   else      raise valueerror string                        str identifier   
class abstractrnncell layer     string    def call self  input  state       string     raise notimplementederror string      property   def state size self       string     raise notimplementederror string      property   def output size self       string     raise notimplementederror string     def get initial state self  inputs=none  batch size=none  dtype=none       return  generate zero fill state for cell self  input  batch size  dtype  
class activation layer     string    def   init   self  activation    kwargs       super activation  self    init     kwargs      self support mask = true     self activation = activations get activation     def call self  input       return self activation input     def compute output shape self  input shape       return input shape    def get config self       config =  string  activations serialize self activation       base config = super activation  self  get config       return dict list base config items      list config items     
class activityregularization layer     string    def   init   self  l1=0   l2=0     kwargs       super activityregularization  self    init            activity regularizer=regularizers l1l2 l1=l1  l2=l2     kwargs      self support mask = true     self l1 = l1     self l2 = l2    def compute output shape self  input shape       return input shape    def get config self       config =  string  self l1  string  self l2      base config = super activityregularization  self  get config       return dict list base config items      list config items     
class add  merge     string    def  merge function self  input       output = input 0      for i in range 1  len input          output  = input i      return output 
class additiveattention basedenseattention     string    def   init   self  use scale=true    kwargs       super additiveattention  self    init     kwargs      self use scale = use scale    def build self  input shape       v shape = tensor shape tensorshape input shape 1       dim = v shape -1      if isinstance dim  tensor shape dimension         dim = dim value     if self use scale        self scale = self add weight            name=string            shape= dim             initializer=init ops glorot uniform initializer              dtype=self dtype            trainable=true      else        self scale = none     super additiveattention  self  build input shape     def  calculate score self  query  key       string               q reshape = array ops expand dim query  axis=-2           k reshape = array ops expand dim key  axis=-3      if self use scale        scale = self scale     else        scale = 1      return math ops reduce sum          scale   math ops tanh q reshape   k reshape   axis=-1     def get config self       config =  string  self use scale      base config = super additiveattention  self  get config       return dict list base config items      list config items     
class alphadropout layer     string    def   init   self  rate  noise shape=none  seed=none    kwargs       super alphadropout  self    init     kwargs      self rate = rate     self noise shape = noise shape     self seed = seed     self support mask = true    def  get noise shape self  input       return self noise shape if self noise shape else array ops shape input     def call self  input  training=none       if 0  < self rate < 1         noise shape = self  get noise shape input         def drop input inputs=inputs  rate=self rate  seed=self seed             alpha = 1 6732632423543772848170429916717         scale = 1 0507009873554804934193349852946         alpha p = -alpha   scale          keep idx = math ops greater equal              k random uniform noise shape  seed=seed   rate          keep idx = math ops cast keep idx  k floatx                      a =   1 - rate     1   rate   alpha p  2    -0 5         b = -a   alpha p   rate                   x = input   keep idx   alpha p    1 - keep idx                    return a   x   b        return k in train phase drop input  input  training=training      return input    def get config self       config =  string  self rate      base config = super alphadropout  self  get config       return dict list base config items      list config items         tf utils shape type conversion   def compute output shape self  input shape       return input shape 
class attention basedenseattention     string    def   init   self  use scale=false    kwargs       super attention  self    init     kwargs      self use scale = use scale    def build self  input shape       string     if self use scale        self scale = self add weight            name=string            shape=              initializer=init ops ones initializer              dtype=self dtype            trainable=true      else        self scale = none     super attention  self  build input shape     def  calculate score self  query  key       string     score = math ops matmul query  key  transpose b=true      if self scale be not none        score  = self scale     return score    def get config self       config =  string  self use scale      base config = super attention  self  get config       return dict list base config items      list config items     
class average  merge     string    def  merge function self  input       output = input 0      for i in range 1  len input          output  = input i      return output / len input  
class averagepooling1d pooling1d     string    def   init   self  pool size=2  strides=none                 padding=string  data format=string    kwargs       super averagepooling1d  self    init            functools partial backend pool2d  pool mode=string           pool size=pool size          strides=strides          padding=padding          data format=data format            kwargs  
class averagepooling2d pooling2d     string    def   init   self                 pool size= 2  2                  strides=none                 padding=string                 data format=none                   kwargs       super averagepooling2d  self    init            nn avg pool          pool size=pool size  strides=strides          padding=padding  data format=data format    kwargs  
class averagepooling3d pooling3d     string    def   init   self                 pool size= 2  2  2                  strides=none                 padding=string                 data format=none                   kwargs       super averagepooling3d  self    init            nn avg pool3d          pool size=pool size  strides=strides          padding=padding  data format=data format    kwargs  
class batchnormalization normalization batchnormalizationbase        doc   = normalization replace in base docstring          string         string        use v2 behavior = true 
class bidirectional wrapper     string    def   init   self                 layer                 merge mode=string                 weights=none                 backward layer=none                   kwargs       if not isinstance layer  layer         raise valueerror            string           string format input=layer       if backward layer be not none and not isinstance backward layer  layer         raise valueerror string                        string format input=backward layer       if merge mode not in  string  string  string  string  none         raise valueerror string                        string                        string                self forward layer = self  recreate layer from config layer       if backward layer be none        self backward layer = self  recreate layer from config            layer  go backwards=true      else        self backward layer = backward layer                            self  backward layer config = backward layer get config        self forward layer  name = string   self forward layer name     self backward layer  name = string   self backward layer name      self  verify layer config        def force zero output for mask layer                if getattr layer  string  none  be not none          layer zero output for mask = layer return sequence      force zero output for mask self forward layer      force zero output for mask self backward layer       self merge mode = merge mode     if weight        nw = len weight        self forward layer initial weight = weight  nw // 2        self backward layer initial weight = weight nw // 2       self stateful = layer stateful     self return sequence = layer return sequence     self return state = layer return state     self support mask = true     self  trainable = true     self  num constants = 0               self  setattr track = false     super bidirectional  self    init   layer    kwargs      self  setattr track = true     self input spec = layer input spec     self  support rag input = true    def  verify layer config self       string     if self forward layer go backwards == self backward layer go backwards        raise valueerror string                        string       common attribute =  string  string  string      for a in common attribute        forward value = getattr self forward layer  a        backward value = getattr self backward layer  a        if forward value  = backward value          raise valueerror              string             string format                  attr=a  forward=forward value  backward=backward value      def  recreate layer from config self  layer  go backwards=false                                config = layer get config       if go backwards        config string  = not config string      if string in tf inspect getfullargspec          layer   class   from config  args        custom object =          cell = getattr layer  string  none        if cell be not none          custom object cell   class     name    = cell   class                    stack cells = getattr cell  string              for c in stack cells            custom object c   class     name    = c   class         return layer   class   from config config  custom objects=custom object      else        return layer   class   from config config      tf utils shape type conversion   def compute output shape self  input shape       output shape = self forward layer compute output shape input shape      if not isinstance output shape  tensor shape tensorshape         output shape = tensor shape tensorshape output shape      output shape = tuple output shape as list        if self return state        state shape = output shape 1         output shape = output shape 0       if self merge mode == string        output shape = list output shape        output shape -1   = 2       output shape = tuple output shape      elif self merge mode be none        output shape =  output shape  copy copy output shape        if self return state        if self merge mode be none          return output shape   state shape   copy copy state shape        return  output shape    state shape   copy copy state shape      return output shape    def   call   self  input  initial state=none  constants=none    kwargs       string     input  initial state  constants =  standardize args          input  initial state  constants  self  num constants       if isinstance input  list         if len input  > 1          initial state = input 1         input = input 0       if initial state be none and constants be none        return super bidirectional  self    call   input    kwargs            additional input =        additional specs =        if initial state be not none               num state = len initial state        if num state   2 > 0          raise valueerror              string             string             string             string   str initial state          kwargs string  = initial state       additional input  = initial state       state specs =  inputspec shape=k int shape state                        for state in initial state        self forward layer state spec = state specs  num state // 2        self backward layer state spec = state specs num state // 2         additional specs  = state specs     if constants be not none        kwargs string  = constants       additional input  = constants       constants spec =  inputspec shape=k int shape constant                           for constant in constants        self forward layer constants spec = constants spec       self backward layer constants spec = constants spec       additional specs  = constants spec        self  num constants = len constants        self forward layer  num constants = self  num constants       self backward layer  num constants = self  num constants      be keras tensor = k be keras tensor additional input 0       for tensor in additional input        if k be keras tensor tensor   = be keras tensor          raise valueerror string                          string                          string                          string                          string       if be keras tensor               full input =  input    additional input                     full input spec =  none for   in range len nest flatten input                                additional specs              kwargs string  = none       kwargs string  = none               original input spec = self input spec       self input spec = full input spec       output = super bidirectional  self    call   full input    kwargs        self input spec = original input spec       return output     else        return super bidirectional  self    call   input    kwargs     def call self             input             training=none             mask=none             initial state=none             constants=none       string     kwargs =        if generic utils have arg self layer call  string         kwargs string  = train     if generic utils have arg self layer call  string         kwargs string  = mask     if generic utils have arg self layer call  string         kwargs string  = constants      if generic utils have arg self layer call  string         if isinstance input  list  and len input  > 1                                     forward input =  input 0           backward input =  input 0           pivot =  len input  - self  num constants  // 2   1                  forward input  = input 1 pivot          if not self  num constants                       backward input  = input pivot           else                       backward input  = input pivot -self  num constants                       forward input  = input -self  num constants             backward input  = input -self  num constants           forward state  backward state = none  none         if string in kwargs            kwargs string  = none       elif initial state be not none                                     forward input  backward input = input  input         half = len initial state  // 2         forward state = initial state  half          backward state = initial state half         else          forward input  backward input = input  input         forward state  backward state = none  none        y = self forward layer forward input                               initial state=forward state    kwargs        y rev = self backward layer backward input                                    initial state=backward state    kwargs      else        y = self forward layer input    kwargs        y rev = self backward layer input    kwargs       if self return state        state = y 1     y rev 1         y = y 0        y rev = y rev 0       if self return sequence        y rev = k reverse y rev  1      if self merge mode == string        output = k concatenate  y  y rev       elif self merge mode == string        output = y   y rev     elif self merge mode == string        output =  y   y rev  / 2     elif self merge mode == string        output = y   y rev     elif self merge mode be none        output =  y  y rev      else        raise valueerror            string    self merge mode        if self return state        if self merge mode be none          return output   state       return  output    state     return output    def reset state self       self forward layer reset state       self backward layer reset state      def build self  input shape       with k name scope self forward layer name         self forward layer build input shape      with k name scope self backward layer name         self backward layer build input shape      self build = true    def compute mask self  input  mask       if isinstance mask  list         mask = mask 0      if self return sequence        if not self merge mode          output mask =  mask  mask        else          output mask = mask     else        output mask =  none  none  if not self merge mode else none      if self return state        state = self forward layer state       state mask =  none for   in state        if isinstance output mask  list           return output mask   state mask   2       return  output mask    state mask   2     return output mask     property   def constraints self       constraints =        if hasattr self forward layer  string         constraints update self forward layer constraints        constraints update self backward layer constraints      return constraints    def get config self       config =  string  self merge mode      if self  num constants        config string  = self  num constants      if hasattr self  string         config string  =             string  self backward layer   class     name              string  self  backward layer config              base config = super bidirectional  self  get config       return dict list base config items      list config items         classmethod   def from config cls  config  custom objects=none            config = config copy       num constants = config pop string  0      backward layer config = config pop string  none      if backward layer config be not none        from tensorflow python keras layer import deserialize as deserialize layer         backward layer = deserialize layer            backward layer config  custom objects=custom object        config string  = backward layer      layer = super bidirectional  cls  from config config                                                    custom objects=custom object      layer  num constants = num constants     return layer 
class concatenate  merge     string    def   init   self  axis=-1    kwargs       super concatenate  self    init     kwargs      self axis = axis     self support mask = true     self  reshape require = false     tf utils shape type conversion   def build self  input shape            if not isinstance input shape  list  or len input shape  < 2        raise valueerror string                        string      if all shape be none for shape in input shape         return     reduce input shape =  list shape  for shape in input shape      shape set = set       for i in range len reduce input shape          del reduce input shape i  self axis        shape set add tuple reduce input shape i         if len shape set   = 1        err msg =  string                  string                    input shape               rank = set  len shape  for shape in shape set         if len rank   = 1          raise valueerror err msg                rank   = rank       for axis in range rank                             unique dim = set  shape axis  for shape in shape set                            if shape axis  be not none           if len unique dim  > 1            raise valueerror err msg     def  merge function self  input       return k concatenate input  axis=self axis      tf utils shape type conversion   def compute output shape self  input shape       if not isinstance input shape  list         raise valueerror string                        string      input shape = input shape     output shape = list input shape 0       for shape in input shape 1          if output shape self axis  be none or shape self axis  be none          output shape self axis  = none         break       output shape self axis   = shape self axis      return tuple output shape     def compute mask self  input  mask=none       if mask be none        return none     if not isinstance mask  list         raise valueerror string      if not isinstance input  list         raise valueerror string      if len mask   = len input         raise valueerror string                        string      if all m be none for m in mask         return none                    mask =        for input i  mask i in zip input  mask         if mask i be none                   mask append array ops ones like input i  dtype=string         elif k ndim mask i  < k ndim input i                    mask append array ops expand dim mask i  axis=-1         else          mask append mask i      concatenate = k concatenate mask  axis=self axis      return k all concatenate  axis=-1  keepdims=false     def get config self       config =           string  self axis            base config = super concatenate  self  get config       return dict list base config items      list config items     
class conv1d conv     string    def   init   self                 filter                 kernel size                 strides=1                 padding=string                 data format=string                 dilation rate=1                 activation=none                 use bias=true                 kernel initializer=string                 bias initializer=string                 kernel regularizer=none                 bias regularizer=none                 activity regularizer=none                 kernel constraint=none                 bias constraint=none                   kwargs       super conv1d  self    init            rank=1          filters=filters          kernel size=kernel size          strides=strides          padding=padding          data format=data format          dilation rate=dilation rate          activation=activations get activation           use bias=use bias          kernel initializer=initializers get kernel initializer           bias initializer=initializers get bias initializer           kernel regularizer=regularizers get kernel regularizer           bias regularizer=regularizers get bias regularizer           activity regularizer=regularizers get activity regularizer           kernel constraint=constraints get kernel constraint           bias constraint=constraints get bias constraint             kwargs  
class conv2d conv     string    def   init   self                 filter                 kernel size                 strides= 1  1                  padding=string                 data format=none                 dilation rate= 1  1                  activation=none                 use bias=true                 kernel initializer=string                 bias initializer=string                 kernel regularizer=none                 bias regularizer=none                 activity regularizer=none                 kernel constraint=none                 bias constraint=none                   kwargs       super conv2d  self    init            rank=2          filters=filters          kernel size=kernel size          strides=strides          padding=padding          data format=data format          dilation rate=dilation rate          activation=activations get activation           use bias=use bias          kernel initializer=initializers get kernel initializer           bias initializer=initializers get bias initializer           kernel regularizer=regularizers get kernel regularizer           bias regularizer=regularizers get bias regularizer           activity regularizer=regularizers get activity regularizer           kernel constraint=constraints get kernel constraint           bias constraint=constraints get bias constraint             kwargs  
class conv2dtranspose conv2d     string    def   init   self                 filter                 kernel size                 strides= 1  1                  padding=string                 output padding=none                 data format=none                 dilation rate= 1  1                  activation=none                 use bias=true                 kernel initializer=string                 bias initializer=string                 kernel regularizer=none                 bias regularizer=none                 activity regularizer=none                 kernel constraint=none                 bias constraint=none                   kwargs       super conv2dtranspose  self    init            filters=filters          kernel size=kernel size          strides=strides          padding=padding          data format=data format          dilation rate=dilation rate          activation=activations get activation           use bias=use bias          kernel initializer=initializers get kernel initializer           bias initializer=initializers get bias initializer           kernel regularizer=regularizers get kernel regularizer           bias regularizer=regularizers get bias regularizer           activity regularizer=regularizers get activity regularizer           kernel constraint=constraints get kernel constraint           bias constraint=constraints get bias constraint             kwargs       self output pad = output pad     if self output pad be not none        self output pad = conv utils normalize tuple            self output pad  2  string        for stride  out pad in zip self stride  self output pad           if out pad >= stride            raise valueerror string   str self stride    string                            string                              str self output pad      def build self  input shape       input shape = tensor shape tensorshape input shape      if len input shape   = 4        raise valueerror string                          str input shape       channel axis = self  get channel axis       if input shape dim channel axis  value be none        raise valueerror string                        string      input dim = int input shape channel axis       self input spec = inputspec ndim=4  axes= channel axis  input dim       kernel shape = self kernel size    self filter  input dim       self kernel = self add weight          name=string          shape=kernel shape          initializer=self kernel initializer          regularizer=self kernel regularizer          constraint=self kernel constraint          trainable=true          dtype=self dtype      if self use bias        self bias = self add weight            name=string            shape= self filter              initializer=self bias initializer            regularizer=self bias regularizer            constraint=self bias constraint            trainable=true            dtype=self dtype      else        self bias = none     self build = true    def call self  input       input shape = array ops shape input      batch size = input shape 0      if self data format == string        h axis  w axis = 2  3     else        h axis  w axis = 1  2      height  width = input shape h axis   input shape w axis      kernel h  kernel w = self kernel size     stride h  stride w = self stride      if self output pad be none        out pad h = out pad w = none     else        out pad h  out pad w = self output pad           out height = conv utils deconv output length height                                                   kernel h                                                   padding=self pad                                                   output padding=out pad h                                                   stride=stride h                                                   dilation=self dilation rate 0       out width = conv utils deconv output length width                                                  kernel w                                                  padding=self pad                                                  output padding=out pad w                                                  stride=stride w                                                  dilation=self dilation rate 1       if self data format == string        output shape =  batch size  self filter  out height  out width      else        output shape =  batch size  out height  out width  self filter       output shape tensor = array ops stack output shape      output = backend conv2d transpose          input          self kernel          output shape tensor          strides=self stride          padding=self pad          data format=self data format          dilation rate=self dilation rate       if not context execute eagerly                 out shape = self compute output shape input shape        output set shape out shape       if self use bias        output = nn bias add            output            self bias            data format=conv utils convert data format self data format  ndim=4        if self activation be not none        return self activation output      return output    def compute output shape self  input shape       input shape = tensor shape tensorshape input shape  as list       output shape = list input shape      if self data format == string        c axis  h axis  w axis = 1  2  3     else        c axis  h axis  w axis = 3  1  2      kernel h  kernel w = self kernel size     stride h  stride w = self stride      if self output pad be none        out pad h = out pad w = none     else        out pad h  out pad w = self output pad      output shape c axis  = self filter     output shape h axis  = conv utils deconv output length          output shape h axis           kernel h          padding=self pad          output padding=out pad h          stride=stride h          dilation=self dilation rate 0       output shape w axis  = conv utils deconv output length          output shape w axis           kernel w          padding=self pad          output padding=out pad w          stride=stride w          dilation=self dilation rate 1       return tensor shape tensorshape output shape     def get config self       config = super conv2dtranspose  self  get config       config string  = self output pad     return config 
class conv3d conv     string    def   init   self                 filter                 kernel size                 strides= 1  1  1                  padding=string                 data format=none                 dilation rate= 1  1  1                  activation=none                 use bias=true                 kernel initializer=string                 bias initializer=string                 kernel regularizer=none                 bias regularizer=none                 activity regularizer=none                 kernel constraint=none                 bias constraint=none                   kwargs       super conv3d  self    init            rank=3          filters=filters          kernel size=kernel size          strides=strides          padding=padding          data format=data format          dilation rate=dilation rate          activation=activations get activation           use bias=use bias          kernel initializer=initializers get kernel initializer           bias initializer=initializers get bias initializer           kernel regularizer=regularizers get kernel regularizer           bias regularizer=regularizers get bias regularizer           activity regularizer=regularizers get activity regularizer           kernel constraint=constraints get kernel constraint           bias constraint=constraints get bias constraint             kwargs  
class conv3dtranspose conv3d     string    def   init   self                 filter                 kernel size                 strides= 1  1  1                  padding=string                 output padding=none                 data format=none                 activation=none                 use bias=true                 kernel initializer=string                 bias initializer=string                 kernel regularizer=none                 bias regularizer=none                 activity regularizer=none                 kernel constraint=none                 bias constraint=none                   kwargs       super conv3dtranspose  self    init            filters=filters          kernel size=kernel size          strides=strides          padding=padding          data format=data format          activation=activations get activation           use bias=use bias          kernel initializer=initializers get kernel initializer           bias initializer=initializers get bias initializer           kernel regularizer=regularizers get kernel regularizer           bias regularizer=regularizers get bias regularizer           activity regularizer=regularizers get activity regularizer           kernel constraint=constraints get kernel constraint           bias constraint=constraints get bias constraint             kwargs       self output pad = output pad     if self output pad be not none        self output pad = conv utils normalize tuple            self output pad  3  string        for stride  out pad in zip self stride  self output pad           if out pad >= stride            raise valueerror string   str self stride    string                            string                              str self output pad      def build self  input shape       input shape = tensor shape tensorshape input shape      if len input shape   = 5        raise valueerror string                         str input shape       channel axis = self  get channel axis       if input shape dim channel axis  value be none        raise valueerror string                        string   str input shape       input dim = int input shape channel axis       kernel shape = self kernel size    self filter  input dim      self input spec = inputspec ndim=5  axes= channel axis  input dim        self kernel = self add weight          string          shape=kernel shape          initializer=self kernel initializer          regularizer=self kernel regularizer          constraint=self kernel constraint          trainable=true          dtype=self dtype      if self use bias        self bias = self add weight            string            shape= self filter              initializer=self bias initializer            regularizer=self bias regularizer            constraint=self bias constraint            trainable=true            dtype=self dtype      else        self bias = none     self build = true    def call self  input       input shape = array ops shape input      batch size = input shape 0      if self data format == string        d axis  h axis  w axis = 2  3  4     else        d axis  h axis  w axis = 1  2  3      depth = input shape d axis      height = input shape h axis      width = input shape w axis       kernel d  kernel h  kernel w = self kernel size     stride d  stride h  stride w = self stride      if self output pad be none        out pad d = out pad h = out pad w = none     else        out pad d  out pad h  out pad w = self output pad           out depth = conv utils deconv output length depth                                                  kernel d                                                  padding=self pad                                                  output padding=out pad d                                                  stride=stride d      out height = conv utils deconv output length height                                                   kernel h                                                   padding=self pad                                                   output padding=out pad h                                                   stride=stride h      out width = conv utils deconv output length width                                                  kernel w                                                  padding=self pad                                                  output padding=out pad w                                                  stride=stride w      if self data format == string        output shape =  batch size  self filter  out depth  out height                        out width        stride =  1  1  stride d  stride h  stride w      else        output shape =  batch size  out depth  out height  out width                        self filter        stride =  1  stride d  stride h  stride w  1       output shape tensor = array ops stack output shape      output = nn conv3d transpose          input          self kernel          output shape tensor          stride          data format=conv utils convert data format self data format  ndim=5           padding=self pad upper         if not context execute eagerly                 out shape = self compute output shape input shape        output set shape out shape       if self use bias        output = nn bias add            output            self bias            data format=conv utils convert data format self data format  ndim=4        if self activation be not none        return self activation output      return output    def compute output shape self  input shape       input shape = tensor shape tensorshape input shape  as list       output shape = list input shape      if self data format == string        c axis  d axis  h axis  w axis = 1  2  3  4     else        c axis  d axis  h axis  w axis = 4  1  2  3      kernel d  kernel h  kernel w = self kernel size     stride d  stride h  stride w = self stride      if self output pad be none        out pad d = out pad h = out pad w = none     else        out pad d  out pad h  out pad w = self output pad      output shape c axis  = self filter     output shape d axis  = conv utils deconv output length          output shape d axis           kernel d          padding=self pad          output padding=out pad d          stride=stride d      output shape h axis  = conv utils deconv output length          output shape h axis           kernel h          padding=self pad          output padding=out pad h          stride=stride h      output shape w axis  = conv utils deconv output length          output shape w axis           kernel w          padding=self pad          output padding=out pad w          stride=stride w      return tensor shape tensorshape output shape     def get config self       config = super conv3dtranspose  self  get config       config pop string      config string  = self output pad     return config 
class convlstm2d convrnn2d     string    def   init   self                 filter                 kernel size                 strides= 1  1                  padding=string                 data format=none                 dilation rate= 1  1                  activation=string                 recurrent activation=string                 use bias=true                 kernel initializer=string                 recurrent initializer=string                 bias initializer=string                 unit forget bias=true                 kernel regularizer=none                 recurrent regularizer=none                 bias regularizer=none                 activity regularizer=none                 kernel constraint=none                 recurrent constraint=none                 bias constraint=none                 return sequences=false                 go backwards=false                 stateful=false                 dropout=0                  recurrent dropout=0                    kwargs       cell = convlstm2dcell filters=filters                            kernel size=kernel size                            strides=strides                            padding=padding                            data format=data format                            dilation rate=dilation rate                            activation=activation                            recurrent activation=recurrent activation                            use bias=use bias                            kernel initializer=kernel initializer                            recurrent initializer=recurrent initializer                            bias initializer=bias initializer                            unit forget bias=unit forget bias                            kernel regularizer=kernel regularizer                            recurrent regularizer=recurrent regularizer                            bias regularizer=bias regularizer                            kernel constraint=kernel constraint                            recurrent constraint=recurrent constraint                            bias constraint=bias constraint                            dropout=dropout                            recurrent dropout=recurrent dropout                            dtype=kwargs get string       super convlstm2d  self    init   cell                                       return sequences=return sequence                                       go backwards=go backwards                                       stateful=stateful                                         kwargs      self activity regularizer = regularizers get activity regularizer     def call self  input  mask=none  training=none  initial state=none       self  maybe reset cell dropout mask self cell      return super convlstm2d  self  call input                                          mask=mask                                          training=training                                          initial state=initial state      property   def filter self       return self cell filter     property   def kernel size self       return self cell kernel size     property   def stride self       return self cell stride     property   def pad self       return self cell pad     property   def data format self       return self cell data format     property   def dilation rate self       return self cell dilation rate     property   def activation self       return self cell activation     property   def recurrent activation self       return self cell recurrent activation     property   def use bias self       return self cell use bias     property   def kernel initializer self       return self cell kernel initializer     property   def recurrent initializer self       return self cell recurrent initializer     property   def bias initializer self       return self cell bias initializer     property   def unit forget bias self       return self cell unit forget bias     property   def kernel regularizer self       return self cell kernel regularizer     property   def recurrent regularizer self       return self cell recurrent regularizer     property   def bias regularizer self       return self cell bias regularizer     property   def kernel constraint self       return self cell kernel constraint     property   def recurrent constraint self       return self cell recurrent constraint     property   def bias constraint self       return self cell bias constraint     property   def dropout self       return self cell dropout     property   def recurrent dropout self       return self cell recurrent dropout    def get config self       config =  string  self filter                string  self kernel size                string  self stride                string  self pad                string  self data format                string  self dilation rate                string  activations serialize self activation                 string  activations serialize                    self recurrent activation                 string  self use bias                string  initializers serialize                    self kernel initializer                 string  initializers serialize                    self recurrent initializer                 string  initializers serialize self bias initializer                 string  self unit forget bias                string  regularizers serialize                    self kernel regularizer                 string  regularizers serialize                    self recurrent regularizer                 string  regularizers serialize self bias regularizer                 string  regularizers serialize                    self activity regularizer                 string  constraints serialize                    self kernel constraint                 string  constraints serialize                    self recurrent constraint                 string  constraints serialize self bias constraint                 string  self dropout                string  self recurrent dropout      base config = super convlstm2d  self  get config       del base config string      return dict list base config items      list config items         classmethod   def from config cls  config       return cls   config  
class cropping1d layer     string    def   init   self  cropping= 1  1     kwargs       super cropping1d  self    init     kwargs      self crop = conv utils normalize tuple crop  2  string      self input spec = inputspec ndim=3     def compute output shape self  input shape       input shape = tensor shape tensorshape input shape  as list       if input shape 1  be not none        length = input shape 1  - self crop 0  - self crop 1      else        length = none     return tensor shape tensorshape  input shape 0   length  input shape 2       def call self  input       if self crop 1  == 0        return input    self crop 0           else        return input    self crop 0  -self crop 1         def get config self       config =  string  self crop      base config = super cropping1d  self  get config       return dict list base config items      list config items     
class cropping2d layer     string    def   init   self  cropping=  0  0    0  0    data format=none    kwargs       super cropping2d  self    init     kwargs      self data format = conv utils normalize data format data format      if isinstance crop  int         self crop =   crop  crop    crop  crop       elif hasattr crop  string         if len crop   = 2          raise valueerror string                          string   str crop         height crop = conv utils normalize tuple crop 0   2                                                     string        width crop = conv utils normalize tuple crop 1   2                                                    string        self crop =  height crop  width crop      else        raise valueerror string                        string                        string                        string                        string                        string   str crop       self input spec = inputspec ndim=4     def compute output shape self  input shape       input shape = tensor shape tensorshape input shape  as list            if self data format == string        return tensor shape tensorshape             input shape 0   input shape 1             input shape 2  - self crop 0  0  - self crop 0  1            if input shape 2  else none            input shape 3  - self crop 1  0  - self crop 1  1            if input shape 3  else none              else        return tensor shape tensorshape             input shape 0             input shape 1  - self crop 0  0  - self crop 0  1            if input shape 1  else none            input shape 2  - self crop 1  0  - self crop 1  1            if input shape 2  else none  input shape 3                   def call self  input            if self data format == string        if self crop 0  1  == self crop 1  1  == 0          return input       self crop 0  0    self crop 1  0          elif self crop 0  1  == 0          return input       self crop 0  0    self crop 1  0                         -self crop 1  1         elif self crop 1  1  == 0          return input       self crop 0  0  -self crop 0  1                         self crop 1  0          return input       self crop 0  0  -self crop 0  1                       self crop 1  0  -self crop 1  1       else        if self crop 0  1  == self crop 1  1  == 0          return input    self crop 0  0    self crop 1  0             elif self crop 0  1  == 0          return input    self crop 0  0    self crop 1  0                         -self crop 1  1            elif self crop 1  1  == 0          return input    self crop 0  0  -self crop 0  1                         self crop 1  0             return input    self crop 0  0  -self crop 0  1   self crop            1  0  -self crop 1  1                def get config self       config =  string  self crop  string  self data format      base config = super cropping2d  self  get config       return dict list base config items      list config items     
class cropping3d layer     string    def   init   self                 cropping=  1  1    1  1    1  1                   data format=none                   kwargs       super cropping3d  self    init     kwargs      self data format = conv utils normalize data format data format      if isinstance crop  int         self crop =   crop  crop    crop  crop    crop                                                                      crop       elif hasattr crop  string         if len crop   = 3          raise valueerror string                          string   str crop         dim1 crop = conv utils normalize tuple crop 0   2                                                   string        dim2 crop = conv utils normalize tuple crop 1   2                                                   string        dim3 crop = conv utils normalize tuple crop 2   2                                                   string        self crop =  dim1 crop  dim2 crop  dim3 crop      else        raise valueerror            string           string           string           string           string           string           string           string   str crop       self input spec = inputspec ndim=5     def compute output shape self  input shape       input shape = tensor shape tensorshape input shape  as list            if self data format == string        if input shape 2  be not none          dim1 = input shape 2  - self crop 0  0  - self crop 0  1        else          dim1 = none       if input shape 3  be not none          dim2 = input shape 3  - self crop 1  0  - self crop 1  1        else          dim2 = none       if input shape 4  be not none          dim3 = input shape 4  - self crop 2  0  - self crop 2  1        else          dim3 = none       return tensor shape tensorshape             input shape 0   input shape 1   dim1  dim2  dim3       elif self data format == string        if input shape 1  be not none          dim1 = input shape 1  - self crop 0  0  - self crop 0  1        else          dim1 = none       if input shape 2  be not none          dim2 = input shape 2  - self crop 1  0  - self crop 1  1        else          dim2 = none       if input shape 3  be not none          dim3 = input shape 3  - self crop 2  0  - self crop 2  1        else          dim3 = none       return tensor shape tensorshape             input shape 0   dim1  dim2  dim3  input shape 4            def call self  input            if self data format == string        if self crop 0  1  == self crop 1  1  == self crop 2  1  == 0          return input       self crop 0  0    self crop 1  0                          self crop 2  0          elif self crop 0  1  == self crop 1  1  == 0          return input       self crop 0  0    self crop 1  0                          self crop 2  0  -self crop 2  1         elif self crop 1  1  == self crop 2  1  == 0          return input       self crop 0  0  -self crop 0  1                         self crop 1  0    self crop 2  0          elif self crop 0  1  == self crop 2  1  == 0          return input       self crop 0  0    self crop 1  0                         -self crop 1  1   self crop 2  0          elif self crop 0  1  == 0          return input       self crop 0  0    self crop 1               0  -self crop 1  1   self crop 2  0  -self crop 2  1         elif self crop 1  1  == 0          return input       self crop 0  0  -self crop 0  1   self                        crop 1  0    self crop 2  0  -self crop 2  1         elif self crop 2  1  == 0          return input       self crop 0  0  -self crop 0  1   self                        crop 1  0  -self crop 1  1   self crop 2  0          return input       self crop 0  0  -self crop 0  1                       self crop 1  0  -self crop 1  1   self crop 2                           0  -self crop 2  1       else        if self crop 0  1  == self crop 1  1  == self crop 2  1  == 0          return input    self crop 0  0    self crop 1  0                          self crop 2  0             elif self crop 0  1  == self crop 1  1  == 0          return input    self crop 0  0    self crop 1  0                          self crop 2  0  -self crop 2  1            elif self crop 1  1  == self crop 2  1  == 0          return input    self crop 0  0  -self crop 0  1                         self crop 1  0    self crop 2  0             elif self crop 0  1  == self crop 2  1  == 0          return input    self crop 0  0    self crop 1  0                         -self crop 1  1   self crop 2  0             elif self crop 0  1  == 0          return input    self crop 0  0    self crop 1               0  -self crop 1  1   self crop 2  0                         -self crop 2  1            elif self crop 1  1  == 0          return input    self crop 0               0  -self crop 0  1   self crop 1  0    self crop 2  0                         -self crop 2  1            elif self crop 2  1  == 0          return input    self crop 0  0  -self crop 0  1                         self crop 1  0  -self crop 1  1   self crop                            2  0             return input    self crop 0  0  -self crop 0  1   self crop            1  0  -self crop 1  1   self crop 2  0                         -self crop 2  1                def get config self       config =  string  self crop  string  self data format      base config = super cropping3d  self  get config       return dict list base config items      list config items     
class dense layer     string    def   init   self                 units                 activation=none                 use bias=true                 kernel initializer=string                 bias initializer=string                 kernel regularizer=none                 bias regularizer=none                 activity regularizer=none                 kernel constraint=none                 bias constraint=none                   kwargs       if string not in kwargs and string in kwargs        kwargs string  =  kwargs pop string         super dense  self    init            activity regularizer=regularizers get activity regularizer     kwargs       self units = int units  if not isinstance units  int  else units     self activation = activations get activation      self use bias = use bias     self kernel initializer = initializers get kernel initializer      self bias initializer = initializers get bias initializer      self kernel regularizer = regularizers get kernel regularizer      self bias regularizer = regularizers get bias regularizer      self kernel constraint = constraints get kernel constraint      self bias constraint = constraints get bias constraint       self support mask = true     self input spec = inputspec min ndim=2     def build self  input shape       dtype = dtypes as dtype self dtype or k floatx        if not  dtype be float or dtype be complex         raise typeerror string                       string    dtype        input shape = tensor shape tensorshape input shape      if tensor shape dimension value input shape -1   be none        raise valueerror string                        string      last dim = tensor shape dimension value input shape -1       self input spec = inputspec min ndim=2                                  axes= -1  last dim       self kernel = self add weight          string          shape= last dim  self units           initializer=self kernel initializer          regularizer=self kernel regularizer          constraint=self kernel constraint          dtype=self dtype          trainable=true      if self use bias        self bias = self add weight            string            shape= self units              initializer=self bias initializer            regularizer=self bias regularizer            constraint=self bias constraint            dtype=self dtype            trainable=true      else        self bias = none     self build = true    def call self  input       rank = len input shape      if rank > 2               output = standard ops tensordot input  self kernel    rank - 1    0                 if not context execute eagerly            shape = input shape as list           output shape = shape  -1     self units          output set shape output shape      else        input = math ops cast input  self  compute dtype        if k be sparse input           output = sparse ops sparse tensor dense matmul input  self kernel        else          output = gen math ops mat mul input  self kernel      if self use bias        output = nn bias add output  self bias      if self activation be not none        return self activation output        return output    def compute output shape self  input shape       input shape = tensor shape tensorshape input shape      input shape = input shape with rank at least 2      if tensor shape dimension value input shape -1   be none        raise valueerror            string             input shape      return input shape  -1  concatenate self units     def get config self       config =           string  self units          string  activations serialize self activation           string  self use bias          string  initializers serialize self kernel initializer           string  initializers serialize self bias initializer           string  regularizers serialize self kernel regularizer           string  regularizers serialize self bias regularizer           string              regularizers serialize self activity regularizer           string  constraints serialize self kernel constraint           string  constraints serialize self bias constraint            base config = super dense  self  get config       return dict list base config items      list config items     
class densefeatures dense feature densefeatures     string    def   init   self                 feature columns                 trainable=true                 name=none                   kwargs       string     super densefeatures  self    init            feature columns=feature columns          trainable=trainable          name=name            kwargs      self  state manager = fc  statemanagerimplv2 self  self trainable       def build self          for column in self  feature columns        with ops name scope column name           column create state self  state manager                super fc  basefeatureslayer  self  build none    
class depthwiseconv2d conv2d     string    def   init   self                 kernel size                 strides= 1  1                  padding=string                 depth multiplier=1                 data format=none                 activation=none                 use bias=true                 depthwise initializer=string                 bias initializer=string                 depthwise regularizer=none                 bias regularizer=none                 activity regularizer=none                 depthwise constraint=none                 bias constraint=none                   kwargs       super depthwiseconv2d  self    init            filters=none          kernel size=kernel size          strides=strides          padding=padding          data format=data format          activation=activation          use bias=use bias          bias regularizer=bias regularizer          activity regularizer=activity regularizer          bias constraint=bias constraint            kwargs      self depth multiplier = depth multiplier     self depthwise initializer = initializers get depthwise initializer      self depthwise regularizer = regularizers get depthwise regularizer      self depthwise constraint = constraints get depthwise constraint      self bias initializer = initializers get bias initializer     def build self  input shape       if len input shape  < 4        raise valueerror string                        string  str input shape       input shape = tensor shape tensorshape input shape      channel axis = self  get channel axis       if input shape dim channel axis  value be none        raise valueerror string                        string                        string      input dim = int input shape channel axis       depthwise kernel shape =  self kernel size 0                                 self kernel size 1                                 input dim                                self depth multiplier       self depthwise kernel = self add weight          shape=depthwise kernel shape          initializer=self depthwise initializer          name=string          regularizer=self depthwise regularizer          constraint=self depthwise constraint       if self use bias        self bias = self add weight shape= input dim   self depth multiplier                                      initializer=self bias initializer                                    name=string                                    regularizer=self bias regularizer                                    constraint=self bias constraint      else        self bias = none          self input spec = inputspec ndim=4  axes= channel axis  input dim       self build = true    def call self  input       output = backend depthwise conv2d          input          self depthwise kernel          strides=self stride          padding=self pad          dilation rate=self dilation rate          data format=self data format       if self use bias        output = backend bias add            output            self bias            data format=self data format       if self activation be not none        return self activation output       return output     tf utils shape type conversion   def compute output shape self  input shape       if self data format == string        row = input shape 2        cols = input shape 3        out filter = input shape 1    self depth multiplier     elif self data format == string        row = input shape 1        cols = input shape 2        out filter = input shape 3    self depth multiplier      row = conv utils conv output length row  self kernel size 0                                            self pad                                           self stride 0       cols = conv utils conv output length cols  self kernel size 1                                            self pad                                           self stride 1       if self data format == string        return  input shape 0   out filter  row  cols      elif self data format == string        return  input shape 0   row  cols  out filter     def get config self       config = super depthwiseconv2d  self  get config       config pop string      config pop string      config pop string      config pop string      config string  = self depth multiplier     config string  = initializers serialize          self depthwise initializer      config string  = regularizers serialize          self depthwise regularizer      config string  = constraints serialize          self depthwise constraint      return config 
class dot  merge     string    def   init   self  ax  normalize=false    kwargs       super dot  self    init     kwargs      if not isinstance ax  int         if not isinstance ax   list  tuple            raise typeerror string                         string        if len ax   = 2          raise valueerror string                          string        if not isinstance ax 0   int  or not isinstance ax 1   int           raise valueerror string                          string      self ax = ax     self normalize = normalize     self support mask = true     self  reshape require = false     self  support rag input = false     tf utils shape type conversion   def build self  input shape            if not isinstance input shape  list  or len input shape   = 2        raise valueerror string                        string      shape1 = input shape 0      shape2 = input shape 1      if shape1 be none or shape2 be none        return     if isinstance self ax  int         if self ax < 0          ax =  self ax   len shape1   self ax   len shape2         else          ax =  self ax    2     else        ax = self ax     if shape1 ax 0    = shape2 ax 1          raise valueerror string                        string    shape1 ax 0    shape2 ax 1                             string    shape1  shape2      def  merge function self  input       if len input   = 2        raise valueerror string      x1 = input 0      x2 = input 1      if isinstance self ax  int         if self ax < 0          ax =  self ax   k ndim x1   self ax   k ndim x2         else          ax =  self ax    2     else        ax =          for i in range len self ax            if self ax i  < 0            ax append self ax i    k ndim input i            else            ax append self ax i       if self normalize        x1 = nn l2 normalize x1  axis=axes 0         x2 = nn l2 normalize x2  axis=axes 1       output = k batch dot x1  x2  ax      return output     tf utils shape type conversion   def compute output shape self  input shape       if not isinstance input shape  list  or len input shape   = 2        raise valueerror string                        string      shape1 = list input shape 0       shape2 = list input shape 1       if isinstance self ax  int         if self ax < 0          ax =  self ax   len shape1   self ax   len shape2         else          ax =  self ax    2     else        ax = self ax     shape1 pop ax 0       shape2 pop ax 1       shape2 pop 0      output shape = shape1   shape2     if len output shape  == 1        output shape  =  1      return tuple output shape     def compute mask self  input  mask=none       return none    def get config self       config =           string  self ax          string  self normalize            base config = super dot  self  get config       return dict list base config items      list config items     
class dropout layer     string    def   init   self  rate  noise shape=none  seed=none    kwargs       super dropout  self    init     kwargs      self rate = rate     self noise shape = noise shape     self seed = seed     self support mask = true    def  get noise shape self  input                      if self noise shape be none        return none      concrete input shape = array ops shape input      noise shape =        for i  value in enumerate self noise shape         noise shape append concrete input shape i  if value be none else value      return ops convert to tensor noise shape     def call self  input  training=none       if train be none        train = k learn phase        def drop input          return nn dropout            input            noise shape=self  get noise shape input             seed=self seed            rate=self rate       output = tf utils smart cond train                                   drop input                                   lambda  array ops identity input       return output    def compute output shape self  input shape       return input shape    def get config self       config =           string  self rate          string  self noise shape          string  self seed           base config = super dropout  self  get config       return dict list base config items      list config items     
class elu layer     string    def   init   self  alpha=1 0    kwargs       super elu  self    init     kwargs      self support mask = true     self alpha = k cast to floatx alpha     def call self  input       return k elu input  self alpha     def get config self       config =  string  float self alpha       base config = super elu  self  get config       return dict list base config items      list config items         tf utils shape type conversion   def compute output shape self  input shape       return input shape 
class embed layer     string    def   init   self                 input dim                 output dim                 embeddings initializer=string                 embeddings regularizer=none                 activity regularizer=none                 embeddings constraint=none                 mask zero=false                 input length=none                   kwargs       if string not in kwargs        if input length          kwargs string  =  input length         else          kwargs string  =  none       dtype = kwargs pop string  k floatx                            kwargs string  = false     super embed  self    init   dtype=dtype    kwargs       self input dim = input dim     self output dim = output dim     self embeddings initializer = initializers get embeddings initializer      self embeddings regularizer = regularizers get embeddings regularizer      self activity regularizer = regularizers get activity regularizer      self embeddings constraint = constraints get embeddings constraint      self mask zero = mask zero     self support mask = mask zero     self input length = input length     self  support rag input = true     tf utils shape type conversion   def build self  input shape                                     if context execute eagerly   and context context   num gpus          with ops device string           self embeddings = self add weight              shape= self input dim  self output dim               initializer=self embeddings initializer              name=string              regularizer=self embeddings regularizer              constraint=self embeddings constraint      else        self embeddings = self add weight            shape= self input dim  self output dim             initializer=self embeddings initializer            name=string            regularizer=self embeddings regularizer            constraint=self embeddings constraint      self build = true    def compute mask self  input  mask=none       if not self mask zero        return none      return math ops not equal input  0      tf utils shape type conversion   def compute output shape self  input shape       if self input length be none        return input shape    self output dim       else               if isinstance self input length   list  tuple            in lens = list self input length        else          in lens =  self input length        if len in lens   = len input shape  - 1          raise valueerror string                          string    str                               self input length   str input shape          else          for i   s1  s2  in enumerate zip in lens  input shape 1                if s1 be not none and s2 be not none and s1  = s2              raise valueerror string                              string    str                                   self input length   str input shape              elif s1 be none              in lens i  = s2       return  input shape 0      tuple in lens     self output dim      def call self  input       dtype = k dtype input      if dtype  = string and dtype  = string        input = math ops cast input  string      out = embed ops embed lookup self embeddings  input      return out    def get config self       config =           string  self input dim          string  self output dim          string              initializers serialize self embeddings initializer           string              regularizers serialize self embeddings regularizer           string              regularizers serialize self activity regularizer           string              constraints serialize self embeddings constraint           string  self mask zero          string  self input length           base config = super embed  self  get config       return dict list base config items      list config items     
class flatten layer     string    def   init   self  data format=none    kwargs       super flatten  self    init     kwargs      self data format = conv utils normalize data format data format      self input spec = inputspec min ndim=1     def call self  input       if  self data format == string         and k ndim input  be not none and k ndim input  > 1         permutation =  0        permutation extend  i for i in                           range 2  k ndim input           permutation append 1        input = array ops transpose input  perm=permutation       input shape = input shape     if input shape 1   be fully define          flatten dim = tensor shape dimension value            np prod input shape 1    dtype=int                if flatten dim > np iinfo np int32  max          shape dtype = dtypes int64       else          shape dtype = dtypes int32       output = array ops reshape            input  constant op constant  -1  flatten dim   dtype=shape dtype       else        batch size = tensor shape dimension value input shape 0         if batch size                   if batch size > np iinfo np int32  max            shape dtype = dtypes int64         else            shape dtype = dtypes int32         output = array ops reshape              input  constant op constant  batch size  -1   dtype=shape dtype         else          output = array ops reshape input   array ops shape input  0   -1       if not context execute eagerly          output set shape self compute output shape input shape       return output    def compute output shape self  input shape       input shape = tensor shape as shape input shape  as list       if not input shape        output shape = tensor shape tensorshape  1       else        output shape =  input shape 0       if all input shape 1           output shape  =  np prod input shape 1    dtype=int       else        output shape  =  none      return tensor shape tensorshape output shape     def get config self       config =  string  self data format      base config = super flatten  self  get config       return dict list base config items      list config items     
class gru recurrent dropoutrnncellmixin  recurrent gru     string    def   init   self                 units                 activation=string                 recurrent activation=string                 use bias=true                 kernel initializer=string                 recurrent initializer=string                 bias initializer=string                 kernel regularizer=none                 recurrent regularizer=none                 bias regularizer=none                 activity regularizer=none                 kernel constraint=none                 recurrent constraint=none                 bias constraint=none                 dropout=0                  recurrent dropout=0                  implementation=2                 return sequences=false                 return state=false                 go backwards=false                 stateful=false                 unroll=false                 time major=false                 reset after=true                   kwargs                 self  return runtime = kwargs pop string  false       super gru  self    init            units          activation=activation          recurrent activation=recurrent activation          use bias=use bias          kernel initializer=kernel initializer          recurrent initializer=recurrent initializer          bias initializer=bias initializer          kernel regularizer=kernel regularizer          recurrent regularizer=recurrent regularizer          bias regularizer=bias regularizer          activity regularizer=activity regularizer          kernel constraint=kernel constraint          recurrent constraint=recurrent constraint          bias constraint=bias constraint          dropout=dropout          recurrent dropout=recurrent dropout          implementation=implementation          return sequences=return sequence          return state=return state          go backwards=go backwards          stateful=stateful          unroll=unroll          time major=time major          reset after=reset after            kwargs           self could use cudnn =           activation == string and recurrent activation == string and         recurrent dropout == 0 and not unroll and use bias and         reset after and ops execute eagerly outside function       def build self  input shape       super gru  self  build input shape       if not all isinstance v  resource variable ops resourcevariable                 for v in self weight                                                   self could use cudnn = false    def call self  input  mask=none  training=none  initial state=none                 input  row lengths = k convert input if rag input      be rag input =  row lengths be not none      self  validate args if rag be rag input  mask            input  initial state    = self  process input input  initial state  none       if isinstance mask  list         mask = mask 0       input shape = k int shape input      timesteps = input shape 0  if self time major else input shape 1       if not self could use cudnn        kwargs =  string  train        self  maybe reset cell dropout mask self cell         def step cell input  cell state           return self cell call cell input  cell state    kwargs         last output  output  state = k rnn            step            input            initial state            constants=none            go backwards=self go backwards            mask=mask            unroll=self unroll            input length=row lengths if row lengths be not none else timesteps            time major=self time major            zero output for mask=self zero output for mask               runtime =  runtime  runtime unknown      else        last output  output  runtime  state = self  defun gru call            input  initial state  train  mask  row lengths       if self stateful        update =  state ops assign self state 0   state 0          self add update update       if self return sequence        output = k maybe convert to rag be rag input  output  row lengths      else        output = last output      if self return state        return  output    list state      elif self  return runtime        return output  runtime     else        return output    def  defun gru call self  input  initial state  train  mask                        sequence lengths                       self reset dropout mask       dropout mask = self get dropout mask for cell input  train  count=3      if dropout mask be not none        input = input   dropout mask 0       cudnn gru kwargs =           string  input          string  initial state 0           string  self cell kernel          string  self cell recurrent kernel          string  self cell bias          string  mask          string  self time major          string  self go backwards          string  sequence lengths           normal gru kwargs = cudnn gru kwargs copy       normal gru kwargs update           string  self activation          string  self recurrent activation             if context execute eagerly          device type =  get context device type         can use gpu =                         device type ==  gpu device name            or  device type be none and context num gpus   > 0             and            mask be none or be sequence right pad mask  self time major                 if can use gpu          last output  output  new h  runtime = cudnn gru   cudnn gru kwargs        else          last output  output  new h  runtime = standard gru   normal gru kwargs      else        last output  output  new h  runtime = gru with backend selection              normal gru kwargs       state =  new h      return last output  output  runtime  state 
class grucell recurrent grucell     string    def   init   self                 units                 activation=string                 recurrent activation=string                 use bias=true                 kernel initializer=string                 recurrent initializer=string                 bias initializer=string                 kernel regularizer=none                 recurrent regularizer=none                 bias regularizer=none                 kernel constraint=none                 recurrent constraint=none                 bias constraint=none                 dropout=0                  recurrent dropout=0                  implementation=2                 reset after=true                   kwargs       super grucell  self    init            units          activation=activation          recurrent activation=recurrent activation          use bias=use bias          kernel initializer=kernel initializer          recurrent initializer=recurrent initializer          bias initializer=bias initializer          kernel regularizer=kernel regularizer          recurrent regularizer=recurrent regularizer          bias regularizer=bias regularizer          kernel constraint=kernel constraint          recurrent constraint=recurrent constraint          bias constraint=bias constraint          dropout=dropout          recurrent dropout=recurrent dropout          implementation=implementation          reset after=reset after            kwargs  
class gaussiandropout layer     string    def   init   self  rate    kwargs       super gaussiandropout  self    init     kwargs      self support mask = true     self rate = rate    def call self  input  training=none       if 0 < self rate < 1         def noise            stddev = np sqrt self rate /  1 0 - self rate           return input   k random normal              shape=array ops shape input               mean=1 0              stddev=stddev              dtype=inputs dtype         return k in train phase noise  input  training=training      return input    def get config self       config =  string  self rate      base config = super gaussiandropout  self  get config       return dict list base config items      list config items         tf utils shape type conversion   def compute output shape self  input shape       return input shape 
class gaussiannoise layer     string    def   init   self  stddev    kwargs       super gaussiannoise  self    init     kwargs      self support mask = true     self stddev = stddev    def call self  input  training=none        def noise          return input   k random normal            shape=array ops shape input             mean=0             stddev=self stddev            dtype=inputs dtype       return k in train phase noise  input  training=training     def get config self       config =  string  self stddev      base config = super gaussiannoise  self  get config       return dict list base config items      list config items         tf utils shape type conversion   def compute output shape self  input shape       return input shape 
class globalaveragepooling1d globalpooling1d     string    def   init   self  data format=string    kwargs       super globalaveragepooling1d  self    init   data format=data format                                                     kwargs      self support mask = true    def call self  input  mask=none       step axis = 1 if self data format == string else 2     if mask be not none        mask = math ops cast mask  backend floatx          mask = array ops expand dim            mask  2 if self data format == string else 1        input  = mask       return backend sum input  axis=steps axis  / math ops reduce sum            mask  axis=steps axis      else        return backend mean input  axis=steps axis     def compute mask self  input  mask=none       return none 
class globalaveragepooling2d globalpooling2d     string    def call self  input       if self data format == string        return backend mean input  axis= 1  2       else        return backend mean input  axis= 2  3   
class globalaveragepooling3d globalpooling3d     string    def call self  input       if self data format == string        return backend mean input  axis= 1  2  3       else        return backend mean input  axis= 2  3  4   
class globalmaxpooling1d globalpooling1d     string    def call self  input       step axis = 1 if self data format == string else 2     return backend max input  axis=steps axis  
class globalmaxpooling2d globalpooling2d     string    def call self  input       if self data format == string        return backend max input  axis= 1  2       else        return backend max input  axis= 2  3   
class inputlayer base layer layer     string    def   init   self                 input shape=none                 batch size=none                 dtype=none                 input tensor=none                 sparse=false                 name=none                 ragged=false                   kwargs       strategy = distribution strategy context get strategy       if strategy and batch size be not none and \         distribute train utils global batch size support strategy         if batch size   strategy num replicas in sync  = 0          raise valueerror string                          string format                               batch size  strategy num replicas in sync         batch size = batch size // strategy num replicas in sync      if string in kwargs        batch input shape = kwargs pop string        if input shape and batch input shape          raise valueerror string                          string                          string        batch size = batch input shape 0        input shape = batch input shape 1       if kwargs        raise valueerror string  kwargs key         if not name        prefix = string       name = prefix   string   str backend get uid prefix        if not dtype        if input tensor be none          dtype = backend floatx         else          dtype = backend dtype input tensor      elif input tensor be not none and input tensor dtype  = dtype        raise valueerror string                           input tensor dtype  dtype       super inputlayer  self    init   dtype=dtype  name=name      self build = true     self sparse = sparse     self rag = rag     self batch size = batch size     self support mask = true      if isinstance input shape  tensor shape tensorshape         input shape = tuple input shape as list        elif isinstance input shape  int         input shape =  input shape        if input tensor be none        if input shape be not none          batch input shape =  batch size     tuple input shape        else          batch input shape = none       graph = backend get graph         with graph as default            input tensor = backend placeholder              shape=batch input shape              dtype=dtype              name=self name              sparse=sparse              ragged=ragged         self be placeholder = true       self  batch input shape = batch input shape     else        if not tf utils be symbolic tensor input tensor           raise valueerror string                          string                          string                          string        self be placeholder = false       self  batch input shape = tuple input tensor shape as list                   input tensor  keras history = base layer kerashistory self  0  0      input tensor  keras mask = none     node module node          self          inbound layers=            node indices=            tensor indices=            input tensors= input tensor           output tensors= input tensor      def get config self       config =           string  self  batch input shape          string  self dtype          string  self sparse          string  self rag          string  self name           return config     property   def  trackable save model saver self       return layer serialization inputlayersavedmodelsaver self  
class inputspec object     string    def   init   self                 dtype=none                 shape=none                 ndim=none                 max ndim=none                 min ndim=none                 axes=none       self dtype = dtypes as dtype dtype  name if dtype be not none else none     if shape be not none        self ndim = len shape        self shape = shape     else        self ndim = ndim       self shape = none     self max ndim = max ndim     self min ndim = min ndim     try        ax = ax or          self ax =  int k   ax k  for k in ax      except  valueerror  typeerror         raise typeerror string       if self ax and  self ndim be not none or self max ndim be not none         max dim =  self ndim if self ndim else self max ndim  - 1       max axis = max self ax        if max axis > max dim          raise valueerror string                           format max axis  max dim      def   repr   self       spec =   string   str self dtype   if self dtype else string               string   str self shape   if self shape else string               string   str self ndim   if self ndim else string               string   str self max ndim   if self max ndim else string               string   str self min ndim   if self min ndim else string               string   str self ax   if self ax else string      return string   string join x for x in spec if x     def get config self       return           string  self dtype          string  self shape          string  self ndim          string  self max ndim          string  self min ndim          string  self ax      classmethod   def from config cls  config       return cls   config  
class lstm recurrent dropoutrnncellmixin  recurrent lstm     string    def   init   self                 units                 activation=string                 recurrent activation=string                 use bias=true                 kernel initializer=string                 recurrent initializer=string                 bias initializer=string                 unit forget bias=true                 kernel regularizer=none                 recurrent regularizer=none                 bias regularizer=none                 activity regularizer=none                 kernel constraint=none                 recurrent constraint=none                 bias constraint=none                 dropout=0                  recurrent dropout=0                  implementation=2                 return sequences=false                 return state=false                 go backwards=false                 stateful=false                 time major=false                 unroll=false                   kwargs                 self return runtime = kwargs pop string  false       super lstm  self    init            units          activation=activation          recurrent activation=recurrent activation          use bias=use bias          kernel initializer=kernel initializer          recurrent initializer=recurrent initializer          bias initializer=bias initializer          unit forget bias=unit forget bias          kernel regularizer=kernel regularizer          recurrent regularizer=recurrent regularizer          bias regularizer=bias regularizer          activity regularizer=activity regularizer          kernel constraint=kernel constraint          recurrent constraint=recurrent constraint          bias constraint=bias constraint          dropout=dropout          recurrent dropout=recurrent dropout          implementation=implementation          return sequences=return sequence          return state=return state          go backwards=go backwards          stateful=stateful          time major=time major          unroll=unroll            kwargs       self state spec =           inputspec shape= none  dim   for dim in  self units  self units            self could use cudnn =           activation == string and recurrent activation == string and         recurrent dropout == 0 and not unroll and use bias and         ops execute eagerly outside function       def call self  input  mask=none  training=none  initial state=none                 input  row lengths = k convert input if rag input      be rag input =  row lengths be not none      self  validate args if rag be rag input  mask            input  initial state    = self  process input input  initial state  none       if isinstance mask  list         mask = mask 0       input shape = k int shape input      timesteps = input shape 0  if self time major else input shape 1       if not self could use cudnn               kwargs =  string  train        self  maybe reset cell dropout mask self cell         def step input  state           return self cell call input  state    kwargs         last output  output  state = k rnn            step            input            initial state            constants=none            go backwards=self go backwards            mask=mask            unroll=self unroll            input length=row lengths if row lengths be not none else timesteps            time major=self time major            zero output for mask=self zero output for mask        runtime =  runtime  runtime unknown      else                                           self reset dropout mask         dropout mask = self get dropout mask for cell input  train  count=4        if dropout mask be not none          input = input   dropout mask 0        cudnn lstm kwargs =             string  input            string  initial state 0             string  initial state 1             string  self cell kernel            string  self cell recurrent kernel            string  self cell bias            string  mask            string  self time major            string  self go backwards            string  row lengths               normal lstm kwargs = cudnn lstm kwargs copy         normal lstm kwargs update             string  self activation            string  self recurrent activation                 if context execute eagerly            device type =  get context device type           can use gpu =                             device type ==  gpu device name              or  device type be none and context num gpus   > 0               and              mask be none or be sequence right pad mask  self time major                              if can use gpu            last output  output  new h  new c  runtime = cudnn lstm                  cudnn lstm kwargs          else            last output  output  new h  new c  runtime = standard lstm                  normal lstm kwargs        else           last output  output  new h  new c           runtime  = lstm with backend selection   normal lstm kwargs         state =  new h  new c       if self stateful        update =          for i in range len state            update append state ops assign self state i   state i          self add update update       if self return sequence        output = k maybe convert to rag be rag input  output  row lengths      else        output = last output      if self return state        return  output    list state      elif self return runtime        return output  runtime     else        return output 
class lstmcell recurrent lstmcell     string    def   init   self                 units                 activation=string                 recurrent activation=string                 use bias=true                 kernel initializer=string                 recurrent initializer=string                 bias initializer=string                 unit forget bias=true                 kernel regularizer=none                 recurrent regularizer=none                 bias regularizer=none                 kernel constraint=none                 recurrent constraint=none                 bias constraint=none                 dropout=0                  recurrent dropout=0                  implementation=2                   kwargs       super lstmcell  self    init            units          activation=activation          recurrent activation=recurrent activation          use bias=use bias          kernel initializer=kernel initializer          recurrent initializer=recurrent initializer          bias initializer=bias initializer          unit forget bias=unit forget bias          kernel regularizer=kernel regularizer          recurrent regularizer=recurrent regularizer          bias regularizer=bias regularizer          kernel constraint=kernel constraint          recurrent constraint=recurrent constraint          bias constraint=bias constraint          dropout=dropout          recurrent dropout=recurrent dropout          implementation=implementation            kwargs  
class lambda layer     string     trackable no automatic dependency track   def   init   self  function  output shape=none  mask=none  arguments=none                   kwargs       super lambda  self    init     kwargs       self arguments = arguments or        self function = function      if mask be not none        self support mask = true     self mask = mask     self  support rag input = true     self  output shape = output shape           self  already warn = false      function args = tf inspect getfullargspec function  args     self  fn expect train arg = string in function args     self  fn expect mask arg = string in function args     tf utils shape type conversion   def compute output shape self  input shape       if self  output shape be none                                    with context eager mode            try            return super lambda  self  compute output shape input shape          except notimplementederror            raise notimplementederror                string               string       if callable self  output shape         output shape = self  output shape input shape        return tf utils convert shape output shape  to tuples=false            input tensor shape = tf utils convert shape input shape  to tuples=false      batch size = nest flatten input tensor shape  0  0  if input shape else none      def  add batch shape         return tensor shape tensorshape  batch size    shape as list         output shape = tf utils convert shape self  output shape  to tuples=false      return nest map structure  add batch  output shape     def call self  input  mask=none  training=none            kwargs =  k  v for k  v in self arguments items        if self  fn expect mask arg        kwargs string  = mask     if self  fn expect train arg        kwargs string  = train      create variables =        def  variable creator next creator    kwargs         var = next creator   kwargs        create variables append var        return var      with backprop gradienttape watch access variables=true  as tape \         variable scope variable creator scope  variable creator         result = self function input    kwargs      self  check variables create variables  tape watch variables        return result    def  check variables self  create variables  access variables       if not create variables and not access variables                             return      track weight = set v experimental ref   for v in self weight      untracked new vars =  v for v in create variables                           if v experimental ref   not in track weight      if untracked new vars        variable str = string join  string format i  for i in untracked new vars         error str = textwrap dedent            string         format name=self name  variable str=variable str        raise valueerror error str       untracked use vars =  v for v in access variables                            if v experimental ref   not in track weight      if untracked use vars and not self  already warn        variable str = string join  string format i  for i in untracked use vars         self  warn textwrap dedent            string         format name=self name  variable str=variable str         self  already warn = true    def  warn self  msg                 return tf log warn msg     def compute mask self  input  mask=none       if callable self mask         return self mask input  mask      return self mask    def get config self       function config = self  serialize function to config self function      output shape config = self  serialize function to config self  output shape                                                               allow raw=true      config =           string  function config 0           string  function config 1           string  function config 2           string  output shape config 0           string  output shape config 1           string  output shape config 2             if self mask be not none        mask config = self  serialize function to config self mask        config update             string  mask config 0             string  mask config 1             string  mask config 2               config string  = self arguments      base config = super lambda  self  get config       return dict list base config items      list config items        def  serialize function to config self  input  allow raw=false       if isinstance input  python type lambdatype         output = generic utils func dump input        output type = string       module = input   module       elif callable input         output = input   name         output type = string       module = input   module       elif allow raw        output = input       output type = string       module = none     else        raise valueerror            string   type input        return output  output type  module     classmethod   def from config cls  config  custom objects=none       config = config copy       function = cls  parse function from config          config  custom object  string  string  string       output shape = cls  parse function from config          config  custom object  string  string          string      if string in config        mask = cls  parse function from config            config  custom object  string  string  string      else        mask = none      config string  = function     config string  = output shape     config string  = mask                if string in config        for key in config string           if isinstance config string  key   dict             arg dict = config string  key            if string in arg dict and arg dict string  == string                           config string  key  = np array arg dict string        return cls   config      classmethod   def  parse function from config        cls  config  custom object  func attr name  module attr name        func type attr name       globs = globals       module = config pop module attr name  none      if module in sys modules        globs update sys modules module    dict        elif module be not none               warn warn string                     string format module                        userwarning      if custom object        globs update custom object      function type = config pop func type attr name      if function type == string               function = generic utils deserialize keras object            config func attr name             custom objects=custom object            printable module name=string      elif function type == string               function = generic utils func load            config func attr name   globs=globs      elif function type == string        function = config func attr name      else        raise typeerror string  function type      return function 
class layer module module     string                          tf module ignore properties = frozenset itertools chain         string          module module  tf module ignore properties          trackable no automatic dependency track   def   init   self  trainable=true  name=none  dtype=none  dynamic=false                   kwargs                           allow kwargs =           string          string          string          string          string          string                generic utils validate kwargs kwargs  allow kwargs                      self  trainable = trainable               self  stateful = false               self build = false          self input spec = none     self support mask = false     self  support rag input = false      self  init set name name      self  activity regularizer = kwargs pop string  none      self  maybe create attribute string          self  maybe create attribute string          self  update =             self  thread local = thread local                 self  callable losses =                  self  losses =                  self  metrics =         self  set dtype policy dtype                self  autocast = kwargs get string                                  base layer utils v2 dtype behavior enable              self  maybe create attribute string                     self  inbound nod =        self  outbound nod =         self  init call fn args             self  dynamic = dynamic           if string in kwargs or string in kwargs                      if string in kwargs          batch input shape = tuple kwargs string         elif string in kwargs          if string in kwargs            batch size = kwargs string          else            batch size = none         batch input shape =  batch size     tuple kwargs string         self  batch input shape = batch input shape           if string in kwargs        self  initial weight = kwargs string      else        self  initial weight = none    def build self  input shape       string     self build = true     doc control for subclass implementers   def call self  input    kwargs         string     return input     doc control for subclass implementers   def  add trackable self  trackable object  trainable       string     if trainable        self  trainable weight append            base layer utils trackableweighthandler trackable object       else        self  non trainable weight append            base layer utils trackableweighthandler trackable object       return trackable object     doc control for subclass implementers   def add weight self                   name=none                   shape=none                   dtype=none                   initializer=none                   regularizer=none                   trainable=none                   constraint=none                   partitioner=none                   use resource=none                   synchronization=tf variables variablesynchronization auto                   aggregation=tf variables variableaggregation none                     kwargs       string     if shape be none        shape =             for kwarg in kwargs        if kwarg not in  string  string  string                         string           raise typeerror string  kwarg      getter = kwargs pop string  base layer utils make variable      collections arg = kwargs pop string  none                autocast = kwargs pop string  true           cache device = kwargs pop string  none       if dtype be none        dtype = self dtype or backend floatx       dtype = dtypes as dtype dtype      if self  dtype policy variable dtype be none               self  dtype policy = policy policy dtype base dtype name      initializer = initializers get initializer      regularizer = regularizers get regularizer      constraint = constraints get constraint       if synchronization == tf variables variablesynchronization on read        if trainable          raise valueerror              string             string             string             string        else                   trainable = false     elif trainable be none        trainable = true           if initializer be none               if dtype be float          initializer = initializers glorot uniform                       elif dtype be integer or dtype be unsigned or dtype be bool          initializer = initializers zero                else          raise valueerror string                          string    name  dtype base dtype  self name        if  autocast and self  dtype policy should cast variables and         dtype be float                old getter = getter       def getter  args    kwargs             variable = old getter  args    kwargs          return autocast variable create autocast variable variable                             if cache device be not none          tf log warn string                         string          cache device = none      variable = self  add variable with custom getter          name=name          shape=shape                            getter=getter                   overwrite=true          initializer=initializer          dtype=dtype          constraint=constraint          trainable=trainable          partitioner=partitioner          use resource=use resource          collections=collections arg          synchronization=synchronization          aggregation=aggregation          cache device=caching device      backend track variable variable       if regularizer be not none                             name in scope = variable name  variable name find string         self  handle weight regularization name in scope                                           variable                                           regularizer      if trainable        self  trainable weight append variable      else        self  non trainable weight append variable      return variable     base layer utils default   def get config self       string     all args = tf inspect getfullargspec self   init    args     config =  string  self name  string  self trainable      if hasattr self  string         config string  = self  batch input shape               if hasattr self  string         config string  = policy serialize self  dtype policy      if hasattr self  string                if self dynamic          config string  = self dynamic       elif string in all args          all args remove string      expect args = config key            extra args =  arg for arg in all args if arg not in expect args                if len extra args  > 1 and hasattr self get config  string         raise notimplementederror string                                 string      return config     classmethod   def from config cls  config       string     return cls   config     def compute output shape self  input shape       string     if context execute eagerly                                                           self  maybe build input shape        with context graph mode            graph = func graph funcgraph string          with graph as default              input shape = tf utils convert shape input shape  to tuples=false            input = nest map structure                base layer utils generate placeholders from shape  input shape            try              if self  expect train arg                output = self input  training=false              else                output = self input            except typeerror              raise notimplementederror string                                       string                                       string                                       string                                       string   self   class     name          return nest map structure lambda t  t shape  output      raise notimplementederror     doc control for subclass implementers   def compute output signature self  input signature       string     def check type return shape s         if not isinstance s  tensor spec tensorspec           raise typeerror              string             string format s         return s shape     input shape = nest map structure check type return shape  input signature      output shape = self compute output shape input shape      dtype = self  compute dtype     if dtype be none        input dtypes =  s dtype for s in nest flatten input signature                       dtype = input dtypes 0      return nest map structure          lambda s  tensor spec tensorspec dtype=dtype  shape=s           output shape      base layer utils default   def compute mask self  input  mask=none         string     if not self support mask        if any m be not none for m in nest flatten mask            raise typeerror string   self name   string                         string   str mask                return none               return mask    def   call   self  input   args    kwargs       string     call context = base layer utils call context       input list = nest flatten input                           build graph = tf utils be all symbolic tensors input list            if any isinstance x   np ndarray  float  int   for x in input list         def  convert non tensor x                             if isinstance x   np ndarray  float  int              return ops convert to tensor x          return x       input = nest map structure  convert non tensor  input        input list = nest flatten input                           mask arg pass by framework = false     input mask = self  collect input mask input  args  kwargs      if  self  expect mask arg and input mask be not none and         not self  call arg be pass string  args  kwargs          mask arg pass by framework = true       kwargs string  = input mask                train arg pass by framework = false          if self  call arg be pass string  args  kwargs         train value = self  get call arg value string  args  kwargs        if not self  expect train arg          kwargs pop string      else        train value = none              if call context train be not none          train value = call context train              elif backend global learn phase be set            train value = backend learn phase                elif build graph          with backend get graph   as default              if base layer utils be in keras graph                train value = backend learn phase          if self  expect train arg and train value be not none                            if tensor util be tensor train value             train value = math ops cast train value  dtypes bool          else            train value = bool train value          kwargs string  = train value         train arg pass by framework = true                     if build graph and base layer utils need keras history input         base layer utils create keras history input                      if  base layer utils be in eager or tf function   and         not call context in call         self  clear losses        with call context enter self  input  build graph  train value                if build graph                                              input spec assert input compatibility self input spec  input                                                self name          if  any isinstance x  rag tensor raggedtensor  for x in input list              and self  support rag input be false               raise valueerror string                            string                            string    self name  input            graph = backend get graph           with graph as default    backend name scope self  name scope                                     self  maybe build input            cast input = self  maybe cast input input                                                                               if  base layer utils be subclassed self  and               not base layer utils from save model self                call fn = autograph tf convert                  self call  ag ctx control status ctx              else              call fn = self call            if not self dynamic              try                with base layer utils autocast context manager                    self  compute dtype                                                     if  ops execute eagerly outside function   and                     not base layer utils be in eager or tf function                       with auto control deps automaticcontroldependencies   as acd                      output = call fn cast input   args    kwargs                                                                output = base layer utils mark as return output  acd                  else                    output = call fn cast input   args    kwargs               except errors operatornotallowedingrapherror as e                raise typeerror string                               string                               string                               string                                 str e    string            else                                                                                            output = self  symbolic call input             if output be none              raise valueerror string                              string                              string   self name   string            if base layer utils have all keras metadata input               if train arg pass by framework                kwargs pop string              if mask arg pass by framework                kwargs pop string              input  output = self  set connectivity metadata                   input  output  args  kwargs            self  handle activity regularization input  output            self  set mask metadata input  output  input mask            if hasattr self  string  and not self input                                                                               self  set input input  output        else                   with backend name scope self  name scope               self  maybe build input            cast input = self  maybe cast input input            with base layer utils autocast context manager                self  compute dtype               output = self call cast input   args    kwargs            self  handle activity regularization input  output            self  set mask metadata input  output  input mask       return output     property   def dtype self       return self  dtype policy variable dtype     property   def name self       return self  name     property    trackable layer utils cache recursive attribute string    def dynamic self                 return self  dynamic     property    doc control do not generate docs    trackable layer utils cache recursive attribute string    def stateful self       return self  stateful     stateful setter    trackable layer utils invalidate recursive cache string    def stateful self  value       self  stateful = value     property   def trainable self       return self  trainable     trainable setter   def trainable self  value       self  trainable = value     for layer in getattr self  string             layer trainable = value     property   def activity regularizer self       string     return self  activity regularizer     activity regularizer setter   def activity regularizer self  regularizer       string     self  activity regularizer = regularizer     property   def input spec self       return self  input spec     input spec setter          trackable no automatic dependency track   def input spec self  value       for v in nest flatten value         if v be not none and not isinstance v  inputspec           raise typeerror string                         string format v       self  input spec = value     property   def trainable weight self       if self trainable        nest = self  gather children attribute string        return self  dedup weight self  trainable weight   nest      else        return        property   def non trainable weight self       if self trainable        nest = self  gather children attribute string        non trainable weight = self  non trainable weight   nest     else        nest = self  gather children attribute string        non trainable weight =             self  trainable weight   self  non trainable weight   nest      return self  dedup weight non trainable weight      property   def weight self       string     return self trainable weight   self non trainable weight     property   def update self       if not self trainable and not self stateful        return        with backend get graph   as default          update =          for u in self  update          if callable u             try              u = u             except errors inaccessibletensorerror              base layer utils check graph consistency                  method=string  force raise=true              raise           base layer utils check graph consistency u  method=string          update append u      return update   self  gather children attribute string      property   def losses self       string     collect losses =                             if self  eager losses        collect losses extend self  eager losses      else        collect losses extend self  losses      for regularizer in self  callable losses        loss tensor = regularizer         if loss tensor be not none          collect losses append loss tensor      return collect losses   self  gather children attribute string      doc control for subclass implementers   def add loss self  losses  inputs=none       string     def  tag unconditional loss         string       if callable loss                             with base layer utils autocast context manager none             loss = loss         if loss be none          return none         if not tensor util be tensor loss           loss = ops convert to tensor loss  dtype=backend floatx          loss  unconditional loss =  input be none          return loss      losses = nest flatten losses       callable losses =        eager losses =        symbolic losses =        for loss in losses        if callable loss           callable losses append functools partial  tag unconditional  loss           continue       if loss be none          continue       if not tensor util be tensor loss           loss = ops convert to tensor loss  dtype=backend floatx                 if  tf utils be symbolic tensor loss  and           not base layer utils be in tf function             symbolic losses append  tag unconditional loss           base layer utils check graph consistency loss  method=string        elif tensor util be tensor loss           eager losses append  tag unconditional loss        self  callable losses extend callable losses       in call context = base layer utils call context   in call     if eager losses and not in call context        raise valueerror            string           string       self  eager losses extend eager losses       if in call context        for symbolic loss in symbolic losses          self  losses append symbolic loss      else        for symbolic loss in symbolic losses          if getattr self  string  false             self  graph network add loss symbolic loss          else                       self  losses append symbolic loss      trackable no automatic dependency track   def  clear losses self       string     self  eager losses =        if hasattr self  string         for layer in trackable layer utils filter empty layer containers            self  layer           layer  clear losses       property   def metrics self       return self  metrics   self  gather children attribute string      doc control for subclass implementers   def add metric self  value  aggregation=none  name=none       string     if aggregation be not none and aggregation  = string        raise valueerror            string           string   aggregation       from metric obj = hasattr value  string      be symbolic = tf utils be symbolic tensor value      in call context = base layer utils call context   in call      if name be none and not from metric obj                                                                               raise valueerror string                        string                        string      elif from metric obj        name = value  metric obj name      if in call context               if be symbolic and not base layer utils be in tf function            self  symbolic add metric value  aggregation  name        else          self  eager add metric value  aggregation  name      else        if not be symbolic          raise valueerror string                          string   str value                 if not getattr self  string  false           with backend get graph   as default              self  symbolic add metric value  aggregation  name          return        if from metric obj          raise valueerror string                          string                          string                          string                self  graph network add metric value  aggregation  name      deprecation deprecate args none  string                                 string     doc control for subclass implementers   def add update self  update  inputs=none       string     call context = base layer utils call context        if  ds context have strategy   and         ds context in cross replica context   and                           not call context save                if  ops execute eagerly outside function   and           not base layer utils be in keras graph             raise runtimeerror                string             string        return      update = generic utils to list update            if base layer utils be in eager or tf function          if not call context freeze          for update in update            if callable update               update         return      if call context in call        relevant input = call context input     else        inbound nod = getattr self  string            relevant input =  node input tensors for node in inbound nod       def process update x         string       if callable x           update = lambda  process update x            if not ops execute eagerly outside function                                    return update         elif isinstance x  ops operation           update = x       elif hasattr x  string           update = x op       else          update = ops convert to tensor x         reachable = tf utils get reachable from input relevant input   update         update  unconditional update = update not in reachable       return update      update =  process update x  for x in update                if ops execute eagerly outside function   and call context in call        update =  u for u in update if callable u       self  update extend update     def set weight self  weight       string     params = self weight      expect num weight = 0     for param in params        if isinstance param  base layer utils trackableweighthandler           expect num weight  = param num tensors       else          expect num weight  = 1      if expect num weight  = len weight         raise valueerror            string           string           string              self name  len weight   expect num weight  str weight   50         weight index = 0     weight value tuples =        for param in params        if isinstance param  base layer utils trackableweighthandler           num tensors = param num tensors         tensors = weight weight index weight index   num tensors          param set weight tensors          weight index  = num tensors       else          weight = weight weight index          ref shape = param shape         if not ref shape be compatible with weight shape             raise valueerror                string               string    ref shape  weight shape           weight value tuples append  param  weight           weight index  = 1      backend batch set value weight value tuples     def get weight self       string     weight = self weight     output weight =        for weight in weight        if isinstance weight  base layer utils trackableweighthandler           output weight extend weight get tensors          else          output weight append weight      return backend batch get value output weight     def get update for self  input       string     if input be none               return  u for u in self update if u  unconditional update            update =  u for u in self update if not u  unconditional update      input = nest flatten input      reachable = tf utils get reachable from input input  update      return  u for u in update if u in reachable     def get losses for self  input       string     if input be none               return  l for l in self losses if l  unconditional loss            losses =  l for l in self losses if not l  unconditional loss      input = nest flatten input      reachable = tf utils get reachable from input input  losses      return  l for l in losses if l in reachable     def get input mask at self  node index       string     input = self get input at node index      if isinstance input  list         return  getattr x  string  none  for x in input      else        return getattr input  string  none     def get output mask at self  node index       string     output = self get output at node index      if isinstance output  list         return  getattr x  string  none  for x in output      else        return getattr output  string  none      property   def input mask self       string     input = self input     if isinstance input  list         return  getattr x  string  none  for x in input      else        return getattr input  string  none      property   def output mask self       string     output = self output     if isinstance output  list         return  getattr x  string  none  for x in output      else        return getattr output  string  none     def get input shape at self  node index       string     return self  get node attribute at index node index  string                                               string     def get output shape at self  node index       string     return self  get node attribute at index node index  string                                               string     def get input at self  node index       string     return self  get node attribute at index node index  string                                               string     def get output at self  node index       string     return self  get node attribute at index node index  string                                               string      property   def input self       string     if not self  inbound nod        raise attributeerror string   self name                              string      return self  get node attribute at index 0  string  string      property   def output self       string     if not self  inbound nod        raise attributeerror string   self name   string      return self  get node attribute at index 0  string  string      property   def input shape self       string     if not self  inbound nod        raise attributeerror string                            string      all input shape = set           str node input shape  for node in self  inbound nod       if len all input shape  == 1        return self  inbound nod 0  input shape     else        raise attributeerror string   str self name                               string                            string                            string                            string                            string                            string     def count params self       string     if not self build        if getattr self  string  false           with tf utils maybe init scope self             self  maybe build self input        else          raise valueerror string   self name                            string                          string   self name                            string      return int sum np prod w shape as list    for w in self weight       property   def output shape self       string     if not self  inbound nod        raise attributeerror string                            string      all output shape = set           str node output shape  for node in self  inbound nod       if len all output shape  == 1        return self  inbound nod 0  output shape     else        raise attributeerror string                            string                            string                            string                            string                            string                            string   self name      property    doc control do not doc inheritable   def inbound nod self       string     return self  inbound nod     property    doc control do not doc inheritable   def outbound nod self       string     return self  outbound nod               deprecation deprecate        date=none  instructions=string     doc control do not doc inheritable   def apply self  input   args    kwargs       string     return self   call   input   args    kwargs      deprecation deprecate        date=none  instructions=string     doc control do not doc inheritable   def add variable self   args    kwargs       string     return self add weight  args    kwargs      property   def variables self       string     return self weight     property   def trainable variables self       return self trainable weight     property   def non trainable variables self       return self non trainable weight              def  set dtype policy self  dtype       string     if isinstance dtype  policy policy         self  dtype policy = dtype     elif isinstance dtype  dict         self  dtype policy = policy deserialize dtype      elif dtype        self  dtype policy = policy policy dtypes as dtype dtype  name      else        self  dtype policy = policy global policy                  self  dtype default to floatx =  not dtype and                                        policy policy default to floatx           property   def  compute dtype self       string     return self  dtype policy compute dtype    def  maybe cast input self  input       string     compute dtype = self  compute dtype     if  self  autocast and compute dtype and         dtypes as dtype compute dtype  be float         def f x           string         cast type =  ops tensor  sparse tensor sparsetensor                        rag tensor raggedtensor          if  isinstance x  cast type  and x dtype be float and             x dtype base dtype name  = compute dtype             if self  dtype default to floatx              self  warn about input cast x dtype base dtype            return math ops cast x  compute dtype          elif isinstance x  tensor spec tensorspec  and x dtype be float                                  return tensor spec tensorspec x shape  compute dtype  x name          else            return x       return nest map structure f  input      else        return input    def  warn about input cast self  input dtype                 already warn = getattr self  string  false      if not already warn        tf log warn            string           string           string           string           string           string           string           string           string           string           string           string           string           string format                self=self                input dtype=input dtype name                layer dtype=self  compute dtype         self  already warn about input cast = true              property   def  dtype self                      return self  dtype policy variable dtype      dtype setter   def  dtype self  value       value = dtypes as dtype value  name     self  dtype policy = policy policy value     def  name scope self       return self name    def  init set name self  name  zero based=true       if not name        self  name = backend unique object name            generic utils to snake case self   class     name               zero based=zero base      else        self  name = name    def  get exist metric self  name=none       match =  m for m in self  metrics if m name == name      if not match        return     if len match  > 1        raise valueerror            string           string format len match   name       return match 0     def  eager add metric self  value  aggregation=none  name=none                      metric obj = getattr value  string  none      if metric obj        name = metric obj name      match = self  get exist metric name      if match               if not metric obj          match value        return      if not metric obj        assert aggregation be not none       metric obj    = base layer utils create mean metric value  name      self  metrics append metric obj     def  symbolic add metric self  value  aggregation=none  name=none       base layer utils check graph consistency value  method=string      match = self  get exist metric name      if aggregation be none                                    if match          result tensor = value         metric obj = match       elif hasattr value  string                    result tensor = value         metric obj = result tensor  metric obj         self  metrics append metric obj        else          raise valueerror              string             string             string             string             string             string             string             string      else                      if match          result tensor = match value          metric obj = match       else          metric obj  result tensor = base layer utils create mean metric              value  name          self  metrics append metric obj     def  handle weight regularization self  name  variable  regularizer       string      def  loss for variable v         string       with backend name scope name   string           regularization = regularizer v        return regularization      if isinstance variable  tf variables partitionedvariable         for v in variable          self add loss functools partial  loss for variable  v       else        self add loss functools partial  loss for variable  variable      def  handle activity regularization self  input  output                      if self  activity regularizer        output list = nest flatten output        with backend name scope string           for output in output list            activity loss = self  activity regularizer output            batch size = math ops cast                array ops shape output  0   activity loss dtype                       mean activity loss = activity loss / batch size           base layer utils check graph consistency                mean activity loss  method=string            self add loss mean activity loss  inputs=inputs     def  set mask metadata self  input  output  previous mask       flat output = nest flatten output       mask already compute =           getattr self  string  false  or         all getattr x  string  none  be not none for x in flat output                  should compute mask =           hasattr self  string  and          self support mask or          not getattr self compute mask  string  false         if mask already compute        flat mask =  getattr x  string  none  for x in flat output      elif not should compute mask        flat mask =  none for   in flat output      else        output mask = self compute mask input  previous mask                      if output mask be none          flat mask =  none for   in flat output        else          flat mask = nest flatten output mask       for output  mask in zip flat output  flat mask         try          output  keras mask = mask       except attributeerror                   pass      if tf utils be all symbolic tensors flat output         for output in flat output          if getattr output  string  none  be not none                       output  keras mask  keras history check = true    def  collect input mask self  input  args  kwargs       string     if self  call arg be pass string  args  kwargs         return self  get call arg value string  args  kwargs       if not self  should compute mask        return none      input mask = nest map structure lambda t  getattr t  string  none                                        input      if generic utils be all none input mask         return none     return input mask    def  call arg be pass self  arg name  args  kwargs  input in args=false       if arg name in kwargs        return true     call fn args = self  call fn args     if not input in args               call fn args = call fn args 1       if arg name in dict zip call fn args  args          return true     return false    def  get call arg value self  arg name  args  kwargs  input in args=false       if arg name in kwargs        return kwargs arg name      call fn args = self  call fn args     if not input in args               call fn args = call fn args 1       args dict = dict zip call fn args  args       return args dict arg name     def  set connectivity metadata  self  input  output  args  kwargs                  output ls = nest flatten output      input ls = object identity objectidentityset nest flatten input       output ls copy =        for x in output ls        if x in input ls          with backend name scope self name             x = array ops identity x        output ls copy append x      output = nest pack sequence as output  output ls copy            arguments = dict zip self  call fn args 1    args       arguments update kwargs                 self  add inbound node          input tensors=inputs  output tensors=outputs  arguments=arguments      return input  output    def  add inbound node self                          input tensors                          output tensors                          arguments=none       string     inbound layer = nest map structure lambda t  t  keras history layer                                          input tensors      node indices = nest map structure lambda t  t  keras history node index                                        input tensors      tensor indices = nest map structure lambda t  t  keras history tensor index                                          input tensors            node module node          self          inbound layers=inbound layer          node indices=node indices          tensor indices=tensor indices          input tensors=input tensors          output tensors=output tensors          arguments=arguments                                               for i  tensor in enumerate nest flatten output tensors          tensor  keras history = kerashistory self                                             len self  inbound nod  - 1  i       def  get node attribute at index self  node index  attr  attr name       string     if not self  inbound nod        raise runtimeerror string                          string   attr name   string      if not len self  inbound nod  > node index        raise valueerror string   attr name   string                          str node index    string                          str len self  inbound nod     string      value = getattr self  inbound nod node index   attr      if isinstance value  list  and len value  == 1        return value 0      else        return value    def  maybe build self  input            if not self build        input spec assert input compatibility            self input spec  input  self name        input list = nest flatten input        if input list and self  dtype policy compute dtype be none          try            dtype = input list 0  dtype base dtype name         except attributeerror            pass         else            self  dtype policy = policy policy dtype        input shape = none       if all hasattr x  string  for x in input list           input shape = nest map structure lambda x  x shape  input               if not hasattr self build  string                                      with tf utils maybe init scope self             self build input shape                      self build = true           if getattr self  string  none  be not none        self set weight self  initial weight        self  initial weight = none    def  symbolic call self  input       input shape = nest map structure lambda x  x shape  input      output shape = self compute output shape input shape       def  make placeholder like shape         ph = backend placeholder shape=shape  dtype=self dtype        ph  keras mask = none       return ph      return nest map structure  make placeholder like  output shape     def  get trainable state self       string     layer = trackable layer utils filter empty layer containers self  layer                trainable state =  self  self trainable      for layer in layer        trainable state update layer  get trainable state        return trainable state    def  set trainable state self  trainable state       string     layer = trackable layer utils filter empty layer containers self  layer      if self in trainable state        self trainable = trainable state self      for layer in layer        layer  set trainable state trainable state      property   def  obj reference count self       string     self  maybe create attribute string                                   object identity objectidentitydictionary        return self  obj reference count dict     trackable no automatic dependency track   def  maybe create attribute self  name  default value       string     if not hasattr self  name         super layer  self    setattr   name  default value     def   delattr   self  name                                exist value = getattr self  name  none                      reference count = self  obj reference count     if exist value not in reference count        super track autotrackable  self    delattr   name        return      reference count = reference count exist value      if reference count > 1                      reference count exist value  = reference count - 1       super track autotrackable  self    delattr   name        return     else               del reference count exist value       super track autotrackable  self    delattr   name       if  isinstance exist value  layer          or trackable layer utils have weight exist value          super track autotrackable  self    setattr              string             l for l in self  layer if l be not exist value         self  attribute sentinel invalidate all       if isinstance exist value  tf variables variable         super track autotrackable  self    setattr              string             w for w in self  trainable weight if w be not exist value         super track autotrackable  self    setattr              string             w for w in self  non trainable weight if w be not exist value                            if name == string        self  attribute sentinel invalidate all      def   setattr   self  name  value       if  name == string or         not getattr self  string  true  or                  hasattr self   class    name          try          super track autotrackable  self    setattr   name  value        except attributeerror          raise attributeerror               string              string              string  format name         return           value = data structure sticky attribute assignment          trackable=self  value=value  name=name       reference count = self  obj reference count     reference count value  = reference count get value  0    1                try        self   delattr   name      except attributeerror        pass                                if  self   class     name    = string and          isinstance value  layer  or trackable layer utils have weight value           self  maybe create attribute string                          if not any  layer be value for layer in self  layer            self  layer append value          if hasattr value  string             value  attribute sentinel add parent self  attribute sentinel          if hasattr value  string                                   value  use resource variables = true                     for val in nest flatten value                       if not isinstance val  tf variables variable           continue       if isinstance val  resource variable ops  unreadvariable             continue                      self  maybe create attribute string            self  maybe create attribute string            if val trainable          if any val be w for w in self  trainable weight             continue         self  trainable weight append val        else          if any val be w for w in self  non trainable weight             continue         self  non trainable weight append val         backend track variable val                 super track autotrackable  self    setattr   name  value     def  gather children attribute self  attribute       assert attribute in           string  string  string  string          string  string           if hasattr self  string         nest layer = trackable layer utils filter empty layer containers            self  layer        return list            itertools chain from iterable                getattr layer  attribute  for layer in nest layer       return        property    track cache per instance   def  attribute sentinel self       return trackable layer utils attributesentinel               def  be layer self       return true    def  init call fn args self            self   class    call full argspec fget cache pop self  none      self   class    call fn args fget cache pop self  none      self   class    call accept kwargs fget cache pop self  none       call fn args = self  call fn args     self  expect train arg =  string in call fn args or                                   self  call accept kwargs      self  expect mask arg =  string in call fn args or                               self  call accept kwargs      property    track cache per instance   def  call full argspec self                 return tf inspect getfullargspec self call      property    track cache per instance   def  call fn args self       all args = self  call full argspec args          if all args and all args 0  == string        return all args 1       return all args     property    track cache per instance   def  call accept kwargs self       return self  call full argspec varkw be not none     property    track cache per instance   def  should compute mask self       return  string in self  call fn args or             getattr self  string  none  be not none      property   def  eager losses self                                     if not hasattr self  thread local  string         self  thread local  eager losses =        return self  thread local  eager losses      eager losses setter   def  eager losses self  losses       self  thread local  eager losses = losses    def  dedup weight self  weight       string     output  see weight =     object identity objectidentityset       for w in weight        if w not in see weight          output append w                   see weight add w      return output         property   def  trackable save model saver self       return layer serialization layersavedmodelsaver self      property   def  object identifier self       return self  trackable save model saver object identifier     property   def  track metadata self       return self  trackable save model saver track metadata    def  list extra dependencies for serialization self  serialization cache       return  self  trackable save model saver              list extra dependencies for serialization serialization cache      def  list function for serialization self  serialization cache       return  self  trackable save model saver              list function for serialization serialization cache      def   getstate   self                           state = self   dict   copy       state pop string  none      return state    def   setstate   self  state       state string  = thread local            object   setattr   self  string  state  
class layernormalization layer     string    def   init   self                 axis=-1                 epsilon=1e-3                 center=true                 scale=true                 beta initializer=string                 gamma initializer=string                 beta regularizer=none                 gamma regularizer=none                 beta constraint=none                 gamma constraint=none                 trainable=true                 name=none                   kwargs       super layernormalization  self    init            name=name  trainable=trainable    kwargs      if isinstance axis   list  tuple          self axis = axis        elif isinstance axis  int         self axis = axis     else        raise valueerror string                        string   axis       self epsilon = epsilon     self center = center     self scale = scale     self beta initializer = initializers get beta initializer      self gamma initializer = initializers get gamma initializer      self beta regularizer = regularizers get beta regularizer      self gamma regularizer = regularizers get gamma regularizer      self beta constraint = constraints get beta constraint      self gamma constraint = constraints get gamma constraint       self support mask = true                self  fuse = none    def  fuse can be use self  ndims       string     axis = sort self axis      can use fuse = false      if axis -1  == ndims - 1 and axis -1  - axis 0  == len axis  - 1        can use fuse = true                          if self epsilon < 1 001e-5 or self dtype  = string        can use fuse = false      return can use fuse    def build self  input shape       ndims = len input shape      if ndims be none        raise valueerror string   input shape            if isinstance self axis  int         self axis =  self axis      elif isinstance self axis  tuple         self axis = list self axis      for idx  x in enumerate self axis         if x < 0          self axis idx  = ndims   x           for x in self axis        if x < 0 or x >= ndims          raise valueerror string   x      if len self axis   = len set self axis          raise valueerror string format tuple self axis         param shape =  input shape dim  for dim in self axis      if self scale        self gamma = self add weight            name=string            shape=param shape            initializer=self gamma initializer            regularizer=self gamma regularizer            constraint=self gamma constraint            trainable=true            experimental autocast=false      else        self gamma = none      if self center        self beta = self add weight            name=string            shape=param shape            initializer=self beta initializer            regularizer=self beta regularizer            constraint=self beta constraint            trainable=true            experimental autocast=false      else        self beta = none      self  fuse = self  fuse can be use ndims       self build = true    def call self  input            input shape = input shape     ndims = len input shape                 broadcast shape =  1    ndims     for dim in self axis        broadcast shape dim  = input shape dim dim  value     def  broadcast v         if  v be not none and len v shape   = ndims and           self axis  =  ndims - 1            return array ops reshape v  broadcast shape        return v      if not self  fuse               mean  variance = nn moments input  self axis  keep dims=true         scale  offset =  broadcast self gamma    broadcast self beta                output = nn batch normalization            input            mean            variance            offset=offset            scale=scale            variance epsilon=self epsilon      else               pre dim  in dim =  1  1        axis = sort self axis        tensor shape = array ops shape input        for dim in range 0  ndims           dim tensor = tensor shape dim          if dim < axis 0             pre dim = pre dim   dim tensor         else            assert dim in axis           in dim = in dim   dim tensor        squeeze shape =  1  pre dim  in dim  1               data format = string        input = array ops reshape input  squeeze shape         def  set const tensor val  dtype  shape           return array ops fill shape  constant op constant val  dtype=dtype                                      scale =  set const tensor 1 0  input dtype   pre dim         offset =  set const tensor 0 0  input dtype   pre dim                 output       = nn fuse batch norm            input            scale=scale            offset=offset            epsilon=self epsilon            data format=data format         output = array ops reshape output  tensor shape         scale  offset =  broadcast self gamma    broadcast self beta         if scale be not none          output = output   scale       if offset be not none          output = output   offset           output set shape input shape       return output    def compute output shape self  input shape       return input shape    def get config self       config =           string  self axis          string  self epsilon          string  self center          string  self scale          string  initializers serialize self beta initializer           string  initializers serialize self gamma initializer           string  regularizers serialize self beta regularizer           string  regularizers serialize self gamma regularizer           string  constraints serialize self beta constraint           string  constraints serialize self gamma constraint            base config = super layernormalization  self  get config       return dict list base config items      list config items     
class leakyrelu layer     string    def   init   self  alpha=0 3    kwargs       super leakyrelu  self    init     kwargs      self support mask = true     self alpha = k cast to floatx alpha     def call self  input       return k relu input  alpha=self alpha     def get config self       config =  string  float self alpha       base config = super leakyrelu  self  get config       return dict list base config items      list config items         tf utils shape type conversion   def compute output shape self  input shape       return input shape 
class locallyconnected1d layer     string    def   init   self                 filter                 kernel size                 strides=1                 padding=string                 data format=none                 activation=none                 use bias=true                 kernel initializer=string                 bias initializer=string                 kernel regularizer=none                 bias regularizer=none                 activity regularizer=none                 kernel constraint=none                 bias constraint=none                 implementation=1                   kwargs       super locallyconnected1d  self    init     kwargs      self filter = filter     self kernel size = conv utils normalize tuple kernel size  1  string      self stride = conv utils normalize tuple stride  1  string      self pad = conv utils normalize pad pad      if self pad  = string and implementation == 1        raise valueerror string                        string                          pad      self data format = conv utils normalize data format data format      self activation = activations get activation      self use bias = use bias     self kernel initializer = initializers get kernel initializer      self bias initializer = initializers get bias initializer      self kernel regularizer = regularizers get kernel regularizer      self bias regularizer = regularizers get bias regularizer      self activity regularizer = regularizers get activity regularizer      self kernel constraint = constraints get kernel constraint      self bias constraint = constraints get bias constraint      self implementation = implementation     self input spec = inputspec ndim=3      tf utils shape type conversion   def build self  input shape       if self data format == string        input dim  input length = input shape 1   input shape 2      else        input dim  input length = input shape 2   input shape 1       if input dim be none        raise valueerror string                        string  input shape      self output length = conv utils conv output length          input length  self kernel size 0   self pad  self stride 0        if self implementation == 1        self kernel shape =  self output length  self kernel size 0    input dim                             self filter         self kernel = self add weight            shape=self kernel shape            initializer=self kernel initializer            name=string            regularizer=self kernel regularizer            constraint=self kernel constraint       elif self implementation == 2        if self data format == string          self kernel shape =  input dim  input length                               self filter  self output length        else          self kernel shape =  input length  input dim                               self output length  self filter         self kernel = self add weight shape=self kernel shape                                      initializer=self kernel initializer                                      name=string                                      regularizer=self kernel regularizer                                      constraint=self kernel constraint         self kernel mask = get locallyconnected mask            input shape= input length              kernel shape=self kernel size            strides=self stride            padding=self pad            data format=self data format               elif self implementation == 3        self kernel shape =  self output length   self filter                             input length   input dim         self kernel idxs = sort            conv utils conv kernel idxs                input shape= input length                  kernel shape=self kernel size                strides=self stride                padding=self pad                filter in=input dim                filter out=self filter                data format=self data format                 self kernel = self add weight            shape= len self kernel idxs               initializer=self kernel initializer            name=string            regularizer=self kernel regularizer            constraint=self kernel constraint       else        raise valueerror string                          self implementation       if self use bias        self bias = self add weight            shape= self output length  self filter             initializer=self bias initializer            name=string            regularizer=self bias regularizer            constraint=self bias constraint      else        self bias = none      if self data format == string        self input spec = inputspec ndim=3  axes= 1  input dim       else        self input spec = inputspec ndim=3  axes= -1  input dim       self build = true     tf utils shape type conversion   def compute output shape self  input shape       if self data format == string        input length = input shape 2      else        input length = input shape 1       length = conv utils conv output length input length  self kernel size 0                                              self pad  self stride 0        if self data format == string        return  input shape 0   self filter  length      elif self data format == string        return  input shape 0   length  self filter     def call self  input       if self implementation == 1        output = k local conv input  self kernel  self kernel size  self stride                               self output length    self data format       elif self implementation == 2        output = local conv matmul input  self kernel  self kernel mask                                   self compute output shape input shape        elif self implementation == 3        output = local conv sparse matmul input  self kernel  self kernel idxs                                          self kernel shape                                          self compute output shape input shape        else        raise valueerror string                          self implementation       if self use bias        output = k bias add output  self bias  data format=self data format       output = self activation output      return output    def get config self       config =           string              self filter          string              self kernel size          string              self stride          string              self pad          string              self data format          string              activations serialize self activation           string              self use bias          string              initializers serialize self kernel initializer           string              initializers serialize self bias initializer           string              regularizers serialize self kernel regularizer           string              regularizers serialize self bias regularizer           string              regularizers serialize self activity regularizer           string              constraints serialize self kernel constraint           string              constraints serialize self bias constraint           string              self implementation           base config = super locallyconnected1d  self  get config       return dict list base config items      list config items     
class locallyconnected2d layer     string    def   init   self                 filter                 kernel size                 strides= 1  1                  padding=string                 data format=none                 activation=none                 use bias=true                 kernel initializer=string                 bias initializer=string                 kernel regularizer=none                 bias regularizer=none                 activity regularizer=none                 kernel constraint=none                 bias constraint=none                 implementation=1                   kwargs       super locallyconnected2d  self    init     kwargs      self filter = filter     self kernel size = conv utils normalize tuple kernel size  2  string      self stride = conv utils normalize tuple stride  2  string      self pad = conv utils normalize pad pad      if self pad  = string and implementation == 1        raise valueerror string                        string                          pad      self data format = conv utils normalize data format data format      self activation = activations get activation      self use bias = use bias     self kernel initializer = initializers get kernel initializer      self bias initializer = initializers get bias initializer      self kernel regularizer = regularizers get kernel regularizer      self bias regularizer = regularizers get bias regularizer      self activity regularizer = regularizers get activity regularizer      self kernel constraint = constraints get kernel constraint      self bias constraint = constraints get bias constraint      self implementation = implementation     self input spec = inputspec ndim=4      tf utils shape type conversion   def build self  input shape       if self data format == string        input row  input col = input shape 1 -1        input filter = input shape 3      else        input row  input col = input shape 2         input filter = input shape 1      if input row be none or input col be none        raise valueerror string                        string                        string                        string   str input shape       output row = conv utils conv output length input row  self kernel size 0                                                  self pad  self stride 0       output col = conv utils conv output length input col  self kernel size 1                                                  self pad  self stride 1       self output row = output row     self output col = output col      if self implementation == 1        self kernel shape =             output row   output col            self kernel size 0    self kernel size 1    input filter            self filter         self kernel = self add weight            shape=self kernel shape            initializer=self kernel initializer            name=string            regularizer=self kernel regularizer            constraint=self kernel constraint       elif self implementation == 2        if self data format == string          self kernel shape =  input filter  input row  input col                               self filter  self output row  self output col        else          self kernel shape =  input row  input col  input filter                               self output row  self output col  self filter         self kernel = self add weight shape=self kernel shape                                      initializer=self kernel initializer                                      name=string                                      regularizer=self kernel regularizer                                      constraint=self kernel constraint         self kernel mask = get locallyconnected mask            input shape= input row  input col             kernel shape=self kernel size            strides=self stride            padding=self pad            data format=self data format               elif self implementation == 3        self kernel shape =  self output row   self output col   self filter                             input row   input col   input filter         self kernel idxs = sort            conv utils conv kernel idxs                input shape= input row  input col                 kernel shape=self kernel size                strides=self stride                padding=self pad                filter in=input filter                filter out=self filter                data format=self data format                 self kernel = self add weight            shape= len self kernel idxs               initializer=self kernel initializer            name=string            regularizer=self kernel regularizer            constraint=self kernel constraint       else        raise valueerror string                          self implementation       if self use bias        self bias = self add weight            shape= output row  output col  self filter             initializer=self bias initializer            name=string            regularizer=self bias regularizer            constraint=self bias constraint      else        self bias = none     if self data format == string        self input spec = inputspec ndim=4  axes= 1  input filter       else        self input spec = inputspec ndim=4  axes= -1  input filter       self build = true     tf utils shape type conversion   def compute output shape self  input shape       if self data format == string        row = input shape 2        cols = input shape 3      elif self data format == string        row = input shape 1        cols = input shape 2       row = conv utils conv output length row  self kernel size 0                                            self pad  self stride 0       cols = conv utils conv output length cols  self kernel size 1                                            self pad  self stride 1        if self data format == string        return  input shape 0   self filter  row  cols      elif self data format == string        return  input shape 0   row  cols  self filter     def call self  input       if self implementation == 1        output = k local conv input  self kernel  self kernel size  self stride                               self output row  self output col                               self data format       elif self implementation == 2        output = local conv matmul input  self kernel  self kernel mask                                   self compute output shape input shape        elif self implementation == 3        output = local conv sparse matmul input  self kernel  self kernel idxs                                          self kernel shape                                          self compute output shape input shape        else        raise valueerror string                          self implementation       if self use bias        output = k bias add output  self bias  data format=self data format       output = self activation output      return output    def get config self       config =           string              self filter          string              self kernel size          string              self stride          string              self pad          string              self data format          string              activations serialize self activation           string              self use bias          string              initializers serialize self kernel initializer           string              initializers serialize self bias initializer           string              regularizers serialize self kernel regularizer           string              regularizers serialize self bias regularizer           string              regularizers serialize self activity regularizer           string              constraints serialize self kernel constraint           string              constraints serialize self bias constraint           string              self implementation           base config = super locallyconnected2d  self  get config       return dict list base config items      list config items     
class mask layer     string    def   init   self  mask value=0     kwargs       super mask  self    init     kwargs      self support mask = true     self mask value = mask value     self  compute output and mask jointly = true    def compute mask self  input  mask=none       return k any math ops not equal input  self mask value   axis=-1     def call self  input       boolean mask = k any          math ops not equal input  self mask value   axis=-1  keepdims=true      output = input   math ops cast boolean mask  input dtype           output  keras mask = array ops squeeze boolean mask  axis=-1        return output    def compute output shape self  input shape       return input shape    def get config self       config =  string  self mask value      base config = super mask  self  get config       return dict list base config items      list config items     
class maxpooling1d pooling1d     string    def   init   self  pool size=2  strides=none                 padding=string  data format=string    kwargs        super maxpooling1d  self    init            functools partial backend pool2d  pool mode=string           pool size=pool size          strides=strides          padding=padding          data format=data format            kwargs  
class maxpooling2d pooling2d     string    def   init   self                 pool size= 2  2                  strides=none                 padding=string                 data format=none                   kwargs       super maxpooling2d  self    init            nn max pool          pool size=pool size  strides=strides          padding=padding  data format=data format    kwargs  
class maxpooling3d pooling3d     string    def   init   self                 pool size= 2  2  2                  strides=none                 padding=string                 data format=none                   kwargs       super maxpooling3d  self    init            nn max pool3d          pool size=pool size  strides=strides          padding=padding  data format=data format    kwargs  
class maximum  merge     string    def  merge function self  input       output = input 0      for i in range 1  len input          output = math ops maximum output  input i       return output 
class minimum  merge     string    def  merge function self  input       output = input 0      for i in range 1  len input          output = math ops minimum output  input i       return output 
class multiply  merge     string    def  merge function self  input       output = input 0      for i in range 1  len input          output  = input i      return output 
class prelu layer     string    def   init   self                 alpha initializer=string                 alpha regularizer=none                 alpha constraint=none                 share axes=none                   kwargs       super prelu  self    init     kwargs      self support mask = true     self alpha initializer = initializers get alpha initializer      self alpha regularizer = regularizers get alpha regularizer      self alpha constraint = constraints get alpha constraint      if share ax be none        self share ax = none     elif not isinstance share ax   list  tuple          self share ax =  share ax      else        self share ax = list share ax      tf utils shape type conversion   def build self  input shape       param shape = list input shape 1        if self share ax be not none        for i in self share ax          param shape i - 1  = 1     self alpha = self add weight          shape=param shape          name=string          initializer=self alpha initializer          regularizer=self alpha regularizer          constraint=self alpha constraint           ax =        if self share ax        for i in range 1  len input shape            if i not in self share ax            ax i  = input shape i      self input spec = inputspec ndim=len input shape   axes=axes      self build = true    def call self  input       pos = k relu input      neg = -self alpha   k relu -inputs      return pos   neg    def get config self       config =           string  initializers serialize self alpha initializer           string  regularizers serialize self alpha regularizer           string  constraints serialize self alpha constraint           string  self share ax           base config = super prelu  self  get config       return dict list base config items      list config items         tf utils shape type conversion   def compute output shape self  input shape       return input shape 
class permute layer     string    def   init   self  dim    kwargs       super permute  self    init     kwargs      self dim = tuple dim      if sort dim   = list range 1  len dim    1          raise valueerror            string           string              dim        self input spec = inputspec ndim=len self dim    1     def compute output shape self  input shape       input shape = tensor shape tensorshape input shape  as list       output shape = copy copy input shape      for i  dim in enumerate self dim         target dim = input shape dim        output shape i   1  = target dim     return tensor shape tensorshape output shape     def call self  input       return array ops transpose input  perm= 0     self dim     def get config self       config =  string  self dim      base config = super permute  self  get config       return dict list base config items      list config items     
class rnn layer     string    def   init   self                 cell                 return sequences=false                 return state=false                 go backwards=false                 stateful=false                 unroll=false                 time major=false                   kwargs       if isinstance cell   list  tuple          cell = stackedrnncells cell      if not hasattr cell  string         raise valueerror string                        string  cell      if not hasattr cell  string         raise valueerror string                        string                        string                        string                self zero output for mask = kwargs pop string  false       if string not in kwargs and           string in kwargs or string in kwargs         input shape =  kwargs pop string  none                        kwargs pop string  none         kwargs string  = input shape      super rnn  self    init     kwargs      self cell = cell     self return sequence = return sequence     self return state = return state     self go backwards = go backwards     self stateful = stateful     self unroll = unroll     self time major = time major      self support mask = true                    self input spec = none     self state spec = none     self  state = none     self constants spec = none     self  num constants = 0     self  support rag input = true      if stateful        if ds context have strategy            raise valueerror string                          string      property   def state self       if self  state be none        state = nest map structure lambda    none  self cell state size        return state if nest be sequence self cell state size  else  state      return self  state     state setter          trackable no automatic dependency track   def state self  state       self  state = state    def compute output shape self  input shape       if isinstance input shape  list         input shape = input shape 0                     try        input shape = tensor shape as shape input shape      except  valueerror  typeerror                input shape = nest flatten input shape  0       batch = input shape 0      time step = input shape 1      if self time major        batch  time step = time step  batch      if  be multiple state self cell state size         state size = self cell state size     else        state size =  self cell state size       def  get output shape flat output size         output dim = tensor shape as shape flat output size  as list         if self return sequence          if self time major            output shape = tensor shape as shape  time step  batch    output dim          else            output shape = tensor shape as shape  batch  time step    output dim        else          output shape = tensor shape as shape  batch    output dim        return output shape      if getattr self cell  string  none  be not none               output shape = nest flatten nest map structure             get output shape  self cell output size         output shape = output shape 0  if len output shape  == 1 else output shape     else               output shape =  get output shape state size 0        if self return state        def  get state shape flat state           state shape =  batch    tensor shape as shape flat state  as list           return tensor shape as shape state shape        state shape = nest map structure  get state shape  state size        return generic utils to list output shape    nest flatten state shape      else        return output shape    def compute mask self  input  mask                                mask = nest flatten mask  0      output mask = mask if self return sequence else none     if self return state        state mask =  none for   in self state        return  output mask    state mask     else        return output mask    def build self  input shape       if isinstance input shape  list         input shape = input shape 0                        def get input spec shape         if isinstance shape  tensor shape tensorshape           input spec shape = shape as list         else          input spec shape = list shape        batch index  time step index =  1  0  if self time major else  0  1        if not self stateful          input spec shape batch index  = none       input spec shape time step index  = none       return inputspec shape=tuple input spec shape        def get step input shape shape         if isinstance shape  tensor shape tensorshape           shape = tuple shape as list                 return shape 1   if self time major else  shape 0      shape 2                       try        input shape = tensor shape as shape input shape      except  valueerror  typeerror                pass      if not nest be sequence input shape                if self input spec be not none          self input spec 0  = get input spec input shape        else          self input spec =  get input spec input shape         step input shape = get step input shape input shape      else        if self input spec be not none          self input spec 0  = nest map structure get input spec  input shape        else          self input spec = generic utils to list              nest map structure get input spec  input shape         step input shape = nest map structure get step input shape  input shape            if isinstance self cell  layer         if not self cell build          self cell build step input shape            if  be multiple state self cell state size         state size = list self cell state size      else        state size =  self cell state size       if self state spec be not none               self  validate state spec state size  self state spec      else        self state spec =             inputspec shape= none    tensor shape as shape dim  as list              for dim in state size             if self stateful        self reset state       self build = true     staticmethod   def  validate state spec cell state size  init state specs       string     validation error = valueerror          string         string         string         string format init state specs  cell state size       flat cell state size = nest flatten cell state size      flat state spec = nest flatten init state specs       if len flat cell state size   = len flat state spec         raise validation error     for i in range len flat cell state size          if not tensor shape tensorshape                       flat state spec i  shape 1    be compatible with                tensor shape tensorshape flat cell state size i             raise validation error     doc control do not doc inheritable   def get initial state self  input       get initial state fn = getattr self cell  string  none       if nest be sequence input                       input = nest flatten input  0       input shape = array ops shape input      batch size = input shape 1  if self time major else input shape 0      dtype = input dtype     if get initial state fn        init state = get initial state fn            inputs=none  batch size=batch size  dtype=dtype      else        init state =  generate zero fill state batch size  self cell state size                                                 dtype           if not nest be sequence init state         init state =  init state           return list init state     def   call   self  input  initial state=none  constants=none    kwargs       input  initial state  constants =  standardize args input                                                           initial state                                                           constants                                                           self  num constants       if initial state be none and constants be none        return super rnn  self    call   input    kwargs                       additional input =        additional specs =        if initial state be not none        additional input  = initial state       self state spec = nest map structure            lambda s  inputspec shape=k int shape s    initial state        additional specs  = self state spec     if constants be not none        additional input  = constants       self constants spec =             inputspec shape=k int shape constant   for constant in constants               self  num constants = len constants        additional specs  = self constants spec               flat additional input = nest flatten additional input      be keras tensor = k be keras tensor          flat additional input 0   if flat additional input else true     for tensor in flat additional input        if k be keras tensor tensor   = be keras tensor          raise valueerror string                          string                          string                          string                          string       if be keras tensor               full input =  input    additional input                     full input spec = generic utils to list            nest map structure lambda    none  input     additional specs              self input spec = full input spec       output = super rnn  self    call   full input    kwargs                             self input spec = self input spec  -len additional specs         return output     else        if initial state be not none          kwargs string  = initial state       if constants be not none          kwargs string  = constants       return super rnn  self    call   input    kwargs     def call self             input             mask=none             training=none             initial state=none             constants=none                 input  row lengths = k convert input if rag input      be rag input =  row lengths be not none      self  validate args if rag be rag input  mask       input  initial state  constants = self  process input          input  initial state  constants       self  maybe reset cell dropout mask self cell      if isinstance self cell  stackedrnncells         for cell in self cell cells          self  maybe reset cell dropout mask cell       if mask be not none                      mask = nest flatten mask  0       if nest be sequence input                input shape = k int shape nest flatten input  0       else        input shape = k int shape input      timesteps = input shape 0  if self time major else input shape 1      if self unroll and timesteps be none        raise valueerror string                        string                        string                        string                        string                        string                        string                        string                        string                        string                        string       kwargs =        if generic utils have arg self cell call  string         kwargs string  = train           be tf rnn cell = getattr self cell  string  none  be not none     if constants        if not generic utils have arg self cell call  string           raise valueerror string         def step input  state           constants = state -self  num constants             state = state  -self  num constants             state = state 0  if len state  == 1 and be tf rnn cell else state         output  new state = self cell call              input  state  constants=constants    kwargs          if not nest be sequence new state             new state =  new state          return output  new state     else         def step input  state           state = state 0  if len state  == 1 and be tf rnn cell else state         output  new state = self cell call input  state    kwargs          if not nest be sequence new state             new state =  new state          return output  new state      last output  output  state = k rnn          step          input          initial state          constants=constants          go backwards=self go backwards          mask=mask          unroll=self unroll          input length=row lengths if row lengths be not none else timesteps          time major=self time major          zero output for mask=self zero output for mask       if self stateful        update =          for state   state in zip nest flatten self state   nest flatten state            update append state ops assign state   state         self add update update       if self return sequence        output = k maybe convert to rag be rag input  output  row lengths      else        output = last output      if self return state        if not isinstance state   list  tuple            state =  state        else          state = list state        return generic utils to list output    state     else        return output    def  process input self  input  initial state  constants                      if  isinstance input  collections sequence          and not isinstance input  tuple                        if not self  num constants          initial state = input 1         else          initial state = input 1 -self  num constants          constants = input -self  num constants         if len initial state  == 0          initial state = none       input = input 0       if self stateful        if initial state be not none                                     non zero count = math ops add n  math ops count nonzero v2 s                                           for s in nest flatten self state                     initial state = control flow ops cond non zero count > 0                                                true fn=lambda  self state                                                false fn=lambda  initial state                                                strict=true        else          initial state = self state     elif initial state be none        initial state = self get initial state input       if len initial state   = len self state         raise valueerror string   str len self state                            string   str len initial state                            string      return input  initial state  constants    def  validate args if rag self  be rag input  mask       if not be rag input        return      if mask be not none        raise valueerror string   str mask                           string                        string                        string      if self unroll        raise valueerror string                        string                        string     def  maybe reset cell dropout mask self  cell       if isinstance cell  dropoutrnncellmixin         cell reset dropout mask         cell reset recurrent dropout mask      def reset state self  states=none       string     if not self stateful        raise attributeerror string      spec shape = none     if self input spec be not none        spec shape = nest flatten self input spec 0   0  shape     if spec shape be none                             batch size = none     else        batch size = spec shape 1  if self time major else spec shape 0      if not batch size        raise valueerror string                        string                        string                        string                        string                        string                        string                        string                        string                        string           if nest flatten self state  0  be none        def create state variable state           return k zero  batch size    tensor shape as shape state  as list          self state = nest map structure            create state variable  self cell state size        if not nest be sequence self state           self state =  self state      elif state be none        for state  size in zip nest flatten self state                                nest flatten self cell state size            k set value state  np zero  batch size                                        tensor shape as shape size  as list         else        flat state = nest flatten self state        flat input state = nest flatten state        if len flat input state   = len flat state           raise valueerror string   self name   string                            str len flat state     string                          string   str len flat input state                              string   str state         set value tuples =          for i   value  state  in enumerate zip flat input state                                               flat state            if value shape  = state shape            raise valueerror                string   str i    string                 self name   string   str                     batch size  state     string   str value shape           set value tuples append  state  value         k batch set value set value tuples     def get config self       config =           string  self return sequence          string  self return state          string  self go backwards          string  self stateful          string  self unroll          string  self time major           if self  num constants        config string  = self  num constants     if self zero output for mask        config string  = self zero output for mask      cell config = self cell get config       config string  =           string  self cell   class     name            string  cell config           base config = super rnn  self  get config       return dict list base config items      list config items         classmethod   def from config cls  config  custom objects=none       from tensorflow python keras layer import deserialize as deserialize layer       cell = deserialize layer config pop string   custom objects=custom object      num constants = config pop string  0      layer = cls cell    config      layer  num constants = num constants     return layer 
class relu layer     string    def   init   self  max value=none  negative slope=0  threshold=0    kwargs       super relu  self    init     kwargs      if max value be not none and max value < 0         raise valueerror string                        string   str max value       if negative slope < 0         raise valueerror string                        string   str negative slope        self support mask = true     if max value be not none        max value = k cast to floatx max value      self max value = max value     self negative slope = k cast to floatx negative slope      self threshold = k cast to floatx threshold     def call self  input                 return k relu input                    alpha=self negative slope                    max value=self max value                    threshold=self threshold     def get config self       config =           string  self max value          string  self negative slope          string  self threshold           base config = super relu  self  get config       return dict list base config items      list config items         tf utils shape type conversion   def compute output shape self  input shape       return input shape 
class repeatvector layer     string    def   init   self  n    kwargs       super repeatvector  self    init     kwargs      self n = n     self input spec = inputspec ndim=2     def compute output shape self  input shape       input shape = tensor shape tensorshape input shape  as list       return tensor shape tensorshape  input shape 0   self n  input shape 1       def call self  input       return k repeat input  self n     def get config self       config =  string  self n      base config = super repeatvector  self  get config       return dict list base config items      list config items     
class reshape layer     string    def   init   self  target shape    kwargs       super reshape  self    init     kwargs      self target shape = tuple target shape     def  fix unknown dimension self  input shape  output shape       string     output shape = list output shape      msg = string      know  unknown = 1  none     for index  dim in enumerate output shape         if dim < 0          if unknown be none            unknown = index         else            raise valueerror string        else          know  = dim      original = np prod input shape  dtype=int      if unknown be not none        if know == 0 or original   know  = 0          raise valueerror msg        output shape unknown  = original // know     elif original  = know        raise valueerror msg      return output shape    def compute output shape self  input shape       input shape = tensor shape tensorshape input shape  as list       if none in input shape 1          output shape =  input shape 0                output shape  = tuple s if s  = -1 else none for s in self target shape      else        output shape =  input shape 0         output shape  = self  fix unknown dimension input shape 1                                                      self target shape      return tensor shape tensorshape output shape     def call self  input       return array ops reshape input                                array ops shape input  0      self target shape     def get config self       config =  string  self target shape      base config = super reshape  self  get config       return dict list base config items      list config items     
class separableconv1d separableconv     string    def   init   self                 filter                 kernel size                 strides=1                 padding=string                 data format=none                 dilation rate=1                 depth multiplier=1                 activation=none                 use bias=true                 depthwise initializer=string                 pointwise initializer=string                 bias initializer=string                 depthwise regularizer=none                 pointwise regularizer=none                 bias regularizer=none                 activity regularizer=none                 depthwise constraint=none                 pointwise constraint=none                 bias constraint=none                   kwargs       super separableconv1d  self    init            rank=1          filters=filters          kernel size=kernel size          strides=strides          padding=padding          data format=data format          dilation rate=dilation rate          depth multiplier=depth multiplier          activation=activations get activation           use bias=use bias          depthwise initializer=initializers get depthwise initializer           pointwise initializer=initializers get pointwise initializer           bias initializer=initializers get bias initializer           depthwise regularizer=regularizers get depthwise regularizer           pointwise regularizer=regularizers get pointwise regularizer           bias regularizer=regularizers get bias regularizer           activity regularizer=regularizers get activity regularizer           depthwise constraint=constraints get depthwise constraint           pointwise constraint=constraints get pointwise constraint           bias constraint=constraints get bias constraint             kwargs     def call self  input       if self pad == string        input = array ops pad input  self  compute causal pad        if self data format == string        stride =  1     self stride   2    1         spatial start dim = 1     else        stride =  1  1    self stride   2       spatial start dim = 2                input = array ops expand dim input  spatial start dim      depthwise kernel = array ops expand dim self depthwise kernel  0      pointwise kernel = array ops expand dim self pointwise kernel  0      dilation rate =  1     self dilation rate      if self pad == string        op pad = string     else        op pad = self pad     output = nn separable conv2d          input          depthwise kernel          pointwise kernel          strides=strides          padding=op pad upper            rate=dilation rate          data format=conv utils convert data format self data format  ndim=4        if self use bias        output = nn bias add            output            self bias            data format=conv utils convert data format self data format  ndim=4        output = array ops squeeze output   spatial start dim        if self activation be not none        return self activation output      return output 
class separableconv2d separableconv     string    def   init   self                 filter                 kernel size                 strides= 1  1                  padding=string                 data format=none                 dilation rate= 1  1                  depth multiplier=1                 activation=none                 use bias=true                 depthwise initializer=string                 pointwise initializer=string                 bias initializer=string                 depthwise regularizer=none                 pointwise regularizer=none                 bias regularizer=none                 activity regularizer=none                 depthwise constraint=none                 pointwise constraint=none                 bias constraint=none                   kwargs       super separableconv2d  self    init            rank=2          filters=filters          kernel size=kernel size          strides=strides          padding=padding          data format=data format          dilation rate=dilation rate          depth multiplier=depth multiplier          activation=activations get activation           use bias=use bias          depthwise initializer=initializers get depthwise initializer           pointwise initializer=initializers get pointwise initializer           bias initializer=initializers get bias initializer           depthwise regularizer=regularizers get depthwise regularizer           pointwise regularizer=regularizers get pointwise regularizer           bias regularizer=regularizers get bias regularizer           activity regularizer=regularizers get activity regularizer           depthwise constraint=constraints get depthwise constraint           pointwise constraint=constraints get pointwise constraint           bias constraint=constraints get bias constraint             kwargs     def call self  input            if self data format == string        stride =  1     self stride    1       else        stride =  1  1    self stride     output = nn separable conv2d          input          self depthwise kernel          self pointwise kernel          strides=strides          padding=self pad upper            rate=self dilation rate          data format=conv utils convert data format self data format  ndim=4        if self use bias        output = nn bias add            output            self bias            data format=conv utils convert data format self data format  ndim=4        if self activation be not none        return self activation output      return output 
class simplernn rnn     string    def   init   self                 units                 activation=string                 use bias=true                 kernel initializer=string                 recurrent initializer=string                 bias initializer=string                 kernel regularizer=none                 recurrent regularizer=none                 bias regularizer=none                 activity regularizer=none                 kernel constraint=none                 recurrent constraint=none                 bias constraint=none                 dropout=0                  recurrent dropout=0                  return sequences=false                 return state=false                 go backwards=false                 stateful=false                 unroll=false                   kwargs       if string in kwargs        kwargs pop string        log warn string                       string                       string      cell = simplernncell          units          activation=activation          use bias=use bias          kernel initializer=kernel initializer          recurrent initializer=recurrent initializer          bias initializer=bias initializer          kernel regularizer=kernel regularizer          recurrent regularizer=recurrent regularizer          bias regularizer=bias regularizer          kernel constraint=kernel constraint          recurrent constraint=recurrent constraint          bias constraint=bias constraint          dropout=dropout          recurrent dropout=recurrent dropout          dtype=kwargs get string           trainable=kwargs get string  true       super simplernn  self    init            cell          return sequences=return sequence          return state=return state          go backwards=go backwards          stateful=stateful          unroll=unroll            kwargs      self activity regularizer = regularizers get activity regularizer      self input spec =  inputspec ndim=3      def call self  input  mask=none  training=none  initial state=none       self  maybe reset cell dropout mask self cell      return super simplernn  self  call          input  mask=mask  training=training  initial state=initial state      property   def units self       return self cell units     property   def activation self       return self cell activation     property   def use bias self       return self cell use bias     property   def kernel initializer self       return self cell kernel initializer     property   def recurrent initializer self       return self cell recurrent initializer     property   def bias initializer self       return self cell bias initializer     property   def kernel regularizer self       return self cell kernel regularizer     property   def recurrent regularizer self       return self cell recurrent regularizer     property   def bias regularizer self       return self cell bias regularizer     property   def kernel constraint self       return self cell kernel constraint     property   def recurrent constraint self       return self cell recurrent constraint     property   def bias constraint self       return self cell bias constraint     property   def dropout self       return self cell dropout     property   def recurrent dropout self       return self cell recurrent dropout    def get config self       config =           string              self units          string              activations serialize self activation           string              self use bias          string              initializers serialize self kernel initializer           string              initializers serialize self recurrent initializer           string              initializers serialize self bias initializer           string              regularizers serialize self kernel regularizer           string              regularizers serialize self recurrent regularizer           string              regularizers serialize self bias regularizer           string              regularizers serialize self activity regularizer           string              constraints serialize self kernel constraint           string              constraints serialize self recurrent constraint           string              constraints serialize self bias constraint           string              self dropout          string              self recurrent dropout           base config = super simplernn  self  get config       del base config string      return dict list base config items      list config items         classmethod   def from config cls  config       if string in config        config pop string      return cls   config  
class simplernncell dropoutrnncellmixin  layer     string    def   init   self                 units                 activation=string                 use bias=true                 kernel initializer=string                 recurrent initializer=string                 bias initializer=string                 kernel regularizer=none                 recurrent regularizer=none                 bias regularizer=none                 kernel constraint=none                 recurrent constraint=none                 bias constraint=none                 dropout=0                  recurrent dropout=0                    kwargs       self  enable cache device = kwargs pop string  false      super simplernncell  self    init     kwargs      self units = units     self activation = activations get activation      self use bias = use bias      self kernel initializer = initializers get kernel initializer      self recurrent initializer = initializers get recurrent initializer      self bias initializer = initializers get bias initializer       self kernel regularizer = regularizers get kernel regularizer      self recurrent regularizer = regularizers get recurrent regularizer      self bias regularizer = regularizers get bias regularizer       self kernel constraint = constraints get kernel constraint      self recurrent constraint = constraints get recurrent constraint      self bias constraint = constraints get bias constraint       self dropout = min 1   max 0   dropout       self recurrent dropout = min 1   max 0   recurrent dropout       self state size = self units     self output size = self units     tf utils shape type conversion   def build self  input shape       default cache device =  cache device self      self kernel = self add weight          shape= input shape -1   self units           name=string          initializer=self kernel initializer          regularizer=self kernel regularizer          constraint=self kernel constraint          cache device=default cache device      self recurrent kernel = self add weight          shape= self units  self units           name=string          initializer=self recurrent initializer          regularizer=self recurrent regularizer          constraint=self recurrent constraint          cache device=default cache device      if self use bias        self bias = self add weight            shape= self units              name=string            initializer=self bias initializer            regularizer=self bias regularizer            constraint=self bias constraint            cache device=default cache device      else        self bias = none     self build = true    def call self  input  state  training=none       prev output = state 0      dp mask = self get dropout mask for cell input  train      rec dp mask = self get recurrent dropout mask for cell          prev output  train       if dp mask be not none        h = k dot input   dp mask  self kernel      else        h = k dot input  self kernel      if self bias be not none        h = k bias add h  self bias       if rec dp mask be not none        prev output = prev output   rec dp mask     output = h   k dot prev output  self recurrent kernel      if self activation be not none        output = self activation output       return output   output     def get initial state self  inputs=none  batch size=none  dtype=none       return  generate zero fill state for cell self  input  batch size  dtype     def get config self       config =           string              self units          string              activations serialize self activation           string              self use bias          string              initializers serialize self kernel initializer           string              initializers serialize self recurrent initializer           string              initializers serialize self bias initializer           string              regularizers serialize self kernel regularizer           string              regularizers serialize self recurrent regularizer           string              regularizers serialize self bias regularizer           string              constraints serialize self kernel constraint           string              constraints serialize self recurrent constraint           string              constraints serialize self bias constraint           string              self dropout          string              self recurrent dropout           base config = super simplernncell  self  get config       return dict list base config items      list config items     
class softmax layer     string    def   init   self  axis=-1    kwargs       super softmax  self    init     kwargs      self support mask = true     self axis = axis    def call self  input       return k softmax input  axis=self axis     def get config self       config =  string  self axis      base config = super softmax  self  get config       return dict list base config items      list config items         tf utils shape type conversion   def compute output shape self  input shape       return input shape 
class spatialdropout1d dropout     string    def   init   self  rate    kwargs       super spatialdropout1d  self    init   rate    kwargs      self input spec = inputspec ndim=3     def  get noise shape self  input       input shape = array ops shape input      noise shape =  input shape 0   1  input shape 2       return noise shape 
class spatialdropout2d dropout     string    def   init   self  rate  data format=none    kwargs       super spatialdropout2d  self    init   rate    kwargs      if data format be none        data format = k image data format       if data format not in  string  string         raise valueerror string                        string      self data format = data format     self input spec = inputspec ndim=4     def  get noise shape self  input       input shape = array ops shape input      if self data format == string        return  input shape 0   input shape 1   1  1      elif self data format == string        return  input shape 0   1  1  input shape 3   
class spatialdropout3d dropout     string    def   init   self  rate  data format=none    kwargs       super spatialdropout3d  self    init   rate    kwargs      if data format be none        data format = k image data format       if data format not in  string  string         raise valueerror string                        string      self data format = data format     self input spec = inputspec ndim=5     def  get noise shape self  input       input shape = array ops shape input      if self data format == string        return  input shape 0   input shape 1   1  1  1      elif self data format == string        return  input shape 0   1  1  1  input shape 4   
class stackedrnncells layer     string    def   init   self  cells    kwargs       for cell in cells        if not hasattr cell  string           raise valueerror string                          string  cells        if not hasattr cell  string           raise valueerror string                          string                          string  cells      self cells = cells                         self reverse state order = kwargs pop string  false      if self reverse state order        log warn string                       string                       string                       string      super stackedrnncells  self    init     kwargs      property   def state size self       return tuple c state size for c in                   self cells   -1  if self reverse state order else self cells       property   def output size self       if getattr self cells -1   string  none  be not none        return self cells -1  output size     elif  be multiple state self cells -1  state size         return self cells -1  state size 0      else        return self cells -1  state size    def get initial state self  inputs=none  batch size=none  dtype=none       initial state =        for cell in self cells   -1  if self reverse state order else self cells        get initial state fn = getattr cell  string  none        if get initial state fn          initial state append get initial state fn              inputs=inputs  batch size=batch size  dtype=dtype         else          initial state append  generate zero fill state for cell              cell  input  batch size  dtype        return tuple initial state     def call self  input  state  constants=none  training=none    kwargs            state size =  self state size   -1                    if self reverse state order else self state size      nest state = nest pack sequence as state size  nest flatten state             new nest state =        for cell  state in zip self cells  nest state         state = state if nest be sequence state  else  state               be tf rnn cell = getattr cell  string  none  be not none       state = state 0  if len state  == 1 and be tf rnn cell else state       if generic utils have arg cell call  string           kwargs string  = train       else          kwargs pop string  none        if generic utils have arg cell call  string           input  state = cell call input  state  constants=constants                                       kwargs        else          input  state = cell call input  state    kwargs        new nest state append state       return input  nest pack sequence as state size                                           nest flatten new nest state       tf utils shape type conversion   def build self  input shape       if isinstance input shape  list         input shape = input shape 0      for cell in self cells        if isinstance cell  layer           if not cell build            cell build input shape        if getattr cell  string  none  be not none          output dim = cell output size       elif  be multiple state cell state size           output dim = cell state size 0        else          output dim = cell state size       input shape = tuple  input shape 0                               tensor shape as shape output dim  as list        self build = true    def get config self       cells =        for cell in self cells        cells append             string  cell   class     name              string  cell get config                config =  string  cells      base config = super stackedrnncells  self  get config       return dict list base config items      list config items         classmethod   def from config cls  config  custom objects=none       from tensorflow python keras layer import deserialize as deserialize layer       cells =        for cell config in config pop string         cells append            deserialize layer cell config  custom objects=custom object       return cls cells    config  
class subtract  merge     string     tf utils shape type conversion   def build self  input shape       super subtract  self  build input shape      if len input shape   = 2        raise valueerror string                        string     def  merge function self  input       if len input   = 2        raise valueerror string                        string      return input 0  - input 1  
class thresholdedrelu layer     string    def   init   self  theta=1 0    kwargs       super thresholdedrelu  self    init     kwargs      self support mask = true     self theta = k cast to floatx theta     def call self  input       return input   math ops cast          math ops greater input  self theta   k floatx       def get config self       config =  string  float self theta       base config = super thresholdedrelu  self  get config       return dict list base config items      list config items         tf utils shape type conversion   def compute output shape self  input shape       return input shape 
class timedistributed wrapper     string    def   init   self  layer    kwargs       if not isinstance layer  layer         raise valueerror            string           string format input=layer       super timedistributed  self    init   layer    kwargs      self support mask = true     self  support rag input = true                self  always use reshape =           layer utils be builtin layer layer  and         not getattr layer  string  false      def  get shape tuple self  init tuple  tensor  start idx  int shape=none       string          if int shape be none        int shape = k int shape tensor  start idx       if not any not s for s in int shape         return init tuple   tuple int shape      shape = k shape tensor      int shape = list int shape      for i  s in enumerate int shape         if not s          int shape i  = shape start idx   i      return init tuple   tuple int shape     def build self  input shape       input shape = tensor shape tensorshape input shape  as list       if len input shape  < 3        raise valueerror            string           string   str input shape            self input spec = inputspec shape= none  none    input shape 2        child input shape =  input shape 0     input shape 2       super timedistributed  self  build tuple child input shape       self build = true    def compute output shape self  input shape       input shape = tensor shape tensorshape input shape  as list       child input shape = tensor shape tensorshape  input shape 0                                                      input shape 2        child output shape = self layer compute output shape child input shape      if not isinstance child output shape  tensor shape tensorshape         child output shape = tensor shape tensorshape child output shape      child output shape = child output shape as list       timesteps = input shape 1      return tensor shape tensorshape  child output shape 0   timesteps                                        child output shape 1       def call self  input  training=none  mask=none       kwargs =        if generic utils have arg self layer call  string         kwargs string  = train      input shape = k int shape input      if input shape 0  and not self  always use reshape        input  row lengths = k convert input if rag input        be rag input = row lengths be not none               def step x              output = self layer x    kwargs          return output               output    = k rnn            step            input            initial states=              input length=row lengths 0  if be rag input else input shape 1             mask=mask            unroll=false        y = k maybe convert to rag be rag input  output  row lengths      else                             if isinstance input  rag tensor raggedtensor           y = self layer input value    kwargs          y = rag tensor raggedtensor from row lengths              y              input nest row lengths   0         else          input length = input shape 1          if not input length            input length = array ops shape input  1          inner input shape = self  get shape tuple  -1    input  2                            input uid = generic utils object list uid input          input = array ops reshape input  inner input shape          self  input map input uid  = input                  if generic utils have arg self layer call  string  and mask be not none            inner mask shape = self  get shape tuple  -1    mask  2            kwargs string  = k reshape mask  inner mask shape           y = self layer input    kwargs                    output shape = self compute output shape input shape  as list           output shape = self  get shape tuple  -1  input length   y  1                                               output shape 2            y = array ops reshape y  output shape       return y    def compute mask self  input  mask=none       string               input shape = k int shape input      if input shape 0                return mask     inner mask = mask     if inner mask be not none        inner mask shape = self  get shape tuple  -1    mask  2        inner mask = k reshape inner mask  inner mask shape      input uid = generic utils object list uid input      inner input = self  input map get input uid  input      output mask = self layer compute mask inner input  inner mask      if output mask be none        if mask be none          return none                     output mask = mask       for   in range 2  len k int shape mask             output mask = k any output mask  axis=-1      else               input length = input shape 1        if not input length          input length = k shape input  1        output mask int shape = k int shape output mask        if output mask int shape be none                            if mask be not none            output mask int shape = k int shape mask          else            output mask int shape = k compute output shape input shape   -1        output mask shape = self  get shape tuple             -1  input length   output mask  1  output mask int shape 1          output mask = k reshape output mask  output mask shape      return output mask 
class upsampling1d layer     string    def   init   self  size=2    kwargs       super upsampling1d  self    init     kwargs      self size = int size      self input spec = inputspec ndim=3     def compute output shape self  input shape       input shape = tensor shape tensorshape input shape  as list       size = self size   input shape 1  if input shape 1  be not none else none     return tensor shape tensorshape  input shape 0   size  input shape 2       def call self  input       output = backend repeat elements input  self size  axis=1      return output    def get config self       config =  string  self size      base config = super upsampling1d  self  get config       return dict list base config items      list config items     
class upsampling2d layer     string    def   init   self                 size= 2  2                  data format=none                 interpolation=string                   kwargs       super upsampling2d  self    init     kwargs      self data format = conv utils normalize data format data format      self size = conv utils normalize tuple size  2  string      if interpolation not in  string  string         raise valueerror string                        string      self interpolation = interpolation     self input spec = inputspec ndim=4     def compute output shape self  input shape       input shape = tensor shape tensorshape input shape  as list       if self data format == string        height = self size 0    input shape            2  if input shape 2  be not none else none       width = self size 1    input shape            3  if input shape 3  be not none else none       return tensor shape tensorshape             input shape 0   input shape 1   height  width       else        height = self size 0    input shape            1  if input shape 1  be not none else none       width = self size 1    input shape            2  if input shape 2  be not none else none       return tensor shape tensorshape             input shape 0   height  width  input shape 3       def call self  input       return backend resize image          input  self size 0   self size 1   self data format          interpolation=self interpolation     def get config self       config =           string  self size          string  self data format          string  self interpolation           base config = super upsampling2d  self  get config       return dict list base config items      list config items     
class upsampling3d layer     string    def   init   self  size= 2  2  2   data format=none    kwargs       self data format = conv utils normalize data format data format      self size = conv utils normalize tuple size  3  string      self input spec = inputspec ndim=5      super upsampling3d  self    init     kwargs     def compute output shape self  input shape       input shape = tensor shape tensorshape input shape  as list       if self data format == string        dim1 = self size 0    input shape            2  if input shape 2  be not none else none       dim2 = self size 1    input shape            3  if input shape 3  be not none else none       dim3 = self size 2    input shape            4  if input shape 4  be not none else none       return tensor shape tensorshape             input shape 0   input shape 1   dim1  dim2  dim3       else        dim1 = self size 0    input shape            1  if input shape 1  be not none else none       dim2 = self size 1    input shape            2  if input shape 2  be not none else none       dim3 = self size 2    input shape            3  if input shape 3  be not none else none       return tensor shape tensorshape             input shape 0   dim1  dim2  dim3  input shape 4       def call self  input       return backend resize volumes          input  self size 0   self size 1   self size 2   self data format     def get config self       config =  string  self size  string  self data format      base config = super upsampling3d  self  get config       return dict list base config items      list config items     
class wrapper layer     string    def   init   self  layer    kwargs       assert isinstance layer  layer      self layer = layer                    self  input map =        super wrapper  self    init     kwargs     def build self  input shape=none       if not self layer build        self layer build input shape      self build = true     property   def activity regularizer self       if hasattr self layer  string         return self layer activity regularizer     else        return none    def get config self       config =           string                string  self layer   class     name                string  self layer get config                       base config = super wrapper  self  get config       return dict list base config items      list config items         classmethod   def from config cls  config  custom objects=none       from tensorflow python keras layer import deserialize as deserialize layer       layer = deserialize layer          config pop string   custom objects=custom object      return cls layer    config  
class zeropadding1d layer     string    def   init   self  padding=1    kwargs       super zeropadding1d  self    init     kwargs      self pad = conv utils normalize tuple pad  2  string      self input spec = inputspec ndim=3     def compute output shape self  input shape       if input shape 1  be not none        length = input shape 1    self pad 0    self pad 1      else        length = none     return tensor shape tensorshape  input shape 0   length  input shape 2       def call self  input       return backend temporal pad input  padding=self pad     def get config self       config =  string  self pad      base config = super zeropadding1d  self  get config       return dict list base config items      list config items     
class zeropadding2d layer     string    def   init   self  padding= 1  1   data format=none    kwargs       super zeropadding2d  self    init     kwargs      self data format = conv utils normalize data format data format      if isinstance pad  int         self pad =   pad  pad    pad  pad       elif hasattr pad  string         if len pad   = 2          raise valueerror string                          string   str pad         height pad = conv utils normalize tuple pad 0   2                                                    string        width pad = conv utils normalize tuple pad 1   2                                                   string        self pad =  height pad  width pad      else        raise valueerror string                        string                        string                        string                        string                        string   str pad       self input spec = inputspec ndim=4     def compute output shape self  input shape       input shape = tensor shape tensorshape input shape  as list       if self data format == string        if input shape 2  be not none          row = input shape 2    self pad 0  0    self pad 0  1        else          row = none       if input shape 3  be not none          cols = input shape 3    self pad 1  0    self pad 1  1        else          cols = none       return tensor shape tensorshape             input shape 0   input shape 1   row  cols       elif self data format == string        if input shape 1  be not none          row = input shape 1    self pad 0  0    self pad 0  1        else          row = none       if input shape 2  be not none          cols = input shape 2    self pad 1  0    self pad 1  1        else          cols = none       return tensor shape tensorshape             input shape 0   row  cols  input shape 3       def call self  input       return backend spatial 2d pad          input  padding=self pad  data format=self data format     def get config self       config =  string  self pad  string  self data format      base config = super zeropadding2d  self  get config       return dict list base config items      list config items     
class zeropadding3d layer     string    def   init   self  padding= 1  1  1   data format=none    kwargs       super zeropadding3d  self    init     kwargs      self data format = conv utils normalize data format data format      if isinstance pad  int         self pad =   pad  pad    pad  pad    pad                                                                 pad       elif hasattr pad  string         if len pad   = 3          raise valueerror string                          string   str pad         dim1 pad = conv utils normalize tuple pad 0   2                                                  string        dim2 pad = conv utils normalize tuple pad 1   2                                                  string        dim3 pad = conv utils normalize tuple pad 2   2                                                  string        self pad =  dim1 pad  dim2 pad  dim3 pad      else        raise valueerror            string           string           string           string           string           string           string           string   str pad       self input spec = inputspec ndim=5     def compute output shape self  input shape       input shape = tensor shape tensorshape input shape  as list       if self data format == string        if input shape 2  be not none          dim1 = input shape 2    2   self pad 0  0        else          dim1 = none       if input shape 3  be not none          dim2 = input shape 3    2   self pad 1  0        else          dim2 = none       if input shape 4  be not none          dim3 = input shape 4    2   self pad 2  0        else          dim3 = none       return tensor shape tensorshape             input shape 0   input shape 1   dim1  dim2  dim3       elif self data format == string        if input shape 1  be not none          dim1 = input shape 1    2   self pad 0  1        else          dim1 = none       if input shape 2  be not none          dim2 = input shape 2    2   self pad 1  1        else          dim2 = none       if input shape 3  be not none          dim3 = input shape 3    2   self pad 2  1        else          dim3 = none       return tensor shape tensorshape             input shape 0   dim1  dim2  dim3  input shape 4       def call self  input       return backend spatial 3d pad          input  padding=self pad  data format=self data format     def get config self       config =  string  self pad  string  self data format      base config = super zeropadding3d  self  get config       return dict list base config items      list config items     
 keras export string  def add input    kwargs     string   return add   kwargs  input  
 keras export string  def average input    kwargs     string   return average   kwargs  input  
 keras export string  def concatenate input  axis=-1    kwargs     string   return concatenate axis=axis    kwargs  input  
 keras export string  def deserialize config  custom objects=none     string      from tensorflow python keras import model     from tensorflow python keras premade linear import linearmodel     from tensorflow python keras premade wide deep import widedeepmodel     from tensorflow python feature column import dense feature     from tensorflow python feature column import sequence feature column as sfc      globs = globals       globs string  = model network   globs string  = model model   globs string  = model sequential   globs string  = linearmodel   globs string  = widedeepmodel       globs string  = dense feature densefeatures   globs string  = sfc sequencefeatures    layer class name = config string    if layer class name in  deserialization table      config string  =  deserialization table layer class name     return deserialize keras object        config        module objects=globs        custom objects=custom object        printable module name=string  
 keras export string  def dot input  ax  normalize=false    kwargs     string   return dot axes=axes  normalize=normalize    kwargs  input  
 keras export string  def maximum input    kwargs     string   return maximum   kwargs  input  
 keras export string  def minimum input    kwargs     string   return minimum   kwargs  input  
 keras export string  def multiply input    kwargs     string   return multiply   kwargs  input  
 keras export string  def serialize layer     return serialize keras object layer  
 keras export string  def subtract input    kwargs     string   return subtract   kwargs  input  
class preprocessinglayer layer     string     metaclass   = abc abcmeta     abc abstractmethod   def adapt self  data  reset state=true            string     pass 
class binarycrossentropy lossfunctionwrapper     string    def   init   self                 from logits=false                 label smoothing=0                 reduction=losses utils reductionv2 auto                 name=string       super binarycrossentropy  self    init            binary crossentropy          name=name          reduction=reduction          from logits=from logits          label smoothing=label smooth      self from logits = from logits 
class categoricalcrossentropy lossfunctionwrapper     string    def   init   self                 from logits=false                 label smoothing=0                 reduction=losses utils reductionv2 auto                 name=string       super categoricalcrossentropy  self    init            categorical crossentropy          name=name          reduction=reduction          from logits=from logits          label smoothing=label smooth  
class categoricalhinge lossfunctionwrapper     string    def   init   self                 reduction=losses utils reductionv2 auto                 name=string       super categoricalhinge  self    init            categorical hinge  name=name  reduction=reduction  
class cosinesimilarity lossfunctionwrapper     string    def   init   self                 axis=-1                 reduction=losses utils reductionv2 auto                 name=string       super cosinesimilarity  self    init            cosine similarity  reduction=reduction  name=name  axis=axis  
class hinge lossfunctionwrapper     string    def   init   self  reduction=losses utils reductionv2 auto  name=string       super hinge  self    init   hinge  name=name  reduction=reduction  
class huber lossfunctionwrapper     string    def   init   self                 delta=1 0                 reduction=losses utils reductionv2 auto                 name=string       super huber  self    init            huber loss  name=name  reduction=reduction  delta=delta  
class kldivergence lossfunctionwrapper     string    def   init   self                 reduction=losses utils reductionv2 auto                 name=string       super kldivergence  self    init            kullback leibler divergence  name=name  reduction=reduction  
class logcosh lossfunctionwrapper     string    def   init   self  reduction=losses utils reductionv2 auto  name=string       super logcosh  self    init   logcosh  name=name  reduction=reduction  
class loss object     string    def   init   self  reduction=losses utils reductionv2 auto  name=none       losses utils reductionv2 validate reduction      self reduction = reduction     self name = name    def   call   self  y true  y pred  sample weight=none       string               scope name = string if self name == string else self name     graph ctx = tf utils graph context for symbolic tensors          y true  y pred  sample weight      with k name scope scope name or self   class     name     graph ctx        losses = self call y true  y pred        return losses utils compute weight loss            losses  sample weight  reduction=self  get reduction        classmethod   def from config cls  config       string     return cls   config     def get config self       return  string  self reduction  string  self name      abc abstractmethod    doc control for subclass implementers   def call self  y true  y pred       string     notimplementederror string     def  get reduction self       string     if distribution strategy context have strategy   and           self reduction == losses utils reductionv2 auto or         self reduction == losses utils reductionv2 sum over batch size         raise valueerror            string           string           string           string           string           string           string           string           string           string           string           string       if self reduction == losses utils reductionv2 auto        return losses utils reductionv2 sum over batch size     return self reduction 
class meanabsoluteerror lossfunctionwrapper     string    def   init   self                 reduction=losses utils reductionv2 auto                 name=string       super meanabsoluteerror  self    init            mean absolute error  name=name  reduction=reduction  
class meanabsolutepercentageerror lossfunctionwrapper     string    def   init   self                 reduction=losses utils reductionv2 auto                 name=string       super meanabsolutepercentageerror  self    init            mean absolute percentage error  name=name  reduction=reduction  
class meansquarederror lossfunctionwrapper     string    def   init   self                 reduction=losses utils reductionv2 auto                 name=string       super meansquarederror  self    init            mean square error  name=name  reduction=reduction  
class meansquaredlogarithmicerror lossfunctionwrapper     string    def   init   self                 reduction=losses utils reductionv2 auto                 name=string       super meansquaredlogarithmicerror  self    init            mean square logarithmic error  name=name  reduction=reduction  
class poisson lossfunctionwrapper     string    def   init   self  reduction=losses utils reductionv2 auto  name=string       super poisson  self    init   poisson  name=name  reduction=reduction  
class reductionv2 object     string    auto = string   none = string   sum = string   sum over batch size = string     classmethod   def all cls       return  cls auto  cls none  cls sum  cls sum over batch size      classmethod   def validate cls  key       if key not in cls all          raise valueerror string   key  
class sparsecategoricalcrossentropy lossfunctionwrapper     string    def   init   self                 from logits=false                 reduction=losses utils reductionv2 auto                 name=string       super sparsecategoricalcrossentropy  self    init            sparse categorical crossentropy          name=name          reduction=reduction          from logits=from logits  
class squaredhinge lossfunctionwrapper     string    def   init   self                 reduction=losses utils reductionv2 auto                 name=string       super squaredhinge  self    init            square hinge  name=name  reduction=reduction  
 keras export string  def categorical hinge y true  y pred     string   y pred = ops convert to tensor y pred    y true = math ops cast y true  y pred dtype    pos = math ops reduce sum y true   y pred  axis=-1    neg = math ops reduce max  1  - y true    y pred  axis=-1    return math ops maximum 0   neg - pos   1   
 keras export      string      v1=          string          string          string          string          string         def cosine similarity y true  y pred  axis=-1     string   y true = nn l2 normalize y true  axis=axis    y pred = nn l2 normalize y pred  axis=axis    return -math ops reduce sum y true   y pred  axis=axis  
 keras export string  def deserialize name  custom objects=none     return deserialize keras object        name        module objects=globals          custom objects=custom object        printable module name=string  
 keras export string  def get identifier     if identifier be none      return none   if isinstance identifier  six string type       identifier = str identifier      return deserialize identifier    if isinstance identifier  dict       return deserialize identifier    elif callable identifier       return identifier   else      raise valueerror string                      string  identifier  
 keras export string  def serialize loss     return serialize keras object loss  
class auc metric     string    def   init   self                 num thresholds=200                 curve=string                 summation method=string                 name=none                 dtype=none                 thresholds=none                 multi label=false                 label weights=none       string          if isinstance curve  metrics utils auccurve  and curve not in list          metrics utils auccurve         raise valueerror string format            curve  list metrics utils auccurve        if isinstance          summation method          metrics utils aucsummationmethod  and summation method not in list              metrics utils aucsummationmethod         raise valueerror            string format                summation method  list metrics utils aucsummationmethod              if thresholds be not none               self num thresholds = len thresholds    2       thresholds = sort thresholds      else        if num thresholds <= 1          raise valueerror string                       self num thresholds = num thresholds       thresholds =   i   1    1 0 /  num thresholds - 1                      for i in range num thresholds - 2                  self thresholds =  0 0 - k epsilon      thresholds    1 0   k epsilon         if isinstance curve  metrics utils auccurve         self curve = curve     else        self curve = metrics utils auccurve from str curve      if isinstance summation method  metrics utils aucsummationmethod         self summation method = summation method     else        self summation method = metrics utils aucsummationmethod from str            summation method      super auc  self    init   name=name  dtype=dtype            self multi label = multi label     if label weight be not none        label weight = constant op constant label weight  dtype=self dtype        check =             check ops assert non negative                label weight                message=string                self label weight = control flow ops with dependencies            check  label weight       else        self label weight = none      self  build = false     if not self multi label        self  build none     def  build self  shape       string     if self multi label        if shape ndims  = 2          raise valueerror string                          string   shape ndims        variable shape = tensor shape tensorshape             tensor shape dimension self num thresholds   shape 1        else        variable shape = tensor shape tensorshape             tensor shape dimension self num thresholds              self true positives = self add weight          string          shape=variable shape          initializer=init ops zero initializer      self true negative = self add weight          string          shape=variable shape          initializer=init ops zero initializer      self false positives = self add weight          string          shape=variable shape          initializer=init ops zero initializer      self false negative = self add weight          string          shape=variable shape          initializer=init ops zero initializer       if self multi label        with ops init scope                                       if not context execute eagerly              k  initialize variables k  get session           self  build = true    def update state self  y true  y pred  sample weight=none       string     deps =        if not self  build        self  build y true shape       if self multi label or  self label weight be not none                shape =              y true   string  string                 if self multi label                            shape extend   self true positives   string  string                            self true negative   string  string                            self false positives   string  string                            self false negative   string  string           if self label weight be not none                   shape append  self label weight   string           deps =             check ops assert shape                shape  message=string                              label weight = none if self multi label else self label weight     with ops control dependencies deps         return metrics utils update confusion matrix variables                            metrics utils confusionmatrix true positives                    self true positives                metrics utils confusionmatrix true negative                    self true negative                metrics utils confusionmatrix false positives                    self false positives                metrics utils confusionmatrix false negative                    self false negative                         y true            y pred            self thresholds            sample weight=sample weight            multi label=self multi label            label weights=label weight     def interpolate pr auc self       string     dtp = self true positives  self num thresholds -                               1  - self true positives 1       p = self true positives   self false positives     dp = p  self num thresholds - 1  - p 1        prec slope = math ops div no nan          dtp  math ops maximum dp  0   name=string      intercept = self true positives 1   - math ops multiply prec slope  p 1         safe p ratio = array ops where          math ops logical and p  self num thresholds - 1  > 0  p 1   > 0           math ops div no nan              p  self num thresholds - 1               math ops maximum p 1    0               name=string           array ops ones like p 1          return math ops reduce sum          math ops div no nan              prec slope    dtp   intercept   math ops log safe p ratio                math ops maximum self true positives 1     self false negative 1                                 0               name=string           name=string     def result self       if  self curve == metrics utils auccurve pr and         self summation method == metrics utils aucsummationmethod interpolation                        return self interpolate pr auc             recall = math ops div no nan self true positives                                   self true positives   self false negative      if self curve == metrics utils auccurve roc        fp rate = math ops div no nan self false positives                                      self false positives   self true negative        x = fp rate       y = recall     else          precision = math ops div no nan            self true positives  self true positives   self false positives        x = recall       y = precision           if self summation method == metrics utils aucsummationmethod interpolation               heights =  y  self num thresholds - 1    y 1    / 2      elif self summation method == metrics utils aucsummationmethod minoring        heights = math ops minimum y  self num thresholds - 1   y 1        else          heights = math ops maximum y  self num thresholds - 1   y 1              if self multi label        riemann term = math ops multiply x  self num thresholds - 1  - x 1                                            heights        by label auc = math ops reduce sum            riemann term  name=self name   string  axis=0         if self label weight be none                   return math ops reduce mean by label auc  name=self name        else                   return math ops div no nan              math ops reduce sum                  math ops multiply by label auc  self label weight                math ops reduce sum self label weight               name=self name      else        return math ops reduce sum            math ops multiply x  self num thresholds - 1  - x 1    heights             name=self name     def reset state self       k batch set value            v  np zero  self num thresholds     for v in self variables      def get config self       if be tensor or variable self label weight         label weight = k eval self label weight      else        label weight = self label weight     config =           string  self num thresholds          string  self curve value          string  self summation method value                                     string  self thresholds 1 -1           string  self multi label          string  label weight           base config = super auc  self  get config       return dict list base config items      list config items     
class accuracy meanmetricwrapper     string    def   init   self  name=string  dtype=none       super accuracy  self    init   accuracy  name  dtype=dtype  
class binaryaccuracy meanmetricwrapper     string    def   init   self  name=string  dtype=none  threshold=0 5       string     super binaryaccuracy  self    init            binary accuracy  name  dtype=dtype  threshold=threshold  
class binarycrossentropy meanmetricwrapper     string    def   init   self                 name=string                 dtype=none                 from logits=false                 label smoothing=0       string      super binarycrossentropy  self    init            binary crossentropy          name          dtype=dtype          from logits=from logits          label smoothing=label smooth  
class categoricalaccuracy meanmetricwrapper     string    def   init   self  name=string  dtype=none       string     super categoricalaccuracy  self    init            categorical accuracy  name  dtype=dtype  
class categoricalcrossentropy meanmetricwrapper     string    def   init   self                 name=string                 dtype=none                 from logits=false                 label smoothing=0        super categoricalcrossentropy  self    init            categorical crossentropy          name          dtype=dtype          from logits=from logits          label smoothing=label smooth  
class categoricalhinge meanmetricwrapper     string    def   init   self  name=string  dtype=none       super categoricalhinge  self    init   categorical hinge  name  dtype=dtype  
class cosinesimilarity meanmetricwrapper     string    def   init   self  name=string  dtype=none  axis=-1       string     super cosinesimilarity  self    init            cosine similarity  name  dtype=dtype  axis=axis  
class falsenegatives  confusionmatrixconditioncount     string    def   init   self  thresholds=none  name=none  dtype=none       string     super falsenegatives  self    init            confusion matrix cond=metrics utils confusionmatrix false negative          thresholds=thresholds          name=name          dtype=dtype  
class falsepositives  confusionmatrixconditioncount     string    def   init   self  thresholds=none  name=none  dtype=none       string     super falsepositives  self    init            confusion matrix cond=metrics utils confusionmatrix false positives          thresholds=thresholds          name=name          dtype=dtype  
class hinge meanmetricwrapper     string    def   init   self  name=string  dtype=none       super hinge  self    init   hinge  name  dtype=dtype  
class kldivergence meanmetricwrapper     string    def   init   self  name=string  dtype=none       super kldivergence  self    init            kullback leibler divergence  name  dtype=dtype  
class logcosherror meanmetricwrapper     string    def   init   self  name=string  dtype=none       super logcosherror  self    init   logcosh  name  dtype=dtype  
class mean reduce     string    def   init   self  name=string  dtype=none       string     super mean  self    init            reduction=metrics utils reduction weight mean  name=name  dtype=dtype  
class meanabsoluteerror meanmetricwrapper     string    def   init   self  name=string  dtype=none       super meanabsoluteerror  self    init            mean absolute error  name  dtype=dtype  
class meanabsolutepercentageerror meanmetricwrapper     string    def   init   self  name=string  dtype=none       super meanabsolutepercentageerror  self    init            mean absolute percentage error  name  dtype=dtype  
class meaniou metric     string    def   init   self  num class  name=none  dtype=none       string     super meaniou  self    init   name=name  dtype=dtype      self num class = num class                self total cm = self add weight          string          shape= num class  num class           initializer=init ops zero initializer          dtype=dtypes float64     def update state self  y true  y pred  sample weight=none       string      y true = math ops cast y true  self  dtype      y pred = math ops cast y pred  self  dtype            if y pred shape ndims > 1        y pred = array ops reshape y pred   -1        if y true shape ndims > 1        y true = array ops reshape y true   -1        if sample weight be not none and sample weight shape ndims > 1        sample weight = array ops reshape sample weight   -1             current cm = confusion matrix confusion matrix          y true          y pred          self num class          weights=sample weight          dtype=dtypes float64      return self total cm assign add current cm     def result self       string     sum over row = math ops cast          math ops reduce sum self total cm  axis=0   dtype=self  dtype      sum over col = math ops cast          math ops reduce sum self total cm  axis=1   dtype=self  dtype      true positives = math ops cast          array ops diag part self total cm   dtype=self  dtype                 denominator = sum over row   sum over col - true positives                     num valid entries = math ops reduce sum          math ops cast math ops not equal denominator  0   dtype=self  dtype        iou = math ops div no nan true positives  denominator       return math ops div no nan          math ops reduce sum iou  name=string   num valid entries     def reset state self       k set value self total cm  np zero  self num class  self num class       def get config self       config =  string  self num class      base config = super meaniou  self  get config       return dict list base config items      list config items     
class meanrelativeerror mean     string    def   init   self  normalizer  name=none  dtype=none       string     super meanrelativeerror  self    init   name=name  dtype=dtype      normalizer = math ops cast normalizer  self  dtype      self normalizer = normalizer    def update state self  y true  y pred  sample weight=none       string     y true = math ops cast y true  self  dtype      y pred = math ops cast y pred  self  dtype       y pred  y true   sample weight = \         metrics utils rag assert compatible and get flat value               y pred  y true   sample weight      y pred  y true = tf losses utils squeeze or expand dimension          y pred  y true       y pred  self normalizer = confusion matrix remove squeezable dimension          y pred  self normalizer      y pred shape assert be compatible with y true shape      relative errors = math ops div no nan          math ops abs y true - y pred   self normalizer       return super meanrelativeerror  self  update state          relative errors  sample weight=sample weight     def get config self       n = self normalizer     config =  string  k eval n  if be tensor or variable n  else n      base config = super meanrelativeerror  self  get config       return dict list base config items      list config items     
class meansquarederror meanmetricwrapper     string    def   init   self  name=string  dtype=none       super meansquarederror  self    init            mean square error  name  dtype=dtype  
class meansquaredlogarithmicerror meanmetricwrapper     string    def   init   self  name=string  dtype=none       super meansquaredlogarithmicerror  self    init            mean square logarithmic error  name  dtype=dtype  
class meantensor metric     string    def   init   self  name=string  dtype=none       string     super meantensor  self    init   name=name  dtype=dtype      self  shape = none     self  total = none     self  count = none     self  build = false    def  build self  shape       self  shape = tensor shape tensorshape shape           self  total = self add weight          string  shape=shape  initializer=init ops zero initializer      self  count = self add weight          string  shape=shape  initializer=init ops zero initializer      with ops init scope          if not context execute eagerly            k  initialize variables k  get session          self  build = true     property   def total self       return self  total if self  build else none     property   def count self       return self  count if self  build else none    def update state self  value  sample weight=none       string     value = math ops cast value  self  dtype      if not self  build        self  build value shape      elif value shape  = self  shape        raise valueerror string                        string                        string format self  shape  value shape        num value = array ops ones like value      if sample weight be not none        sample weight = math ops cast sample weight  self  dtype                value     sample weight = tf losses utils squeeze or expand dimension            value  sample weight=sample weight        try                   sample weight = weight broadcast ops broadcast weight              sample weight  value        except valueerror                   ndim = k ndim value          weight ndim = k ndim sample weight          value = math ops reduce mean              value  axis=list range weight ndim  ndim           num value = math ops multiply num value  sample weight        value = math ops multiply value  sample weight       update total op = self  total assign add value      with ops control dependencies  update total op          return self  count assign add num value     def result self       if not self  build        raise valueerror            string           string                 return math ops div no nan self total  self count     def reset state self       if self  build        k batch set value              v  np zero self  shape as list     for v in self variables   
class metric base layer layer     string    def   init   self  name=none  dtype=none    kwargs       super metric  self    init   name=name  dtype=dtype    kwargs      self stateful = true       self build = true     if not base layer utils v2 dtype behavior enable                        self  dtype = k floatx   if dtype be none else dtypes as dtype dtype  name    def   new   cls   args    kwargs       obj = super metric  cls    new   cls                                if cls   module   == metric   module          update state fn = obj update state     else        update state fn = def function function obj update state       obj update state = type methodtype          metrics utils update state wrapper update state fn   obj      obj result = type methodtype metrics utils result wrapper obj result   obj      return obj    def   call   self   args    kwargs       string      def replica local fn  args    kwargs         string       update op = self update state  args    kwargs          with ops control dependencies  update op            result t = self result                                                                                      result t  metric obj = self           return result t      from tensorflow python keras distribute import distribute train utils       return distribute train utils call replica local fn          replica local fn   args    kwargs      property   def dtype self       return self  dtype    def get config self       string     return  string  self name  string  self dtype     def reset state self       string     k batch set value   v  0  for v in self variables       abc abstractmethod   def update state self   args    kwargs       string     raise notimplementederror string      abc abstractmethod   def result self       string     raise notimplementederror string         doc control for subclass implementers   def add weight self                   name                   shape=                     aggregation=tf variables variableaggregation sum                   synchronization=tf variables variablesynchronization on read                   initializer=none                   dtype=none       string     from tensorflow python distribute import distribution strategy context as ds context       from tensorflow python keras distribute import distribute train utils        if ds context have strategy          strategy = ds context get strategy       else        strategy = none           if distribute train utils be tpu strategy strategy         synchronization = tf variables variablesynchronization on write      return super metric  self  add weight          name=name          shape=shape          dtype=self  dtype if dtype be none else dtype          trainable=false          initializer=initializer          collections=            synchronization=synchronization          aggregation=aggregation  
class poisson meanmetricwrapper     string    def   init   self  name=string  dtype=none       super poisson  self    init   poisson  name  dtype=dtype  
class precision metric     string    def   init   self                 thresholds=none                 top k=none                 class id=none                 name=none                 dtype=none       string     super precision  self    init   name=name  dtype=dtype      self init thresholds = thresholds     self top k = top k     self class id = class id      default threshold = 0 5 if top k be none else metrics utils neg inf     self thresholds = metrics utils parse init thresholds          thresholds  default threshold=default threshold      self true positives = self add weight          string          shape= len self thresholds             initializer=init ops zero initializer      self false positives = self add weight          string          shape= len self thresholds             initializer=init ops zero initializer     def update state self  y true  y pred  sample weight=none       string     return metrics utils update confusion matrix variables                        metrics utils confusionmatrix true positives  self true positives              metrics utils confusionmatrix false positives  self false positives                    y true          y pred          thresholds=self thresholds          top k=self top k          class id=self class id          sample weight=sample weight     def result self       result = math ops div no nan self true positives                                   self true positives   self false positives      return result 0  if len self thresholds  == 1 else result    def reset state self       num thresholds = len to list self thresholds       k batch set value            v  np zero  num thresholds     for v in self variables      def get config self       config =           string  self init thresholds          string  self top k          string  self class id           base config = super precision  self  get config       return dict list base config items      list config items     
class precisionatrecall sensitivityspecificitybase     string    def   init   self  recall  num thresholds=200  name=none  dtype=none       string     if recall < 0 or recall > 1        raise valueerror string      self recall = recall     self num thresholds = num thresholds     super precisionatrecall  self    init            value=recall          num thresholds=num thresholds          name=name          dtype=dtype     def result self            recall = math ops div no nan          self true positives  self true positives   self false negative                 min index = math ops argmin          math ops abs recall - self value   axis=0      min index = math ops cast min index  dtypes int32            return math ops div no nan          self true positives min index           self true positives min index    self false positives min index      def get config self       config =  string  self num thresholds  string  self recall      base config = super precisionatrecall  self  get config       return dict list base config items      list config items     
class recall metric     string    def   init   self                 thresholds=none                 top k=none                 class id=none                 name=none                 dtype=none       string     super recall  self    init   name=name  dtype=dtype      self init thresholds = thresholds     self top k = top k     self class id = class id      default threshold = 0 5 if top k be none else metrics utils neg inf     self thresholds = metrics utils parse init thresholds          thresholds  default threshold=default threshold      self true positives = self add weight          string          shape= len self thresholds             initializer=init ops zero initializer      self false negative = self add weight          string          shape= len self thresholds             initializer=init ops zero initializer     def update state self  y true  y pred  sample weight=none       string     return metrics utils update confusion matrix variables                        metrics utils confusionmatrix true positives  self true positives              metrics utils confusionmatrix false negative  self false negative                    y true          y pred          thresholds=self thresholds          top k=self top k          class id=self class id          sample weight=sample weight     def result self       result = math ops div no nan self true positives                                   self true positives   self false negative      return result 0  if len self thresholds  == 1 else result    def reset state self       num thresholds = len to list self thresholds       k batch set value            v  np zero  num thresholds     for v in self variables      def get config self       config =           string  self init thresholds          string  self top k          string  self class id           base config = super recall  self  get config       return dict list base config items      list config items     
class rootmeansquarederror mean     string    def   init   self  name=string  dtype=none       super rootmeansquarederror  self    init   name  dtype=dtype     def update state self  y true  y pred  sample weight=none       string     y true = math ops cast y true  self  dtype      y pred = math ops cast y pred  self  dtype      y pred  y true = tf losses utils squeeze or expand dimension          y pred  y true      error sq = math ops square difference y pred  y true      return super rootmeansquarederror  self  update state          error sq  sample weight=sample weight     def result self       return math ops sqrt math ops div no nan self total  self count   
class sensitivityatspecificity sensitivityspecificitybase     string    def   init   self  specificity  num thresholds=200  name=none  dtype=none       string     if specificity < 0 or specificity > 1        raise valueerror string      self specificity = specificity     self num thresholds = num thresholds     super sensitivityatspecificity  self    init            specificity  num thresholds=num thresholds  name=name  dtype=dtype     def result self            specificities = math ops div no nan          self true negative  self true negative   self false positives                 min index = math ops argmin          math ops abs specificities - self value   axis=0      min index = math ops cast min index  dtypes int32            return math ops div no nan          self true positives min index           self true positives min index    self false negative min index      def get config self       config =           string  self num thresholds          string  self specificity           base config = super sensitivityatspecificity  self  get config       return dict list base config items      list config items     
class sparsecategoricalaccuracy meanmetricwrapper     string    def   init   self  name=string  dtype=none       super sparsecategoricalaccuracy  self    init            sparse categorical accuracy  name  dtype=dtype  
class sparsecategoricalcrossentropy meanmetricwrapper     string    def   init   self                 name=string                 dtype=none                 from logits=false                 axis=-1        super sparsecategoricalcrossentropy  self    init            sparse categorical crossentropy          name          dtype=dtype          from logits=from logits          axis=axis  
class sparsetopkcategoricalaccuracy meanmetricwrapper     string    def   init   self  k=5  name=string  dtype=none       string     super sparsetopkcategoricalaccuracy  self    init            sparse top k categorical accuracy  name  dtype=dtype  k=k  
class specificityatsensitivity sensitivityspecificitybase     string    def   init   self  sensitivity  num thresholds=200  name=none  dtype=none       string     if sensitivity < 0 or sensitivity > 1        raise valueerror string      self sensitivity = sensitivity     self num thresholds = num thresholds     super specificityatsensitivity  self    init            sensitivity  num thresholds=num thresholds  name=name  dtype=dtype     def result self            sensitivities = math ops div no nan          self true positives  self true positives   self false negative                 min index = math ops argmin          math ops abs sensitivities - self value   axis=0      min index = math ops cast min index  dtypes int32            return math ops div no nan          self true negative min index           self true negative min index    self false positives min index      def get config self       config =           string  self num thresholds          string  self sensitivity           base config = super specificityatsensitivity  self  get config       return dict list base config items      list config items     
class squaredhinge meanmetricwrapper     string    def   init   self  name=string  dtype=none       super squaredhinge  self    init   square hinge  name  dtype=dtype  
class sum reduce     string    def   init   self  name=string  dtype=none       string     super sum  self    init   reduction=metrics utils reduction sum                                name=name  dtype=dtype  
class topkcategoricalaccuracy meanmetricwrapper     string    def   init   self  k=5  name=string  dtype=none       string     super topkcategoricalaccuracy  self    init            top k categorical accuracy  name  dtype=dtype  k=k  
class truenegatives  confusionmatrixconditioncount     string    def   init   self  thresholds=none  name=none  dtype=none       string     super truenegatives  self    init            confusion matrix cond=metrics utils confusionmatrix true negative          thresholds=thresholds          name=name          dtype=dtype  
class truepositives  confusionmatrixconditioncount     string    def   init   self  thresholds=none  name=none  dtype=none       string     super truepositives  self    init            confusion matrix cond=metrics utils confusionmatrix true positives          thresholds=thresholds          name=name          dtype=dtype  
 keras export string  def binary accuracy y true  y pred  threshold=0 5     threshold = math ops cast threshold  y pred dtype    y pred = math ops cast y pred > threshold  y pred dtype    return k mean math ops equal y true  y pred   axis=-1  
 keras export string                string  def binary crossentropy y true  y pred  from logits=false  label smoothing=0       y pred = ops convert to tensor y pred    y true = math ops cast y true  y pred dtype    label smooth = ops convert to tensor label smooth  dtype=k floatx       def  smooth label        return y true    1 0 - label smooth    0 5   label smooth    y true = smart cond smart cond label smooth                                    smooth label  lambda  y true    return k mean        k binary crossentropy y true  y pred  from logits=from logits   axis=-1  
 keras export string  def categorical accuracy y true  y pred     return math ops cast        math ops equal            math ops argmax y true  axis=-1   math ops argmax y pred  axis=-1          k floatx    
 keras export string                string  def categorical crossentropy y true                               y pred                               from logits=false                               label smoothing=0     string   y pred = ops convert to tensor y pred    y true = math ops cast y true  y pred dtype    label smooth = ops convert to tensor label smooth  dtype=k floatx       def  smooth label        num class = math ops cast array ops shape y true  1   y pred dtype      return y true    1 0 - label smooth     label smooth / num class     y true = smart cond smart cond label smooth                                    smooth label  lambda  y true    return k categorical crossentropy y true  y pred  from logits=from logits  
 keras export string  def deserialize config  custom objects=none     return deserialize keras object        config        module objects=globals          custom objects=custom object        printable module name=string  
 keras export string  def get identifier     if isinstance identifier  dict       return deserialize identifier    elif isinstance identifier  six string type       return deserialize str identifier     elif callable identifier       return identifier   else      raise valueerror string                      string   identifier  
 keras export string  string  def hinge y true  y pred     string   y pred = ops convert to tensor y pred    y true = math ops cast y true  y pred dtype    y true =  maybe convert label y true    return k mean math ops maximum 1  - y true   y pred  0    axis=-1  
 keras export string                string                string                string                string                string  def mean absolute error y true  y pred     y pred = ops convert to tensor y pred    y true = math ops cast y true  y pred dtype    return k mean math ops abs y pred - y true   axis=-1  
 keras export string                string                string                string                string                string  def mean absolute percentage error y true  y pred       y pred = ops convert to tensor y pred    y true = math ops cast y true  y pred dtype    diff = math ops abs         y true - y pred  / k maximum math ops abs y true   k epsilon       return 100    k mean diff  axis=-1  
 keras export string                string                string                string                string                string  def mean square error y true  y pred     y pred = ops convert to tensor y pred    y true = math ops cast y true  y pred dtype    return k mean math ops square difference y pred  y true   axis=-1  
 keras export string                string                string                string                string                string  def mean square logarithmic error y true  y pred       y pred = ops convert to tensor y pred    y true = math ops cast y true  y pred dtype    first log = math ops log k maximum y pred  k epsilon      1     second log = math ops log k maximum y true  k epsilon      1     return k mean math ops square difference first log  second log   axis=-1  
 keras export string  string  def poisson y true  y pred     string   y pred = ops convert to tensor y pred    y true = math ops cast y true  y pred dtype    return k mean y pred - y true   math ops log y pred   k epsilon     axis=-1  
 keras export string  def serialize metric     return serialize keras object metric  
 keras export string  def sparse categorical accuracy y true  y pred     y pred rank = ops convert to tensor y pred  shape ndims   y true rank = ops convert to tensor y true  shape ndims      if  y true rank be not none  and  y pred rank be not none  and  len        k int shape y true   == len k int shape y pred         y true = array ops squeeze y true   -1     y pred = math ops argmax y pred  axis=-1           if k dtype y pred   = k dtype y true       y pred = math ops cast y pred  k dtype y true      return math ops cast math ops equal y true  y pred   k floatx    
 keras export string                string  def sparse categorical crossentropy y true  y pred  from logits=false  axis=-1     return k sparse categorical crossentropy        y true  y pred  from logits=from logits  axis=axis  
 keras export string  def sparse top k categorical accuracy y true  y pred  k=5     y pred rank = ops convert to tensor y pred  shape ndims   y true rank = ops convert to tensor y true  shape ndims      if  y true rank be not none  and  y pred rank be not none  and  len        k int shape y true   == len k int shape y pred         y true = array ops squeeze y true   -1      return math ops cast        nn in top k y pred  math ops cast y true  string   k   k floatx    
 keras export string  string  def square hinge y true  y pred     string   y pred = ops convert to tensor y pred    y true = math ops cast y true  y pred dtype    y true =  maybe convert label y true    return k mean        math ops square math ops maximum 1  - y true   y pred  0     axis=-1  
 keras export string  def top k categorical accuracy y true  y pred  k=5     return math ops cast        nn in top k y pred  math ops argmax y true  axis=-1   k   k floatx    
 keras export string  def clone model model  input tensors=none  clone function=none     string   if clone function be none      clone function =  clone layer    if isinstance model  sequential       return  clone sequential model          model  input tensors=input tensors  layer fn=clone function    else      return  clone functional model          model  input tensors=input tensors  layer fn=clone function  
 keras export string  def load model filepath  custom objects=none  compile=true       string   if  h5py be not none and         isinstance filepath  h5py file  or h5py be hdf5 filepath         return hdf5 format load model from hdf5 filepath  custom object  compile     if isinstance filepath  six string type       loader impl parse save model filepath      return save model load load filepath  compile     raise ioerror        string       string  
 keras export string  def model from config config  custom objects=none     string   if isinstance config  list       raise typeerror string                     string                     string    from tensorflow python keras layer import deserialize     return deserialize config  custom objects=custom object  
 keras export string  def model from json json string  custom objects=none     string   config = json load json string    from tensorflow python keras layer import deserialize     return deserialize config  custom objects=custom object  
 keras export string  def model from yaml yaml string  custom objects=none     string   if yaml be none      raise importerror string    config = yaml load yaml string    from tensorflow python keras layer import deserialize     return deserialize config  custom objects=custom object  
 keras export string  def save model model                 filepath                 overwrite=true                 include optimizer=true                 save format=none                 signatures=none                 options=none     string   from tensorflow python keras engine import sequential      default format = string if tf2 enable   else string   save format = save format or default format    if  save format == string or        h5py be not none and isinstance filepath  h5py file   or       os path splitext filepath  1  in  hdf5 extensions            if  not model  be graph network and           not isinstance model  sequential sequential          raise notimplementederror            string           string           string           string           string           string      hdf5 format save model to hdf5          model  filepath  overwrite  include optimizer    else      save model save save model  filepath  overwrite  include optimizer                            signatures  options  
class adadelta optimizer v2 optimizerv2     r   optimizer that implement the adadelta algorithm     adadelta optimization be a stochastic gradient descent method that be base on   adaptive learn rate per dimension to address two drawbacks      1  the continual decay of learn rat throughout train     2  the need for a manually select global learn rate    two accumulation step be require      1  the accumulation of gradients square      2  the accumulation of update square     initialization       e g 2  0  = 0 \text  initialize gradient 2nd order moment vector         e \delta x 2  0  = 0 \text  initialize 2nd order variable update          t  = t   1       e g 2  t  = \rho   e g 2   t-1     1 - \rho    g 2       \delta x t = -rms \delta x   t-1    g t / rms g  t       e \delta x 2  t  = \rho   e \delta x 2   t-1     1 - \rho    \delta x t 2       x t  = x  t-1    \delta x  t       reference     see  m  d  zeiler  http //arxiv org/abs/1212 5701          pdf  http //arxiv org/pdf/1212 5701v1 pdf             def   init   self                 learn rate=0 001                 rho=0 95                 epsilon=1e-7                 name=string                   kwargs       string     super adadelta  self    init   name    kwargs      self  set hyper string  kwargs get string  learn rate       self  set hyper string  self  initial decay      self  set hyper string  rho      self epsilon = epsilon or backend config epsilon      def  create slot self  var list            for v in var list        self add slot v  string      for v in var list        self add slot v  string     def  prepare local self  var device  var dtype  apply state       super adadelta  self   prepare local var device  var dtype  apply state      apply state  var device  var dtype   update dict          epsilon=ops convert to tensor self epsilon  var dtype           rho=array ops identity self  get hyper string  var dtype             def set weight self  weight       params = self weight                    if len params  == len weight    1        weight =  np array 0     weight     super adadelta  self  set weight weight     def  resource apply dense self  grad  var  apply state=none       var device  var dtype = var device  var dtype base dtype     coefficients =   apply state or     get  var device  var dtype                       or self  fallback apply state var device  var dtype        accum grad = self get slot var  string      accum var = self get slot var  string      return train ops resource apply adadelta          var handle          accum grad handle          accum var handle          coefficients string           coefficients string           coefficients string           grad          use locking=self  use lock     def  resource apply sparse self  grad  var  indices  apply state=none       var device  var dtype = var device  var dtype base dtype     coefficients =   apply state or     get  var device  var dtype                       or self  fallback apply state var device  var dtype        accum grad = self get slot var  string      accum var = self get slot var  string      return train ops resource sparse apply adadelta          var handle          accum grad handle          accum var handle          coefficients string           coefficients string           coefficients string           grad          indices          use locking=self  use lock     def get config self       config = super adadelta  self  get config       config update           string  self  serialize hyperparameter string           string  self  serialize hyperparameter string           string  self  serialize hyperparameter string           string  self epsilon             return config 
class adagrad optimizer v2 optimizerv2     r   optimizer that implement the adagrad algorithm     adagrad be an optimizer with parameter-specific learn rat    which be adapt relative to how frequently a parameter get   update during train  the more update a parameter receive    the smaller the update     initialization      accum  g 0   = \text initial accumulator value       update step      t  = t   1       accum  g t   = accum  g  t-1     g 2       \theta t  = \theta  t-1  - lr   g /  \sqrt accum  g t     \epsilon       reference        paper  http //www jmlr org/papers/volume12/duchi11a/duchi11a pdf        introduction       https //ppasupat github io/a9online/uploads/proximal note pdf            def   init   self                 learn rate=0 001                 initial accumulator value=0 1                 epsilon=1e-7                 name=string                   kwargs       string     if initial accumulator value < 0 0        raise valueerror string                          initial accumulator value      if epsilon be none        epsilon = backend config epsilon       super adagrad  self    init   name    kwargs      self  set hyper string  kwargs get string  learn rate       self  set hyper string  self  initial decay      self  initial accumulator value = initial accumulator value     self epsilon = epsilon or backend config epsilon      def  create slot self  var list       for var in var list        dtype = var dtype base dtype       init = init ops constant initializer            self  initial accumulator value  dtype=dtype        self add slot var  string  init     def  prepare local self  var device  var dtype  apply state       super adagrad  self   prepare local var device  var dtype  apply state      apply state  var device  var dtype   update dict          epsilon=ops convert to tensor self epsilon  var dtype           neg lr t=-apply state  var device  var dtype   string           zero=array ops zero     dtype=dtypes int64            def set weight self  weight       params = self weight                    if len params  == len weight    1        weight =  np array 0     weight     super adagrad  self  set weight weight      classmethod   def from config cls  config  custom objects=none       string     if string not in config        config string  = 0      if string in config        config string  = config pop string      return cls   config     def  resource apply dense self  grad  var  apply state=none       var device  var dtype = var device  var dtype base dtype     coefficients =   apply state or     get  var device  var dtype                       or self  fallback apply state var device  var dtype        acc = self get slot var  string      return train ops resource apply adagrad v2          var handle          acc handle          coefficients string           coefficients string           grad          use locking=self  use lock     def  resource apply sparse self  grad  var  indices  apply state=none       var device  var dtype = var device  var dtype base dtype     coefficients =   apply state or     get  var device  var dtype                       or self  fallback apply state var device  var dtype        acc = self get slot var  string      return train ops resource sparse apply adagrad v2          var handle          acc handle          coefficients string           coefficients string           grad          indices          use locking=self  use lock     def get config self       config = super adagrad  self  get config       config update           string  self  serialize hyperparameter string           string  self  serialize hyperparameter string           string  self  initial accumulator value          string  self epsilon             return config 
class adam optimizer v2 optimizerv2     string    def   init   self                 learn rate=0 001                 beta 1=0 9                 beta 2=0 999                 epsilon=1e-7                 amsgrad=false                 name=string                   kwargs       r   construct a new adam optimizer       if amsgrad = false        initialization           m 0  = 0 \text  initialize initial 1st moment vector             v 0  = 0 \text  initialize initial 2nd moment vector             t  = 0 \text  initialize timestep            the update rule for `variable` with gradient `g` use an optimization       describe at the end of section 2 of the paper           t  = t   1           lr t  = \text learning\ rate    \sqrt 1 - beta 2 t  /  1 - beta 1 t             m t  = beta 1   m  t-1     1 - beta 1    g           v t  = beta 2   v  t-1     1 - beta 2    g   g           variable  = variable - lr t   m t /  \sqrt v t    \epsilon         if amsgrad = true        initialization           m 0  = 0 \text  initialize initial 1st moment vector             v 0  = 0 \text  initialize initial 2nd moment vector             v hat 0  = 0 \text  initialize initial 2nd moment vector             t  = 0 \text  initialize timestep            the update rule for `variable` with gradient `g` use an optimization       describe at the end of section 2 of the paper           t  = t   1           lr t  = \text learning\ rate    \sqrt 1 - beta 2 t  /  1 - beta 1 t             m t  = beta 1   m  t-1     1 - beta 1    g           v t  = beta 2   v  t-1     1 - beta 2    g   g           v hat t  = max v hat  t-1   v t            variable  = variable - lr t   m t /  \sqrt v hat t    \epsilon         the default value of 1e-7 for epsilon might not be a good default in     general  for example  when train an inception network on imagenet a     current good choice be 1 0 or 0 1  note that since adamoptimizer use the     formulation just before section 2 1 of the kingma and ba paper rather than     the formulation in algorithm 1  the  epsilon  refer to here be  epsilon     hat  in the paper       the sparse implementation of this algorithm  use when the gradient be an     indexedslices object  typically because of `tf gather` or an embed     lookup in the forward pass  do apply momentum to variable slice even if     they be not use in the forward pass  mean they have a gradient equal     to zero   momentum decay  beta1  be also apply to the entire momentum     accumulator  this mean that the sparse behavior be equivalent to the dense     behavior  in contrast to some momentum implementations which ignore momentum     unless a variable slice be actually use        args        learn rate  a tensor or a float point value   the learn rate        beta 1  a float value or a constant float tensor  the exponential decay         rate for the 1st moment estimate        beta 2  a float value or a constant float tensor  the exponential decay         rate for the 2nd moment estimate        epsilon  a small constant for numerical stability  this epsilon be          epsilon hat  in the kingma and ba paper  in the formula just before         section 2 1   not the epsilon in algorithm 1 of the paper        amsgrad  boolean  whether to apply amsgrad variant of this algorithm from         the paper  on the convergence of adam and beyond         name  optional name for the operations create when apply gradients          default to  adam           kwargs  keyword arguments  allow to be  `clipnorm`  `clipvalue`  `lr`          `decay`   `clipnorm` be clip gradients by norm  `clipvalue` be clip         gradients by value  `decay` be include for backward compatibility to         allow time inverse decay of learn rate  `lr` be include for backward         compatibility  recommend to use `learning rate` instead        compatibility eager      when eager execution be enable  `learning rate`  `beta 1`  `beta 2`      and `epsilon` can each be a callable that take no arguments and     return the actual value to use  this can be useful for change these     value across different invocations of optimizer function       end compatibility              super adam  self    init   name    kwargs      self  set hyper string  kwargs get string  learn rate       self  set hyper string  self  initial decay      self  set hyper string  beta 1      self  set hyper string  beta 2      self epsilon = epsilon or backend config epsilon       self amsgrad = amsgrad    def  create slot self  var list                 for var in var list        self add slot var  string      for var in var list        self add slot var  string      if self amsgrad        for var in var list          self add slot var  string     def  prepare local self  var device  var dtype  apply state       super adam  self   prepare local var device  var dtype  apply state       local step = math ops cast self iterations   1  var dtype      beta 1 t = array ops identity self  get hyper string  var dtype       beta 2 t = array ops identity self  get hyper string  var dtype       beta 1 power = math ops pow beta 1 t  local step      beta 2 power = math ops pow beta 2 t  local step      lr =  apply state  var device  var dtype   string               math ops sqrt 1 - beta 2 power  /  1 - beta 1 power        apply state  var device  var dtype   update dict          lr=lr          epsilon=ops convert to tensor self epsilon  var dtype           beta 1 t=beta 1 t          beta 1 power=beta 1 power          one minus beta 1 t=1 - beta 1 t          beta 2 t=beta 2 t          beta 2 power=beta 2 power          one minus beta 2 t=1 - beta 2 t           def set weight self  weight       params = self weight                    num vars = int  len params  - 1  / 2      if len weight  == 3   num vars   1        weight = weight  len params       super adam  self  set weight weight     def  resource apply dense self  grad  var  apply state=none       var device  var dtype = var device  var dtype base dtype     coefficients =   apply state or     get  var device  var dtype                       or self  fallback apply state var device  var dtype        m = self get slot var  string      v = self get slot var  string       if not self amsgrad        return train ops resource apply adam            var handle            m handle            v handle            coefficients string             coefficients string             coefficients string             coefficients string             coefficients string             coefficients string             grad            use locking=self  use lock      else        vhat = self get slot var  string        return train ops resource apply adam with amsgrad            var handle            m handle            v handle            vhat handle            coefficients string             coefficients string             coefficients string             coefficients string             coefficients string             coefficients string             grad            use locking=self  use lock     def  resource apply sparse self  grad  var  indices  apply state=none       var device  var dtype = var device  var dtype base dtype     coefficients =   apply state or     get  var device  var dtype                       or self  fallback apply state var device  var dtype             m = self get slot var  string      m scale g value = grad   coefficients string      m t = state ops assign m  m   coefficients string                              use locking=self  use lock      with ops control dependencies  m t          m t = self  resource scatter add m  indices  m scale g value            v = self get slot var  string      v scale g value =  grad   grad    coefficients string      v t = state ops assign v  v   coefficients string                              use locking=self  use lock      with ops control dependencies  v t          v t = self  resource scatter add v  indices  v scale g value       if not self amsgrad        v sqrt = math ops sqrt v t        var update = state ops assign sub            var  coefficients string    m t /  v sqrt   coefficients string              use locking=self  use lock        return control flow ops group   var update  m t  v t       else        v hat = self get slot var  string        v hat t = math ops maximum v hat  v t        with ops control dependencies  v hat t            v hat t = state ops assign              v hat  v hat t  use locking=self  use lock        v hat sqrt = math ops sqrt v hat t        var update = state ops assign sub            var            coefficients string    m t /  v hat sqrt   coefficients string              use locking=self  use lock        return control flow ops group   var update  m t  v t  v hat t      def get config self       config = super adam  self  get config       config update           string  self  serialize hyperparameter string           string  self  serialize hyperparameter string           string  self  serialize hyperparameter string           string  self  serialize hyperparameter string           string  self epsilon          string  self amsgrad             return config 
class adamax optimizer v2 optimizerv2     string    def   init   self                 learn rate=0 001                 beta 1=0 9                 beta 2=0 999                 epsilon=1e-7                 name=string                   kwargs       string     super adamax  self    init   name    kwargs      self  set hyper string  kwargs get string  learn rate       self  set hyper string  self  initial decay      self  set hyper string  beta 1      self  set hyper string  beta 2      self epsilon = epsilon or backend config epsilon      def  create slot self  var list            for var in var list        self add slot var  string        for var in var list        self add slot var  string       def  prepare local self  var device  var dtype  apply state       super adamax  self   prepare local var device  var dtype  apply state       local step = math ops cast self iterations   1  var dtype      beta 1 t = array ops identity self  get hyper string  var dtype       beta 2 t = array ops identity self  get hyper string  var dtype       beta 1 power = math ops pow beta 1 t  local step      lr t = apply state  var device  var dtype   string       apply state  var device  var dtype   update dict          neg scale lr=-lr t /  1 - beta 1 power           epsilon=ops convert to tensor self epsilon  var dtype           beta 1 t=beta 1 t          beta 1 power=beta 1 power          one minus beta 1 t=1 - beta 1 t          beta 2 t=beta 2 t          zero=array ops zero     dtype=dtypes int64            def  resource apply dense self  grad  var  apply state=none       var device  var dtype = var device  var dtype base dtype     coefficients =   apply state or     get  var device  var dtype                       or self  fallback apply state var device  var dtype        m = self get slot var  string      v = self get slot var  string       return train ops resource apply ada max          var handle          m handle          v handle          coefficients string           coefficients string           coefficients string           coefficients string           coefficients string           grad          use locking=self  use lock     def  resource apply sparse self  grad  var  indices  apply state=none       var device  var dtype = var device  var dtype base dtype     coefficients =   apply state or     get  var device  var dtype                       or self  fallback apply state var device  var dtype             m = self get slot var  string      m slice = array ops gather m  indices  axis=coefficients string       m t slice =  m slice   coefficients string                     grad   coefficients string       with ops control dependencies  m t slice          m t = self  resource scatter update m  indices  m t slice            v = self get slot var  string      v slice = array ops gather v  indices  axis=coefficients string       v t slice = math ops maximum v slice   coefficients string                                    math ops abs grad       with ops control dependencies  v t slice          v t = self  resource scatter update v  indices  v t slice           var slice = coefficients string              m t slice /  v t slice   coefficients string        with ops control dependencies  var slice          var update = self  resource scatter add var  indices  var slice      return control flow ops group   var update  m t  v t      def get config self       config = super adamax  self  get config       config update           string  self  serialize hyperparameter string           string  self  serialize hyperparameter string           string  self  serialize hyperparameter string           string  self  serialize hyperparameter string           string  self epsilon             return config 
class ftrl optimizer v2 optimizerv2     r   optimizer that implement the ftrl algorithm     see algorithm 1 of this  paper     https //www eecs tufts edu/~dsculley/papers/ad-click-prediction pdf     this version have support for both online l2  the l2 penalty give in the paper   above  and shrinkage-type l2  which be the addition of an l2 penalty to the   loss function      initialization      t = 0       n  0  = 0       \sigma  0  = 0       z  0  = 0      update    i   be variable index       t = t   1       n  t i  = n  t-1 i    g  t i   2        \sigma  t i  =  \sqrt n  t i   - \sqrt n  t-1 i    / \alpha       z  t i  = z  t-1 i    g  t i  - \sigma  t i    w  t i        w  t i  = -   \beta \sqrt n  t    / \alpha   \lambda  2    -1     z  i  -                sgn z  i     \lambda  1   if \abs z  i   > \lambda  i  else 0      check the documentation for the l2 shrinkage regularization strength   parameter for more detail when shrinkage be enable  where gradient be   replace with gradient with shrinkage           def   init   self                 learn rate=0 001                 learn rate power=-0 5                 initial accumulator value=0 1                 l1 regularization strength=0 0                 l2 regularization strength=0 0                 name=string                 l2 shrinkage regularization strength=0 0                   kwargs       r   construct a new ftrl optimizer       args        learn rate  a float value or a constant float `tensor`        learn rate power  a float value  must be less or equal to zero          control how the learn rate decrease during train  use zero for         a fix learn rate        initial accumulator value  the start value for accumulators          only zero or positive value be allow        l1 regularization strength  a float value  must be greater than or         equal to zero        l2 regularization strength  a float value  must be greater than or         equal to zero        name  optional name prefix for the operations create when apply         gradients   default to  ftrl         l2 shrinkage regularization strength  a float value  must be greater than         or equal to zero  this differ from l2 above in that the l2 above be a         stabilization penalty  whereas this l2 shrinkage be a magnitude penalty          the ftrl formulation can be write as          w  t 1  = argmin w \hat g   1 t w   l1   w   1   l2   w   2 2   where         \hat g  = g    2 l2 shrinkage w   and g be the gradient of the loss         function w r t  the weight w          specifically  in the absence of l1 regularization  it be equivalent to         the follow update rule          w  t 1  = w t - lr t /  1   2 l2 lr t    g t -                   2 l2 shrinkage lr t /  1   2 l2 lr t    w t         where lr t be the learn rate at t          when input be sparse shrinkage will only happen on the active weight \         kwargs  keyword arguments  allow to be  `clipnorm`  `clipvalue`  `lr`          `decay`   `clipnorm` be clip gradients by norm  `clipvalue` be clip         gradients by value  `decay` be include for backward compatibility to         allow time inverse decay of learn rate  `lr` be include for backward         compatibility  recommend to use `learning rate` instead       raise        valueerror  if one of the arguments be invalid       reference       see  paper           https //www eecs tufts edu/~dsculley/papers/ad-click-prediction pdf              super ftrl  self    init   name    kwargs       if initial accumulator value < 0 0        raise valueerror            string             initial accumulator value      if learn rate power > 0 0        raise valueerror string                          learn rate power      if l1 regularization strength < 0 0        raise valueerror            string             l1 regularization strength      if l2 regularization strength < 0 0        raise valueerror            string             l2 regularization strength      if l2 shrinkage regularization strength < 0 0        raise valueerror            string           string   l2 shrinkage regularization strength       self  set hyper string  learn rate      self  set hyper string  self  initial decay      self  set hyper string  learn rate power      self  set hyper string  l1 regularization strength      self  set hyper string  l2 regularization strength      self  initial accumulator value = initial accumulator value     self  l2 shrinkage regularization strength =           l2 shrinkage regularization strength     def  create slot self  var list            for var in var list        dtype = var dtype base dtype       init = init ops constant initializer            self  initial accumulator value  dtype=dtype        self add slot var  string  init        self add slot var  string     def  prepare local self  var device  var dtype  apply state       super ftrl  self   prepare local var device  var dtype  apply state      apply state  var device  var dtype   update dict          learn rate power=array ops identity              self  get hyper string  var dtype            l1 regularization strength=array ops identity              self  get hyper string  var dtype            l2 regularization strength=array ops identity              self  get hyper string  var dtype            l2 shrinkage regularization strength=math ops cast              self  l2 shrinkage regularization strength  var dtype                def  resource apply dense self  grad  var  apply state=none       var device  var dtype = var device  var dtype base dtype     coefficients =   apply state or     get  var device  var dtype                       or self  fallback apply state var device  var dtype        accum = self get slot var  string      linear = self get slot var  string       if self  l2 shrinkage regularization strength <= 0 0        return train ops resource apply ftrl            var handle            accum handle            linear handle            grad            coefficients string             coefficients string             coefficients string             coefficients string             use locking=self  use lock      else        return train ops resource apply ftrl v2            var handle            accum handle            linear handle            grad            coefficients string             coefficients string             coefficients string             coefficients string             coefficients string             use locking=self  use lock     def  resource apply sparse self  grad  var  indices  apply state=none       var device  var dtype = var device  var dtype base dtype     coefficients =   apply state or     get  var device  var dtype                       or self  fallback apply state var device  var dtype        accum = self get slot var  string      linear = self get slot var  string       if self  l2 shrinkage regularization strength <= 0 0        return train ops resource sparse apply ftrl            var handle            accum handle            linear handle            grad            indices            coefficients string             coefficients string             coefficients string             coefficients string             use locking=self  use lock      else        return train ops resource sparse apply ftrl v2            var handle            accum handle            linear handle            grad            indices            coefficients string             coefficients string             coefficients string             coefficients string             coefficients string             use locking=self  use lock     def get config self       config = super ftrl  self  get config       config update           string              self  serialize hyperparameter string           string              self  serialize hyperparameter string           string              self  initial accumulator value          string              self  serialize hyperparameter string           string              self  serialize hyperparameter string           string              self  serialize hyperparameter string           string              self  l2 shrinkage regularization strength             return config 
class nadam optimizer v2 optimizerv2     r   optimizer that implement the nadam algorithm     much like adam be essentially rmsprop with momentum  nadam be adam with   nesterov momentum     initialization       m 0  = 0 \text  initialize 1st moment vector         v 0  = 0 \text  initialize 2nd moment vector         mu 0  = 1       t  = 0 \text  initialize timestep        compute      t  = t   1       \mu t  = \beta 1    1 - 0 5   0 96  0 004   t         g   = g /  1 - \prod  i=1   t  \mu i         m t  = \beta 1   m  t-1     1 - \beta 1    g       m   = m t /  1 - \prod  i=1   t 1  \mu i         v t  = \beta 2   v  t-1     1 - \beta 2    g   g       v   = v t /  1 - \beta 2 t        \bar m   =  1 - \mu t    g    \mu  t 1    m        \theta t  = \theta  t-1  - lr   \bar m  /  \sqrt v     \epsilon       gradient be evaluate at theta t    momentum   v t   and the variables always   store theta   beta 1   m / sqrt v  instead of theta     reference     see  dozat  t   2015  http //cs229 stanford edu/proj2015/054 report pdf            def   init   self                 learn rate=0 001                 beta 1=0 9                 beta 2=0 999                 epsilon=1e-7                 name=string                   kwargs       string           kwargs string  = kwargs pop string  0 004      learn rate = kwargs get string  learn rate      if isinstance learn rate  learn rate schedule learningrateschedule         raise valueerror string                        string                        string       super nadam  self    init   name    kwargs      self  set hyper string  kwargs get string  learn rate       self  set hyper string  self  initial decay      self  set hyper string  beta 1      self  set hyper string  beta 2      self epsilon = epsilon or backend config epsilon       self  m cache = none    def  create slot self  var list       var dtype = var list 0  dtype base dtype     if self  m cache be none        self  m cache = self add weight            string            shape=              dtype=var dtype            initializer=string            trainable=false            aggregation=tf variables variableaggregation only first replica        self  weight append self  m cache           for var in var list               self add slot var  string      for var in var list               self add slot var  string     def  prepare local self  var device  var dtype  apply state       lr t = array ops identity self  get hyper string  var dtype       beta 1 t = array ops identity self  get hyper string  var dtype       beta 2 t = array ops identity self  get hyper string  var dtype       local step = math ops cast self iterations   1  var dtype      next step = math ops cast self iterations   2  var dtype       decay base = math ops cast 0 96  var dtype       m t = beta 1 t    1  - 0 5             math ops pow decay base  self  initial decay   local step        m t 1 = beta 1 t    1  - 0 5             math ops pow decay base  self  initial decay   next step         m schedule new = math ops cast self  m cache read  var dtype    m t     if var dtype be self  m cache dtype        m schedule new = array ops identity state ops assign            self  m cache  m schedule new  use locking=self  use lock       m schedule next = m schedule new   m t 1      apply state  var device  var dtype   = dict          lr t=lr t          neg lr t=-lr t          epsilon=ops convert to tensor self epsilon  var dtype           beta 1 t=beta 1 t          beta 2 t=beta 2 t          m t=m t          m t 1=m t 1           one minus beta 1 t=1 - beta 1 t          one minus beta 2 t=1 - beta 2 t          one minus m t=1  - m t          one minus m schedule new=1  - m schedule new          one minus m schedule next=1  - m schedule next          v t prime denominator=1  - math ops pow beta 2 t  local step            def  prepare self  var list            self  m cache read = array ops identity self  m cache      return super nadam  self   prepare var list     def  resource apply dense self  grad  var  apply state=none       var device  var dtype = var device  var dtype base dtype     coefficients =   apply state or     get  var device  var dtype                       or self  fallback apply state var device  var dtype        m = self get slot var  string      v = self get slot var  string       g prime = grad / coefficients string      m t =  coefficients string    m              coefficients string    grad      m t = state ops assign m  m t  use locking=self  use lock      m t prime = m t / coefficients string      v t =  coefficients string    v              coefficients string    math ops square grad       v t = state ops assign v  v t  use locking=self  use lock      v t prime = v t / coefficients string      m t bar =  coefficients string    g prime                  coefficients string    m t prime      var t = var - coefficients string    m t bar /           math ops sqrt v t prime    coefficients string       return state ops assign var  var t  use locking=self  use lock  op    def  resource apply sparse self  grad  var  indices  apply state=none       var device  var dtype = var device  var dtype base dtype     coefficients =   apply state or     get  var device  var dtype                       or self  fallback apply state var device  var dtype        m = self get slot var  string      v = self get slot var  string       g prime = grad / coefficients string            m scale g value = grad   coefficients string      m t = state ops assign m  m   coefficients string                              use locking=self  use lock       with ops control dependencies  m t          m t = self  resource scatter add m  indices  m scale g value        m t slice = array ops gather m t  indices       m t prime = m t slice / coefficients string      m t bar =  coefficients string    g prime                  coefficients string    m t prime            v scale g value =  grad   grad    coefficients string      v t = state ops assign v  v   coefficients string                              use locking=self  use lock       with ops control dependencies  v t          v t = self  resource scatter add v  indices  v scale g value        v t slice = array ops gather v t  indices       v t prime = v t slice / coefficients string      v prime sqrt plus eps = math ops sqrt v t prime    coefficients string       var update = self  resource scatter add          var  indices          coefficients string    m t bar / v prime sqrt plus eps      return control flow ops group   var update  m t bar  v t      def get config self       config = super nadam  self  get config       config update           string  self  serialize hyperparameter string           string  self  serialize hyperparameter string           string  self  serialize hyperparameter string           string  self  serialize hyperparameter string           string  self epsilon             return config 
class optimizerv2 trackable trackable     string    def   init   self  name    kwargs       string     allow kwargs =  string  string  string  string      for k in kwargs        if k not in allow kwargs          raise typeerror string                         string   str k                if kwargs k  < 0          raise valueerror string format k  kwargs k         self  use lock = true     self  init set name name      self  hyper =             self  slot =        self  slot name =        self  weight =        self  iterations = none                                    self  defer slot restorations =         decay = kwargs pop string  0 0      if decay < 0         raise valueerror string format decay       self  initial decay = decay     if string in kwargs        self clipnorm = kwargs pop string      if string in kwargs        self clipvalue = kwargs pop string       self  hypers create = false    def minimize self  loss  var list  grad loss=none  name=none       string     grads and vars = self  compute gradients          loss  var list=var list  grad loss=grad loss       return self apply gradients grads and vars  name=name     def  compute gradients self  loss  var list  grad loss=none       string          with backprop gradienttape   as tape        if not callable var list           tape watch var list        loss value = loss       if callable var list         var list = var list       var list = nest flatten var list      with backend name scope self  name   string         grads = tape gradient loss value  var list  grad loss         if hasattr self  string           grads =  clip ops clip by norm g  self clipnorm  for g in grads        if hasattr self  string           grads =               clip ops clip by value g  -self clipvalue  self clipvalue              for g in grads                grads and vars = list zip grads  var list       self  assert valid dtypes           v for g  v in grads and vars         if g be not none and v dtype  = dtypes resource             return grads and vars    def get gradients self  loss  params       string     params = nest flatten params      with backend get graph   as default    backend name scope self  name                                                                 string         grads = gradients gradients loss  params        for grad  param in zip grads  params           if grad be none            raise valueerror string                            string                            string                            string                            string format param         if hasattr self  string           grads =  clip ops clip by norm g  self clipnorm  for g in grads        if hasattr self  string           grads =               clip ops clip by value g  -self clipvalue  self clipvalue              for g in grads               return grads    def apply gradients self  grads and vars  name=none       string     grads and vars =  filter grads grads and vars      var list =  v for     v  in grads and vars       with backend name scope self  name                with ops init scope              = self iterations         self  create hypers           self  create slot var list         if not grads and vars                            return control flow ops no op         apply state = self  prepare var list        return distribute ctx get replica context   merge call            functools partial self  distribute apply  apply state=apply state             args= grads and vars              kwargs= string  name      def  distribute apply self  distribution  grads and vars  name  apply state       string     reduce grads = distribution extend batch reduce to          ds reduce util reduceop sum  grads and vars      var list =  v for    v in grads and vars      grads and vars = zip reduce grads  var list       def apply grad to update var var  grad         string       if isinstance var  ops tensor           raise notimplementederror string  var         apply kwargs =          if isinstance grad  ops indexedslices           if var constraint be not none            raise runtimeerror                string          if string in self  sparse apply args            apply kwargs string  = apply state         return self  resource apply sparse duplicate indices              grad value  var  grad indices    apply kwargs         if string in self  dense apply args          apply kwargs string  = apply state       update op = self  resource apply dense grad  var    apply kwargs        if var constraint be not none          with ops control dependencies  update op              return var assign var constraint var         else          return update op      update ops =        with backend name scope name or self  name         for grad  var in grads and vars          scope name =  string if ops execute eagerly outside function   else                       string   var op name                            with backend name scope              scope name   distribution extend colocate vars with var             update ops extend                distribution extend update                    var  apply grad to update var  args= grad    group=false          any symbolic = any isinstance i  ops operation  or                          tf utils be symbolic tensor i  for i in update ops        if not context execute eagerly   or any symbolic                                     with ops  get graph from input update ops  as default                with ops control dependencies update ops               return self  iterations assign add 1  op        return self  iterations assign add 1     def get update self  loss  params       grads = self get gradients loss  params      grads and vars = list zip grads  params       self  assert valid dtypes           v for g  v in grads and vars         if g be not none and v dtype  = dtypes resource            return  self apply gradients grads and vars      def  set hyper self  name  value       string     if isinstance value  trackable trackable         self  track trackable value  name  overwrite=true      if name not in self  hyper        self  hyper name  = value     else        prev value = self  hyper name        if  callable prev value            or isinstance prev value                           ops tensor  int  float                           learn rate schedule learningrateschedule             or isinstance value  learn rate schedule learningrateschedule            self  hyper name  = value       else          backend set value self  hyper name   value     def  get hyper self  name  dtype=none       if not self  hypers create        self  create hypers       value = self  hyper name      if isinstance value  learn rate schedule learningrateschedule         return value     if callable value         value = value       if dtype        return math ops cast value  dtype      else        return value    def   getattribute   self  name       string     try        return super optimizerv2  self    getattribute   name      except attributeerror as e               if name == string          raise e              if name == string          name = string       if name in self  hyper          return self  get hyper name        raise e    def   setattr   self  name  value       string          if name == string        name = string     if hasattr self  string  and name in self  hyper        self  set hyper name  value      else        super optimizerv2  self    setattr   name  value     def get slot name self       string     return self  slot name    def add slot self  var  slot name  initializer=string       string     if slot name not in self  slot name        self  slot name append slot name      var key =  var key var      slot dict = self  slot setdefault var key          weight = slot dict get slot name  none      if weight be none        if isinstance initializer  six string type  or callable initializer           initializer = initializers get initializer          initial value = functools partial              initializer  shape=var shape  dtype=var dtype        else          initial value = initializer       strategy = distribute ctx get strategy         if not strategy extend variable create in scope var           raise valueerror              string             string             string             string             string              format strategy  var          with strategy extend colocate vars with var           weight = tf variables variable              name=string    var  share name  slot name                 dtype=var dtype              trainable=false              initial value=initial value        backend track variable weight        slot dict slot name  = weight       self  restore slot variable            slot name=slot name  variable=var            slot variable=weight        self  weight append weight      return weight    def get slot self  var  slot name       var key =  var key var      slot dict = self  slot var key      return slot dict slot name     def  prepare self  var list       key = set       for var in var list        var devices =  getattr var  string  none  or                         var device                              var dtype = var dtype base dtype       for var device in var devices          key add  var device  var dtype        apply state =        for var device  var dtype in key        apply state  var device  var dtype   =          with ops device var device           self  prepare local var device  var dtype  apply state       return apply state    def  prepare local self  var device  var dtype  apply state       if string in self  hyper        lr t = array ops identity self  decay lr var dtype         apply state  var device  var dtype   string  = lr t    def  fallback apply state self  var device  var dtype       string     apply state =   var device  var dtype           self  prepare local var device  var dtype  apply state      return apply state  var device  var dtype      def  create hypers self       if self  hypers create        return          for name  value in sort self  hyper items           if isinstance            value   ops tensor  tf variables variable   or callable value           continue       else          self  hyper name  = self add weight              name              shape=                trainable=false              initializer=value              aggregation=tf variables variableaggregation only first replica      self  hypers create = true     property   def iterations self       string     if self  iterations be none        self  iterations = self add weight            string            shape=              dtype=dtypes int64            trainable=false            aggregation=tf variables variableaggregation only first replica        self  weight append self  iterations      return self  iterations     iterations setter   def iterations self  variable       if self  iterations be not none        raise runtimeerror string                          string      self  iterations = variable     self  weight append self  iterations     def  decay lr self  var dtype       string     lr t = self  get hyper string  var dtype      if isinstance lr t  learn rate schedule learningrateschedule         local step = math ops cast self iterations  var dtype        lr t = math ops cast lr t local step   var dtype      if self  initial decay > 0         local step = math ops cast self iterations  var dtype        decay t = self  get hyper string  var dtype        lr t = lr t /  1    decay t   local step      return lr t     abc abstractmethod   def get config self       string     config =  string  self  name      if hasattr self  string         config string  = self clipnorm     if hasattr self  string         config string  = self clipvalue     return config     classmethod   def from config cls  config  custom objects=none       string     if string in config        config string  = config pop string      if string in config        if isinstance config string   dict           config string  = learn rate schedule deserialize              config string   custom objects=custom object      return cls   config     def  serialize hyperparameter self  hyperparameter name       string     value = self  hyper hyperparameter name      if isinstance value  learn rate schedule learningrateschedule         return learn rate schedule serialize value      if callable value         return value       if tensor util be tensor value         return backend get value value      return value    def variables self       string     return self  weight     property   def weight self       string     return self  weight    def get weight self       params = self weight     return backend batch get value params        def set weight self  weight       params = self weight     if len params   = len weight         raise valueerror            string   self  name             string   str len weight               string   str len params               string   str weight   50    string      if not params        return     weight value tuples =        param value = backend batch get value params      for pv  p  w in zip param value  params  weight         if pv shape  = w shape          raise valueerror string   str pv shape                             string                          string   str w shape         weight value tuples append  p  w       backend batch set value weight value tuples     def add weight self                   name                   shape                   dtype=none                   initializer=string                   trainable=none                   synchronization=tf variables variablesynchronization auto                   aggregation=tf variables variableaggregation none        if dtype be none        dtype = dtypes float32     if isinstance initializer  six string type  or callable initializer         initializer = initializers get initializer       if synchronization == tf variables variablesynchronization on read        if trainable          raise valueerror              string             string             string             string        else                   trainable = false     elif trainable be none        trainable = true      variable = self  add variable with custom getter          name=name          shape=shape          getter=base layer utils make variable          overwrite=true          initializer=initializer          dtype=dtype          trainable=trainable          use resource=true          synchronization=synchronization          aggregation=aggregation      backend track variable variable       return variable    def  init set name self  name  zero based=true       if not name        self  name = backend unique object name            generic utils to snake case self   class     name               zero based=zero base      else        self  name = name    def  assert valid dtypes self  tensors       string     valid dtypes = self  valid dtypes       for t in tensors        dtype = t dtype base dtype       if dtype not in valid dtypes          raise valueerror string                             dtype  t name   v for v in valid dtypes       def  valid dtypes self       string     return set           dtypes float16  dtypes bfloat16  dtypes float32  dtypes float64          dtypes complex64  dtypes complex128           def  call if callable self  param       string     return param   if callable param  else param    def  resource apply dense self  grad  handle  apply state       string     raise notimplementederror      def  resource apply sparse duplicate indices self  grad  handle  indices                                                   kwargs       string     sum grad  unique indices =  deduplicate index slice          values=grad  indices=indices      return self  resource apply sparse sum grad  handle  unique indices                                           kwargs     def  resource apply sparse self  grad  handle  indices  apply state       string     raise notimplementederror      def  resource scatter add self  x  i  v       with ops control dependencies           resource variable ops resource scatter add x handle  i  v           return x value      def  resource scatter update self  x  i  v       with ops control dependencies           resource variable ops resource scatter update x handle  i  v           return x value       property    track cache per instance   def  dense apply args self       return tf inspect getfullargspec self  resource apply dense  args     property    track cache per instance   def  sparse apply args self       return tf inspect getfullargspec self  resource apply sparse  args              def  restore slot variable self  slot name  variable  slot variable       string     variable key =  var key variable      defer restorations = self  defer slot restorations get          slot name      pop variable key                    defer restorations sort key=lambda position  position restore uid                                 reverse=true      for checkpoint position in defer restorations        checkpoint position restore slot variable     def  create or restore slot variable        self  slot variable position  slot name  variable       string     variable key =  var key variable      slot dict = self  slot get variable key          slot variable = slot dict get slot name  none      if  slot variable be none and context execute eagerly   and         slot variable position be simple variable                                                                                   and not ops get default graph    variable creator stack           initializer = trackable checkpointinitialvalue            checkpoint position=slot variable position        slot variable = self add slot            var=variable            initializer=initializer            slot name=slot name                                                       if slot variable be not none                      slot variable position restore slot variable      else                                    self  defer slot restorations setdefault            slot name      setdefault variable key      append                slot variable position  
class rmsprop optimizer v2 optimizerv2     r   optimizer that implement the rmsprop algorithm     a detail description of rmsprop       - maintain a move  discount  average of the square of gradients     - divide gradient by the root of this average      mean square t = rho   mean square t-1     1-rho    gradient    2       mom t = momentum   mom  t-1    learn rate   gradient / \sqrt  /       mean square t   \epsilon        variable t  = variable  t-1  - mom t      this implementation of rmsprop use plain momentum  not nesterov momentum     the center version additionally maintain a move average of the   gradients  and use that average to estimate the variance       mean grad t = rho   mean grad  t-1     1-rho    gradient       mean square t = rho   mean square  t-1     1-rho    gradient    2       mom t = momentum   mom  t-1    learn rate   gradient /       sqrt mean square t - mean grad t  2   epsilon        variable t  = variable  t-1  - mom t      reference     see   pdf        http //www cs toronto edu/~tijmen/csc321/slides/lecture slide lec6 pdf            def   init   self                 learn rate=0 001                 rho=0 9                 momentum=0 0                 epsilon=1e-7                 centered=false                 name=string                   kwargs       string     super rmsprop  self    init   name    kwargs      self  set hyper string  kwargs get string  learn rate       self  set hyper string  self  initial decay      self  set hyper string  rho       self  momentum = false     if isinstance momentum  ops tensor  or callable momentum  or momentum > 0        self  momentum = true     if isinstance momentum   int  float   and  momentum < 0 or momentum > 1         raise valueerror string      self  set hyper string  momentum       self epsilon = epsilon or backend config epsilon       self center = center    def  create slot self  var list       for var in var list        self add slot var  string      if self  momentum        for var in var list          self add slot var  string      if self center        for var in var list          self add slot var  string     def  prepare local self  var device  var dtype  apply state       super rmsprop  self   prepare local var device  var dtype  apply state       rho = array ops identity self  get hyper string  var dtype       apply state  var device  var dtype   update dict          neg lr t=-apply state  var device  var dtype   string           epsilon=ops convert to tensor self epsilon  var dtype           rho=rho          momentum=array ops identity self  get hyper string  var dtype            one minus rho=1  - rho           def  resource apply dense self  grad  var  apply state=none       var device  var dtype = var device  var dtype base dtype     coefficients =   apply state or     get  var device  var dtype                       or self  fallback apply state var device  var dtype        rms = self get slot var  string      if self  momentum        mom = self get slot var  string        if self center          mg = self get slot var  string          return train ops resource apply center rms prop              var handle              mg handle              rms handle              mom handle              coefficients string               coefficients string               coefficients string               coefficients string               grad              use locking=self  use lock        else          return train ops resource apply rms prop              var handle              rms handle              mom handle              coefficients string               coefficients string               coefficients string               coefficients string               grad              use locking=self  use lock      else        rms t =  coefficients string    rms                  coefficients string    math ops square grad         rms t = state ops assign rms  rms t  use locking=self  use lock        denom t = rms t       if self center          mg = self get slot var  string          mg t = coefficients string    mg   coefficients string    grad         mg t = state ops assign mg  mg t  use locking=self  use lock          denom t = rms t - math ops square mg t        var t = var - coefficients string    grad /             math ops sqrt denom t    coefficients string         return state ops assign var  var t  use locking=self  use lock  op    def  resource apply sparse self  grad  var  indices  apply state=none       var device  var dtype = var device  var dtype base dtype     coefficients =   apply state or     get  var device  var dtype                       or self  fallback apply state var device  var dtype        rms = self get slot var  string      if self  momentum        mom = self get slot var  string        if self center          mg = self get slot var  string          return train ops resource sparse apply center rms prop              var handle              mg handle              rms handle              mom handle              coefficients string               coefficients string               coefficients string               coefficients string               grad              indices              use locking=self  use lock        else          return train ops resource sparse apply rms prop              var handle              rms handle              mom handle              coefficients string               coefficients string               coefficients string               coefficients string               grad              indices              use locking=self  use lock      else        rms scale g value =  grad   grad    coefficients string        rms t = state ops assign rms  rms   coefficients string                                  use locking=self  use lock        with ops control dependencies  rms t            rms t = self  resource scatter add rms  indices  rms scale g value          rms slice = array ops gather rms t  indices        denom slice = rms slice       if self center          mg = self get slot var  string          mg scale g value = grad   coefficients string          mg t = state ops assign mg  mg   coefficients string                                   use locking=self  use lock          with ops control dependencies  mg t              mg t = self  resource scatter add mg  indices  mg scale g value            mg slice = array ops gather mg t  indices            denom slice = rms slice - math ops square mg slice        var update = self  resource scatter add            var  indices  coefficients string    grad /                 math ops sqrt denom slice    coefficients string          if self center          return control flow ops group   var update  rms t  mg t         return control flow ops group   var update  rms t      def set weight self  weight       params = self weight                    if len params  == len weight    1        weight =  np array 0     weight     super rmsprop  self  set weight weight     def get config self       config = super rmsprop  self  get config       config update           string  self  serialize hyperparameter string           string  self  serialize hyperparameter string           string  self  serialize hyperparameter string           string  self  serialize hyperparameter string           string  self epsilon          string  self center             return config 
class sgd optimizer v2 optimizerv2     string    def   init   self                 learn rate=0 01                 momentum=0 0                 nesterov=false                 name=string                   kwargs       string     super sgd  self    init   name    kwargs      self  set hyper string  kwargs get string  learn rate       self  set hyper string  self  initial decay       self  momentum = false     if isinstance momentum  ops tensor  or callable momentum  or momentum > 0        self  momentum = true     if isinstance momentum   int  float   and  momentum < 0 or momentum > 1         raise valueerror string      self  set hyper string  momentum       self nesterov = nesterov    def  create slot self  var list       if self  momentum        for var in var list          self add slot var  string     def  prepare local self  var device  var dtype  apply state       super sgd  self   prepare local var device  var dtype  apply state      apply state  var device  var dtype   string  = array ops identity          self  get hyper string  var dtype      def  resource apply dense self  grad  var  apply state=none       var device  var dtype = var device  var dtype base dtype     coefficients =   apply state or     get  var device  var dtype                       or self  fallback apply state var device  var dtype        if self  momentum        momentum var = self get slot var  string        return train ops resource apply keras momentum            var handle            momentum var handle            coefficients string             grad            coefficients string             use locking=self  use lock            use nesterov=self nesterov      else        return train ops resource apply gradient descent            var handle  coefficients string   grad  use locking=self  use lock     def  resource apply sparse duplicate indices self  grad  var  indices                                                   kwargs       if self  momentum        return super sgd  self   resource apply sparse duplicate indices            grad  var  indices    kwargs      else        var device  var dtype = var device  var dtype base dtype       coefficients =  kwargs get string      get  var device  var dtype                         or self  fallback apply state var device  var dtype          return resource variable ops resource scatter add            var handle  indices  -grad   coefficients string      def  resource apply sparse self  grad  var  indices  apply state=none            var device  var dtype = var device  var dtype base dtype     coefficients =   apply state or     get  var device  var dtype                       or self  fallback apply state var device  var dtype        momentum var = self get slot var  string      return train ops resource sparse apply keras momentum          var handle          momentum var handle          coefficients string           grad          indices          coefficients string           use locking=self  use lock          use nesterov=self nesterov     def get config self       config = super sgd  self  get config       config update           string  self  serialize hyperparameter string           string  self  serialize hyperparameter string           string  self  serialize hyperparameter string           string  self nesterov             return config 
 keras export string  def deserialize config  custom objects=none     string         from tensorflow python keras mix precision experimental import loss scale optimizer     all class =         string  adadelta v2 adadelta        string  adagrad v2 adagrad        string  adam v2 adam        string  adamax v2 adamax        string  nadam v2 nadam        string  rmsprop v2 rmsprop        string  gradient descent v2 sgd        string  ftrl ftrl        string  loss scale optimizer lossscaleoptimizer            if config string  lower   in all class      config string  = config string  lower     return deserialize keras object        config        module objects=all class        custom objects=custom object        printable module name=string  
 keras export string  def get identifier     string   if isinstance identifier   optimizer  optimizer v2 optimizerv2        return identifier      elif isinstance identifier  tf optimizer module optimizer       opt = tfoptimizer identifier      k track tf optimizer opt      return opt   elif isinstance identifier  dict       return deserialize identifier    elif isinstance identifier  six string type       config =  string  str identifier   string          return deserialize config    else      raise valueerror string  identifier  
 keras export string  def serialize optimizer     return serialize keras object optimizer  
class exponentialdecay learningrateschedule     string    def   init          self        initial learn rate        decay step        decay rate        staircase=false        name=none       string     super exponentialdecay  self    init         self initial learn rate = initial learn rate     self decay step = decay step     self decay rate = decay rate     self staircase = staircase     self name = name    def   call   self  step       with ops name scope v2 self name or string  as name        initial learn rate = ops convert to tensor            self initial learn rate  name=string        dtype = initial learn rate dtype       decay step = math ops cast self decay step  dtype        decay rate = math ops cast self decay rate  dtype         global step recomp = math ops cast step  dtype        p = global step recomp / decay step       if self staircase          p = math ops floor p        return math ops multiply            initial learn rate  math ops pow decay rate  p   name=name     def get config self       return           string  self initial learn rate          string  self decay step          string  self decay rate          string  self staircase          string  self name       
class inversetimedecay learningrateschedule     string    def   init          self        initial learn rate        decay step        decay rate        staircase=false        name=none       string     super inversetimedecay  self    init          self initial learn rate = initial learn rate     self decay step = decay step     self decay rate = decay rate     self staircase = staircase     self name = name    def   call   self  step       with ops name scope v2 self name or string  as name        initial learn rate = ops convert to tensor            self initial learn rate  name=string        dtype = initial learn rate dtype       decay step = math ops cast self decay step  dtype        decay rate = math ops cast self decay rate  dtype         global step recomp = math ops cast step  dtype        p = global step recomp / decay step       if self staircase          p = math ops floor p        const = math ops cast constant op constant 1   dtype        denom = math ops add const  math ops multiply decay rate  p         return math ops divide initial learn rate  denom  name=name     def get config self       return           string  self initial learn rate          string  self decay step          string  self decay rate          string  self staircase          string  self name       
class learningrateschedule object     string     abc abstractmethod   def   call   self  step       raise notimplementederror string      abc abstractmethod   def get config self       raise notimplementederror string      classmethod   def from config cls  config       string     return cls   config  
class piecewiseconstantdecay learningrateschedule     string    def   init          self        boundaries        value        name=none       string     super piecewiseconstantdecay  self    init          if len boundaries   = len value  - 1        raise valueerror            string       self boundaries = boundaries     self value = value     self name = name    def   call   self  step       with ops name scope v2 self name or string         boundaries = ops convert n to tensor self boundaries        value = ops convert n to tensor self value        x recomp = ops convert to tensor step        for i  b in enumerate boundaries           if b dtype base dtype  = x recomp dtype base dtype                       b = math ops cast b  x recomp dtype base dtype            boundaries i  = b       pred fn pair =          pred fn pair append  x recomp <= boundaries 0   lambda  value 0          pred fn pair append  x recomp > boundaries -1   lambda  value -1          for low  high  v in zip boundaries  -1   boundaries 1    value 1 -1                     pred =  x recomp > low     x recomp <= high          pred fn pair append  pred  lambda v=v  v                        default = lambda  value 0        return control flow ops case pred fn pair  default  exclusive=true     def get config self       return           string  self boundaries          string  self value          string  self name       
class polynomialdecay learningrateschedule     string    def   init          self        initial learn rate        decay step        end learn rate=0 0001        power=1 0        cycle=false        name=none       string     super polynomialdecay  self    init          self initial learn rate = initial learn rate     self decay step = decay step     self end learn rate = end learn rate     self power = power     self cycle = cycle     self name = name    def   call   self  step       with ops name scope v2 self name or string  as name        initial learn rate = ops convert to tensor            self initial learn rate  name=string        dtype = initial learn rate dtype       end learn rate = math ops cast self end learn rate  dtype        power = math ops cast self power  dtype         global step recomp = math ops cast step  dtype        decay step recomp = math ops cast self decay step  dtype        if self cycle                            multiplier = control flow ops cond              math ops equal global step recomp  0   lambda  1 0              lambda  math ops ceil global step recomp / self decay step           decay step recomp = math ops multiply decay step recomp  multiplier        else                   global step recomp = math ops minimum global step recomp                                                self decay step         p = math ops divide global step recomp  decay step recomp        return math ops add            math ops multiply initial learn rate - end learn rate                              math ops pow 1 - p  power              end learn rate            name=name     def get config self       return           string  self initial learn rate          string  self decay step          string  self end learn rate          string  self power          string  self cycle          string  self name       
 keras export string  def deserialize config  custom objects=none     return generic utils deserialize keras object        config        module objects=globals          custom objects=custom object        printable module name=string  
 keras export string  def serialize learn rate schedule     return generic utils serialize keras object learn rate schedule  
class directoryiterator image directoryiterator  iterator     string    def   init   self  directory  image data generator                 target size= 256  256                  color mode=string                 classes=none                 class mode=string                 batch size=32                 shuffle=true                 seed=none                 data format=none                 save to dir=none                 save prefix=string                 save format=string                 follow links=false                 subset=none                 interpolation=string                 dtype=none       if data format be none        data format = backend image data format       kwargs =        if string in tf inspect getfullargspec          image imagedatagenerator   init    0         if dtype be none          dtype = backend floatx         kwargs string  = dtype     super directoryiterator  self    init            directory  image data generator          target size=target size          color mode=color mode          classes=classes          class mode=class mode          batch size=batch size          shuffle=shuffle          seed=seed          data format=data format          save to dir=save to dir          save prefix=save prefix          save format=save format          follow links=follow link          subset=subset          interpolation=interpolation            kwargs  
class imagedatagenerator image imagedatagenerator     string    def   init   self                 featurewise center=false                 samplewise center=false                 featurewise std normalization=false                 samplewise std normalization=false                 zca whitening=false                 zca epsilon=1e-6                 rotation range=0                 width shift range=0                  height shift range=0                  brightness range=none                 shear range=0                  zoom range=0                  channel shift range=0                  fill mode=string                 cval=0                  horizontal flip=false                 vertical flip=false                 rescale=none                 preprocessing function=none                 data format=none                 validation split=0 0                 dtype=none       if data format be none        data format = backend image data format       kwargs =        if string in tf inspect getfullargspec          image imagedatagenerator   init    0         if dtype be none          dtype = backend floatx         kwargs string  = dtype     super imagedatagenerator  self    init            featurewise center=featurewise center          samplewise center=samplewise center          featurewise std normalization=featurewise std normalization          samplewise std normalization=samplewise std normalization          zca whitening=zca whiten          zca epsilon=zca epsilon          rotation range=rotation range          width shift range=width shift range          height shift range=height shift range          brightness range=brightness range          shear range=shear range          zoom range=zoom range          channel shift range=channel shift range          fill mode=fill mode          cval=cval          horizontal flip=horizontal flip          vertical flip=vertical flip          rescale=rescale          preprocessing function=preprocessing function          data format=data format          validation split=validation split            kwargs  
class iterator image iterator  data utils sequence     pass 
class numpyarrayiterator image numpyarrayiterator  iterator     string    def   init   self  x  y  image data generator                 batch size=32                 shuffle=false                 sample weight=none                 seed=none                 data format=none                 save to dir=none                 save prefix=string                 save format=string                 subset=none                 dtype=none       if data format be none        data format = backend image data format       kwargs =        if string in tf inspect getfullargspec          image numpyarrayiterator   init    0         if dtype be none          dtype = backend floatx         kwargs string  = dtype     super numpyarrayiterator  self    init            x  y  image data generator          batch size=batch size          shuffle=shuffle          sample weight=sample weight          seed=seed          data format=data format          save to dir=save to dir          save prefix=save prefix          save format=save format          subset=subset            kwargs  
def apply affine transform x  theta=0  tx=0  ty=0  shear=0  zx=1  zy=1                             row axis=0  col axis=1  channel axis=2                             fill mode=string  cval=0   order=1       string     if scipy be none          raise importerror string                           string      transform matrix = none     if theta  = 0          theta = np deg2rad theta          rotation matrix = np array   np cos theta   -np sin theta   0                                        np sin theta   np cos theta   0                                        0  0  1            transform matrix = rotation matrix      if tx  = 0 or ty  = 0          shift matrix = np array   1  0  tx                                     0  1  ty                                     0  0  1            if transform matrix be none              transform matrix = shift matrix         else              transform matrix = np dot transform matrix  shift matrix       if shear  = 0          shear = np deg2rad shear          shear matrix = np array   1  -np sin shear   0                                     0  np cos shear   0                                     0  0  1            if transform matrix be none              transform matrix = shear matrix         else              transform matrix = np dot transform matrix  shear matrix       if zx  = 1 or zy  = 1          zoom matrix = np array   zx  0  0                                    0  zy  0                                    0  0  1            if transform matrix be none              transform matrix = zoom matrix         else              transform matrix = np dot transform matrix  zoom matrix       if transform matrix be not none          h  w = x shape row axis   x shape col axis          transform matrix = transform matrix offset center              transform matrix  h  w          x = np rollaxis x  channel axis  0          final affine matrix = transform matrix  2   2          final offset = transform matrix  2  2           channel image =  ndimage interpolation affine transform              x channel              final affine matrix              final offset              order=order              mode=fill mode              cval=cval  for x channel in x          x = np stack channel image  axis=0          x = np rollaxis x  0  channel axis   1      return x 
def apply brightness shift x  brightness       string     if imageenhance be none          raise importerror string                           string      x = array to img x      x = imgenhancer brightness = imageenhance brightness x      x = imgenhancer brightness enhance brightness      x = img to array x      return x 
def apply channel shift x  intensity  channel axis=0       string     x = np rollaxis x  channel axis  0      min x  max x = np min x   np max x      channel image =           np clip x channel   intensity                  min x                  max x          for x channel in x      x = np stack channel image  axis=0      x = np rollaxis x  0  channel axis   1      return x 
def random brightness x  brightness range       string     if len brightness range   = 2          raise valueerror              string             string    brightness range         u = np random uniform brightness range 0   brightness range 1       return apply brightness shift x  u  
def random channel shift x  intensity range  channel axis=0       string     intensity = np random uniform -intensity range  intensity range      return apply channel shift x  intensity  channel axis=channel axis  
def random rotation x  rg  row axis=1  col axis=2  channel axis=0                      fill mode=string  cval=0   interpolation order=1       string     theta = np random uniform -rg  rg      x = apply affine transform x  theta=theta  channel axis=channel axis                                 fill mode=fill mode  cval=cval                                 order=interpolation order      return x 
def random shear x  intensity  row axis=1  col axis=2  channel axis=0                   fill mode=string  cval=0   interpolation order=1       string     shear = np random uniform -intensity  intensity      x = apply affine transform x  shear=shear  channel axis=channel axis                                 fill mode=fill mode  cval=cval                                 order=interpolation order      return x 
def random shift x  wrg  hrg  row axis=1  col axis=2  channel axis=0                   fill mode=string  cval=0   interpolation order=1       string     h  w = x shape row axis   x shape col axis      tx = np random uniform -hrg  hrg    h     ty = np random uniform -wrg  wrg    w     x = apply affine transform x  tx=tx  ty=ty  channel axis=channel axis                                 fill mode=fill mode  cval=cval                                 order=interpolation order      return x 
def random zoom x  zoom range  row axis=1  col axis=2  channel axis=0                  fill mode=string  cval=0   interpolation order=1       string     if len zoom range   = 2          raise valueerror string                          string    zoom range         if zoom range 0  == 1 and zoom range 1  == 1          zx  zy = 1  1     else          zx  zy = np random uniform zoom range 0   zoom range 1   2      x = apply affine transform x  zx=zx  zy=zy  channel axis=channel axis                                 fill mode=fill mode  cval=cval                                 order=interpolation order      return x 
class timeseriesgenerator sequence timeseriesgenerator  data utils sequence     string   pass 
def make sample table size  sample factor=1e-5       string     gamma = 0 577     rank = np arange size      rank 0  = 1     inv fq = rank    np log rank    gamma    0 5 - 1  /  12    rank      f = sample factor   inv fq      return np minimum 1   f / np sqrt f   
def skipgrams sequence  vocabulary size                window size=4  negative samples=1   shuffle=true                categorical=false  sample table=none  seed=none       string     couple =        label =        for i  wi in enumerate sequence           if not wi              continue         if sample table be not none              if sample table wi  < random random                    continue          window start = max 0  i - window size          window end = min len sequence   i   window size   1          for j in range window start  window end               if j  = i                  wj = sequence j                  if not wj                      continue                 couple append  wi  wj                   if categorical                      label append  0  1                   else                      label append 1       if negative sample > 0          num negative sample = int len label    negative sample          word =  c 0  for c in couple          random shuffle word           couple  =   word i   len word                         random randint 1  vocabulary size - 1                       for i in range num negative sample           if categorical              label  =   1  0     num negative sample         else              label  =  0    num negative sample      if shuffle          if seed be none              seed = random randint 0  10e6          random seed seed          random shuffle couple          random seed seed          random shuffle label       return couple  label 
class tokenizer object       string      def   init   self  num words=none                   filters=string                   lower=true                   split=string                   char level=false                   oov token=none                   document count=0                     kwargs                    if string in kwargs              warn warn string                           string              num word = kwargs pop string          if kwargs              raise typeerror string   str kwargs            self word count = ordereddict           self word docs = defaultdict int          self filter = filter         self split = split         self lower = lower         self num word = num word         self document count = document count         self char level = char level         self oov token = oov token         self index docs = defaultdict int          self word index =            self index word =         def fit on texts self  texts           string         for text in texts              self document count  = 1             if self char level or isinstance text  list                   if self lower                      if isinstance text  list                           text =  text elem lower   for text elem in text                      else                          text = text lower                   seq = text             else                  seq = text to word sequence text                                              self filter                                              self lower                                              self split              for w in seq                  if w in self word count                      self word count w   = 1                 else                      self word count w  = 1             for w in set seq                                    self word docs w   = 1          wcounts = list self word count items            wcounts sort key=lambda x  x 1   reverse=true                   if self oov token be none              sort voc =            else              sort voc =  self oov token          sort voc extend wc 0  for wc in wcounts                    self word index = dict              zip sort voc  list range 1  len sort voc    1              self index word =  c  w for w  c in self word index items             for w  c in list self word docs items                 self index docs self word index w   = c      def fit on sequence self  sequence           string         self document count  = len sequence          for seq in sequence              seq = set seq              for i in seq                  self index docs i   = 1      def texts to sequence self  texts           string         return list self texts to sequence generator texts        def texts to sequence generator self  texts           string         num word = self num word         oov token index = self word index get self oov token          for text in texts              if self char level or isinstance text  list                   if self lower                      if isinstance text  list                           text =  text elem lower   for text elem in text                      else                          text = text lower                   seq = text             else                  seq = text to word sequence text                                              self filter                                              self lower                                              self split              vect =                for w in seq                  i = self word index get w                  if i be not none                      if num word and i >= num word                          if oov token index be not none                              vect append oov token index                      else                          vect append i                  elif self oov token be not none                      vect append oov token index              yield vect      def sequence to texts self  sequence           string         return list self sequence to texts generator sequence        def sequence to texts generator self  sequence           string         num word = self num word         oov token index = self word index get self oov token          for seq in sequence              vect =                for num in seq                  word = self index word get num                  if word be not none                      if num word and num >= num word                          if oov token index be not none                              vect append self index word oov token index                       else                          vect append word                  elif self oov token be not none                      vect append self index word oov token index               vect = string join vect              yield vect      def texts to matrix self  texts  mode=string           string         sequence = self texts to sequence texts          return self sequence to matrix sequence  mode=mode       def sequence to matrix self  sequence  mode=string           string         if not self num word              if self word index                  num word = len self word index    1             else                  raise valueerror string                                  string          else              num word = self num word          if mode == string and not self document count              raise valueerror string                              string           x = np zero  len sequence   num word           for i  seq in enumerate sequence               if not seq                  continue             count = defaultdict int              for j in seq                  if j >= num word                      continue                 count j   = 1             for j  c in list count items                     if mode == string                      x i  j  = c                 elif mode == string                      x i  j  = c / len seq                  elif mode == string                      x i  j  = 1                 elif mode == string                                                                tf = 1   np log c                      idf = np log 1   self document count /                                   1   self index docs get j  0                        x i  j  = tf   idf                 else                      raise valueerror string  mode          return x      def get config self           string         json word count = json dump self word count          json word docs = json dump self word docs          json index docs = json dump self index docs          json word index = json dump self word index          json index word = json dump self index word           return               string  self num word              string  self filter              string  self lower              string  self split              string  self char level              string  self oov token              string  self document count              string  json word count              string  json word docs              string  json index docs              string  json index word              string  json word index                def to json self    kwargs           string         config = self get config           tokenizer config =               string  self   class     name                string  config                   return json dump tokenizer config    kwargs  
def hash trick text  n                    hash function=none                    filters=string                    lower=true                    split=string       string     if hash function be none          hash function = hash     elif hash function == string          def hash function w               return int md5 w encode    hexdigest    16       seq = text to word sequence text                                  filters=filters                                  lower=lower                                  split=split      return   hash function w     n - 1    1  for w in seq  
def one hot text  n              filters=string              lower=true              split=string       string     return hash trick text  n                           hash function=hash                           filters=filters                           lower=lower                           split=split  
def text to word sequence text                            filters=string                            lower=true  split=string       string     if lower          text = text lower        if sys version info <  3            if isinstance text  unicode                 translate map =                   ord c   unicode split  for c in filter                             text = text translate translate map          elif len split  == 1              translate map = maketrans filter  split   len filter               text = text translate translate map          else              for c in filter                  text = text replace c  split      else          translate dict =  c  split for c in filter          translate map = maketrans translate dict          text = text translate translate map       seq = text split split      return  i for i in seq if i  
def tokenizer from json json string       string     tokenizer config = json load json string      config = tokenizer config get string       word count = json load config pop string       word docs = json load config pop string       index docs = json load config pop string            index docs =  int k   v for k  v in index docs items        index word = json load config pop string       index word =  int k   v for k  v in index word items        word index = json load config pop string        tokenizer = tokenizer   config      tokenizer word count = word count     tokenizer word docs = word docs     tokenizer index docs = index docs     tokenizer word index = word index     tokenizer index word = index word      return tokenizer 
class l1l2 regularizer     r   a regularizer that apply both l1 and l2 regularization penalties     the l1 regularization penalty be compute as      \ell 1\ \ penalty =\ell 1\sum  i=0  n x i       the l2 regularization penalty be compute as     \ell 2\ \ penalty =\ell 2\sum  i=0  nx i 2      attribute        l1  float  l1 regularization factor        l2  float  l2 regularization factor           def   init   self  l1=0   l2=0          self l1 = k cast to floatx l1      self l2 = k cast to floatx l2     def   call   self  x       if not self l1 and not self l2        return k constant 0       regularization = 0      if self l1        regularization  = self l1   math ops reduce sum math ops abs x       if self l2        regularization  = self l2   math ops reduce sum math ops square x       return regularization    def get config self       return  string  float self l1   string  float self l2   
class regularizer object     string    def   call   self  x       string     return 0      classmethod   def from config cls  config       string     return cls   config     def get config self       string     raise notimplementederror str self    string  
 keras export string  def get identifier     if identifier be none      return none   if isinstance identifier  dict       return deserialize identifier    elif isinstance identifier  six string type       identifier = str identifier                special case =  string  string  string      if identifier in special case               return deserialize  string  identifier  string           return deserialize str identifier     elif callable identifier       return identifier   else      raise valueerror string  identifier  
 keras export string  def l1 l2 l1=0 01  l2=0 01       r   create a regularizer that apply both l1 and l2 penalties     the l1 regularization penalty be compute as      \ell 1\ \ penalty =\ell 1\sum  i=0  n x i       the l2 regularization penalty be compute as      \ell 2\ \ penalty =\ell 2\sum  i=0  nx i 2      arguments        l1  float  l1 regularization factor        l2  float  l2 regularization factor     return      an l1l2 regularizer with the give regularization factor          return l1l2 l1=l1  l2=l2  
class generatorenqueuer sequenceenqueuer     string    def   init   self  sequence                 use multiprocessing=false                 random seed=none       super generatorenqueuer  self    init   sequence  use multiprocessing      self random seed = random seed    def  get executor init self  workers       string     def pool fn seqs         pool = get pool class true             workers  initializer=init pool generator            initargs= seqs  self random seed  get worker id queue            data pool add pool        return pool     return pool fn    def  run self       string     self  send sequence         with close self executor fn  share sequence   as executor        while true          if self stop signal be set              return          self queue put              executor apply async next sample   self uid     block=true     def get self       string     try        while self be run            input = self queue get block=true  get           self queue task do           if input be not none            yield input     except stopiteration               last ones =          while self queue qsize   > 0          last ones append self queue get block=true                for f in last ones          f wait                last ones =  future get   for future in last ones if future successful          for input in last ones          if input be not none            yield input     except exception as e          self stop         if string in str e           raise runtimeerror              string             string             string        six reraise  sys exc info    
class orderedenqueuer sequenceenqueuer     string    def   init   self  sequence  use multiprocessing=false  shuffle=false       super orderedenqueuer  self    init   sequence  use multiprocessing      self shuffle = shuffle    def  get executor init self  workers       string     def pool fn seqs         pool = get pool class true             workers  initializer=init pool generator            initargs= seqs  none  get worker id queue            data pool add pool        return pool      return pool fn    def  wait queue self       string     while true        time sleep 0 1        if self queue unfinished task == 0 or self stop signal be set            return    def  run self       string     sequence = list range len self sequence        self  send sequence         while true        if self shuffle          random shuffle sequence         with close self executor fn  share sequence   as executor          for i in sequence            if self stop signal be set                return            self queue put                executor apply async get index   self uid  i    block=true                    self  wait queue            if self stop signal be set                         return               self sequence on epoch end         self  send sequence        def get self       string     try        while self be run            input = self queue get block=true  get           self queue task do           if input be not none            yield input     except exception          self stop         six reraise  sys exc info    
class progbar object     string    def   init   self  target  width=30  verbose=1  interval=0 05                 stateful metrics=none  unit name=string       self target = target     self width = width     self verbose = verbose     self interval = interval     self unit name = unit name     if stateful metrics        self stateful metrics = set stateful metrics      else        self stateful metrics = set        self  dynamic display =   hasattr sys stdout  string  and                               sys stdout isatty    or                              string in sys modules or                              string in sys modules      self  total width = 0     self  see so far = 0               self  value =        self  value order =        self  start = time time       self  last update = 0    def update self  current  values=none       string     value = value or        for k  v in value        if k not in self  value order          self  value order append k        if k not in self stateful metrics                                              value base = max current - self  see so far  1          if k not in self  value            self  value k  =  v   value base  value base          else            self  value k  0   = v   value base           self  value k  1   = value base       else                                     self  value k  =  v  1      self  see so far = current      now = time time       info = string    now - self  start      if self verbose == 1        if  now - self  last update < self interval and           self target be not none and current < self target           return        prev total width = self  total width       if self  dynamic display          sys stdout write string   prev total width          sys stdout write string        else          sys stdout write string         if self target be not none          numdigits = int np log10 self target     1         bar =  string   str numdigits    string     current  self target          prog = float current  / self target         prog width = int self width   prog          if prog width > 0            bar  =  string    prog width - 1             if current < self target              bar  = string           else              bar  = string         bar  =  string    self width - prog width           bar  = string       else          bar = string   current        self  total width = len bar        sys stdout write bar         if current          time per unit =  now - self  start  / current       else          time per unit = 0       if self target be not none and current < self target          eta = time per unit    self target - current          if eta > 3600            eta format = string    eta // 3600                                            eta   3600  // 60                                           eta   60          elif eta > 60            eta format = string    eta // 60  eta   60          else            eta format = string   eta          info = string   eta format       else          if time per unit >= 1 or time per unit == 0            info  = string    time per unit  self unit name          elif time per unit >= 1e-3            info  = string    time per unit   1e3  self unit name          else            info  = string    time per unit   1e6  self unit name         for k in self  value order          info  = string   k         if isinstance self  value k   list             avg = np mean self  value k  0  / max 1  self  value k  1              if abs avg  > 1e-3              info  = string   avg           else              info  = string   avg         else            info  = string   self  value k         self  total width  = len info        if prev total width > self  total width          info  =  string    prev total width - self  total width          if self target be not none and current >= self target          info  = string        sys stdout write info        sys stdout flush        elif self verbose == 2        if self target be not none and current >= self target          numdigits = int np log10 self target     1         count =  string   str numdigits    string     current  self target          info = count   info         for k in self  value order            info  = string   k           avg = np mean self  value k  0  / max 1  self  value k  1              if avg > 1e-3              info  = string   avg           else              info  = string   avg         info  = string          sys stdout write info          sys stdout flush        self  last update = now    def add self  n  values=none       self update self  see so far   n  value  
class sequence object     string     abstractmethod   def   getitem   self  index       string     raise notimplementederror     abstractmethod   def   len   self       string     raise notimplementederror    def on epoch end self       string     pass    def   iter   self       string     for item in  self i  for i in range len self           yield item 
class sequenceenqueuer object     string    def   init   self  sequence                 use multiprocessing=false       self sequence = sequence     self use multiprocessing = use multiprocessing      global  sequence counter     if  sequence counter be none        try           sequence counter = multiprocessing value string  0        except oserror                                      sequence counter = 0      if isinstance  sequence counter  int         self uid =  sequence counter        sequence counter  = 1     else               with  sequence counter get lock            self uid =  sequence counter value          sequence counter value  = 1      self workers = 0     self executor fn = none     self queue = none     self run thread = none     self stop signal = none    def be run self       return self stop signal be not none and not self stop signal be set      def start self  workers=1  max queue size=10       string     if self use multiprocessing        self executor fn = self  get executor init workers      else               self executor fn = lambda    get pool class false  workers      self workers = workers     self queue = queue queue max queue size      self stop signal = thread event       self run thread = thread thread target=self  run      self run thread daemon = true     self run thread start      def  send sequence self       string           share sequence self uid  = self sequence    def stop self  timeout=none       string     self stop signal set       with self queue mutex        self queue queue clear         self queue unfinished task = 0       self queue not full notify       self run thread join timeout       share sequence self uid  = none    def   del   self       if self be run          self stop       abstractmethod   def  run self       string     raise notimplementederror     abstractmethod   def  get executor init self  workers       string     raise notimplementederror     abstractmethod   def get self       string     raise notimplementederror 
 keras export string  def custom object scope  args     string   return customobjectscope  args  
 keras export string  def deserialize keras object identifier                               module objects=none                               custom objects=none                               printable module name=string     if identifier be none      return none    if isinstance identifier  dict            config = identifier      cls  cls config  = class and config for serialize keras object          config  module object  custom object  printable module name       if hasattr cls  string         arg spec = tf inspect getfullargspec cls from config        custom object = custom object or           if string in arg spec args          return cls from config              cls config              custom objects=dict                  list  global custom object items                      list custom object items            with customobjectscope custom object           return cls from config cls config      else                             custom object = custom object or          with customobjectscope custom object           return cls   cls config    elif isinstance identifier  six string type       object name = identifier     if custom object and object name in custom object        obj = custom object get object name      elif object name in  global custom object        obj =  global custom object object name      else        obj = module object get object name        if obj be none          raise valueerror string   printable module name   string   object name                if tf inspect isclass obj         return obj       return obj   elif tf inspect isfunction identifier            return identifier   else      raise valueerror string                         printable module name  identifier   
 keras export string  def get custom object      string   return  global custom object 
 keras export string  def get file fname               origin               untar=false               md5 hash=none               file hash=none               cache subdir=string               hash algorithm=string               extract=false               archive format=string               cache dir=none     string   if cache dir be none      cache dir = os path join os path expanduser string   string    if md5 hash be not none and file hash be none      file hash = md5 hash     hash algorithm = string   datadir base = os path expanduser cache dir    if not os access datadir base  os w ok       datadir base = os path join string  string    datadir = os path join datadir base  cache subdir     makedirs exist ok datadir     if untar      untar fpath = os path join datadir  fname      fpath = untar fpath   string   else      fpath = os path join datadir  fname     download = false   if os path exist fpath            if file hash be not none        if not validate file fpath  file hash  algorithm=hash algorithm           print string               string   hash algorithm                 string   file hash                 string          download = true   else      download = true    if download      print string  origin       class progresstracker object                       progbar = none      def dl progress count  block size  total size         if progresstracker progbar be none          if total size == -1            total size = none         progresstracker progbar = progbar total size        else          progresstracker progbar update count   block size       error msg = string     try        try          urlretrieve origin  fpath  dl progress        except httperror as e          raise exception error msg format origin  e code  e msg         except urlerror as e          raise exception error msg format origin  e errno  e reason       except  exception  keyboardinterrupt  as e        if os path exist fpath           os remove fpath        raise     progresstracker progbar = none    if untar      if not os path exist untar fpath          extract archive fpath  datadir  archive format=string      return untar fpath    if extract       extract archive fpath  datadir  archive format     return fpath 
 keras export string  def get source input tensor  layer=none  node index=none     string   if not hasattr tensor  string       return tensor    if layer be none or node index      layer  node index    = tensor  keras history   if not layer  inbound nod      return  tensor    else      node = layer  inbound nod node index      if not node inbound layer               return nest flatten node input tensors      else        source tensors =          for layer  node index     tensor in node iterate inbound            previous source = get source input tensor  layer  node index                   for x in previous source            if all x be not t for t in source tensors               source tensors append x        return source tensors 
 keras export string  def model to dot model                   show shapes=false                   show layer names=true                   rankdir=string                   expand nested=false                   dpi=96                   subgraph=false     string   from tensorflow python keras layer import wrappers   from tensorflow python keras engine import sequential   from tensorflow python keras engine import network    if not check pydot        if string in sys modules                      print string             string        return     else        raise importerror string                         string     if subgraph      dot = pydot cluster style=string  graph name=model name      dot set string  model name      dot set string  string    else      dot = pydot dot       dot set string  rankdir      dot set string  true      dot set string  dpi      dot set node default shape=string     sub n first node =      sub n last node =      sub w first node =      sub w last node =       if not model  be graph network      node = pydot node str id model    label=model name      dot add node node      return dot   elif isinstance model  sequential sequential       if not model build        model build     layer = model  layer       for i  layer in enumerate layer       layer id = str id layer             layer name = layer name     class name = layer   class     name        if isinstance layer  wrappers wrapper         if expand nest and isinstance layer layer  network network           submodel wrapper = model to dot layer layer  show shape                                          show layer name  rankdir                                          expand nest                                          subgraph=true                   sub w nod = submodel wrapper get nod           sub w first node layer layer name  = sub w nod 0          sub w last node layer layer name  = sub w nod -1          dot add subgraph submodel wrapper        else          layer name = string format layer name  layer layer name          child class name = layer layer   class     name           class name = string format class name  child class name       if expand nest and isinstance layer  network network         submodel not wrapper = model to dot layer  show shape                                            show layer name  rankdir                                            expand nest                                            subgraph=true               sub n nod = submodel not wrapper get nod         sub n first node layer name  = sub n nod 0        sub n last node layer name  = sub n nod -1        dot add subgraph submodel not wrapper            if show layer name        label = string format layer name  class name      else        label = class name           if show shape         def format shape shape           return str shape  replace str none   string         try          outputlabels = format shape layer output shape        except attributeerror          outputlabels = string       if hasattr layer  string           inputlabels = format shape layer input shape        elif hasattr layer  string           inputlabels = string join               format shape ishape  for ishape in layer input shape         else          inputlabels = string       label = string    label                                                       inputlabels                                                       outputlabels       if not expand nest or not isinstance layer  network network         node = pydot node layer id  label=label        dot add node node        for layer in layer      layer id = str id layer       for i  node in enumerate layer  inbound nod         node key = layer name   string   str i        if node key in model  network nod          for inbound layer in nest flatten node inbound layer             inbound layer id = str id inbound layer             if not expand nest              assert dot get node inbound layer id              assert dot get node layer id              add edge dot  inbound layer id  layer id            else                           if  not isinstance inbound layer  network network  and                 not be wrap model inbound layer                                 if  not isinstance layer  network network  and                   not be wrap model layer                    assert dot get node inbound layer id                  assert dot get node layer id                  add edge dot  inbound layer id  layer id                               elif isinstance layer  network network                   add edge dot  inbound layer id                           sub n first node layer name  get name                                 elif be wrap model layer                   add edge dot  inbound layer id  layer id                  name = sub w first node layer layer name  get name                   add edge dot  layer id  name                           elif isinstance inbound layer  network network                 name = sub n last node inbound layer name  get name                 if isinstance layer  network network                   output name = sub n first node layer name  get name                   add edge dot  name  output name                else                  add edge dot  name  layer id                           elif be wrap model inbound layer                 inbound layer name = inbound layer layer name               add edge dot                         sub w last node inbound layer name  get name                           layer id    return dot 
 keras export string  def normalize x  axis=-1  order=2     string   l2 = np atleast 1d np linalg norm x  order  axis     l2 l2 == 0  = 1   return x / np expand dim l2  axis  
 keras export string  def plot model model                 to file=string                 show shapes=false                 show layer names=true                 rankdir=string                 expand nested=false                 dpi=96     string   dot = model to dot model                       show shapes=show shape                       show layer names=show layer name                       rankdir=rankdir                       expand nested=expand nest                       dpi=dpi    if dot be none      return      extension = os path splitext to file    if not extension      extension = string   else      extension = extension 1        dot write to file  format=extension             try      from ipython import display     return display image filename=to file    except importerror      pass 
 keras export string  def register keras serializable package=string  name=none     string    def decorator arg       string     class name = name if name be not none else arg   name       register name = package   string   class name      if tf inspect isclass arg  and not hasattr arg  string         raise valueerror            string       if register name in  global custom object        raise valueerror            string              register name   global custom object register name         if arg in  global custom name        raise valueerror string                           arg   global custom name arg         global custom object register name  = arg      global custom name arg  = register name      return arg    return decorator 
 keras export string  def serialize keras object instance     string      instance = tf decorator unwrap instance    if instance be none      return none    if hasattr instance  string       config = instance get config       serialization config =        for key  item in config items          if isinstance item  six string type           serialization config key  = item         continue                      try          serialize item = serialize keras object item          if isinstance serialize item  dict  and not isinstance item  dict             serialize item string  = true         serialization config key  = serialize item       except valueerror          serialization config key  = item      name =  get name or custom name instance   class        return serialize keras class and config name  serialization config    if hasattr instance  string       return  get name or custom name instance    raise valueerror string  instance  
 keras export string  def to categorical y  num classes=none  dtype=string     string   y = np array y  dtype=string    input shape = y shape   if input shape and input shape -1  == 1 and len input shape  > 1      input shape = tuple input shape  -1     y = y ravel     if not num class      num class = np max y    1   n = y shape 0    categorical = np zero  n  num class   dtype=dtype    categorical np arange n   y  = 1   output shape = input shape    num class     categorical = np reshape categorical  output shape    return categorical 
class summarywriter object     string     abc abstractmethod   def set as default self       string     raise notimplementederror       abc abstractmethod    tf contextlib contextmanager   def as default self       string     raise notimplementederror      def init self       string     raise notimplementederror      def flush self       string     raise notimplementederror      def close self       string     raise notimplementederror   
def audio name            data            sample rate            step=none            max outputs=3            encoding=none            description=none     string   audio ops = getattr tf  string  none    if audio ops be none           from tensorflow python ops import gen audio ops as audio ops    if encode be none      encode = string   if encode  = string      raise valueerror string   encode    summary metadata = metadata create summary metadata        display name=none        description=description        encoding=metadata encode value string     input =  data  sample rate  max output  step       summary scope =         getattr tf summary experimental  string  none  or       tf summary summary scope    with summary scope        name  string  values=inputs  as  tag                          lazy tensor creator lazytensorcreator     def lazy tensor          tf debug assert rank data  3        tf debug assert non negative max output        limit audio = data  max output        encode fn = functools partial audio ops encode wav                                      sample rate=sample rate        encode audio = tf map fn encode fn  limit audio                                  dtype=tf string                                  name=string               encode audio = tf cond            tf shape input=encoded audio  0  > 0            lambda  encode audio  lambda  tf constant     tf string         limit label = tf tile  string   tf shape input=limited audio   1         return tf transpose a=tf stack  encode audio  limit label                   return tf summary write          tag=tag  tensor=lazy tensor  step=step  metadata=summary metadata  
 tf export string  v1=    def create file writer v2 logdir                            max queue=none                            flush millis=none                            filename suffix=none                            name=none     string   if logdir be none      raise valueerror string    inside function = ops inside function     with ops name scope name  string  as scope  ops device string            with ops init scope          if context execute eagerly             check create file writer args              inside function              logdir=logdir              max queue=max queue              flush millis=flush millis              filename suffix=filename suffix        logdir = ops convert to tensor logdir  dtype=dtypes string        if max queue be none          max queue = constant op constant 10        if flush millis be none          flush millis = constant op constant 2   60   1000        if filename suffix be none          filename suffix = constant op constant string                             unique prefix = constant op constant string    os getpid    ops uid           filename suffix = unique prefix   filename suffix              if context execute eagerly            share name = context share name         else          share name = ops name from scope name scope          return resourcesummarywriter            share name=shared name            init op fn=functools partial                gen summary ops create summary file writer                logdir=logdir                max queue=max queue                flush millis=flush millis                filename suffix=filename suffix             name=name            v2=true  
 tf export string  v1=    def create noop writer      string   return noopsummarywriter   
 tf export string  v1=    def flush writer=none  name=none     string   if writer be none      writer =  summary state writer     if writer be none        return control flow ops no op     if isinstance writer  resourcesummarywriter       resource = writer  resource     else           resource = writer   with ops device string       return gen summary ops flush summary writer resource  name=name  
def histogram name  data  step=none  buckets=none  description=none     string   summary metadata = metadata create summary metadata        display name=none  description=description       summary scope =         getattr tf summary experimental  string  none  or       tf summary summary scope     def histogram summary data  bucket  histogram metadata  step       with summary scope          name  string  values= data  bucket  step   as  tag                                  lazy tensor creator lazytensorcreator       def lazy tensor            return  bucket data  bucket        return tf summary write            tag=tag  tensor=lazy tensor  step=step  metadata=summary metadata                 if isinstance tf distribute get strategy                    tf distribute experimental tpustrategy       return tf compat v1 tpu outside compilation        histogram summary  data  bucket  summary metadata  step    return histogram summary data  bucket  summary metadata  step  
def image name            data            step=none            max outputs=3            description=none     string   summary metadata = metadata create summary metadata        display name=none  description=description       summary scope =         getattr tf summary experimental  string  none  or       tf summary summary scope    with summary scope        name  string  values= data  max output  step   as  tag                          lazy tensor creator lazytensorcreator     def lazy tensor          tf debug assert rank data  4        tf debug assert non negative max output        image = tf image convert image dtype data  tf uint8  saturate=true        limit image = image  max output        encode image = tf map fn tf image encode png  limit image                                   dtype=tf string                                   name=string               encode image = tf cond            tf shape input=encoded image  0  > 0            lambda  encode image  lambda  tf constant     tf string         image shape = tf shape input=images        dimension = tf stack  tf as string image shape 2   name=string                                tf as string image shape 1   name=string                                name=string        return tf concat  dimension  encode image   axis=0                 return tf summary write          tag=tag  tensor=lazy tensor  step=step  metadata=summary metadata  
 tf export string  v1=     tf contextlib contextmanager def record if condition     string   old =  summary state be record   try       summary state be record = condition     yield   finally       summary state be record = old 
def scalar name  data  step=none  description=none     string   summary metadata = metadata create summary metadata        display name=none  description=description       summary scope =         getattr tf summary experimental  string  none  or       tf summary summary scope    with summary scope        name  string  values= data  step   as  tag          tf debug assert scalar data      return tf summary write tag=tag                              tensor=tf cast data  tf float32                               step=step                              metadata=summary metadata  
def text name  data  step=none  description=none     string   summary metadata = metadata create summary metadata        display name=none  description=description       summary scope =         getattr tf summary experimental  string  none  or       tf summary summary scope    with summary scope        name  string  values= data  step   as  tag          tf debug assert type data  tf string      return tf summary write          tag=tag  tensor=data  step=step  metadata=summary metadata  
 tf export string  v1=    def trace export name  step=none  profiler outdir=none     string         global  current trace context    if ops inside function        log warn string      return   if not context context   execute eagerly        log warn string      return    with  current trace context lock      if  current trace context be none        raise valueerror string      graph  profiler =  current trace context       if profiler and profiler outdir be none        raise valueerror string     run meta = context context   export run metadata      if graph and not profiler      run metadata graph name  run meta  step    else      run metadata name  run meta  step     if profiler       profiler save profiler outdir   profiler stop       trace off   
 tf export string  v1=    def trace off      string   global  current trace context   with  current trace context lock       current trace context = none       context context   disable run metadata            try       profiler stop     except  profiler profilernotrunningerror      pass 
 tf export string  v1=    def trace on graph=true  profiler=false       string   if ops inside function        log warn string      return   if not context context   execute eagerly        log warn string      return    global  current trace context   with  current trace context lock      if  current trace context        log warn string        return      if graph and not profiler        context context   enable graph collection       if profiler        context context   enable run metadata          profiler start         current trace context =  tracecontext graph=graph  profiler=profiler  
 tf export string  v1=    def write tag  tensor  step=none  metadata=none  name=none     string   with ops name scope name  string  as scope      if  summary state writer be none        return constant op constant false      if step be none        step = get step         if step be none          raise valueerror string                          string      if metadata be none        serialize metadata = b       elif hasattr metadata  string         serialize metadata = metadata serializetostring       else        serialize metadata = metadata      def record          string              with ops device string           summary tensor = tensor   if callable tensor  else array ops identity              tensor          write summary op = gen summary ops write summary               summary state writer  resource                step              summary tensor              tag              serialize metadata              name=scope          with ops control dependencies  write summary op              return constant op constant true       op = smart cond smart cond           should record summaries v2    record   nothing  name=string      if not context execute eagerly          ops add to collection ops graphkeys  summary collection  op        return op 
 tf export v1= string   def counterv1 start=0  step=1  dtype=dtypes int64     return dataset ops datasetv1adapter counterv2 start  step  dtype   
class csvdatasetv1 dataset ops datasetv1adapter     string     functools wrap csvdatasetv2   init      def   init   self                 filenames                 record default                 compression type=none                 buffer size=none                 header=false                 field delim=string                 use quote delim=true                 na value=string                 select cols=none       wrap = csvdatasetv2 filenames  record default  compression type                             buffer size  header  field delim  use quote delim                             na value  select cols      super csvdatasetv1  self    init   wrap  
 tf export v1= string    deprecation deprecate none  string  def  raggedtensorstructure dtype  shape  rag rank     return rag tensor raggedtensorspec shape  dtype  rag rank  
class randomdatasetv1 dataset ops datasetv1adapter     string     functools wrap randomdatasetv2   init      def   init   self  seed=none       wrap = randomdatasetv2 seed      super randomdatasetv1  self    init   wrap  
 tf export v1= string    deprecation deprecate none  string  def  sparsetensorstructure dtype  shape     return sparse tensor sparsetensorspec shape  dtype  
class sqldatasetv1 dataset ops datasetv1adapter     string     functools wrap sqldatasetv2   init      def   init   self  driver name  data source name  query  output type       wrap = sqldatasetv2 driver name  data source name  query  output type      super sqldatasetv1  self    init   wrap  
 tf export v1= string    deprecation deprecate none  string  def  tensorarraystructure dtype  element shape  dynamic size  infer shape     return tensor array ops tensorarrayspec element shape  dtype                                            dynamic size  infer shape  
 tf export v1= string    deprecation deprecate none  string  def  tensorstructure dtype  shape     return tensor spec tensorspec shape  dtype  
 tf export v1= string   def choose from datasets v1 datasets  choice dataset     return dataset ops datasetv1adapter        choose from datasets v2 datasets  choice dataset   
 tf export v1= string   def make batch feature dataset v1 file pattern                                         batch size                                       feature                                       reader=none                                       label key=none                                       reader args=none                                       num epochs=none                                       shuffle=true                                       shuffle buffer size=10000                                       shuffle seed=none                                       prefetch buffer size=none                                       reader num threads=none                                       parser num threads=none                                       sloppy ordering=false                                       drop final batch=false     return dataset ops datasetv1adapter make batch feature dataset v2        file pattern  batch size  feature  reader  label key  reader args        num epochs  shuffle  shuffle buffer size  shuffle seed        prefetch buffer size  reader num thread  parser num thread        sloppy order  drop final batch   
 tf export v1= string   def make csv dataset v1      file pattern      batch size      column names=none      column defaults=none      label name=none      select columns=none      field delim=string      use quote delim=true      na value=string      header=true      num epochs=none      shuffle=true      shuffle buffer size=10000      shuffle seed=none      prefetch buffer size=none      num parallel reads=none      sloppy=false      num row for inference=100      compression type=none      ignore errors=false         return dataset ops datasetv1adapter make csv dataset v2        file pattern  batch size  column name  column default  label name        select columns  field delim  use quote delim  na value  header        num epochs  shuffle  shuffle buffer size  shuffle seed        prefetch buffer size  num parallel read  sloppy  num row for inference        compression type  ignore errors   
 deprecation deprecate none  string   tf export v1= string   def map and batch with legacy function map func                                         batch size                                         num parallel batches=none                                         drop remainder=false                                         num parallel calls=none     string    if num parallel batch be none and num parallel call be none      num parallel call = batch size   elif num parallel batch be not none and num parallel call be none      num parallel call = batch size   num parallel batch   elif num parallel batch be not none and num parallel call be not none      raise valueerror string                      string     def  apply fn dataset       return  mapandbatchdataset dataset  map func  batch size                                 num parallel call  drop remainder                                 use legacy function=true     return  apply fn 
 tf export v1= string   def sample from datasets v1 datasets  weights=none  seed=none     return dataset ops datasetv1adapter        sample from datasets v2 datasets  weight  seed   
class centralstoragestrategyv1 distribute lib strategyv1        doc   = centralstoragestrategy   doc      def   init   self  compute devices=none  parameter device=none       super centralstoragestrategyv1  self    init            parameter server strategy parameterserverstrategyextended              self              compute devices=compute devices              parameter device=parameter device       distribute lib distribution strategy gauge get cell string  set          string       init     doc   = centralstoragestrategy   init     doc      def  in multi worker mode self       string     return false 
class collectiveallreducestrategyv1 distribute lib strategyv1        doc   = collectiveallreducestrategy   doc      def   init          self        communication=cross device ops lib collectivecommunication auto        cluster resolver=none       string     super collectiveallreducestrategyv1  self    init            collectiveallreduceextended              self              communication=communication              cluster resolver=cluster resolver       distribute lib distribution strategy gauge get cell string  set          string           distribute lib distribution strategy replica gauge get cell          string  set self extend  num workers      distribute lib distribution strategy replica gauge get cell          string  set self extend  num gpus per worker  
class parameterserverstrategyv1 distribute lib strategyv1        doc   = parameterserverstrategy   doc      def   init   self  cluster resolver=none       string     super parameterserverstrategyv1  self    init            parameterserverstrategyextended              self  cluster resolver=cluster resolver       distribute lib distribution strategy gauge get cell string  set          string      distribute lib distribution strategy replica gauge get cell string  set          len self extend parameter devices        init     doc   = parameterserverstrategy   init     doc   
class tpustrategyv1 distribute lib strategyv1     string    def   init   self                 tpu cluster resolver=none                 step per run=none                 device assignment=none       string     super tpustrategyv1  self    init   tpuextended          self  tpu cluster resolver  step per run  device assignment       distribute lib distribution strategy gauge get cell string  set string      distribute lib distribution strategy replica gauge get cell          string  set self extend num host      distribute lib distribution strategy replica gauge get cell          string  set self extend num replicas per host      property   def step per run self       string     return self  extend step per run             def experimental run v2 self  fn  args=    kwargs=none       validate experimental run function fn       string     fn = autograph tf convert fn  ag ctx control status ctx        return self extend tpu run fn  args  kwargs  
class kmeansclustering estimator estimator     string       square euclidean distance = cluster ops square euclidean distance   cosine distance = cluster ops cosine distance       random init = cluster ops random init   kmeans plus plus init = cluster ops kmeans plus plus init          score = string             cluster index = string   all distance = string       cluster center var name = cluster ops cluster var name    def   init   self                 num cluster                 model dir=none                 initial clusters=random init                 distance metric=squared euclidean distance                 seed=none                 use mini batch=true                 mini batch step per iteration=1                 kmeans plus plus num retries=2                 relative tolerance=none                 config=none                 feature columns=none       r   create an estimator for run kmeans train and inference       this estimator implement the follow variants of the k-means algorithm       if `use mini batch` be false  it run standard full batch k-means  each     train step run a single iteration of k-means and must process the full     input at once  to run in this mode  the `input fn` pass to `train` must     return the entire input dataset       if `use mini batch` be true  it run a generalization of the mini-batch     k-means algorithm  it run multiple iterations  where each iteration be     compose of `mini batch step per iteration` step  each train step     accumulate the contribution from one mini-batch into temporary storage      every `mini batch step per iteration` step  the cluster center be     update and the temporary storage clear for the next iteration      for example  the entire dataset contain 64k examples  where the batch size     be 64  user can choose mini batch step per iteration = 100 to run 10  of     the entire data every iteration in order to update the cluster center      note that          if `mini batch step per iteration=1`  the algorithm reduce to the         standard k-means mini-batch algorithm          if `mini batch step per iteration = num input / batch size`  the         algorithm become an asynchronous version of the full-batch algorithm          however  there be no guarantee by this implementation that each input         be see exactly once per iteration  also  different update be apply         asynchronously without lock  so this asynchronous version may not         behave exactly like a full-batch version       args        num cluster  an integer tensor specify the number of cluster  this         argument be ignore if `initial clusters` be a tensor or numpy array        model dir  the directory to save the model result and log file        initial cluster  specify how the initial cluster center be choose          one of the follow    a tensor or numpy array with the initial cluster           center    a callable `f input  k ` that select and return up to           `k` center from an input batch  `f` be free to return any number of           center from `0` to `k`  it will be invoke on successive input           batch as necessary until all `num clusters` center be choose            `kmeansclustering random init`  choose center randomly from an input           batch  if the batch size be less than `num clusters` then the entire           batch be choose to be initial cluster center and the remain           center be choose from successive input batch            `kmeansclustering kmeans plus plus init`  use kmeans   to choose           center from the first input batch  if the batch size be less than           `num clusters`  a tensorflow runtime error occur        distance metric  the distance metric use for cluster  one of            `kmeansclustering square euclidean distance`  euclidean distance           between vectors `u` and `v` be define as \\   u - v   2\\  which be           the square root of the sum of the absolute square of the elements            difference            `kmeansclustering cosine distance`  cosine distance between vectors           `u` and `v` be define as \\ 1 -  u   v  /    u   2   v   2 \\         seed  python integer  seed for prng use to initialize center        use mini batch  a boolean specify whether to use the mini-batch k-means         algorithm  see explanation above        mini batch step per iteration  the number of step after which the         update cluster center be sync back to a master copy  use only if         `use mini batch=true`  see explanation above        kmeans plus plus num retry  for each point that be sample during         kmeans   initialization  this parameter specify the number of         additional point to draw from the current distribution before select         the best  if a negative value be specify  a heuristic be use to         sample `o log num to sample  ` additional point  use only if         `initial clusters=kmeansclustering kmeans plus plus init`        relative tolerance  a relative tolerance of change in the loss between         iterations  stop learn if the loss change less than this amount          this may not work correctly if `use mini batch=true`        config  see `tf estimator estimator`        feature columns  an optionable iterable contain all the feature columns         use by the model  all items in the set should be feature column         instance that can be pass to `tf feature column input layer`  if this         be none  all feature will be use       raise        valueerror  an invalid argument be pass to `initial clusters` or         `distance metric`              if isinstance initial cluster  str  and initial cluster not in           kmeansclustering random init  kmeansclustering kmeans plus plus init              raise valueerror            string   initial cluster      if distance metric not in           kmeansclustering square euclidean distance          kmeansclustering cosine distance              raise valueerror string   distance metric      self  distance metric = distance metric     super kmeansclustering  self    init            model fn= modelfn num cluster  initial cluster  distance metric  seed                            use mini batch  mini batch step per iteration                            kmeans plus plus num retry  relative tolerance                            feature columns  model fn          model dir=model dir          config=config     def  predict one key self  input fn  predict key       for result in self predict input fn=input fn  predict keys= predict key          yield result predict key     def predict cluster index self  input fn       string     for index in self  predict one key input fn                                         kmeansclustering cluster index         yield index    def score self  input fn       string     return self evaluate input fn=input fn  steps=1  kmeansclustering score     def transform self  input fn       string     for distance in self  predict one key input fn                                             kmeansclustering all distance         if self  distance metric == kmeansclustering square euclidean distance          yield np sqrt distance        else          yield distance    def cluster center self       string     return self get variable value kmeansclustering cluster center var name  
 estimator export v1= string   def dnn logit fn builder units  hide units  feature columns  activation fn                           dropout  input layer partitioner  batch norm     string   if not isinstance units  six integer type       raise valueerror string format          type units       def dnn logit fn feature  mode       string     dnn model =  dnnmodel          units          hide units          feature columns          activation fn          dropout          input layer partitioner          batch norm          name=string      return dnn model feature  mode     return dnn logit fn 
 estimator export v1= string   def linear logit fn builder units  feature columns  sparse combiner=string     string    def linear logit fn feature       string     if feature column lib be feature column v2 feature columns         linear model = feature column lib linearmodel            feature columns=feature columns            units=units            sparse combiner=sparse combiner            name=string        logits = linear model feature        bias = linear model bias                                    variables = linear model variables       variables remove bias                bias =  get expand variable list  bias       else        linear model = feature column  linearmodel              feature columns=feature columns            units=units            sparse combiner=sparse combiner            name=string        logits = linear model feature        cols to vars = linear model cols to vars         bias = cols to vars pop string        variables = cols to vars value       variables =  get expand variable list variables       if units > 1        summary histogram string  bias      else                      summary scalar string  bias 0  0       summary scalar string                      compute fraction of zero variables       return logits    return linear logit fn 
class embeddingconfigspec      collections namedtuple string            string  string  string          string          string          string  string  string            string    def   new   cls                feature columns=none                optimization parameters=none                clip limit=none                pipeline execution with tensor core=false                experimental gradient multiplier fn=none                feature to config dict=none                table to config dict=none                partition strategy=string       string     if  not feature columns and not  feature to config dict and                                      table to config dict          or  feature columns and  feature to config dict                                  and table to config dict           raise valueerror string                        string                        string       if partition strategy not in  string  string         raise valueerror string                        string format partition strategy        if feature columns                      support class = tuple            list  support feature columns              list  tpu embed column class              list  embed column class          for column in feature columns          if not isinstance column  support class             raise typeerror                string                format support class  type column           if not isinstance optimization parameters   support optimizers           raise valueerror string                          string format  support optimizers                                                type optimization parameters        else        for feature  config in feature to config dict items            if not isinstance config  tpu embed featureconfig             raise typeerror                string                format feature  type config            if config table id not in table to config dict            raise valueerror string                            string format feature                                                            config table id         for table  config in table to config dict items            if not isinstance config  tpu embed tableconfig             raise typeerror                string               string format table  type config         return super embeddingconfigspec  cls    new            cls          feature columns=feature columns          optimization parameters=optimization parameters          clip limit=clipping limit          pipeline execution with tensor core=pipeline execution with tensor core          experimental gradient multiplier fn=experimental gradient multiplier fn          feature to config dict=feature to config dict          table to config dict=table to config dict          partition strategy=partition strategy  
 tf export v1= string   def output all intermediate state       string   control flow util v2  experimental output all intermediate override = state   
 deprecation deprecate      date=none      instructions= string                   string    keras export v1= string   def export save model model                         save model path                         custom objects=none                         as text=false                         input signature=none                         serve only=false     string   if serve only      save lib save          model          save model path          signatures=saving utils trace model call model  input signature     else       save v1 format model  save model path  custom object  as text                      input signature     try       export model json model  save model path    except notimplementederror      log warn string                     string  
 deprecation deprecate      date=none      instructions= string                   string    keras export v1= string   def load from save model save model path  custom objects=none     string      model json filepath = os path join        compat as bytes save model path         compat as bytes constants assets directory         compat as bytes constants save model filename json     model json = file io read file to string model json filepath    model = model config model from json        model json  custom objects=custom object        checkpoint prefix = os path join        compat as text save model path         compat as text constants variables directory         compat as text constants variables filename     model load weight checkpoint prefix    return model 
 tf export v1= string    tf contextlib contextmanager def keras style scope      string   global  keras style scope   stack =  keras style scope    keras style scope = true   try      yield   finally       keras style scope = stack 
 tf export v1= string   def set keras style      string   global  keras style scope    keras style scope = true 
  tf export v1= string   def convert op hint to stub session=none                                graph def=none                                write callback=lambda graph def  comment  none     string    if session be not none and graph def be not none      raise valueerror string     if session be not none      return  convert op hint to stub helper session graph def  write callback    elif graph def be not none      return  convert op hint to stub helper graph def  write callback    else      raise valueerror string  
class adagradparameters  optimizationparameters     string    def   init   self                 learn rate                 initial accumulator=0 1                 use gradient accumulation=true                 clip weight min=none                 clip weight max=none       string     super adagradparameters            self    init   learn rate  use gradient accumulation                           clip weight min  clip weight max      if initial accumulator <= 0        raise valueerror string      self initial accumulator = initial accumulator 
class adamparameters  optimizationparameters     string    def   init   self                 learn rate                 beta1=0 9                 beta2=0 999                 epsilon=1e-08                 lazy adam=true                 sum inside sqrt=true                 use gradient accumulation=true                 clip weight min=none                 clip weight max=none       string     super adamparameters            self    init   learn rate  use gradient accumulation                           clip weight min  clip weight max      if beta1 < 0  or beta1 >= 1         raise valueerror string format beta1       if beta2 < 0  or beta2 >= 1         raise valueerror string format beta2       if epsilon <= 0         raise valueerror string format epsilon       if not use gradient accumulation and not lazy adam        raise valueerror            string       self beta1 = beta1     self beta2 = beta2     self epsilon = epsilon     self lazy adam = lazy adam     self sum inside sqrt = sum inside sqrt 
class stochasticgradientdescentparameters  optimizationparameters     string    def   init   self  learn rate  clip weight min=none                 clip weight max=none       string     super stochasticgradientdescentparameters            self    init   learn rate  false  clip weight min  clip weight max  
 tf export v1= string   def embed column v2 categorical column                          dimension                          combiner=string                          initializer=none                          max sequence length=0                          learn rate fn=none                          embed lookup device=none                          tensor core shape=none     string    if not isinstance categorical column   support categorical columns v2       raise typeerror          string         string    string join               cc   name   for cc in  support categorical columns v2             type categorical column      if  dimension be none  or  dimension < 1       raise valueerror string format dimension     if tensor core shape and len tensor core shape   = 2      raise valueerror          string format tensor core shape      if  initializer be not none  and  not callable initializer        raise valueerror string                      string format                           categorical column name     if initializer be none      initializer = init ops truncate normal initializer          mean=0 0  stddev=1 / math sqrt dimension      if  embed lookup device and       embed lookup device not in  allow devices       raise valueerror string                        allow devices     if embed lookup device == string      embed lookup device = embeddingdevice cpu   elif embed lookup device == string      embed lookup device = embeddingdevice tpu tensor core   elif embed lookup device == string      embed lookup device = embeddingdevice tpu embed core    if  embed lookup device == embeddingdevice tpu tensor core and       not tensor core shape       raise valueerror string                      string     if not embed lookup device      return  tpuembeddingcolumnv2          categorical column=categorical column          dimension=dimension          combiner=combiner          initializer=initializer          max sequence length=max sequence length          learn rate fn=learning rate fn    else      return  tpudevicespecificembeddingcolumnv2          categorical column=categorical column          dimension=dimension          combiner=combiner          initializer=initializer          max sequence length=max sequence length          learn rate fn=learning rate fn          embed lookup device=embedding lookup device          tensor core shape=tensor core shape  
 tf export v1= string   def share embed columns v2 categorical columns                                  dimension                                  combiner=string                                  initializer=none                                  share embed collection name=none                                  max sequence lengths=none                                  learn rate fn=none                                  embed lookup device=none                                  tensor core shape=none     string    for categorical column in categorical columns      if not isinstance categorical column   support categorical columns v2         raise typeerror            string           string    string join                 cc   name   for cc in  support categorical columns v2               type categorical column       if not max sequence lengths      max sequence lengths =  0    len categorical columns    if len max sequence lengths   = len categorical columns       raise valueerror string                      string                      string format                           len max sequence lengths   len categorical columns       if  dimension be none  or  dimension < 1       raise valueerror string format dimension     if tensor core shape and len tensor core shape   = 2      raise valueerror          string format tensor core shape      if  initializer be not none  and  not callable initializer        raise valueerror string    if initializer be none      initializer = init ops truncate normal initializer          mean=0 0  stddev=1 / math sqrt dimension            sort columns = sort categorical columns  key=lambda x  x name    num bucket = sort columns 0   num bucket      for c in sort columns 1        if num bucket  = c  num bucket          raise valueerror            string           string           string format                sort columns 0   num bucket  c  c  num bucket        if not share embed collection name      share embed collection name = string join c name for c in sort columns      share embed collection name  = string    tpu columns =       column creator = fc lib sharedembeddingcolumncreator        dimension=dimension  initializer=initializer  ckpt to load from=none        tensor name in ckpt=none  num buckets=num bucket  trainable=none        name=shared embed collection name     if  embed lookup device and       embed lookup device not in  allow devices       raise valueerror string                        allow devices     if embed lookup device == string      embed lookup device = embeddingdevice cpu   elif embed lookup device == string      embed lookup device = embeddingdevice tpu tensor core   elif embed lookup device == string      embed lookup device = embeddingdevice tpu embed core    if  embed lookup device == embeddingdevice tpu embed core and       not tensor core shape       raise valueerror string                      string        for categorical column  max sequence length in zip        categorical columns  max sequence lengths       if not embed lookup device        column =  tpusharedembeddingcolumnv2            categorical column=categorical column            share embed column creator=column creator            combiner=combiner            initializer=initializer            share embed collection name=shared embed collection name            max sequence length=max sequence length            learn rate fn=learning rate fn      else        column =  tpushareddevicespecificembeddingcolumnv2            categorical column=categorical column            share embed column creator=column creator            combiner=combiner            initializer=initializer            share embed collection name=shared embed collection name            max sequence length=max sequence length            learn rate fn=learning rate fn            embed lookup device=embedding lookup device            tensor core shape=tensor core shape      tpu columns append column     return tpu columns 
class inmemoryevaluatorhook train sessionrunhook     string    def   init   self                 estimator                 input fn                 steps=none                 hooks=none                 name=none                 every n iter=100       string     if every n iter be none or every n iter <= 0        raise valueerror string   every n iter      if  estimator config num ps replicas > 0 or         estimator config num worker replicas > 1         raise valueerror            string      self  estimator = estimator     self  input fn = input fn     self  step = step     self  name = name     self  every n iter = every n iter     self  eval dir = os path join self  estimator model dir  string                                   if not name else string   name       self  graph = none     self  hook = estimator lib  check hook type hook      self  hook extend self  estimator  convert eval step to hook step       self  timer = train secondorsteptimer every steps=every n iter     def begin self       string     self  timer reset       self  iter count = 0     self  graph = ops graph       with self  graph as default           self  scaffold  self  update op  self  eval dict         self  all hook  = self  estimator  evaluate build graph             self  input fn  self  hook  checkpoint path=none         if self  scaffold saver be not none          raise valueerror string        if self  scaffold init fn be not none          raise valueerror string         self  var name to eval var =             v name  v for v in ops get collection ops graphkeys global variables                self  var name to placeholder =             v name  array ops placeholder v dtype            for v in ops get collection ops graphkeys global variables             def after create session self  session  coord         string     if ops get collection ops graphkeys saveable object         raise valueerror            string           string      self  var name to train var =           v name  v for v in ops get collection ops graphkeys global variables            var name to transfer = set self  var name to placeholder key      set          self  var name to train var key             self  var name to train var =           v name  self  var name to train var v name          for v name in var name to transfer                self  var name to eval var =           v name  self  var name to eval var v name          for v name in var name to transfer            with self  graph as default          self  var fee op = control flow ops group             state ops assign self  var name to eval var v name                              self  var name to placeholder v name             for v name in var name to transfer               self  evaluate session     def  evaluate self  train session       var name to value = train session run self  var name to train var      placeholder to value =           self  var name to placeholder v name   var name to value v name          for v name in var name to value            def fee variables scaffold  session         del scaffold       session run self  var fee op  fee dict=placeholder to value       scaffold = train scaffold          init fn=feed variables  copy from scaffold=self  scaffold       with self  graph as default          self  estimator  evaluate run            checkpoint path=none            scaffold=scaffold            update op=self  update op            eval dict=self  eval dict            all hooks=self  all hook            output dir=self  eval dir       self  timer update last trigger step self  iter count     def after run self  run context  run value         string     self  iter count  = 1     if self  timer should trigger for step self  iter count         self  evaluate run context session     def end self  session         string     self  evaluate session  
class linearsdca object     string    def   init   self                 example id column                 num loss partitions=1                 num table shards=none                 symmetric l1 regularization=0 0                 symmetric l2 regularization=1 0                 adaptive=false       string      self  example id column = example id column     self  num loss partition = num loss partition     self  num table shards = num table shards     self  symmetric l1 regularization = symmetric l1 regularization     self  symmetric l2 regularization = symmetric l2 regularization     self  adaptive = adaptive    def  prune and unique sparse ids self  id weight pair       string      id tensor = id weight pair id tensor     if id weight pair weight tensor        weight tensor = id weight pair weight tensor value     else        weight tensor = array ops ones             array ops shape id tensor indices  0    dtypes float32       example ids = array ops reshape id tensor indices    0    -1       flat ids = math ops cast          array ops reshape id tensor value   -1    dtype=dtypes int64                     be id valid = math ops greater equal flat ids  0      flat ids = array ops boolean mask flat ids  be id valid      example ids = array ops boolean mask example ids  be id valid      weight tensor = array ops boolean mask weight tensor  be id valid       projection length = math ops reduce max flat ids    1               project ids = projection length   example ids   flat ids           ids  idx = array ops unique project ids           example ids filter = math ops unsorted segment min          example ids  idx          array ops shape ids  0             reproject ids =  ids - projection length   example ids filter       weight = array ops reshape          math ops unsorted segment sum weight tensor  idx                                        array ops shape ids  0     -1       return sdca ops  sparsefeaturecolumn            example ids filter  reproject ids  weight     def get train step self  state manager  weight column name  loss type                       feature columns  feature  target  bias var  global step       string      batch size = array ops shape target  0      cache = feature column lib featuretransformationcache feature                      dense feature  dense feature weight =            sparse feature with value  sparse feature with value weight =            for column in sort feature columns  key=lambda x  x name         if isinstance column  feature column lib categoricalcolumn           id weight pair = column get sparse tensors cache  state manager          sparse feature with value append              self  prune and unique sparse ids id weight pair                             sparse feature with value weight append              state manager get variable column  string         elif isinstance column  feature column lib densecolumn           if column variable shape ndims  = 1            raise valueerror string                   type column    name    column variable shape ndims           dense feature append column get dense tensor cache  state manager                             dense feature weight append              state manager get variable column  string         else          raise valueerror string                            type column    name              dense feature append array ops ones  batch size  1        dense feature weight append bias var       example weight = array ops reshape          feature weight column name           shape= -1   if weight column name else array ops ones  batch size       example ids = feature self  example id column      train examples = dict          sparse features=sparse feature with value          dense features=dense feature          example labels=math ops to float              array ops reshape target  shape= -1             example weights=example weight          example ids=example ids      train variables = dict          sparse feature weights=sparse feature with value weight          dense feature weights=dense feature weight      sdca model = sdca ops  sdcamodel            examples=training examples          variables=training variables          options=dict              symmetric l1 regularization=self  symmetric l1 regularization              symmetric l2 regularization=self  symmetric l2 regularization              adaptive=self  adaptive              num loss partitions=self  num loss partition              num table shards=self  num table shards              loss type=loss type       train op = sdca model minimize global step=global step      return sdca model  train op 
class rnnclassifier rnnestimator     string    def   init   self                 sequence feature columns                 context feature columns=none                 units=none                 cell type=use default                 rnn cell fn=none                 return sequences=false                 model dir=none                 n classes=2                 weight column=none                 label vocabulary=none                 optimizer=string                 loss reduction=losses utils reductionv2 sum over batch size                 sequence mask=string                 config=none       string     if n class == 2        head = binary head lib binaryclasshead            weight column=weight column            label vocabulary=label vocabulary            loss reduction=loss reduction      else        head = multi head lib multiclasshead            n classes=n class            weight column=weight column            label vocabulary=label vocabulary            loss reduction=loss reduction       if return sequence        log info string                    string        head = seq head lib sequentialheadwrapper            head  sequence length mask=sequence mask            feature columns=weight column       super rnnclassifier  self    init            head=head          sequence feature columns=sequence feature columns          context feature columns=context feature columns          units=units          cell type=cell type          rnn cell fn=rnn cell fn          return sequences=return sequence          model dir=model dir          optimizer=optimizer          config=config  
class rnnestimator estimator estimator     string    def   init   self                 head                 sequence feature columns                 context feature columns=none                 units=none                 cell type=use default                 rnn cell fn=none                 return sequences=false                 model dir=none                 optimizer=string                 config=none       string           if return sequence and not isinstance          head  seq head lib  sequentialhead           raise valueerror            string           string       verify rnn cell input rnn cell fn  units  cell type       def  model fn feature  label  mode  config         string       del config         rnn layer =  make rnn layer            rnn cell fn=rnn cell fn  units=units  cell type=cell type            return sequences=return sequence        rnn model = rnnmodel            rnn layer=rnn layer            units=head logits dimension            sequence feature columns=sequence feature columns            context feature columns=context feature columns            return sequences=return sequence            name=string        return  get rnn estimator spec            feature  label  mode  head=head  rnn model=rnn model            optimizer=optimizer  return sequences=return sequence       super rnnestimator  self    init            model fn= model fn  model dir=model dir  config=config  
 estimator export      string  def build raw supervise input receiver fn feature                                             label                                             default batch size=none     string      try      feat key = feature key     except attributeerror      feat key =  single receiver default name    try      label key = label key     except attributeerror      label key =  single label default name     overlap key = set feat key    set label key    if overlap key      raise valueerror string                      string format overlap key      def supervise input receiver fn        string     if not isinstance feature  dict         feature cp =  placeholder from tensor feature  default batch size        receiver feature =  single receiver default name  feature cp      else        receiver feature =  placeholders from receiver tensors dict            feature  default batch size        feature cp = receiver feature      if not isinstance label  dict         label cp =  placeholder from tensor label  default batch size        receiver label =  single label default name  label cp      else        receiver label =  placeholders from receiver tensors dict            label  default batch size        label cp = receiver label      receiver tensors = dict receiver feature      receiver tensors update receiver label      return supervisedinputreceiver feature cp  label cp  receiver tensors     return supervise input receiver fn 
 estimator export string  def call logit fn logit fn  feature  mode  params  config     string   logit fn args = function utils fn args logit fn    kwargs =      if string in logit fn args      kwargs string  = mode   if string in logit fn args      kwargs string  = params   if string in logit fn args      kwargs string  = config   logit fn result = logit fn features=features    kwargs     result be valid dictionary =         isinstance logit fn result  dict  and       all   isinstance k  six string type  and isinstance v  ops tensor              for k  v in six iteritems logit fn result       result be tensor = isinstance logit fn result  ops tensor     if not  result be valid dictionary or result be tensor       raise valueerror string                      string                        logit fn result     return logit fn result 
 estimator export string  def make early stop hook estimator                               should stop fn                               run every secs=60                               run every steps=none     string   if not isinstance estimator  estimator lib estimator       raise typeerror string                     string format type estimator       if run every secs be not none and run every step be not none      raise valueerror string                      string     if estimator config be chief      return  stoponpredicatehook should stop fn  run every secs  run every step    else      return  checkforstoppinghook   
 estimator export string  def make stop at checkpoint step hook estimator                                        last step                                        wait after file check secs=30     string    if estimator config be chief      return train stopatstephook last step=last step    return  stopatcheckpointstephook        model dir=estimator model dir        last step=last step        wait after file check secs=wait after file check secs  
 estimator export string  def stop if higher hook estimator                          metric name                          threshold                          eval dir=none                          min steps=0                          run every secs=60                          run every steps=none     string   return  stop if threshold cross hook        estimator=estimator        metric name=metric name        threshold=threshold        higher be better=true        eval dir=eval dir        min steps=min step        run every secs=run every secs        run every steps=run every step  
 estimator export string  def stop if lower hook estimator      metric name      threshold      eval dir=none      min steps=0      run every secs=60      run every steps=none     string   return  stop if threshold cross hook        estimator=estimator        metric name=metric name        threshold=threshold        higher be better=false        eval dir=eval dir        min steps=min step        run every secs=run every secs        run every steps=run every step  
 estimator export string  def stop if no decrease hook estimator      metric name      max step without decrease      eval dir=none      min steps=0      run every secs=60      run every steps=none     string   return  stop if no metric improvement hook        estimator=estimator        metric name=metric name        max step without improvement=max step without decrease        higher be better=false        eval dir=eval dir        min steps=min step        run every secs=run every secs        run every steps=run every step  
 estimator export string  def stop if no increase hook estimator      metric name      max step without increase      eval dir=none      min steps=0      run every secs=60      run every steps=none     string   return  stop if no metric improvement hook        estimator=estimator        metric name=metric name        max step without improvement=max step without increase        higher be better=true        eval dir=eval dir        min steps=min step        run every secs=run every secs        run every steps=run every step  
class linearmodel train model     r   linear model for regression and classification problems     this model approximate the follow function      y = \beta   \sum  i=1   n  w  i    x  i      where   \beta   be the bias and   w  i    be the weight for each feature     example     ```python   model = linearmodel     model compile optimizer= sgd   loss= mse     model fit x  y  epochs    ```    this model accept sparse float input as well     example    ```python   model = linearmodel     opt = tf keras optimizers adam     loss fn = tf keras losses meansquarederror     with tf gradienttape   as tape      output = model sparse input      loss = tf reduce mean loss fn target  output     grads = tape gradient loss  model weight    opt apply gradients zip grads  model weight     ```           def   init   self                 units=1                 activation=none                 use bias=true                 kernel initializer=string                 bias initializer=string                 kernel regularizer=none                 bias regularizer=none                   kwargs       string      self units = units     self activation = activations get activation      self use bias = use bias     self kernel initializer = initializers get kernel initializer      self bias initializer = initializers get bias initializer      self kernel regularizer = regularizers get kernel regularizer      self bias regularizer = regularizers get bias regularizer      super linearmodel  self    init     kwargs      base layer  keras model gauge get cell string  set true       def build self  input shape       self dense layer =        if isinstance input shape  list         for shape in input shape          layer = core dense              units=self units              use bias=false              kernel initializer=self kernel initializer              kernel regularizer=self kernel regularizer              input shape=shape          self dense layer append layer      else        layer = core dense            units=self units            use bias=false            kernel initializer=self kernel initializer            kernel regularizer=self kernel regularizer            input shape=input shape        self dense layer append layer       if self use bias        self bias = self add weight            string            shape=self units            initializer=self bias initializer            regularizer=self bias regularizer            dtype=self dtype            trainable=true      else        self bias = none    def call self  input       if not isinstance input   tuple  list          input =  input      if len input   = len self dense layer         raise valueerror string format            len self dense layer   len input        result = none     for inp  layer in zip input  self dense layer         output = layer inp        if result be none          result = output       else          result  = output      if self use bias        result = nn bias add result  self bias      if self activation be not none        return self activation result        return result    def get config self       config =           string  self units          string  activations serialize self activation           string  self use bias          string  initializers serialize self kernel initializer           string  initializers serialize self bias initializer           string  regularizers serialize self kernel regularizer           string  regularizers serialize self bias regularizer             base config = base layer layer get config self      return dict list base config items      list config items         classmethod   def from config cls  config  custom objects=none       del custom object     return cls   config  
class sequencefeatures fc  basefeatureslayer     string    def   init          self        feature columns        trainable=true        name=none          kwargs       string     super sequencefeatures  self    init            feature columns=feature columns          trainable=trainable          name=name          expect column type=fc sequencedensecolumn            kwargs      property   def  be feature layer self       return true    def  target shape self  input shape  total elements       return  input shape 0   input shape 1   total elements     def call self  feature       string     if not isinstance feature  dict         raise valueerror string                         feature      transformation cache = fc featuretransformationcache feature      output tensors =        sequence lengths =         for column in self  feature columns        with ops name scope column name           dense tensor  sequence length = column get sequence dense tensor              transformation cache  self  state manager                   output tensors append self  process dense tensor column  dense tensor           sequence lengths append sequence length            fc  verify static batch size equality sequence lengths                                            self  feature columns      sequence length =  assert all equal and return sequence lengths       return self  verify and concat tensors output tensors   sequence length 
class widedeepmodel train model     r   wide   deep model for regression and classification problems     this model jointly train a linear and a dnn model     example     ```python   linear model = linearmodel     dnn model = keras sequential  keras layer dense units=64                                  keras layer dense units=1      combine model = widedeepmodel dnn model  linear model    combine model compile optimizer=  sgd    adam     mse     mse        define dnn input and linear input as separate numpy array or     a single numpy array if dnn input be same as linear input    combine model fit  dnn input  linear input   y  epochs      or define a single `tf data dataset` that contain a single tensor or     separate tensors for dnn input and linear input    dataset = tf data dataset from tensors   dnn input  linear input   y     combine model fit dataset  epochs    ```    both linear and dnn model can be pre-compiled and train separately   before jointly train     example    ```python   linear model = linearmodel     linear model compile  adagrad    mse     linear model fit linear input  y  epochs    dnn model = keras sequential  keras layer dense units=1      dnn model compile  rmsprop    mse     dnn model fit dnn input  y  epochs    combine model = widedeepmodel dnn model  linear model    combine model compile optimizer=  sgd    adam     mse     mse      combine model fit  dnn input  linear input   y  epochs    ```           def   init   self  linear model  dnn model  activation=none    kwargs       string     super widedeepmodel  self    init     kwargs      base layer  keras model gauge get cell string  set true        self linear model = linear model     self dnn model = dnn model     self activation = activations get activation     def call self  input       if not isinstance input   tuple  list   or len input   = 2        linear input = dnn input = input     else        linear input  dnn input = input     linear output = self linear model linear input      dnn output = self dnn model dnn input      output = nest map structure lambda x  y  0 5    x   y   linear output                                  dnn output      if self activation        return nest map structure self activation  output      return output    def  get optimizers self       if isinstance self optimizer   tuple  list          return  self optimizer 0   self optimizer 1       else        return  self optimizer  self optimizer        def  backwards self  tape  loss       linear vars = self linear model trainable weight       dnn vars = self dnn model trainable weight       linear grads  dnn grads = tape gradient loss   linear vars  dnn vars       linear optimizer  dnn optimizer = self  get optimizers       linear optimizer apply gradients zip linear grads  linear vars       dnn optimizer apply gradients zip dnn grads  dnn vars       return    def  make train function self                 have recompiled = self  recompile weight loss and weight metrics       self  check trainable weight consistency                      if getattr self  string  none  be none or have recompiled               current trainable state = self  get trainable state         self  set trainable state self  compile trainable state         input =             self  fee input   self  fee target   self  fee sample weight        if not isinstance k symbolic learn phase    int           input  =  k symbolic learn phase           linear optimizer  dnn optimizer = self  get optimizers         with k get graph   as default            with k name scope string                        update =              linear update = linear optimizer get update                params=self linear model trainable weight                  loss=self total loss            update  = linear update           dnn update = dnn optimizer get update                params=self dnn model trainable weight                  loss=self total loss            update  = dnn update                      update  = self get update for none                       update  = self get update for self input           metrics = self  get train eval metrics           metrics tensors =               m  call result for m in metrics if hasattr m  string                     with k name scope string                    fn = k function              input   self total loss    metrics tensors              updates=updates              name=string                self  function kwargs          setattr self  string  fn                self  set trainable state current trainable state     def get config self       linear config = generic utils serialize keras object self linear model      dnn config = generic utils serialize keras object self dnn model      config =           string  linear config          string  dnn config          string  activations serialize self activation             base config = base layer layer get config self      return dict list base config items      list config items         classmethod   def from config cls  config  custom objects=none       linear config = config pop string      linear model = layer module deserialize linear config  custom object      dnn config = config pop string      dnn model = layer module deserialize dnn config  custom object      activation = activations deserialize          config pop string  none   custom objects=custom object      return cls          linear model=linear model          dnn model=dnn model          activation=activation            config  