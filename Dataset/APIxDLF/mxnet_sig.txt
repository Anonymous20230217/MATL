( data : input array ) ( a min : minimum value ) ( a max : maximum value ) ( out : the output ndarray to hold the result )
( data : input ndarray ) ( block size : block of block size block size be move ) ( out : the output ndarray to hold the result )
( prob : the probability of sample '1' only one of prob or logit should be pass in ) ( logit : the log odds of sample '1' only one of prob or logit should be pass in ) ( size : output shape ) ( dtype : desire dtype of the result ) ( ctx : device context of output ) ( out : the output symbol default be none )
( scale : the scale parameter beta 1/lambda ) ( shape : the number of sample to draw if shape be e g m n and scale be a scalar output shape will be m n if scale be an ndarray with shape e g x y then output will have shape x y m n where m n sample be draw for each entry in scale ) ( dtype : data type of output sample default be 'float32' ) ( ctx : device context of output default be current context override by scale context when scale be an ndarray ) ( out : store output to an exist ndarray )
( alpha : the shape of the gamma distribution should be greater than zero ) ( beta : the scale of the gamma distribution should be greater than zero default be equal to 1 ) ( shape : the number of sample to draw if shape be e g m n and alpha and beta be scalars output shape will be m n if alpha and beta be ndarrays with shape e g x y then output will have shape x y m n where m n sample be draw for each alpha beta ) pair dtype : data type of output sample default be 'float32' ctx device context of output default be current context override by alpha context when alpha be an ndarray out store output to an exist ndarray
( loc : mean centre of the distribution ) ( scale : standard deviation spread or width of the distribution ) ( shape : the number of sample to draw if shape be e g m n and loc and scale be scalars output shape will be m n if loc and scale be ndarrays with shape e g x y then output will have shape x y m n where m n sample be draw for each loc scale ) pair dtype : data type of output sample default be 'float32' ctx device context of output default be current context override by loc context when loc be an ndarray out store output to an exist ndarray
( low : lower boundary of the output interval all value generate will be greater than or equal to low the default value be 0 ) ( high : upper boundary of the output interval all value generate will be less than high the default value be 1 0 ) ( shape : the number of sample to draw if shape be e g m n and low and high be scalars output shape will be m n if low and high be ndarrays with shape e g x y then output will have shape x y m n where m n sample be draw for each low high ) pair dtype : data type of output sample default be 'float32' ctx device context of output default be current context override by low context when low be an ndarray out store output to an exist ndarray
( value : value to set )
( rnd type : random generator type can be 'gaussian' or 'uniform' ) ( factor type : can be 'avg' 'in' or 'out' ) ( magnitude : scale of random number )
( )
( scale : scale factor of weight ) ( rand type : use â€œuniformâ€ or â€œnormalâ€ random number to initialize weight )
( sigma : standard deviation of the normal distribution default standard deviation be 0 01 )
( scale : the bind on the range of the generate random value value be generate from the range scale scale default scale be 0 07 )
( )
( axis : the axis that should be normalize this be typically the channel c axis for instance after a conv2d layer with layout 'nchw' set axis 1 in batchnorm if layout 'nhwc' then set axis 3 ) ( momentum : momentum for the move average ) ( epsilon : small float add to variance to avoid divide by zero ) ( center : if true add offset of beta to normalize tensor if false beta be ignore ) ( scale : if true multiply by gamma if false gamma be not use when the next layer be linear also e g nn relu this can be disable since the scale will be do by the next layer ) ( use global stats : if true use global move statistics instead of local batch norm this will force change batch norm into a scale shift operator if false use local batch norm ) ( beta initializer : initializer for the beta weight ) ( gamma initializer : initializer for the gamma weight ) ( run mean initializer : initializer for the run mean ) ( run variance initializer : initializer for the run variance ) ( in channel : number of channel feature map in input data if not specify initialization will be defer to the first time forward be call and in channel will be infer from the shape of input data )
( hide size : the number of feature in the hide state h ) ( num layer : number of recurrent layer ) ( layout : the format of input and output tensors t n and c stand for sequence length batch size and feature dimension respectively ) ( dropout : if non zero introduce a dropout layer on the output of each rnn layer except the last layer ) ( bidirectional : if true become a bidirectional rnn ) ( i2h weight initializer : initializer for the input weight matrix use for the linear transformation of the input ) ( h2h weight initializer : initializer for the recurrent weight matrix use for the linear transformation of the recurrent state ) ( i2h bias initializer : initializer for the bias vector ) ( h2h bias initializer : initializer for the bias vector ) ( dtype : type to initialize the parameters and default state to ) ( input size : the number of expect feature in the input x if not specify it will be infer from input ) ( prefix : prefix of this block ) ( params : share parameters for this block )
( hide size : number of units in output symbol ) ( activation : type of activation function ) ( i2h weight initializer : initializer for the input weight matrix use for the linear transformation of the input ) ( h2h weight initializer : initializer for the recurrent weight matrix use for the linear transformation of the recurrent state ) ( i2h bias initializer : initializer for the bias vector ) ( h2h bias initializer : initializer for the bias vector ) ( prefix : prefix for name of block s and name of weight if params be none ) ( params : container for weight share between cells create if none )
( hide size : the number of feature in the hide state h ) ( num layer : number of recurrent layer ) ( layout : the format of input and output tensors t n and c stand for sequence length batch size and feature dimension respectively ) ( dropout : if non zero introduce a dropout layer on the output of each rnn layer except the last layer ) ( bidirectional : if true become a bidirectional rnn ) ( i2h weight initializer : initializer for the input weight matrix use for the linear transformation of the input ) ( h2h weight initializer : initializer for the input weight matrix use for the linear transformation of the input ) ( i2h bias initializer : initializer for the bias vector by default bias for the forget gate be initialize to 1 while all other bias be initialize to zero ) ( h2h bias initializer : initializer for the bias vector ) ( projection size : the number of feature after projection ) ( h2r weight initializer : initializer for the project recurrent weight matrix use for the linear transformation of the recurrent state to the project space ) ( state clip min : minimum clip value of lstm state this option must be use together with state clip max if none clip be not apply ) ( state clip max : maximum clip value of lstm state this option must be use together with state clip min if none clip be not apply ) ( state clip nan : whether to stop nan from propagate in state by clip it to min/max if the clip range be not specify this option be ignore ) ( dtype : type to initialize the parameters and default state to ) ( input size : the number of expect feature in the input x if not specify it will be infer from input ) ( prefix : prefix of this block ) ( params : share parameters for this block )
( hide size : number of units in output symbol ) ( i2h weight initializer : initializer for the input weight matrix use for the linear transformation of the input ) ( h2h weight initializer : initializer for the recurrent weight matrix use for the linear transformation of the recurrent state ) ( i2h bias initializer : initializer for the bias vector ) ( h2h bias initializer : initializer for the bias vector ) ( prefix : prefix for name of block s and name of weight if params be none ) ( params : container for weight share between cells create if none ) ( activation : activation type to use see nd/symbol activation for support type ) ( recurrent activation : activation type to use for the recurrent step see nd/symbol activation for support type ) ( input : data input tensor with shape batch size input size state a list of two initial recurrent state tensors each have shape batch size num hide ) ( output : out output tensor with shape batch size num hide next state a list of two output recurrent state tensors each have the same shape as state )
( pool size : size of the average pool windows ) ( stride : factor by which to downscale e g 2 will halve the input size if none it will default to pool size ) ( pad : if pad be non zero then the input be implicitly zero pad on both side for pad number of point ) ( layout : dimension order of data and out â€˜ncwâ€™ or â€˜nwcâ€™ â€˜nâ€™ â€˜câ€™ â€˜wâ€™ stand for batch channel and width time dimension respectively pad be apply on â€˜wâ€™ dimension ) ( ceil mode : when true will use ceil instead of floor to compute the output shape ) ( count include pad : when â€˜falseâ€™ will exclude pad elements when compute the average value )
( pool size : size of the average pool windows ) ( stride : factor by which to downscale e g 2 will halve the input size if none it will default to pool size ) ( pad : if pad be non zero then the input be implicitly zero pad on both side for pad number of point ) ( layout : dimension order of data and out â€˜ncwâ€™ or â€˜nwcâ€™ â€˜nâ€™ â€˜câ€™ â€˜wâ€™ stand for batch channel and width time dimension respectively pad be apply on â€˜wâ€™ dimension ) ( ceil mode : when true will use ceil instead of floor to compute the output shape ) ( count include pad : when â€˜falseâ€™ will exclude pad elements when compute the average value )
( pool size : size of the average pool windows ) ( stride : factor by which to downscale e g 2 will halve the input size if none it will default to pool size ) ( pad : if pad be non zero then the input be implicitly zero pad on both side for pad number of point ) ( layout : dimension order of data and out â€˜ncwâ€™ or â€˜nwcâ€™ â€˜nâ€™ â€˜câ€™ â€˜wâ€™ stand for batch channel and width time dimension respectively pad be apply on â€˜wâ€™ dimension ) ( ceil mode : when true will use ceil instead of floor to compute the output shape ) ( count include pad : when â€˜falseâ€™ will exclude pad elements when compute the average value )
( channel : the dimensionality of the output space i e the number of output channel filter in the convolution ) ( kernel size : specify the dimension of the convolution window ) ( stride : specify the stride of the convolution ) ( pad : if pad be non zero then the input be implicitly zero pad on both side for pad number of point ) ( dilation : specify the dilation rate to use for dilate convolution ) ( group : control the connections between input and output at group 1 all input be convolve to all output at group 2 the operation become equivalent to have two conv layer side by side each see half the input channel and produce half the output channel and both subsequently concatenate ) ( layout : dimension order of data and weight only support â€˜ncwâ€™ layout for now â€˜nâ€™ â€˜câ€™ â€˜wâ€™ stand for batch channel and width time dimension respectively convolution be apply on the â€˜wâ€™ dimension ) ( in channel : the number of input channel to this layer if not specify initialization will be defer to the first time forward be call and in channel will be infer from the shape of input data ) ( activation : activation function to use see activation if you donâ€™t specify anything no activation be apply ie â€œlinearâ€ activation a x x ) ( use bias : whether the layer use a bias vector ) ( weight initializer : initializer for the weight weight matrix ) ( bias initializer : initializer for the bias vector )
( channel : the dimensionality of the output space i e the number of output channel filter in the convolution ) ( kernel size : specify the dimension of the convolution window ) ( stride : specify the stride of the convolution ) ( pad : if pad be non zero then the input be implicitly zero pad on both side for pad number of point ) ( dilation : specify the dilation rate to use for dilate convolution ) ( group : control the connections between input and output at group 1 all input be convolve to all output at group 2 the operation become equivalent to have two conv layer side by side each see half the input channel and produce half the output channel and both subsequently concatenate ) ( layout : dimension order of data and weight only support â€˜ncwâ€™ layout for now â€˜nâ€™ â€˜câ€™ â€˜wâ€™ stand for batch channel and width time dimension respectively convolution be apply on the â€˜wâ€™ dimension ) ( in channel : the number of input channel to this layer if not specify initialization will be defer to the first time forward be call and in channel will be infer from the shape of input data ) ( activation : activation function to use see activation if you donâ€™t specify anything no activation be apply ie â€œlinearâ€ activation a x x ) ( use bias : whether the layer use a bias vector ) ( weight initializer : initializer for the weight weight matrix ) ( bias initializer : initializer for the bias vector )
( channel : the dimensionality of the output space i e the number of output channel filter in the convolution ) ( kernel size : specify the dimension of the convolution window ) ( stride : specify the stride of the convolution ) ( pad : if pad be non zero then the input be implicitly zero pad on both side for pad number of point ) ( output pad : control the amount of implicit zero paddings on both side of the output for output pad number of point for each dimension ) ( dilation : specify the dilation rate to use for dilate convolution ) ( group : control the connections between input and output at group 1 all input be convolve to all output at group 2 the operation become equivalent to have two conv layer side by side each see half the input channel and produce half the output channel and both subsequently concatenate ) ( layout : dimension order of data and weight only support â€˜ncwâ€™ layout for now â€˜nâ€™ â€˜câ€™ â€˜wâ€™ stand for batch channel and width time dimension respectively convolution be apply on the â€˜wâ€™ dimension ) ( in channel : the number of input channel to this layer if not specify initialization will be defer to the first time forward be call and in channel will be infer from the shape of input data ) ( activation : activation function to use see activation if you donâ€™t specify anything no activation be apply ie â€œlinearâ€ activation a x x ) ( use bias : whether the layer use a bias vector ) ( weight initializer : initializer for the weight weight matrix ) ( bias initializer : initializer for the bias vector )
( channel : the dimensionality of the output space i e the number of output channel filter in the convolution ) ( kernel size : specify the dimension of the convolution window ) ( stride : specify the stride of the convolution ) ( pad : if pad be non zero then the input be implicitly zero pad on both side for pad number of point ) ( dilation : specify the dilation rate to use for dilate convolution ) ( group : control the connections between input and output at group 1 all input be convolve to all output at group 2 the operation become equivalent to have two conv layer side by side each see half the input channel and produce half the output channel and both subsequently concatenate ) ( layout : dimension order of data and weight only support â€˜ncwâ€™ layout for now â€˜nâ€™ â€˜câ€™ â€˜wâ€™ stand for batch channel and width time dimension respectively convolution be apply on the â€˜wâ€™ dimension ) ( in channel : the number of input channel to this layer if not specify initialization will be defer to the first time forward be call and in channel will be infer from the shape of input data ) ( activation : activation function to use see activation if you donâ€™t specify anything no activation be apply ie â€œlinearâ€ activation a x x ) ( use bias : whether the layer use a bias vector ) ( weight initializer : initializer for the weight weight matrix ) ( bias initializer : initializer for the bias vector )
( channel : the dimensionality of the output space i e the number of output channel filter in the convolution ) ( kernel size : specify the dimension of the convolution window ) ( stride : specify the stride of the convolution ) ( pad : if pad be non zero then the input be implicitly zero pad on both side for pad number of point ) ( output pad : control the amount of implicit zero paddings on both side of the output for output pad number of point for each dimension ) ( dilation : specify the dilation rate to use for dilate convolution ) ( group : control the connections between input and output at group 1 all input be convolve to all output at group 2 the operation become equivalent to have two conv layer side by side each see half the input channel and produce half the output channel and both subsequently concatenate ) ( layout : dimension order of data and weight only support â€˜ncwâ€™ layout for now â€˜nâ€™ â€˜câ€™ â€˜wâ€™ stand for batch channel and width time dimension respectively convolution be apply on the â€˜wâ€™ dimension ) ( in channel : the number of input channel to this layer if not specify initialization will be defer to the first time forward be call and in channel will be infer from the shape of input data ) ( activation : activation function to use see activation if you donâ€™t specify anything no activation be apply ie â€œlinearâ€ activation a x x ) ( use bias : whether the layer use a bias vector ) ( weight initializer : initializer for the weight weight matrix ) ( bias initializer : initializer for the bias vector )
( units : dimensionality of the output space ) ( activation : activation function to use see help on activation layer if you donâ€™t specify anything no activation be apply ie â€œlinearâ€ activation a x x ) ( use bias : whether the layer use a bias vector ) ( flatten : whether the input tensor should be flatten if true all but the first axis of input data be collapse together if false all but the last axis of input data be keep the same and the transformation apply on the last axis ) ( dtype : data type of output embeddings ) ( weight initializer : initializer for the kernel weight matrix ) ( bias initializer : initializer for the bias vector ) ( in units : size of the input data if not specify initialization will be defer to the first time forward be call and in units will be infer from the shape of input data ) ( prefix : see document of block ) ( params : see document of block )
( rate : fraction of the input units to drop must be a number between 0 and 1 ) ( ax : the ax on which dropout mask be share if empty regular dropout be apply )
( kwargs : )
( pool size : size of the max pool windows ) ( stride : factor by which to downscale e g 2 will halve the input size if none it will default to pool size ) ( pad : if pad be non zero then the input be implicitly zero pad on both side for pad number of point ) ( layout : dimension order of data and out â€˜ncwâ€™ or â€˜nwcâ€™ â€˜nâ€™ â€˜câ€™ â€˜wâ€™ stand for batch channel and width time dimension respectively pool be apply on the w dimension ) ( ceil mode : when true will use ceil instead of floor to compute the output shape )
( pool size : size of the max pool windows ) ( stride : factor by which to downscale e g 2 will halve the input size if none it will default to pool size ) ( pad : if pad be non zero then the input be implicitly zero pad on both side for pad number of point ) ( layout : dimension order of data and out â€˜ncwâ€™ or â€˜nwcâ€™ â€˜nâ€™ â€˜câ€™ â€˜wâ€™ stand for batch channel and width time dimension respectively pool be apply on the w dimension ) ( ceil mode : when true will use ceil instead of floor to compute the output shape )
( pool size : size of the max pool windows ) ( stride : factor by which to downscale e g 2 will halve the input size if none it will default to pool size ) ( pad : if pad be non zero then the input be implicitly zero pad on both side for pad number of point ) ( layout : dimension order of data and out â€˜ncwâ€™ or â€˜nwcâ€™ â€˜nâ€™ â€˜câ€™ â€˜wâ€™ stand for batch channel and width time dimension respectively pool be apply on the w dimension ) ( ceil mode : when true will use ceil instead of floor to compute the output shape )
( )
( )
( )
( data : the input array ) ( axis : the axis along which to compute softmax ) ( temperature : temperature parameter in softmax ) ( dtype : dtype of the output in case this canâ€™t be infer default to the same as inputâ€™s dtype if not define dtype none ) ( use length : whether to use the length input as a mask over the data input ) ( out : the output ndarray to hold the result )
( data : the input array ) ( mode : specify how to compute the softmax if set to instance it compute softmax for each instance if set to channel it compute cross channel softmax for each position of each instance ) ( out : the output ndarray to hold the result ) ( name : null )
( layout : layout of prediction tensor â€˜nâ€™ â€˜tâ€™ â€˜câ€™ stand for batch size sequence length and alphabet size respectively ) ( label layout : layout of the label â€˜nâ€™ â€˜tâ€™ stand for batch size and sequence length respectively ) ( weight : global scalar weight for loss )
( )
( hide size : number of units in output symbol ) ( i2h weight initializer : initializer for the input weight matrix use for the linear transformation of the input ) ( h2h weight initializer : initializer for the recurrent weight matrix use for the linear transformation of the recurrent state ) ( i2h bias initializer : initializer for the bias vector ) ( h2h bias initializer : initializer for the bias vector ) ( prefix : prefix for name of block s and name of weight if params be none ) ( params : container for weight share between cells create if none )
( data : the input ) ( ord : order of the norm currently ord 1 and ord 2 be support ) ( axis : the axis or ax along which to perform the reduction the default axis will compute over all elements into a scalar array with shape 1 if axis be int a reduction be perform on a particular axis if axis be a 2 tuple it specify the ax that hold 2 d matrices and the matrix norms of these matrices be compute ) ( out dtype : the data type of the output ) ( keepdims : if this be set to true the reduce axis be leave in the result as dimension with size one ) ( out : the output ndarray to hold the result ) ( name : null )
( data : an n dimensional input array ) ( mode : pad type to use â€œconstantâ€ pad with constant value â€œedgeâ€ pad use the edge value of the input array â€œreflectâ€ pad by reflect value with respect to the edge ) ( pad width : widths of the pad regions apply to the edge of each axis it be a tuple of integer pad widths for each axis of the format before 1 after 1 before n after n it should be of length 2 n where n be the number of dimension of the array this be equivalent to pad width in numpy pad but flatten ) ( constant value : the value use for pad when mode be â€œconstantâ€ ) ( out : the output ndarray to hold the result ) ( name : null )
( self : null ) ( filename : path to parameter file ) ( ctx : context s initialize load parameters on ) ( allow miss : whether to silently skip load parameters not represent in the file ) ( ignore extra : whether to silently ignore parameters from the file that be not present in this parameterdict ) ( restore prefix : prepend prefix to name of store parameters before load ) ( cast dtype : cast the data type of the parameter ) ( dtype source : must be in â€˜currentâ€™ â€˜savedâ€™ only valid if cast dtype true specify the source of the dtype for cast the parameters )
( data : input ndarray ) ( block size : block of block size block size be move ) ( out : the output ndarray to hold the result ) ( name : null )
( rho : decay rate for both square gradients and delta ) ( epsilon : small value to avoid division by 0 )
( eps : initial value of the history accumulator avoid division by 0 )
( beta1 : exponential decay rate for the first moment estimate ) ( beta2 : exponential decay rate for the second moment estimate ) ( epsilon : small value to avoid division by 0 ) ( lazy update : default be true if true lazy update be apply if the storage type of weight and grad be both row sparse )
( gamma1 : a decay factor of move average over past square gradient ) ( gamma2 : a â€œmomentumâ€ factor only use if center true ) ( epsilon : small value to avoid division by 0 ) ( center : flag to control which version of rmsprop to use ) ( clip weight : clip weight into range clip weight clip weight )
( head : output ndarray s ) ( variables : input variables to compute gradients for ) ( head grads : gradients with respect to head ) ( retain graph : whether to keep computation graph to differentiate again instead of clear history and release memory default to the same value as create graph ) ( create graph : whether to record gradient graph for compute higher order ) ( train mode : whether to do backward for train or prediction )
( )
( alpha : the alpha parameter as describe by clevert et al 2016 )
( data : the input array ) ( alpha : slope of hard sigmoid ) ( beta : bias of hard sigmoid ) ( out : the output ndarray to hold the result ) ( name : null )
( )
( lhs : the first input ) ( rhs : the second input ) ( transpose a : if true then transpose the first input before dot ) ( transpose b : if true then transpose the second input before dot ) ( forward stype : the desire storage type of the forward output give by user if thecombination of input storage type and this hint do not matchany implement ones the dot operator will perform fallback operationand still produce an output of the desire storage type ) ( out : the output ndarray to hold the result ) ( name : null )
( input dim : size of the vocabulary i e maximum integer index 1 ) ( output dim : dimension of the dense embed ) ( dtype : data type of output embeddings ) ( weight initializer : initializer for the embeddings matrix ) ( sparse grad : if true gradient w r t weight will be a â€˜row sparseâ€™ ndarray ) ( input : data n 1 d tensor with shape x1 x2 xn 1 ) ( output : out n d tensor with shape x1 x2 xn 1 output dim )
( axis : the axis that should be normalize this be typically the axis of the channel ) ( epsilon : small float add to variance to avoid divide by zero ) ( center : if true add offset of beta to normalize tensor if false beta be ignore ) ( scale : if true multiply by gamma if false gamma be not use ) ( beta initializer : initializer for the beta weight ) ( gamma initializer : initializer for the gamma weight ) ( in channel : number of channel feature map in input data if not specify initialization will be defer to the first time forward be call and in channel will be infer from the shape of input data )
( alpha : slope coefficient for the negative half axis must be 0 )
( alpha initializer : initializer for the embeddings matrix ) ( in channel : number of channel alpha parameters to learn can either be 1 or n where n be the size of the second dimension of the input tensor ) ( input : data input tensor with arbitrary shape ) ( output : out output tensor with the same shape as data )
( hide size : the number of feature in the hide state h ) ( num layer : number of recurrent layer ) ( activation : the activation function to use ) ( layout : the format of input and output tensors t n and c stand for sequence length batch size and feature dimension respectively ) ( dropout : if non zero introduce a dropout layer on the output of each rnn layer except the last layer ) ( bidirectional : if true become a bidirectional rnn ) ( i2h weight initializer : initializer for the input weight matrix use for the linear transformation of the input ) ( h2h weight initializer : initializer for the recurrent weight matrix use for the linear transformation of the recurrent state ) ( i2h bias initializer : initializer for the bias vector ) ( h2h bias initializer : initializer for the bias vector ) ( input size : the number of expect feature in the input x if not specify it will be infer from input ) ( dtype : type to initialize the parameters and default state to ) ( prefix : prefix of this block ) ( params : share parameters for this block )
( data : array of tensors to upsample for bilinear upsampling there should be 2 input 1 data and 1 weight ) ( scale : up sample scale ) ( num filter : input filter only use by bilinear sample type since bilinear upsampling use deconvolution num filter be set to the number of channel ) ( sample type : upsampling method ) ( multi input mode : how to handle multiple input concat mean concatenate upsampled image along the channel dimension sum mean add all image together only available for nearest neighbor upsampling ) ( workspace : tmp workspace for deconvolution mb ) ( out : the output ndarray to hold the result )
( axis : the axis to sum over when compute softmax and entropy ) ( sparse label : whether label be an integer array instead of probability distribution ) ( from logits : whether input be a log probability usually from log softmax instead of unnormalized number ) ( weight : global scalar weight for loss ) ( batch axis : the axis that represent mini batch )
( axis : the axis to sum over when compute softmax and entropy ) ( sparse label : whether label be an integer array instead of probability distribution ) ( from logits : whether input be a log probability usually from log softmax instead of unnormalized number ) ( weight : global scalar weight for loss ) ( batch axis : the axis that represent mini batch )
( weight : global scalar weight for loss ) ( batch axis : the axis that represent mini batch ) ( margin : margin of separation between correct and incorrect pair )
( from logits : indicate whether log predict value have already be compute if true the loss be compute as exp pred âˆ’targetâˆ—pred and if false then loss be compute as predâˆ’targetâˆ—log pred epsilon the default value ) ( weight : global scalar weight for loss ) ( batch axis : the axis that represent mini batch ) ( compute full : indicate whether to add an approximation stirling factor for the factorial term in the formula for the loss the stirling factor be targetâˆ—log target âˆ’target 0 5âˆ—log 2âˆ—ðœ‹âˆ—target ) ( epsilon : this be to avoid calculate log 0 which be not define )
( prefix : the file prefix to checkpoint to ) ( epoch : the current epoch number )
( step : change the learn rate for every n update ) ( factor : the factor to change the learn rate ) ( stop factor lr : stop update the learn rate if it be less than this value )
( momentum : the momentum value ) ( lazy update : default be true if true lazy update be apply if the storage type of weight and grad be both row sparse ) ( multi precision : flag to control the internal precision of the optimizer false result in use the same precision as the weight default true make internal 32 bite copy of the weight and apply gradients in 32 bite precision even if actual weight use in the model have lower precision turn this on can improve convergence and accuracy when train with float16 )
( )
( train mode : whether to do forward for train or predict )
( data : the input ) ( num output : number of split note that this should evenly divide the length of the axis ) ( axis : axis along which to split ) ( squeeze axis : if true remove the axis with length 1 from the shape of the output array note that set squeeze axis to true remove axis with length 1 only along the axis which it be split also squeeze axis can be set to true only if input shape axis num output ) ( out : the output ndarray to hold the result ) ( name : null )
( ar : input array unless axis be specify this will be flatten if it be not already 1 d ) ( return index : if true also return the indices of ar along the specify axis if provide or in the flatten array that result in the unique array ) ( return inverse : if true also return the indices of the unique array for the specify axis if provide that can be use to reconstruct ar ) ( return count : if true also return the number of time each unique item appear in ar ) ( axis : the axis to operate on if none ar will be flatten if an integer the subarrays index by the give axis will be flatten and treat as the elements of a 1 d array with the dimension of the give axis see the note for more detail the default be none )
( x2 xn : 1 d array represent the coordinate of a grid ) ( index : cartesian 'xy' default or matrix 'ij' index of output see note for more detail ) ( sparse : if true a sparse grid be return in order to conserve memory default be false please note that sparse true be currently not support ) ( copy : if false a view into the original array be return in order to conserve memory default be true please note that copy false be currently not support )
( n : number of row in the output ) ( m : number of columns in the output if none default to n ) ( k : index of the diagonal 0 the default refer to the main diagonal a positive value refer to an upper diagonal and a negative value to a lower diagonal ) ( dtype : data type of the return array when npx be np default dtype return false default dtype be float32 when npx be np default dtype return true default dtype be float64 ) ( device : device context on which the memory be allocate default be mxnet device current device )
( subscripts : specify the subscripts for summation as comma separate list of subscript label an implicit classical einstein summation calculation be perform unless the explicit indicator >â€™ be include as well as subscript label of the precise output form ) ( operands : these be the array for the operation ) ( out : if provide the calculation be do into this array ) ( optimize : control if intermediate optimization should occur no optimization will occur if false default to false )
