
( enabled  bool  optional  : set this to false make this context manager a no op  ) ( use cuda  bool  optional  : enable time of cuda events as well use the cudaevent api  add approximately 4us of overhead to each tensor operation  ) ( record shapes  bool  optional  : if shape record be set  information about input dimension will be collect  this allow one to see which dimension have be use under the hood and further group by them use prof key average group by input shape true   please note that shape record might skew your profile data  it be recommend to use separate run with and without shape record to validate the time  most likely the skew will be negligible for bottom most events  in a case of nest function call   but for higher level function the total self cpu time might be artificially increase because of the shape collection  ) ( with flops  bool  optional  : if with flop be set  the profiler will estimate the flop  float point operations  value use the operator’s input shape  this allow one to estimate the hardware performance  currently  this option only work for the matrix multiplication and 2d convolution operators  ) ( profile memory  bool  optional  : track tensor memory allocation/deallocation  ) ( with stack  bool  optional  : record source information  file and line number  for the ops  ) ( with modules  bool  : record module hierarchy  include function name  correspond to the callstack of the op  e g  if module a’s forward call’s module b’s forward which contain an aten  add op  then aten  add’s module hierarchy be a b note that this support exist  at the moment  only for torchscript model and not eager mode model  ) ( use kineto  bool  optional  : experimental  enable profile with kineto profiler  ) ( use cpu  bool  optional  : profile cpu events  set to false require use kineto true and can be use to lower the overhead for gpu only profile  )
( enabled  bool  optional  default True  : set enable false make this context manager a no op  default  true  ) ( record shapes  bool  optional  default False  : if record shape true  the nvtx range wrap each autograd op will append information about the size of tensor arguments receive by that op  in the follow format    arg0 size 0   arg0 size 1          arg1 size 0   arg1 size 1              non tensor arguments will be represent by     arguments will be list in the order they be receive by the backend op  please note that this order may not match the order in which those arguments be pass on the python side   also note that shape record may increase the overhead of nvtx range creation  )

( mode  bool  : flag whether to enable anomaly detection  true   or disable  false   )

( new scale  float or torch cuda FloatTensor  optional  default None  : new scale factor  )





( state dict  dict  : scaler state   should be an object return from a call to state dict    )
( outputs  Tensor or iterable of Tensors  : output to scale  )
( new scale  float  : value to use as the new scale backoff factor  )
( new scale  float  : value to use as the new scale growth factor  )
( new interval  int  : value to use as the new growth interval  )

( optimizer  torch optim Optimizer  : optimizer that apply the gradients  ) ( args : any arguments  ) ( kwargs : any keyword arguments  )
( optimizer  torch optim Optimizer  : optimizer that own the gradients to be unscaled  )
( new scale  float or torch cuda FloatTensor  optional  default None  : new scale factor  )
( cast inputs  torch dtype or None  optional  default None  : if not none  when forward run in an autocast enable region  cast incoming float point cuda tensors to the target dtype  non float point tensors be not affect   then execute forward with autocast disable  if none  forward’s internal ops execute with the current autocast state  )



( name  str  : backend name of the processgroup extension  it should match the one in init process group    ) ( func  function  : function handler that instantiate the backend  the function should be implement in the backend extension and take four arguments  include store  rank  world size  and timeout  )

( backend  str or Backend  : the backend to use  depend on build time configurations  valid value include mpi  gloo  and nccl  this field should be give as a lowercase string  e g   "gloo"   which can also be access via backend attribute  e g   backend gloo   if use multiple process per machine with nccl backend  each process must have exclusive access to every gpu it use  as share gpus between process can result in deadlocks  ) ( init method  str  optional  : url specify how to initialize the process group  default be “env //” if no init method or store be specify  mutually exclusive with store  ) ( world size  int  optional  : number of process participate in the job  require if store be specify  ) ( rank  int  optional  : rank of the current process  it should be a number between 0 and world size 1   require if store be specify  ) ( store  Store  optional  : key/value store accessible to all workers  use to exchange connection/address information  mutually exclusive with init method  ) ( timeout  timedelta  optional  : timeout for operations execute against the process group  default value equal 30 minutes  this be applicable for the gloo backend  for nccl  this be applicable only if the environment variable nccl block wait or nccl async error handle be set to 1  when nccl block wait be set  this be the duration for which the process will block and wait for collectives to complete before throw an exception  when nccl async error handle be set  this be the duration after which collectives will be abort asynchronously and the process will crash  nccl block wait will provide errors to the user which can be catch and handle  but due to its block nature  it have a performance overhead  on the other hand  nccl async error handle have very little performance overhead  but crash the process on errors  this be do since cuda execution be async and it be no longer safe to continue execute user code since fail async nccl operations might result in subsequent cuda operations run on corrupt data  only one of these two environment variables should be set  ) ( group name  str  optional  deprecated  : group name  ) ( pg options  ProcessGroupOptions  optional  : process group options specify what additional options need to be pass in during the construction of specific process group  as of now  the only options we support be processgroupnccl options for the nccl backend  be high priority stream can be specify so that the nccl backend can pick up high priority cuda stream when there’re compute kernels wait  )



( group  ProcessGroup  optional  : the process group to work on  the default be the general main process group  if another specific group be specify  the call process must be part of group  )
( group  ProcessGroup  optional  : the process group to work on  if none  the default process group will be use  )
( group  ProcessGroup  optional  : the process group to work on  if none  the default process group will be use  )
( ranks  list int   : list of rank of group members  if none  will be set to all rank  default be none  ) ( timeout  timedelta  optional  : timeout for operations execute against the process group  default value equal 30 minutes  this be applicable for the gloo backend  for nccl  this be applicable only if the environment variable nccl block wait or nccl async error handle be set to 1  when nccl block wait be set  this be the duration for which the process will block and wait for collectives to complete before throw an exception  when nccl async error handle be set  this be the duration after which collectives will be abort asynchronously and the process will crash  nccl block wait will provide errors to the user which can be catch and handle  but due to its block nature  it have a performance overhead  on the other hand  nccl async error handle have very little performance overhead  but crash the process on errors  this be do since cuda execution be async and it be no longer safe to continue execute user code since fail async nccl operations might result in subsequent cuda operations run on corrupt data  only one of these two environment variables should be set  ) ( backend  str or Backend  optional  : the backend to use  depend on build time configurations  valid value be gloo and nccl  by default use the same backend as the global group  this field should be give as a lowercase string  e g   "gloo"   which can also be access via backend attribute  e g   backend gloo   if none be pass in  the backend correspond to the default process group will be use  default be none  ) ( pg options  ProcessGroupOptions  optional  : process group options specify what additional options need to be pass in during the construction of specific process group  i e  for the nccl backend  be high priority stream can be specify so that process group can pick up high priority cuda stream  )
( tensor  Tensor  : tensor to send  ) ( dst  int  : destination rank  ) ( group  ProcessGroup  optional  : the process group to work on  if none  the default process group will be use  ) ( tag  int  optional  : tag to match send with remote recv )
( tensor  Tensor  : tensor to fill with receive data  ) ( src  int  optional  : source rank  will receive from any process if unspecified  ) ( group  ProcessGroup  optional  : the process group to work on  if none  the default process group will be use  ) ( tag  int  optional  : tag to match recv with remote send )
( tensor  Tensor  : tensor to send  ) ( dst  int  : destination rank  ) ( group  ProcessGroup  optional  : the process group to work on  if none  the default process group will be use  ) ( tag  int  optional  : tag to match send with remote recv )
( tensor  Tensor  : tensor to fill with receive data  ) ( src  int  optional  : source rank  will receive from any process if unspecified  ) ( group  ProcessGroup  optional  : the process group to work on  if none  the default process group will be use  ) ( tag  int  optional  : tag to match recv with remote send )
( tensor  Tensor  : data to be send if src be the rank of current process  and tensor to be use to save receive data otherwise  ) ( src  int  : source rank  ) ( group  ProcessGroup  optional  : the process group to work on  if none  the default process group will be use  ) ( async op  bool  optional  : whether this op should be an async op )
( object list  List Any   : list of input object to broadcast  each object must be picklable  only object on the src rank will be broadcast  but each rank must provide list of equal size  ) ( src  int  : source rank from which to broadcast object list  ) ( group :  processgroup  optional   the process group to work on  if none  the default process group will be use  default be none  ) ( device  torch device  optional  : if not none  the object be serialize and convert to tensors which be move to the device before broadcast  default be none  )
( tensor  Tensor  : input and output of the collective  the function operate in place  ) ( op  optional  : one of the value from torch distribute reduceop enum   specify an operation use for element wise reductions  ) ( group  ProcessGroup  optional  : the process group to work on  if none  the default process group will be use  ) ( async op  bool  optional  : whether this op should be an async op )
( tensor  Tensor  : input and output of the collective  the function operate in place  ) ( dst  int  : destination rank ) ( op  optional  : one of the value from torch distribute reduceop enum   specify an operation use for element wise reductions  ) ( group  ProcessGroup  optional  : the process group to work on  if none  the default process group will be use  ) ( async op  bool  optional  : whether this op should be an async op )
( tensor list  list Tensor   : output list  it should contain correctly size tensors to be use for output of the collective  ) ( tensor  Tensor  : tensor to be broadcast from current process  ) ( group  ProcessGroup  optional  : the process group to work on  if none  the default process group will be use  ) ( async op  bool  optional  : whether this op should be an async op )
( object list  list Any   : output list  it should be correctly size as the size of the group for this collective and will contain the output  ) ( object  Any  : pickable python object to be broadcast from current process  ) ( group  ProcessGroup  optional  : the process group to work on  if none  the default process group will be use  default be none  )
( tensor  Tensor  : input tensor  ) ( gather list  list Tensor   optional  : list of appropriately size tensors to use for gather data  default be none  must be specify on the destination rank  ) ( dst  int  optional  : destination rank  default be 0  ) ( group  ProcessGroup  optional  : the process group to work on  if none  the default process group will be use  ) ( async op  bool  optional  : whether this op should be an async op )
( obj  Any  : input object  must be picklable  ) ( object gather list  list Any   : output list  on the dst rank  it should be correctly size as the size of the group for this collective and will contain the output  must be none on non dst rank   default be none  ) ( dst  int  optional  : destination rank   default be 0  ) ( group :  processgroup  optional   the process group to work on  if none  the default process group will be use  default be none  )
( tensor  Tensor  : output tensor  ) ( scatter list  list Tensor   : list of tensors to scatter  default be none  must be specify on the source rank  ) ( src  int  : source rank  default be 0  ) ( group  ProcessGroup  optional  : the process group to work on  if none  the default process group will be use  ) ( async op  bool  optional  : whether this op should be an async op )
( scatter object output list  List Any   : non empty list whose first element will store the object scatter to this rank  ) ( scatter object input list  List Any   : list of input object to scatter  each object must be picklable  only object on the src rank will be scatter  and the argument can be none for non src rank  ) ( src  int  : source rank from which to scatter scatter object input list  ) ( group :  processgroup  optional   the process group to work on  if none  the default process group will be use  default be none  )
( output  Tensor  : output tensor  ) ( input list  list Tensor   : list of tensors to reduce and scatter  ) ( group  ProcessGroup  optional  : the process group to work on  if none  the default process group will be use  ) ( async op  bool  optional  : whether this op should be an async op  )
( output tensor list  list Tensor   : list of tensors to be gather one per rank  ) ( input tensor list  list Tensor   : list of tensors to scatter one per rank  ) ( group  ProcessGroup  optional  : the process group to work on  if none  the default process group will be use  ) ( async op  bool  optional  : whether this op should be an async op  )
( group  ProcessGroup  optional  : the process group to work on  if none  the default process group will be use  ) ( async op  bool  optional  : whether this op should be an async op ) ( device ids   int   optional  : list of device/gpu ids  valid only for nccl backend  )
( group  ProcessGroup  optional  : the process group to work on  if none  the default process group will be use  ) ( timeout  datetime timedelta  optional  : timeout for monitor barrier  if none  the default process group timeout will be use  ) ( wait all ranks  bool  optional  : whether to collect all fail rank or not  by default  this be false and monitor barrier on rank 0 will throw on the first fail rank it encounter in order to fail fast  by set wait all rank true monitor barrier will collect all fail rank and throw an error contain information about all fail rank  )
( tensor list  List Tensor   : tensors that participate in the collective operation  if src be the rank  then the specify src tensor element of tensor list  tensor list src tensor   will be broadcast to all other tensors  on different gpus  in the src process and all tensors in tensor list of other non src process  you also need to make sure that len tensor list  be the same for all the distribute process call this function  ) ( src  int  : source rank  ) ( group  ProcessGroup  optional  : the process group to work on  if none  the default process group will be use  ) ( async op  bool  optional  : whether this op should be an async op ) ( src tensor  int  optional  : source tensor rank within tensor list )
( tensor list  List Tensor   : list of input and output tensors of the collective  the function operate in place and require that each tensor to be a gpu tensor on different gpus  you also need to make sure that len tensor list  be the same for all the distribute process call this function  ) ( op  optional  : one of the value from torch distribute reduceop enum   specify an operation use for element wise reductions  ) ( group  ProcessGroup  optional  : the process group to work on  if none  the default process group will be use  ) ( async op  bool  optional  : whether this op should be an async op )
( tensor list  List Tensor   : input and output gpu tensors of the collective  the function operate in place  you also need to make sure that len tensor list  be the same for all the distribute process call this function  ) ( dst  int  : destination rank ) ( op  optional  : one of the value from torch distribute reduceop enum   specify an operation use for element wise reductions  ) ( group  ProcessGroup  optional  : the process group to work on  if none  the default process group will be use  ) ( async op  bool  optional  : whether this op should be an async op ) ( dst tensor  int  optional  : destination tensor rank within tensor list )
( output tensor lists  List List Tensor    : output list  it should contain correctly size tensors on each gpu to be use for output of the collective  e g  output tensor list i  contain the all gather result that reside on the gpu of input tensor list i   note that each element of output tensor list have the size of world size   len input tensor list   since the function all gather the result from every single gpu in the group  to interpret each element of output tensor list i   note that input tensor list j  of rank k will be appear in output tensor list i  k   world size   j  also note that len output tensor list   and the size of each element in output tensor list  each element be a list  therefore len output tensor list i    need to be the same for all the distribute process call this function  ) ( input tensor list  List Tensor   : list of tensors on different gpus  to be broadcast from current process  note that len input tensor list  need to be the same for all the distribute process call this function  ) ( group  ProcessGroup  optional  : the process group to work on  if none  the default process group will be use  ) ( async op  bool  optional  : whether this op should be an async op )
( output tensor list  List Tensor   : output tensors  on different gpus  to receive the result of the operation  note that len output tensor list  need to be the same for all the distribute process call this function  ) ( input tensor lists  List List Tensor    : input list   it should contain correctly size tensors on each gpu to be use for input of the collective  e g  input tensor list i  contain the reduce scatter input that reside on the gpu of output tensor list i   note that each element of input tensor list have the size of world size   len output tensor list   since the function scatter the result from every single gpu in the group   to interpret each element of input tensor list i   note that output tensor list j  of rank k receive the reduce scatter result from input tensor list i  k   world size   j  also note that len input tensor list   and the size of each element in input tensor list  each element be a list  therefore len input tensor list i    need to be the same for all the distribute process call this function  ) ( group  ProcessGroup  optional  : the process group to work on  if none  the default process group will be use  ) ( async op  bool  optional  : whether this op should be an async op  )
( value  bool  : whether to enable validation  )

( probs  Number  Tensor  : the probability of sample 1 ) ( logits  Number  Tensor  : the log odds of sample 1 )
( concentration1  float or Tensor  : 1st concentration parameter of the distribution  often refer to as alpha  ) ( concentration0  float or Tensor  : 2nd concentration parameter of the distribution  often refer to as beta  )
( total count  int or Tensor  : number of bernoulli trials ) ( probs  Tensor  : event probabilities ) ( logits  Tensor  : event log odds )
( probs  Tensor  : event probabilities ) ( logits  Tensor  : event log probabilities  unnormalized  )
( loc  float or Tensor  : mode or median of the distribution  ) ( scale  float or Tensor  : half width at half maximum  )
( df  float or Tensor  : shape parameter of the distribution )
( probs  Number  Tensor  :  0 1  value parameters ) ( logits  Number  Tensor  : real value parameters whose sigmoid match ‘probs’ )
( concentration  Tensor  : concentration parameter of the distribution  often refer to as alpha  )
( rate  float or Tensor  : rate   1 / scale of the distribution )
( df1  float or Tensor  : degrees of freedom parameter 1 ) ( df2  float or Tensor  : degrees of freedom parameter 2 )
( concentration  float or Tensor  : shape parameter of the distribution  often refer to as alpha  ) ( rate  float or Tensor  : rate   1 / scale of the distribution  often refer to as beta  )
( probs  Number  Tensor  : the probability of sample 1  must be in range  0  1  ) ( logits  Number  Tensor  : the log odds of sample 1  )
( loc  float or Tensor  : location parameter of the distribution ) ( scale  float or Tensor  : scale parameter of the distribution )
( scale  float or Tensor  : scale of the full cauchy distribution )
( scale  float or Tensor  : scale of the full normal distribution )
( base distribution  torch distributions distribution Distribution  : a base distribution ) ( reinterpreted batch ndims  int  : the number of batch dim to reinterpret as event dim )
( concentration1  float or Tensor  : 1st concentration parameter of the distribution  often refer to as alpha  ) ( concentration0  float or Tensor  : 2nd concentration parameter of the distribution  often refer to as beta  )
( dimension  dim  : dimension of the matrices ) ( concentration  float or Tensor  : concentration/shape parameter of the distribution  often refer to as eta  )
( loc  float or Tensor  : mean of the distribution ) ( scale  float or Tensor  : scale of the distribution )
( loc  float or Tensor  : mean of log of distribution ) ( scale  float or Tensor  : standard deviation of log of the distribution )
( loc  Tensor  : mean of the distribution with shape batch shape   event shape ) ( cov factor  Tensor  : factor part of low rank form of covariance matrix with shape batch shape   event shape    rank   ) ( cov diag  Tensor  : diagonal part of low rank form of covariance matrix with shape batch shape   event shape )
( mixture distribution : torch distributions categorical like instance  manage the probability of select component  the number of categories must match the rightmost batch dimension of the component distribution  must have either scalar batch shape or batch shape match component distribution batch shape   1  ) ( component distribution : torch distributions distribution like instance  right most batch dimension index component  )
( total count  int  : number of trials ) ( probs  Tensor  : event probabilities ) ( logits  Tensor  : event log probabilities  unnormalized  )
( loc  Tensor  : mean of the distribution ) ( covariance matrix  Tensor  : positive definite covariance matrix ) ( precision matrix  Tensor  : positive definite precision matrix ) ( scale tril  Tensor  : lower triangular factor of covariance  with positive value diagonal )
( total count  float or Tensor  : non negative number of negative bernoulli trials to stop  although the distribution be still valid for real value count ) ( probs  Tensor  : event probabilities of success in the half open interval  0  1  ) ( logits  Tensor  : event log odds for probabilities of success )
( loc  float or Tensor  : mean of the distribution  often refer to as mu  ) ( scale  float or Tensor  : standard deviation of the distribution  often refer to as sigma  )
( probs  Tensor  : event probabilities ) ( logits  Tensor  : event log probabilities  unnormalized  )
( scale  float or Tensor  : scale parameter of the distribution ) ( alpha  float or Tensor  : shape parameter of the distribution )
( rate  Number  Tensor  : the rate parameter )
( temperature  Tensor  : relaxation temperature ) ( probs  Number  Tensor  : the probability of sample 1 ) ( logits  Number  Tensor  : the log odds of sample 1 )
( temperature  Tensor  : relaxation temperature ) ( probs  Number  Tensor  : the probability of sample 1 ) ( logits  Number  Tensor  : the log odds of sample 1 )
( temperature  Tensor  : relaxation temperature ) ( probs  Tensor  : event probabilities ) ( logits  Tensor  : unnormalized log probability for each event )
( df  float or Tensor  : degrees of freedom ) ( loc  float or Tensor  : mean of the distribution ) ( scale  float or Tensor  : scale of the distribution )

( low  float or Tensor  : lower range  inclusive   ) ( high  float or Tensor  : upper range  exclusive   )
( loc  torch Tensor  : an angle in radians  ) ( concentration  torch Tensor  : concentration parameter )
( scale  float or Tensor  : scale parameter of distribution  lambda   ) ( concentration  float or Tensor  : concentration parameter of distribution  k/shape   )

( loc  Tensor or float  : location parameter  ) ( scale  Tensor or float  : scale parameter  ) ( event dim  int  : optional size of event shape  this should be zero for univariate random variables  1 for distributions over vectors  2 for distributions over matrices  etc  )
( parts  list of Transform  : a list of transform to compose  ) ( cache size  int  : size of cache  if zero  no cache be do  if one  the latest single value be cache  only 0 and 1 be support  )


( base transform  Transform  : a base transform  ) ( reinterpreted batch ndims  int  : the number of extra rightmost dimension to treat as dependent  )


( in shape  torch Size  : the input event shape  ) ( out shape  torch Size  : the output event shape  )





( cache size  int  : size of cache  if zero  no cache be do  if one  the latest single value be cache  only 0 and 1 be support  )

( constraint  subclass of Constraint  : a subclass of constraint  or a singleton object of the desire class  ) ( factory  callable  : a callable that input a constraint object and return a  transform object  )
( value  Tensor  :  )

( expand  bool  : whether to expand the support over the batch dim to match the distribution’s batch shape  )
( batch shape  torch Size  : the desire expand size  ) (  instance : new instance provide by subclasses that need to override  expand  )
( value  Tensor  :  )
( value  Tensor  :  )




( value  bool  : whether to enable validation  )

























































































































































( constraint  subclass of Constraint  : a subclass of constraint  or a singleton object of the desire class  ) ( factory  callable  : a callable that input a constraint object and return a  transform object  )
( p  Distribution  : a distribution object  ) ( q  Distribution  : a distribution object  )
( type p  type  : a subclass of distribution  ) ( type q  type  : a subclass of distribution  )










( callback  Future  : a callable that take in one argument  which be the reference to this future  )

( result  BaseException  : the exception for this future  )
( result  object  : the result object of this future  )
( callback  Callable  : a callable that take this future as the only argument  )


( futures  list  : a list of future object  )
( futures  list  : a list of future object  )
( github  string  : a string with format “repo owner/repo name  tag name ” with an optional tag/branch  if tag name be not specify  the default branch be assume to be main if it exist  and otherwise master  example  ‘pytorch/vision 0 10’ ) ( force reload  bool  optional  : whether to discard the exist cache and force a fresh download  default be false  ) ( skip validation  bool  optional  : if false  torchhub will check that the branch or commit specify by the github argument properly belong to the repo owner  this will make request to the github api  you can specify a non default github token by set the github token environment variable  default be false  )
( github  string  : a string with format <repo owner/repo name  tag name > with an optional tag/branch  if tag name be not specify  the default branch be assume to be main if it exist  and otherwise master  example  ‘pytorch/vision 0 10’ ) ( model  string  : a string of entrypoint name define in repo’s hubconf py ) ( force reload  bool  optional  : whether to discard the exist cache and force a fresh download  default be false  ) ( skip validation  bool  optional  : if false  torchhub will check that the branch or commit specify by the github argument properly belong to the repo owner  this will make request to the github api  you can specify a non default github token by set the github token environment variable  default be false  )
( repo or dir  string  : if source be ‘github’  this should correspond to a github repo with format repo owner/repo name  tag name  with an optional tag/branch  for example ‘pytorch/vision 0 10’  if tag name be not specify  the default branch be assume to be main if it exist  and otherwise master  if source be ‘local’  then it should be a path to a local directory  ) ( model  string  : the name of a callable  entrypoint  define in the repo/dir’s hubconf py  ) (  args  optional  : the correspond args for callable model  ) ( source  string  optional  : ‘github’ or ‘local’  specify how repo or dir be to be interpret  default be ‘github’  ) ( force reload  bool  optional  : whether to force a fresh download of the github repo unconditionally  do not have any effect if source   'local'  default be false  ) ( verbose  bool  optional  : if false  mute message about hit local cache  note that the message about first download cannot be mute  do not have any effect if source   'local'  default be true  ) ( skip validation  bool  optional  : if false  torchhub will check that the branch or commit specify by the github argument properly belong to the repo owner  this will make request to the github api  you can specify a non default github token by set the github token environment variable  default be false  ) (   kwargs  optional  : the correspond kwargs for callable model  )
( url  string  : url of the object to download ) ( dst  string  : full path where object will be save  e g  /tmp/temporary file ) ( hash prefix  string  optional  : if not none  the sha256 download file should start with hash prefix  default  none ) ( progress  bool  optional  : whether or not to display a progress bar to stderr default  true )
( url  string  : url of the object to download ) ( model dir  string  optional  : directory in which to save the object ) ( map location  optional  : a function or a dict specify how to remap storage locations  see torch load  ) ( progress  bool  optional  : whether or not to display a progress bar to stderr  default  true ) ( check hash  bool  optional  : if true  the filename part of the url should follow the name convention filename <sha256> ext where <sha256> be the first eight or more digits of the sha256 hash of the content of the file  the hash be use to ensure unique name and to verify the content of the file  default  false ) ( file name  string  optional  : name for the download file  filename from url will be use if not set  )

( d  string  : path to a local folder to save download model   weight  )
( public api  function  : function expose by the public torch api originally call like public api  args    kwargs  on which arguments be now be check  ) ( relevant args  iterable  : iterable of arguments to check for   torch function   methods  ) ( args  tuple  : arbitrary positional arguments originally pass into public api  ) ( kwargs  tuple  : arbitrary keyword arguments originally pass into public api  )


( dispatcher  Callable  : a callable that return an iterable of tensor like pass into the function  )


( package  str  : the name of module package this resource should go it  e g  "my package my subpackage"   ) ( resource  str  : a unique name for the resource  use to identify it to load  ) ( text  str  : the content to save  )
( package  str  : the name of module package  e g  "my package my subpackage"   ) ( resource  str  : the unique name for the resource  ) ( encoding  str  optional  : pass to decode  default to 'utf 8'  ) ( errors  str  optional  : pass to decode  default to 'strict'  )
( filename  str  : path of file to search for  )
( f : the location to export to  can be a  string/path object contain a filename or a binary i/o object  ) ( importer : if a single importer be pass  use that to search for modules  if a sequence of importers be pass  an orderedimporter will be construct out of them  )

( include  Union List str   str   : a string e g  "my package my subpackage"  or list of string for the name of the modules to be externed  this can also be a glob style pattern  as describe in mock    ) ( exclude  Union List str   str   : an optional pattern that exclude some pattern that match the include string  )
( include  Union List str   str   : a string e g  "my package my subpackage"  or list of string for the name of the modules to be externed  this can also be a glob style pattern  as describe in mock    ) ( exclude  Union List str   str   : an optional pattern that exclude some pattern that match the include string  ) ( allow empty  bool  : an optional flag that specify whether the extern modules specify by this call to the extern method must be match to some module during package  if an extern module glob pattern be add with allow empty false  and close   be call  either explicitly or via   exit    before any modules match that pattern  an exception be throw  if allow empty true  no such exception be throw  )

( include  Union List str   str   : a string e g  “my package my subpackage”  or list of string for the name of the modules to be externed  this can also be a glob style pattern  as describe in mock    ) ( exclude  Union List str   str   : an optional pattern that exclude some pattern that match the include string  ) ( allow empty  bool  : an optional flag that specify whether the intern modules specify by this call to the intern method must be match to some module during package  if an intern module glob pattern be add with allow empty false  and close   be call  either explicitly or via   exit    before any modules match that pattern  an exception be throw  if allow empty true  no such exception be throw  )
( include  Union List str   str   : a string e g  "my package my subpackage"  or list of string for the name of the modules to be mock out  string can also be a glob style pattern string that may match multiple modules  any require dependencies that match this pattern string will be mock out automatically   examples  'torch   ' – match torch and all submodules of torch  e g  'torch nn' and 'torch nn functional' 'torch  ' – match 'torch nn' or 'torch functional'  but not 'torch nn functional' ) ( exclude  Union List str   str   : an optional pattern that exclude some pattern that match the include string  e g  include 'torch   '  exclude 'torch foo' will mock all torch package except 'torch foo'  default  be     ) ( allow empty  bool  : an optional flag that specify whether the mock implementation s  specify by this call to the mock   method must be match to some module during package  if a mock be add with allow empty false  and close   be call  either explicitly or via   exit    and the mock have not be match to a module use by the package be export  an exception be throw  if allow empty true  no such exception be throw  )



( package  str  : the name of module package this resource should go it  e g  "my package my subpackage"   ) ( resource  str  : a unique name for the resource  use to identify it to load  ) ( binary  str  : the data to save  )
( module name  str  : e g  my package my subpackage  code will be save to provide code for this package  ) ( dependencies  bool  optional  : if true  we scan the source for dependencies  )
( package  str  : the name of module package this resource should go in  e g  "my package my subpackage"   ) ( resource  str  : a unique name for the resource  use to identify it to load  ) ( obj  Any  : the object to save  must be picklable  ) ( dependencies  bool  optional  : if true  we scan the source for dependencies  )
( module name  str  : e g  my package my subpackage  code will be save to provide code for this package  ) ( src  str  : the python source code to save for this package  ) ( is package  bool  optional  : if true  this module be treat as a package  package be allow to have submodules  e g  my package my subpackage my subsubpackage   and resources can be save inside them  default to false  ) ( dependencies  bool  optional  : if true  we scan the source for dependencies  )
( package  str  : the name of module package this resource should go it  e g  "my package my subpackage"   ) ( resource  str  : a unique name for the resource  use to identify it to load  ) ( text  str  : the content to save  )
( file or buffer : a file like object  have to implement read    readline    tell    and seek     a string  or an os pathlike object contain a filename  ) ( module allowed  Callable  str   bool   optional  : a method to determine if a externally provide module should be allow  can be use to ensure package load do not depend on modules that the server do not support  default to allow anything  )
( include  Union List str   str   : an optional string e g  "my package my subpackage"  or optional list of string for the name of the file to be inluded in the zipfile representation  this can also be a glob style pattern  as describe in packageexporter mock   ) ( exclude  Union List str   str   : an optional pattern that exclude file whose name match the pattern  )

( name  str  : fully qualify name of the module to load  ) ( package   type   optional  : unused  but present to match the signature of importlib import module  default to none  )
( package  str  : the name of module package  e g  "my package my subpackage"   ) ( resource  str  : the unique name for the resource  )
( package  str  : the name of module package  e g  "my package my subpackage"   ) ( resource  str  : the unique name for the resource  ) ( map location : pass to torch load to determine how tensors be map to devices  default to none  )
( package  str  : the name of module package  e g  "my package my subpackage"   ) ( resource  str  : the unique name for the resource  ) ( encoding  str  optional  : pass to decode  default to 'utf 8'  ) ( errors  str  optional  : pass to decode  default to 'strict'  )
( filename  str  : path of file to search for  )
( activities  iterable  : list of activity group  cpu  cuda  to use in profile  support value  torch profiler profileractivity cpu  torch profiler profileractivity cuda  default value  profileractivity cpu and  when available  profileractivity cuda  ) ( schedule  callable  : callable that take step  int  as a single parameter and return profileraction value that specify the profiler action to perform at each step  ) ( on trace ready  callable  : callable that be call at each step when schedule return profileraction record and save during the profile  ) ( record shapes  bool  : save information about operator’s input shape  ) ( profile memory  bool  : track tensor memory allocation/deallocation  ) ( with stack  bool  : record source information  file and line number  for the ops  ) ( with flops  bool  : use formula to estimate the flop  float point operations  of specific operators  matrix multiplication and 2d convolution   ) ( with modules  bool  : record module hierarchy  include function name  correspond to the callstack of the op  e g  if module a’s forward call’s module b’s forward which contain an aten  add op  then aten  add’s module hierarchy be a b note that this support exist  at the moment  only for torchscript model and not eager mode model  ) ( use cuda  bool  : deprecate since version 1 8 1  use activities instead  )




( nonlinearity : the non linear function  nn functional name  ) ( param : optional parameter for the non linear function )
( tensor : an n dimensional torch tensor ) ( a : the lower bind of the uniform distribution ) ( b : the upper bind of the uniform distribution )
( tensor : an n dimensional torch tensor ) ( mean : the mean of the normal distribution ) ( std : the standard deviation of the normal distribution )
( tensor : an n dimensional torch tensor ) ( val : the value to fill the tensor with )
( tensor : an n dimensional torch tensor )
( tensor : an n dimensional torch tensor )
( tensor : a 2 dimensional torch tensor )
( tensor : a  3  4  5  dimensional torch tensor ) ( groups  optional  : number of group in the conv layer  default  1  )
( tensor : an n dimensional torch tensor ) ( gain : an optional scale factor )
( tensor : an n dimensional torch tensor ) ( gain : an optional scale factor )
( tensor : an n dimensional torch tensor ) ( a : the negative slope of the rectifier use after this layer  only use with 'leaky relu'  ) ( mode : either 'fan in'  default  or 'fan out'  choose 'fan in' preserve the magnitude of the variance of the weight in the forward pass  choose 'fan out' preserve the magnitudes in the backwards pass  ) ( nonlinearity : the non linear function  nn functional name   recommend to use only with 'relu' or 'leaky relu'  default   )
( tensor : an n dimensional torch tensor ) ( a : the negative slope of the rectifier use after this layer  only use with 'leaky relu'  ) ( mode : either 'fan in'  default  or 'fan out'  choose 'fan in' preserve the magnitude of the variance of the weight in the forward pass  choose 'fan out' preserve the magnitudes in the backwards pass  ) ( nonlinearity : the non linear function  nn functional name   recommend to use only with 'relu' or 'leaky relu'  default   )
( tensor : an n dimensional torch tensor  where n≥2n \geq 2n≥2 ) ( gain : optional scale factor )
( tensor : an n dimensional torch tensor ) ( sparsity : the fraction of elements in each column to be set to zero ) ( std : the standard deviation of the normal distribution use to generate the non zero value )
( model  torch nn Module  torch jit ScriptModule or torch jit ScriptFunction  : the model to be export  ) ( args  tuple or torch Tensor  : args can be structure either as   only a tuple of arguments  args    x  y  z      the tuple should contain model input such that model  args  be a valid invocation of the model  any non tensor arguments will be hard cod into the export model  any tensor arguments will become input of the export model  in the order they occur in the tuple   a tensor  args   torch tensor  1       this be equivalent to a 1 ary tuple of that tensor   a tuple of arguments end with a dictionary of name arguments  args    x           'y'  input y           'z'  input z       all but the last element of the tuple will be pass as non keyword arguments  and name arguments will be set from the last element  if a name argument be not present in the dictionary  it be assign the default value  or none if a default value be not provide   note if a dictionary be the last element of the args tuple  it will be interpret as contain name arguments  in order to pass a dict as the last non keyword arg  provide an empty dict as the last element of the args tuple  for example  instead of  torch onnx export      model       x         wrong  will be interpret as name arguments       y  z        "test onnx pb"    write  torch onnx export      model       x        y  z                 "test onnx pb"  ) ( f : a file like object  such that f fileno   return a file descriptor  or a string contain a file name   a binary protocol buffer will be write to this file  ) ( export params  bool  default True  : if true  all parameters will be export  set this to false if you want to export an untrained model  in this case  the export model will first take all of its parameters as arguments  with the order as specify by model state dict   value   ) ( verbose  bool  default False  : if true  print a description of the model be export to stdout  in addition  the final onnx graph will include the field doc string` from the export model which mention the source code locations for model  ) ( training  enum  default TrainingMode EVAL  : trainingmode eval  export the model in inference mode  trainingmode preserve  export the model in inference mode if model train be false and in train mode if model train be true  trainingmode train  export the model in train mode  disable optimizations which might interfere with train  ) ( input names  list of str  default empty list  : name to assign to the input nod of the graph  in order  ) ( output names  list of str  default empty list  : name to assign to the output nod of the graph  in order  ) ( operator export type  enum  default None  : none usually mean operatorexporttypes onnx  however if pytorch be build with  dpytorch onnx caffe2 bundle  none mean operatorexporttypes onnx aten fallback   operatorexporttypes onnx  export all ops as regular onnx ops  in the default opset domain   operatorexporttypes onnx fallthrough  try to convert all ops to standard onnx ops in the default opset domain  if unable to do so  e g  because support have not be add to convert a particular torch op to onnx   fall back to export the op into a custom opset domain without conversion  apply to custom ops as well as aten ops  for the export model to be usable  the runtime must support these non standard ops  operatorexporttypes onnx aten  all aten ops  in the torchscript namespace “aten”  be export as aten ops  in opset domain “org pytorch aten”   aten be pytorch’s build in tensor library  so this instruct the runtime to use pytorch’s implementation of these ops   warn model export this way be probably runnable only by caffe2   this may be useful if the numeric differences in implementations of operators be cause large differences in behavior between pytorch and caffe2  which be more common on untrained model    operatorexporttypes onnx aten fallback  try to export each aten op  in the torchscript namespace “aten”  as a regular onnx op  if we be unable to do so  e g  because support have not be add to convert a particular torch op to onnx   fall back to export an aten op  see documentation on operatorexporttypes onnx aten for context  for example  graph  0   float      3   int   prim  constant value 0        conversion unsupported    4   float   aten  triu  0   3      conversion support    5   float   aten  mul  4   0    return   5    assume aten  triu be not support in onnx  this will be export as  graph  0   float      1   long     onnx  constant value  0         not convert    2   float   aten  aten operator "triu"   0   1      convert    3   float   onnx  mul  2   0    return   3    if pytorch be build with caffe2  i e  with build caffe2 1   then caffe2 specific behavior will be enable  include special support for ops be produce by the modules describe in quantization   warn model export this way be probably runnable only by caffe2  ) ( opset version  int  default 9  : the version of the default  ai onnx  opset to target  must be >  7 and <  15  ) ( do constant folding  bool  default True  : apply the constant fold optimization  constant fold will replace some of the ops that have all constant input with pre compute constant nod  ) ( dynamic axes  dict<string  dict<python int  string>> or dict<string  list int >  default empty dict  : by default the export model will have the shape of all input and output tensors set to exactly match those give in args  to specify ax of tensors as dynamic  i e  know only at run time   set dynamic ax to a dict with schema   key  str   an input or output name  each name must also be provide in input name or output name  value  dict or list   if a dict  key be axis indices and value be axis name  if a list  each element be an axis index   for example  class summodule torch nn module       def forward self  x           return torch sum x  dim 1   torch onnx export summodule     torch ones 2  2     "onnx pb"                    input name  "x"   output name  "sum"     produce  input     name  "x"             shape           dim             dim value  2    axis 0                   dim             dim value  2    axis 1     output     name  "sum"             shape           dim             dim value  2    axis 0       while  torch onnx export summodule     torch ones 2  2     "onnx pb"                    input name  "x"   output name  "sum"                     dynamic ax                           dict value  manually name ax                       "x"   0  "my custom axis name"                           list value  automatic name                       "sum"   0                          produce  input     name  "x"             shape           dim             dim param  "my custom axis name"    axis 0                   dim             dim value  2    axis 1     output     name  "sum"             shape           dim             dim param  "sum dynamic ax 1"    axis 0     ) ( keep initializers as inputs  bool  default None  : if true  all the initializers  typically correspond to parameters  in the export graph will also be add as input to the graph  if false  then initializers be not add as input to the graph  and only the non parameter input be add as input  this may allow for better optimizations  e g  constant fold  by backends/runtimes  if opset version < 9  initializers must be part of graph input and this argument will be ignore and the behavior will be equivalent to set this argument to true  if none  then the behavior be choose automatically as follow   if operator export type operatorexporttypes onnx  the behavior be equivalent to set this argument to false  else  the behavior be equivalent to set this argument to true  ) ( custom opsets  dict<str  int>  default empty dict  : a dict with schema   key  str   opset domain name value  int   opset version  if a custom opset be reference by model but not mention in this dictionary  the opset version be set to 1  only custom opset domain name and version should be indicate through this argument  ) ( export modules as functions  bool or set of python type of nn Module  default False  : flag to enable export all nn module forward call as local function in onnx  or a set to indicate the particular type of modules to export as local function in onnx  this feature require opset version >  15  otherwise the export will fail  this be because opset version < 15 imply ir version < 8  which mean no local function support   false`` default   export ``nn module forward call as fine grain nod  true  export all nn module forward call as local function nod  set of type of nn module  export nn module forward call as local function nod  only if the type of the nn module be find in the set  )
( add node names  bool  default True  : whether or not to set nodeproto name  this make no difference unless google printer true  ) ( google printer  bool  default False  : if false  will return a custom  compact representation of the model  if true will return the protobuf’s message  debugstring    which be more verbose  )
( symbolic name  str  : the name of the custom operator in “<domain>  <op>” format  ) ( symbolic fn  Callable  : a function that take in the onnx graph and the input arguments to the current operator  and return new operator nod to add to the graph  ) ( opset version  int  : the onnx opset version in which to register  )
( model : same type and mean as model arg to export    ) ( mode : same type and mean as train arg to export    )

( params  iterable  : an iterable of torch tensor s or dict s  specify what tensors should be optimize  ) ( defaults :  dict   a dict contain default value of optimization options  use when a parameter group doesn’t specify them   )

( devices  iterable of CUDA IDs  : cuda devices for which to fork the rng   cpu rng state be always fork   by default  fork rng   operate on all devices  but will emit a warn if your machine have a lot of devices  since this function will run very slowly in that case  if you explicitly specify devices  this warn will be suppress ) ( enabled  bool  : if false  the rng be not fork   this be a convenience argument for easily disable the context manager without have to delete it and unindent your python code under it  )


( seed  int  : the desire seed  value must be within the inclusive range   0x8000 0000 0000 0000  0xffff ffff ffff ffff   otherwise  a runtimeerror be raise  negative input be remapped to positive value with the formula 0xffff ffff ffff ffff   seed  )

( new state  torch ByteTensor  : the desire state )
















































( actual  Any  : actual input  ) ( expected  Any  : expect input  ) ( allow subclasses  bool  : if true  default  and except for python scalars  input of directly relate type be allow  otherwise type equality be require  ) ( rtol  Optional float   : relative tolerance  if specify atol must also be specify  if omit  default value base on the dtype be select with the below table  ) ( atol  Optional float   : absolute tolerance  if specify rtol must also be specify  if omit  default value base on the dtype be select with the below table  ) ( equal nan  Union bool  str   : if true  two nan value will be consider equal  ) ( check device  bool  : if true  default   assert that correspond tensors be on the same device  if this check be disable  tensors on different device’s be move to the cpu before be compare  ) ( check dtype  bool  : if true  default   assert that correspond tensors have the same dtype  if this check be disable  tensors with different dtype’s be promote  to a common dtype  accord to torch promote type    before be compare  ) ( check layout  bool  : if true  default   assert that correspond tensors have the same layout  if this check be disable  tensors with different layout’s be convert to stride tensors before be compare  ) ( check stride  bool  : if true and correspond tensors be stride  assert that they have the same stride  ) ( msg  Optional str   : optional error message to use in case a failure occur during the comparison  )
( dataset  Dataset  : dataset from which to load the data  ) ( batch size  int  optional  : how many sample per batch to load  default  1   ) ( shuffle  bool  optional  : set to true to have the data reshuffle at every epoch  default  false   ) ( sampler  Sampler or Iterable  optional  : define the strategy to draw sample from the dataset  can be any iterable with   len   implement  if specify  shuffle must not be specify  ) ( batch sampler  Sampler or Iterable  optional  : like sampler  but return a batch of indices at a time  mutually exclusive with batch size  shuffle  sampler  and drop last  ) ( num workers  int  optional  : how many subprocesses to use for data load  0 mean that the data will be load in the main process   default  0  ) ( collate fn  callable  optional  : merge a list of sample to form a mini batch of tensor s    use when use batch load from a map style dataset  ) ( pin memory  bool  optional  : if true  the data loader will copy tensors into cuda pin memory before return them   if your data elements be a custom type  or your collate fn return a batch that be a custom type  see the example below  ) ( drop last  bool  optional  : set to true to drop the last incomplete batch  if the dataset size be not divisible by the batch size  if false and the size of dataset be not divisible by the batch size  then the last batch will be smaller   default  false  ) ( timeout  numeric  optional  : if positive  the timeout value for collect a batch from workers  should always be non negative   default  0  ) ( worker init fn  callable  optional  : if not none  this will be call on each worker subprocess with the worker id  an int in  0  num workers   1   as input  after seed and before data load   default  none  ) ( generator  torch Generator  optional  : if not none  this rng will be use by randomsampler to generate random index and multiprocessing to generate base seed for workers   default  none  ) ( prefetch factor  int  optional  keyword only arg  : number of sample load in advance by each worker  2 mean there will be a total of 2   num workers sample prefetched across all workers   default  2  ) ( persistent workers  bool  optional  : if true  the data loader will not shutdown the worker process after a dataset have be consume once  this allow to maintain the workers dataset instance alive   default  false  )


(  tensors  Tensor  : tensors that have the same size of the first dimension  )
( datasets  sequence  : list of datasets to be concatenate )
( datasets  iterable of IterableDataset  : datasets to be chain together )
( dataset  Dataset  : the whole dataset ) ( indices  sequence  : indices in the whole set select for subset )

( data source  Dataset  : dataset to sample from )
( data source  Dataset  : dataset to sample from ) ( replacement  bool  : sample be draw on demand with replacement if true  default ``false`` ) ( num samples  int  : number of sample to draw  default `len dataset `  ) ( generator  Generator  : generator use in sample  )
( indices  sequence  : a sequence of indices ) ( generator  Generator  : generator use in sample  )
( weights  sequence  : a sequence of weight  not necessary sum up to one ) ( num samples  int  : number of sample to draw ) ( replacement  bool  : if true  sample be draw with replacement  if not  they be draw without replacement  which mean that when a sample index be draw for a row  it cannot be draw again for that row  ) ( generator  Generator  : generator use in sample  )
( sampler  Sampler or Iterable  : base sampler  can be any iterable object ) ( batch size  int  : size of mini batch  ) ( drop last  bool  : if true  the sampler will drop the last batch if its size would be less than batch size )
( dataset : dataset use for sample  ) ( num replicas  int  optional  : number of process participate in distribute train  by default  world size be retrieve from the current distribute group  ) ( rank  int  optional  : rank of the current process within num replicas  by default  rank be retrieve from the current distribute group  ) ( shuffle  bool  optional  : if true  default   sampler will shuffle the indices  ) ( seed  int  optional  : random seed use to shuffle the sampler if shuffle true  this number should be identical across all process in the distribute group  default  0  ) ( drop last  bool  optional  : if true  then the sampler will drop the tail of the data to make it evenly divisible across the number of replicas  if false  the sampler will add extra indices to make the data evenly divisible across the replicas  default  false  )

( dataset  Dataset  : dataset to be split ) ( lengths  sequence  : lengths of split to be produce ) ( generator  Generator  : generator use for the random permutation  )


( names  iterable of str  : the desire name of the output tensor  may contain up to one ellipsis  )
( names  iterable of str  : the desire dimension order of the output tensor  may contain up to one ellipsis that be expand to all unmentioned dim name of self  )
( dim  Union int  str   : dimension to unflatten ) ( sizes  Union Tuple int  or torch Size  Tuple Tuple str  int     : new shape of the unflattened dimension )
( obj  Object  : object to test )
( obj  Object  : object to test )
( d  torch dtype  : the float point dtype to make the default  either torch float32 or torch float64  )
( t  type or string  : the float point tensor type or its name )
( precision : number of digits of precision for float point output  default   4   ) ( threshold : total number of array elements which trigger summarization rather than full repr  default   1000   ) ( edgeitems : number of array items in summary at begin and end of each dimension  default   3   ) ( linewidth : the number of character per line for the purpose of insert line break  default   80   thresholded matrices will ignore this parameter  ) ( profile : sane default for pretty print  can override with any of the above options   any one of default  short  full  ) ( sci mode : enable  true  or disable  false  scientific notation  if none  default  be specify  the value be define by torch  tensor str  formatter  this value be automatically choose by the framework  )
( tensor  Tensor  : tensor to split  ) ( split size or sections  int  or  list int   : size of a single chunk or list of size for each chunk ) ( dim  int  : dimension along which to split the tensor  )

( seed  int  : the desire seed  value must be within the inclusive range   0x8000 0000 0000 0000  0xffff ffff ffff ffff   otherwise  a runtimeerror be raise  negative input be remapped to positive value with the formula 0xffff ffff ffff ffff   seed  )


( new state  torch ByteTensor  : the desire state )
( dimension  Int  : the dimensionality of the sequence to be draw ) ( scramble  bool  optional  : set this to true will produce scramble sobol sequence  scramble be capable of produce better sobol sequence  default  false  ) ( seed  Int  optional  : this be the seed for the scramble  the seed of the random number generator be set to this  if specify  otherwise  it use a random seed  default  none )
( obj : save object ) ( f : a file like object  have to implement write and flush  or a string or os pathlike object contain a file name ) ( pickle module : module use for pickle metadata and object ) ( pickle protocol : can be specify to override the default protocol )
( f : a file like object  have to implement read    readline    tell    and seek     or a string or os pathlike object contain a file name ) ( map location : a function  torch device  string or a dict specify how to remap storage locations ) ( pickle module : module use for unpickling metadata and object  have to match the pickle module use to serialize file  ) ( pickle load args :  python 3 only  optional keyword arguments pass over to pickle module load   and pickle module unpickler    e g   errors      )


( mode  bool  : flag whether to enable grad  true   or disable  false   this can be use to conditionally enable gradients  )
( mode  bool  : flag whether to enable or disable inference mode )
( input  Tensor  : the input tensor  its data type must be either a float point or complex type  for complex input  the norm be calculate use the absolute value of each element  if the input be complex and neither dtype nor out be specify  the result’s data type will be the correspond float point type  e g  float if input be complexfloat   ) ( p  int  float  inf   inf  'fro'  'nuc'  optional  : the order of norm  default  'fro' the follow norms can be calculate    ord matrix norm vector norm    ’fro’ frobenius norm –  ‘nuc’ nuclear norm –  number – sum abs x   ord    1 /ord     the vector norm can be calculate across any number of dimension  the correspond dimension of input be flatten into one dimension  and the norm be calculate on the flatten dimension  frobenius norm produce the same result as p 2 in all case except when dim be a list of three or more dim  in which case frobenius norm throw an error  nuclear norm can only be calculate across exactly two dimension  ) ( dim  int  tuple of python ints  list of python ints  optional  : specify which dimension or dimension of input to calculate the norm across  if dim be none  the norm will be calculate across all dimension of input  if the norm type indicate by p do not support the specify number of dimension  an error will occur  ) ( keepdim  bool  optional  : whether the output tensors have dim retain or not  ignore if dim   none and out   none  default  false ) ( out  Tensor  optional  : the output tensor  ignore if dim   none and out   none  ) ( dtype  torch dtype  optional  : the desire data type of return tensor  if specify  the input tensor be cast to dtype while perform the operation  default  none  )
( input  Tensor  : the input tensor ) ( sorted  bool  : whether to sort the unique elements in ascend order before return as output  ) ( return inverse  bool  : whether to also return the indices for where elements in the original input end up in the return unique list  ) ( return counts  bool  : whether to also return the count for each unique element  ) ( dim  int  : the dimension to apply unique  if none  the unique of the flatten input be return  default  none )
( input  Tensor  : the input tensor ) ( return inverse  bool  : whether to also return the indices for where elements in the original input end up in the return unique list  ) ( return counts  bool  : whether to also return the count for each unique element  ) ( dim  int  : the dimension to apply unique  if none  the unique of the flatten input be return  default  none )
( input  Tensor  : the input tensor ) ( n fft  int  : size of fourier transform ) ( hop length  int  optional  : the distance between neighbor slide window frame  default  none  treat as equal to floor n fft / 4   ) ( win length  int  optional  : the size of window frame and stft filter  default  none   treat as equal to n fft  ) ( window  Tensor  optional  : the optional window function  default  none  treat as window of all 111 s  ) ( center  bool  optional  : whether to pad input on both side so that the ttt th frame be center at time t×hop lengtht \times \text hop\ length t×hop length  default  true ) ( pad mode  string  optional  : control the pad method use when center be true  default  "reflect" ) ( normalized  bool  optional  : control whether to return the normalize stft result default  false ) ( onesided  bool  optional  : control whether to return half of result to avoid redundancy for real input  default  true for real input and window  false otherwise  ) ( return complex  bool  optional  : whether to return a complex tensor  or a real tensor with an extra last dimension for the real and imaginary components  )
( input  Tensor  : the input tensor  expect to be output of stft    can either be complex  channel  fft size  n frame   or real  channel  fft size  n frame  2  where the channel dimension be optional   deprecate since version 1 8 0  real input be deprecate  use complex input as return by stft      return complex true  instead  ) ( n fft  int  : size of fourier transform ) ( hop length  Optional int   : the distance between neighbor slide window frame   default  n fft // 4  ) ( win length  Optional int   : the size of window frame and stft filter   default  n fft  ) ( window  Optional torch Tensor   : the optional window function   default  torch ones win length   ) ( center  bool  : whether input be pad on both side so that the ttt th frame be center at time t×hop lengtht \times \text hop\ length t×hop length   default  true  ) ( normalized  bool  : whether the stft be normalize   default  false  ) ( onesided  Optional bool   : whether the stft be onesided   default  true if n fft    fft size in the input size  ) ( length  Optional int   : the amount to trim the signal by  i e  the original signal length    default  whole signal  ) ( return complex  Optional bool   : whether the output should be complex  or if the input should be assume to derive from a real signal and window  note that this be incompatible with onesided true   default  false  )
( input  Tensor or list of Tensors  :  )
( input  Tensor or list of Tensors  :  )
( input  Tensor or list of Tensors  :  )
(  tensors : one or more tensors with 0  1  or 2 dimension  )
(  tensors : any number of tensors of the same type )
(  shapes  torch Size  : shape of tensors  )
(  tensors : any number of 1 dimensional tensors  )
( x1  Tensor  : input tensor of shape b×p×mb \times p \times mb×p×m  ) ( x2  Tensor  : input tensor of shape b×r×mb \times r \times mb×r×m  ) ( p : p value for the p norm distance to calculate between each vector pair ∈ 0 ∞ \in  0  \infty ∈ 0 ∞   ) ( compute mode : ‘use mm for euclid dist if necessary’   will use matrix multiplication approach to calculate euclidean distance  p   2  if p > 25 or r > 25 ‘use mm for euclid dist’   will always use matrix multiplication approach to calculate euclidean distance  p   2  ‘donot use mm for euclid dist’   will never use matrix multiplication approach to calculate euclidean distance  p   2  default  use mm for euclid dist if necessary  )
( equation  string  : the subscripts for the einstein summation  ) ( operands  List Tensor   : the tensors to compute the einstein summation of  )
( tensors  list of Tensor  : list of scalars or 1 dimensional tensors  scalars will be treat as tensors of size  1   1   1   automatically ) ( indexing :  str  optional   the index mode  either “xy” or “ij”  default to “ij”  see warn for future change  if “xy” be select  the first dimension correspond to the cardinality of the second input and the second dimension correspond to the cardinality of the first input  if “ij” be select  the dimension be in the same order as the cardinality of the input  )
( a  Tensor  : leave tensor to contract ) ( b  Tensor  : right tensor to contract ) ( dims  int or Tuple List int   List int   or List List int   containing two lists or Tensor  : number of dimension to contract or explicit list of dimension for a and b respectively )
( matrices  Tensors     : a sequence of 2 or more 2 d tensors whose product be to be determine  ) ( out  Tensor  optional  : the output tensor  ignore if out   none  )
( A  Tensor  : the tensor to factor of size  ∗ m n     m  n  ∗ m n  ) ( pivot  bool  optional  : control whether pivot be do  default  true ) ( get infos  bool  optional  : if set to true  return an info inttensor  default  false ) ( out  tuple  optional  : optional output tuple  if get infos be true  then the elements in the tuple be tensor  inttensor  and inttensor  if get infos be false  then the elements in the tuple be tensor  inttensor  default  none )

( A  Tensor  : the input tensor of size  ∗ m n     m  n  ∗ m n  ) ( q  int  optional  : a slightly overestimate rank of aaa  by default  q   min 6  m  n   ) ( center  bool  optional  : if true  center the input tensor  otherwise  assume that the input be center  ) ( niter  int  optional  : the number of subspace iterations to conduct  niter must be a nonnegative integer  and default to 2  )
( A  Tensor  : the input tensor of size  ∗ m m     m  m  ∗ m m  ) ( B  Tensor  optional  : the input tensor of size  ∗ m m     m  m  ∗ m m   when not specify  b be interpereted as identity matrix  ) ( X  tensor  optional  : the input tensor of size  ∗ m n     m  n  ∗ m n  where k <  n <  m  when specify  it be use as initial approximation of eigenvectors  x must be a dense tensor  ) ( iK  tensor  optional  : the input tensor of size  ∗ m m     m  m  ∗ m m   when specify  it will be use as preconditioner  ) ( k  integer  optional  : the number of request eigenpairs  default be the number of xxx columns  when specify  or 1  ) ( n  integer  optional  : if xxx be not specify then n specify the size of the generate random approximation of eigenvectors  default value for n be k  if xxx be specify  the value of n  when specify  must be the number of xxx columns  ) ( tol  float  optional  : residual tolerance for stop criterion  default be feps    0 5 where feps be smallest non zero float point number of the give input tensor a data type  ) ( largest  bool  optional  : when true  solve the eigenproblem for the largest eigenvalues  otherwise  solve the eigenproblem for smallest eigenvalues  default be true  ) ( method  str  optional  : select lobpcg method  see the description of the function above  default be “ortho”  ) ( niter  int  optional  : maximum number of iterations  when reach  the iteration process be hard stop and the current approximation of eigenpairs be return  for infinite iteration but until convergence criteria be meet  use  1  ) ( tracker  callable  optional  : a function for trace the iteration process  when specify  it be call at each iteration step with lobpcg instance as an argument  the lobpcg instance hold the full state of the iteration process in the follow attribute   iparams  fparams  bparams   dictionaries of integer  float  and boolean value input parameters  respectively ivars  fvars  bvars  tvars   dictionaries of integer  float  boolean  and tensor value iteration variables  respectively  a  b  ik   input tensor arguments  e  x  s  r   iteration tensor variables   for instance   ivars “istep”    the current iteration step x   the current approximation of eigenvectors e   the current approximation of eigenvalues r   the current residual ivars “converged count”    the current number of converge eigenpairs tvars “rerr”    the current state of convergence criteria  note that when tracker store tensor object from the lobpcg instance  it must make copy of these  if tracker set bvars “force stop”    true  the iteration process will be hard stop  ) ( ortho iparams  dict  optional  : various parameters to lobpcg algorithm when use method ”ortho”  ) ( ortho fparams  dict  optional  : various parameters to lobpcg algorithm when use method ”ortho”  ) ( ortho bparams  dict  optional  : various parameters to lobpcg algorithm when use method ”ortho”  )

( mode  bool  : if true  make potentially nondeterministic operations switch to a deterministic algorithm or throw a runtime error  if false  allow nondeterministic operations  )

( b  bool  : if true  force warn to always be emit if false  set to the default behaviour )

( data  Tensor  : parameter tensor  ) ( requires grad  bool  optional  : if the parameter require gradient  see locally disable gradient computation for more detail  default  true )



( name  string  : name of the child module  the child module can be access from this module use the give name ) ( module  Module  : child module to be add to the module  )
( recurse  bool  : if true  then yield buffer of this module and all submodules  otherwise  yield only buffer that be direct members of this module  )


( device  int  optional  : if specify  all parameters will be copy to that device )


( prefix  str  : prefix to prepend to all buffer name  ) ( recurse  bool  : if true  then yield buffer of this module and all submodules  otherwise  yield only buffer that be direct members of this module  )

( memo : a memo to store the set of modules already add to the result ) ( prefix : a prefix that will be add to the name of the module ) ( remove duplicate : whether to remove the duplicate module instance in the result or not )
( prefix  str  : prefix to prepend to all parameter name  ) ( recurse  bool  : if true  then yield parameters of this module and all submodules  otherwise  yield only parameters that be direct members of this module  )
( recurse  bool  : if true  then yield parameters of this module and all submodules  otherwise  yield only parameters that be direct members of this module  )

( name  string  : name of the buffer  the buffer can be access from this module use the give name ) ( tensor  Tensor or None  : buffer to be register  if none  then operations that run on buffer  such as cuda  be ignore  if none  the buffer be not include in the module’s state dict  ) ( persistent  bool  : whether the buffer be part of this module’s state dict  )



( name  string  : name of the parameter  the parameter can be access from this module use the give name ) ( param  Parameter or None  : parameter to be add to the module  if none  then operations that run on parameters  such as cuda  be ignore  if none  the parameter be not include in the module’s state dict  )
( requires grad  bool  : whether autograd should record operations on parameters in this module  default  true  )

( device  torch device  : the desire device of the parameters and buffer in this module ) ( dtype  torch dtype  : the desire float point or complex dtype of the parameters and buffer in this module ) ( tensor  torch Tensor  : tensor whose dtype and device be the desire dtype and device for all parameters and buffer in this module ) ( memory format  torch memory format  : the desire memory format for 4d parameters and buffer in this module  keyword only argument  )
( device  torch device  : the desire device of the parameters and buffer in this module  )
( mode  bool  : whether to set train mode  true  or evaluation mode  false   default  true  )
( device  int  optional  : if specify  all parameters will be copy to that device )

( modules  iterable  optional  : an iterable of modules to add )
( module  nn Module  : module to append )
( modules  iterable  : iterable of modules to append )
( index  int  : index to insert  ) ( module  nn Module  : module to insert )
( modules  iterable  optional  : a map  dictionary  of  string  module  or an iterable of key value pair of type  string  module  )
( key  string  : key to pop from the moduledict )
( parameters  iterable  optional  : an iterable of parameter to add )
( parameter  nn Parameter  : parameter to append )
( parameters  iterable  : iterable of parameters to append )
( parameters  iterable  optional  : a map  dictionary  of  string   parameter  or an iterable of key value pair of type  string  parameter  )
( key  string  : key to pop from the parameterdict )




( in channels  int  : number of channel in the input image ) ( out channels  int  : number of channel produce by the convolution ) ( kernel size  int or tuple  : size of the convolve kernel ) ( stride  int or tuple  optional  : stride of the convolution  default  1 ) ( padding  int  tuple or str  optional  : pad add to both side of the input  default  0 ) ( padding mode  string  optional  : 'zeros'  'reflect'  'replicate' or 'circular'  default  'zeros' ) ( dilation  int or tuple  optional  : space between kernel elements  default  1 ) ( groups  int  optional  : number of block connections from input channel to output channel  default  1 ) ( bias  bool  optional  : if true  add a learnable bias to the output  default  true )
( in channels  int  : number of channel in the input image ) ( out channels  int  : number of channel produce by the convolution ) ( kernel size  int or tuple  : size of the convolve kernel ) ( stride  int or tuple  optional  : stride of the convolution  default  1 ) ( padding  int  tuple or str  optional  : pad add to all four side of the input  default  0 ) ( padding mode  string  optional  : 'zeros'  'reflect'  'replicate' or 'circular'  default  'zeros' ) ( dilation  int or tuple  optional  : space between kernel elements  default  1 ) ( groups  int  optional  : number of block connections from input channel to output channel  default  1 ) ( bias  bool  optional  : if true  add a learnable bias to the output  default  true )
( in channels  int  : number of channel in the input image ) ( out channels  int  : number of channel produce by the convolution ) ( kernel size  int or tuple  : size of the convolve kernel ) ( stride  int or tuple  optional  : stride of the convolution  default  1 ) ( padding  int  tuple or str  optional  : pad add to all six side of the input  default  0 ) ( padding mode  string  optional  : 'zeros'  'reflect'  'replicate' or 'circular'  default  'zeros' ) ( dilation  int or tuple  optional  : space between kernel elements  default  1 ) ( groups  int  optional  : number of block connections from input channel to output channel  default  1 ) ( bias  bool  optional  : if true  add a learnable bias to the output  default  true )
( in channels  int  : number of channel in the input image ) ( out channels  int  : number of channel produce by the convolution ) ( kernel size  int or tuple  : size of the convolve kernel ) ( stride  int or tuple  optional  : stride of the convolution  default  1 ) ( padding  int or tuple  optional  : dilation    kernel size   1    pad zero pad will be add to both side of the input  default  0 ) ( output padding  int or tuple  optional  : additional size add to one side of the output shape  default  0 ) ( groups  int  optional  : number of block connections from input channel to output channel  default  1 ) ( bias  bool  optional  : if true  add a learnable bias to the output  default  true ) ( dilation  int or tuple  optional  : space between kernel elements  default  1 )
( in channels  int  : number of channel in the input image ) ( out channels  int  : number of channel produce by the convolution ) ( kernel size  int or tuple  : size of the convolve kernel ) ( stride  int or tuple  optional  : stride of the convolution  default  1 ) ( padding  int or tuple  optional  : dilation    kernel size   1    pad zero pad will be add to both side of each dimension in the input  default  0 ) ( output padding  int or tuple  optional  : additional size add to one side of each dimension in the output shape  default  0 ) ( groups  int  optional  : number of block connections from input channel to output channel  default  1 ) ( bias  bool  optional  : if true  add a learnable bias to the output  default  true ) ( dilation  int or tuple  optional  : space between kernel elements  default  1 )
( in channels  int  : number of channel in the input image ) ( out channels  int  : number of channel produce by the convolution ) ( kernel size  int or tuple  : size of the convolve kernel ) ( stride  int or tuple  optional  : stride of the convolution  default  1 ) ( padding  int or tuple  optional  : dilation    kernel size   1    pad zero pad will be add to both side of each dimension in the input  default  0 ) ( output padding  int or tuple  optional  : additional size add to one side of each dimension in the output shape  default  0 ) ( groups  int  optional  : number of block connections from input channel to output channel  default  1 ) ( bias  bool  optional  : if true  add a learnable bias to the output  default  true ) ( dilation  int or tuple  optional  : space between kernel elements  default  1 )
( out channels  int  : number of channel produce by the convolution ) ( kernel size  int or tuple  : size of the convolve kernel ) ( stride  int or tuple  optional  : stride of the convolution  default  1 ) ( padding  int or tuple  optional  : zero pad add to both side of the input  default  0 ) ( padding mode  string  optional  : 'zeros'  'reflect'  'replicate' or 'circular'  default  'zeros' ) ( dilation  int or tuple  optional  : space between kernel elements  default  1 ) ( groups  int  optional  : number of block connections from input channel to output channel  default  1 ) ( bias  bool  optional  : if true  add a learnable bias to the output  default  true )
( out channels  int  : number of channel produce by the convolution ) ( kernel size  int or tuple  : size of the convolve kernel ) ( stride  int or tuple  optional  : stride of the convolution  default  1 ) ( padding  int or tuple  optional  : zero pad add to both side of the input  default  0 ) ( padding mode  string  optional  : 'zeros'  'reflect'  'replicate' or 'circular'  default  'zeros' ) ( dilation  int or tuple  optional  : space between kernel elements  default  1 ) ( groups  int  optional  : number of block connections from input channel to output channel  default  1 ) ( bias  bool  optional  : if true  add a learnable bias to the output  default  true )
( out channels  int  : number of channel produce by the convolution ) ( kernel size  int or tuple  : size of the convolve kernel ) ( stride  int or tuple  optional  : stride of the convolution  default  1 ) ( padding  int or tuple  optional  : zero pad add to both side of the input  default  0 ) ( padding mode  string  optional  : 'zeros'  'reflect'  'replicate' or 'circular'  default  'zeros' ) ( dilation  int or tuple  optional  : space between kernel elements  default  1 ) ( groups  int  optional  : number of block connections from input channel to output channel  default  1 ) ( bias  bool  optional  : if true  add a learnable bias to the output  default  true )
( out channels  int  : number of channel produce by the convolution ) ( kernel size  int or tuple  : size of the convolve kernel ) ( stride  int or tuple  optional  : stride of the convolution  default  1 ) ( padding  int or tuple  optional  : dilation    kernel size   1    pad zero pad will be add to both side of the input  default  0 ) ( output padding  int or tuple  optional  : additional size add to one side of the output shape  default  0 ) ( groups  int  optional  : number of block connections from input channel to output channel  default  1 ) ( bias  bool  optional  : if true  add a learnable bias to the output  default  true ) ( dilation  int or tuple  optional  : space between kernel elements  default  1 )
( out channels  int  : number of channel produce by the convolution ) ( kernel size  int or tuple  : size of the convolve kernel ) ( stride  int or tuple  optional  : stride of the convolution  default  1 ) ( padding  int or tuple  optional  : dilation    kernel size   1    pad zero pad will be add to both side of each dimension in the input  default  0 ) ( output padding  int or tuple  optional  : additional size add to one side of each dimension in the output shape  default  0 ) ( groups  int  optional  : number of block connections from input channel to output channel  default  1 ) ( bias  bool  optional  : if true  add a learnable bias to the output  default  true ) ( dilation  int or tuple  optional  : space between kernel elements  default  1 )
( out channels  int  : number of channel produce by the convolution ) ( kernel size  int or tuple  : size of the convolve kernel ) ( stride  int or tuple  optional  : stride of the convolution  default  1 ) ( padding  int or tuple  optional  : dilation    kernel size   1    pad zero pad will be add to both side of each dimension in the input  default  0 ) ( output padding  int or tuple  optional  : additional size add to one side of each dimension in the output shape  default  0 ) ( groups  int  optional  : number of block connections from input channel to output channel  default  1 ) ( bias  bool  optional  : if true  add a learnable bias to the output  default  true ) ( dilation  int or tuple  optional  : space between kernel elements  default  1 )
( kernel size  int or tuple  : the size of the slide block ) ( stride  int or tuple  optional  : the stride of the slide block in the input spatial dimension  default  1 ) ( padding  int or tuple  optional  : implicit zero pad to be add on both side of input  default  0 ) ( dilation  int or tuple  optional  : a parameter that control the stride of elements within the neighborhood  default  1 )
( output size  int or tuple  : the shape of the spatial dimension of the output  i e   output size   2    ) ( kernel size  int or tuple  : the size of the slide block ) ( stride  int or tuple  : the stride of the slide block in the input spatial dimension  default  1 ) ( padding  int or tuple  optional  : implicit zero pad to be add on both side of input  default  0 ) ( dilation  int or tuple  optional  : a parameter that control the stride of elements within the neighborhood  default  1 )
( kernel size : the size of the slide window  must be > 0  ) ( stride : the stride of the slide window  must be > 0  default value be kernel size  ) ( padding : implicit negative infinity pad to be add on both side  must be >  0 and <  kernel size / 2  ) ( dilation : the stride between elements within a slide window  must be > 0  ) ( return indices : if true  will return the argmax along with the max value  useful for torch nn maxunpool1d later ) ( ceil mode : if true  will use ceil instead of floor to compute the output shape  this ensure that every element in the input tensor be cover by a slide window  )
( kernel size : the size of the window to take a max over ) ( stride : the stride of the window  default value be kernel size ) ( padding : implicit zero pad to be add on both side ) ( dilation : a parameter that control the stride of elements in the window ) ( return indices : if true  will return the max indices along with the output  useful for torch nn maxunpool2d later ) ( ceil mode : when true  will use ceil instead of floor to compute the output shape )
( kernel size : the size of the window to take a max over ) ( stride : the stride of the window  default value be kernel size ) ( padding : implicit zero pad to be add on all three side ) ( dilation : a parameter that control the stride of elements in the window ) ( return indices : if true  will return the max indices along with the output  useful for torch nn maxunpool3d later ) ( ceil mode : when true  will use ceil instead of floor to compute the output shape )
( kernel size  int or tuple  : size of the max pool window  ) ( stride  int or tuple  : stride of the max pool window  it be set to kernel size by default  ) ( padding  int or tuple  : pad that be add to the input )
( kernel size  int or tuple  : size of the max pool window  ) ( stride  int or tuple  : stride of the max pool window  it be set to kernel size by default  ) ( padding  int or tuple  : pad that be add to the input )
( kernel size  int or tuple  : size of the max pool window  ) ( stride  int or tuple  : stride of the max pool window  it be set to kernel size by default  ) ( padding  int or tuple  : pad that be add to the input )
( kernel size : the size of the window ) ( stride : the stride of the window  default value be kernel size ) ( padding : implicit zero pad to be add on both side ) ( ceil mode : when true  will use ceil instead of floor to compute the output shape ) ( count include pad : when true  will include the zero pad in the average calculation )
( kernel size : the size of the window ) ( stride : the stride of the window  default value be kernel size ) ( padding : implicit zero pad to be add on both side ) ( ceil mode : when true  will use ceil instead of floor to compute the output shape ) ( count include pad : when true  will include the zero pad in the average calculation ) ( divisor override : if specify  it will be use as divisor  otherwise size of the pool region will be use  )
( kernel size : the size of the window ) ( stride : the stride of the window  default value be kernel size ) ( padding : implicit zero pad to be add on all three side ) ( ceil mode : when true  will use ceil instead of floor to compute the output shape ) ( count include pad : when true  will include the zero pad in the average calculation ) ( divisor override : if specify  it will be use as divisor  otherwise kernel size will be use )
( kernel size : the size of the window to take a max over  can be a single number k  for a square kernel of k x k  or a tuple  kh  kw  ) ( output size : the target output size of the image of the form oh x ow  can be a tuple  oh  ow  or a single number oh for a square image oh x oh ) ( output ratio : if one want to have an output size as a ratio of the input size  this option can be give  this have to be a number or tuple in the range  0  1  ) ( return indices : if true  will return the indices along with the output  useful to pass to nn maxunpool2d    default  false )
( kernel size : the size of the window to take a max over  can be a single number k  for a square kernel of k x k x k  or a tuple  kt x kh x kw  ) ( output size : the target output size of the image of the form ot x oh x ow  can be a tuple  ot  oh  ow  or a single number oh for a square image oh x oh x oh ) ( output ratio : if one want to have an output size as a ratio of the input size  this option can be give  this have to be a number or tuple in the range  0  1  ) ( return indices : if true  will return the indices along with the output  useful to pass to nn maxunpool3d    default  false )
( kernel size : a single int  the size of the window ) ( stride : a single int  the stride of the window  default value be kernel size ) ( ceil mode : when true  will use ceil instead of floor to compute the output shape )
( kernel size : the size of the window ) ( stride : the stride of the window  default value be kernel size ) ( ceil mode : when true  will use ceil instead of floor to compute the output shape )
( output size : the target output size loutl  out lout​  ) ( return indices : if true  will return the indices along with the output  useful to pass to nn maxunpool1d  default  false )
( output size : the target output size of the image of the form hout×wouth  out  \times w  out hout​×wout​  can be a tuple  hout wout  h  out   w  out   hout​ wout​  or a single houth  out hout​ for a square image hout×houth  out  \times h  out hout​×hout​  houth  out hout​ and woutw  out wout​ can be either a int  or none which mean the size will be the same as that of the input  ) ( return indices : if true  will return the indices along with the output  useful to pass to nn maxunpool2d  default  false )
( output size : the target output size of the image of the form dout×hout×woutd  out  \times h  out  \times w  out dout​×hout​×wout​  can be a tuple  dout hout wout  d  out   h  out   w  out   dout​ hout​ wout​  or a single doutd  out dout​ for a cube dout×dout×doutd  out  \times d  out  \times d  out dout​×dout​×dout​  doutd  out dout​  houth  out hout​ and woutw  out wout​ can be either a int  or none which mean the size will be the same as that of the input  ) ( return indices : if true  will return the indices along with the output  useful to pass to nn maxunpool3d  default  false )
( output size : the target output size loutl  out lout​  )
( output size : the target output size of the image of the form h x w  can be a tuple  h  w  or a single h for a square image h x h  h and w can be either a int  or none which mean the size will be the same as that of the input  )
( output size : the target output size of the form d x h x w  can be a tuple  d  h  w  or a single number d for a cube d x d x d  d  h and w can be either a int  or none which mean the size will be the same as that of the input  )
( padding  int  tuple  : the size of the pad  if be int  use the same pad in all boundaries  if a 2 tuple  use  pad left\text padding\ leave pad leave  pad right\text padding\ right pad right  )
( padding  int  tuple  : the size of the pad  if be int  use the same pad in all boundaries  if a 4 tuple  use  pad left\text padding\ leave pad leave  pad right\text padding\ right pad right  pad top\text padding\ top pad top  pad bottom\text padding\ bottom pad bottom  )
( padding  int  tuple  : the size of the pad  if be int  use the same pad in all boundaries  if a 2 tuple  use  pad left\text padding\ leave pad leave  pad right\text padding\ right pad right  )
( padding  int  tuple  : the size of the pad  if be int  use the same pad in all boundaries  if a 4 tuple  use  pad left\text padding\ leave pad leave  pad right\text padding\ right pad right  pad top\text padding\ top pad top  pad bottom\text padding\ bottom pad bottom  )
( padding  int  tuple  : the size of the pad  if be int  use the same pad in all boundaries  if a 6 tuple  use  pad left\text padding\ leave pad leave  pad right\text padding\ right pad right  pad top\text padding\ top pad top  pad bottom\text padding\ bottom pad bottom  pad front\text padding\ front pad front  pad back\text padding\ back pad back  )
( padding  int  tuple  : the size of the pad  if be int  use the same pad in all boundaries  if a 4 tuple  use  pad left\text padding\ leave pad leave  pad right\text padding\ right pad right  pad top\text padding\ top pad top  pad bottom\text padding\ bottom pad bottom  )
( padding  int  tuple  : the size of the pad  if be int  use the same pad in both boundaries  if a 2 tuple  use  pad left\text padding\ leave pad leave  pad right\text padding\ right pad right  )
( padding  int  tuple  : the size of the pad  if be int  use the same pad in all boundaries  if a 4 tuple  use  pad left\text padding\ leave pad leave  pad right\text padding\ right pad right  pad top\text padding\ top pad top  pad bottom\text padding\ bottom pad bottom  )
( padding  int  tuple  : the size of the pad  if be int  use the same pad in all boundaries  if a 6 tuple  use  pad left\text padding\ leave pad leave  pad right\text padding\ right pad right  pad top\text padding\ top pad top  pad bottom\text padding\ bottom pad bottom  pad front\text padding\ front pad front  pad back\text padding\ back pad back  )
( alpha : the α\alphaα value for the elu formulation  default  1 0 ) ( inplace : can optionally do the operation in place  default  false )
( lambd : the λ\lambdaλ value for the hardshrink formulation  default  0 5 )
( inplace : can optionally do the operation in place  default  false )
( min val : minimum value of the linear region range  default   1 ) ( max val : maximum value of the linear region range  default  1 ) ( inplace : can optionally do the operation in place  default  false )
( inplace : can optionally do the operation in place  default  false )
( negative slope : control the angle of the negative slope  default  1e 2 ) ( inplace : can optionally do the operation in place  default  false )

( embed dim : total dimension of the model  ) ( num heads : number of parallel attention head  note that embed dim will be split across num head  i e  each head will have dimension embed dim // num head   ) ( dropout : dropout probability on attn output weight  default  0 0  no dropout   ) ( bias : if specify  add bias to input / output projection layer  default  true  ) ( add bias kv : if specify  add bias to the key and value sequence at dim 0  default  false  ) ( add zero attn : if specify  add a new batch of zero to the key and value sequence at dim 1  default  false  ) ( kdim : total number of feature for key  default  none  use kdim embed dim   ) ( vdim : total number of feature for value  default  none  use vdim embed dim   ) ( batch first : if true  then the input and output tensors be provide as  batch  seq  feature   default  false  seq  batch  feature   )
( num parameters  int  : number of aaa to learn  although it take an int as input  there be only two value be legitimate  1  or the number of channel at input  default  1 ) ( init  float  : the initial value of aaa  default  0 25 )
( inplace : can optionally do the operation in place  default  false )
( inplace : can optionally do the operation in place  default  false )
( lower : lower bind of the uniform distribution  default  18\frac 1  8 81​ ) ( upper : upper bind of the uniform distribution  default  13\frac 1  3 31​ ) ( inplace : can optionally do the operation in place  default  false )
( inplace  bool  optional  : can optionally do the operation in place  default  false )
( alpha : the α\alphaα value for the celu formulation  default  1 0 ) ( inplace : can optionally do the operation in place  default  false )




( beta : the β\betaβ value for the softplus formulation  default  1 ) ( threshold : value above this revert to a linear function  default  20 )
( lambd : the λ\lambdaλ  must be no less than zero  value for the softshrink formulation  default  0 5 )



( threshold : the value to threshold at ) ( value : the value to replace with ) ( inplace : can optionally do the operation in place  default  false )
( dim  int  : the dimension on which to split the input  default   1 )
( dim  int  : a dimension along which softmin will be compute  so every slice along dim will sum to 1   )
( dim  int  : a dimension along which softmax will be compute  so every slice along dim will sum to 1   )

( dim  int  : a dimension along which logsoftmax will be compute  )
( in features  int  : number of feature in the input tensor ) ( n classes  int  : number of class in the dataset ) ( cutoffs  Sequence  : cutoffs use to assign target to their bucket ) ( div value  float  optional  : value use as an exponent to compute size of the cluster  default  4 0 ) ( head bias  bool  optional  : if true  add a bias term to the ‘head’ of the adaptive softmax  default  false )
( num features : number of feature or channel ccc of the input ) ( eps : a value add to the denominator for numerical stability  default  1e 5 ) ( momentum : the value use for the run mean and run var computation  can be set to none for cumulative move average  i e  simple average   default  0 1 ) ( affine : a boolean value that when set to true  this module have learnable affine parameters  default  true ) ( track running stats : a boolean value that when set to true  this module track the run mean and variance  and when set to false  this module do not track such statistics  and initialize statistics buffer run mean and run var as none  when these buffer be none  this module always use batch statistics  in both train and eval modes  default  true )
( num features : ccc from an expect input of size  n c h w  n  c  h  w  n c h w  ) ( eps : a value add to the denominator for numerical stability  default  1e 5 ) ( momentum : the value use for the run mean and run var computation  can be set to none for cumulative move average  i e  simple average   default  0 1 ) ( affine : a boolean value that when set to true  this module have learnable affine parameters  default  true ) ( track running stats : a boolean value that when set to true  this module track the run mean and variance  and when set to false  this module do not track such statistics  and initialize statistics buffer run mean and run var as none  when these buffer be none  this module always use batch statistics  in both train and eval modes  default  true )
( num features : ccc from an expect input of size  n c d h w  n  c  d  h  w  n c d h w  ) ( eps : a value add to the denominator for numerical stability  default  1e 5 ) ( momentum : the value use for the run mean and run var computation  can be set to none for cumulative move average  i e  simple average   default  0 1 ) ( affine : a boolean value that when set to true  this module have learnable affine parameters  default  true ) ( track running stats : a boolean value that when set to true  this module track the run mean and variance  and when set to false  this module do not track such statistics  and initialize statistics buffer run mean and run var as none  when these buffer be none  this module always use batch statistics  in both train and eval modes  default  true )
( eps : a value add to the denominator for numerical stability  default  1e 5 ) ( momentum : the value use for the run mean and run var computation  can be set to none for cumulative move average  i e  simple average   default  0 1 ) ( affine : a boolean value that when set to true  this module have learnable affine parameters  default  true ) ( track running stats : a boolean value that when set to true  this module track the run mean and variance  and when set to false  this module do not track such statistics  and initialize statistics buffer run mean and run var as none  when these buffer be none  this module always use batch statistics  in both train and eval modes  default  true )
( eps : a value add to the denominator for numerical stability  default  1e 5 ) ( momentum : the value use for the run mean and run var computation  can be set to none for cumulative move average  i e  simple average   default  0 1 ) ( affine : a boolean value that when set to true  this module have learnable affine parameters  default  true ) ( track running stats : a boolean value that when set to true  this module track the run mean and variance  and when set to false  this module do not track such statistics  and initialize statistics buffer run mean and run var as none  when these buffer be none  this module always use batch statistics  in both train and eval modes  default  true )
( eps : a value add to the denominator for numerical stability  default  1e 5 ) ( momentum : the value use for the run mean and run var computation  can be set to none for cumulative move average  i e  simple average   default  0 1 ) ( affine : a boolean value that when set to true  this module have learnable affine parameters  default  true ) ( track running stats : a boolean value that when set to true  this module track the run mean and variance  and when set to false  this module do not track such statistics  and initialize statistics buffer run mean and run var as none  when these buffer be none  this module always use batch statistics  in both train and eval modes  default  true )
( num groups  int  : number of group to separate the channel into ) ( num channels  int  : number of channel expect in input ) ( eps : a value add to the denominator for numerical stability  default  1e 5 ) ( affine : a boolean value that when set to true  this module have learnable per channel affine parameters initialize to ones  for weight  and zero  for bias   default  true  )
( num features : ccc from an expect input of size  n c    n  c     n c    ) ( eps : a value add to the denominator for numerical stability  default  1e 5 ) ( momentum : the value use for the run mean and run var computation  can be set to none for cumulative move average  i e  simple average   default  0 1 ) ( affine : a boolean value that when set to true  this module have learnable affine parameters  default  true ) ( track running stats : a boolean value that when set to true  this module track the run mean and variance  and when set to false  this module do not track such statistics  and initialize statistics buffer run mean and run var as none  when these buffer be none  this module always use batch statistics  in both train and eval modes  default  true ) ( process group : synchronization of stats happen within each process group individually  default behavior be synchronization across the whole world )
( num features : number of feature or channel ccc of the input ) ( eps : a value add to the denominator for numerical stability  default  1e 5 ) ( momentum : the value use for the run mean and run var computation  default  0 1 ) ( affine : a boolean value that when set to true  this module have learnable affine parameters  initialize the same way as do for batch normalization  default  false  ) ( track running stats : a boolean value that when set to true  this module track the run mean and variance  and when set to false  this module do not track such statistics and always use batch statistics in both train and eval modes  default  false )
( num features : ccc from an expect input of size  n c h w  n  c  h  w  n c h w  or  c h w  c  h  w  c h w  ) ( eps : a value add to the denominator for numerical stability  default  1e 5 ) ( momentum : the value use for the run mean and run var computation  default  0 1 ) ( affine : a boolean value that when set to true  this module have learnable affine parameters  initialize the same way as do for batch normalization  default  false  ) ( track running stats : a boolean value that when set to true  this module track the run mean and variance  and when set to false  this module do not track such statistics and always use batch statistics in both train and eval modes  default  false )
( num features : ccc from an expect input of size  n c d h w  n  c  d  h  w  n c d h w  or  c d h w  c  d  h  w  c d h w  ) ( eps : a value add to the denominator for numerical stability  default  1e 5 ) ( momentum : the value use for the run mean and run var computation  default  0 1 ) ( affine : a boolean value that when set to true  this module have learnable affine parameters  initialize the same way as do for batch normalization  default  false  ) ( track running stats : a boolean value that when set to true  this module track the run mean and variance  and when set to false  this module do not track such statistics and always use batch statistics in both train and eval modes  default  false )
( normalized shape  int or list or torch Size  : input shape from an expect input of size   ∗×normalized shape 0 ×normalized shape 1 ×…×normalized shape −1     \times \text normalized\ shape  0  \times \text normalized\ shape  1      \times \ldots \times \text normalized\ shape   1     ∗×normalized shape 0 ×normalized shape 1 ×…×normalized shape −1  if a single integer be use  it be treat as a singleton list  and this module will normalize over the last dimension which be expect to be of that specific size  ) ( eps : a value add to the denominator for numerical stability  default  1e 5 ) ( elementwise affine : a boolean value that when set to true  this module have learnable per element affine parameters initialize to ones  for weight  and zero  for bias   default  true  )
( size : amount of neighbour channel use for normalization ) ( alpha : multiplicative factor  default  0 0001 ) ( beta : exponent  default  0 75 ) ( k : additive factor  default  1 )
( input size : the number of expect feature in the input x ) ( hidden size : the number of feature in the hide state h ) ( num layers : number of recurrent layer  e g   set num layer 2 would mean stack two rnns together to form a stack rnn  with the second rnn take in output of the first rnn and compute the final result  default  1 ) ( nonlinearity : the non linearity to use  can be either 'tanh' or 'relu'  default  'tanh' ) ( bias : if false  then the layer do not use bias weight b ih and b hh  default  true ) ( batch first : if true  then the input and output tensors be provide as  batch  seq  feature  instead of  seq  batch  feature   note that this do not apply to hide or cell state  see the inputs/outputs section below for detail   default  false ) ( dropout : if non zero  introduce a dropout layer on the output of each rnn layer except the last layer  with dropout probability equal to dropout  default  0 ) ( bidirectional : if true  become a bidirectional rnn  default  false )
( input size : the number of expect feature in the input x ) ( hidden size : the number of feature in the hide state h ) ( num layers : number of recurrent layer  e g   set num layer 2 would mean stack two lstms together to form a stack lstm  with the second lstm take in output of the first lstm and compute the final result  default  1 ) ( bias : if false  then the layer do not use bias weight b ih and b hh  default  true ) ( batch first : if true  then the input and output tensors be provide as  batch  seq  feature  instead of  seq  batch  feature   note that this do not apply to hide or cell state  see the inputs/outputs section below for detail   default  false ) ( dropout : if non zero  introduce a dropout layer on the output of each lstm layer except the last layer  with dropout probability equal to dropout  default  0 ) ( bidirectional : if true  become a bidirectional lstm  default  false ) ( proj size : if > 0  will use lstm with projections of correspond size  default  0 )
( input size : the number of expect feature in the input x ) ( hidden size : the number of feature in the hide state h ) ( num layers : number of recurrent layer  e g   set num layer 2 would mean stack two grus together to form a stack gru  with the second gru take in output of the first gru and compute the final result  default  1 ) ( bias : if false  then the layer do not use bias weight b ih and b hh  default  true ) ( batch first : if true  then the input and output tensors be provide as  batch  seq  feature  instead of  seq  batch  feature   note that this do not apply to hide or cell state  see the inputs/outputs section below for detail   default  false ) ( dropout : if non zero  introduce a dropout layer on the output of each gru layer except the last layer  with dropout probability equal to dropout  default  0 ) ( bidirectional : if true  become a bidirectional gru  default  false )
( input size : the number of expect feature in the input x ) ( hidden size : the number of feature in the hide state h ) ( bias : if false  then the layer do not use bias weight b ih and b hh  default  true ) ( nonlinearity : the non linearity to use  can be either 'tanh' or 'relu'  default  'tanh' )
( input size : the number of expect feature in the input x ) ( hidden size : the number of feature in the hide state h ) ( bias : if false  then the layer do not use bias weight b ih and b hh  default  true )
( input size : the number of expect feature in the input x ) ( hidden size : the number of feature in the hide state h ) ( bias : if false  then the layer do not use bias weight b ih and b hh  default  true )
( d model : the number of expect feature in the encoder/decoder input  default 512   ) ( nhead : the number of head in the multiheadattention model  default 8   ) ( num encoder layers : the number of sub encoder layer in the encoder  default 6   ) ( num decoder layers : the number of sub decoder layer in the decoder  default 6   ) ( dim feedforward : the dimension of the feedforward network model  default 2048   ) ( dropout : the dropout value  default 0 1   ) ( activation : the activation function of encoder/decoder intermediate layer  can be a string  “relu” or “gelu”  or a unary callable  default  relu ) ( custom encoder : custom encoder  default none   ) ( custom decoder : custom decoder  default none   ) ( layer norm eps : the eps value in layer normalization components  default 1e 5   ) ( batch first : if true  then the input and output tensors be provide as  batch  seq  feature   default  false  seq  batch  feature   ) ( norm first : if true  encoder and decoder layer will perform layernorms before other attention and feedforward operations  otherwise after  default  false  after   )

( encoder layer : an instance of the transformerencoderlayer   class  require   ) ( num layers : the number of sub encoder layer in the encoder  require   ) ( norm : the layer normalization component  optional   )

( decoder layer : an instance of the transformerdecoderlayer   class  require   ) ( num layers : the number of sub decoder layer in the decoder  require   ) ( norm : the layer normalization component  optional   )

( d model : the number of expect feature in the input  require   ) ( nhead : the number of head in the multiheadattention model  require   ) ( dim feedforward : the dimension of the feedforward network model  default 2048   ) ( dropout : the dropout value  default 0 1   ) ( activation : the activation function of the intermediate layer  can be a string  “relu” or “gelu”  or a unary callable  default  relu ) ( layer norm eps : the eps value in layer normalization components  default 1e 5   ) ( batch first : if true  then the input and output tensors be provide as  batch  seq  feature   default  false  seq  batch  feature   ) ( norm first : if true  layer norm be do prior to attention and feedforward operations  respectivaly  otherwise it’s do after  default  false  after   )

( d model : the number of expect feature in the input  require   ) ( nhead : the number of head in the multiheadattention model  require   ) ( dim feedforward : the dimension of the feedforward network model  default 2048   ) ( dropout : the dropout value  default 0 1   ) ( activation : the activation function of the intermediate layer  can be a string  “relu” or “gelu”  or a unary callable  default  relu ) ( layer norm eps : the eps value in layer normalization components  default 1e 5   ) ( batch first : if true  then the input and output tensors be provide as  batch  seq  feature   default  false  seq  batch  feature   ) ( norm first : if true  layer norm be do prior to self attention  multihead attention and feedforward operations  respectivaly  otherwise it’s do after  default  false  after   )

( args : any argument  unused  ) ( kwargs : any keyword argument  unused  )
( in features : size of each input sample ) ( out features : size of each output sample ) ( bias : if set to false  the layer will not learn an additive bias  default  true )
( in1 features : size of each first input sample ) ( in2 features : size of each second input sample ) ( out features : size of each output sample ) ( bias : if set to false  the layer will not learn an additive bias  default  true )
( out features : size of each output sample ) ( bias : if set to false  the layer will not learn an additive bias  default  true )
( p : probability of an element to be zero  default  0 5 ) ( inplace : if set to true  will do this operation in place  default  false )
( p  float  optional  : probability of an element to be zero ed  ) ( inplace  bool  optional  : if set to true  will do this operation in place )
( p  float  optional  : probability of an element to be zero  ) ( inplace  bool  optional  : if set to true  will do this operation in place )
( p  float  : probability of an element to be drop  default  0 5 ) ( inplace  bool  optional  : if set to true  will do this operation in place )
( p  float  optional  : probability of an element to be zero  default  0 5 ) ( inplace  bool  optional  : if set to true  will do this operation in place )
( num embeddings  int  : size of the dictionary of embeddings ) ( embedding dim  int  : the size of each embed vector ) ( padding idx  int  optional  : if specify  the entries at pad idx do not contribute to the gradient  therefore  the embed vector at pad idx be not update during train  i e  it remain as a fix “pad”  for a newly construct embed  the embed vector at pad idx will default to all zero  but can be update to another value to be use as the pad vector  ) ( max norm  float  optional  : if give  each embed vector with norm larger than max norm be renormalize to have norm max norm  ) ( norm type  float  optional  : the p of the p norm to compute for the max norm option  default 2  ) ( scale grad by freq  boolean  optional  : if give  this will scale gradients by the inverse of frequency of the word in the mini batch  default false  ) ( sparse  bool  optional  : if true  gradient w r t  weight matrix will be a sparse tensor  see note for more detail regard sparse gradients  )
( num embeddings  int  : size of the dictionary of embeddings ) ( embedding dim  int  : the size of each embed vector ) ( max norm  float  optional  : if give  each embed vector with norm larger than max norm be renormalize to have norm max norm  ) ( norm type  float  optional  : the p of the p norm to compute for the max norm option  default 2  ) ( scale grad by freq  boolean  optional  : if give  this will scale gradients by the inverse of frequency of the word in the mini batch  default false  note  this option be not support when mode "max"  ) ( mode  string  optional  : "sum"  "mean" or "max"  specify the way to reduce the bag  "sum" compute the weight sum  take per sample weight into consideration  "mean" compute the average of the value in the bag  "max" compute the max value over each bag  default  "mean" ) ( sparse  bool  optional  : if true  gradient w r t  weight matrix will be a sparse tensor  see note for more detail regard sparse gradients  note  this option be not support when mode "max"  ) ( include last offset  bool  optional  : if true  offset have one additional element  where the last element be equivalent to the size of indices  this match the csr format  ) ( padding idx  int  optional  : if specify  the entries at pad idx do not contribute to the gradient  therefore  the embed vector at pad idx be not update during train  i e  it remain as a fix “pad”  for a newly construct embeddingbag  the embed vector at pad idx will default to all zero  but can be update to another value to be use as the pad vector  note that the embed vector at pad idx be exclude from the reduction  )
( input  Tensor  : tensor contain bag of indices into the embed matrix  ) ( offsets  Tensor  optional  : only use when input be 1d  offset determine the start index position of each bag  sequence  in input  ) ( per sample weights  Tensor  optional  : a tensor of float / double weight  or none to indicate all weight should be take to be 1  if specify  per sample weight must have exactly the same shape as input and be treat as have the same offset  if those be not none  only support for mode 'sum'  )
( dim  int  optional  : dimension where cosine similarity be compute  default  1 ) ( eps  float  optional  : small value to avoid division by zero  default  1e 8 )
( p  real  : the norm degree  default  2 ) ( eps  float  optional  : small value to avoid division by zero  default  1e 6 ) ( keepdim  bool  optional  : determine whether or not to keep the vector dimension  default  false )
( size average  bool  optional  : deprecate  see reduction   by default  the losses be average over each loss element in the batch  note that for some losses  there be multiple elements per sample  if the field size average be set to false  the losses be instead sum for each minibatch  ignore when reduce be false  default  true ) ( reduce  bool  optional  : deprecate  see reduction   by default  the losses be average or sum over observations for each minibatch depend on size average  when reduce be false  return a loss per batch element instead and ignore size average  default  true ) ( reduction  string  optional  : specify the reduction to apply to the output  'none'   'mean'   'sum'  'none'  no reduction will be apply  'mean'  the sum of the output will be divide by the number of elements in the output  'sum'  the output will be sum  note  size average and reduce be in the process of be deprecate  and in the meantime  specify either of those two args will override reduction  default  'mean' )
( size average  bool  optional  : deprecate  see reduction   by default  the losses be average over each loss element in the batch  note that for some losses  there be multiple elements per sample  if the field size average be set to false  the losses be instead sum for each minibatch  ignore when reduce be false  default  true ) ( reduce  bool  optional  : deprecate  see reduction   by default  the losses be average or sum over observations for each minibatch depend on size average  when reduce be false  return a loss per batch element instead and ignore size average  default  true ) ( reduction  string  optional  : specify the reduction to apply to the output  'none'   'mean'   'sum'  'none'  no reduction will be apply  'mean'  the sum of the output will be divide by the number of elements in the output  'sum'  the output will be sum  note  size average and reduce be in the process of be deprecate  and in the meantime  specify either of those two args will override reduction  default  'mean' )
( weight  Tensor  optional  : a manual rescale weight give to each class  if give  have to be a tensor of size c ) ( size average  bool  optional  : deprecate  see reduction   by default  the losses be average over each loss element in the batch  note that for some losses  there be multiple elements per sample  if the field size average be set to false  the losses be instead sum for each minibatch  ignore when reduce be false  default  true ) ( ignore index  int  optional  : specify a target value that be ignore and do not contribute to the input gradient  when size average be true  the loss be average over non ignore target  note that ignore index be only applicable when the target contain class indices  ) ( reduce  bool  optional  : deprecate  see reduction   by default  the losses be average or sum over observations for each minibatch depend on size average  when reduce be false  return a loss per batch element instead and ignore size average  default  true ) ( reduction  string  optional  : specify the reduction to apply to the output  'none'   'mean'   'sum'  'none'  no reduction will be apply  'mean'  the weight mean of the output be take  'sum'  the output will be sum  note  size average and reduce be in the process of be deprecate  and in the meantime  specify either of those two args will override reduction  default  'mean' ) ( label smoothing  float  optional  : a float in  0 0  1 0   specify the amount of smooth when compute the loss  where 0 0 mean no smooth  the target become a mixture of the original grind truth and a uniform distribution as describe in rethink the inception architecture for computer vision  default  0 00 00 0  )
( blank  int  optional  : blank label  default 000  ) ( reduction  string  optional  : specify the reduction to apply to the output  'none'   'mean'   'sum'  'none'  no reduction will be apply  'mean'  the output losses will be divide by the target lengths and then the mean over the batch be take  default  'mean' ) ( zero infinity  bool  optional  : whether to zero infinite losses and the associate gradients  default  false infinite losses mainly occur when the input be too short to be align to the target  )
( weight  Tensor  optional  : a manual rescale weight give to each class  if give  it have to be a tensor of size c  otherwise  it be treat as if have all ones  ) ( size average  bool  optional  : deprecate  see reduction   by default  the losses be average over each loss element in the batch  note that for some losses  there be multiple elements per sample  if the field size average be set to false  the losses be instead sum for each minibatch  ignore when reduce be false  default  none ) ( ignore index  int  optional  : specify a target value that be ignore and do not contribute to the input gradient  when size average be true  the loss be average over non ignore target  ) ( reduce  bool  optional  : deprecate  see reduction   by default  the losses be average or sum over observations for each minibatch depend on size average  when reduce be false  return a loss per batch element instead and ignore size average  default  none ) ( reduction  string  optional  : specify the reduction to apply to the output  'none'   'mean'   'sum'  'none'  no reduction will be apply  'mean'  the weight mean of the output be take  'sum'  the output will be sum  note  size average and reduce be in the process of be deprecate  and in the meantime  specify either of those two args will override reduction  default  'mean' )
( log input  bool  optional  : if true the loss be compute as exp⁡ input −target∗input\exp \text input     \text target  \text input exp input −target∗input  if false the loss be input−target∗log⁡ input eps \text input    \text target  \log \text input  \text eps  input−target∗log input eps   ) ( full  bool  optional  : whether to compute full loss  i  e  to add the stirling approximation term  target∗log⁡ target −target 0 5∗log⁡ 2πtarget  \text target  \log \text target     \text target    0 5   \log 2\pi\text target     target∗log target −target 0 5∗log 2πtarget   ) ( size average  bool  optional  : deprecate  see reduction   by default  the losses be average over each loss element in the batch  note that for some losses  there be multiple elements per sample  if the field size average be set to false  the losses be instead sum for each minibatch  ignore when reduce be false  default  true ) ( eps  float  optional  : small value to avoid evaluation of log⁡ 0 \log 0 log 0  when log input   false  default  1e 8 ) ( reduce  bool  optional  : deprecate  see reduction   by default  the losses be average or sum over observations for each minibatch depend on size average  when reduce be false  return a loss per batch element instead and ignore size average  default  true ) ( reduction  string  optional  : specify the reduction to apply to the output  'none'   'mean'   'sum'  'none'  no reduction will be apply  'mean'  the sum of the output will be divide by the number of elements in the output  'sum'  the output will be sum  note  size average and reduce be in the process of be deprecate  and in the meantime  specify either of those two args will override reduction  default  'mean' )
( full  bool  optional  : include the constant term in the loss calculation  default  false  ) ( eps  float  optional  : value use to clamp var  see note below   for stability  default  1e 6  ) ( reduction  string  optional  : specify the reduction to apply to the output 'none'   'mean'   'sum'  'none'  no reduction will be apply  'mean'  the output be the average of all batch member losses  'sum'  the output be the sum of all batch member losses  default  'mean'  )
( size average  bool  optional  : deprecate  see reduction   by default  the losses be average over each loss element in the batch  note that for some losses  there be multiple elements per sample  if the field size average be set to false  the losses be instead sum for each minibatch  ignore when reduce be false  default  true ) ( reduce  bool  optional  : deprecate  see reduction   by default  the losses be average or sum over observations for each minibatch depend on size average  when reduce be false  return a loss per batch element instead and ignore size average  default  true ) ( reduction  string  optional  : specify the reduction to apply to the output  default  “mean” ) ( log target  bool  optional  : specify whether target be the log space  default  false )
( weight  Tensor  optional  : a manual rescale weight give to the loss of each batch element  if give  have to be a tensor of size nbatch  ) ( size average  bool  optional  : deprecate  see reduction   by default  the losses be average over each loss element in the batch  note that for some losses  there be multiple elements per sample  if the field size average be set to false  the losses be instead sum for each minibatch  ignore when reduce be false  default  true ) ( reduce  bool  optional  : deprecate  see reduction   by default  the losses be average or sum over observations for each minibatch depend on size average  when reduce be false  return a loss per batch element instead and ignore size average  default  true ) ( reduction  string  optional  : specify the reduction to apply to the output  'none'   'mean'   'sum'  'none'  no reduction will be apply  'mean'  the sum of the output will be divide by the number of elements in the output  'sum'  the output will be sum  note  size average and reduce be in the process of be deprecate  and in the meantime  specify either of those two args will override reduction  default  'mean' )
( weight  Tensor  optional  : a manual rescale weight give to the loss of each batch element  if give  have to be a tensor of size nbatch  ) ( size average  bool  optional  : deprecate  see reduction   by default  the losses be average over each loss element in the batch  note that for some losses  there be multiple elements per sample  if the field size average be set to false  the losses be instead sum for each minibatch  ignore when reduce be false  default  true ) ( reduce  bool  optional  : deprecate  see reduction   by default  the losses be average or sum over observations for each minibatch depend on size average  when reduce be false  return a loss per batch element instead and ignore size average  default  true ) ( reduction  string  optional  : specify the reduction to apply to the output  'none'   'mean'   'sum'  'none'  no reduction will be apply  'mean'  the sum of the output will be divide by the number of elements in the output  'sum'  the output will be sum  note  size average and reduce be in the process of be deprecate  and in the meantime  specify either of those two args will override reduction  default  'mean' ) ( pos weight  Tensor  optional  : a weight of positive examples  must be a vector with length equal to the number of class  )
( margin  float  optional  : have a default value of 000  ) ( size average  bool  optional  : deprecate  see reduction   by default  the losses be average over each loss element in the batch  note that for some losses  there be multiple elements per sample  if the field size average be set to false  the losses be instead sum for each minibatch  ignore when reduce be false  default  true ) ( reduce  bool  optional  : deprecate  see reduction   by default  the losses be average or sum over observations for each minibatch depend on size average  when reduce be false  return a loss per batch element instead and ignore size average  default  true ) ( reduction  string  optional  : specify the reduction to apply to the output  'none'   'mean'   'sum'  'none'  no reduction will be apply  'mean'  the sum of the output will be divide by the number of elements in the output  'sum'  the output will be sum  note  size average and reduce be in the process of be deprecate  and in the meantime  specify either of those two args will override reduction  default  'mean' )
( margin  float  optional  : have a default value of 1  ) ( size average  bool  optional  : deprecate  see reduction   by default  the losses be average over each loss element in the batch  note that for some losses  there be multiple elements per sample  if the field size average be set to false  the losses be instead sum for each minibatch  ignore when reduce be false  default  true ) ( reduce  bool  optional  : deprecate  see reduction   by default  the losses be average or sum over observations for each minibatch depend on size average  when reduce be false  return a loss per batch element instead and ignore size average  default  true ) ( reduction  string  optional  : specify the reduction to apply to the output  'none'   'mean'   'sum'  'none'  no reduction will be apply  'mean'  the sum of the output will be divide by the number of elements in the output  'sum'  the output will be sum  note  size average and reduce be in the process of be deprecate  and in the meantime  specify either of those two args will override reduction  default  'mean' )
( size average  bool  optional  : deprecate  see reduction   by default  the losses be average over each loss element in the batch  note that for some losses  there be multiple elements per sample  if the field size average be set to false  the losses be instead sum for each minibatch  ignore when reduce be false  default  true ) ( reduce  bool  optional  : deprecate  see reduction   by default  the losses be average or sum over observations for each minibatch depend on size average  when reduce be false  return a loss per batch element instead and ignore size average  default  true ) ( reduction  string  optional  : specify the reduction to apply to the output  'none'   'mean'   'sum'  'none'  no reduction will be apply  'mean'  the sum of the output will be divide by the number of elements in the output  'sum'  the output will be sum  note  size average and reduce be in the process of be deprecate  and in the meantime  specify either of those two args will override reduction  default  'mean' )
( reduction  string  optional  : specify the reduction to apply to the output  'none'   'mean'   'sum'  'none'  no reduction will be apply  'mean'  the sum of the output will be divide by the number of elements in the output  'sum'  the output will be sum  default  'mean' ) ( delta  float  optional  : specify the threshold at which to change between delta scale l1 and l2 loss  the value must be positive   default  1 0 )
( size average  bool  optional  : deprecate  see reduction   by default  the losses be average over each loss element in the batch  note that for some losses  there be multiple elements per sample  if the field size average be set to false  the losses be instead sum for each minibatch  ignore when reduce be false  default  true ) ( reduce  bool  optional  : deprecate  see reduction   by default  the losses be average or sum over observations for each minibatch depend on size average  when reduce be false  return a loss per batch element instead and ignore size average  default  true ) ( reduction  string  optional  : specify the reduction to apply to the output  'none'   'mean'   'sum'  'none'  no reduction will be apply  'mean'  the sum of the output will be divide by the number of elements in the output  'sum'  the output will be sum  note  size average and reduce be in the process of be deprecate  and in the meantime  specify either of those two args will override reduction  default  'mean' ) ( beta  float  optional  : specify the threshold at which to change between l1 and l2 loss  the value must be non negative  default  1 0 )
( size average  bool  optional  : deprecate  see reduction   by default  the losses be average over each loss element in the batch  note that for some losses  there be multiple elements per sample  if the field size average be set to false  the losses be instead sum for each minibatch  ignore when reduce be false  default  true ) ( reduce  bool  optional  : deprecate  see reduction   by default  the losses be average or sum over observations for each minibatch depend on size average  when reduce be false  return a loss per batch element instead and ignore size average  default  true ) ( reduction  string  optional  : specify the reduction to apply to the output  'none'   'mean'   'sum'  'none'  no reduction will be apply  'mean'  the sum of the output will be divide by the number of elements in the output  'sum'  the output will be sum  note  size average and reduce be in the process of be deprecate  and in the meantime  specify either of those two args will override reduction  default  'mean' )
( weight  Tensor  optional  : a manual rescale weight give to each class  if give  it have to be a tensor of size c  otherwise  it be treat as if have all ones  ) ( size average  bool  optional  : deprecate  see reduction   by default  the losses be average over each loss element in the batch  note that for some losses  there be multiple elements per sample  if the field size average be set to false  the losses be instead sum for each minibatch  ignore when reduce be false  default  true ) ( reduce  bool  optional  : deprecate  see reduction   by default  the losses be average or sum over observations for each minibatch depend on size average  when reduce be false  return a loss per batch element instead and ignore size average  default  true ) ( reduction  string  optional  : specify the reduction to apply to the output  'none'   'mean'   'sum'  'none'  no reduction will be apply  'mean'  the sum of the output will be divide by the number of elements in the output  'sum'  the output will be sum  note  size average and reduce be in the process of be deprecate  and in the meantime  specify either of those two args will override reduction  default  'mean' )
( margin  float  optional  : should be a number from −1 1−1 to 111  000 to 0 50 50 5 be suggest  if margin be miss  the default value be 000  ) ( size average  bool  optional  : deprecate  see reduction   by default  the losses be average over each loss element in the batch  note that for some losses  there be multiple elements per sample  if the field size average be set to false  the losses be instead sum for each minibatch  ignore when reduce be false  default  true ) ( reduce  bool  optional  : deprecate  see reduction   by default  the losses be average or sum over observations for each minibatch depend on size average  when reduce be false  return a loss per batch element instead and ignore size average  default  true ) ( reduction  string  optional  : specify the reduction to apply to the output  'none'   'mean'   'sum'  'none'  no reduction will be apply  'mean'  the sum of the output will be divide by the number of elements in the output  'sum'  the output will be sum  note  size average and reduce be in the process of be deprecate  and in the meantime  specify either of those two args will override reduction  default  'mean' )
( p  int  optional  : have a default value of 111  111 and 222 be the only support value  ) ( margin  float  optional  : have a default value of 111  ) ( weight  Tensor  optional  : a manual rescale weight give to each class  if give  it have to be a tensor of size c  otherwise  it be treat as if have all ones  ) ( size average  bool  optional  : deprecate  see reduction   by default  the losses be average over each loss element in the batch  note that for some losses  there be multiple elements per sample  if the field size average be set to false  the losses be instead sum for each minibatch  ignore when reduce be false  default  true ) ( reduce  bool  optional  : deprecate  see reduction   by default  the losses be average or sum over observations for each minibatch depend on size average  when reduce be false  return a loss per batch element instead and ignore size average  default  true ) ( reduction  string  optional  : specify the reduction to apply to the output  'none'   'mean'   'sum'  'none'  no reduction will be apply  'mean'  the sum of the output will be divide by the number of elements in the output  'sum'  the output will be sum  note  size average and reduce be in the process of be deprecate  and in the meantime  specify either of those two args will override reduction  default  'mean' )
( margin  float  optional  : default  111  ) ( p  int  optional  : the norm degree for pairwise distance  default  222  ) ( swap  bool  optional  : the distance swap be describe in detail in the paper learn shallow convolutional feature descriptors with triplet losses by v  balntas  e  riba et al  default  false  ) ( size average  bool  optional  : deprecate  see reduction   by default  the losses be average over each loss element in the batch  note that for some losses  there be multiple elements per sample  if the field size average be set to false  the losses be instead sum for each minibatch  ignore when reduce be false  default  true ) ( reduce  bool  optional  : deprecate  see reduction   by default  the losses be average or sum over observations for each minibatch depend on size average  when reduce be false  return a loss per batch element instead and ignore size average  default  true ) ( reduction  string  optional  : specify the reduction to apply to the output  'none'   'mean'   'sum'  'none'  no reduction will be apply  'mean'  the sum of the output will be divide by the number of elements in the output  'sum'  the output will be sum  note  size average and reduce be in the process of be deprecate  and in the meantime  specify either of those two args will override reduction  default  'mean' )
( distance function  callable  optional  : a nonnegative  real value function that quantify the closeness of two tensors  if not specify  nn pairwisedistance will be use   default  none ) ( margin  float  optional  : a nonnegative margin represent the minimum difference between the positive and negative distance require for the loss to be 0  larger margins penalize case where the negative examples be not distant enough from the anchor  relative to the positives  default  111  ) ( swap  bool  optional  : whether to use the distance swap describe in the paper learn shallow convolutional feature descriptors with triplet losses by v  balntas  e  riba et al  if true  and if the positive example be closer to the negative example than the anchor be  swap the positive example and the anchor in the loss computation  default  false  ) ( reduction  string  optional  : specify the  optional  reduction to apply to the output  'none'   'mean'   'sum'  'none'  no reduction will be apply  'mean'  the sum of the output will be divide by the number of elements in the output  'sum'  the output will be sum  default  'mean' )
( upscale factor  int  : factor to increase spatial resolution by )
( downscale factor  int  : factor to decrease spatial resolution by )
( size  int or Tuple int  or Tuple int  int  or Tuple int  int  int   optional  : output spatial size ) ( scale factor  float or Tuple float  or Tuple float  float  or Tuple float  float  float   optional  : multiplier for spatial size  have to match input size if it be a tuple  ) ( mode  str  optional  : the upsampling algorithm  one of 'nearest'  'linear'  'bilinear'  'bicubic' and 'trilinear'  default  'nearest' ) ( align corners  bool  optional  : if true  the corner pixels of the input and output tensors be align  and thus preserve the value at those pixels  this only have effect when mode be 'linear'  'bilinear'  'bicubic'  or 'trilinear'  default  false ) ( recompute scale factor  bool  optional  : recompute the scale factor for use in the interpolation calculation  if recompute scale factor be true  then scale factor must be pass in and scale factor be use to compute the output size  the compute output size will be use to infer new scale for the interpolation  note that when scale factor be float point  it may differ from the recomputed scale factor due to round and precision issue  if recompute scale factor be false  then size or scale factor will be use directly for interpolation  )
( size  int or Tuple int  int   optional  : output spatial size ) ( scale factor  float or Tuple float  float   optional  : multiplier for spatial size  )
( size  int or Tuple int  int   optional  : output spatial size ) ( scale factor  float or Tuple float  float   optional  : multiplier for spatial size  )
( groups  int  : number of group to divide channel in  )
( module  Module  : module to be parallelize ) ( device ids  list of python int or torch device  : cuda devices  default  all devices  ) ( output device  int or torch device  : device location of output  default  device ids 0   )
( module  Module  : module to be parallelize ) ( device ids  list of python int or torch device  : cuda devices  1  for single device modules  device ids can contain exactly one device id  which represent the only cuda device where the input module correspond to this process reside  alternatively  device ids can also be none  2  for multi device modules and cpu modules  device ids must be none  when device ids be none for both case  both the input data for the forward pass and the actual module must be place on the correct device   default  none  ) ( output device  int or torch device  : device location of output for single device cuda modules  for multi device modules and cpu modules  it must be none  and the module itself dictate the output location   default  device ids 0  for single device modules  ) ( broadcast buffers  bool  : flag that enable sync  broadcast  buffer of the module at begin of the forward function   default  true  ) ( process group : the process group to be use for distribute data all reduction  if none  the default process group  which be create by torch distribute init process group    will be use   default  none  ) ( bucket cap mb : distributeddataparallel will bucket parameters into multiple bucket so that gradient reduction of each bucket can potentially overlap with backward computation  bucket cap mb control the bucket size in megabytes  mb    default  25  ) ( find unused parameters  bool  : traverse the autograd graph from all tensors contain in the return value of the wrap module’s forward function  parameters that don’t receive gradients as part of this graph be preemptively mark as be ready to be reduce  in addition  parameters that may have be use in the wrap module’s forward function but be not part of loss computation and thus would also not receive gradients be preemptively mark as ready to be reduce   default  false  ) ( check reduction : this argument be deprecate  ) ( gradient as bucket view  bool  : when set to true  gradients will be view point to different offset of allreduce communication bucket  this can reduce peak memory usage  where the save memory size will be equal to the total gradients size  moreover  it avoid the overhead of copy between gradients and allreduce communication bucket  when gradients be view  detach    cannot be call on the gradients  if hit such errors  please fix it by refer to the zero grad   function in torch/optim/optimizer py as a solution  note that gradients will be view after first iteration  so the peak memory save should be check after first iteration  ) ( static graph  bool  : when set to true  ddp know the train graph be static  static graph mean 1  the set of use and unused parameters will not change during the whole train loop  in this case  it do not matter whether users set find unused parameters   true or not  2  how the graph be train will not change during the whole train loop  mean there be no control flow depend on iterations   when static graph be set to be true  ddp will support case that can not be support in the past  1  reentrant backwards  2  activation checkpointing multiple time  3  activation checkpointing when model have unused parameters  4  there be model parameters that be outside of forward function  5  potentially improve performance when there be unused parameters  as ddp will not search graph in each iteraton to detect unused parameters when static graph be set to be true  to check whether you can set static graph to be true  one way be to check ddp log data at the end of your previous model train  if ddp log data get "can set static graph"     true  mostly you can set static graph   true as well   example  >>> model ddp   torch nn parallel distributeddataparallel model  >>>   train loop >>>       >>> ddp log data   model ddp  get ddp log data   >>> static graph   ddp log data get "can set static graph"  )

( parameters  Iterable Tensor  or Tensor  : an iterable of tensors or a single tensor that will have gradients normalize ) ( max norm  float or int  : max norm of the gradients ) ( norm type  float or int  : type of the use p norm  can be 'inf' for infinity norm  ) ( error if nonfinite  bool  : if true  an error be throw if the total norm of the gradients from parameters be nan  inf  or  inf  default  false  will switch to true in the future  )
( parameters  Iterable Tensor  or Tensor  : an iterable of tensors or a single tensor that will have gradients normalize ) ( clip value  float or int  : maximum allow value of the gradients  the gradients be clip in the range   clip value clip value \left \text  clip\ value   \text clip\ value \right   clip value clip value  )
( parameters  Iterable Tensor   : an iterator of tensors that be the parameters of a model  )
( vec  Tensor  : a single vector represent the parameters of a model  ) ( parameters  Iterable Tensor   : an iterator of tensors that be the parameters of a model  )
( module  Module  : contain module ) ( name  str  optional  : name of weight parameter )
( module  Module  : contain module ) ( name  str  optional  : name of weight parameter )
( module  nn Module  : contain module ) ( name  str  optional  : name of weight parameter  default  "weight"  ) ( n power iterations  int  optional  : number of power iterations to calculate spectral norm  default  1  ) ( eps  float  optional  : epsilon for numerical stability in calculate norms  default  1e 12  ) ( dim  int  optional  : dimension correspond to number of output  default  0  except for modules that be instance of convtranspose 1 2 3 d  when it be 1 )
( module  nn Module  : module on which to register the parametrization ) ( tensor name  str  : name of the parameter or buffer on which to register the parametrization ) ( parametrization  nn Module  : the parametrization to register )
( module  nn Module  : module from which remove the parametrization ) ( tensor name  str  : name of the parametrization to be remove ) ( leave parametrized  bool  optional  : leave the attribute tensor name parametrized  default  true )

( module  nn Module  : module to query ) ( name  str  optional  : attribute in the module to query default  none )
( modules  sequence  : sequence of modules represent the parametrizations ) ( original  Parameter or Tensor  : parameter or buffer that be parametrized ) ( unsafe  bool  : a boolean flag that denote whether the parametrization may change the dtype and shape of the tensor  default  false warn  the parametrization be not check for consistency upon registration  enable this flag at your own risk  )

( input  Tensor  : pad batch of variable length sequence  ) ( lengths  Tensor or list int   : list of sequence lengths of each batch element  must be on the cpu if provide as a tensor   ) ( batch first  bool  optional  : if true  the input be expect in b x t x   format  ) ( enforce sorted  bool  optional  : if true  the input be expect to contain sequence sort by length in a decrease order  if false  the input will get sort unconditionally  default  true  )
( sequence  PackedSequence  : batch to pad ) ( batch first  bool  optional  : if true  the output will be in b x t x   format  ) ( padding value  float  optional  : value for pad elements  ) ( total length  int  optional  : if not none  the output will be pad to have length total length  this method will throw valueerror if total length be less than the max sequence length in sequence  )
( sequences  list Tensor   : list of variable length sequence  ) ( batch first  bool  optional  : output will be in b x t x   if true  or in t x b x   otherwise  default  false  ) ( padding value  float  optional  : value for pad elements  default  0  )
( sequences  list Tensor   : a list of sequence of decrease length  ) ( enforce sorted  bool  optional  : if true  check that the input contain sequence sort by length in a decrease order  if false  this condition be not check  default  true  )
( start dim : first dim to flatten  default   1   ) ( end dim : last dim to flatten  default    1   )
( dim  Union int  str   : dimension to be unflattened ) ( unflattened size  Union torch Size  Tuple  List  NamedShape   : new shape of the unflattened dimension )



( input : input tensor of shape  minibatch in channel iw  \text minibatch    \text in\ channel    iw  minibatch in channel iw   minibatch dim optional  ) ( kernel size : the size of the window  can be a single number or a tuple  kw   ) ( stride : the stride of the window  can be a single number or a tuple  sw    default  kernel size ) ( padding : implicit negative infinity pad to be add on both side  must be >  0 and <  kernel size / 2  ) ( dilation : the stride between elements within a slide window  must be > 0  ) ( ceil mode : if true  will use ceil instead of floor to compute the output shape  this ensure that every element in the input tensor be cover by a slide window  ) ( return indices : if true  will return the argmax along with the max value  useful for torch nn functional max unpool1d later )
( input : input tensor  minibatch in channel ih iw  \text minibatch    \text in\ channel    ih   iw  minibatch in channel ih iw   minibatch dim optional  ) ( kernel size : size of the pool region  can be a single number or a tuple  kh  kw  ) ( stride : stride of the pool operation  can be a single number or a tuple  sh  sw   default  kernel size ) ( padding : implicit negative infinity pad to be add on both side  must be >  0 and <  kernel size / 2  ) ( dilation : the stride between elements within a slide window  must be > 0  ) ( ceil mode : if true  will use ceil instead of floor to compute the output shape  this ensure that every element in the input tensor be cover by a slide window  ) ( return indices : if true  will return the argmax along with the max value  useful for torch nn functional max unpool2d later )
( input : input tensor  minibatch in channel id ih iw  \text minibatch    \text in\ channel    id  ih   iw  minibatch in channel id ih iw   minibatch dim optional  ) ( kernel size : size of the pool region  can be a single number or a tuple  kt  kh  kw  ) ( stride : stride of the pool operation  can be a single number or a tuple  st  sh  sw   default  kernel size ) ( padding : implicit negative infinity pad to be add on both side  must be >  0 and <  kernel size / 2  ) ( dilation : the stride between elements within a slide window  must be > 0  ) ( ceil mode : if true  will use ceil instead of floor to compute the output shape  this ensure that every element in the input tensor be cover by a slide window  ) ( return indices : if true  will return the argmax along with the max value  useful for torch nn functional max unpool3d later )





( output size : the target output size  single integer  ) ( return indices : whether to return pool indices  default  false )
( output size : the target output size  single integer or double integer tuple  ) ( return indices : whether to return pool indices  default  false )
( output size : the target output size  single integer or triple integer tuple  ) ( return indices : whether to return pool indices  default  false )
( output size : the target output size  single integer or double integer tuple  )
( output size : the target output size  single integer or triple integer tuple  )
( kernel size : the size of the window to take a max over  can be a single number kkk  for a square kernel of k×kk \times kk×k  or a tuple  kh  kw  ) ( output size : the target output size of the image of the form oh×owoh \times owoh×ow  can be a tuple  oh  ow  or a single number ohohoh for a square image oh×ohoh \times ohoh×oh ) ( output ratio : if one want to have an output size as a ratio of the input size  this option can be give  this have to be a number or tuple in the range  0  1  ) ( return indices : if true  will return the indices along with the output  useful to pass to max unpool2d    )
( kernel size : the size of the window to take a max over  can be a single number kkk  for a square kernel of k×k×kk \times k \times kk×k×k  or a tuple  kt  kh  kw  ) ( output size : the target output size of the form ot×oh×owot \times oh \times owot×oh×ow  can be a tuple  ot  oh  ow  or a single number ohohoh for a cubic output oh×oh×ohoh \times oh \times ohoh×oh×oh ) ( output ratio : if one want to have an output size as a ratio of the input size  this option can be give  this have to be a number or tuple in the range  0  1  ) ( return indices : if true  will return the indices along with the output  useful to pass to max unpool3d    )











( input  Tensor  : input tensor ) ( dim  int  : dimension on which to split the input  default   1 )




( input  Tensor  : input ) ( dim  int  : a dimension along which softmin will be compute  so every slice along dim will sum to 1   ) ( dtype  torch dtype  optional  : the desire data type of return tensor  if specify  the input tensor be cast to dtype before the operation be perform  this be useful for prevent data type overflow  default  none  )
( input  Tensor  : input ) ( dim  int  : a dimension along which softmax will be compute  ) ( dtype  torch dtype  optional  : the desire data type of return tensor  if specify  the input tensor be cast to dtype before the operation be perform  this be useful for prevent data type overflow  default  none  )
( logits :  …  num feature  unnormalized log probabilities ) ( tau : non negative scalar temperature ) ( hard : if true  the return sample will be discretized as one hot vectors  but will be differentiate as if it be the soft sample in autograd ) ( dim  int  : a dimension along which softmax will be compute  default   1  )
( input  Tensor  : input ) ( dim  int  : a dimension along which log softmax will be compute  ) ( dtype  torch dtype  optional  : the desire data type of return tensor  if specify  the input tensor be cast to dtype before the operation be perform  this be useful for prevent data type overflow  default  none  )


( inplace : if set to true  will do this operation in place  default  false )







( input : input tensor of any shape ) ( p  float  : the exponent value in the norm formulation  default  2 ) ( dim  int  : the dimension to reduce  default  1 ) ( eps  float  : small value to avoid division by zero  default  1e 12 ) ( out  Tensor  optional  : the output tensor  if out be use  this operation won’t be differentiable  )


( p : probability of an element to be zero  default  0 5 ) ( training : apply dropout if be true  default  true ) ( inplace : if set to true  will do this operation in place  default  false )

( p : dropout probability of a channel to be zero  default  0 5 ) ( training : apply dropout if be true  default  true ) ( inplace : if set to true  will do this operation in place  default  false )
( p : probability of a channel to be zero  default  0 5 ) ( training : apply dropout if be true  default  true ) ( inplace : if set to true  will do this operation in place  default  false )
( p : probability of a channel to be zero  default  0 5 ) ( training : apply dropout if be true  default  true ) ( inplace : if set to true  will do this operation in place  default  false )
( input  LongTensor  : tensor contain indices into the embed matrix ) ( weight  Tensor  : the embed matrix with number of row equal to the maximum possible index   1  and number of columns equal to the embed size ) ( padding idx  int  optional  : if specify  the entries at pad idx do not contribute to the gradient  therefore  the embed vector at pad idx be not update during train  i e  it remain as a fix “pad”  ) ( max norm  float  optional  : if give  each embed vector with norm larger than max norm be renormalize to have norm max norm  note  this will modify weight in place  ) ( norm type  float  optional  : the p of the p norm to compute for the max norm option  default 2  ) ( scale grad by freq  boolean  optional  : if give  this will scale gradients by the inverse of frequency of the word in the mini batch  default false  ) ( sparse  bool  optional  : if true  gradient w r t  weight will be a sparse tensor  see note under torch nn embed for more detail regard sparse gradients  )
( input  LongTensor  : tensor contain bag of indices into the embed matrix ) ( weight  Tensor  : the embed matrix with number of row equal to the maximum possible index   1  and number of columns equal to the embed size ) ( offsets  LongTensor  optional  : only use when input be 1d  offset determine the start index position of each bag  sequence  in input  ) ( max norm  float  optional  : if give  each embed vector with norm larger than max norm be renormalize to have norm max norm  note  this will modify weight in place  ) ( norm type  float  optional  : the p in the p norm to compute for the max norm option  default 2  ) ( scale grad by freq  boolean  optional  : if give  this will scale gradients by the inverse of frequency of the word in the mini batch  default false  note  this option be not support when mode "max"  ) ( mode  string  optional  : "sum"  "mean" or "max"  specify the way to reduce the bag  default  "mean" ) ( sparse  bool  optional  : if true  gradient w r t  weight will be a sparse tensor  see note under torch nn embed for more detail regard sparse gradients  note  this option be not support when mode "max"  ) ( per sample weights  Tensor  optional  : a tensor of float / double weight  or none to indicate all weight should be take to be 1  if specify  per sample weight must have exactly the same shape as input and be treat as have the same offset  if those be not none  ) ( include last offset  bool  optional  : if true  the size of offset be equal to the number of bag   1  the last element be the size of the input  or the end index position of the last bag  sequence   ) ( padding idx  int  optional  : if specify  the entries at pad idx do not contribute to the gradient  therefore  the embed vector at pad idx be not update during train  i e  it remain as a fix “pad”  note that the embed vector at pad idx be exclude from the reduction  )

( input : tensor of arbitrary shape as probabilities  ) ( target : tensor of the same shape as input with value between 0 and 1  ) ( weight  Tensor  optional  : a manual rescale weight if provide it’s repeat to match input tensor shape ) ( size average  bool  optional  : deprecate  see reduction   by default  the losses be average over each loss element in the batch  note that for some losses  there multiple elements per sample  if the field size average be set to false  the losses be instead sum for each minibatch  ignore when reduce be false  default  true ) ( reduce  bool  optional  : deprecate  see reduction   by default  the losses be average or sum over observations for each minibatch depend on size average  when reduce be false  return a loss per batch element instead and ignore size average  default  true ) ( reduction  string  optional  : specify the reduction to apply to the output  'none'   'mean'   'sum'  'none'  no reduction will be apply  'mean'  the sum of the output will be divide by the number of elements in the output  'sum'  the output will be sum  note  size average and reduce be in the process of be deprecate  and in the meantime  specify either of those two args will override reduction  default  'mean' )
( input : tensor of arbitrary shape as unnormalized score  often refer to as logits   ) ( target : tensor of the same shape as input with value between 0 and 1 ) ( weight  Tensor  optional  : a manual rescale weight if provide it’s repeat to match input tensor shape ) ( size average  bool  optional  : deprecate  see reduction   by default  the losses be average over each loss element in the batch  note that for some losses  there multiple elements per sample  if the field size average be set to false  the losses be instead sum for each minibatch  ignore when reduce be false  default  true ) ( reduce  bool  optional  : deprecate  see reduction   by default  the losses be average or sum over observations for each minibatch depend on size average  when reduce be false  return a loss per batch element instead and ignore size average  default  true ) ( reduction  string  optional  : specify the reduction to apply to the output  'none'   'mean'   'sum'  'none'  no reduction will be apply  'mean'  the sum of the output will be divide by the number of elements in the output  'sum'  the output will be sum  note  size average and reduce be in the process of be deprecate  and in the meantime  specify either of those two args will override reduction  default  'mean' ) ( pos weight  Tensor  optional  : a weight of positive examples  must be a vector with length equal to the number of class  )
( input : expectation of underlie poisson distribution  ) ( target : random sample target∼poisson input target \sim \text poisson  input target∼poisson input   ) ( log input : if true the loss be compute as exp⁡ input −target∗input\exp \text input     \text target    \text input exp input −target∗input  if false then loss be input−target∗log⁡ input eps \text input    \text target    \log \text input  \text eps  input−target∗log input eps   default  true ) ( full : whether to compute full loss  i  e  to add the stirling approximation term  default  false target∗log⁡ target −target 0 5∗log⁡ 2∗π∗target \text target    \log \text target     \text target    0 5   \log 2   \pi   \text target  target∗log target −target 0 5∗log 2∗π∗target   ) ( size average  bool  optional  : deprecate  see reduction   by default  the losses be average over each loss element in the batch  note that for some losses  there multiple elements per sample  if the field size average be set to false  the losses be instead sum for each minibatch  ignore when reduce be false  default  true ) ( eps  float  optional  : small value to avoid evaluation of log⁡ 0 \log 0 log 0  when log input false  default  1e 8 ) ( reduce  bool  optional  : deprecate  see reduction   by default  the losses be average or sum over observations for each minibatch depend on size average  when reduce be false  return a loss per batch element instead and ignore size average  default  true ) ( reduction  string  optional  : specify the reduction to apply to the output  'none'   'mean'   'sum'  'none'  no reduction will be apply  'mean'  the sum of the output will be divide by the number of elements in the output  'sum'  the output will be sum  note  size average and reduce be in the process of be deprecate  and in the meantime  specify either of those two args will override reduction  default  'mean' )

( input  Tensor  : predict unnormalized score  often refer to as logits   see shape section below for support shape  ) ( target  Tensor  : grind truth class indices or class probabilities  see shape section below for support shape  ) ( weight  Tensor  optional  : a manual rescale weight give to each class  if give  have to be a tensor of size c ) ( size average  bool  optional  : deprecate  see reduction   by default  the losses be average over each loss element in the batch  note that for some losses  there multiple elements per sample  if the field size average be set to false  the losses be instead sum for each minibatch  ignore when reduce be false  default  true ) ( ignore index  int  optional  : specify a target value that be ignore and do not contribute to the input gradient  when size average be true  the loss be average over non ignore target  note that ignore index be only applicable when the target contain class indices  default   100 ) ( reduce  bool  optional  : deprecate  see reduction   by default  the losses be average or sum over observations for each minibatch depend on size average  when reduce be false  return a loss per batch element instead and ignore size average  default  true ) ( reduction  string  optional  : specify the reduction to apply to the output  'none'   'mean'   'sum'  'none'  no reduction will be apply  'mean'  the sum of the output will be divide by the number of elements in the output  'sum'  the output will be sum  note  size average and reduce be in the process of be deprecate  and in the meantime  specify either of those two args will override reduction  default  'mean' ) ( label smoothing  float  optional  : a float in  0 0  1 0   specify the amount of smooth when compute the loss  where 0 0 mean no smooth  the target become a mixture of the original grind truth and a uniform distribution as describe in rethink the inception architecture for computer vision  default  0 00 00 0  )
( log probs :  t n c  t  n  c  t n c  or  t c  t  c  t c  where c   number of character in alphabet include blank  t   input length  and n   batch size  the logarithmized probabilities of the output  e g  obtain with torch nn functional log softmax     ) ( targets :  n s  n  s  n s  or  sum target lengths    target cannot be blank  in the second form  the target be assume to be concatenate  ) ( input lengths :  n  n  n  or         lengths of the input  must each be ≤t\leq t≤t  ) ( target lengths :  n  n  n  or         lengths of the target ) ( blank  int  optional  : blank label  default 000  ) ( reduction  string  optional  : specify the reduction to apply to the output  'none'   'mean'   'sum'  'none'  no reduction will be apply  'mean'  the output losses will be divide by the target lengths and then the mean over the batch be take  'sum'  the output will be sum  default  'mean' ) ( zero infinity  bool  optional  : whether to zero infinite losses and the associate gradients  default  false infinite losses mainly occur when the input be too short to be align to the target  )
( input : expectation of the gaussian distribution  ) ( target : sample from the gaussian distribution  ) ( var : tensor of positive variance s   one for each of the expectations in the input  heteroscedastic   or a single one  homoscedastic   ) ( full  bool  optional  : include the constant term in the loss calculation  default  false  ) ( eps  float  optional  : value add to var  for stability  default  1e 6  ) ( reduction  string  optional  : specify the reduction to apply to the output  'none'   'mean'   'sum'  'none'  no reduction will be apply  'mean'  the output be the average of all batch member losses  'sum'  the output be the sum of all batch member losses  default  'mean'  )

( input : tensor of arbitrary shape in log probabilities  ) ( target : tensor of the same shape as input  see log target for the target’s interpretation  ) ( size average  bool  optional  : deprecate  see reduction   by default  the losses be average over each loss element in the batch  note that for some losses  there multiple elements per sample  if the field size average be set to false  the losses be instead sum for each minibatch  ignore when reduce be false  default  true ) ( reduce  bool  optional  : deprecate  see reduction   by default  the losses be average or sum over observations for each minibatch depend on size average  when reduce be false  return a loss per batch element instead and ignore size average  default  true ) ( reduction  string  optional  : specify the reduction to apply to the output  'none'   'batchmean'   'sum'   'mean'  'none'  no reduction will be apply 'batchmean'  the sum of the output will be divide by the batchsize 'sum'  the output will be sum 'mean'  the output will be divide by the number of elements in the output default  'mean' ) ( log target  bool  : a flag indicate whether target be pass in the log space  it be recommend to pass certain distributions  like softmax  in the log space to avoid numerical issue cause by explicit log  default  false )






( input :  n c  n  c  n c  where c   number of class or  n c h w  n  c  h  w  n c h w  in case of 2d loss  or  n c d1 d2     dk  n  c  d 1  d 2       d k  n c d1​ d2​     dk​  where k≥1k \geq 1k≥1 in the case of k dimensional loss  input be expect to be log probabilities  ) ( target :  n  n  n  where each value be 0≤targets i ≤c−10 \leq \text target  i  \leq c 10≤targets i ≤c−1  or  n d1 d2     dk  n  d 1  d 2       d k  n d1​ d2​     dk​  where k≥1k \geq 1k≥1 for k dimensional loss  ) ( weight  Tensor  optional  : a manual rescale weight give to each class  if give  have to be a tensor of size c ) ( size average  bool  optional  : deprecate  see reduction   by default  the losses be average over each loss element in the batch  note that for some losses  there multiple elements per sample  if the field size average be set to false  the losses be instead sum for each minibatch  ignore when reduce be false  default  true ) ( ignore index  int  optional  : specify a target value that be ignore and do not contribute to the input gradient  when size average be true  the loss be average over non ignore target  default   100 ) ( reduce  bool  optional  : deprecate  see reduction   by default  the losses be average or sum over observations for each minibatch depend on size average  when reduce be false  return a loss per batch element instead and ignore size average  default  true ) ( reduction  string  optional  : specify the reduction to apply to the output  'none'   'mean'   'sum'  'none'  no reduction will be apply  'mean'  the sum of the output will be divide by the number of elements in the output  'sum'  the output will be sum  note  size average and reduce be in the process of be deprecate  and in the meantime  specify either of those two args will override reduction  default  'mean' )





( input  Tensor  : n dimensional tensor ) ( pad  tuple  : m elements tuple  where m2≤\frac m  2  \leq2m​≤ input dimension and mmm be even  ) ( mode : 'constant'  'reflect'  'replicate' or 'circular'  default  'constant' ) ( value : fill value for 'constant' pad  default  0 )
( input  Tensor  : the input tensor ) ( size  int or Tuple int  or Tuple int  int  or Tuple int  int  int   : output spatial size  ) ( scale factor  float or Tuple float   : multiplier for spatial size  if scale factor be a tuple  its length have to match input dim    ) ( mode  str  : algorithm use for upsampling  'nearest'   'linear'   'bilinear'   'bicubic'   'trilinear'   'area'   'nearest exact'  default  'nearest' ) ( align corners  bool  optional  : geometrically  we consider the pixels of the input and output as square rather than point  if set to true  the input and output tensors be align by the center point of their corner pixels  preserve the value at the corner pixels  if set to false  the input and output tensors be align by the corner point of their corner pixels  and the interpolation use edge value pad for out of boundary value  make this operation independent of input size when scale factor be keep the same  this only have an effect when mode be 'linear'  'bilinear'  'bicubic' or 'trilinear'  default  false ) ( recompute scale factor  bool  optional  : recompute the scale factor for use in the interpolation calculation  if recompute scale factor be true  then scale factor must be pass in and scale factor be use to compute the output size  the compute output size will be use to infer new scale for the interpolation  note that when scale factor be float point  it may differ from the recomputed scale factor due to round and precision issue  if recompute scale factor be false  then size or scale factor will be use directly for interpolation  default  none  ) ( antialias  bool  optional  : flag to apply anti aliasing  default  false  use anti alias option together with align corner false  interpolation result would match pillow result for downsampling operation  support modes  'bilinear'  'bicubic'  )
( input  Tensor  : the input tensor ) ( size  int or Tuple int  or Tuple int  int  or Tuple int  int  int   : output spatial size  ) ( scale factor  float or Tuple float   : multiplier for spatial size  have to match input size if it be a tuple  ) ( mode  string  : algorithm use for upsampling  'nearest'   'linear'   'bilinear'   'bicubic'   'trilinear'  default  'nearest' ) ( align corners  bool  optional  : geometrically  we consider the pixels of the input and output as square rather than point  if set to true  the input and output tensors be align by the center point of their corner pixels  preserve the value at the corner pixels  if set to false  the input and output tensors be align by the corner point of their corner pixels  and the interpolation use edge value pad for out of boundary value  make this operation independent of input size when scale factor be keep the same  this only have an effect when mode be 'linear'  'bilinear'  'bicubic' or 'trilinear'  default  false )
( input  Tensor  : input ) ( size  int or Tuple int  int  or Tuple int  int  int   : output spatia size  ) ( scale factor  int  : multiplier for spatial size  have to be an integer  )
( input  Tensor  : input ) ( size  int or Tuple int  int   : output spatial size  ) ( scale factor  int or Tuple int  int   : multiplier for spatial size )
( input  Tensor  : input of shape  n c hin win  n  c  h \text in   w \text in   n c hin​ win​   4 d case  or  n c din hin win  n  c  d \text in   h \text in   w \text in   n c din​ hin​ win​   5 d case  ) ( grid  Tensor  : flow field of shape  n hout wout 2  n  h \text out   w \text out   2  n hout​ wout​ 2   4 d case  or  n dout hout wout 3  n  d \text out   h \text out   w \text out   3  n dout​ hout​ wout​ 3   5 d case  ) ( mode  str  : interpolation mode to calculate output value 'bilinear'   'nearest'   'bicubic'  default  'bilinear' note  mode 'bicubic' support only 4 d input  when mode 'bilinear' and the input be 5 d  the interpolation mode use internally will actually be trilinear  however  when the input be 4 d  the interpolation mode will legitimately be bilinear  ) ( padding mode  str  : pad mode for outside grid value 'zeros'   'border'   'reflection'  default  'zeros' ) ( align corners  bool  optional  : geometrically  we consider the pixels of the input  as square rather than point  if set to true  the extrema   1 and 1  be consider as refer to the center point of the input’s corner pixels  if set to false  they be instead consider as refer to the corner point of the input’s corner pixels  make the sample more resolution agnostic  this option parallel the align corner option in interpolate    and so whichever option be use here should also be use there to resize the input image before grid sample  default  false )
( theta  Tensor  : input batch of affine matrices with shape  n×2×3n \times 2 \times 3n×2×3  for 2d or  n×3×4n \times 3 \times 4n×3×4  for 3d ) ( size  torch Size  : the target output image size   n×c×h×wn \times c \times h \times wn×c×h×w for 2d or n×c×d×h×wn \times c \times d \times h \times wn×c×d×h×w for 3d  example  torch size  32  3  24  24   ) ( align corners  bool  optional  : if true  consider  1 and 1 to refer to the center of the corner pixels rather than the image corner  refer to grid sample   for a more complete description  a grid generate by affine grid   should be pass to grid sample   with the same set for this option  default  false )
( gradient  Tensor or None  : gradient w r t  the tensor  if it be a tensor  it will be automatically convert to a tensor that do not require grad unless create graph be true  none value can be specify for scalar tensors or ones that don’t require grad  if a none value would be acceptable then this argument be optional  ) ( retain graph  bool  optional  : if false  the graph use to compute the grads will be free  note that in nearly all case set this option to true be not need and often can be work around in a much more efficient way  default to the value of create graph  ) ( create graph  bool  optional  : if true  graph of the derivative will be construct  allow to compute higher order derivative products  default to false  ) ( inputs  sequence of Tensor  : input w r t  which the gradient will be accumulate into  grad  all other tensors will be ignore  if not provide  the gradient be accumulate into all the leaf tensors that be use to compute the attr  tensors  )











( tensors  Sequence Tensor  or Tensor  : tensors of which the derivative will be compute  ) ( grad tensors  Sequence Tensor or None  or Tensor  optional  : the “vector” in the jacobian vector product  usually gradients w r t  each element of correspond tensors  none value can be specify for scalar tensors or ones that don’t require grad  if a none value would be acceptable for all grad tensors  then this argument be optional  ) ( retain graph  bool  optional  : if false  the graph use to compute the grad will be free  note that in nearly all case set this option to true be not need and often can be work around in a much more efficient way  default to the value of create graph  ) ( create graph  bool  optional  : if true  graph of the derivative will be construct  allow to compute higher order derivative products  default to false  ) ( inputs  Sequence Tensor  or Tensor  optional  : input w r t  which the gradient be will accumulate into  grad  all other tensors will be ignore  if not provide  the gradient be accumulate into all the leaf tensors that be use to compute the attr  tensors  )
( outputs  sequence of Tensor  : output of the differentiate function  ) ( inputs  sequence of Tensor  : input w r t  which the gradient will be return  and not accumulate into  grad   ) ( grad outputs  sequence of Tensor  : the “vector” in the vector jacobian product  usually gradients w r t  each output  none value can be specify for scalar tensors or ones that don’t require grad  if a none value would be acceptable for all grad tensors  then this argument be optional  default  none  ) ( retain graph  bool  optional  : if false  the graph use to compute the grad will be free  note that in nearly all case set this option to true be not need and often can be work around in a much more efficient way  default to the value of create graph  ) ( create graph  bool  optional  : if true  graph of the derivative will be construct  allow to compute higher order derivative products  default  false  ) ( allow unused  bool  optional  : if false  specify input that be not use when compute output  and therefore their grad be always zero  be an error  default to false  ) ( is grads batched  bool  optional  : if true  the first dimension of each tensor in grad output will be interpret as the batch dimension  instead of compute a single vector jacobian product  we compute a batch of vector jacobian products for each “vector” in the batch  we use the vmap prototype feature as the backend to vectorize call to the autograd engine so that this computation can be perform in a single call  this should lead to performance improvements when compare to manually loop and perform backward multiple time  note that due to this feature be experimental  there may be performance cliffs  please use torch  c  debug only display vmap fallback warn true  to show any performance warn and file an issue on github if warn exist for your use case  default to false  )



( func  function  : a python function that take tensor input and return a tuple of tensors or a tensor  ) ( inputs  tuple of Tensors or Tensor  : input to the function func  ) ( create graph  bool  optional  : if true  the jacobian will be compute in a differentiable manner  note that when strict be false  the result can not require gradients or be disconnect from the input   default to false  ) ( strict  bool  optional  : if true  an error will be raise when we detect that there exist an input such that all the output be independent of it  if false  we return a tensor of zero as the jacobian for say input  which be the expect mathematical value  default to false  ) ( vectorize  bool  optional  : this feature be experimental  please consider use functorch’s jacrev or jacfwd instead if you be look for something less experimental and more performant  when compute the jacobian  usually we invoke autograd grad once per row of the jacobian  if this flag be true  we perform only a single autograd grad call with batch grad true which use the vmap prototype feature  though this should lead to performance improvements in many case  because this feature be still experimental  there may be performance cliffs  see torch autograd grad  ’s batch grad parameter for more information  ) ( strategy  str  optional  : set to "forward mode" or "reverse mode" to determine whether the jacobian will be compute with forward or reverse mode ad  currently  "forward mode" require vectorized true  default to "reverse mode"  if func have more output than input  "forward mode" tend to be more performant  otherwise  prefer to use "reverse mode"  )
( func  function  : a python function that take tensor input and return a tensor with a single element  ) ( inputs  tuple of Tensors or Tensor  : input to the function func  ) ( create graph  bool  optional  : if true  the hessian will be compute in a differentiable manner  note that when strict be false  the result can not require gradients or be disconnect from the input  default to false  ) ( strict  bool  optional  : if true  an error will be raise when we detect that there exist an input such that all the output be independent of it  if false  we return a tensor of zero as the hessian for say input  which be the expect mathematical value  default to false  ) ( vectorize  bool  optional  : this feature be experimental  please consider use functorch instead if you be look for something less experimental and more performant  when compute the hessian  usually we invoke autograd grad once per row of the hessian  if this flag be true  we use the vmap prototype feature as the backend to vectorize call to autograd grad so we only invoke it once instead of once per row  this should lead to performance improvements in many use case  however  due to this feature be incomplete  there may be performance cliffs  please use torch  c  debug only display vmap fallback warn true  to show any performance warn and file us issue if warn exist for your use case  default to false  ) ( outer jacobian strategy  str  optional  : the hessian be compute by compute the jacobian of a jacobian  the inner jacobian be always compute in reverse mode ad  set strategy to "forward mode" or "reverse mode" determine whether the outer jacobian will be compute with forward or reverse mode ad  currently  compute the outer jacobian in "forward mode" require vectorized true  default to "reverse mode"  )
( func  function  : a python function that take tensor input and return a tuple of tensors or a tensor  ) ( inputs  tuple of Tensors or Tensor  : input to the function func  ) ( v  tuple of Tensors or Tensor  : the vector for which the vector jacobian product be compute   must be the same size as the output of func  this argument be optional when the output of func contain a single element and  if it be not provide  will be set as a tensor contain a single 1  ) ( create graph  bool  optional  : if true  both the output and result will be compute in a differentiable way  note that when strict be false  the result can not require gradients or be disconnect from the input   default to false  ) ( strict  bool  optional  : if true  an error will be raise when we detect that there exist an input such that all the output be independent of it  if false  we return a tensor of zero as the vjp for say input  which be the expect mathematical value  default to false  )
( func  function  : a python function that take tensor input and return a tuple of tensors or a tensor  ) ( inputs  tuple of Tensors or Tensor  : input to the function func  ) ( v  tuple of Tensors or Tensor  : the vector for which the jacobian vector product be compute  must be the same size as the input of func  this argument be optional when the input to func contain a single element and  if it be not provide  will be set as a tensor contain a single 1  ) ( create graph  bool  optional  : if true  both the output and result will be compute in a differentiable way  note that when strict be false  the result can not require gradients or be disconnect from the input   default to false  ) ( strict  bool  optional  : if true  an error will be raise when we detect that there exist an input such that all the output be independent of it  if false  we return a tensor of zero as the jvp for say input  which be the expect mathematical value  default to false  )
( func  function  : a python function that take tensor input and return a tensor with a single element  ) ( inputs  tuple of Tensors or Tensor  : input to the function func  ) ( v  tuple of Tensors or Tensor  : the vector for which the vector hessian product be compute  must be the same size as the input of func  this argument be optional when func’s input contain a single element and  if it be not provide  will be set as a tensor contain a single 1  ) ( create graph  bool  optional  : if true  both the output and result will be compute in a differentiable way  note that when strict be false  the result can not require gradients or be disconnect from the input  default to false  ) ( strict  bool  optional  : if true  an error will be raise when we detect that there exist an input such that all the output be independent of it  if false  we return a tensor of zero as the vhp for say input  which be the expect mathematical value  default to false  )
( func  function  : a python function that take tensor input and return a tensor with a single element  ) ( inputs  tuple of Tensors or Tensor  : input to the function func  ) ( v  tuple of Tensors or Tensor  : the vector for which the hessian vector product be compute  must be the same size as the input of func  this argument be optional when func’s input contain a single element and  if it be not provide  will be set as a tensor contain a single 1  ) ( create graph  bool  optional  : if true  both the output and result will be compute in a differentiable way  note that when strict be false  the result can not require gradients or be disconnect from the input   default to false  ) ( strict  bool  optional  : if true  an error will be raise when we detect that there exist an input such that all the output be independent of it  if false  we return a tensor of zero as the hvp for say input  which be the expect mathematical value  default to false  )


( mode  bool  : flag whether to enable grad  true   or disable  false   this can be use to conditionally enable gradients  )
( mode  bool  : flag whether to enable or disable inference mode )


( func  function  : a python function that take tensor input and return a tensor or a tuple of tensors ) ( inputs  tuple of Tensor or Tensor  : input to the function ) ( grad outputs  tuple of Tensor or Tensor  optional  : the gradients with respect to the function’s output  ) ( eps  float  optional  : perturbation for finite differences ) ( atol  float  optional  : absolute tolerance ) ( rtol  float  optional  : relative tolerance ) ( gen non contig grad outputs  bool  optional  : if grad output be none and gen non contig grad output be true  the randomly generate gradient output be make to be noncontiguous ) ( raise exception  bool  optional  : indicate whether to raise an exception if the check fail  the exception give more information about the exact nature of the failure  this be helpful when debug gradchecks  ) ( nondet tol  float  optional  : tolerance for non determinism  when run identical input through the differentiation  the result must either match exactly  default  0 0  or be within this tolerance  note that a small amount of nondeterminism in the gradient will lead to larger inaccuracies in the second derivative  ) ( check undefined grad  bool  optional  : if true  check if undefined output grads be support and treat as zero ) ( check batched grad  bool  optional  : if true  check if we can compute batch gradients use prototype vmap support  default to false  ) ( fast mode  bool  optional  : if true  run a faster implementation of gradgradcheck that no longer compute the entire jacobian  )
( path  str  : path where the trace will be write  )
( group by input shapes : group entries by  event name  input shape  rather than just event name  this be useful to see which input shape contribute to the runtime the most and may help with size specific optimizations or choose the best candidates for quantization  aka fit a roof line  ) ( group by stack n : group by top n stack trace entries )

( path  str  : path to nvprof trace )
( Stream  Stream  : select stream  this manager be a no op if it’s none  )



( device  torch device or int  optional  : select device  return the currently select stream for the current device  give by current device    if device be none  default   )
( device  torch device or int  optional  : select device  return the default stream for the current device  give by current device    if device be none  default   )
( device  torch device or int  : device index to select  it’s a no op if this argument be a negative integer or none  )

( obj  Tensor or Storage  : object allocate on the select device  )

( device  torch device or int  optional  : device for which to return the device capability  this function be a no op if this argument be a negative integer  it use the current device  give by current device    if device be none  default   )
( device  torch device or int  optional  : device for which to return the name  this function be a no op if this argument be a negative integer  it use the current device  give by current device    if device be none  default   )
( device  torch device or int or str  : device for which to return the properties of the device  )





( device  torch device or int  : select device  this function be a no op if this argument be negative  )
( stream  Stream  : select stream  this function be a no op if this argument be none  )
( device  torch device or int  optional  : a device on which to allocate the stream  if device be none  default  or a negative integer  this will use the current device  ) ( priority  int  optional  : priority of the stream  can be either  1  high priority  or 0  low priority   by default  stream have priority 0  )

( event  torch cuda Event  optional  : event to record  if not give  a new one will be allocate  )
( event  torch cuda Event  : an event to wait for  )
( stream  Stream  : a stream to synchronize  )
( device  torch device or int  optional  : device for which to synchronize  it use the current device  give by current device    if device be none  default   )
( device  torch device or int  optional  : the device to return the rng state of  default  'cuda'  i e   torch device 'cuda'   the current cuda device   )

( new state  torch ByteTensor  : the desire state ) ( device  torch device or int  optional  : the device to set the rng state  default  'cuda'  i e   torch device 'cuda'   the current cuda device   )
( new states  Iterable of torch ByteTensor  : the desire state for each device )
( seed  int  : the desire seed  )
( seed  int  : the desire seed  )



( enable timing  bool  optional  : indicate if the event should measure time  default  false  ) ( blocking  bool  optional  : if true  wait   will be block  default  false  ) ( interprocess  bool  : if true  the event can be share between process  default  false  )


( device  torch device or int  optional  : select device  return printout for the current device  give by current device    if device be none  default   )
( device  torch device or int  optional  : select device  return statistics for the current device  give by current device    if device be none  default   )
( device  torch device or int  optional  : select device  return printout for the current device  give by current device    if device be none  default   ) ( abbreviated  bool  optional  : whether to return an abbreviate summary  default  false   )

( device  torch device or int  optional  : select device  return statistic for the current device  give by current device    if device be none  default   )
( device  torch device or int  optional  : select device  return statistic for the current device  give by current device    if device be none  default   )
( device  torch device or int  optional  : select device  return statistic for the current device  give by current device    if device be none  default   )
( device  torch device or int  optional  : select device  return statistic for the current device  give by current device    if device be none  default   )
( device  torch device or int  optional  : select device  return statistic for the current device  give by current device    if device be none  default   )
( fraction  float  : range  0~1  allow memory equal total memory   fraction  ) ( device  torch device or int  optional  : select device  if it be none the default cuda device be use  )


( device  torch device or int  optional  : select device  return statistic for the current device  give by current device    if device be none  default   )
( device  torch device or int  optional  : select device  return statistic for the current device  give by current device    if device be none  default   )
( size  int  : number of bytes to be allocate  ) ( device  torch device or int  optional  : select device  if it be none the default cuda device be use  ) ( stream  torch cuda Stream or int  optional  : select stream  if be none then the default stream for the select device be use  )
( mem ptr  int  : memory address to be free by the allocator  )
( msg  string  : ascii message to associate with the event  )
( msg  string  : ascii message to associate with range )

( param group  dict  : specify what tensors should be optimize along with group specific optimization options  )
( state dict  dict  : optimizer state  should be an object return from a call to state dict    )

( closure  callable  : a closure that reevaluate the model and return the loss  optional for most optimizers  )
( set to none  bool  : instead of set to zero  set the grads to none  this will in general have lower memory footprint  and can modestly improve performance  however  it change certain behaviors  for example  1  when the user try to access a gradient and perform manual ops on it  a none attribute or a tensor full of 0s will behave differently  2  if the user request zero grad set to none true  follow by a backward pass   grads be guarantee to be none for params that do not receive a gradient  3  torch optim optimizers have a different behavior if the gradient be 0 or none  in one case it do the step with a gradient of 0 and in the other it skip the step altogether   )
( params  iterable  : iterable of parameters to optimize or dicts define parameter group ) ( rho  float  optional  : coefficient use for compute a run average of square gradients  default  0 9  ) ( eps  float  optional  : term add to the denominator to improve numerical stability  default  1e 6  ) ( lr  float  optional  : coefficient that scale delta before it be apply to the parameters  default  1 0  ) ( weight decay  float  optional  : weight decay  l2 penalty   default  0  )
( state dict  dict  : optimizer state  should be an object return from a call to state dict    )
( closure  callable  optional  : a closure that reevaluate the model and return the loss  )
( params  iterable  : iterable of parameters to optimize or dicts define parameter group ) ( lr  float  optional  : learn rate  default  1e 2  ) ( lr decay  float  optional  : learn rate decay  default  0  ) ( weight decay  float  optional  : weight decay  l2 penalty   default  0  ) ( eps  float  optional  : term add to the denominator to improve numerical stability  default  1e 10  )
( state dict  dict  : optimizer state  should be an object return from a call to state dict    )
( closure  callable  optional  : a closure that reevaluate the model and return the loss  )
( params  iterable  : iterable of parameters to optimize or dicts define parameter group ) ( lr  float  optional  : learn rate  default  1e 3  ) ( betas  Tuple float  float   optional  : coefficients use for compute run average of gradient and its square  default   0 9  0 999   ) ( eps  float  optional  : term add to the denominator to improve numerical stability  default  1e 8  ) ( weight decay  float  optional  : weight decay  l2 penalty   default  0  ) ( amsgrad  boolean  optional  : whether to use the amsgrad variant of this algorithm from the paper on the convergence of adam and beyond  default  false  ) ( maximize  bool  optional  : maximize the params base on the objective  instead of minimize  default  false  )
( state dict  dict  : optimizer state  should be an object return from a call to state dict    )
( closure  callable  optional  : a closure that reevaluate the model and return the loss  )
( params  iterable  : iterable of parameters to optimize or dicts define parameter group ) ( lr  float  optional  : learn rate  default  1e 3  ) ( betas  Tuple float  float   optional  : coefficients use for compute run average of gradient and its square  default   0 9  0 999   ) ( eps  float  optional  : term add to the denominator to improve numerical stability  default  1e 8  ) ( weight decay  float  optional  : weight decay coefficient  default  1e 2  ) ( amsgrad  boolean  optional  : whether to use the amsgrad variant of this algorithm from the paper on the convergence of adam and beyond  default  false  ) ( maximize  bool  optional  : maximize the params base on the objective  instead of minimize  default  false  )
( state dict  dict  : optimizer state  should be an object return from a call to state dict    )
( closure  callable  optional  : a closure that reevaluate the model and return the loss  )
( params  iterable  : iterable of parameters to optimize or dicts define parameter group ) ( lr  float  optional  : learn rate  default  1e 3  ) ( betas  Tuple float  float   optional  : coefficients use for compute run average of gradient and its square  default   0 9  0 999   ) ( eps  float  optional  : term add to the denominator to improve numerical stability  default  1e 8  )
( state dict  dict  : optimizer state  should be an object return from a call to state dict    )
( closure  callable  optional  : a closure that reevaluate the model and return the loss  )
( params  iterable  : iterable of parameters to optimize or dicts define parameter group ) ( lr  float  optional  : learn rate  default  2e 3  ) ( betas  Tuple float  float   optional  : coefficients use for compute run average of gradient and its square ) ( eps  float  optional  : term add to the denominator to improve numerical stability  default  1e 8  ) ( weight decay  float  optional  : weight decay  l2 penalty   default  0  )
( state dict  dict  : optimizer state  should be an object return from a call to state dict    )
( closure  callable  optional  : a closure that reevaluate the model and return the loss  )
( params  iterable  : iterable of parameters to optimize or dicts define parameter group ) ( lr  float  optional  : learn rate  default  1e 2  ) ( lambd  float  optional  : decay term  default  1e 4  ) ( alpha  float  optional  : power for eta update  default  0 75  ) ( t0  float  optional  : point at which to start average  default  1e6  ) ( weight decay  float  optional  : weight decay  l2 penalty   default  0  )
( state dict  dict  : optimizer state  should be an object return from a call to state dict    )
( closure  callable  optional  : a closure that reevaluate the model and return the loss  )
( lr  float  : learn rate  default  1  ) ( max iter  int  : maximal number of iterations per optimization step  default  20  ) ( max eval  int  : maximal number of function evaluations per optimization step  default  max iter   1 25   ) ( tolerance grad  float  : termination tolerance on first order optimality  default  1e 5   ) ( tolerance change  float  : termination tolerance on function value/parameter change  default  1e 9   ) ( history size  int  : update history size  default  100   ) ( line search fn  str  : either ‘strong wolfe’ or none  default  none   )
( state dict  dict  : optimizer state  should be an object return from a call to state dict    )
( closure  callable  : a closure that reevaluate the model and return the loss  )
( params  iterable  : iterable of parameters to optimize or dicts define parameter group ) ( lr  float  optional  : learn rate  default  1e 2  ) ( momentum  float  optional  : momentum factor  default  0  ) ( alpha  float  optional  : smooth constant  default  0 99  ) ( eps  float  optional  : term add to the denominator to improve numerical stability  default  1e 8  ) ( centered  bool  optional  : if true  compute the center rmsprop  the gradient be normalize by an estimation of its variance ) ( weight decay  float  optional  : weight decay  l2 penalty   default  0  )
( state dict  dict  : optimizer state  should be an object return from a call to state dict    )
( closure  callable  optional  : a closure that reevaluate the model and return the loss  )
( params  iterable  : iterable of parameters to optimize or dicts define parameter group ) ( lr  float  optional  : learn rate  default  1e 2  ) ( etas  Tuple float  float   optional  : pair of  etaminus  etaplis   that be multiplicative increase and decrease factor  default   0 5  1 2   ) ( step sizes  Tuple float  float   optional  : a pair of minimal and maximal allow step size  default   1e 6  50   )
( state dict  dict  : optimizer state  should be an object return from a call to state dict    )
( closure  callable  optional  : a closure that reevaluate the model and return the loss  )
( params  iterable  : iterable of parameters to optimize or dicts define parameter group ) ( lr  float  : learn rate ) ( momentum  float  optional  : momentum factor  default  0  ) ( weight decay  float  optional  : weight decay  l2 penalty   default  0  ) ( dampening  float  optional  : dampen for momentum  default  0  ) ( nesterov  bool  optional  : enable nesterov momentum  default  false  ) ( maximize  bool  optional  : maximize the params base on the objective  instead of minimize  default  false  )
( state dict  dict  : optimizer state  should be an object return from a call to state dict    )
( closure  callable  optional  : a closure that reevaluate the model and return the loss  )
( optimizer  Optimizer  : wrap optimizer  ) ( lr lambda  function or list  : a function which compute a multiplicative factor give an integer parameter epoch  or a list of such function  one for each group in optimizer param group  ) ( last epoch  int  : the index of last epoch  default   1  ) ( verbose  bool  : if true  print a message to stdout for each update  default  false  )
( state dict  dict  : scheduler state  should be an object return from a call to state dict    )
( optimizer  Optimizer  : wrap optimizer  ) ( lr lambda  function or list  : a function which compute a multiplicative factor give an integer parameter epoch  or a list of such function  one for each group in optimizer param group  ) ( last epoch  int  : the index of last epoch  default   1  ) ( verbose  bool  : if true  print a message to stdout for each update  default  false  )
( state dict  dict  : scheduler state  should be an object return from a call to state dict    )
( optimizer  Optimizer  : wrap optimizer  ) ( step size  int  : period of learn rate decay  ) ( gamma  float  : multiplicative factor of learn rate decay  default  0 1  ) ( last epoch  int  : the index of last epoch  default   1  ) ( verbose  bool  : if true  print a message to stdout for each update  default  false  )
( state dict  dict  : scheduler state  should be an object return from a call to state dict    )
( optimizer  Optimizer  : wrap optimizer  ) ( milestones  list  : list of epoch indices  must be increase  ) ( gamma  float  : multiplicative factor of learn rate decay  default  0 1  ) ( last epoch  int  : the index of last epoch  default   1  ) ( verbose  bool  : if true  print a message to stdout for each update  default  false  )
( state dict  dict  : scheduler state  should be an object return from a call to state dict    )
( optimizer  Optimizer  : wrap optimizer  ) ( gamma  float  : multiplicative factor of learn rate decay  ) ( last epoch  int  : the index of last epoch  default   1  ) ( verbose  bool  : if true  print a message to stdout for each update  default  false  )
( state dict  dict  : scheduler state  should be an object return from a call to state dict    )
( optimizer  Optimizer  : wrap optimizer  ) ( T max  int  : maximum number of iterations  ) ( eta min  float  : minimum learn rate  default  0  ) ( last epoch  int  : the index of last epoch  default   1  ) ( verbose  bool  : if true  print a message to stdout for each update  default  false  )
( state dict  dict  : scheduler state  should be an object return from a call to state dict    )
( optimizer  Optimizer  : wrap optimizer  ) ( mode  str  : one of min  max  in min mode  lr will be reduce when the quantity monitor have stop decrease  in max mode it will be reduce when the quantity monitor have stop increase  default  ‘min’  ) ( factor  float  : factor by which the learn rate will be reduce  new lr   lr   factor  default  0 1  ) ( patience  int  : number of epochs with no improvement after which learn rate will be reduce  for example  if patience   2  then we will ignore the first 2 epochs with no improvement  and will only decrease the lr after the 3rd epoch if the loss still hasn’t improve then  default  10  ) ( threshold  float  : threshold for measure the new optimum  to only focus on significant change  default  1e 4  ) ( threshold mode  str  : one of rel  abs  in rel mode  dynamic threshold   best     1   threshold   in ‘max’ mode or best     1   threshold   in min mode  in abs mode  dynamic threshold   best   threshold in max mode or best   threshold in min mode  default  ‘rel’  ) ( cooldown  int  : number of epochs to wait before resume normal operation after lr have be reduce  default  0  ) ( min lr  float or list  : a scalar or a list of scalars  a lower bind on the learn rate of all param group or each group respectively  default  0  ) ( eps  float  : minimal decay apply to lr  if the difference between new and old lr be smaller than eps  the update be ignore  default  1e 8  ) ( verbose  bool  : if true  print a message to stdout for each update  default  false  )
( optimizer  Optimizer  : wrap optimizer  ) ( base lr  float or list  : initial learn rate which be the lower boundary in the cycle for each parameter group  ) ( max lr  float or list  : upper learn rate boundaries in the cycle for each parameter group  functionally  it define the cycle amplitude  max lr   base lr   the lr at any cycle be the sum of base lr and some scale of the amplitude  therefore max lr may not actually be reach depend on scale function  ) ( step size up  int  : number of train iterations in the increase half of a cycle  default  2000 ) ( step size down  int  : number of train iterations in the decrease half of a cycle  if step size down be none  it be set to step size up  default  none ) ( mode  str  : one of  triangular  triangular2  exp range   value correspond to policies detail above  if scale fn be not none  this argument be ignore  default  ‘triangular’ ) ( gamma  float  : constant in ‘exp range’ scale function  gamma   cycle iterations  default  1 0 ) ( scale fn  function  : custom scale policy define by a single argument lambda function  where 0 <  scale fn x  <  1 for all x >  0  if specify  then ‘mode’ be ignore  default  none ) ( scale mode  str  :  ‘cycle’  ‘iterations’   define whether scale fn be evaluate on cycle number or cycle iterations  train iterations since start of cycle   default  ‘cycle’ ) ( cycle momentum  bool  : if true  momentum be cycle inversely to learn rate between ‘base momentum’ and ‘max momentum’  default  true ) ( base momentum  float or list  : lower momentum boundaries in the cycle for each parameter group  note that momentum be cycle inversely to learn rate  at the peak of a cycle  momentum be ‘base momentum’ and learn rate be ‘max lr’  default  0 8 ) ( max momentum  float or list  : upper momentum boundaries in the cycle for each parameter group  functionally  it define the cycle amplitude  max momentum   base momentum   the momentum at any cycle be the difference of max momentum and some scale of the amplitude  therefore base momentum may not actually be reach depend on scale function  note that momentum be cycle inversely to learn rate  at the start of a cycle  momentum be ‘max momentum’ and learn rate be ‘base lr’ default  0 9 ) ( last epoch  int  : the index of the last batch  this parameter be use when resume a train job  since step   should be invoke after each batch instead of after each epoch  this number represent the total number of batch compute  not the total number of epochs compute  when last epoch  1  the schedule be start from the begin  default   1 ) ( verbose  bool  : if true  print a message to stdout for each update  default  false  )
( state dict  dict  : scheduler state  should be an object return from a call to state dict    )
( optimizer  Optimizer  : wrap optimizer  ) ( max lr  float or list  : upper learn rate boundaries in the cycle for each parameter group  ) ( total steps  int  : the total number of step in the cycle  note that if a value be not provide here  then it must be infer by provide a value for epochs and step per epoch  default  none ) ( epochs  int  : the number of epochs to train for  this be use along with step per epoch in order to infer the total number of step in the cycle if a value for total step be not provide  default  none ) ( steps per epoch  int  : the number of step per epoch to train for  this be use along with epochs in order to infer the total number of step in the cycle if a value for total step be not provide  default  none ) ( pct start  float  : the percentage of the cycle  in number of step  spend increase the learn rate  default  0 3 ) ( anneal strategy  str  :  ‘cos’  ‘linear’  specify the anneal strategy  “cos” for cosine anneal  “linear” for linear anneal  default  ‘cos’ ) ( cycle momentum  bool  : if true  momentum be cycle inversely to learn rate between ‘base momentum’ and ‘max momentum’  default  true ) ( base momentum  float or list  : lower momentum boundaries in the cycle for each parameter group  note that momentum be cycle inversely to learn rate  at the peak of a cycle  momentum be ‘base momentum’ and learn rate be ‘max lr’  default  0 85 ) ( max momentum  float or list  : upper momentum boundaries in the cycle for each parameter group  functionally  it define the cycle amplitude  max momentum   base momentum   note that momentum be cycle inversely to learn rate  at the start of a cycle  momentum be ‘max momentum’ and learn rate be ‘base lr’ default  0 95 ) ( div factor  float  : determine the initial learn rate via initial lr   max lr/div factor default  25 ) ( final div factor  float  : determine the minimum learn rate via min lr   initial lr/final div factor default  1e4 ) ( three phase  bool  : if true  use a third phase of the schedule to annihilate the learn rate accord to ‘final div factor’ instead of modify the second phase  the first two phase will be symmetrical about the step indicate by ‘pct start’   ) ( last epoch  int  : the index of the last batch  this parameter be use when resume a train job  since step   should be invoke after each batch instead of after each epoch  this number represent the total number of batch compute  not the total number of epochs compute  when last epoch  1  the schedule be start from the begin  default   1 ) ( verbose  bool  : if true  print a message to stdout for each update  default  false  )
( state dict  dict  : scheduler state  should be an object return from a call to state dict    )
( optimizer  Optimizer  : wrap optimizer  ) ( T 0  int  : number of iterations for the first restart  ) ( T mult  int  optional  : a factor increase tit  i ti​ after a restart  default  1  ) ( eta min  float  optional  : minimum learn rate  default  0  ) ( last epoch  int  optional  : the index of last epoch  default   1  ) ( verbose  bool  : if true  print a message to stdout for each update  default  false  )
( state dict  dict  : scheduler state  should be an object return from a call to state dict    )
( input  Tensor  : the input sparse tensor ) ( dim  int or tuple of python ints  : a dimension or a list of dimension to reduce  default  reduce over all dim  ) ( dtype  torch dtype  optional  : the desire data type of return tensor  default  dtype of input  )
( mat  Tensor  : a dense matrix to be add ) ( mat1  Tensor  : a sparse matrix to be multiply ) ( mat2  Tensor  : a dense matrix to be multiply ) ( beta  Number  optional  : multiplier for mat  β\betaβ  ) ( alpha  Number  optional  : multiplier for mat1 mat2mat1   mat2mat1 mat2  α\alphaα  )
( mat1  SparseTensor  : the first sparse matrix to be multiply ) ( mat2  Tensor  : the second matrix to be multiply  which could be sparse or dense )
( input  Tensor  : input ) ( dim  int  : a dimension along which softmax will be compute  ) ( dtype  torch dtype  optional  : the desire data type of return tensor   if specify  the input tensor be cast to dtype before the operation be perform  this be useful for prevent data type overflow  default  none )
( input  Tensor  : input ) ( dim  int  : a dimension along which softmax will be compute  ) ( dtype  torch dtype  optional  : the desire data type of return tensor   if specify  the input tensor be cast to dtype before the operation be perform  this be useful for prevent data type overflow  default  none )