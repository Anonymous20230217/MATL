decorator def depth to space operand block size name string rearrange elements in the input tensor from the depth dimension into spatial block this operation be useful for implement sub pixel convolution that be part of model for image super resolution see 1 it rearrange elements of an input tensor of shape c xb xb h w to a tensor of shape c b xh bx w where b be the block size example x np array np reshape range 8 8 1 1 d type np float 32 x np tile x 1 2 3 a c input variable 8 2 3 d 2 s op c depth to space a block size 2 d 2 s op e val a x array 0 2 0 2 0 2 4 6 4 6 4 6 0 2 0 2 0 2 4 6 4 6 4 6 blank line 1 3 1 3 1 3 5 7 5 7 5 7 1 3 1 3 1 3 5 7 5 7 5 7 d type float 32 arg s operand input tensor with dimension math c \\times h \\times w block size int integer value this define the size of the spatial block where the depth elements move to number of channel c in the input tensor must be divisible by math block size \\times block size name str optional the name of the function instance in the network return class ~cntk ops function function see also 1 w shi et al real time single image and video super resolution use an efficient sub pixel convolutional neural network https //arxiv org/abs/1609 05158 from cnt k cnt k py import depth to space operand sanitize input operand get data type operand if not float block size be integer raise value error string return depth to space operand block size name
decorator def bernoulli shape d type default override or np float 32 mean number seed auto select name string bernoulli shape d type default override or np float 32 mean 0 5 seed auto select name generate sample from the bernoulli distribution with success probability mean arg s shape tuple shape of the output entries be independent random draw d type np float 32 or np float 64 or np float 16 data type default be np float 32 mean float success probability seed int pseudo random number generator seed default automatically select a unique seed name str optional the name of the function instance in the network return class ~cntk ops function function examples b c random bernoulli 2 3 seed 98052 b e val device c cpu explicitly set cpu because this be test on multiple platforms leave it unspecified in your code array 1 1 0 1 0 0 d type float 32 from cnt k cnt k py import bernoulli random shape d type sanitize random arg s shape d type return bernoulli random shape d type mean seed name
decorator def normal shape d type default override or np float 32 mean number scale number seed auto select name string normal shape d type default override or np float 32 mean 0 0 scale 1 0 seed auto select name generate sample from the normal distribution with mean mean and standard deviation scale arg s shape tuple shape of the output entries be independent random draw d type np float 32 or np float 64 or np float 16 data type default be np float 32 mean float mean of the distribution scale float scale standard deviation of the distribution seed int pseudo random number generator seed default automatically select a unique seed name str optional the name of the function instance in the network return class ~cntk ops function function examples z c random normal 2 3 seed 98052 z e val device c cpu explicitly set cpu because this be test on multiple platforms leave it unspecified in your code array 1 803254 0 995395 0 631974 1 736721 0 005615 0 340025 d type float 32 from cnt k cnt k py import normal random shape d type sanitize random arg s shape d type return normal random shape d type mean scale seed name
decorator def uniform shape d type default override or np float 32 low number high number seed auto select name string uniform shape d type default override or np float 32 low 0 0 high 1 0 seed auto select name generate sample from the uniform distribution in the interval low high arg s shape tuple shape of the output entries be independent random draw d type np float 32 or np float 64 or np float 16 data type default be np float 32 low float lower end of the range of the random number high float upper end of the range of the random number seed int pseudo random number generator seed default automatically select a unique seed name str optional the name of the function instance in the network return class ~cntk ops function function examples u c random uniform 2 3 seed 98052 u e val device c cpu explicitly set cpu because this be test on multiple platforms leave it unspecified in your code array 0 931785 0 814722 0 479606 0 937468 0 004351 0 185131 d type float 32 from cnt k cnt k py import uniform random shape d type sanitize random arg s shape d type return uniform random shape d type low high seed name
def he normal scale default param in it scale output rank sentinel value for infer param in it rank filter rank sentinel value for infer param in it rank seed none initializer arg s scale float scale output rank int output rank filter rank int filter rank seed int random seed return initializer for class ~cntk variables parameter initialize to gaussian distribution with mean 0 and standard deviation scale sqr t 2 0/fanin if seed be none seed sentinel value for auto select random seed return cnt k py he normal initializer scale output rank filter rank seed
def he uniform scale default param in it scale output rank sentinel value for infer param in it rank filter rank sentinel value for infer param in it rank seed none initializer arg s scale float scale output rank int output rank filter rank int filter rank seed int random seed return initializer for class ~cntk variables parameter initialize to uniform distribution between scale sqr t 6 0/fanin 1 1 if seed be none seed sentinel value for auto select random seed return cnt k py he uniform initializer scale output rank filter rank seed
def gl or ot normal scale default param in it scale output rank sentinel value for infer param in it rank filter rank sentinel value for infer param in it rank seed none initializer arg s scale float scale output rank int output rank filter rank int filter rank seed int random seed return initializer for class ~cntk variables parameter initialize to gaussian distribution with mean 0 and standard deviation scale sqr t 2 0/ fan in fan out if seed be none seed sentinel value for auto select random seed return cnt k py gl or ot normal initializer scale output rank filter rank seed
def gl or ot uniform scale default param in it scale output rank sentinel value for infer param in it rank filter rank sentinel value for infer param in it rank seed none gl or ot initializer arg s scale float scale output rank int output rank filter rank int filter rank seed int random seed return initializer for class ~cntk variables parameter initialize to uniform distribution between scale sqr t 6 0/ fan in fan out 1 1 if seed be none seed sentinel value for auto select random seed return cnt k py gl or ot uniform initializer scale output rank filter rank seed
def normal scale output rank sentinel value for infer param in it rank filter rank sentinel value for infer param in it rank seed none normal initializer arg s scale float scale output rank int output rank filter rank int filter rank seed int random seed return initializer for class ~cntk variables parameter initialize to normal distribution with mean 0 and standard deviation scale if seed be none seed sentinel value for auto select random seed return cnt k py normal initializer scale output rank filter rank seed
def uniform scale seed none uniform initializer arg s scale float scale seed int random seed return initializer for class ~cntk variables parameter initialize to uniform distribution between scale 1 0 1 0 note this map to the uniform 1 distribution in brain script if seed be none seed sentinel value for auto select random seed return cnt k py uniform initializer scale seed
decorator def batch normalization operand scale bias run mean run in v std spatial normalization time constant number blend time constant number epsilon number use cudnn engine false disable regular iz ation false name string run count none to do run count should be right after run in v std no need for upwards comp at normalize layer output for every mini batch for each output feature independently and apply affine transformation to preserve representation of the layer arg s operand input of the batch normalization operation scale parameter tensor that hold the learn component wise scale factor bias parameter tensor that hold the learn bias scale and bias must have the same dimension which must be equal to the input dimension in case of spatial false or number of output convolution feature map in case of spatial true run mean run mean which be use during evaluation phase and might be use during train as well you must pass a constant tensor with initial value 0 and the same dimension as scale and bias run in v std run variance represent as run mean run count denote the total number of sample that have be use so far to compute the run mean and run in v std parameters you must pass a scalar either rank 0 constant val spatial boo l flag that indicate whether to compute mean/var for each feature in a mini batch independently or in case of convolutional layer per future map normalization time constant float default 5000 time constant for compute run average of mean and variance as a low pass filter version of the batch statistics blend time constant float default 0 constant for smooth batch estimate with the run statistics epsilon conditioner constant add to the variance when compute the inverse standard deviation use cudnn engine boo l default false name str optional the name of the function instance in the network disable regular iz ation boo l default false turn off regular iz ation in batch normalization return class ~cntk ops function function if run count be none run count constant number import warn warn warn string string string warn from cnt k cnt k py import batch normalization operand sanitize input operand return batch normalization operand scale bias run mean run in v std run count spatial normalization time constant blend time constant epsilon use cudnn engine disable regular iz ation name
decorator def batch normalization operand scale bias run mean run in v std spatial normalization time constant number blend time constant number epsilon number use cudnn engine false disable regular iz ation false name string run count none to do run count should be right after run in v std no need for upwards comp at normalize layer output for every mini batch for each output feature independently and apply affine transformation to preserve representation of the layer arg s operand input of the batch normalization operation scale parameter tensor that hold the learn component wise scale factor bias parameter tensor that hold the learn bias scale and bias must have the same dimension which must be equal to the input dimension in case of spatial false or number of output convolution feature map in case of spatial true run mean run mean which be use during evaluation phase and might be use during train as well you must pass a constant tensor with initial value 0 and the same dimension as scale and bias run in v std run variance represent as run mean run count denote the total number of sample that have be use so far to compute the run mean and run in v std parameters you must pass a scalar either rank 0 constant val spatial boo l flag that indicate whether to compute mean/var for each feature in a mini batch independently or in case of convolutional layer per future map normalization time constant float default 5000 time constant for compute run average of mean and variance as a low pass filter version of the batch statistics blend time constant float default 0 constant for smooth batch estimate with the run statistics epsilon conditioner constant add to the variance when compute the inverse standard deviation use cudnn engine boo l default false name str optional the name of the function instance in the network disable regular iz ation boo l default false turn off regular iz ation in batch normalization return class ~cntk ops function function if run count be none run count constant number import warn warn warn string string string warn from cnt k cnt k py import batch normalization operand sanitize input operand return batch normalization operand scale bias run mean run in v std run count spatial normalization time constant blend time constant epsilon use cudnn engine disable regular iz ation name
def gru shape cell shape none activation default override or tan h in it default override or gl or ot uniform in it bias default override or number enable self stabilization default override or false name string gru shape cell shape none activation tan h in it gl or ot uniform in it bias 0 enable self stabilization false name layer factory function to create a gru block for use inside a recurrence the gru block implement one step of the recurrence and be stateless it accept the previous state as its first argument and output its new state example a gate recurrent layer from cnt k layer import gru layer recurrence gru 500 arg s shape int or tuple of in ts vector or tensor dimension of the output of this layer cell shape tuple default to none if give then the output state be first compute at cell shape and linearly project to shape activation class ~cntk ops function function default to fun c ~cntk ops tan h function to apply at the end e g relu in it scalar or num py array or mod cnt k initializer default to gl or ot uniform initial value of weight w in it bias scalar or num py array or mod cnt k initializer default to 0 initial value of weight b enable self stabilization boo l default to false if true then add a fun c ~cntk layer block stabilizer to all state relate projections but not the data input name str default to the name of the function instance in the network return class ~cntk ops function function a function prev h input h that implement one step of a recurrent gru layer activation get default override gru activation activation in it get default override gru in it in it in it bias get default override gru in it bias in it bias enable self stabilization get default override gru enable self stabilization enable self stabilization return recurrent block string shape cell shape activation activation use peepholes false in it in it in it bias in it bias enable self stabilization enable self stabilization name name
def lstm shape cell shape none activation default override or tan h use peepholes default override or false in it default override or gl or ot uniform in it bias default override or number enable self stabilization default override or false name string lstm shape cell shape none activation tan h use peepholes false in it gl or ot uniform in it bias 0 enable self stabilization false name layer factory function to create an lstm block for use inside a recurrence the lstm block implement one step of the recurrence and be stateless it accept the previous state as its first two arguments and output its new state as a two value tuple h c example a typical recurrent lstm layer from cnt k layer import lstm layer recurrence lstm 500 arg s shape int or tuple of in ts vector or tensor dimension of the output of this layer cell shape tuple default to none if give then the output state be first compute at cell shape and linearly project to shape activation class ~cntk ops function function default to fun c ~cntk ops tan h function to apply at the end e g relu use peepholes boo l default to false in it scalar or num py array or mod cnt k initializer default to gl or ot uniform initial value of weight w in it bias scalar or num py array or mod cnt k initializer default to 0 initial value of weight b enable self stabilization boo l default to false if true then add a fun c ~cntk layer block stabilizer to all state relate projections but not the data input name str default to the name of the function instance in the network return class ~cntk ops function function a function prev h prev c input h c that implement one step of a recurrent lstm layer activation get default override lstm activation activation use peepholes get default override lstm use peepholes use peepholes in it get default override lstm in it in it in it bias get default override lstm in it bias in it bias enable self stabilization get default override lstm enable self stabilization enable self stabilization return recurrent block string shape cell shape activation activation use peepholes use peepholes in it in it in it bias in it bias enable self stabilization enable self stabilization name name
def average pool filter shape shape of receptive field e g 3 3 stride number pad default override or false name string average pool filter shape stride 1 pad false name layer factory function to create an average pool layer like convolution average pool process items arrange on an n dimensional grid such as an image typically each item be a vector for each item average pool compute the element wise mean over a window receptive field of items surround the item s position on the grid the size spatial extent of the receptive field be give by filter shape e g for 2 d pool filter shape should be a tuple of two integers such as 5 5 example f average pool 3 3 stride 2 reduce dimensionality by 2 pool over windows of 3 x 3 h c input variable 32 240 320 e g 32 dim feature map hp f h hp shape spatial dimension have be halve due to stride and lose one due to 3 x 3 window without pad 32 119 159 f average pool 2 2 stride 2 f update signature 1 4 4 im np array 3 5 2 6 4 2 8 3 1 6 4 7 7 3 5 9 a 4 x 4 image feature map depth 1 for simplicity im array 3 5 2 6 4 2 8 3 1 6 4 7 7 3 5 9 f im due to stride 2 this compute the average of each 2 x 2 sub block array 3 5 4 75 4 25 6 25 d type float 32 arg s filter shape int or tuple of in ts shape spatial extent of the receptive field not include the input feature map depth e g 3 3 for a 2 d convolution stride int or tuple of in ts default to 1 stride increment when slide over the input use a tuple to specify a per axis value pad boo l or tuple of boo ls default to false if false then the pool operation will be shift over the valid area of input that be no value outside the area be use if pad true on the other hand pool will be apply to all input position and position outside the valid region will be exclude from the average use a tuple to specify a per axis value name str default to the name of the function instance in the network return cnt k ops function function a function that accept one argument and apply the average pool operation to it pad get default override average pool pad pad return pool pool type average filter shape stride stride pad pad op name string name name
def convolution 1 d filter shape shape of receptive field e g 3 num filter none e g 64 or none which mean 1 channel and don t add a dimension activation default override or identity in it default override or c gl or ot uniform pad default override or false stride number bias default override or true in it bias default override or number reduction rank number 0 mean input have no depth dimension e g audio signal or b w image dilation number name string convolution 1 d filter shape num filter none activation identity in it gl or ot uniform pad false stride 1 bias true in it bias 0 reduction rank 1 name layer factory function to create a 1 d convolution layer with optional non linearity same as convolution except that filter shape be verify to be 1 dimensional see convolution for extensive documentation arg s filter shape int or tuple of in ts shape spatial extent of the receptive field not include the input feature map depth e g 3 3 for a 2 d convolution num filter int default to none number of filter output feature map depth or to denote scalar output items output shape will have no depth axis activation class ~cntk ops function function default to identity optional function to apply at the end e g relu in it scalar or num py array or mod cnt k initializer default to fun c ~cntk initializer gl or ot uniform initial value of weight w pad boo l or tuple of boo ls default to false if false then the filter will be shift over the valid area of input that be no value outside the area be use if pad true on the other hand the filter will be apply to all input position and position outside the valid region will be consider contain zero use a tuple to specify a per axis value stride int or tuple of in ts default to 1 stride of the convolution increment when slide the filter over the input use a tuple to specify a per axis value bias boo l default to true the layer will have no bias if false be pass here in it bias scalar or num py array or mod cnt k initializer default to 0 initial value of weight b reduction rank int default to 1 set to 0 if input items be scalars input have no depth axis e g an audio signal or a black and white image that be store with tensor shape h w instead of 1 h w dilation tuple optional the dilation value along each axis default 1 mean no dilation name str default to the name of the function instance in the network return cnt k ops function function a function that accept one argument and apply the convolution operation to it activation get default override convolution 1 d activation activation in it get default override convolution 1 d in it in it pad get default override convolution 1 d pad pad bias get default override convolution 1 d bias bias in it bias get default override convolution 1 d in it bias in it bias if len as tuple filter shape number raise value error string return convolution filter shape num filter num filter activation activation in it in it pad pad sequential false stride stride share true bias bias in it bias in it bias reduction rank reduction rank dilation dilation op name string name name
def convolution 2 d filter shape shape of receptive field e g 3 3 must be a 2 element tuple num filter none e g 64 or none which mean 1 channel and don t add a dimension activation default override or identity in it default override or c gl or ot uniform pad default override or false stride number bias default override or true in it bias default override or number reduction rank number 0 mean input have no depth dimension e g audio signal or b w image dilation number group number name string convolution 2 d filter shape num filter none activation identity in it gl or ot uniform pad false stride 1 bias true in it bias 0 reduction rank 1 name layer factory function to create a 2 d convolution layer with optional non linearity same as convolution except that filter shape be verify to be 2 dimensional see convolution for extensive documentation arg s filter shape int or tuple of in ts shape spatial extent of the receptive field not include the input feature map depth e g 3 3 for a 2 d convolution num filter int default to none number of filter output feature map depth or to denote scalar output items output shape will have no depth axis activation class ~cntk ops function function default to identity optional function to apply at the end e g relu in it scalar or num py array or mod cnt k initializer default to fun c ~cntk initializer gl or ot uniform initial value of weight w pad boo l or tuple of boo ls default to false if false then the filter will be shift over the valid area of input that be no value outside the area be use if pad true on the other hand the filter will be apply to all input position and position outside the valid region will be consider contain zero use a tuple to specify a per axis value stride int or tuple of in ts default to 1 stride of the convolution increment when slide the filter over the input use a tuple to specify a per axis value bias boo l default to true the layer will have no bias if false be pass here in it bias scalar or num py array or mod cnt k initializer default to 0 initial value of weight b reduction rank int default to 1 set to 0 if input items be scalars input have no depth axis e g an audio signal or a black and white image that be store with tensor shape h w instead of 1 h w dilation tuple optional the dilation value along each axis default 1 mean no dilation group int default 1 number of group during convolution that control the connections between input and output channel deaf ul t value be 1 which mean that all input channel be conv ol ved to produce all output channel a value of n would mean that the input and output channel be divide into n group with the input channel in one group say i th input group contribute to output channel in only one group i th output group number of input and output channel must be div be ble by value of group argument also value of this argument must be strictly positive i e group 0 name str default to the name of the function instance in the network return cnt k ops function function a function that accept one argument and apply the convolution operation to it activation get default override convolution 2 d activation activation in it get default override convolution 2 d in it in it pad get default override convolution 2 d pad pad bias get default override convolution 2 d bias bias in it bias get default override convolution 2 d in it bias in it bias if len as tuple filter shape number raise value error string filter shape pad to shape number number filter shape string return convolution filter shape num filter num filter activation activation in it in it pad pad sequential false stride stride share true bias bias in it bias in it bias reduction rank reduction rank dilation dilation group group op name string name name
def convolution transpose 2 d filter shape a 2 d tuple e g 3 3 num filter activation default override or identity in it default override or c gl or ot uniform pad default override or false stride number bias default override or true in it bias default override or number output shape none reduction rank number 0 mean input have no depth dimension e g audio signal or b w image dilation number name string convolution transpose 2 d filter shape num filter activation identity in it gl or ot uniform pad false stride 1 bias true in it bias 0 output shape none name layer factory function to create a 2 d convolution transpose layer with optional non linearity same as convolution transpose except that filter shape be verify to be 2 dimensional see convolution transpose for extensive documentation activation get default override convolution transpose 2 d activation activation in it get default override convolution transpose 2 d in it in it pad get default override convolution transpose 2 d pad pad bias get default override convolution transpose 2 d bias bias in it bias get default override convolution transpose 2 d in it bias in it bias output shape get default override convolution transpose 2 d output shape output shape if len as tuple filter shape number raise value error string filter shape pad to shape number number filter shape string return convolution transpose filter shape num filter activation in it pad stride true bias in it bias output shape reduction rank reduction rank dilation dilation name name
def convolution 3 d filter shape shape of receptive field e g 3 3 3 must be a 3 element tuple num filter none e g 64 or none which mean 1 channel and don t add a dimension activation default override or identity in it default override or c gl or ot uniform pad default override or false stride number bias default override or true in it bias default override or number reduction rank number 0 mean input have no depth dimension e g audio signal or b w image dilation number group number name string convolution 3 d filter shape num filter none activation identity in it gl or ot uniform pad false stride 1 bias true in it bias 0 reduction rank 1 name layer factory function to create a 3 d convolution layer with optional non linearity same as convolution except that filter shape be verify to be 3 dimensional see convolution for extensive documentation arg s filter shape int or tuple of in ts shape spatial extent of the receptive field not include the input feature map depth e g 3 3 for a 2 d convolution num filter int default to none number of filter output feature map depth or to denote scalar output items output shape will have no depth axis activation class ~cntk ops function function default to identity optional function to apply at the end e g relu in it scalar or num py array or mod cnt k initializer default to fun c ~cntk initializer gl or ot uniform initial value of weight w pad boo l or tuple of boo ls default to false if false then the filter will be shift over the valid area of input that be no value outside the area be use if pad true on the other hand the filter will be apply to all input position and position outside the valid region will be consider contain zero use a tuple to specify a per axis value stride int or tuple of in ts default to 1 stride of the convolution increment when slide the filter over the input use a tuple to specify a per axis value bias boo l default to true the layer will have no bias if false be pass here in it bias scalar or num py array or mod cnt k initializer default to 0 initial value of weight b reduction rank int default to 1 set to 0 if input items be scalars input have no depth axis e g an audio signal or a black and white image that be store with tensor shape h w instead of 1 h w dilation tuple optional the dilation value along each axis default 1 mean no dilation group int default 1 number of group during convolution that control the connections between input and output channel deaf ul t value be 1 which mean that all input channel be conv ol ved to produce all output channel a value of n would mean that the input and output channel be divide into n group with the input channel in one group say i th input group contribute to output channel in only one group i th output group number of input and output channel must be div be ble by value of group argument also value of this argument must be strictly positive i e group 0 name str default to the name of the function instance in the network return cnt k ops function function a function that accept one argument and apply the convolution operation to it activation get default override convolution 3 d activation activation in it get default override convolution 3 d in it in it pad get default override convolution 3 d pad pad bias get default override convolution 3 d bias bias in it bias get default override convolution 3 d in it bias in it bias if len as tuple filter shape number raise value error string filter shape pad to shape number number number filter shape string return convolution filter shape num filter num filter activation activation in it in it pad pad sequential false stride stride share true bias bias in it bias in it bias reduction rank reduction rank dilation dilation group group op name string name name
def convolution transpose 3 d filter shape a 3 d tuple e g 3 3 3 num filter activation default override or identity in it default override or c gl or ot uniform pad default override or false stride number bias default override or true in it bias default override or number output shape none reduction rank number 0 mean input have no depth dimension e g audio signal or b w image dilation number name string convolution transpose 3 d filter shape num filter activation identity in it gl or ot uniform pad false stride 1 bias true in it bias 0 output shape none name layer factory function to create a 3 d convolution transpose layer with optional non linearity same as convolution transpose except that filter shape be verify to be 3 dimensional see convolution transpose for extensive documentation activation get default override convolution transpose 3 d activation activation in it get default override convolution transpose 3 d in it in it pad get default override convolution transpose 3 d pad pad bias get default override convolution transpose 3 d bias bias in it bias get default override convolution transpose 3 d in it bias in it bias output shape get default override convolution transpose 3 d output shape output shape if len as tuple filter shape number raise value error string filter shape pad to shape number number number filter shape string return convolution transpose filter shape num filter activation in it pad stride true bias in it bias output shape reduction rank reduction rank dilation dilation name name
def dense shape activation default override or identity in it default override or c gl or ot uniform input rank none map rank none bias default override or true in it bias default override or number name string dense shape activation identity in it gl or ot uniform input rank none map rank none bias true in it bias 0 name layer factory function to create an instance of a fully connect linear layer of the form activation input w b with weight w and bias b and activation and b be optional shape may describe a tensor as well a dense layer instance own its parameter tensors w and b and expose them as attribute w and b example f dense 5 activation c relu x c input variable 3 h f x h shape 5 f w shape 3 5 f b value array 0 0 0 0 0 d type float 32 activation through default options with c default options activation c relu f dense 500 the dense layer can be apply to input that be tensors not just vectors this be useful e g at the top of a image process cascade where after many convolutions with pad and stride it be difficult to know the precise dimension for this case cnt k have an extend definition of matrix product in which the input tensor will be treat as if it have be automatically flatten the weight matrix will be a tensor that reflect the flatten dimension in its ax example f dense 5 activation c soft max a 5 class class i fier x c input variable 64 16 16 e g an image reduce by a convolution stack y f x y shape 5 f w shape row dimension of matrix consist of 3 ax that match the input 64 16 16 5 this behavior can be modify by tell cnt k either the number of ax that should not be project map rank or the rank of the input input rank if neither be specify all input dimension be project as in the example above example f dense 5 activation c soft max input rank 2 a 5 class class i fier x c input variable 10 3 3 e g 10 parallel 3 x 3 object input have input rank 2 ax y f x y shape the 10 parallel object be classify separately the 10 dimension be retain 10 5 f w shape row dimension of matrix consist of 3 3 match the input ax to project 3 3 5 f dense 5 activation c soft max map rank 2 x c input variable 4 6 3 3 3 e g 24 parallel 3 x 3 x 3 object arrange in a 4 x 6 grid the grid be to be retain y f x y shape the 4 x 6 elements be classify separately the grid structure be retain 4 6 5 f w shape row dimension of matrix consist of 3 3 match the input ax to project 3 3 3 5 z y np zero x shape assert z shape 1 4 6 5 arg s shape int or tuple of in ts vector or tensor dimension of the output of this layer activation class ~cntk ops function function default to identity optional function to apply at the end e g relu in it scalar or num py array or mod cnt k initializer default to fun c ~cntk initializer gl or ot uniform initial value of weight w input rank int default to none number of infer ax to add to w map rank must not be give map rank int default to none expand w to leave exactly map rank ax input rank must not be give bias boo l optional default to true the layer will have no bias if false be pass here in it bias scalar or num py array or mod cnt k initializer default to 0 initial value of weight b name str default to the name of the function instance in the network return cnt k ops function function a function that accept one argument and apply the operation to it activation get default override dense activation activation in it get default override dense in it in it bias get default override dense bias bias in it bias get default override dense in it bias in it bias output shape as tuple shape if input rank be not none and map rank be not none raise value error string determine mean of ax w get dimension input shape shape where input shape be determine as by default equal to the dimension of the input pass to dense if input rank be give then the last input rank dimension of the input all others be not reduce over if map rank be give then the all but the first map rank dimension of the input those be not reduce over where input rank and map rank be mutually exclusive output rank len output shape support output with tensor layouts if input rank not give then pass a single infer map rank if give will determine the input rank the dimension inference may still create multiple ax input shape infer input rank if input rank be not none else number if input rank be not none infer input rank to map number mean map rank be not specify input rank rule el if map rank be none infer input rank to map number neither give default to infer w to use all input dim else infer input rank to map map rank infer w to use all input dim except the first static map rank ones parameters bind to this function in it weight initializer for in it record output rank output rank w parameter input shape output shape in it in it weight name string b parameter output shape in it in it bias name string if bias else none expression of this function decorator def dense x r time x w output rank output rank infer input rank to map infer input rank to map if b r r b if activation be not none r activation r return r return dense
def dropout dropout rate none keep pro b none seed sentinel value for auto select random seed name string layer factory function to create a drop out layer the dropout rate can be specify as the probability of drop a value dropout rate e g dropout 0 3 mean drop 30 of the activation value alternatively it can also be specify as the probability of keep a value keep pro b the dropout operation be only apply during train during test this be a no op to make sure that this lead to correct result the dropout operation in train multiply the result by 1/ 1 dropout rate example f dropout 0 2 drop 20 of activation s h c input variable 3 hd f h f dropout keep pro b 0 8 keep 80 h c input variable 3 hd f h arg s dropout rate float probability of drop out an element mutually exclusive with keep pro b keep pro b float probability of keep an element mutually exclusive with dropout rate seed int random seed name str default to the name of the function instance in the network return cnt k ops function function a function that accept one argument and apply the operation to it if dropout rate be none and keep pro b be none raise value error string el if dropout rate be not none and keep pro b be not none raise value error string el if keep pro b be not none if keep pro b number or keep pro b number raise value error string dropout rate number keep pro b decorator def dropout f x return dropout x dropout rate dropout rate seed seed return dropout f
decorator def dropout x dropout rate number seed sentinel value for auto select random seed name string each element of the input be independently set to 0 with probability dropout rate or to 1 / 1 dropout rate time its original value with probability 1 dropout rate dropout be a good way to reduce over fit this behavior only happen during train during inference dropout be a no op in the paper that introduce dropout it be suggest to scale the weight during inference in cnt k s implementation because the value that be not set to 0 be multiply with 1 / 1 dropout rate this be not necessary example data 10 20 30 40 50 60 c dropout data 0 5 e val doc test skip array 0 40 0 80 0 0 d type float 32 c dropout data 0 75 e val doc test skip array 0 0 0 160 0 240 d type float 32 arg s x input tensor dropout rate float 0 1 probability that an element of x will be set to zero seed int random seed name class str optional the name of the function instance in the network return class ~cntk ops function function if dropout rate number or dropout rate number raise value error string from cnt k cnt k py import dropout x sanitize input x return dropout x dropout rate seed name
def max pool filter shape shape of receptive field e g 3 3 stride number pad default override or false name string max pool filter shape stride 1 pad false name layer factory function to create a max pool layer like convolution max pool process items arrange on an n dimensional grid such as an image typically each item be a vector for each item max pool compute the element wise maximum over a window receptive field of items surround the item s position on the grid the size spatial extent of the receptive field be give by filter shape e g for 2 d pool filter shape should be a tuple of two integers such as 5 5 example f max pool 3 3 stride 2 reduce dimensionality by 2 pool over windows of 3 x 3 h c input variable 32 240 320 e g 32 dim feature map hp f h hp shape spatial dimension have be halve due to stride and lose one due to 3 x 3 window without pad 32 119 159 f max pool 2 2 stride 2 f update signature 1 4 4 im np array 3 5 2 6 4 2 8 3 1 6 4 7 7 3 5 9 a 4 x 4 image feature map depth 1 for simplicity im array 3 5 2 6 4 2 8 3 1 6 4 7 7 3 5 9 f im due to stride 2 this pick the max out of each 2 x 2 sub block array 5 8 7 9 d type float 32 arg s filter shape int or tuple of in ts shape spatial extent of the receptive field not include the input feature map depth e g 3 3 for a 2 d convolution stride int or tuple of in ts default to 1 stride increment when slide over the input use a tuple to specify a per axis value pad boo l or tuple of boo ls default to false if false then the pool operation will be shift over the valid area of input that be no value outside the area be use if pad true on the other hand pool will be apply to all input position and position outside the valid region will be consider contain zero use a tuple to specify a per axis value name str default to the name of the function instance in the network return cnt k ops function function a function that accept one argument and apply the max pool operation to it pad get default override max pool pad pad return pool pool type max filter shape stride stride pad pad op name string name name
decorator def square error output target name string this operation compute the sum of the square difference between elements in the two input matrices the result be a scalar i e one by one matrix this be often use as a train criterion example i 1 c input variable 1 2 i 2 c input variable 1 2 c square error i 1 i 2 e val i 1 np as array 2 1 d type np float 32 i 2 np as array 4 6 d type np float 32 array 29 d type float 32 c square error i 1 i 2 e val i 1 np as array 1 2 d type np float 32 i 2 np as array 1 2 d type np float 32 array 0 d type float 32 arg s output the output value from the network target it be usually a one hot vector where the hot bite correspond to the label index name str optional the name of the function instance in the network return class ~cntk ops function function from cnt k cnt k py import square error d type get data type output target output sanitize input output d type target sanitize input target d type return square error output target name
decorator def log soft max x axis none name string compute the log soft max normalize value of x that be y x log reduce sum exp x axis the implementation use an equivalent formula for numerical stability it be also possible to use x reduce log sum exp x axis instead of log soft max this can be faster one reduce pass instead of two but can behave slightly differently numerically arg s x num py array or any class ~cntk ops function function that output a tensor axis int axis along which the log soft max operation will be perform the default be the last axis name str optional the name of the function instance in the network return class ~cntk ops function function from cnt k cnt k py import log soft max x sanitize input x if axis be not none axis sanitize axis axis return log soft max x axis name else return log soft max x name
decorator def soft max x axis none name string r compute the gradient of math f z \log\sum i\exp z i at z x concretely math \mathrm soft max x \left \frac \exp x 1 \sum i\exp x i \quad\frac \exp x 1 \sum i\exp x i \quad\ldots\quad\frac \exp x 1 \sum i\exp x i \right with the understand that the implementation can use equivalent formulas for efficiency and numerical stability the output be a vector of non negative number that sum to 1 and can therefore be interpret as probabilities for mutually exclusive outcomes as in the case of multi class classification if axis be give as integer then the soft max will be compute along that axis if the provide axis be 1 it will be compute along the last axis otherwise soft max will be apply to all ax example c soft max 1 1 2 3 e val array 0 082595 0 082595 0 224515 0 610296 d type float 32 c soft max 1 1 e val array 0 5 0 5 d type float 32 c soft max 1 1 3 5 axis 1 e val array 0 5 0 5 0 119203 0 880797 d type float 32 c soft max 1 1 3 5 axis 1 e val array 0 119203 0 017986 0 880797 0 982014 d type float 32 arg s x num py array or any class ~cntk ops function function that output a tensor axis int or class ~cntk axis axis axis along which the soft max operation will be perform name str optional the name of the function instance in the network return class ~cntk ops function function from cnt k cnt k py import soft max x sanitize input x if axis be not none axis sanitize axis axis return soft max x axis name else return soft max x name
def max pool filter shape shape of receptive field e g 3 3 stride number pad default override or false name string max pool filter shape stride 1 pad false name layer factory function to create a max pool layer like convolution max pool process items arrange on an n dimensional grid such as an image typically each item be a vector for each item max pool compute the element wise maximum over a window receptive field of items surround the item s position on the grid the size spatial extent of the receptive field be give by filter shape e g for 2 d pool filter shape should be a tuple of two integers such as 5 5 example f max pool 3 3 stride 2 reduce dimensionality by 2 pool over windows of 3 x 3 h c input variable 32 240 320 e g 32 dim feature map hp f h hp shape spatial dimension have be halve due to stride and lose one due to 3 x 3 window without pad 32 119 159 f max pool 2 2 stride 2 f update signature 1 4 4 im np array 3 5 2 6 4 2 8 3 1 6 4 7 7 3 5 9 a 4 x 4 image feature map depth 1 for simplicity im array 3 5 2 6 4 2 8 3 1 6 4 7 7 3 5 9 f im due to stride 2 this pick the max out of each 2 x 2 sub block array 5 8 7 9 d type float 32 arg s filter shape int or tuple of in ts shape spatial extent of the receptive field not include the input feature map depth e g 3 3 for a 2 d convolution stride int or tuple of in ts default to 1 stride increment when slide over the input use a tuple to specify a per axis value pad boo l or tuple of boo ls default to false if false then the pool operation will be shift over the valid area of input that be no value outside the area be use if pad true on the other hand pool will be apply to all input position and position outside the valid region will be consider contain zero use a tuple to specify a per axis value name str default to the name of the function instance in the network return cnt k ops function function a function that accept one argument and apply the max pool operation to it pad get default override max pool pad pad return pool pool type max filter shape stride stride pad pad op name string name name
decorator def relu x name string rectify linear operation compute the element wise rectify linear of x max x 0 the output tensor have the same shape as x example c relu 1 0 5 0 1 2 e val array 0 0 0 1 2 d type float 32 arg s x num py array or any class ~cntk ops function function that output a tensor name str optional the name of the function instance in the network return class ~cntk ops function function from cnt k cnt k py import re lu x sanitize input x return re lu x name
decorator def pad x pattern mode constant pad constant value number name string pad a tensor accord to the specify pattern three pad modes be support constant / reflect / symmetric example data np a range 6 d type np float 32 reshape 2 3 x c constant value data c pad x pattern 1 1 2 2 mode c ops constant pad constant value 1 e val array 1 1 1 1 1 1 1 1 1 0 1 2 1 1 1 1 3 4 5 1 1 1 1 1 1 1 1 1 d type float 32 c pad x pattern 1 1 2 2 mode c ops reflect pad e val array 5 4 3 4 5 4 3 2 1 0 1 2 1 0 5 4 3 4 5 4 3 2 1 0 1 2 1 0 d type float 32 c pad x pattern 1 1 2 2 mode c ops symmetric pad e val array 1 0 0 1 2 2 1 1 0 0 1 2 2 1 4 3 3 4 5 5 4 4 3 3 4 5 5 4 d type float 32 arg s x tensor to be pad pattern list of tuple with 2 integers how many value to add before and after the content of the tensor in each dimension mode int pad mode c ops constant pad c ops reflect pad and c ops symmetric pad constant value the value use to fill the pad cells only meaningful under constant mode name str optional the name of the function instance in the network return class ~cntk ops function function from cnt k cnt k py import pad if any len number for in pattern raise value error string x sanitize input x head p number for p in reverse pattern foot p number for p in reverse pattern return pad x mode head foot constant value name
decorator def load model model device none format model format cnt kv 2 alias for fun c ~cntk ops function function load return function load model device format
decorator def space to depth operand block size name string rearrange elements in the input tensor from the spatial dimension to the depth dimension this be the reverse transformation of depth to space this operation be useful for implement and test sub pixel convolution that be part of model for image super resolution see 1 it rearrange elements of an input tensor of shape c h w to a tensor of shape c b b h/b w/b where b be the block size by rearrange non overlap spatial block of size block size x block size into the depth/channel dimension at each location example np random seed 3 x np random run dint low 0 high 100 size 1 4 6 as type np float 32 a c input variable 1 4 6 s 2 d op c space to depth a block size 2 s 2 d op e val a x array 24 56 0 96 44 39 blank line 3 72 21 20 93 14 blank line 19 41 21 26 90 66 blank line 74 10 38 81 22 2 d type float 32 arg s operand input tensor with dimension math c \\times h \\times w block size int integer value this define the size of the spatial block whose elements be move to the depth dimension size of spatial dimension h w in the input tensor must be divisible by math block size name str optional the name of the function instance in the network return class ~cntk ops function function see also 1 w shi et al real time single image and video super resolution use an efficient sub pixel convolutional neural network https //arxiv org/abs/1609 05158 from cnt k cnt k py import space to depth operand sanitize input operand get data type operand if not float block size be integer raise value error string return space to depth operand block size name
decorator def ada delta parameters lr learn parameter schedule per sample number rho number epsilon number l 1 regular iz ation weight number l 2 regular iz ation weight number gaussian noise injection std dev number gradient clip threshold per sample np inf gradient clip with truncation true use mean gradient none mini batch size none epoch size none ada delta parameters lr rho epsilon l 1 regular iz ation weight 0 l 2 regular iz ation weight 0 gaussian noise injection std dev 0 gradient clip threshold per sample np inf gradient clip with truncation true create an ada delta learner instance to learn the parameters see 1 for more information arg s parameters list of parameters list of network parameters to tune these can be obtain by the root operator s parameters lr float list output of fun c learn parameter schedule a learn rate in float or a learn rate schedule see also fun c learn parameter schedule rho float exponential smooth factor for each mini batch epsilon float epsilon for sqr t l 1 regular iz ation weight float optional the l 1 regular iz ation weight per sample default to 0 0 l 2 regular iz ation weight float optional the l 2 regular iz ation weight per sample default to 0 0 gaussian noise injection std dev float optional the standard deviation of the gaussian noise add to parameters post update default to 0 0 gradient clip threshold per sample float optional clip threshold per sample default to infinity gradient clip with truncation boo l default true use gradient clip with truncation use mean gradient boo l optional use average gradient as input to learner deprecate 2 2 use mini batch size parameter to specify the reference mini batch size mini batch size int default none the mini batch size that the learner s parameters be design or pre tune for this size be usually set to the same as the mini batch data source s size cnt k will perform automatic scale of the parameters to enable efficient model parameter update implementation while approximate the behavior of pre design and pre tune parameters in case that mini batch size be not specify cnt k will inherit the mini batch size from the learn rate schedule if the learn rate schedule do not specify the mini batch size cnt k will set it to at tr ignore set mini batch size to at tr ignore will have the learner apply as it be prevent cnt k perform any hyper parameter scale see also fun c learn parameter schedule epoch size optional int number of sample as a schedule unit for learn rate see also fun c learn parameter schedule return class ~cntk learners learner learner instance that can be pass to the class ~cntk train trainer trainer see also 1 matthew d ze i ler ada delta an adaptive learn rate method https //arxiv org/pdf/1212 5701 pdf gaussian noise injection std dev \ train parameter schedule gaussian noise injection std dev lr mini batch size infer learn rate schedule and ref mini batch size use mean gradient mini batch size lr epoch size additional options cnt k py additional learn options additional options l 1 regular iz ation weight l 1 regular iz ation weight additional options l 2 regular iz ation weight l 2 regular iz ation weight additional options gaussian noise injection std dev gaussian noise injection std dev additional options gradient clip threshold per sample gradient clip threshold per sample additional options gradient clip with truncation gradient clip with truncation mini batch size infer ref mini batch size from legacy use mean gradient mini batch size use mean gradient if mini batch size be not none additional options dic t options cnt k py learner mini batch size cnt k py size t wrapper mini batch size need this to make proper type dictionary value opt cnt k py ada delta learner parameters lr rho epsilon additional options opt be mini batch size explicitly specify mini batch size be not none return opt
decorator def ada grad parameters lr need ave multiplier true l 1 regular iz ation weight number l 2 regular iz ation weight number gaussian noise injection std dev number gradient clip threshold per sample np inf gradient clip with truncation true use mean gradient none mini batch size none epoch size none ada grad parameters lr need ave multiplier true l 1 regular iz ation weight 0 l 2 regular iz ation weight 0 gaussian noise injection std dev 0 gradient clip threshold per sample np inf gradient clip with truncation true create an ada grad learner instance to learn the parameters see 1 for more information arg s parameters list of parameters list of network parameters to tune these can be obtain by the root operator s parameters lr float list output of fun c learn parameter schedule a learn rate in float or a learn rate schedule see also fun c learn parameter schedule need ave multiplier boo l default l 1 regular iz ation weight float optional the l 1 regular iz ation weight per sample default to 0 0 l 2 regular iz ation weight float optional the l 2 regular iz ation weight per sample default to 0 0 gaussian noise injection std dev float optional the standard deviation of the gaussian noise add to parameters post update default to 0 0 gradient clip threshold per sample float optional clip threshold per sample default to infinity gradient clip with truncation boo l default true use gradient clip with truncation use mean gradient boo l optional use average gradient as input to learner deprecate 2 2 use mini batch size parameter to specify the reference mini batch size mini batch size int default none the mini batch size that the learner s parameters be design or pre tune for this size be usually set to the same as the mini batch data source s size cnt k will perform automatic scale of the parameters to enable efficient model parameter update implementation while approximate the behavior of pre design and pre tune parameters in case that mini batch size be not specify cnt k will inherit the mini batch size from the learn rate schedule if the learn rate schedule do not specify the mini batch size cnt k will set it to at tr ignore set mini batch size to at tr ignore will have the learner apply as it be prevent cnt k perform any hyper parameter scale see also fun c learn parameter schedule epoch size optional int number of sample as a schedule unit for learn rate see also fun c learn parameter schedule return class ~cntk learners learner learner instance that can be pass to the class ~cntk train trainer trainer see also 1 j duch i e ha zan and y singer adaptive sub gradient methods for online learn and stochastic optimization http //www magic broom info/papers/duchihasi10 pdf the journal of machine learn research 2011 lr mini batch size infer learn rate schedule and ref mini batch size use mean gradient mini batch size lr epoch size gaussian noise injection std dev \ train parameter schedule gaussian noise injection std dev additional options cnt k py additional learn options additional options l 1 regular iz ation weight l 1 regular iz ation weight additional options l 2 regular iz ation weight l 2 regular iz ation weight additional options gaussian noise injection std dev gaussian noise injection std dev additional options gradient clip threshold per sample gradient clip threshold per sample additional options gradient clip with truncation gradient clip with truncation mini batch size infer ref mini batch size from legacy use mean gradient mini batch size use mean gradient if mini batch size be not none additional options dic t options cnt k py learner mini batch size cnt k py size t wrapper mini batch size need this to make proper type dictionary value opt cnt k py ada grad learner parameters lr need ave multiplier additional options opt be mini batch size explicitly specify mini batch size be not none return opt
decorator def adam parameters lr momentum unit gain default unit gain value variance momentum momentum schedule per sample number l 1 regular iz ation weight number l 2 regular iz ation weight number gaussian noise injection std dev number gradient clip threshold per sample np inf gradient clip with truncation true use mean gradient none epsilon number adama x false mini batch size none epoch size none adam parameters lr momentum unit gain default unit gain value variance momentum momentum schedule per sample 0 9999986111120757 l 1 regular iz ation weight 0 l 2 regular iz ation weight 0 gaussian noise injection std dev 0 gradient clip threshold per sample np inf gradient clip with truncation true epsilon 1 e 8 adama x false create an adam learner instance to learn the parameters see 1 for more information arg s parameters list of parameters list of network parameters to tune these can be obtain by the root operator s parameters lr float list output of fun c learn parameter schedule a learn rate in float or a learn rate schedule see also fun c learn parameter schedule momentum float list output of fun c momentum schedule momentum schedule note that this be the beta 1 parameter in the adam paper 1 for additional information please refer to the cnt k wiki this cnt k wiki article brain script sgd block convert learn rate and momentum parameters from other toolkits unit gain when true momentum be interpret as a unit gain filter default to the value return by fun c default unit gain value variance momentum float list output of fun c momentum schedule variance momentum schedule note that this be the beta 2 parameter in the adam paper 1 default to momentum schedule per sample 0 9999986111120757 l 1 regular iz ation weight float optional the l 1 regular iz ation weight per sample default to 0 0 l 2 regular iz ation weight float optional the l 2 regular iz ation weight per sample default to 0 0 gaussian noise injection std dev float optional the standard deviation of the gaussian noise add to parameters post update default to 0 0 gradient clip threshold per sample float optional clip threshold per sample default to infinity gradient clip with truncation boo l default true use gradient clip with truncation use mean gradient boo l optional use average gradient as input to learner deprecate 2 2 use mini batch size parameter to specify the reference mini batch size epsilon float optional numerical stability constant default to 1 e 8 adama x when true use infinity norm variance momentum update instead of l 2 default to false mini batch size int default none the mini batch size that the learner s parameters be design or pre tune for this size be usually set to the same as the mini batch data source s size cnt k will perform automatic scale of the parameters to enable efficient model parameter update implementation while approximate the behavior of pre design and pre tune parameters in case that mini batch size be not specify cnt k will inherit the mini batch size from the learn rate schedule if the learn rate schedule do not specify the mini batch size cnt k will set it to at tr ignore set mini batch size to at tr ignore will have the learner apply as it be prevent cnt k perform any hyper parameter scale see also fun c learn parameter schedule epoch size optional int number of sample as a schedule unit for learn rate momentum and variance momentum see also fun c learn parameter schedule return class ~cntk learners learner learner instance that can be pass to the class ~cntk train trainer trainer see also 1 d king ma j ba adam a method for stochastic optimization https //arxiv org/abs/1412 6980 international conference for learn representations 2015 lr mini batch size infer learn rate schedule and ref mini batch size use mean gradient mini batch size lr epoch size momentum infer learn parameter schedule momentum mini batch size epoch size verify momentum type momentum variance momentum infer learn parameter schedule variance momentum mini batch size epoch size verify momentum type variance momentum gaussian noise injection std dev \ train parameter schedule gaussian noise injection std dev additional options cnt k py additional learn options additional options l 1 regular iz ation weight l 1 regular iz ation weight additional options l 2 regular iz ation weight l 2 regular iz ation weight additional options gaussian noise injection std dev gaussian noise injection std dev additional options gradient clip threshold per sample gradient clip threshold per sample additional options gradient clip with truncation gradient clip with truncation if mini batch size be not none additional options dic t options cnt k py learner mini batch size cnt k py size t wrapper mini batch size need this to make proper type dictionary value opt cnt k py adam learner parameters lr momentum unit gain variance momentum epsilon adama x additional options opt be mini batch size explicitly specify mini batch size be not none return opt
decorator def rms prop parameters lr gamma inc dec max min need ave multiplier true l 1 regular iz ation weight number l 2 regular iz ation weight number gaussian noise injection std dev number gradient clip threshold per sample np inf gradient clip with truncation true use mean gradient none mini batch size none epoch size none rms prop parameters lr gamma inc dec max min need ave multiplier true l 1 regular iz ation weight 0 l 2 regular iz ation weight 0 gaussian noise injection std dev 0 gradient clip threshold per sample np inf gradient clip with truncation true create an rms prop learner instance to learn the parameters arg s parameters list of parameters list of network parameters to tune these can be obtain by the root operator s parameters lr float list output of fun c learn parameter schedule a learn rate in float or a learn rate schedule see also fun c learn parameter schedule gamma float trade off factor for current and previous gradients common value be 0 95 should be in range 0 0 1 0 inc float increase factor when try to adjust current learn rate should be greater than 1 dec float decrease factor when try to adjust current learn rate should be in range 0 0 1 0 max float maximum scale allow for the initial learn rate should be greater than zero and min min float minimum scale allow for the initial learn rate should be greater than zero need ave multiplier boo l default true l 1 regular iz ation weight float optional the l 1 regular iz ation weight per sample default to 0 0 l 2 regular iz ation weight float optional the l 2 regular iz ation weight per sample default to 0 0 gaussian noise injection std dev float optional the standard deviation of the gaussian noise add to parameters post update default to 0 0 gradient clip threshold per sample float optional clip threshold per sample default to infinity gradient clip with truncation boo l default true use gradient clip with truncation use mean gradient boo l optional use average gradient as input to learner deprecate 2 2 use mini batch size parameter to specify the reference mini batch size mini batch size int default none the mini batch size that the learner s parameters be design or pre tune for this size be usually set to the same as the mini batch data source s size cnt k will perform automatic scale of the parameters to enable efficient model parameter update implementation while approximate the behavior of pre design and pre tune parameters in case that mini batch size be not specify cnt k will inherit the mini batch size from the learn rate schedule if the learn rate schedule do not specify the mini batch size cnt k will set it to at tr ignore set mini batch size to at tr ignore will have the learner apply as it be prevent cnt k perform any hyper parameter scale see also fun c learn parameter schedule epoch size optional int number of sample as a schedule unit for learn rate see also fun c learn parameter schedule return class ~cntk learners learner learner instance that can be pass to the class ~cntk train trainer trainer lr mini batch size infer learn rate schedule and ref mini batch size use mean gradient mini batch size lr epoch size gaussian noise injection std dev \ train parameter schedule gaussian noise injection std dev additional options cnt k py additional learn options additional options l 1 regular iz ation weight l 1 regular iz ation weight additional options l 2 regular iz ation weight l 2 regular iz ation weight additional options gaussian noise injection std dev gaussian noise injection std dev additional options gradient clip threshold per sample gradient clip threshold per sample additional options gradient clip with truncation gradient clip with truncation mini batch size infer ref mini batch size from legacy use mean gradient mini batch size use mean gradient if mini batch size be not none additional options dic t options cnt k py learner mini batch size cnt k py size t wrapper mini batch size need this to make proper type dictionary value opt cnt k py rms prop learner parameters lr gamma inc dec max min need ave multiplier additional options opt be mini batch size explicitly specify mini batch size be not none return opt
decorator def grad self at w rt none output none device none as num py true grad root none compute the gradient of this function at location at with respect to w rt the function must have a single output example x c input variable shape 1 need gradient true y c sqr t x a np as array 1 4 16 d type np float 32 reshape 3 1 y grad x a array 0 5 blank line 0 25 blank line 0 125 d type float 32 arg s at dic t map of the function s arguments to value w rt list default none list of variables with respect to which the gradient will be compute if omit the gradients with respect to all arguments of this function that need gradient will be compute output it er able optional output include intermediate output in the graph to fetch value for if not specify value for none of the output be fetch device class ~cntk device device descriptor default none the device descriptor that contain the type and id of the device on which the computation be perform if none the default device be use as num py boo l default true whether to return the gradients as a num py array default true specify this as false return a cnt k value which avoid a costly conversion but return a somewhat opaque object also the value object be temporary and only guarantee to be valid until the next forward/eval/backward/grad call you must explicitly clone the tempo ray value object if they need to be access later grad root class ~cntk variables variable optional specify the root of gradients calculation if not specify the output of this function will be use as gradient root return dic t or num py array or a tuple of these dic t with key of w rt variables and gradient value of w rt variables a single num py array if there be only one gradient value if output be specify to fetch value for this method return a tuple where the 2 nd element of the tuple be the output value a dic t with key of specify output variables and value of compute output or a single num py array if there be only one output value each element have the same shape as the w rt or output variables include dynamic ax such as the batch axis if device be none device device descriptor use default device in var map sanitize var map self arguments at none device if output be none output if w rt be none w rt arg for arg in self arguments if arg need gradient if len w rt number raise value error string str self output map v none for v in output w rt map v none for v in w rt if grad root be none super function self gradients in var map w rt map output map device else super function self gradients in var map grad root w rt map output map device if as num py for k in output map output map k value as sequence or array output map k k for k in w rt map w rt map k value as sequence or array w rt map k k if len output map number return sanitize variable value dic t w rt map else return sanitize variable value dic t w rt map sanitize variable value dic t output map
decorator def elu x alpha number name string exponential linear unit operation compute the element wise exponential linear of x max x 0 for x 0 and x alpha exp x 1 otherwise the output tensor have the same shape as x example c elu 1 0 5 0 1 2 e val array 0 632121 0 393469 0 1 2 d type float 32 arg s x num py array or class ~cntk ops function function any class ~cntk ops function function that output a tensor name str default to the name of the function instance in the network return cnt k ops function function an instance of class ~cntk ops function function from cnt k cnt k py import elu x sanitize input x return elu x alpha name
decorator def hard s igm oid x alpha beta name string compute the element wise hard s igm oid function y max 0 min 1 alpha x beta example alpha 1 beta 2 c hard s igm oid 2 5 1 5 1 alpha beta e val array 0 0 5 1 d type float 32 arg s x num py array or any class ~cntk ops function function that output a tensor alpha float the alpha term of the above equation beta float the beta term of the above equation name str the name of the function instance in the network return class ~cntk ops function function from cnt k cnt k py import hard s igm oid x sanitize input x return hard s igm oid x alpha beta name
decorator def s elu x scale number alpha number name string scale exponential linear unit operation compute the element wise exponential linear of x scale x for x 0 and x scale alpha exp x 1 otherwise the output tensor have the same shape as x example c s elu 1 0 5 0 1 2 e val array 1 111331 0 691758 0 1 050701 2 101402 d type float 32 arg s x num py array or class ~cntk ops function function any class ~cntk ops function function that output a tensor name str default to the name of the function instance in the network return cnt k ops function function an instance of class ~cntk ops function function from cnt k cnt k py import s elu x sanitize input x return s elu x scale alpha name
decorator def s igm oid x name string compute the element wise s igm oid of x math s igm oid x 1 \\over 1 \\exp x the output tensor have the same shape as x example c s igm oid 2 1 0 1 2 e val array 0 119203 0 268941 0 5 0 731059 0 880797 d type float 32 arg s x num py array or any class ~cntk ops function function that output a tensor name str optional the name of the function instance in the network return class ~cntk ops function function from cnt k cnt k py import s igm oid x sanitize input x return s igm oid x name
decorator def soft plus x steepness number name string soft plus operation compute the element wise soft plus of x math \\mathrm soft plus x \\log 1 \\exp x the optional steepness allow to make the knee sharper steepness 1 or softer by compute soft plus x steepness / steepness for very large steepness this approach a linear rectifier the output tensor have the same shape as x example c soft plus 1 0 5 0 1 2 e val array 0 313262 0 474077 0 693147 1 313262 2 126928 d type float 32 c soft plus 1 0 5 0 1 2 steepness 4 e val array 0 004537 0 031732 0 173287 1 004537 2 000084 d type float 32 arg s x num py array or class ~cntk ops function function any class ~cntk ops function function that output a tensor steepness float optional optional steepness factor name str default to the name of the function instance in the network return cnt k ops function function an instance of class ~cntk ops function function from cnt k cnt k py import soft plus x sanitize input x if steepness number return soft plus x name xp placeholder f type map soft plus steepness xp / steepness return as block f xp x string name
decorator def soft sign x steepness number name string compute the element wise soft sign of x math s igm oid x x \\over 1 \\abs x the output tensor have the same shape as x example c soft sign 1 0 1 e val array 0 5 0 0 5 d type float 32 arg s x num py array or any class ~cntk ops function function that output a tensor name str optional the name of the function instance in the network return class ~cntk ops function function from cnt k cnt k py import soft sign x sanitize input x return soft sign x name
decorator def tan h x name string compute the element wise tan h of x the output tensor have the same shape as x example c tan h 1 2 3 4 e val array 0 761594 0 964028 0 995055 0 999329 d type float 32 arg s x num py array or any class ~cntk ops function function that output a tensor name str optional the name of the function instance in the network return class ~cntk ops function function from cnt k cnt k py import tan h x sanitize input x return tan h x name
decorator decorator def element time leave right name string the output of this operation be the element wise product of the two or more input tensors it support broadcast example c element time 1 1 1 1 0 5 0 25 0 125 0 e val array 0 5 0 25 0 125 0 d type float 32 c element time 5 10 15 30 2 e val array 10 20 30 60 d type float 32 c element time 5 10 15 30 2 1 2 1 2 e val array 10 40 30 120 d type float 32 arg s arg 1 leave side tensor arg 2 right side tensor more arg s additional input name str optional the name of the function instance in the network return class ~cntk ops function function from cnt k cnt k py import element time as cnt k py element time d type get data type leave right leave sanitize input leave d type right sanitize input right d type return cnt k py element time leave right name
def embed shape none in it default override or c gl or ot uniform weight none name string embed shape none in it gl or ot uniform weight none name layer factory function to create a embed layer an embed be conceptually a lookup table for every input token e g a word or any category label the correspond entry in the lookup table be return in cnt k discrete items such as word be represent as one hot vectors the table lookup be realize as a matrix product with a matrix whose row be the embed vectors note that multiply a matrix from the leave with a one hot vector be the same as copy out the row for which the input vector be 1 cnt k have special optimization s to make this operation as efficient as an actual table lookup if the input be sparse the lookup table in this layer be learn able unless a user specify one be supply through the weight parameter for example to use an exist embed table from a file in num py format use this embed weight np load path n py to initialize a learn able lookup table with a give num py array that be to be use as the initial value pass that array to the in it parameter not weight an embed instance own its weight parameter tensor e and expose it as an attribute e example learn able embed f embed 5 x c input variable 3 e f x e shape 5 f e shape 3 5 user supply embed f embed weight 5 3 1 4 2 7 6 3 2 9 f e value array 0 5 0 3 0 1 0 4 0 2 0 7 0 6 0 3 0 2 0 9 d type float 32 x c input variable 2 be sparse true e f x e shape 5 e c value one hot 1 0 0 1 num class 2 array 0 7 0 6 0 3 0 2 0 9 0 5 0 3 0 1 0 4 0 2 0 5 0 3 0 1 0 4 0 2 0 7 0 6 0 3 0 2 0 9 d type float 32 arg s shape int or tuple of in ts vector or tensor dimension of the output of this layer in it scalar or num py array or mod cnt k initializer default to fun c ~cntk initializer gl or ot uniform learn able embed only initial value of weight e weight num py array mutually exclusive with in it de fuat s to none user supply embed only the lookup table the matrix row be the embed vectors weight i be the embed that correspond to input category i name str default to the name of the function instance in the network return cnt k ops function function a function that accept one argument and apply the embed operation to it if not be default override in it and weight be not none raise value error string parameters bind to this function no weight give learn the embed if weight be none if shape be none raise value error string in it get default override embed in it in it shape as tuple shape weight shape infer shape e parameter weight shape in it in it name string weight give use them as constant else import num py as np weight np array weight weight shape np shape weight if shape be not none user may give shape then it must match raise value error string e constant weight name string expression decorator def embed x return time x e return embed
decorator def flatten x axis none name string flatten the input tensor into a 2 d matrix if the input tensor have shape d 0 d 1 d n then the output will have shape d 0 x d 1 d axis 1 d axis x d axis 1 x dn example create 2 x 3 x 4 matrix flatten the matrix at axis 1 shape 2 3 4 data np reshape np a range np prod shape d type np float 32 shape c flatten data 1 e val array 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 d type float 32 arg s x input tensor axis int default to 0 indicate up to which input dimension exclusive should be flatten to the outer dimension of the output name str optional the name of the function instance in the network return class ~cntk ops function function from cnt k cnt k py import flatten x sanitize input x if axis be not none axis sanitize axis axis return flatten x axis name else return flatten x name
def layer normalization initial scale number initial bias number epsilon default override or number name string layer normalization initial scale 1 initial bias 0 epsilon 0 00001 name layer factory function to create a function that implement layer normalization layer normalization apply this formula to every input element element wise y x mean x / std dev x epsilon scale bias where scale and bias be learn parameters of the same di mention as the input/output example f layer normalization initial scale 2 initial bias 1 f update signature 4 f np array 4 0 0 4 result have mean 1 and standard deviation 2 reflect the initial value for scale and bias array 2 99999 0 99999 0 99999 2 99999 d type float 32 arg s initial scale float default 1 initial value for the scale parameter initial bias float default 0 initial value for the bias parameter epsilon float default 0 00001 epsilon add to the standard deviation to avoid division by 0 name str optional the name of the function instance in the network return cnt k ops function function a function that accept one argument and apply the operation to it to do add paper reference epsilon get default override layer normalization epsilon epsilon d type get default override none d type default override or np float 32 parameters bind to this function scale parameter infer in it initial scale name string to do if this get usage then offer a soft plus version like stabilizer for stability? bias parameter infer in it initial bias name string cast to specify data type the default for number be float 32 which might be different than desire epsilon np as array epsilon d type d type expression decorator def layer normalize x mean reduce mean x normalize w r t actual sample statistics x 0 x mean std sqr t reduce mean x 0 x 0 if epsilon number std epsilon x hat x 0 / std return x hat scale bias de normalize with learn parameters return layer normalize
decorator def leaky relu x alpha number name string leaky rectify linear operation compute the element wise leaky rectify linear of x max x 0 for x 0 and x alpha x otherwise the output tensor have the same shape as x example c leaky relu 1 0 5 0 1 2 e val array 0 01 0 005 0 1 2 d type float 32 arg s x num py array or class ~cntk ops function function any class ~cntk ops function function that output a tensor alpha float the alpha term of the above equation name str default to the name of the function instance in the network return cnt k ops function function an instance of class ~cntk ops function function from cnt k cnt k py import leaky re lu x sanitize input x return leaky re lu x alpha name
decorator def param relu alpha x name string parametric rectify linear operation compute the element wise parameter ic rectify linear of x max x 0 for x 0 and x alpha x otherwise the output tensor have the same shape as x example alpha c constant value 0 5 0 5 0 5 0 5 0 5 c param relu alpha 1 0 5 0 1 2 e val array 0 5 0 25 0 1 2 d type float 32 arg s alpha class ~cntk variables parameter same shape as x x num py array or class ~cntk ops function function any class ~cntk ops function function that output a tensor name str default to the name of the function instance in the network return cnt k ops function function an instance of class ~cntk ops function function from cnt k cnt k py import pre lu x sanitize input x return pre lu alpha x name
def rnn step shape cell shape none activation default override or s igm oid in it default override or gl or ot uniform in it bias default override or number enable self stabilization default override or false name string rnn step shape cell shape none activation s igm oid in it gl or ot uniform in it bias 0 enable self stabilization false name layer factory function to create a plain rnn block for use inside a recurrence the rnn block implement one step of the recurrence and be stateless it accept the previous state as its first argument and output its new state example a plain relu rnn layer from cnt k layer import relu rnn layer recurrence rnn step 500 activation c relu arg s shape int or tuple of in ts vector or tensor dimension of the output of this layer cell shape tuple default to none if give then the output state be first compute at cell shape and linearly project to shape activation class ~cntk ops function function default to sign moi d function to apply at the end e g relu in it scalar or num py array or mod cnt k initializer default to gl or ot uniform initial value of weight w in it bias scalar or num py array or mod cnt k initializer default to 0 initial value of weight b enable self stabilization boo l default to false if true then add a fun c ~cntk layer block stabilizer to all state relate projections but not the data input name str default to the name of the function instance in the network return class ~cntk ops function function a function prev h input h where h activation input w prev h r b activation get default override rnn step activation activation in it get default override rnn step in it in it in it bias get default override rnn step in it bias in it bias enable self stabilization get default override rnn step enable self stabilization enable self stabilization return recurrent block string shape cell shape activation activation use peepholes false in it in it in it bias in it bias enable self stabilization enable self stabilization name name
decorator def binary cross entropy output target name string r compute the binary cross entropy aka logistic loss between the output and target arg s output the compute posterior probability for a variable to be 1 from the network ty p a s igm oid target grind truth label 0 or 1 name str optional the name of the function instance in the network return class ~cntk ops function function to do add an example from cnt k cnt k py import binary cross entropy d type get data type output target output sanitize input output d type target sanitize input target d type return binary cross entropy output target name
decorator def cross entropy with soft max output vector target vector axis number name string r this operation compute the cross entropy between the target vector and the soft max of the output vector the elements of target vector have to be non negative and should sum to 1 the output vector can contain any value the function will internally compute the soft max of the output vector concretely math \mathrm soft max x \left \frac \exp x 1 \sum i\exp x i \quad\frac \exp x 1 \sum i\exp x i \quad\ldots\quad\frac \exp x 1 \sum i\exp x i \right math \mathrm cross\ entropy\ with\ soft max o t \sum i t i \log \mathrm soft max o i with the understand that the implementation can use equivalent formulas for efficiency and numerical stability example c cross entropy with soft max 1 1 1 50 0 0 0 1 e val array 0 d type float 32 c cross entropy with soft max 1 2 3 4 0 35 0 15 0 05 0 45 e val array 1 84019 d type float 32 arg s output vector the un scale compute output value from the network target vector usually it be one hot vector where the hot bite correspond to the label index but it can be any probability distribution over the label axis int or class ~cntk axis axis optional if give cross entropy will be compute along this axis name str optional the name of the function instance in the network return class ~cntk ops function function from cnt k cnt k py import cross entropy with soft max d type get data type output vector target vector output vector sanitize input output vector d type target vector sanitize input target vector d type axis sanitize axis axis return cross entropy with soft max output vector target vector axis name
decorator def cosine distance x y name string compute the cosine distance between x and y example a np as array 1 1 1 1 1 1 1 1 1 1 1 1 reshape 3 2 2 b np as array 1 1 1 1 1 1 1 1 1 1 1 1 reshape 3 2 2 x c sequence input variable shape 2 y c sequence input variable shape 2 np round c cosine distance x y e val x a y b 5 array 1 1 1 0 0 1 d type float 32 arg s x num py array or any class ~cntk ops function function that output a tensor name str optional the name of the function instance in the network return class ~cntk ops function function from cnt k cnt k py import cosine distance d type get data type x y x sanitize input x d type y sanitize input y d type return cosine distance x y name
def save checkpoint self filename external state save a checkpoint of the model and other trainer state at the specify file location in distribute environment the check point be do by the main worker arg s filename str filename to store the checkpoint external state dic t additional external state default be empty super trainer self save checkpoint filename py dic t to cnt k dic t external state
decorator def sgd parameters lr l 1 regular iz ation weight number l 2 regular iz ation weight number gaussian noise injection std dev number gradient clip threshold per sample np inf gradient clip with truncation true use mean gradient none mini batch size none epoch size none sgd parameters lr l 1 regular iz ation weight 0 l 2 regular iz ation weight 0 gaussian noise injection std dev 0 gradient clip threshold per sample np inf gradient clip with truncation true create an sgd learner instance to learn the parameters see 1 for more information on how to set the parameters arg s parameters list of parameters list of network parameters to tune these can be obtain by the parameters method of the root operator lr float list output of fun c learn parameter schedule a learn rate in float or a learn rate schedule see also fun c learn parameter schedule l 1 regular iz ation weight float optional the l 1 regular iz ation weight per sample default to 0 0 l 2 regular iz ation weight float optional the l 2 regular iz ation weight per sample default to 0 0 gaussian noise injection std dev float optional the standard deviation of the gaussian noise add to parameters post update default to 0 0 gradient clip threshold per sample float optional clip threshold per sample default to infinity gradient clip with truncation boo l default true use gradient clip with truncation use mean gradient boo l optional use average gradient as input to learner deprecate 2 2 use mini batch size parameter to specify the reference mini batch size mini batch size int default none the mini batch size that the learner s parameters be design or pre tune for this size be usually set to the same as the mini batch data source s size cnt k will perform automatic scale of the parameters to enable efficient model parameter update implementation while approximate the behavior of pre design and pre tune parameters in case that mini batch size be not specify cnt k will inherit the mini batch size from the learn rate schedule if the learn rate schedule do not specify the mini batch size cnt k will set it to at tr ignore set mini batch size to at tr ignore will have the learner apply as it be prevent cnt k perform any hyper parameter scale see also fun c learn parameter schedule epoch size optional int number of sample as a schedule unit for learn rate see also fun c learn parameter schedule return class ~cntk learners learner learner instance that can be pass to the class ~cntk train trainer trainer see also 1 l bot tou stochastic gradient descent trick https //www microsoft com/en us/research/publication/stochastic gradient trick neural network trick of the trade springer 2012 lr mini batch size infer learn rate schedule and ref mini batch size use mean gradient mini batch size lr epoch size gaussian noise injection std dev \ train parameter schedule gaussian noise injection std dev additional options cnt k py additional learn options additional options l 1 regular iz ation weight l 1 regular iz ation weight additional options l 2 regular iz ation weight l 2 regular iz ation weight additional options gaussian noise injection std dev gaussian noise injection std dev additional options gradient clip threshold per sample gradient clip threshold per sample additional options gradient clip with truncation gradient clip with truncation if mini batch size be not none additional options dic t options cnt k py learner mini batch size cnt k py size t wrapper mini batch size need this to make proper type dictionary value opt cnt k py sgd learner parameters lr additional options opt be mini batch size explicitly specify mini batch size be not none return opt
def sequential layer name string sequential layer name layer factory function to create a composite that apply a sequence of layer or any function onto an input sequential f g h x mean the same as h g f x the list of function may also include tuples of function in that case each function in a tuple be apply to the input and the result be a tuple contain the result of these function applications if follow by another function ty p plus or splice the tuple items form the arguments to that function intermediate value in the chain can be access by name by insert a label name layer note an equivalent way of write sequential f g h x be f g h example from cnt k layer import sequence class i fier map a one hot word sequence to a scalar probability value the recurrence be a fold mean only the final hide state be produce the label layer allow to access the final hide layer by name model sequential embed 300 fold lstm 500 label hide dense 1 activation s igm oid model update signature sequence tensor 30000 model hide shape 500 simple example that square an input value f sequential c log lambda x 2 x c exp the second function be a python lambda f update signature 1 f np array 2 log time 2 exp be the same as compute the square array 4 d type float 32 use function tuples to implement a bidirectional lstm bi lstm sequential recurrence lstm 250 first tuple entry forward pass recurrence lstm 250 go backwards true second backward pass splice splice both on top of each other use function tuple to implement a res net block the function tuple apply all items to the input and emit a tuple with the result that then act as the arguments to the next one here we say convolution identity which generate two arguments to the next function the first be the convolution the second be the input pass through follow that with plus implement the res net formula from cnt k ops import plus relu res net layer sequential convolution 3 3 64 activation none first tuple entry identity second tuple entry be a pass through plus this sum both relu activation apply afterwards simple function tuples example with value f sequential lambda x x x identity splice compute tuple x 2 x and splice both value f update signature 1 f np array 2 array 4 2 d type float 32 arg s layer list of class ~cntk ops function function equivalent python function tuples of function or list there of the list of function to apply in sequence a tuple a ply each of its items to the input and result in a tuple value an item that be a list will be flatten return cnt k ops function function a function that accept one argument and apply the give function one after another if not be instance layer list to support nest list run every item recursively through sequential to do be this confuse w r t tuple which be parallel and list which be sequential? return layer from fun c tool import reduce layer sequential layer for layer in layer expand all layer recursively compose function reduce lambda f g f g layer identity return inject name compose function name
decorator def per dim mean variance normalize operand mean in v std dev name string compute per dimension mean variance normalization of the specify input operand arg s operand the variable to be normalize mean num py array per dimension mean to use for the normalization in v std dev num py array per dimension standard deviation to use for the normalization name str optional the name of the function instance in the network return class ~cntk ops function function from cnt k cnt k py import per dim mean variance normalize mean sanitize input mean get data type mean in v std dev sanitize input in v std dev get data type in v std dev return per dim mean variance normalize operand mean in v std dev name
decorator def stop gradient input name string output its input as it be and prevent any gradient contribution from its output to its input arg s input class ~cntk ops function function that output a tensor name str optional the name of the function instance in the network return class ~cntk ops function function from cnt k cnt k py import stop gradient d type get data type input op sanitize input input d type return stop gradient op name
decorator def crop manual node input node referent offset x offset y name string crop input along spatial dimension so that it match spatial size of reference input crop offset be give in pixels arg s node input class ~cntk ops function function that output the tensor to be crop node referent class ~cntk ops function function that output the reference tensor offset x int horizontal crop offset offset y int vertical crop offset name str optional the name of the function instance in the network return class ~cntk ops function function from cnt k cnt k py import crop arg input sanitize input node input get data type node input arg ref sanitize input node referent get data type node referent return crop arg input arg ref offset x offset y name
def unique enumeration class decorator for enumerations ensure unique member value duplicate for name member in enumeration members items if name member name duplicate append name member name if duplicate alias detail string join string alias name for alias name in duplicate raise value error string enumeration alias detail return enumeration
