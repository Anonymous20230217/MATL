
( input : input tensor of any shape ) ( p  float  : the exponent value in the norm formulation  default  2 ) ( dim  int  : the dimension to reduce  default  1 ) ( eps  float  : small value to avoid division by zero  default  1e 12 ) ( out  Tensor  optional  : the output tensor  if out be use  this operation won’t be differentiable  )
( dim  int  optional  : dimension where cosine similarity be compute  default  1 ) ( eps  float  optional  : small value to avoid division by zero  default  1e 8 )
( params  iterable  : iterable of parameters to optimize or dicts define parameter group ) ( lr  float  optional  : learn rate  default  1e 3  ) ( betas  Tuple float  float   optional  : coefficients use for compute run average of gradient and its square  default   0 9  0 999   ) ( eps  float  optional  : term add to the denominator to improve numerical stability  default  1e 8  ) ( weight decay  float  optional  : weight decay  l2 penalty   default  0  ) ( amsgrad  boolean  optional  : whether to use the amsgrad variant of this algorithm from the paper on the convergence of adam and beyond  default  false  ) ( maximize  bool  optional  : maximize the params base on the objective  instead of minimize  default  false  )
( in channels  int  : number of channel in the input image ) ( out channels  int  : number of channel produce by the convolution ) ( kernel size  int or tuple  : size of the convolve kernel ) ( stride  int or tuple  optional  : stride of the convolution  default  1 ) ( padding  int  tuple or str  optional  : pad add to both side of the input  default  0 ) ( padding mode  string  optional  : 'zeros'  'reflect'  'replicate' or 'circular'  default  'zeros' ) ( dilation  int or tuple  optional  : space between kernel elements  default  1 ) ( groups  int  optional  : number of block connections from input channel to output channel  default  1 ) ( bias  bool  optional  : if true  add a learnable bias to the output  default  true )
( input size : the number of expect feature in the input x ) ( hidden size : the number of feature in the hide state h ) ( num layers : number of recurrent layer  e g   set num layer 2 would mean stack two lstms together to form a stack lstm  with the second lstm take in output of the first lstm and compute the final result  default  1 ) ( bias : if false  then the layer do not use bias weight b ih and b hh  default  true ) ( batch first : if true  then the input and output tensors be provide as  batch  seq  feature  instead of  seq  batch  feature   note that this do not apply to hide or cell state  see the inputs/outputs section below for detail   default  false ) ( dropout : if non zero  introduce a dropout layer on the output of each lstm layer except the last layer  with dropout probability equal to dropout  default  0 ) ( bidirectional : if true  become a bidirectional lstm  default  false ) ( proj size : if > 0  will use lstm with projections of correspond size  default  0 )
( tensor : an n dimensional torch tensor ) ( a : the negative slope of the rectifier use after this layer  only use with 'leaky relu'  ) ( mode : either 'fan in'  default  or 'fan out'  choose 'fan in' preserve the magnitude of the variance of the weight in the forward pass  choose 'fan out' preserve the magnitudes in the backwards pass  ) ( nonlinearity : the non linear function  nn functional name   recommend to use only with 'relu' or 'leaky relu'  default   )
( tensor  Tensor  : tensor to split  ) ( split size or sections  int  or  list int   : size of a single chunk or list of size for each chunk ) ( dim  int  : dimension along which to split the tensor  )
( num embeddings  int  : size of the dictionary of embeddings ) ( embedding dim  int  : the size of each embed vector ) ( padding idx  int  optional  : if specify  the entries at pad idx do not contribute to the gradient  therefore  the embed vector at pad idx be not update during train  i e  it remain as a fix “pad”  for a newly construct embed  the embed vector at pad idx will default to all zero  but can be update to another value to be use as the pad vector  ) ( max norm  float  optional  : if give  each embed vector with norm larger than max norm be renormalize to have norm max norm  ) ( norm type  float  optional  : the p of the p norm to compute for the max norm option  default 2  ) ( scale grad by freq  boolean  optional  : if give  this will scale gradients by the inverse of frequency of the word in the mini batch  default false  ) ( sparse  bool  optional  : if true  gradient w r t  weight matrix will be a sparse tensor  see note for more detail regard sparse gradients  )
( output size : the target output size  single integer or double integer tuple  )
( in channels  int  : number of channel in the input image ) ( out channels  int  : number of channel produce by the convolution ) ( kernel size  int or tuple  : size of the convolve kernel ) ( stride  int or tuple  optional  : stride of the convolution  default  1 ) ( padding  int or tuple  optional  : dilation    kernel size   1    pad zero pad will be add to both side of each dimension in the input  default  0 ) ( output padding  int or tuple  optional  : additional size add to one side of each dimension in the output shape  default  0 ) ( groups  int  optional  : number of block connections from input channel to output channel  default  1 ) ( bias  bool  optional  : if true  add a learnable bias to the output  default  true ) ( dilation  int or tuple  optional  : space between kernel elements  default  1 )
( low  float or Tensor  : lower range  inclusive   ) ( high  float or Tensor  : upper range  exclusive   )
( tensor : an n dimensional torch tensor ) ( gain : an optional scale factor )
( kernel size : the size of the window to take a max over ) ( stride : the stride of the window  default value be kernel size ) ( padding : implicit zero pad to be add on all three side ) ( dilation : a parameter that control the stride of elements in the window ) ( return indices : if true  will return the max indices along with the output  useful for torch nn maxunpool3d later ) ( ceil mode : when true  will use ceil instead of floor to compute the output shape )
( beta : the β\betaβ value for the softplus formulation  default  1 ) ( threshold : value above this revert to a linear function  default  20 )
( kernel size : the size of the window to take a max over ) ( stride : the stride of the window  default value be kernel size ) ( padding : implicit zero pad to be add on both side ) ( dilation : a parameter that control the stride of elements in the window ) ( return indices : if true  will return the max indices along with the output  useful for torch nn maxunpool2d later ) ( ceil mode : when true  will use ceil instead of floor to compute the output shape )
( input  Tensor  : input ) ( dim  int  : a dimension along which log softmax will be compute  ) ( dtype  torch dtype  optional  : the desire data type of return tensor  if specify  the input tensor be cast to dtype before the operation be perform  this be useful for prevent data type overflow  default  none  )
( input  Tensor  : input ) ( dim  int  : a dimension along which softmax will be compute  ) ( dtype  torch dtype  optional  : the desire data type of return tensor  if specify  the input tensor be cast to dtype before the operation be perform  this be useful for prevent data type overflow  default  none  )
( upscale factor  int  : factor to increase spatial resolution by )

( weight  Tensor  optional  : a manual rescale weight give to each class  if give  have to be a tensor of size c ) ( size average  bool  optional  : deprecate  see reduction   by default  the losses be average over each loss element in the batch  note that for some losses  there be multiple elements per sample  if the field size average be set to false  the losses be instead sum for each minibatch  ignore when reduce be false  default  true ) ( ignore index  int  optional  : specify a target value that be ignore and do not contribute to the input gradient  when size average be true  the loss be average over non ignore target  note that ignore index be only applicable when the target contain class indices  ) ( reduce  bool  optional  : deprecate  see reduction   by default  the losses be average or sum over observations for each minibatch depend on size average  when reduce be false  return a loss per batch element instead and ignore size average  default  true ) ( reduction  string  optional  : specify the reduction to apply to the output  'none'   'mean'   'sum'  'none'  no reduction will be apply  'mean'  the weight mean of the output be take  'sum'  the output will be sum  note  size average and reduce be in the process of be deprecate  and in the meantime  specify either of those two args will override reduction  default  'mean' ) ( label smoothing  float  optional  : a float in  0 0  1 0   specify the amount of smooth when compute the loss  where 0 0 mean no smooth  the target become a mixture of the original grind truth and a uniform distribution as describe in rethink the inception architecture for computer vision  default  0 00 00 0  )


( input : input tensor  minibatch in channel id ih iw  \text minibatch    \text in\ channel    id  ih   iw  minibatch in channel id ih iw   minibatch dim optional  ) ( kernel size : size of the pool region  can be a single number or a tuple  kt  kh  kw  ) ( stride : stride of the pool operation  can be a single number or a tuple  st  sh  sw   default  kernel size ) ( padding : implicit negative infinity pad to be add on both side  must be >  0 and <  kernel size / 2  ) ( dilation : the stride between elements within a slide window  must be > 0  ) ( ceil mode : if true  will use ceil instead of floor to compute the output shape  this ensure that every element in the input tensor be cover by a slide window  ) ( return indices : if true  will return the argmax along with the max value  useful for torch nn functional max unpool3d later )
( input : tensor of arbitrary shape as probabilities  ) ( target : tensor of the same shape as input with value between 0 and 1  ) ( weight  Tensor  optional  : a manual rescale weight if provide it’s repeat to match input tensor shape ) ( size average  bool  optional  : deprecate  see reduction   by default  the losses be average over each loss element in the batch  note that for some losses  there multiple elements per sample  if the field size average be set to false  the losses be instead sum for each minibatch  ignore when reduce be false  default  true ) ( reduce  bool  optional  : deprecate  see reduction   by default  the losses be average or sum over observations for each minibatch depend on size average  when reduce be false  return a loss per batch element instead and ignore size average  default  true ) ( reduction  string  optional  : specify the reduction to apply to the output  'none'   'mean'   'sum'  'none'  no reduction will be apply  'mean'  the sum of the output will be divide by the number of elements in the output  'sum'  the output will be sum  note  size average and reduce be in the process of be deprecate  and in the meantime  specify either of those two args will override reduction  default  'mean' )

( input  Tensor  : n dimensional tensor ) ( pad  tuple  : m elements tuple  where m2≤\frac m  2  \leq2m​≤ input dimension and mmm be even  ) ( mode : 'constant'  'reflect'  'replicate' or 'circular'  default  'constant' ) ( value : fill value for 'constant' pad  default  0 )
( kernel size : the size of the slide window  must be > 0  ) ( stride : the stride of the slide window  must be > 0  default value be kernel size  ) ( padding : implicit negative infinity pad to be add on both side  must be >  0 and <  kernel size / 2  ) ( dilation : the stride between elements within a slide window  must be > 0  ) ( return indices : if true  will return the argmax along with the max value  useful for torch nn maxunpool1d later ) ( ceil mode : if true  will use ceil instead of floor to compute the output shape  this ensure that every element in the input tensor be cover by a slide window  )
( params  iterable  : iterable of parameters to optimize or dicts define parameter group ) ( rho  float  optional  : coefficient use for compute a run average of square gradients  default  0 9  ) ( eps  float  optional  : term add to the denominator to improve numerical stability  default  1e 6  ) ( lr  float  optional  : coefficient that scale delta before it be apply to the parameters  default  1 0  ) ( weight decay  float  optional  : weight decay  l2 penalty   default  0  )
( normalized shape  int or list or torch Size  : input shape from an expect input of size   ∗×normalized shape 0 ×normalized shape 1 ×…×normalized shape −1     \times \text normalized\ shape  0  \times \text normalized\ shape  1      \times \ldots \times \text normalized\ shape   1     ∗×normalized shape 0 ×normalized shape 1 ×…×normalized shape −1  if a single integer be use  it be treat as a singleton list  and this module will normalize over the last dimension which be expect to be of that specific size  ) ( eps : a value add to the denominator for numerical stability  default  1e 5 ) ( elementwise affine : a boolean value that when set to true  this module have learnable per element affine parameters initialize to ones  for weight  and zero  for bias   default  true  )
( num parameters  int  : number of aaa to learn  although it take an int as input  there be only two value be legitimate  1  or the number of channel at input  default  1 ) ( init  float  : the initial value of aaa  default  0 25 )

( input size : the number of expect feature in the input x ) ( hidden size : the number of feature in the hide state h ) ( num layers : number of recurrent layer  e g   set num layer 2 would mean stack two grus together to form a stack gru  with the second gru take in output of the first gru and compute the final result  default  1 ) ( bias : if false  then the layer do not use bias weight b ih and b hh  default  true ) ( batch first : if true  then the input and output tensors be provide as  batch  seq  feature  instead of  seq  batch  feature   note that this do not apply to hide or cell state  see the inputs/outputs section below for detail   default  false ) ( dropout : if non zero  introduce a dropout layer on the output of each gru layer except the last layer  with dropout probability equal to dropout  default  0 ) ( bidirectional : if true  become a bidirectional gru  default  false )
( input : input tensor of shape  minibatch in channel iw  \text minibatch    \text in\ channel    iw  minibatch in channel iw   minibatch dim optional  ) ( kernel size : the size of the window  can be a single number or a tuple  kw   ) ( stride : the stride of the window  can be a single number or a tuple  sw    default  kernel size ) ( padding : implicit negative infinity pad to be add on both side  must be >  0 and <  kernel size / 2  ) ( dilation : the stride between elements within a slide window  must be > 0  ) ( ceil mode : if true  will use ceil instead of floor to compute the output shape  this ensure that every element in the input tensor be cover by a slide window  ) ( return indices : if true  will return the argmax along with the max value  useful for torch nn functional max unpool1d later )

( in channels  int  : number of channel in the input image ) ( out channels  int  : number of channel produce by the convolution ) ( kernel size  int or tuple  : size of the convolve kernel ) ( stride  int or tuple  optional  : stride of the convolution  default  1 ) ( padding  int  tuple or str  optional  : pad add to all four side of the input  default  0 ) ( padding mode  string  optional  : 'zeros'  'reflect'  'replicate' or 'circular'  default  'zeros' ) ( dilation  int or tuple  optional  : space between kernel elements  default  1 ) ( groups  int  optional  : number of block connections from input channel to output channel  default  1 ) ( bias  bool  optional  : if true  add a learnable bias to the output  default  true )
( start dim : first dim to flatten  default   1   ) ( end dim : last dim to flatten  default    1   )
( num features : number of feature or channel ccc of the input ) ( eps : a value add to the denominator for numerical stability  default  1e 5 ) ( momentum : the value use for the run mean and run var computation  can be set to none for cumulative move average  i e  simple average   default  0 1 ) ( affine : a boolean value that when set to true  this module have learnable affine parameters  default  true ) ( track running stats : a boolean value that when set to true  this module track the run mean and variance  and when set to false  this module do not track such statistics  and initialize statistics buffer run mean and run var as none  when these buffer be none  this module always use batch statistics  in both train and eval modes  default  true )
( obj : save object ) ( f : a file like object  have to implement write and flush  or a string or os pathlike object contain a file name ) ( pickle module : module use for pickle metadata and object ) ( pickle protocol : can be specify to override the default protocol )
( dim  int  : a dimension along which softmax will be compute  so every slice along dim will sum to 1   )
( kernel size : the size of the window to take a max over  can be a single number kkk  for a square kernel of k×kk \times kk×k  or a tuple  kh  kw  ) ( output size : the target output size of the image of the form oh×owoh \times owoh×ow  can be a tuple  oh  ow  or a single number ohohoh for a square image oh×ohoh \times ohoh×oh ) ( output ratio : if one want to have an output size as a ratio of the input size  this option can be give  this have to be a number or tuple in the range  0  1  ) ( return indices : if true  will return the indices along with the output  useful to pass to max unpool2d    )
( input  Tensor  : predict unnormalized score  often refer to as logits   see shape section below for support shape  ) ( target  Tensor  : grind truth class indices or class probabilities  see shape section below for support shape  ) ( weight  Tensor  optional  : a manual rescale weight give to each class  if give  have to be a tensor of size c ) ( size average  bool  optional  : deprecate  see reduction   by default  the losses be average over each loss element in the batch  note that for some losses  there multiple elements per sample  if the field size average be set to false  the losses be instead sum for each minibatch  ignore when reduce be false  default  true ) ( ignore index  int  optional  : specify a target value that be ignore and do not contribute to the input gradient  when size average be true  the loss be average over non ignore target  note that ignore index be only applicable when the target contain class indices  default   100 ) ( reduce  bool  optional  : deprecate  see reduction   by default  the losses be average or sum over observations for each minibatch depend on size average  when reduce be false  return a loss per batch element instead and ignore size average  default  true ) ( reduction  string  optional  : specify the reduction to apply to the output  'none'   'mean'   'sum'  'none'  no reduction will be apply  'mean'  the sum of the output will be divide by the number of elements in the output  'sum'  the output will be sum  note  size average and reduce be in the process of be deprecate  and in the meantime  specify either of those two args will override reduction  default  'mean' ) ( label smoothing  float  optional  : a float in  0 0  1 0   specify the amount of smooth when compute the loss  where 0 0 mean no smooth  the target become a mixture of the original grind truth and a uniform distribution as describe in rethink the inception architecture for computer vision  default  0 00 00 0  )
( tensor : an n dimensional torch tensor ) ( mean : the mean of the normal distribution ) ( std : the standard deviation of the normal distribution )
( alpha : the α\alphaα value for the elu formulation  default  1 0 ) ( inplace : can optionally do the operation in place  default  false )
( in channels  int  : number of channel in the input image ) ( out channels  int  : number of channel produce by the convolution ) ( kernel size  int or tuple  : size of the convolve kernel ) ( stride  int or tuple  optional  : stride of the convolution  default  1 ) ( padding  int or tuple  optional  : dilation    kernel size   1    pad zero pad will be add to both side of each dimension in the input  default  0 ) ( output padding  int or tuple  optional  : additional size add to one side of each dimension in the output shape  default  0 ) ( groups  int  optional  : number of block connections from input channel to output channel  default  1 ) ( bias  bool  optional  : if true  add a learnable bias to the output  default  true ) ( dilation  int or tuple  optional  : space between kernel elements  default  1 )
( in features : size of each input sample ) ( out features : size of each output sample ) ( bias : if set to false  the layer will not learn an additive bias  default  true )
( tensor : an n dimensional torch tensor ) ( a : the lower bind of the uniform distribution ) ( b : the upper bind of the uniform distribution )

( input : input tensor  minibatch in channel ih iw  \text minibatch    \text in\ channel    ih   iw  minibatch in channel ih iw   minibatch dim optional  ) ( kernel size : size of the pool region  can be a single number or a tuple  kh  kw  ) ( stride : stride of the pool operation  can be a single number or a tuple  sh  sw   default  kernel size ) ( padding : implicit negative infinity pad to be add on both side  must be >  0 and <  kernel size / 2  ) ( dilation : the stride between elements within a slide window  must be > 0  ) ( ceil mode : if true  will use ceil instead of floor to compute the output shape  this ensure that every element in the input tensor be cover by a slide window  ) ( return indices : if true  will return the argmax along with the max value  useful for torch nn functional max unpool2d later )
( p : probability of an element to be zero  default  0 5 ) ( training : apply dropout if be true  default  true ) ( inplace : if set to true  will do this operation in place  default  false )
